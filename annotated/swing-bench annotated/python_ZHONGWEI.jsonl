{"problem_statement": "Allow 1 or 3+ columns as input in the array feature selection\nRight now the `array` feature selection only allows combining exactly two input columns into an output column. To make this more flexible, we could support passing any number of columns, with a minimum of 1. This should be a small change in hlink/linking/core/transforms.py, where we unpack `feature_selection[\"input_columns\"]` with\r\n\r\n```\r\ncol1, col2 = feature_selection[\"input_columns\"]\r\n```\r\n\r\nThe `pyspark.sql.functions.array()` function which we're using accepts a variable number of arguments.\n", "patch": "diff --git a/docs/_sources/feature_selection_transforms.md.txt b/docs/_sources/feature_selection_transforms.md.txt\nindex 0e7e332..c87440a 100644\n--- a/docs/_sources/feature_selection_transforms.md.txt\n+++ b/docs/_sources/feature_selection_transforms.md.txt\n@@ -46,10 +46,10 @@ transform = \"sql_condition\"\n \n ## array\n \n-Combine two input columns into an array output column.\n+Combine any number of input columns into a single array output column.\n \n * Attributes:\n-  * `input_columns` - Type: list of strings. Required. The two input columns.\n+  * `input_columns` - Type: list of strings. Required. The list of input columns.\n   * `output_column` - Type: `string`. Required.\n \n ```\ndiff --git a/docs/feature_selection_transforms.html b/docs/feature_selection_transforms.html\nindex bef511e..075d3f2 100644\n--- a/docs/feature_selection_transforms.html\n+++ b/docs/feature_selection_transforms.html\n@@ -83,11 +83,11 @@ <h2>sql_condition<a class=\"headerlink\" href=\"#sql-condition\" title=\"Link to this\n </section>\n <section id=\"array\">\n <h2>array<a class=\"headerlink\" href=\"#array\" title=\"Link to this heading\">\u00b6</a></h2>\n-<p>Combine two input columns into an array output column.</p>\n+<p>Combine any number of input columns into a single array output column.</p>\n <ul class=\"simple\">\n <li><p>Attributes:</p>\n <ul>\n-<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">input_columns</span></code> - Type: list of strings. Required. The two input columns.</p></li>\n+<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">input_columns</span></code> - Type: list of strings. Required. The list of input columns.</p></li>\n <li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">output_column</span></code> - Type: <code class=\"docutils literal notranslate\"><span class=\"pre\">string</span></code>. Required.</p></li>\n </ul>\n </li>\ndiff --git a/docs/searchindex.js b/docs/searchindex.js\nindex c64d96a..f74a1bf 100644\n--- a/docs/searchindex.js\n+++ b/docs/searchindex.js\n@@ -1,1 +1,1 @@\n-Search.setIndex({\"docnames\": [\"column_mappings\", \"comparison_types\", \"config\", \"feature_selection_transforms\", \"index\", \"installation\", \"introduction\", \"link_tasks\", \"models\", \"pipeline_features\", \"running_the_program\", \"substitutions\", \"use_examples\"], \"filenames\": [\"column_mappings.md\", \"comparison_types.md\", \"config.md\", \"feature_selection_transforms.md\", \"index.rst\", \"installation.md\", \"introduction.md\", \"link_tasks.md\", \"models.md\", \"pipeline_features.md\", \"running_the_program.md\", \"substitutions.md\", \"use_examples.md\"], \"titles\": [\"Column Mappings\", \"Comparison types, transform add-ons, aggregate features, and household aggregate features\", \"Configuration\", \"Feature Selection transforms\", \"Welcome to hlink\\u2019s documentation!\", \"Installation\", \"Introduction\", \"Link Tasks\", \"Models\", \"Pipeline generated features\", \"Running hlink\", \"Substitutions\", \"Advanced Workflow Examples\"], \"terms\": {\"each\": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10], \"read\": [0, 1, 2, 7, 10], \"from\": [0, 1, 2, 4, 6, 7, 8, 9, 10, 12], \"input\": [0, 1, 2, 3, 6, 7, 10, 11], \"dataset\": [0, 1, 2, 6, 7, 10, 12], \"hlink\": [0, 1, 2, 5, 6, 7, 12], \"It\": [0, 1, 2, 6, 10, 12], \"ha\": [0, 1, 2, 6, 10, 12], \"column_nam\": [0, 1, 2, 11], \"attribut\": [0, 1, 2, 3, 7, 8, 9, 10, 11], \"which\": [0, 1, 2, 3, 6, 7, 9, 10, 12], \"specifi\": [0, 1, 2, 6, 7, 9, 10, 11], \"name\": [0, 1, 2, 10, 11], \"both\": [0, 1, 2, 7, 12], \"option\": [0, 1, 2, 3, 6, 7, 8, 10, 12], \"mai\": [0, 2, 6, 7, 10], \"have\": [0, 1, 2, 5, 6, 7, 8, 10, 12], \"an\": [0, 1, 2, 3, 6, 8, 10], \"alia\": [0, 2, 7], \"give\": [0, 2], \"new\": [0, 2, 12], \"us\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12], \"support\": [0, 7, 8], \"some\": [0, 1, 2, 3, 6, 7, 10], \"make\": [0, 1, 2, 5, 12], \"chang\": [0, 1, 2, 5, 10, 12], \"data\": [0, 1, 4, 6, 7, 10], \"thei\": [0, 1, 2, 7, 10], \"ar\": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12], \"These\": [0, 1, 2, 3, 6, 7, 8, 9], \"clean\": [0, 6], \"harmon\": 0, \"The\": [0, 1, 2, 3, 5, 7, 8, 9, 10, 12], \"avail\": [0, 1, 2, 3, 5, 7, 8, 9, 12], \"list\": [0, 1, 2, 3, 4, 8, 10, 11], \"below\": [0, 1, 2, 3, 8, 9, 10], \"section\": [0, 1, 2, 12], \"By\": [0, 2, 10], \"default\": [0, 1, 2, 7, 8, 10], \"must\": [0, 1, 2, 8, 9, 11], \"same\": [0, 1, 2, 6, 7, 10], \"With\": [0, 9], \"override_column_a\": [0, 2, 3], \"override_column_b\": [0, 2, 3], \"you\": [0, 1, 2, 5, 10, 11, 12], \"can\": [0, 1, 2, 5, 6, 7, 8, 10, 12], \"differ\": [0, 1, 2, 4, 6, 7], \"either\": [0, 1, 2, 6, 11], \"A\": [0, 1, 2, 9, 10], \"b\": [0, 1, 2, 10], \"when\": [0, 1, 2, 3, 7, 12], \"do\": [0, 1, 3, 10, 12], \"thi\": [0, 1, 2, 5, 6, 7, 9, 10, 12], \"appli\": [0, 2, 3, 7, 12], \"onli\": [0, 1, 2, 7, 12], \"non\": 0, \"overrid\": [0, 2], \"also\": [0, 1, 2, 5, 6, 7, 9, 10, 12], \"provid\": [0, 2, 3, 6, 7, 9, 10], \"override_transform\": [0, 2], \"describ\": [0, 2, 10], \"type\": [0, 2, 3, 4, 7, 8, 10, 11, 12], \"oper\": [0, 2], \"singl\": [0, 2, 10, 12], \"output\": [0, 1, 2, 3, 6, 7, 10, 12], \"more\": [0, 1, 2, 9, 10, 12], \"than\": [0, 1, 2, 8], \"one\": [0, 1, 2, 7], \"order\": [0, 2, 7], \"so\": [0, 1, 2, 5, 12], \"anoth\": [0, 1, 3, 7], \"format\": 0, \"letter\": 0, \"t\": [0, 1, 2, 3, 12], \"u\": 0, \"repres\": [0, 1, 2, 3, 9, 10], \"arbitrari\": 0, \"requir\": [0, 1, 2, 3, 4, 7, 9, 10, 11], \"addit\": [0, 1, 2, 3, 5, 6, 10], \"vari\": [0, 2], \"inform\": [0, 1, 2, 10], \"appear\": [0, 1], \"its\": [0, 1, 6, 10], \"suffix\": 0, \"mean\": [0, 2], \"two\": [0, 1, 2, 3, 6, 7, 9, 10, 12], \"link\": [0, 1, 2, 4, 6, 8], \"most\": [0, 1, 7, 10], \"independ\": [0, 2], \"For\": [0, 1, 2, 7, 10, 12], \"exampl\": [0, 1, 2], \"taken\": [0, 1], \"10\": [0, 2, 5, 12], \"year\": [0, 1, 2, 3, 4], \"apart\": 0, \"want\": [0, 1, 2, 10, 12], \"standard\": [0, 1, 11], \"ag\": [0, 1, 2, 3], \"variabl\": [0, 1, 2, 12], \"i\": [0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12], \"compar\": [0, 1, 2, 6, 7], \"between\": [0, 1, 2, 6, 7, 10, 12], \"To\": [0, 1, 5, 7, 10], \"could\": [0, 2], \"creat\": [0, 2, 6, 7, 9, 10, 11, 12], \"age_at_dataset_b\": 0, \"ad\": [0, 1, 2], \"column_map\": [0, 2, 7], \"valu\": [0, 1, 2, 8, 9, 10, 11], \"As\": 0, \"suppos\": 0, \"record\": [0, 1, 2, 6, 7], \"person\": [0, 1, 6], \"\": [0, 1, 2, 6, 7, 10, 11], \"first\": [0, 1, 2, 5, 7, 10, 11], \"string\": [0, 1, 2, 3, 7, 8, 10, 11], \"In\": [0, 1, 6, 10, 12], \"call\": 0, \"namefrst\": [0, 1, 2], \"entir\": 0, \"lowercas\": 0, \"first_nam\": 0, \"uppercas\": 0, \"follow\": [0, 1, 6, 10, 11, 12], \"configur\": [0, 1, 6, 10, 12], \"add\": [0, 4], \"_\": [0, 1, 2, 3, 4, 8, 9, 10], \"given\": [0, 1, 2, 3, 8, 12], \"numer\": [0, 1], \"11\": [0, 2, 5, 9], \"concat\": 0, \"concaten\": [0, 1], \"end\": [0, 1, 2, 3, 11], \"col\": [0, 1], \"togeth\": [0, 1, 2], \"take\": [0, 1, 2, 3, 7, 10], \"column_to_append\": 0, \"multipl\": [0, 1, 2, 10], \"time\": [0, 2, 7, 10], \"row\": [0, 2], \"If\": [0, 1, 2, 3, 7, 8, 10, 11], \"automat\": [0, 2, 5, 7], \"convert\": [0, 1, 2], \"befor\": [0, 1, 2, 3, 5, 7], \"statefip\": [0, 1, 2], \"counti\": [0, 1], \"strip\": [0, 7], \"alphabet\": 0, \"charact\": 0, \"lower\": [0, 1], \"case\": [0, 1, 2, 3, 6], \"white\": 0, \"space\": [0, 2, 3, 11], \"start\": [0, 11], \"ration\": 0, \"word\": [0, 4], \"replac\": [0, 1, 4], \"sinc\": [0, 2], \"peopl\": [0, 1, 6, 10], \"raw\": [0, 2, 7, 10], \"censu\": [0, 7, 12], \"contain\": [0, 1, 11], \"lead\": 0, \"better\": [0, 6], \"match\": [0, 1, 4, 6, 10, 11, 12], \"remov\": 0, \"qmark\": 0, \"hyphen\": 0, \"punctuat\": 0, \"apostroph\": 0, \"altern\": [0, 2], \"surround\": 0, \"all\": [0, 1, 2, 3, 7, 8, 9, 10], \"them\": [0, 1, 2, 7], \"jr\": [0, 2], \"sr\": [0, 2], \"ii\": [0, 2], \"iii\": [0, 2], \"stop\": 0, \"last\": [0, 1, 7, 9], \"street\": [0, 1], \"avenu\": [0, 11], \"blvd\": 0, \"circl\": 0, \"court\": 0, \"road\": 0, \"prefix\": 0, \"like\": [0, 2, 7, 10], \"m\": [0, 1], \"mr\": 0, \"ah\": 0, \"chines\": 0, \"condens\": 0, \"whitespac\": [0, 7], \"leav\": 0, \"behind\": 0, \"arrai\": [0, 2, 4, 8, 9], \"namefrst_split\": [0, 2], \"namefrst_clean\": [0, 2], \"index\": [0, 5], \"select\": [0, 1, 4, 6, 10, 12], \"element\": 0, \"posit\": [0, 1, 2, 4, 6], \"second\": [0, 1, 2, 11], \"1\": [0, 1, 2, 4, 7, 8, 9, 10, 12], \"item\": 0, \"set\": [0, 1, 2, 3, 6, 7, 10, 12], \"Then\": [0, 5], \"0\": [0, 1, 2, 7, 8, 9, 10, 12], \"initi\": [0, 1, 10], \"probabl\": [0, 2, 8], \"middl\": [0, 1], \"namefrst_mid_init\": [0, 1], \"otherwis\": [0, 1, 9, 12], \"known\": 0, \"recod\": 0, \"birthyr\": [0, 2], \"clean_birthyr\": [0, 2, 3], \"9999\": [0, 2, 9], \"1999\": [0, 2], \"9998\": 0, \"divid\": 0, \"int\": [0, 1, 2, 3, 8], \"integ\": [0, 1, 2, 9], \"result\": [0, 1, 6, 9, 10, 12], \"instanc\": [0, 8], \"birthplac\": 0, \"detail\": [0, 2, 10], \"version\": [0, 5, 12], \"gener\": [0, 1, 4, 6, 7, 10], \"least\": [0, 1], \"signific\": 0, \"digit\": 0, \"we\": [0, 1, 10, 12], \"simpli\": [0, 2], \"drop\": [0, 2, 10], \"100\": [0, 2, 12], \"round\": [0, 2], \"lowest\": 0, \"whole\": [0, 6], \"number\": [0, 1, 2, 7, 8, 10], \"floor\": 0, \"function\": [0, 1, 2, 6, 10], \"bpl\": [0, 1, 2], \"bpl_root\": 0, \"condit\": [0, 1, 2, 3, 4, 7], \"logic\": 0, \"work\": [0, 1, 2, 5, 7, 10, 12], \"sql\": [0, 1, 2, 3, 4, 7, 10], \"express\": [0, 1, 2], \"claus\": [0, 1], \"if_valu\": 0, \"else_valu\": 0, \"race\": [0, 1, 2, 9, 12], \"ipum\": [0, 6], \"code\": [0, 1, 2, 5], \"categori\": [0, 8], \"get\": [0, 1, 10], \"down\": [0, 6, 12], \"nearest\": 0, \"produc\": [0, 10], \"relat\": [0, 1, 2], \"hundr\": 0, \"300\": 0, \"child\": [0, 8], \"household\": [0, 4, 6, 8, 10, 12], \"head\": 0, \"301\": 0, \"302\": 0, \"adopt\": 0, \"303\": 0, \"step\": [0, 1, 2, 6], \"usual\": [0, 7, 12], \"need\": [0, 1, 2, 7, 10, 12], \"2\": [0, 1, 2, 3, 7, 8, 11, 12], \"spous\": 0, \"3\": [0, 1, 2, 5, 7, 8, 9, 12], \"4\": [0, 1, 8], \"law\": 0, \"5\": [0, 1, 2, 8, 9, 10, 12], \"parent\": [0, 1, 11], \"6\": [0, 2, 8, 9, 12], \"7\": [0, 1, 2, 8, 12], \"sibl\": 0, \"12\": [0, 5], \"relate_div_100\": [0, 1, 2], \"page\": [1, 2, 10], \"comparison_featur\": [1, 2, 7], \"along\": 1, \"header\": [1, 2, 3, 9, 11], \"context\": [1, 3, 9], \"relatematch\": [1, 2], \"comparison_typ\": [1, 2], \"categor\": [1, 2, 8, 9], \"true\": [1, 2, 3, 7, 9, 11, 12], \"maximum\": [1, 8], \"jaro\": [1, 9], \"winkler\": [1, 9], \"find\": [1, 7, 12], \"greatest\": 1, \"among\": 1, \"cartesian\": 1, \"product\": [1, 6, 12], \"column\": [1, 3, 4, 7, 9, 10, 11, 12], \"namelast\": [1, 2], \"would\": [1, 2, 12], \"return\": [1, 3, 8, 10], \"four\": 1, \"namefrst_a\": 1, \"namefrst_b\": 1, \"namelast_b\": 1, \"namelast_a\": 1, \"maximum_jw\": 1, \"score\": [1, 2, 7, 9], \"namefrst_jw\": [1, 2, 12], \"geograph\": 1, \"filter\": [1, 4, 7, 11], \"major\": [1, 10], \"locat\": [1, 2, 10], \"boundari\": 1, \"zero\": 1, \"jw_street\": 1, \"enum_dist\": 1, \"max\": [1, 8, 10], \"member\": [1, 7], \"neighborhood\": 1, \"surnam\": 1, \"related_individual_max_jw\": 1, \"namefrst_rel\": 1, \"assert\": [1, 10], \"NOT\": 1, \"distinct\": 1, \"f1\": 1, \"evalu\": [1, 2, 6, 7, 8], \"ani\": [1, 2, 5, 8], \"potenti\": [1, 4, 7], \"mismatch\": 1, \"queri\": [1, 2], \"fi\": 1, \"OR\": 1, \"mi0\": 1, \"mi1\": 1, \"THEN\": 1, \"els\": [1, 2, 3], \"first_init_col\": 1, \"namefrst_init\": 1, \"mid_init_col\": 1, \"namefrst_mid_init_2\": 1, \"f2\": 1, \"empti\": 1, \"null\": [1, 2, 3], \"AND\": [1, 2], \"individu\": [1, 2, 7, 12], \"mainli\": 1, \"caution\": [1, 9], \"flag\": [1, 9, 10, 12], \"f\": [1, 10], \"sp\": 1, \"m_caution\": [1, 2, 9, 12], \"mbpl\": 1, \"mother_birthyr\": 1, \"stepmom\": 1, \"momloc\": 1, \"comp_a\": [1, 2], \"comp_b\": [1, 2], \"comp_c\": 1, \"parent_step_chang\": 1, \"comp_d\": 1, \"check\": [1, 10], \"sign\": 1, \"boolean\": [1, 2, 3, 11, 12], \"form\": [1, 7, 11], \"cast\": 1, \"namelast_equal_as_int\": 1, \"namelast_clean\": [1, 2, 3], \"whether\": [1, 2, 11], \"join\": [1, 11], \"across\": 1, \"being\": [1, 7], \"exact\": [1, 2], \"namefrst_unstd\": [1, 2], \"present\": [1, 2, 9], \"nonzero\": 1, \"primarili\": [1, 7], \"indic\": [1, 12], \"kind\": 1, \"incompar\": 1, \"akin\": 1, \"miss\": [1, 10], \"see\": [1, 2, 5, 10, 12], \"univers\": [1, 4, 7], \"similar\": 1, \"fbpl_nomatch\": 1, \"fbpl\": 1, \"allow\": [1, 2, 7, 12], \"up\": [1, 2, 10, 11], \"sub\": 1, \"object\": [1, 2, 6, 10], \"document\": [1, 8, 10, 12], \"sp_caution\": [1, 2, 12], \"spouse_bpl\": 1, \"spouse_birthyr\": 1, \"durmarr\": [1, 2], \"new_marr\": [1, 2], \"street_jw\": [1, 2, 12], \"9\": 1, \"multipli\": 1, \"after\": [1, 2, 4, 8, 10], \"float\": [1, 2, 8], \"comp\": 1, \"c\": 1, \"sploc\": 1, \"012\": 1, \"fals\": [1, 2, 3, 4, 6, 10], \"d\": 1, \"under\": [1, 2], \"specif\": [1, 2, 10], \"circumst\": 1, \"should\": [1, 2, 8, 9, 10], \"mid_init_match\": 1, \"either_1\": 1, \"nativ\": 1, \"either_0\": 1, \"gen\": 1, \"imm\": [1, 2, 12], \"immigr\": 1, \"look\": [1, 10, 11], \"foreign\": 1, \"born\": 1, \"sgen\": [1, 2, 12], \"rel\": [1, 2, 12], \"scala\": 1, \"determin\": [1, 7], \"greater\": [1, 5], \"jw_threshold\": 1, \"less\": [1, 2], \"age_threshold\": 1, \"sex\": [1, 2, 11], \"sampl\": 1, \"related_individual_row\": 1, \"unrel\": 1, \"depend\": [1, 2, 5, 12], \"name_col\": 1, \"birthyr_col\": 1, \"namefrst_related_row\": 1, \"replaced_birthyr\": [1, 2, 3], \"extra\": 1, \"children\": 1, \"who\": 1, \"base\": [1, 2, 7], \"expect\": 1, \"count\": [1, 10, 12], \"suspect\": [1, 6], \"relate_col\": 1, \"histid_col\": 1, \"id\": [1, 2], \"birth\": 1, \"year_b\": 1, \"wa\": [1, 12], \"minimum\": [1, 8], \"accept\": [1, 2, 12], \"consid\": [1, 8], \"histid\": [1, 2, 12], \"1910\": [1, 2, 12], \"8\": [1, 2, 5, 10], \"rate\": 1, \"calcul\": [1, 12], \"percentag\": 1, \"seen\": 1, \"neighbor\": 1, \"meet\": 1, \"95\": 1, \"nbor\": [1, 2, 12], \"namelast_neighbor\": 1, \"05\": [1, 2], \"namelast_popularity_sum\": 1, \"namelast_popular\": 1, \"length\": [1, 2, 9], \"size\": 1, \"ab\": 1, \"diff\": 1, \"absolut\": 1, \"invalid\": [1, 8], \"instead\": [1, 2, 5, 7], \"marriag\": 1, \"durat\": 1, \"99\": [1, 2], \"placehold\": 1, \"unknown\": 1, \"exclud\": 1, \"those\": [1, 2], \"consider\": 1, \"byrdiff\": [1, 2, 12], \"mardurmatch\": [1, 2], \"14\": 1, \"minu\": [1, 2], \"subtract\": 1, \"geo\": 1, \"distanc\": [1, 8], \"lookup\": 1, \"tabl\": [1, 2, 4, 7, 10, 12], \"core\": [1, 7, 10, 12], \"dist_tabl\": 1, \"py\": [1, 2], \"There\": [1, 2, 3, 7], \"sever\": [1, 6], \"wai\": [1, 5, 10], \"file\": [1, 4, 6, 7, 10, 11, 12], \"kei\": [1, 7, 10], \"key_count\": 1, \"secondari\": 1, \"serv\": 1, \"back\": 1, \"primari\": [1, 6], \"doe\": [1, 7, 12], \"particularli\": 1, \"state\": [1, 6], \"much\": [1, 7], \"fewer\": [1, 8], \"combin\": [1, 2, 3, 7], \"thu\": 1, \"risk\": 1, \"fill\": 1, \"aren\": 1, \"ex\": 1, \"just\": [1, 2, 10, 12], \"even\": 1, \"though\": 1, \"distances_fil\": 1, \"path\": [1, 2, 10, 11, 12], \"table_nam\": 1, \"what\": [1, 2, 10, 12], \"onc\": [1, 10], \"loc_a\": 1, \"where\": [1, 7, 10, 12], \"come\": 1, \"loc_b\": 1, \"distance_col\": 1, \"source_column_a\": 1, \"sourc\": [1, 4, 7, 10, 12], \"source_column_b\": 1, \"loc_a_0\": 1, \"loc_a_1\": 1, \"loc_b_0\": 1, \"loc_b_1\": 1, \"secondary_key_count\": 1, \"backup\": 1, \"secondary_table_nam\": 1, \"secondary_distances_fil\": 1, \"secondary_source_column\": 1, \"secondary_loc_a\": 1, \"secondary_loc_b\": 1, \"secondary_distance_col\": 1, \"state_dist\": 1, \"state_distance_lookup\": 1, \"county_state_dist\": 1, \"csv\": [1, 2, 7, 10, 11, 12], \"statecode1\": 1, \"statecode2\": 1, \"dist\": 1, \"county_dist\": [1, 2, 12], \"county_distance_lookup\": 1, \"county_1900_1910_distances_km\": 1, \"from_icpsrctyi\": 1, \"to_icpsrctyi\": 1, \"from_statefip\": 1, \"to_statefip\": 1, \"distance_km\": 1, \"state_1900_1910_distances_km\": 1, \"fetch\": 1, \"neither\": 1, \"nor\": 1, \"mpre\": 1, \"m_namefrst\": 1, \"accord\": 1, \"niu\": 1, \"other\": [1, 2, 12], \"mfbplmatch\": 1, \"multi\": 1, \"search\": 1, \"special\": 1, \"simplifi\": 1, \"particular\": [1, 2], \"constraint\": 1, \"num_col\": 1, \"whose\": 1, \"templat\": 1, \"n\": [1, 8, 9], \"per\": [1, 2, 8, 9, 10], \"current\": [1, 2, 10], \"respect\": [1, 7], \"jw_col_templ\": 1, \"jw\": 1, \"pair\": [1, 12], \"equal_and_not_null_templ\": 1, \"final\": [1, 2, 12], \"comput\": [1, 3, 7], \"_namefrst\": 1, \"_bpl\": 1, \"_sex\": 1, \"25\": 1, \"nvl\": 1, \"sm_namefrst\": 1, \"sn_namefrst\": 1, \"sm_bpl\": 1, \"sn_bpl\": 1, \"sm_sex\": 1, \"sn_sex\": 1, \"pass\": [1, 2, 7, 8], \"flexibl\": 1, \"user\": [1, 10], \"write\": [1, 10, 12], \"own\": [1, 2], \"favor\": 1, \"reason\": 1, \"good\": 1, \"fallback\": 1, \"defin\": [1, 7, 8, 9, 10], \"spark\": [1, 2, 5, 8, 9, 10, 12], \"builtin\": 1, \"argument\": [1, 10, 12], \"namelast_jw_max\": 1, \"namelast1\": 1, \"namelast2\": 1, \"namelast3\": 1, \"abov\": [1, 5], \"extend\": 1, \"beyond\": 1, \"top\": [1, 4], \"level\": [1, 4, 10], \"everi\": 1, \"jw_f\": [1, 2, 12], \"father_namefrst\": 1, \"rais\": [1, 3], \"exponenti\": 1, \"squar\": 1, \"county_distance_squar\": [1, 2, 12], \"county_a\": 1, \"county_b\": 1, \"upper\": 1, \"gt\": 1, \"btwn\": 1, \"addl\": 1, \"var\": [1, 2], \"program\": [1, 2, 7, 12], \"report\": [1, 4, 6, 10], \"addl_var\": 1, \"check_val_expr\": 1, \"else_v\": 1, \"volumn\": 1, \"datasourc\": [1, 2, 10], \"yrimmig\": 1, \"immyear_diff\": [1, 2, 9, 12], \"includ\": [1, 2, 7, 9, 10], \"train\": [1, 4, 6, 8, 10], \"independent_var\": [1, 2, 12], \"config\": [1, 4, 7, 10, 12], \"id_column\": [1, 2], \"_a\": 1, \"mult\": 1, \"exist\": [1, 2, 10], \"within\": [1, 2, 6, 10, 11], \"hh_train\": [1, 2, 7, 10, 12], \"hh\": 1, \"highest\": [1, 2], \"against\": [1, 11], \"ten\": [1, 2], \"tell\": 2, \"how\": [2, 7], \"descript\": [2, 8, 10], \"refer\": 2, \"here\": [2, 7, 10, 12], \"tutori\": [2, 10], \"script\": [2, 6, 10], \"discuss\": 2, \"readm\": 2, \"note\": 2, \"written\": [2, 6], \"toml\": [2, 6, 10], \"abl\": 2, \"json\": [2, 10], \"datasource_a\": [2, 7], \"datasource_b\": [2, 7], \"transform\": [2, 4, 6, 7], \"lowercase_strip\": 2, \"add_to_a\": 2, \"age_2\": 2, \"derived_from\": 2, \"expand_length\": 2, \"explod\": [2, 7], \"jaro_winkl\": 2, \"namelast_jw\": [2, 12], \"threshold\": [2, 8, 12], \"feature_nam\": 2, \"79\": 2, \"84\": 2, \"complex\": 2, \"machin\": [2, 6, 7, 10, 12], \"learn\": [2, 6, 7, 10, 12], \"probabilist\": [2, 6], \"drop_data_from_scored_match\": 2, \"us1900\": 2, \"us1900m_usa\": 2, \"p\": 2, \"parquet\": [2, 7], \"us1910\": 2, \"us1910m_usa\": 2, \"training_data_subset\": 2, \"serialp\": 2, \"rationalize_name_word\": 2, \"remove_qmark_hyphen\": 2, \"replace_apostroph\": 2, \"remove_suffix\": 2, \"remove_alternate_nam\": 2, \"condense_strip_whitespac\": 2, \"split\": [2, 3, 7, 8, 9, 12], \"namefrst_std\": [2, 11], \"array_index\": 2, \"bpl_orig\": 2, \"divide_by_int\": 2, \"get_floor\": 2, \"statefip_h\": 2, \"output_typ\": 2, \"substitution_column\": [2, 7, 11], \"join_column\": [2, 11], \"join_valu\": [2, 11], \"substitution_fil\": [2, 11], \"name_std\": [2, 11], \"male\": [2, 11], \"femal\": [2, 11], \"feature_select\": [2, 3, 7], \"input_column\": [2, 3, 9], \"output_column\": [2, 3, 9], \"sql_condit\": 2, \"namelast_bigram\": 2, \"bigram\": [2, 4], \"bpl_clean\": 2, \"bpl_str\": 2, \"washington\": 2, \"bpl2_str\": 2, \"53\": 2, \"region\": [2, 12], \"attach_vari\": 2, \"region_dict\": 2, \"col_to_join_on\": 2, \"col_to_add\": 2, \"null_fil\": 2, \"col_typ\": 2, \"potential_matches_univers\": [2, 7], \"birthyr_3\": 2, \"namefrst_std_jw\": [2, 12], \"75\": [2, 8, 12], \"comparis\": 2, \"post\": [2, 7], \"hh_comparison\": [2, 7], \"threshold_expr\": 2, \"fetch_a\": 2, \"sex_equ\": 2, \"equal\": [2, 11], \"relate_a\": [2, 9], \"pipeline_featur\": [2, 7, 9], \"sex_region_interact\": 2, \"transformer_typ\": [2, 9], \"interact\": [2, 4, 7, 12], \"relatetyp\": [2, 9], \"bucket\": [2, 7], \"hit\": [2, 10, 12], \"scale_data\": [2, 12], \"training_data\": [2, 10], \"dependent_var\": [2, 12], \"score_with_model\": [2, 12], \"use_training_data_featur\": [2, 7, 12], \"split_by_id_a\": [2, 12], \"decis\": [2, 4, 8, 12], \"drop_duplicate_with_threshold_ratio\": [2, 12], \"n_training_iter\": [2, 7, 12], \"output_suspicious_td\": [2, 12], \"param_grid\": [2, 12], \"model_paramet\": [2, 7, 8, 12], \"random_forest\": [2, 12], \"maxdepth\": [2, 8, 12], \"numtre\": [2, 8, 12], \"005\": 2, \"threshold_ratio\": [2, 8, 12], \"logistic_regress\": [2, 12], \"50\": [2, 12], \"65\": 2, \"80\": 2, \"chosen_model\": [2, 8, 12], \"prediction_col\": 2, \"predict\": [2, 12], \"hh_col\": 2, \"hh_training_data_1900_1910\": 2, \"probit\": [2, 4], \"go\": [2, 10], \"your\": [2, 5, 7, 10, 12], \"uniqu\": 2, \"identifi\": [2, 6, 12], \"full\": [2, 7, 12], \"short\": 2, \"alphanumer\": 2, \"convert_ints_to_long\": 2, \"long\": [2, 11], \"especi\": 2, \"assum\": 2, \"schema\": 2, \"sometim\": 2, \"term\": 2, \"bigint\": 2, \"thing\": 2, \"my_fil\": 2, \"subset\": [2, 11], \"limit\": 2, \"extract\": 2, \"modifi\": 2, \"meant\": 2, \"usag\": [2, 4, 10], \"set_value_column_a\": [2, 3], \"liter\": 2, \"set_value_column_b\": [2, 3], \"iv\": 2, \"v\": 2, \"vi\": 2, \"vii\": 2, \"viii\": 2, \"namelast_clean_bigram\": [2, 3], \"fed\": [2, 7], \"prep\": 2, \"df\": [2, 10], \"men\": 2, \"newli\": 2, \"attempt\": 2, \"duplic\": [2, 8], \"conjuct\": 2, \"Will\": 2, \"conjunct\": 2, \"rang\": [2, 9], \"original_valu\": 2, \"plu\": 2, \"1870\": 2, \"expand\": 2, \"1867\": 2, \"1868\": 2, \"1869\": 2, \"1871\": 2, \"1872\": 2, \"1873\": 2, \"kept\": 2, \"keep\": 2, \"appropri\": 2, \"treat\": [2, 9], \"import\": [2, 7, 10, 12], \"dure\": [2, 7], \"hot\": 2, \"encod\": [2, 3], \"vector\": [2, 9], \"stage\": 2, \"well\": 2, \"upper_threshold\": 2, \"cannot\": 2, \"robust\": 2, \"ml\": [2, 4, 8, 9], \"typic\": [2, 7], \"leverag\": 2, \"api\": [2, 6, 9], \"piplin\": 2, \"regionf\": 2, \"sex_regionf_interact\": 2, \"immyear_caut\": [2, 9], \"myriad\": 2, \"explor\": [2, 4, 6, 10], \"part\": [2, 7], \"task\": [2, 4, 6, 8, 12], \"drop_duplicate_a\": 2, \"out\": [2, 7, 12], \"best\": [2, 7], \"smallest\": 2, \"possibl\": 2, \"ratio\": [2, 8], \"beta\": [2, 8], \"test\": [2, 7, 12], \"model_explor\": [2, 10, 12], \"hyper\": [2, 6, 12], \"paramet\": [2, 6, 7, 8, 10, 12], \"eval\": 2, \"skip\": [2, 7], \"apply_model\": 2, \"run_all_step\": [2, 10, 12], \"command\": [2, 6, 10, 12], \"try\": 2, \"creation\": 2, \"iter\": 2, \"scale\": 2, \"error\": [2, 9], \"1900\": [2, 12], \"about\": [2, 10, 12], \"1930\": [2, 12], \"1940\": [2, 12], \"fail\": 2, \"were\": 2, \"sure\": [2, 5, 10], \"scratch\": 2, \"although\": 2, \"know\": 2, \"haven\": 2, \"save\": [2, 7, 12], \"small\": 2, \"amount\": 2, \"process\": [2, 6, 10], \"repeatedli\": 2, \"help\": [2, 7, 10], \"neg\": [2, 4, 6], \"area\": 2, \"coverag\": 2, \"increas\": [2, 9], \"represent\": [2, 7], \"ensur\": 2, \"group\": [2, 7], \"a304bt\": 2, \"three\": [2, 7], \"b200\": 2, \"c201\": 2, \"d425\": 2, \"perform\": [2, 6, 7, 11], \"feature_import\": [2, 7, 12], \"coeffici\": [2, 7], \"enabl\": [2, 7, 10], \"srace\": [2, 9, 12], \"race_interacted_srac\": [2, 9, 12], \"hits2\": [2, 12], \"exact_mult\": [2, 12], \"ncount\": [2, 3, 12], \"ncount2\": [2, 3, 12], \"f_interacted_jw_f\": [2, 12], \"f_caution\": [2, 12], \"f_pre\": [2, 12], \"fbplmatch\": [2, 12], \"m_interacted_jw_m\": [2, 9, 12], \"jw_m\": [2, 9, 12], \"m_pre\": [2, 9, 12], \"mbplmatch\": [2, 12], \"sp_interacted_jw_sp\": [2, 12], \"jw_sp\": [2, 12], \"sp_pre\": [2, 12], \"mi\": [2, 12], \"fsoundex\": [2, 12], \"lsoundex\": [2, 12], \"oth\": [2, 12], \"imm_interacted_immyear_caut\": [2, 12], \"1900_1910_training_data_20191023\": 2, \"jw_max_a\": 2, \"jw_max_b\": 2, \"f1_match\": 2, \"f2_match\": 2, \"byrdifcat\": 2, \"racematch\": 2, \"bplmatch\": 2, \"imm_interacted_bplmatch\": 2, \"sexmatch\": 2, \"relatetype_interacted_relatematch\": 2, \"checkpoint\": 3, \"no_first_pad\": 3, \"don\": 3, \"prepend\": 3, \"namefrst_unstd_bigram\": 3, \"namelast_frst_bigram\": 3, \"namelast_clean_soundex\": 3, \"input_col\": 3, \"output_col\": 3, \"expon\": 3, \"introduct\": 4, \"overview\": 4, \"instal\": 4, \"pypi\": 4, \"preprocess\": [4, 6, 10, 12], \"model\": [4, 6, 10], \"run\": [4, 5, 6, 7, 12], \"librari\": [4, 6], \"mode\": [4, 5, 12], \"advanc\": 4, \"workflow\": 4, \"export\": [4, 7, 10], \"featur\": [4, 6, 7, 8, 10], \"reus\": 4, \"basic\": 4, \"map\": [4, 7, 9], \"substitut\": [4, 7], \"block\": [4, 7], \"comparison\": [4, 7], \"pipelin\": 4, \"ons\": 4, \"aggreg\": 4, \"union\": 4, \"soundex\": 4, \"power\": 4, \"regex\": 4, \"random\": [4, 8], \"forest\": [4, 8], \"logist\": [4, 8], \"regress\": [4, 8], \"tree\": [4, 8], \"gradient\": [4, 8], \"boost\": [4, 8], \"system\": 5, \"python\": [5, 6, 10], \"java\": 5, \"integr\": 5, \"apach\": 5, \"via\": [5, 6], \"pyspark\": [5, 8, 9, 10], \"packag\": 5, \"org\": 5, \"latest\": 5, \"pip\": 5, \"easiest\": [5, 10], \"through\": [5, 7, 9, 10], \"instruct\": [5, 10], \"But\": 5, \"clone\": 5, \"github\": 5, \"repositori\": 5, \"root\": 5, \"project\": 5, \"directori\": [5, 10, 12], \"develop\": [5, 6], \"e\": 5, \"dev\": 5, \"edit\": 5, \"made\": 5, \"built\": 5, \"tool\": [5, 6], \"line\": [6, 10], \"share\": 6, \"characterist\": [6, 7], \"correspond\": [6, 7], \"real\": 6, \"world\": 6, \"determinist\": [6, 7], \"rule\": [6, 7], \"algorithm\": [6, 7], \"At\": [6, 7], \"been\": 6, \"unit\": 6, \"census\": 6, \"hierarch\": [6, 10], \"structur\": 6, \"nest\": 6, \"howev\": [6, 12], \"tailor\": 6, \"ignor\": 6, \"common\": [6, 7, 12], \"highli\": [6, 7], \"languag\": 6, \"further\": [6, 12], \"broken\": 6, \"smaller\": 6, \"sequenc\": 6, \"linkrun\": [6, 10], \"prepar\": [6, 7, 10], \"research\": 6, \"experi\": 6, \"understand\": 6, \"tune\": [6, 12], \"relationship\": 6, \"varieti\": 7, \"normal\": 7, \"abbrevi\": [7, 11], \"regist\": [7, 10], \"datafram\": [7, 10, 12], \"request\": 7, \"classif\": [7, 8], \"metadata\": 7, \"introspect\": 7, \"ingest\": 7, \"inspect\": 7, \"mani\": [7, 10], \"aspect\": [7, 10], \"extens\": 7, \"longest\": 7, \"definit\": 7, \"reduc\": 7, \"drastic\": 7, \"improv\": 7, \"runtim\": 7, \"separ\": 7, \"total\": 7, \"potential_match\": [7, 10], \"satisfi\": 7, \"elig\": 7, \"reshap\": 7, \"thought\": 7, \"ahead\": 7, \"chosen\": 7, \"experiment\": [7, 10], \"focus\": 7, \"demograph\": 7, \"moment\": 7, \"veri\": [7, 12], \"anyon\": 7, \"percent\": 7, \"remain\": 7, \"popul\": 7, \"pull\": 7, \"fix\": 7, \"width\": 7, \"crosswalk\": 7, \"construct\": 7, \"alpha\": 8, \"hyperparamet\": [8, 12], \"de\": 8, \"param\": [8, 12], \"label\": 8, \"doc\": [8, 9], \"commonli\": 8, \"explan\": 8, \"randomforestclassifi\": 8, \"depth\": 8, \"20\": 8, \"featuresubsetstrategi\": 8, \"node\": 8, \"auto\": 8, \"onethird\": 8, \"sqrt\": 8, \"log2\": 8, \"15\": 8, \"generalizedlinearregress\": 8, \"famili\": 8, \"binomi\": 8, \"85\": [8, 10], \"logisticregress\": 8, \"decisiontreeclassifi\": 8, \"mininstancespernod\": 8, \"caus\": 8, \"left\": 8, \"right\": [8, 10], \"discard\": 8, \"maxbin\": 8, \"bin\": 8, \"discret\": 8, \"continu\": [8, 9, 12], \"gbtclassifi\": 8, \"mother\": 9, \"point\": [9, 12], \"x\": [9, 10], \"y\": 9, \"hold\": 9, \"except\": 9, \"strictli\": 9, \"inf\": 9, \"explicitli\": 9, \"cover\": 9, \"doubl\": 9, \"outsid\": 9, \"job\": 10, \"high\": 10, \"class\": 10, \"handl\": 10, \"main\": 10, \"complet\": 10, \"access\": [10, 12], \"link_run\": 10, \"factori\": 10, \"sparkfactori\": 10, \"load_config\": 10, \"load_conf_fil\": 10, \"sparksess\": 10, \"now\": 10, \"let\": 10, \"load\": 10, \"our\": 10, \"my_conf\": 10, \"lr\": 10, \"prep_step\": 10, \"get_step\": 10, \"enumer\": 10, \"print\": 10, \"input_table_nam\": 10, \"output_table_nam\": 10, \"run_step\": 10, \"get_tabl\": 10, \"matches_df\": 10, \"hh_model_explor\": 10, \"method\": [10, 12], \"interfac\": 10, \"easili\": 10, \"conveni\": 10, \"adjust\": 10, \"set_loc\": 10, \"set_num_cor\": 10, \"set_executor_memori\": 10, \"5g\": 10, \"ll\": 10, \"dictionari\": 10, \"often\": 10, \"modul\": 10, \"pleas\": 10, \"reproduc\": 10, \"consol\": 10, \"cpu\": 10, \"h\": 10, \"executor_memori\": [10, 12], \"execute_task\": 10, \"execute_command\": 10, \"conf\": [10, 12], \"show\": 10, \"messag\": 10, \"exit\": 10, \"memori\": 10, \"executor\": 10, \"begin\": 10, \"execut\": 10, \"seri\": 10, \"excute_command\": 10, \"filepath\": 10, \"sai\": 10, \"fullcount_1870_1880\": 10, \"pattern\": 10, \"full_count_1870_1880\": 10, \"prompt\": 10, \"enter\": 10, \"text\": 10, \"unstabl\": 10, \"topic\": 10, \"analyz\": [10, 12], \"set_preexisting_t\": 10, \"x_persist\": 10, \"borrow_t\": 10, \"get_task\": 10, \"set_print_sql\": 10, \"x_sql\": 10, \"x_sqlf\": 10, \"ipython\": 10, \"showf\": 10, \"x_summari\": 10, \"desc\": 10, \"x_crosswalk\": 10, \"x_tab\": 10, \"q\": [10, 12], \"x_hh_tfam\": 10, \"x_tfam\": 10, \"drop_al\": 10, \"reload\": 10, \"x_hh_tfam_2a\": 10, \"x_tfam_raw\": 10, \"drop_all_prc\": 10, \"x_hh_tfam_2b\": 10, \"x_union\": 10, \"drop_all_temp\": 10, \"x_load\": 10, \"get_set\": 10, \"set_link_task\": 10, \"x_parquet_from_csv\": 10, \"organ\": 10, \"hierarchi\": 10, \"five\": 10, \"hh_match\": 10, \"someth\": 10, \"choic\": 10, \"preexist\": 10, \"prepped_df_a\": 10, \"prepped_df_b\": 10, \"raw_df_b\": 10, \"raw_df_a\": 10, \"training_featur\": [10, 12], \"scored_potential_match\": 10, \"potential_matches_prep\": 10, \"exploded_df_b\": 10, \"exploded_df_a\": 10, \"predicted_match\": 10, \"hh_training_featur\": [10, 12], \"hh_training_data\": 10, \"hh_predicted_match\": 10, \"hh_scored_potential_match\": 10, \"hh_potential_match\": 10, \"hh_blocked_match\": 10, \"hh_potential_matchs_prep\": 10, \"model_eval_training_vector\": 10, \"model_eval_training_data\": 10, \"model_eval_repeat_fp\": 10, \"model_eval_training_featur\": 10, \"model_eval_training_result\": 10, \"model_eval_repeat_fn\": 10, \"hh_model_eval_training_vector\": 10, \"hh_model_eval_repeat_fp\": 10, \"hh_model_eval_repeat_fn\": 10, \"hh_model_eval_training_result\": 10, \"hh_model_eval_training_featur\": 10, \"hh_model_eval_training_data\": 10, \"persist\": 10, \"hidden\": 10, \"intermedi\": 10, \"yet\": 10, \"databas\": 10, \"tablenam\": 10, \"istemporari\": 10, \"task_nam\": 10, \"num\": 10, \"finish\": 10, \"put\": [10, 12], \"launch\": [10, 12], \"my\": [10, 12], \"subhead\": 11, \"suppli\": 11, \"regex_word_replac\": 11, \"variant\": 11, \"av\": 11, \"7th\": 11, \"swap\": 11, \"still\": 11, \"anywher\": 11, \"proceed\": 11, \"street_unstd\": 11, \"dir\": 11, \"substitutions_street_abbrev\": 11, \"span\": 12, \"1920\": 12, \"deriv\": 12, \"necessari\": 12, \"scenario\": 12, \"copi\": 12, \"use_potential_matches_featur\": 12, \"full_count_1900_1910\": 12, \"50g\": 12, \"ask\": 12, \"arg\": 12, \"partit\": 12, \"training_data_1900_1910_hlink_featur\": 12, \"might\": 12, \"shut\": 12, \"framework\": 12, \"etc\": 12, \"relev\": 12, \"matrix\": 12, \"implement\": 12, \"regular\": 12, \"training_data_1900_1910\": 12, \"weren\": 12, \"ident\": 12, \"manual\": 12, \"updat\": 12, \"isn\": 12, \"analysi\": 12, \"training_result\": 12, \"hh_training_result\": 12, \"1900_1910_training_result\": 12, \"repeat_fp\": 12, \"repeat_fn\": 12, \"hh_repeat_fp\": 12, \"hh_repeat_fn\": 12, \"1900_1910_potential_fp\": 12, \"1900_1910_potential_fn\": 12, \"prefer\": 12, \"ve\": 12}, \"objects\": {}, \"objtypes\": {}, \"objnames\": {}, \"titleterms\": {\"column\": [0, 2], \"map\": [0, 2], \"basic\": [0, 2], \"usag\": 0, \"advanc\": [0, 2, 12], \"transform\": [0, 1, 3, 9], \"add_to_a\": 0, \"concat_to_a\": 0, \"concat_to_b\": 0, \"concat_two_col\": 0, \"lowercase_strip\": 0, \"rationalize_name_word\": 0, \"remove_qmark_hyphen\": 0, \"remove_punctu\": 0, \"replace_apostroph\": 0, \"remove_alternate_nam\": 0, \"remove_suffix\": 0, \"remove_stop_word\": 0, \"remove_prefix\": 0, \"condense_strip_whitespac\": 0, \"remove_one_letter_nam\": 0, \"split\": 0, \"array_index\": 0, \"substr\": 0, \"divide_by_int\": 0, \"when_valu\": 0, \"get_floor\": 0, \"comparison\": [1, 2], \"type\": [1, 9], \"add\": 1, \"ons\": 1, \"aggreg\": 1, \"featur\": [1, 2, 3, 9, 12], \"household\": [1, 2, 7], \"maximum_jaro_winkl\": 1, \"jaro_winkl\": 1, \"jaro_winkler_street\": 1, \"max_jaro_winkl\": 1, \"equal\": 1, \"f1_match\": 1, \"f2_match\": 1, \"not_equ\": 1, \"equals_as_int\": 1, \"all_equ\": 1, \"not_zero_and_not_equ\": 1, \"time\": 1, \"caution_comp_3\": 1, \"caution_comp_3_012\": 1, \"caution_comp_4\": 1, \"caution_comp_4_012\": 1, \"any_equ\": 1, \"either_are_1\": 1, \"either_are_0\": 1, \"second_gen_imm\": 1, \"rel_jaro_winkl\": 1, \"extra_children\": 1, \"jaro_winkler_r\": 1, \"sum\": 1, \"length_b\": 1, \"abs_diff\": 1, \"b_minus_a\": 1, \"geo_dist\": 1, \"fetch_a\": 1, \"fetch_b\": 1, \"present_both_year\": 1, \"neither_are_nul\": 1, \"present_and_matching_categor\": 1, \"present_and_not_equ\": 1, \"present_and_equal_categorical_in_univers\": 1, \"multi_jaro_winkler_search\": 1, \"sql_condit\": [1, 3], \"alia\": 1, \"power\": [1, 3], \"threshold\": 1, \"lower_threshold\": 1, \"upper_threshold\": 1, \"gt_threshold\": 1, \"btwn_threshold\": 1, \"look_at_addl_var\": 1, \"hit\": 1, \"hits2\": 1, \"exact_mult\": 1, \"jw_max_a\": 1, \"jw_max_b\": 1, \"configur\": [2, 4, 7], \"config\": 2, \"file\": 2, \"top\": 2, \"level\": 2, \"data\": [2, 11, 12], \"sourc\": [2, 5], \"filter\": 2, \"substitut\": [2, 11], \"select\": [2, 3], \"potenti\": [2, 12], \"match\": [2, 7], \"univers\": 2, \"block\": 2, \"pipelin\": [2, 9], \"gener\": [2, 9, 12], \"train\": [2, 7, 12], \"model\": [2, 7, 8, 12], \"bigram\": 3, \"arrai\": 3, \"union\": 3, \"soundex\": 3, \"welcom\": 4, \"hlink\": [4, 10], \"\": 4, \"document\": 4, \"api\": 4, \"instal\": 5, \"requir\": 5, \"from\": 5, \"pypi\": 5, \"introduct\": 6, \"overview\": [6, 7], \"link\": [7, 10, 12], \"task\": [7, 10], \"preprocess\": 7, \"step\": [7, 10], \"relat\": 7, \"section\": 7, \"explor\": [7, 12], \"report\": 7, \"random_forest\": 8, \"probit\": 8, \"logistic_regress\": 8, \"decision_tre\": 8, \"gradient_boosted_tre\": 8, \"interact\": [9, 10], \"bucket\": 9, \"run\": 10, \"us\": 10, \"librari\": 10, \"mode\": 10, \"start\": 10, \"program\": 10, \"exampl\": [10, 12], \"workflow\": [10, 12], \"1\": 11, \"tabl\": 11, \"regex\": 11, \"word\": 11, \"replac\": 11, \"export\": 12, \"after\": 12, \"reus\": 12, \"differ\": 12, \"year\": 12, \"ml\": 12, \"list\": 12, \"fals\": 12, \"posit\": 12, \"neg\": 12, \"fp\": 12, \"fn\": 12}, \"envversion\": {\"sphinx.domains.c\": 3, \"sphinx.domains.changeset\": 1, \"sphinx.domains.citation\": 1, \"sphinx.domains.cpp\": 9, \"sphinx.domains.index\": 1, \"sphinx.domains.javascript\": 3, \"sphinx.domains.math\": 2, \"sphinx.domains.python\": 4, \"sphinx.domains.rst\": 2, \"sphinx.domains.std\": 2, \"sphinx\": 60}, \"alltitles\": {\"Column Mappings\": [[0, \"column-mappings\"], [2, \"column-mappings\"]], \"Basic Usage\": [[0, \"basic-usage\"]], \"Advanced Usage\": [[0, \"advanced-usage\"]], \"Transforms\": [[0, \"transforms\"]], \"add_to_a\": [[0, \"add-to-a\"]], \"concat_to_a\": [[0, \"concat-to-a\"]], \"concat_to_b\": [[0, \"concat-to-b\"]], \"concat_two_cols\": [[0, \"concat-two-cols\"]], \"lowercase_strip\": [[0, \"lowercase-strip\"]], \"rationalize_name_words\": [[0, \"rationalize-name-words\"]], \"remove_qmark_hyphen\": [[0, \"remove-qmark-hyphen\"]], \"remove_punctuation\": [[0, \"remove-punctuation\"]], \"replace_apostrophe\": [[0, \"replace-apostrophe\"]], \"remove_alternate_names\": [[0, \"remove-alternate-names\"]], \"remove_suffixes\": [[0, \"remove-suffixes\"]], \"remove_stop_words\": [[0, \"remove-stop-words\"]], \"remove_prefixes\": [[0, \"remove-prefixes\"]], \"condense_strip_whitespace\": [[0, \"condense-strip-whitespace\"]], \"remove_one_letter_names\": [[0, \"remove-one-letter-names\"]], \"split\": [[0, \"split\"]], \"array_index\": [[0, \"array-index\"]], \"mapping\": [[0, \"mapping\"]], \"substring\": [[0, \"substring\"]], \"divide_by_int\": [[0, \"divide-by-int\"]], \"when_value\": [[0, \"when-value\"]], \"get_floor\": [[0, \"get-floor\"]], \"Comparison types, transform add-ons, aggregate features, and household aggregate features\": [[1, \"comparison-types-transform-add-ons-aggregate-features-and-household-aggregate-features\"]], \"Comparison types\": [[1, \"comparison-types\"]], \"maximum_jaro_winkler\": [[1, \"maximum-jaro-winkler\"]], \"jaro_winkler\": [[1, \"jaro-winkler\"]], \"jaro_winkler_street\": [[1, \"jaro-winkler-street\"]], \"max_jaro_winkler\": [[1, \"max-jaro-winkler\"]], \"equals\": [[1, \"equals\"]], \"f1_match\": [[1, \"f1-match\"]], \"f2_match\": [[1, \"f2-match\"]], \"not_equals\": [[1, \"not-equals\"]], \"equals_as_int\": [[1, \"equals-as-int\"]], \"all_equals\": [[1, \"all-equals\"]], \"not_zero_and_not_equals\": [[1, \"not-zero-and-not-equals\"]], \"or\": [[1, \"or\"]], \"and\": [[1, \"and\"]], \"times\": [[1, \"times\"]], \"caution_comp_3\": [[1, \"caution-comp-3\"]], \"caution_comp_3_012\": [[1, \"caution-comp-3-012\"]], \"caution_comp_4\": [[1, \"caution-comp-4\"]], \"caution_comp_4_012\": [[1, \"caution-comp-4-012\"]], \"any_equals\": [[1, \"any-equals\"]], \"either_are_1\": [[1, \"either-are-1\"]], \"either_are_0\": [[1, \"either-are-0\"]], \"second_gen_imm\": [[1, \"second-gen-imm\"]], \"rel_jaro_winkler\": [[1, \"rel-jaro-winkler\"]], \"extra_children\": [[1, \"extra-children\"]], \"jaro_winkler_rate\": [[1, \"jaro-winkler-rate\"]], \"sum\": [[1, \"sum\"]], \"length_b\": [[1, \"length-b\"]], \"abs_diff\": [[1, \"abs-diff\"]], \"b_minus_a\": [[1, \"b-minus-a\"]], \"geo_distance\": [[1, \"geo-distance\"]], \"fetch_a\": [[1, \"fetch-a\"]], \"fetch_b\": [[1, \"fetch-b\"]], \"present_both_years\": [[1, \"present-both-years\"]], \"neither_are_null\": [[1, \"neither-are-null\"]], \"present_and_matching_categorical\": [[1, \"present-and-matching-categorical\"]], \"present_and_not_equal\": [[1, \"present-and-not-equal\"]], \"present_and_equal_categorical_in_universe\": [[1, \"present-and-equal-categorical-in-universe\"]], \"multi_jaro_winkler_search\": [[1, \"multi-jaro-winkler-search\"]], \"sql_condition\": [[1, \"sql-condition\"], [3, \"sql-condition\"]], \"Feature add-ons\": [[1, \"feature-add-ons\"]], \"alias\": [[1, \"alias\"]], \"power\": [[1, \"power\"], [3, \"power\"]], \"threshold\": [[1, \"threshold\"]], \"lower_threshold\": [[1, \"lower-threshold\"]], \"upper_threshold\": [[1, \"upper-threshold\"]], \"gt_threshold\": [[1, \"gt-threshold\"]], \"btwn_threshold\": [[1, \"btwn-threshold\"]], \"look_at_addl_var\": [[1, \"look-at-addl-var\"]], \"Aggregate Features\": [[1, \"aggregate-features\"]], \"hits\": [[1, \"hits\"]], \"hits2\": [[1, \"hits2\"]], \"exact_mult\": [[1, \"exact-mult\"]], \"Household Aggregate Features\": [[1, \"household-aggregate-features\"]], \"jw_max_a\": [[1, \"jw-max-a\"]], \"jw_max_b\": [[1, \"jw-max-b\"]], \"Configuration\": [[2, \"configuration\"]], \"Basic Config File\": [[2, \"basic-config-file\"]], \"Advanced Config File\": [[2, \"advanced-config-file\"]], \"Top level configs\": [[2, \"top-level-configs\"]], \"Data sources\": [[2, \"data-sources\"]], \"Filter\": [[2, \"filter\"]], \"Substitution Columns\": [[2, \"substitution-columns\"]], \"Feature Selections\": [[2, \"feature-selections\"]], \"Potential Matches Universe\": [[2, \"potential-matches-universe\"]], \"Blocking\": [[2, \"blocking\"]], \"Comparisons\": [[2, \"comparisons\"]], \"Household Comparisons\": [[2, \"household-comparisons\"]], \"Comparison Features\": [[2, \"comparison-features\"]], \"Pipeline-generated Features\": [[2, \"pipeline-generated-features\"]], \"Training and models\": [[2, \"training-and-models\"]], \"Household training and models\": [[2, \"household-training-and-models\"]], \"Feature Selection transforms\": [[3, \"feature-selection-transforms\"]], \"bigrams\": [[3, \"bigrams\"]], \"array\": [[3, \"array\"]], \"union\": [[3, \"union\"]], \"soundex\": [[3, \"soundex\"]], \"Welcome to hlink\\u2019s documentation!\": [[4, \"welcome-to-hlink-s-documentation\"]], \"Configuration API\": [[4, \"configuration-api\"], [4, null]], \"Installation\": [[5, \"installation\"]], \"Requirements\": [[5, \"requirements\"]], \"Installing from PyPI\": [[5, \"installing-from-pypi\"]], \"Installing from source\": [[5, \"installing-from-source\"]], \"Introduction\": [[6, \"introduction\"]], \"Overview\": [[6, \"overview\"], [7, \"overview\"], [7, \"id1\"], [7, \"id4\"], [7, \"id7\"], [7, \"id10\"], [7, \"id13\"]], \"Link Tasks\": [[7, \"link-tasks\"]], \"Preprocessing\": [[7, \"preprocessing\"]], \"Task steps\": [[7, \"task-steps\"], [7, \"id2\"], [7, \"id5\"], [7, \"id8\"], [7, \"id11\"], [7, \"id14\"]], \"Related Configuration Sections\": [[7, \"related-configuration-sections\"], [7, \"id3\"], [7, \"id6\"], [7, \"id9\"], [7, \"id12\"], [7, \"id15\"]], \"Training and Household Training\": [[7, \"training-and-household-training\"]], \"Matching\": [[7, \"matching\"]], \"Household Matching\": [[7, \"household-matching\"]], \"Model Exploration and Household Model Exploration\": [[7, \"model-exploration-and-household-model-exploration\"]], \"Reporting\": [[7, \"reporting\"]], \"Models\": [[8, \"models\"]], \"random_forest\": [[8, \"random-forest\"]], \"probit\": [[8, \"probit\"]], \"logistic_regression\": [[8, \"logistic-regression\"]], \"decision_tree\": [[8, \"decision-tree\"]], \"gradient_boosted_trees\": [[8, \"gradient-boosted-trees\"]], \"Pipeline generated features\": [[9, \"pipeline-generated-features\"]], \"Transformer types\": [[9, \"transformer-types\"]], \"interaction\": [[9, \"interaction\"]], \"bucketizer\": [[9, \"bucketizer\"]], \"Running hlink\": [[10, \"running-hlink\"]], \"Using hlink as a Library\": [[10, \"using-hlink-as-a-library\"]], \"Interactive Mode\": [[10, \"interactive-mode\"]], \"Starting the program\": [[10, \"starting-the-program\"]], \"Running Linking Tasks and Steps\": [[10, \"running-linking-tasks-and-steps\"]], \"Example interactive mode workflow\": [[10, \"example-interactive-mode-workflow\"]], \"Substitutions\": [[11, \"substitutions\"]], \"1:1 substitution by data table\": [[11, \"substitution-by-data-table\"]], \"Substitution by regex word replace\": [[11, \"substitution-by-regex-word-replace\"]], \"Advanced Workflow Examples\": [[12, \"advanced-workflow-examples\"]], \"Export training data after generating features to reuse in different linking years\": [[12, \"export-training-data-after-generating-features-to-reuse-in-different-linking-years\"]], \"Example training data export with generated ML features\": [[12, \"example-training-data-export-with-generated-ml-features\"]], \"ML model exploration and export of lists of potential false positives/negatives in training data\": [[12, \"ml-model-exploration-and-export-of-lists-of-potential-false-positives-negatives-in-training-data\"]], \"Example model exploration and FP/FN export workflow\": [[12, \"example-model-exploration-and-fp-fn-export-workflow\"]]}, \"indexentries\": {}})\n\\ No newline at end of file\n+Search.setIndex({\"docnames\": [\"column_mappings\", \"comparison_types\", \"config\", \"feature_selection_transforms\", \"index\", \"installation\", \"introduction\", \"link_tasks\", \"models\", \"pipeline_features\", \"running_the_program\", \"substitutions\", \"use_examples\"], \"filenames\": [\"column_mappings.md\", \"comparison_types.md\", \"config.md\", \"feature_selection_transforms.md\", \"index.rst\", \"installation.md\", \"introduction.md\", \"link_tasks.md\", \"models.md\", \"pipeline_features.md\", \"running_the_program.md\", \"substitutions.md\", \"use_examples.md\"], \"titles\": [\"Column Mappings\", \"Comparison types, transform add-ons, aggregate features, and household aggregate features\", \"Configuration\", \"Feature Selection transforms\", \"Welcome to hlink\\u2019s documentation!\", \"Installation\", \"Introduction\", \"Link Tasks\", \"Models\", \"Pipeline generated features\", \"Running hlink\", \"Substitutions\", \"Advanced Workflow Examples\"], \"terms\": {\"each\": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10], \"read\": [0, 1, 2, 7, 10], \"from\": [0, 1, 2, 4, 6, 7, 8, 9, 10, 12], \"input\": [0, 1, 2, 3, 6, 7, 10, 11], \"dataset\": [0, 1, 2, 6, 7, 10, 12], \"hlink\": [0, 1, 2, 5, 6, 7, 12], \"It\": [0, 1, 2, 6, 10, 12], \"ha\": [0, 1, 2, 6, 10, 12], \"column_nam\": [0, 1, 2, 11], \"attribut\": [0, 1, 2, 3, 7, 8, 9, 10, 11], \"which\": [0, 1, 2, 3, 6, 7, 9, 10, 12], \"specifi\": [0, 1, 2, 6, 7, 9, 10, 11], \"name\": [0, 1, 2, 10, 11], \"both\": [0, 1, 2, 7, 12], \"option\": [0, 1, 2, 3, 6, 7, 8, 10, 12], \"mai\": [0, 2, 6, 7, 10], \"have\": [0, 1, 2, 5, 6, 7, 8, 10, 12], \"an\": [0, 1, 2, 6, 8, 10], \"alia\": [0, 2, 7], \"give\": [0, 2], \"new\": [0, 2, 12], \"us\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12], \"support\": [0, 7, 8], \"some\": [0, 1, 2, 3, 6, 7, 10], \"make\": [0, 1, 2, 5, 12], \"chang\": [0, 1, 2, 5, 10, 12], \"data\": [0, 1, 4, 6, 7, 10], \"thei\": [0, 1, 2, 7, 10], \"ar\": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12], \"These\": [0, 1, 2, 3, 6, 7, 8, 9], \"clean\": [0, 6], \"harmon\": 0, \"The\": [0, 1, 2, 3, 5, 7, 8, 9, 10, 12], \"avail\": [0, 1, 2, 3, 5, 7, 8, 9, 12], \"list\": [0, 1, 2, 3, 4, 8, 10, 11], \"below\": [0, 1, 2, 3, 8, 9, 10], \"section\": [0, 1, 2, 12], \"By\": [0, 2, 10], \"default\": [0, 1, 2, 7, 8, 10], \"must\": [0, 1, 2, 8, 9, 11], \"same\": [0, 1, 2, 6, 7, 10], \"With\": [0, 9], \"override_column_a\": [0, 2, 3], \"override_column_b\": [0, 2, 3], \"you\": [0, 1, 2, 5, 10, 11, 12], \"can\": [0, 1, 2, 5, 6, 7, 8, 10, 12], \"differ\": [0, 1, 2, 4, 6, 7], \"either\": [0, 1, 2, 6, 11], \"A\": [0, 1, 2, 9, 10], \"b\": [0, 1, 2, 10], \"when\": [0, 1, 2, 3, 7, 12], \"do\": [0, 1, 3, 10, 12], \"thi\": [0, 1, 2, 5, 6, 7, 9, 10, 12], \"appli\": [0, 2, 3, 7, 12], \"onli\": [0, 1, 2, 7, 12], \"non\": 0, \"overrid\": [0, 2], \"also\": [0, 1, 2, 5, 6, 7, 9, 10, 12], \"provid\": [0, 2, 3, 6, 7, 9, 10], \"override_transform\": [0, 2], \"describ\": [0, 2, 10], \"type\": [0, 2, 3, 4, 7, 8, 10, 11, 12], \"oper\": [0, 2], \"singl\": [0, 2, 3, 10, 12], \"output\": [0, 1, 2, 3, 6, 7, 10, 12], \"more\": [0, 1, 2, 9, 10, 12], \"than\": [0, 1, 2, 8], \"one\": [0, 1, 2, 7], \"order\": [0, 2, 7], \"so\": [0, 1, 2, 5, 12], \"anoth\": [0, 1, 3, 7], \"format\": 0, \"letter\": 0, \"t\": [0, 1, 2, 3, 12], \"u\": 0, \"repres\": [0, 1, 2, 3, 9, 10], \"arbitrari\": 0, \"requir\": [0, 1, 2, 3, 4, 7, 9, 10, 11], \"addit\": [0, 1, 2, 3, 5, 6, 10], \"vari\": [0, 2], \"inform\": [0, 1, 2, 10], \"appear\": [0, 1], \"its\": [0, 1, 6, 10], \"suffix\": 0, \"mean\": [0, 2], \"two\": [0, 1, 2, 3, 6, 7, 9, 10, 12], \"link\": [0, 1, 2, 4, 6, 8], \"most\": [0, 1, 7, 10], \"independ\": [0, 2], \"For\": [0, 1, 2, 7, 10, 12], \"exampl\": [0, 1, 2], \"taken\": [0, 1], \"10\": [0, 2, 5, 12], \"year\": [0, 1, 2, 3, 4], \"apart\": 0, \"want\": [0, 1, 2, 10, 12], \"standard\": [0, 1, 11], \"ag\": [0, 1, 2, 3], \"variabl\": [0, 1, 2, 12], \"i\": [0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12], \"compar\": [0, 1, 2, 6, 7], \"between\": [0, 1, 2, 6, 7, 10, 12], \"To\": [0, 1, 5, 7, 10], \"could\": [0, 2], \"creat\": [0, 2, 6, 7, 9, 10, 11, 12], \"age_at_dataset_b\": 0, \"ad\": [0, 1, 2], \"column_map\": [0, 2, 7], \"valu\": [0, 1, 2, 8, 9, 10, 11], \"As\": 0, \"suppos\": 0, \"record\": [0, 1, 2, 6, 7], \"person\": [0, 1, 6], \"\": [0, 1, 2, 6, 7, 10, 11], \"first\": [0, 1, 2, 5, 7, 10, 11], \"string\": [0, 1, 2, 3, 7, 8, 10, 11], \"In\": [0, 1, 6, 10, 12], \"call\": 0, \"namefrst\": [0, 1, 2], \"entir\": 0, \"lowercas\": 0, \"first_nam\": 0, \"uppercas\": 0, \"follow\": [0, 1, 6, 10, 11, 12], \"configur\": [0, 1, 6, 10, 12], \"add\": [0, 4], \"_\": [0, 1, 2, 3, 4, 8, 9, 10], \"given\": [0, 1, 2, 3, 8, 12], \"numer\": [0, 1], \"11\": [0, 2, 5, 9], \"concat\": 0, \"concaten\": [0, 1], \"end\": [0, 1, 2, 3, 11], \"col\": [0, 1], \"togeth\": [0, 1, 2], \"take\": [0, 1, 2, 3, 7, 10], \"column_to_append\": 0, \"multipl\": [0, 1, 2, 10], \"time\": [0, 2, 7, 10], \"row\": [0, 2], \"If\": [0, 1, 2, 3, 7, 8, 10, 11], \"automat\": [0, 2, 5, 7], \"convert\": [0, 1, 2], \"befor\": [0, 1, 2, 3, 5, 7], \"statefip\": [0, 1, 2], \"counti\": [0, 1], \"strip\": [0, 7], \"alphabet\": 0, \"charact\": 0, \"lower\": [0, 1], \"case\": [0, 1, 2, 3, 6], \"white\": 0, \"space\": [0, 2, 3, 11], \"start\": [0, 11], \"ration\": 0, \"word\": [0, 4], \"replac\": [0, 1, 4], \"sinc\": [0, 2], \"peopl\": [0, 1, 6, 10], \"raw\": [0, 2, 7, 10], \"censu\": [0, 7, 12], \"contain\": [0, 1, 11], \"lead\": 0, \"better\": [0, 6], \"match\": [0, 1, 4, 6, 10, 11, 12], \"remov\": 0, \"qmark\": 0, \"hyphen\": 0, \"punctuat\": 0, \"apostroph\": 0, \"altern\": [0, 2], \"surround\": 0, \"all\": [0, 1, 2, 3, 7, 8, 9, 10], \"them\": [0, 1, 2, 7], \"jr\": [0, 2], \"sr\": [0, 2], \"ii\": [0, 2], \"iii\": [0, 2], \"stop\": 0, \"last\": [0, 1, 7, 9], \"street\": [0, 1], \"avenu\": [0, 11], \"blvd\": 0, \"circl\": 0, \"court\": 0, \"road\": 0, \"prefix\": 0, \"like\": [0, 2, 7, 10], \"m\": [0, 1], \"mr\": 0, \"ah\": 0, \"chines\": 0, \"condens\": 0, \"whitespac\": [0, 7], \"leav\": 0, \"behind\": 0, \"arrai\": [0, 2, 4, 8, 9], \"namefrst_split\": [0, 2], \"namefrst_clean\": [0, 2], \"index\": [0, 5], \"select\": [0, 1, 4, 6, 10, 12], \"element\": 0, \"posit\": [0, 1, 2, 4, 6], \"second\": [0, 1, 2, 11], \"1\": [0, 1, 2, 4, 7, 8, 9, 10, 12], \"item\": 0, \"set\": [0, 1, 2, 3, 6, 7, 10, 12], \"Then\": [0, 5], \"0\": [0, 1, 2, 7, 8, 9, 10, 12], \"initi\": [0, 1, 10], \"probabl\": [0, 2, 8], \"middl\": [0, 1], \"namefrst_mid_init\": [0, 1], \"otherwis\": [0, 1, 9, 12], \"known\": 0, \"recod\": 0, \"birthyr\": [0, 2], \"clean_birthyr\": [0, 2, 3], \"9999\": [0, 2, 9], \"1999\": [0, 2], \"9998\": 0, \"divid\": 0, \"int\": [0, 1, 2, 3, 8], \"integ\": [0, 1, 2, 9], \"result\": [0, 1, 6, 9, 10, 12], \"instanc\": [0, 8], \"birthplac\": 0, \"detail\": [0, 2, 10], \"version\": [0, 5, 12], \"gener\": [0, 1, 4, 6, 7, 10], \"least\": [0, 1], \"signific\": 0, \"digit\": 0, \"we\": [0, 1, 10, 12], \"simpli\": [0, 2], \"drop\": [0, 2, 10], \"100\": [0, 2, 12], \"round\": [0, 2], \"lowest\": 0, \"whole\": [0, 6], \"number\": [0, 1, 2, 3, 7, 8, 10], \"floor\": 0, \"function\": [0, 1, 2, 6, 10], \"bpl\": [0, 1, 2], \"bpl_root\": 0, \"condit\": [0, 1, 2, 3, 4, 7], \"logic\": 0, \"work\": [0, 1, 2, 5, 7, 10, 12], \"sql\": [0, 1, 2, 3, 4, 7, 10], \"express\": [0, 1, 2], \"claus\": [0, 1], \"if_valu\": 0, \"else_valu\": 0, \"race\": [0, 1, 2, 9, 12], \"ipum\": [0, 6], \"code\": [0, 1, 2, 5], \"categori\": [0, 8], \"get\": [0, 1, 10], \"down\": [0, 6, 12], \"nearest\": 0, \"produc\": [0, 10], \"relat\": [0, 1, 2], \"hundr\": 0, \"300\": 0, \"child\": [0, 8], \"household\": [0, 4, 6, 8, 10, 12], \"head\": 0, \"301\": 0, \"302\": 0, \"adopt\": 0, \"303\": 0, \"step\": [0, 1, 2, 6], \"usual\": [0, 7, 12], \"need\": [0, 1, 2, 7, 10, 12], \"2\": [0, 1, 2, 3, 7, 8, 11, 12], \"spous\": 0, \"3\": [0, 1, 2, 5, 7, 8, 9, 12], \"4\": [0, 1, 8], \"law\": 0, \"5\": [0, 1, 2, 8, 9, 10, 12], \"parent\": [0, 1, 11], \"6\": [0, 2, 8, 9, 12], \"7\": [0, 1, 2, 8, 12], \"sibl\": 0, \"12\": [0, 5], \"relate_div_100\": [0, 1, 2], \"page\": [1, 2, 10], \"comparison_featur\": [1, 2, 7], \"along\": 1, \"header\": [1, 2, 3, 9, 11], \"context\": [1, 3, 9], \"relatematch\": [1, 2], \"comparison_typ\": [1, 2], \"categor\": [1, 2, 8, 9], \"true\": [1, 2, 3, 7, 9, 11, 12], \"maximum\": [1, 8], \"jaro\": [1, 9], \"winkler\": [1, 9], \"find\": [1, 7, 12], \"greatest\": 1, \"among\": 1, \"cartesian\": 1, \"product\": [1, 6, 12], \"column\": [1, 3, 4, 7, 9, 10, 11, 12], \"namelast\": [1, 2], \"would\": [1, 2, 12], \"return\": [1, 3, 8, 10], \"four\": 1, \"namefrst_a\": 1, \"namefrst_b\": 1, \"namelast_b\": 1, \"namelast_a\": 1, \"maximum_jw\": 1, \"score\": [1, 2, 7, 9], \"namefrst_jw\": [1, 2, 12], \"geograph\": 1, \"filter\": [1, 4, 7, 11], \"major\": [1, 10], \"locat\": [1, 2, 10], \"boundari\": 1, \"zero\": 1, \"jw_street\": 1, \"enum_dist\": 1, \"max\": [1, 8, 10], \"member\": [1, 7], \"neighborhood\": 1, \"surnam\": 1, \"related_individual_max_jw\": 1, \"namefrst_rel\": 1, \"assert\": [1, 10], \"NOT\": 1, \"distinct\": 1, \"f1\": 1, \"evalu\": [1, 2, 6, 7, 8], \"ani\": [1, 2, 3, 5, 8], \"potenti\": [1, 4, 7], \"mismatch\": 1, \"queri\": [1, 2], \"fi\": 1, \"OR\": 1, \"mi0\": 1, \"mi1\": 1, \"THEN\": 1, \"els\": [1, 2, 3], \"first_init_col\": 1, \"namefrst_init\": 1, \"mid_init_col\": 1, \"namefrst_mid_init_2\": 1, \"f2\": 1, \"empti\": 1, \"null\": [1, 2, 3], \"AND\": [1, 2], \"individu\": [1, 2, 7, 12], \"mainli\": 1, \"caution\": [1, 9], \"flag\": [1, 9, 10, 12], \"f\": [1, 10], \"sp\": 1, \"m_caution\": [1, 2, 9, 12], \"mbpl\": 1, \"mother_birthyr\": 1, \"stepmom\": 1, \"momloc\": 1, \"comp_a\": [1, 2], \"comp_b\": [1, 2], \"comp_c\": 1, \"parent_step_chang\": 1, \"comp_d\": 1, \"check\": [1, 10], \"sign\": 1, \"boolean\": [1, 2, 3, 11, 12], \"form\": [1, 7, 11], \"cast\": 1, \"namelast_equal_as_int\": 1, \"namelast_clean\": [1, 2, 3], \"whether\": [1, 2, 11], \"join\": [1, 11], \"across\": 1, \"being\": [1, 7], \"exact\": [1, 2], \"namefrst_unstd\": [1, 2], \"present\": [1, 2, 9], \"nonzero\": 1, \"primarili\": [1, 7], \"indic\": [1, 12], \"kind\": 1, \"incompar\": 1, \"akin\": 1, \"miss\": [1, 10], \"see\": [1, 2, 5, 10, 12], \"univers\": [1, 4, 7], \"similar\": 1, \"fbpl_nomatch\": 1, \"fbpl\": 1, \"allow\": [1, 2, 7, 12], \"up\": [1, 2, 10, 11], \"sub\": 1, \"object\": [1, 2, 6, 10], \"document\": [1, 8, 10, 12], \"sp_caution\": [1, 2, 12], \"spouse_bpl\": 1, \"spouse_birthyr\": 1, \"durmarr\": [1, 2], \"new_marr\": [1, 2], \"street_jw\": [1, 2, 12], \"9\": 1, \"multipli\": 1, \"after\": [1, 2, 4, 8, 10], \"float\": [1, 2, 8], \"comp\": 1, \"c\": 1, \"sploc\": 1, \"012\": 1, \"fals\": [1, 2, 3, 4, 6, 10], \"d\": 1, \"under\": [1, 2], \"specif\": [1, 2, 10], \"circumst\": 1, \"should\": [1, 2, 8, 9, 10], \"mid_init_match\": 1, \"either_1\": 1, \"nativ\": 1, \"either_0\": 1, \"gen\": 1, \"imm\": [1, 2, 12], \"immigr\": 1, \"look\": [1, 10, 11], \"foreign\": 1, \"born\": 1, \"sgen\": [1, 2, 12], \"rel\": [1, 2, 12], \"scala\": 1, \"determin\": [1, 7], \"greater\": [1, 5], \"jw_threshold\": 1, \"less\": [1, 2], \"age_threshold\": 1, \"sex\": [1, 2, 11], \"sampl\": 1, \"related_individual_row\": 1, \"unrel\": 1, \"depend\": [1, 2, 5, 12], \"name_col\": 1, \"birthyr_col\": 1, \"namefrst_related_row\": 1, \"replaced_birthyr\": [1, 2, 3], \"extra\": 1, \"children\": 1, \"who\": 1, \"base\": [1, 2, 7], \"expect\": 1, \"count\": [1, 10, 12], \"suspect\": [1, 6], \"relate_col\": 1, \"histid_col\": 1, \"id\": [1, 2], \"birth\": 1, \"year_b\": 1, \"wa\": [1, 12], \"minimum\": [1, 8], \"accept\": [1, 2, 12], \"consid\": [1, 8], \"histid\": [1, 2, 12], \"1910\": [1, 2, 12], \"8\": [1, 2, 5, 10], \"rate\": 1, \"calcul\": [1, 12], \"percentag\": 1, \"seen\": 1, \"neighbor\": 1, \"meet\": 1, \"95\": 1, \"nbor\": [1, 2, 12], \"namelast_neighbor\": 1, \"05\": [1, 2], \"namelast_popularity_sum\": 1, \"namelast_popular\": 1, \"length\": [1, 2, 9], \"size\": 1, \"ab\": 1, \"diff\": 1, \"absolut\": 1, \"invalid\": [1, 8], \"instead\": [1, 2, 5, 7], \"marriag\": 1, \"durat\": 1, \"99\": [1, 2], \"placehold\": 1, \"unknown\": 1, \"exclud\": 1, \"those\": [1, 2], \"consider\": 1, \"byrdiff\": [1, 2, 12], \"mardurmatch\": [1, 2], \"14\": 1, \"minu\": [1, 2], \"subtract\": 1, \"geo\": 1, \"distanc\": [1, 8], \"lookup\": 1, \"tabl\": [1, 2, 4, 7, 10, 12], \"core\": [1, 7, 10, 12], \"dist_tabl\": 1, \"py\": [1, 2], \"There\": [1, 2, 3, 7], \"sever\": [1, 6], \"wai\": [1, 5, 10], \"file\": [1, 4, 6, 7, 10, 11, 12], \"kei\": [1, 7, 10], \"key_count\": 1, \"secondari\": 1, \"serv\": 1, \"back\": 1, \"primari\": [1, 6], \"doe\": [1, 7, 12], \"particularli\": 1, \"state\": [1, 6], \"much\": [1, 7], \"fewer\": [1, 8], \"combin\": [1, 2, 3, 7], \"thu\": 1, \"risk\": 1, \"fill\": 1, \"aren\": 1, \"ex\": 1, \"just\": [1, 2, 10, 12], \"even\": 1, \"though\": 1, \"distances_fil\": 1, \"path\": [1, 2, 10, 11, 12], \"table_nam\": 1, \"what\": [1, 2, 10, 12], \"onc\": [1, 10], \"loc_a\": 1, \"where\": [1, 7, 10, 12], \"come\": 1, \"loc_b\": 1, \"distance_col\": 1, \"source_column_a\": 1, \"sourc\": [1, 4, 7, 10, 12], \"source_column_b\": 1, \"loc_a_0\": 1, \"loc_a_1\": 1, \"loc_b_0\": 1, \"loc_b_1\": 1, \"secondary_key_count\": 1, \"backup\": 1, \"secondary_table_nam\": 1, \"secondary_distances_fil\": 1, \"secondary_source_column\": 1, \"secondary_loc_a\": 1, \"secondary_loc_b\": 1, \"secondary_distance_col\": 1, \"state_dist\": 1, \"state_distance_lookup\": 1, \"county_state_dist\": 1, \"csv\": [1, 2, 7, 10, 11, 12], \"statecode1\": 1, \"statecode2\": 1, \"dist\": 1, \"county_dist\": [1, 2, 12], \"county_distance_lookup\": 1, \"county_1900_1910_distances_km\": 1, \"from_icpsrctyi\": 1, \"to_icpsrctyi\": 1, \"from_statefip\": 1, \"to_statefip\": 1, \"distance_km\": 1, \"state_1900_1910_distances_km\": 1, \"fetch\": 1, \"neither\": 1, \"nor\": 1, \"mpre\": 1, \"m_namefrst\": 1, \"accord\": 1, \"niu\": 1, \"other\": [1, 2, 12], \"mfbplmatch\": 1, \"multi\": 1, \"search\": 1, \"special\": 1, \"simplifi\": 1, \"particular\": [1, 2], \"constraint\": 1, \"num_col\": 1, \"whose\": 1, \"templat\": 1, \"n\": [1, 8, 9], \"per\": [1, 2, 8, 9, 10], \"current\": [1, 2, 10], \"respect\": [1, 7], \"jw_col_templ\": 1, \"jw\": 1, \"pair\": [1, 12], \"equal_and_not_null_templ\": 1, \"final\": [1, 2, 12], \"comput\": [1, 3, 7], \"_namefrst\": 1, \"_bpl\": 1, \"_sex\": 1, \"25\": 1, \"nvl\": 1, \"sm_namefrst\": 1, \"sn_namefrst\": 1, \"sm_bpl\": 1, \"sn_bpl\": 1, \"sm_sex\": 1, \"sn_sex\": 1, \"pass\": [1, 2, 7, 8], \"flexibl\": 1, \"user\": [1, 10], \"write\": [1, 10, 12], \"own\": [1, 2], \"favor\": 1, \"reason\": 1, \"good\": 1, \"fallback\": 1, \"defin\": [1, 7, 8, 9, 10], \"spark\": [1, 2, 5, 8, 9, 10, 12], \"builtin\": 1, \"argument\": [1, 10, 12], \"namelast_jw_max\": 1, \"namelast1\": 1, \"namelast2\": 1, \"namelast3\": 1, \"abov\": [1, 5], \"extend\": 1, \"beyond\": 1, \"top\": [1, 4], \"level\": [1, 4, 10], \"everi\": 1, \"jw_f\": [1, 2, 12], \"father_namefrst\": 1, \"rais\": [1, 3], \"exponenti\": 1, \"squar\": 1, \"county_distance_squar\": [1, 2, 12], \"county_a\": 1, \"county_b\": 1, \"upper\": 1, \"gt\": 1, \"btwn\": 1, \"addl\": 1, \"var\": [1, 2], \"program\": [1, 2, 7, 12], \"report\": [1, 4, 6, 10], \"addl_var\": 1, \"check_val_expr\": 1, \"else_v\": 1, \"volumn\": 1, \"datasourc\": [1, 2, 10], \"yrimmig\": 1, \"immyear_diff\": [1, 2, 9, 12], \"includ\": [1, 2, 7, 9, 10], \"train\": [1, 4, 6, 8, 10], \"independent_var\": [1, 2, 12], \"config\": [1, 4, 7, 10, 12], \"id_column\": [1, 2], \"_a\": 1, \"mult\": 1, \"exist\": [1, 2, 10], \"within\": [1, 2, 6, 10, 11], \"hh_train\": [1, 2, 7, 10, 12], \"hh\": 1, \"highest\": [1, 2], \"against\": [1, 11], \"ten\": [1, 2], \"tell\": 2, \"how\": [2, 7], \"descript\": [2, 8, 10], \"refer\": 2, \"here\": [2, 7, 10, 12], \"tutori\": [2, 10], \"script\": [2, 6, 10], \"discuss\": 2, \"readm\": 2, \"note\": 2, \"written\": [2, 6], \"toml\": [2, 6, 10], \"abl\": 2, \"json\": [2, 10], \"datasource_a\": [2, 7], \"datasource_b\": [2, 7], \"transform\": [2, 4, 6, 7], \"lowercase_strip\": 2, \"add_to_a\": 2, \"age_2\": 2, \"derived_from\": 2, \"expand_length\": 2, \"explod\": [2, 7], \"jaro_winkl\": 2, \"namelast_jw\": [2, 12], \"threshold\": [2, 8, 12], \"feature_nam\": 2, \"79\": 2, \"84\": 2, \"complex\": 2, \"machin\": [2, 6, 7, 10, 12], \"learn\": [2, 6, 7, 10, 12], \"probabilist\": [2, 6], \"drop_data_from_scored_match\": 2, \"us1900\": 2, \"us1900m_usa\": 2, \"p\": 2, \"parquet\": [2, 7], \"us1910\": 2, \"us1910m_usa\": 2, \"training_data_subset\": 2, \"serialp\": 2, \"rationalize_name_word\": 2, \"remove_qmark_hyphen\": 2, \"replace_apostroph\": 2, \"remove_suffix\": 2, \"remove_alternate_nam\": 2, \"condense_strip_whitespac\": 2, \"split\": [2, 3, 7, 8, 9, 12], \"namefrst_std\": [2, 11], \"array_index\": 2, \"bpl_orig\": 2, \"divide_by_int\": 2, \"get_floor\": 2, \"statefip_h\": 2, \"output_typ\": 2, \"substitution_column\": [2, 7, 11], \"join_column\": [2, 11], \"join_valu\": [2, 11], \"substitution_fil\": [2, 11], \"name_std\": [2, 11], \"male\": [2, 11], \"femal\": [2, 11], \"feature_select\": [2, 3, 7], \"input_column\": [2, 3, 9], \"output_column\": [2, 3, 9], \"sql_condit\": 2, \"namelast_bigram\": 2, \"bigram\": [2, 4], \"bpl_clean\": 2, \"bpl_str\": 2, \"washington\": 2, \"bpl2_str\": 2, \"53\": 2, \"region\": [2, 12], \"attach_vari\": 2, \"region_dict\": 2, \"col_to_join_on\": 2, \"col_to_add\": 2, \"null_fil\": 2, \"col_typ\": 2, \"potential_matches_univers\": [2, 7], \"birthyr_3\": 2, \"namefrst_std_jw\": [2, 12], \"75\": [2, 8, 12], \"comparis\": 2, \"post\": [2, 7], \"hh_comparison\": [2, 7], \"threshold_expr\": 2, \"fetch_a\": 2, \"sex_equ\": 2, \"equal\": [2, 11], \"relate_a\": [2, 9], \"pipeline_featur\": [2, 7, 9], \"sex_region_interact\": 2, \"transformer_typ\": [2, 9], \"interact\": [2, 4, 7, 12], \"relatetyp\": [2, 9], \"bucket\": [2, 7], \"hit\": [2, 10, 12], \"scale_data\": [2, 12], \"training_data\": [2, 10], \"dependent_var\": [2, 12], \"score_with_model\": [2, 12], \"use_training_data_featur\": [2, 7, 12], \"split_by_id_a\": [2, 12], \"decis\": [2, 4, 8, 12], \"drop_duplicate_with_threshold_ratio\": [2, 12], \"n_training_iter\": [2, 7, 12], \"output_suspicious_td\": [2, 12], \"param_grid\": [2, 12], \"model_paramet\": [2, 7, 8, 12], \"random_forest\": [2, 12], \"maxdepth\": [2, 8, 12], \"numtre\": [2, 8, 12], \"005\": 2, \"threshold_ratio\": [2, 8, 12], \"logistic_regress\": [2, 12], \"50\": [2, 12], \"65\": 2, \"80\": 2, \"chosen_model\": [2, 8, 12], \"prediction_col\": 2, \"predict\": [2, 12], \"hh_col\": 2, \"hh_training_data_1900_1910\": 2, \"probit\": [2, 4], \"go\": [2, 10], \"your\": [2, 5, 7, 10, 12], \"uniqu\": 2, \"identifi\": [2, 6, 12], \"full\": [2, 7, 12], \"short\": 2, \"alphanumer\": 2, \"convert_ints_to_long\": 2, \"long\": [2, 11], \"especi\": 2, \"assum\": 2, \"schema\": 2, \"sometim\": 2, \"term\": 2, \"bigint\": 2, \"thing\": 2, \"my_fil\": 2, \"subset\": [2, 11], \"limit\": 2, \"extract\": 2, \"modifi\": 2, \"meant\": 2, \"usag\": [2, 4, 10], \"set_value_column_a\": [2, 3], \"liter\": 2, \"set_value_column_b\": [2, 3], \"iv\": 2, \"v\": 2, \"vi\": 2, \"vii\": 2, \"viii\": 2, \"namelast_clean_bigram\": [2, 3], \"fed\": [2, 7], \"prep\": 2, \"df\": [2, 10], \"men\": 2, \"newli\": 2, \"attempt\": 2, \"duplic\": [2, 8], \"conjuct\": 2, \"Will\": 2, \"conjunct\": 2, \"rang\": [2, 9], \"original_valu\": 2, \"plu\": 2, \"1870\": 2, \"expand\": 2, \"1867\": 2, \"1868\": 2, \"1869\": 2, \"1871\": 2, \"1872\": 2, \"1873\": 2, \"kept\": 2, \"keep\": 2, \"appropri\": 2, \"treat\": [2, 9], \"import\": [2, 7, 10, 12], \"dure\": [2, 7], \"hot\": 2, \"encod\": [2, 3], \"vector\": [2, 9], \"stage\": 2, \"well\": 2, \"upper_threshold\": 2, \"cannot\": 2, \"robust\": 2, \"ml\": [2, 4, 8, 9], \"typic\": [2, 7], \"leverag\": 2, \"api\": [2, 6, 9], \"piplin\": 2, \"regionf\": 2, \"sex_regionf_interact\": 2, \"immyear_caut\": [2, 9], \"myriad\": 2, \"explor\": [2, 4, 6, 10], \"part\": [2, 7], \"task\": [2, 4, 6, 8, 12], \"drop_duplicate_a\": 2, \"out\": [2, 7, 12], \"best\": [2, 7], \"smallest\": 2, \"possibl\": 2, \"ratio\": [2, 8], \"beta\": [2, 8], \"test\": [2, 7, 12], \"model_explor\": [2, 10, 12], \"hyper\": [2, 6, 12], \"paramet\": [2, 6, 7, 8, 10, 12], \"eval\": 2, \"skip\": [2, 7], \"apply_model\": 2, \"run_all_step\": [2, 10, 12], \"command\": [2, 6, 10, 12], \"try\": 2, \"creation\": 2, \"iter\": 2, \"scale\": 2, \"error\": [2, 9], \"1900\": [2, 12], \"about\": [2, 10, 12], \"1930\": [2, 12], \"1940\": [2, 12], \"fail\": 2, \"were\": 2, \"sure\": [2, 5, 10], \"scratch\": 2, \"although\": 2, \"know\": 2, \"haven\": 2, \"save\": [2, 7, 12], \"small\": 2, \"amount\": 2, \"process\": [2, 6, 10], \"repeatedli\": 2, \"help\": [2, 7, 10], \"neg\": [2, 4, 6], \"area\": 2, \"coverag\": 2, \"increas\": [2, 9], \"represent\": [2, 7], \"ensur\": 2, \"group\": [2, 7], \"a304bt\": 2, \"three\": [2, 7], \"b200\": 2, \"c201\": 2, \"d425\": 2, \"perform\": [2, 6, 7, 11], \"feature_import\": [2, 7, 12], \"coeffici\": [2, 7], \"enabl\": [2, 7, 10], \"srace\": [2, 9, 12], \"race_interacted_srac\": [2, 9, 12], \"hits2\": [2, 12], \"exact_mult\": [2, 12], \"ncount\": [2, 3, 12], \"ncount2\": [2, 3, 12], \"f_interacted_jw_f\": [2, 12], \"f_caution\": [2, 12], \"f_pre\": [2, 12], \"fbplmatch\": [2, 12], \"m_interacted_jw_m\": [2, 9, 12], \"jw_m\": [2, 9, 12], \"m_pre\": [2, 9, 12], \"mbplmatch\": [2, 12], \"sp_interacted_jw_sp\": [2, 12], \"jw_sp\": [2, 12], \"sp_pre\": [2, 12], \"mi\": [2, 12], \"fsoundex\": [2, 12], \"lsoundex\": [2, 12], \"oth\": [2, 12], \"imm_interacted_immyear_caut\": [2, 12], \"1900_1910_training_data_20191023\": 2, \"jw_max_a\": 2, \"jw_max_b\": 2, \"f1_match\": 2, \"f2_match\": 2, \"byrdifcat\": 2, \"racematch\": 2, \"bplmatch\": 2, \"imm_interacted_bplmatch\": 2, \"sexmatch\": 2, \"relatetype_interacted_relatematch\": 2, \"checkpoint\": 3, \"no_first_pad\": 3, \"don\": 3, \"prepend\": 3, \"namefrst_unstd_bigram\": 3, \"namelast_frst_bigram\": 3, \"namelast_clean_soundex\": 3, \"input_col\": 3, \"output_col\": 3, \"expon\": 3, \"introduct\": 4, \"overview\": 4, \"instal\": 4, \"pypi\": 4, \"preprocess\": [4, 6, 10, 12], \"model\": [4, 6, 10], \"run\": [4, 5, 6, 7, 12], \"librari\": [4, 6], \"mode\": [4, 5, 12], \"advanc\": 4, \"workflow\": 4, \"export\": [4, 7, 10], \"featur\": [4, 6, 7, 8, 10], \"reus\": 4, \"basic\": 4, \"map\": [4, 7, 9], \"substitut\": [4, 7], \"block\": [4, 7], \"comparison\": [4, 7], \"pipelin\": 4, \"ons\": 4, \"aggreg\": 4, \"union\": 4, \"soundex\": 4, \"power\": 4, \"regex\": 4, \"random\": [4, 8], \"forest\": [4, 8], \"logist\": [4, 8], \"regress\": [4, 8], \"tree\": [4, 8], \"gradient\": [4, 8], \"boost\": [4, 8], \"system\": 5, \"python\": [5, 6, 10], \"java\": 5, \"integr\": 5, \"apach\": 5, \"via\": [5, 6], \"pyspark\": [5, 8, 9, 10], \"packag\": 5, \"org\": 5, \"latest\": 5, \"pip\": 5, \"easiest\": [5, 10], \"through\": [5, 7, 9, 10], \"instruct\": [5, 10], \"But\": 5, \"clone\": 5, \"github\": 5, \"repositori\": 5, \"root\": 5, \"project\": 5, \"directori\": [5, 10, 12], \"develop\": [5, 6], \"e\": 5, \"dev\": 5, \"edit\": 5, \"made\": 5, \"built\": 5, \"tool\": [5, 6], \"line\": [6, 10], \"share\": 6, \"characterist\": [6, 7], \"correspond\": [6, 7], \"real\": 6, \"world\": 6, \"determinist\": [6, 7], \"rule\": [6, 7], \"algorithm\": [6, 7], \"At\": [6, 7], \"been\": 6, \"unit\": 6, \"census\": 6, \"hierarch\": [6, 10], \"structur\": 6, \"nest\": 6, \"howev\": [6, 12], \"tailor\": 6, \"ignor\": 6, \"common\": [6, 7, 12], \"highli\": [6, 7], \"languag\": 6, \"further\": [6, 12], \"broken\": 6, \"smaller\": 6, \"sequenc\": 6, \"linkrun\": [6, 10], \"prepar\": [6, 7, 10], \"research\": 6, \"experi\": 6, \"understand\": 6, \"tune\": [6, 12], \"relationship\": 6, \"varieti\": 7, \"normal\": 7, \"abbrevi\": [7, 11], \"regist\": [7, 10], \"datafram\": [7, 10, 12], \"request\": 7, \"classif\": [7, 8], \"metadata\": 7, \"introspect\": 7, \"ingest\": 7, \"inspect\": 7, \"mani\": [7, 10], \"aspect\": [7, 10], \"extens\": 7, \"longest\": 7, \"definit\": 7, \"reduc\": 7, \"drastic\": 7, \"improv\": 7, \"runtim\": 7, \"separ\": 7, \"total\": 7, \"potential_match\": [7, 10], \"satisfi\": 7, \"elig\": 7, \"reshap\": 7, \"thought\": 7, \"ahead\": 7, \"chosen\": 7, \"experiment\": [7, 10], \"focus\": 7, \"demograph\": 7, \"moment\": 7, \"veri\": [7, 12], \"anyon\": 7, \"percent\": 7, \"remain\": 7, \"popul\": 7, \"pull\": 7, \"fix\": 7, \"width\": 7, \"crosswalk\": 7, \"construct\": 7, \"alpha\": 8, \"hyperparamet\": [8, 12], \"de\": 8, \"param\": [8, 12], \"label\": 8, \"doc\": [8, 9], \"commonli\": 8, \"explan\": 8, \"randomforestclassifi\": 8, \"depth\": 8, \"20\": 8, \"featuresubsetstrategi\": 8, \"node\": 8, \"auto\": 8, \"onethird\": 8, \"sqrt\": 8, \"log2\": 8, \"15\": 8, \"generalizedlinearregress\": 8, \"famili\": 8, \"binomi\": 8, \"85\": [8, 10], \"logisticregress\": 8, \"decisiontreeclassifi\": 8, \"mininstancespernod\": 8, \"caus\": 8, \"left\": 8, \"right\": [8, 10], \"discard\": 8, \"maxbin\": 8, \"bin\": 8, \"discret\": 8, \"continu\": [8, 9, 12], \"gbtclassifi\": 8, \"mother\": 9, \"point\": [9, 12], \"x\": [9, 10], \"y\": 9, \"hold\": 9, \"except\": 9, \"strictli\": 9, \"inf\": 9, \"explicitli\": 9, \"cover\": 9, \"doubl\": 9, \"outsid\": 9, \"job\": 10, \"high\": 10, \"class\": 10, \"handl\": 10, \"main\": 10, \"complet\": 10, \"access\": [10, 12], \"link_run\": 10, \"factori\": 10, \"sparkfactori\": 10, \"load_config\": 10, \"load_conf_fil\": 10, \"sparksess\": 10, \"now\": 10, \"let\": 10, \"load\": 10, \"our\": 10, \"my_conf\": 10, \"lr\": 10, \"prep_step\": 10, \"get_step\": 10, \"enumer\": 10, \"print\": 10, \"input_table_nam\": 10, \"output_table_nam\": 10, \"run_step\": 10, \"get_tabl\": 10, \"matches_df\": 10, \"hh_model_explor\": 10, \"method\": [10, 12], \"interfac\": 10, \"easili\": 10, \"conveni\": 10, \"adjust\": 10, \"set_loc\": 10, \"set_num_cor\": 10, \"set_executor_memori\": 10, \"5g\": 10, \"ll\": 10, \"dictionari\": 10, \"often\": 10, \"modul\": 10, \"pleas\": 10, \"reproduc\": 10, \"consol\": 10, \"cpu\": 10, \"h\": 10, \"executor_memori\": [10, 12], \"execute_task\": 10, \"execute_command\": 10, \"conf\": [10, 12], \"show\": 10, \"messag\": 10, \"exit\": 10, \"memori\": 10, \"executor\": 10, \"begin\": 10, \"execut\": 10, \"seri\": 10, \"excute_command\": 10, \"filepath\": 10, \"sai\": 10, \"fullcount_1870_1880\": 10, \"pattern\": 10, \"full_count_1870_1880\": 10, \"prompt\": 10, \"enter\": 10, \"text\": 10, \"unstabl\": 10, \"topic\": 10, \"analyz\": [10, 12], \"set_preexisting_t\": 10, \"x_persist\": 10, \"borrow_t\": 10, \"get_task\": 10, \"set_print_sql\": 10, \"x_sql\": 10, \"x_sqlf\": 10, \"ipython\": 10, \"showf\": 10, \"x_summari\": 10, \"desc\": 10, \"x_crosswalk\": 10, \"x_tab\": 10, \"q\": [10, 12], \"x_hh_tfam\": 10, \"x_tfam\": 10, \"drop_al\": 10, \"reload\": 10, \"x_hh_tfam_2a\": 10, \"x_tfam_raw\": 10, \"drop_all_prc\": 10, \"x_hh_tfam_2b\": 10, \"x_union\": 10, \"drop_all_temp\": 10, \"x_load\": 10, \"get_set\": 10, \"set_link_task\": 10, \"x_parquet_from_csv\": 10, \"organ\": 10, \"hierarchi\": 10, \"five\": 10, \"hh_match\": 10, \"someth\": 10, \"choic\": 10, \"preexist\": 10, \"prepped_df_a\": 10, \"prepped_df_b\": 10, \"raw_df_b\": 10, \"raw_df_a\": 10, \"training_featur\": [10, 12], \"scored_potential_match\": 10, \"potential_matches_prep\": 10, \"exploded_df_b\": 10, \"exploded_df_a\": 10, \"predicted_match\": 10, \"hh_training_featur\": [10, 12], \"hh_training_data\": 10, \"hh_predicted_match\": 10, \"hh_scored_potential_match\": 10, \"hh_potential_match\": 10, \"hh_blocked_match\": 10, \"hh_potential_matchs_prep\": 10, \"model_eval_training_vector\": 10, \"model_eval_training_data\": 10, \"model_eval_repeat_fp\": 10, \"model_eval_training_featur\": 10, \"model_eval_training_result\": 10, \"model_eval_repeat_fn\": 10, \"hh_model_eval_training_vector\": 10, \"hh_model_eval_repeat_fp\": 10, \"hh_model_eval_repeat_fn\": 10, \"hh_model_eval_training_result\": 10, \"hh_model_eval_training_featur\": 10, \"hh_model_eval_training_data\": 10, \"persist\": 10, \"hidden\": 10, \"intermedi\": 10, \"yet\": 10, \"databas\": 10, \"tablenam\": 10, \"istemporari\": 10, \"task_nam\": 10, \"num\": 10, \"finish\": 10, \"put\": [10, 12], \"launch\": [10, 12], \"my\": [10, 12], \"subhead\": 11, \"suppli\": 11, \"regex_word_replac\": 11, \"variant\": 11, \"av\": 11, \"7th\": 11, \"swap\": 11, \"still\": 11, \"anywher\": 11, \"proceed\": 11, \"street_unstd\": 11, \"dir\": 11, \"substitutions_street_abbrev\": 11, \"span\": 12, \"1920\": 12, \"deriv\": 12, \"necessari\": 12, \"scenario\": 12, \"copi\": 12, \"use_potential_matches_featur\": 12, \"full_count_1900_1910\": 12, \"50g\": 12, \"ask\": 12, \"arg\": 12, \"partit\": 12, \"training_data_1900_1910_hlink_featur\": 12, \"might\": 12, \"shut\": 12, \"framework\": 12, \"etc\": 12, \"relev\": 12, \"matrix\": 12, \"implement\": 12, \"regular\": 12, \"training_data_1900_1910\": 12, \"weren\": 12, \"ident\": 12, \"manual\": 12, \"updat\": 12, \"isn\": 12, \"analysi\": 12, \"training_result\": 12, \"hh_training_result\": 12, \"1900_1910_training_result\": 12, \"repeat_fp\": 12, \"repeat_fn\": 12, \"hh_repeat_fp\": 12, \"hh_repeat_fn\": 12, \"1900_1910_potential_fp\": 12, \"1900_1910_potential_fn\": 12, \"prefer\": 12, \"ve\": 12}, \"objects\": {}, \"objtypes\": {}, \"objnames\": {}, \"titleterms\": {\"column\": [0, 2], \"map\": [0, 2], \"basic\": [0, 2], \"usag\": 0, \"advanc\": [0, 2, 12], \"transform\": [0, 1, 3, 9], \"add_to_a\": 0, \"concat_to_a\": 0, \"concat_to_b\": 0, \"concat_two_col\": 0, \"lowercase_strip\": 0, \"rationalize_name_word\": 0, \"remove_qmark_hyphen\": 0, \"remove_punctu\": 0, \"replace_apostroph\": 0, \"remove_alternate_nam\": 0, \"remove_suffix\": 0, \"remove_stop_word\": 0, \"remove_prefix\": 0, \"condense_strip_whitespac\": 0, \"remove_one_letter_nam\": 0, \"split\": 0, \"array_index\": 0, \"substr\": 0, \"divide_by_int\": 0, \"when_valu\": 0, \"get_floor\": 0, \"comparison\": [1, 2], \"type\": [1, 9], \"add\": 1, \"ons\": 1, \"aggreg\": 1, \"featur\": [1, 2, 3, 9, 12], \"household\": [1, 2, 7], \"maximum_jaro_winkl\": 1, \"jaro_winkl\": 1, \"jaro_winkler_street\": 1, \"max_jaro_winkl\": 1, \"equal\": 1, \"f1_match\": 1, \"f2_match\": 1, \"not_equ\": 1, \"equals_as_int\": 1, \"all_equ\": 1, \"not_zero_and_not_equ\": 1, \"time\": 1, \"caution_comp_3\": 1, \"caution_comp_3_012\": 1, \"caution_comp_4\": 1, \"caution_comp_4_012\": 1, \"any_equ\": 1, \"either_are_1\": 1, \"either_are_0\": 1, \"second_gen_imm\": 1, \"rel_jaro_winkl\": 1, \"extra_children\": 1, \"jaro_winkler_r\": 1, \"sum\": 1, \"length_b\": 1, \"abs_diff\": 1, \"b_minus_a\": 1, \"geo_dist\": 1, \"fetch_a\": 1, \"fetch_b\": 1, \"present_both_year\": 1, \"neither_are_nul\": 1, \"present_and_matching_categor\": 1, \"present_and_not_equ\": 1, \"present_and_equal_categorical_in_univers\": 1, \"multi_jaro_winkler_search\": 1, \"sql_condit\": [1, 3], \"alia\": 1, \"power\": [1, 3], \"threshold\": 1, \"lower_threshold\": 1, \"upper_threshold\": 1, \"gt_threshold\": 1, \"btwn_threshold\": 1, \"look_at_addl_var\": 1, \"hit\": 1, \"hits2\": 1, \"exact_mult\": 1, \"jw_max_a\": 1, \"jw_max_b\": 1, \"configur\": [2, 4, 7], \"config\": 2, \"file\": 2, \"top\": 2, \"level\": 2, \"data\": [2, 11, 12], \"sourc\": [2, 5], \"filter\": 2, \"substitut\": [2, 11], \"select\": [2, 3], \"potenti\": [2, 12], \"match\": [2, 7], \"univers\": 2, \"block\": 2, \"pipelin\": [2, 9], \"gener\": [2, 9, 12], \"train\": [2, 7, 12], \"model\": [2, 7, 8, 12], \"bigram\": 3, \"arrai\": 3, \"union\": 3, \"soundex\": 3, \"welcom\": 4, \"hlink\": [4, 10], \"\": 4, \"document\": 4, \"api\": 4, \"instal\": 5, \"requir\": 5, \"from\": 5, \"pypi\": 5, \"introduct\": 6, \"overview\": [6, 7], \"link\": [7, 10, 12], \"task\": [7, 10], \"preprocess\": 7, \"step\": [7, 10], \"relat\": 7, \"section\": 7, \"explor\": [7, 12], \"report\": 7, \"random_forest\": 8, \"probit\": 8, \"logistic_regress\": 8, \"decision_tre\": 8, \"gradient_boosted_tre\": 8, \"interact\": [9, 10], \"bucket\": 9, \"run\": 10, \"us\": 10, \"librari\": 10, \"mode\": 10, \"start\": 10, \"program\": 10, \"exampl\": [10, 12], \"workflow\": [10, 12], \"1\": 11, \"tabl\": 11, \"regex\": 11, \"word\": 11, \"replac\": 11, \"export\": 12, \"after\": 12, \"reus\": 12, \"differ\": 12, \"year\": 12, \"ml\": 12, \"list\": 12, \"fals\": 12, \"posit\": 12, \"neg\": 12, \"fp\": 12, \"fn\": 12}, \"envversion\": {\"sphinx.domains.c\": 3, \"sphinx.domains.changeset\": 1, \"sphinx.domains.citation\": 1, \"sphinx.domains.cpp\": 9, \"sphinx.domains.index\": 1, \"sphinx.domains.javascript\": 3, \"sphinx.domains.math\": 2, \"sphinx.domains.python\": 4, \"sphinx.domains.rst\": 2, \"sphinx.domains.std\": 2, \"sphinx\": 60}, \"alltitles\": {\"Column Mappings\": [[0, \"column-mappings\"], [2, \"column-mappings\"]], \"Basic Usage\": [[0, \"basic-usage\"]], \"Advanced Usage\": [[0, \"advanced-usage\"]], \"Transforms\": [[0, \"transforms\"]], \"add_to_a\": [[0, \"add-to-a\"]], \"concat_to_a\": [[0, \"concat-to-a\"]], \"concat_to_b\": [[0, \"concat-to-b\"]], \"concat_two_cols\": [[0, \"concat-two-cols\"]], \"lowercase_strip\": [[0, \"lowercase-strip\"]], \"rationalize_name_words\": [[0, \"rationalize-name-words\"]], \"remove_qmark_hyphen\": [[0, \"remove-qmark-hyphen\"]], \"remove_punctuation\": [[0, \"remove-punctuation\"]], \"replace_apostrophe\": [[0, \"replace-apostrophe\"]], \"remove_alternate_names\": [[0, \"remove-alternate-names\"]], \"remove_suffixes\": [[0, \"remove-suffixes\"]], \"remove_stop_words\": [[0, \"remove-stop-words\"]], \"remove_prefixes\": [[0, \"remove-prefixes\"]], \"condense_strip_whitespace\": [[0, \"condense-strip-whitespace\"]], \"remove_one_letter_names\": [[0, \"remove-one-letter-names\"]], \"split\": [[0, \"split\"]], \"array_index\": [[0, \"array-index\"]], \"mapping\": [[0, \"mapping\"]], \"substring\": [[0, \"substring\"]], \"divide_by_int\": [[0, \"divide-by-int\"]], \"when_value\": [[0, \"when-value\"]], \"get_floor\": [[0, \"get-floor\"]], \"Comparison types, transform add-ons, aggregate features, and household aggregate features\": [[1, \"comparison-types-transform-add-ons-aggregate-features-and-household-aggregate-features\"]], \"Comparison types\": [[1, \"comparison-types\"]], \"maximum_jaro_winkler\": [[1, \"maximum-jaro-winkler\"]], \"jaro_winkler\": [[1, \"jaro-winkler\"]], \"jaro_winkler_street\": [[1, \"jaro-winkler-street\"]], \"max_jaro_winkler\": [[1, \"max-jaro-winkler\"]], \"equals\": [[1, \"equals\"]], \"f1_match\": [[1, \"f1-match\"]], \"f2_match\": [[1, \"f2-match\"]], \"not_equals\": [[1, \"not-equals\"]], \"equals_as_int\": [[1, \"equals-as-int\"]], \"all_equals\": [[1, \"all-equals\"]], \"not_zero_and_not_equals\": [[1, \"not-zero-and-not-equals\"]], \"or\": [[1, \"or\"]], \"and\": [[1, \"and\"]], \"times\": [[1, \"times\"]], \"caution_comp_3\": [[1, \"caution-comp-3\"]], \"caution_comp_3_012\": [[1, \"caution-comp-3-012\"]], \"caution_comp_4\": [[1, \"caution-comp-4\"]], \"caution_comp_4_012\": [[1, \"caution-comp-4-012\"]], \"any_equals\": [[1, \"any-equals\"]], \"either_are_1\": [[1, \"either-are-1\"]], \"either_are_0\": [[1, \"either-are-0\"]], \"second_gen_imm\": [[1, \"second-gen-imm\"]], \"rel_jaro_winkler\": [[1, \"rel-jaro-winkler\"]], \"extra_children\": [[1, \"extra-children\"]], \"jaro_winkler_rate\": [[1, \"jaro-winkler-rate\"]], \"sum\": [[1, \"sum\"]], \"length_b\": [[1, \"length-b\"]], \"abs_diff\": [[1, \"abs-diff\"]], \"b_minus_a\": [[1, \"b-minus-a\"]], \"geo_distance\": [[1, \"geo-distance\"]], \"fetch_a\": [[1, \"fetch-a\"]], \"fetch_b\": [[1, \"fetch-b\"]], \"present_both_years\": [[1, \"present-both-years\"]], \"neither_are_null\": [[1, \"neither-are-null\"]], \"present_and_matching_categorical\": [[1, \"present-and-matching-categorical\"]], \"present_and_not_equal\": [[1, \"present-and-not-equal\"]], \"present_and_equal_categorical_in_universe\": [[1, \"present-and-equal-categorical-in-universe\"]], \"multi_jaro_winkler_search\": [[1, \"multi-jaro-winkler-search\"]], \"sql_condition\": [[1, \"sql-condition\"], [3, \"sql-condition\"]], \"Feature add-ons\": [[1, \"feature-add-ons\"]], \"alias\": [[1, \"alias\"]], \"power\": [[1, \"power\"], [3, \"power\"]], \"threshold\": [[1, \"threshold\"]], \"lower_threshold\": [[1, \"lower-threshold\"]], \"upper_threshold\": [[1, \"upper-threshold\"]], \"gt_threshold\": [[1, \"gt-threshold\"]], \"btwn_threshold\": [[1, \"btwn-threshold\"]], \"look_at_addl_var\": [[1, \"look-at-addl-var\"]], \"Aggregate Features\": [[1, \"aggregate-features\"]], \"hits\": [[1, \"hits\"]], \"hits2\": [[1, \"hits2\"]], \"exact_mult\": [[1, \"exact-mult\"]], \"Household Aggregate Features\": [[1, \"household-aggregate-features\"]], \"jw_max_a\": [[1, \"jw-max-a\"]], \"jw_max_b\": [[1, \"jw-max-b\"]], \"Configuration\": [[2, \"configuration\"]], \"Basic Config File\": [[2, \"basic-config-file\"]], \"Advanced Config File\": [[2, \"advanced-config-file\"]], \"Top level configs\": [[2, \"top-level-configs\"]], \"Data sources\": [[2, \"data-sources\"]], \"Filter\": [[2, \"filter\"]], \"Substitution Columns\": [[2, \"substitution-columns\"]], \"Feature Selections\": [[2, \"feature-selections\"]], \"Potential Matches Universe\": [[2, \"potential-matches-universe\"]], \"Blocking\": [[2, \"blocking\"]], \"Comparisons\": [[2, \"comparisons\"]], \"Household Comparisons\": [[2, \"household-comparisons\"]], \"Comparison Features\": [[2, \"comparison-features\"]], \"Pipeline-generated Features\": [[2, \"pipeline-generated-features\"]], \"Training and models\": [[2, \"training-and-models\"]], \"Household training and models\": [[2, \"household-training-and-models\"]], \"Welcome to hlink\\u2019s documentation!\": [[4, \"welcome-to-hlink-s-documentation\"]], \"Configuration API\": [[4, \"configuration-api\"], [4, null]], \"Installation\": [[5, \"installation\"]], \"Requirements\": [[5, \"requirements\"]], \"Installing from PyPI\": [[5, \"installing-from-pypi\"]], \"Installing from source\": [[5, \"installing-from-source\"]], \"Introduction\": [[6, \"introduction\"]], \"Overview\": [[6, \"overview\"], [7, \"overview\"], [7, \"id1\"], [7, \"id4\"], [7, \"id7\"], [7, \"id10\"], [7, \"id13\"]], \"Link Tasks\": [[7, \"link-tasks\"]], \"Preprocessing\": [[7, \"preprocessing\"]], \"Task steps\": [[7, \"task-steps\"], [7, \"id2\"], [7, \"id5\"], [7, \"id8\"], [7, \"id11\"], [7, \"id14\"]], \"Related Configuration Sections\": [[7, \"related-configuration-sections\"], [7, \"id3\"], [7, \"id6\"], [7, \"id9\"], [7, \"id12\"], [7, \"id15\"]], \"Training and Household Training\": [[7, \"training-and-household-training\"]], \"Matching\": [[7, \"matching\"]], \"Household Matching\": [[7, \"household-matching\"]], \"Model Exploration and Household Model Exploration\": [[7, \"model-exploration-and-household-model-exploration\"]], \"Reporting\": [[7, \"reporting\"]], \"Models\": [[8, \"models\"]], \"random_forest\": [[8, \"random-forest\"]], \"probit\": [[8, \"probit\"]], \"logistic_regression\": [[8, \"logistic-regression\"]], \"decision_tree\": [[8, \"decision-tree\"]], \"gradient_boosted_trees\": [[8, \"gradient-boosted-trees\"]], \"Pipeline generated features\": [[9, \"pipeline-generated-features\"]], \"Transformer types\": [[9, \"transformer-types\"]], \"interaction\": [[9, \"interaction\"]], \"bucketizer\": [[9, \"bucketizer\"]], \"Running hlink\": [[10, \"running-hlink\"]], \"Using hlink as a Library\": [[10, \"using-hlink-as-a-library\"]], \"Interactive Mode\": [[10, \"interactive-mode\"]], \"Starting the program\": [[10, \"starting-the-program\"]], \"Running Linking Tasks and Steps\": [[10, \"running-linking-tasks-and-steps\"]], \"Example interactive mode workflow\": [[10, \"example-interactive-mode-workflow\"]], \"Substitutions\": [[11, \"substitutions\"]], \"1:1 substitution by data table\": [[11, \"substitution-by-data-table\"]], \"Substitution by regex word replace\": [[11, \"substitution-by-regex-word-replace\"]], \"Advanced Workflow Examples\": [[12, \"advanced-workflow-examples\"]], \"Export training data after generating features to reuse in different linking years\": [[12, \"export-training-data-after-generating-features-to-reuse-in-different-linking-years\"]], \"Example training data export with generated ML features\": [[12, \"example-training-data-export-with-generated-ml-features\"]], \"ML model exploration and export of lists of potential false positives/negatives in training data\": [[12, \"ml-model-exploration-and-export-of-lists-of-potential-false-positives-negatives-in-training-data\"]], \"Example model exploration and FP/FN export workflow\": [[12, \"example-model-exploration-and-fp-fn-export-workflow\"]], \"Feature Selection transforms\": [[3, \"feature-selection-transforms\"]], \"bigrams\": [[3, \"bigrams\"]], \"array\": [[3, \"array\"]], \"union\": [[3, \"union\"]], \"soundex\": [[3, \"soundex\"]]}, \"indexentries\": {}})\n\\ No newline at end of file\ndiff --git a/hlink/linking/core/transforms.py b/hlink/linking/core/transforms.py\nindex 6df980b..e7073b5 100755\n--- a/hlink/linking/core/transforms.py\n+++ b/hlink/linking/core/transforms.py\n@@ -3,6 +3,8 @@\n # in this project's top-level directory, and also on-line at:\n #   https://github.com/ipums/hlink\n \n+from typing import Any\n+\n from pyspark.sql.functions import (\n     array,\n     collect_list,\n@@ -24,13 +26,18 @@\n )\n from pyspark.sql.types import ArrayType, LongType, StringType\n from pyspark.ml import Pipeline\n-from pyspark.sql import Window\n+from pyspark.sql import DataFrame, SparkSession, Window\n from pyspark.ml.feature import NGram, RegexTokenizer, CountVectorizer, MinHashLSH\n \n \n def generate_transforms(\n-    spark, df_selected, feature_selections, link_task, is_a, id_col\n-):\n+    spark: SparkSession,\n+    df_selected: DataFrame,\n+    feature_selections: list[dict[str, Any]],\n+    link_task,\n+    is_a: bool,\n+    id_col: str,\n+) -> DataFrame:\n     not_skipped_feature_selections = [\n         c\n         for c in feature_selections\n@@ -43,7 +50,9 @@ def generate_transforms(\n         if (\"post_agg_feature\" in c) and c[\"post_agg_feature\"]\n     ]\n \n-    def parse_feature_selections(df_selected, feature_selection, is_a):\n+    def parse_feature_selections(\n+        df_selected: DataFrame, feature_selection: dict[str, Any], is_a: bool\n+    ) -> DataFrame:\n         transform = feature_selection[\"transform\"]\n \n         if not feature_selection.get(\"output_column\", False):\n@@ -114,9 +123,9 @@ def parse_feature_selections(df_selected, feature_selection, is_a):\n             return df_selected\n \n         elif transform == \"array\":\n-            col1, col2 = feature_selection[\"input_columns\"]\n+            input_cols = feature_selection[\"input_columns\"]\n             output_col = feature_selection[\"output_column\"]\n-            df_selected = df_selected.withColumn(output_col, array(col1, col2))\n+            df_selected = df_selected.withColumn(output_col, array(input_cols))\n             return df_selected\n \n         elif transform == \"union\":\n@@ -300,7 +309,7 @@ def union_list(list_a, list_b):\n     for feature_selection in not_skipped_feature_selections:\n         df_selected = parse_feature_selections(df_selected, feature_selection, is_a)\n \n-    def get_transforms(name, is_a):\n+    def get_transforms(name: str, is_a: bool) -> list[dict[str, Any]]:\n         to_process = []\n         for f in not_skipped_feature_selections:\n             if (\"override_column_a\" in f) and is_a:\ndiff --git a/sphinx-docs/feature_selection_transforms.md b/sphinx-docs/feature_selection_transforms.md\nindex 0e7e332..c87440a 100644\n--- a/sphinx-docs/feature_selection_transforms.md\n+++ b/sphinx-docs/feature_selection_transforms.md\n@@ -46,10 +46,10 @@ transform = \"sql_condition\"\n \n ## array\n \n-Combine two input columns into an array output column.\n+Combine any number of input columns into a single array output column.\n \n * Attributes:\n-  * `input_columns` - Type: list of strings. Required. The two input columns.\n+  * `input_columns` - Type: list of strings. Required. The list of input columns.\n   * `output_column` - Type: `string`. Required.\n \n ```\n", "instance_id": "ipums__hlink-135", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear, with a well-defined goal of extending the `array` feature selection to support 1 or 3+ input columns instead of exactly two. The input and output expectations are implied through the context of the existing code and the provided code changes. The statement identifies the specific file (`hlink/linking/core/transforms.py`) and the relevant code snippet to modify, which adds to its clarity. However, there are minor ambiguities: it does not explicitly mention how to handle edge cases (e.g., empty input lists or invalid column names) or whether there are specific constraints on the number of columns beyond the minimum of 1. Additionally, while it notes that `pyspark.sql.functions.array()` supports variable arguments, it does not discuss potential performance implications or validation requirements. These missing details prevent it from being comprehensive, but the overall intent and scope are clear enough for implementation.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to its straightforward nature and limited scope. The required change involves modifying a single line in `transforms.py` to handle a variable number of input columns using the existing `array()` function from PySpark, which already supports variable arguments. This requires basic understanding of Python list unpacking and PySpark's API, but no complex logic or deep architectural changes are needed. The code changes are localized to one file and one function, with minimal impact on the broader codebase. Additionally, the problem does not introduce significant new technical concepts beyond basic list handling, and the provided diff already shows the necessary modification. While there might be minor edge cases (e.g., handling an empty list of columns), they are not explicitly mentioned or complex to address. Documentation updates are also included, which are trivial to implement. Overall, this is a simple feature addition that a junior to mid-level developer with basic PySpark knowledge could handle with ease.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc1b [Bug] Importing `torchao` first breaks `torch_tensorrt.dynamo.compile` during `run_decompositions`\n##  Bug Description\r\n\r\nImporting `torchao` before importing `torch_tensorrt` causes `F.interpolate` to fail during `run_decompositions` with:\r\n\r\n`AssertionError: Expected aten.upsample_nearest2d.default to have CompositeImplicitAutograd kernel`\r\n\r\n## To Reproduce\r\n\r\n```py\r\nimport torchao\r\nimport torch_tensorrt\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        _, _, h, w = x.shape\r\n        z = F.interpolate(x, [h*2, w*2])\r\n        return z\r\n\r\nmodel = Model().cuda()\r\ninputs = (torch.randn((2, 4, 8, 8)).cuda(),)\r\n\r\nwith torch.no_grad():\r\n    ep = torch.export.export(\r\n        model,\r\n        args=inputs,\r\n        strict=True\r\n    )\r\n    with torch_tensorrt.logging.debug():\r\n        trt_gm = torch_tensorrt.dynamo.compile(\r\n            ep,\r\n            inputs,\r\n            reuse_cached_engines=False,\r\n            cache_built_engines=False,\r\n            require_full_compilation=True,\r\n            min_block_size=1,\r\n        )  \r\n```\r\n\r\n**Logs**:\r\n\r\n```txt\r\nWARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/projects/scripts/mre/torchao_tensorrt_import.py\", line 25, in <module>\r\n    trt_gm = torch_tensorrt.dynamo.compile(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch_tensorrt/dynamo/_compiler.py\", line 228, in compile\r\n    exported_program = exported_program.run_decompositions(\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/export/exported_program.py\", line 116, in wrapper\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/export/exported_program.py\", line 1111, in run_decompositions\r\n    return _decompose_exported_program(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/export/exported_program.py\", line 654, in _decompose_exported_program\r\n    gm, new_graph_signature = _decompose_and_get_gm_with_new_signature_constants(\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/export/exported_program.py\", line 446, in _decompose_and_get_gm_with_new_signature_constants\r\n    gm, graph_signature = aot_export_module(\r\n                          ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1262, in aot_export_module\r\n    fx_g, metadata, in_spec, out_spec = _aot_export_function(\r\n                                        ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1497, in _aot_export_function\r\n    fx_g, meta = create_aot_dispatcher_function(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 524, in create_aot_dispatcher_function\r\n    return _create_aot_dispatcher_function(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 625, in _create_aot_dispatcher_function\r\n    fw_metadata = run_functionalized_fw_and_collect_metadata(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\", line 194, in inner\r\n    flat_f_outs = f(*flat_f_args)\r\n                  ^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\r\n    tree_out = fn(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 859, in functional_call\r\n    out = PropagateUnbackedSymInts(mod).run(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 146, in run\r\n    self.env[node] = self.run_node(node)\r\n                     ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6495, in run_node\r\n    result = super().run_node(n)\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 203, in run_node\r\n    return getattr(self, n.op)(n.target, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 275, in call_function\r\n    return target(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_ops.py\", line 723, in __call__\r\n    return self._op(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_higher_order_ops/utils.py\", line 64, in inner\r\n    return autograd_not_implemented_inner(op, deferred_error, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_higher_order_ops/utils.py\", line 37, in autograd_not_implemented_inner\r\n    result = operator(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_ops.py\", line 723, in __call__\r\n    return self._op(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py\", line 449, in __torch_dispatch__\r\n    r = func.decompose(*args, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_ops.py\", line 764, in decompose\r\n    return self.py_kernels[dk](*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\", line 3184, in upsample_nearest2d_vec\r\n    return torch.ops.aten.upsample_nearest2d.default(input, osize)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_ops.py\", line 723, in __call__\r\n    return self._op(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py\", line 449, in __torch_dispatch__\r\n    r = func.decompose(*args, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_ops.py\", line 764, in decompose\r\n    return self.py_kernels[dk](*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/dgcnz/.conda/envs/edge/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 376, in _special_op_to_decompose_cia\r\n    raise AssertionError(\r\nAssertionError: Expected aten.upsample_nearest2d.default to have CompositeImplicitAutograd kernel\r\n\r\nWhile executing %upsample_nearest2d : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%x, [16, 16], None), kwargs = {})\r\nOriginal traceback:\r\n  File \"/projects/scripts/mre/torchao_tensorrt_import.py\", line 12, in forward\r\n    z = F.interpolate(x, [h*2, w*2])\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nThe import order between`torch_tensorrt` and `torchao` should not matter.\r\n\r\n## Environment\r\n\r\n> Build information about Torch-TensorRT can be found by turning on debug messages\r\n\r\n - Torch-TensorRT Version (e.g. 1.0.0): 2.6.0.dev20241008+cu124\r\n - PyTorch Version (e.g. 1.0): 2.6.0.dev20241009+cu124\r\n - CPU Architecture: x86_64\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, `libtorch`, source): pip\r\n - Build command you used (if compiling from source): -\r\n - Are you using local sources or building from archives: -\r\n - Python version: 3.12.7\r\n - CUDA version: 12.4\r\n - GPU models and configuration: NVIDIA RTX 4060 TI \r\n - Any other relevant information:\r\n - torchao==0.6.0.dev20241009+cu124\r\n\r\n## Additional context\r\n\r\nIf you import `torch_tensorrt` first and then `torchao` the error disappears.\n", "patch": "diff --git a/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py b/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\nindex f88a1a5e3c..07c8c03697 100644\n--- a/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\n+++ b/py/torch_tensorrt/dynamo/conversion/aten_ops_converters.py\n@@ -3110,232 +3110,21 @@ def aten_ops_pad(\n     )\n \n \n-for op in (\n-    torch.ops.aten.upsample_nearest1d,\n-    torch.ops.aten.upsample_nearest2d,\n-    torch.ops.aten.upsample_nearest3d,\n-    torch.ops.aten.upsample_linear1d,\n-    torch.ops.aten.upsample_bilinear2d,\n-    torch.ops.aten.upsample_trilinear3d,\n-    torch.ops.aten.upsample_bicubic2d,\n-):\n-    for key in (\n-        torch._C.DispatchKey.Autograd,\n-        torch._C.DispatchKey.CompositeImplicitAutograd,\n-    ):\n-        if key in op.default.py_kernels:\n-            del op.default.py_kernels[key]\n-        if key in op.vec.py_kernels:\n-            del op.vec.py_kernels[key]\n-\n-\n-def upsample_compute_output_size(\n-    input_size: torch.Size,\n-    output_size: Optional[Sequence[int]],\n-    scale_factors: Optional[Sequence[float]],\n-) -> Optional[Sequence[int]]:\n-    spatial_dimensions = len(input_size) - 2\n-\n-    if output_size is None and scale_factors is None:\n-        raise AssertionError(\n-            \"Must specify exactly one of output_size and scale_factors\"\n-        )\n-\n-    if output_size is not None:\n-        torch._check(\n-            scale_factors is None,\n-            lambda: \"Must specify exactly one of output_size and scale_factors\",\n-        )\n-        torch._check(len(output_size) == spatial_dimensions)\n-\n-    if scale_factors is not None:\n-        torch._check(\n-            output_size is None,\n-            lambda: \"Must specify exactly one of output_size and scale_factors\",\n-        )\n-        torch._check(len(scale_factors) == spatial_dimensions)\n-        output_size = []\n-        for i, s in enumerate(scale_factors):\n-            output_size.append(int(input_size[i + 2] * s))\n-\n-    return output_size\n-\n-\n-@torch.ops.aten.upsample_nearest1d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_nearest1d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_nearest1d.default(input, osize, *scale_factors)\n-    return torch.ops.aten.upsample_nearest1d.default(input, osize)\n-\n-\n-@torch.ops.aten.upsample_nearest2d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_nearest2d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_nearest2d.default(input, osize, *scale_factors)\n-    return torch.ops.aten.upsample_nearest2d.default(input, osize)\n-\n-\n-@torch.ops.aten.upsample_nearest3d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_nearest3d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_nearest3d.default(input, osize, *scale_factors)\n-    return torch.ops.aten.upsample_nearest3d.default(input, osize)\n-\n-\n-@torch.ops.aten.upsample_linear1d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_linear1d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    align_corners: bool,\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_linear1d.default(\n-            input, osize, align_corners, *scale_factors\n-        )\n-    return torch.ops.aten.upsample_linear1d.default(input, osize, align_corners)\n-\n-\n-@torch.ops.aten.upsample_bilinear2d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_bilinear2d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    align_corners: bool,\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_bilinear2d.default(\n-            input, osize, align_corners, *scale_factors\n-        )\n-    return torch.ops.aten.upsample_bilinear2d.default(input, osize, align_corners)\n-\n-\n-@torch.ops.aten.upsample_trilinear3d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_trilinear3d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    align_corners: bool,\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_trilinear3d.default(\n-            input, osize, align_corners, *scale_factors\n-        )\n-    return torch.ops.aten.upsample_trilinear3d.default(input, osize, align_corners)\n-\n-\n-@torch.ops.aten.upsample_bicubic2d.vec.py_impl(\n-    torch._C.DispatchKey.CompositeImplicitAutograd\n-)\n-def upsample_bicubic2d_vec(\n-    input: torch.Tensor,\n-    output_size: Optional[Sequence[int]],\n-    align_corners: bool,\n-    scale_factors: Optional[Sequence[float]],\n-) -> torch.Tensor:\n-    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)\n-    if scale_factors is not None:\n-        return torch.ops.aten.upsample_bicubic2d.default(\n-            input, osize, align_corners, *scale_factors\n-        )\n-    return torch.ops.aten.upsample_bicubic2d.default(input, osize, align_corners)\n-\n-\n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_nearest1d.default, supports_dynamic_shapes=True\n+    torch.ops.aten.upsample_nearest1d.vec, supports_dynamic_shapes=True\n )\n-@enforce_tensor_types(\n-    {\n-        0: (TRTTensor,),\n-    }\n-)\n-def aten_ops_upsample_nearest1d(\n-    ctx: ConversionContext,\n-    target: Target,\n-    args: Tuple[Argument, ...],\n-    kwargs: Dict[str, Argument],\n-    name: str,\n-) -> Union[TRTTensor, Sequence[TRTTensor]]:\n-    return impl.upsample.upsample(\n-        ctx,\n-        target,\n-        SourceIR.ATEN,\n-        name,\n-        args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 3 else [args[2]],\n-        mode=\"nearest\",\n-        align_corners=False,\n-    )\n-\n-\n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_nearest2d.default, supports_dynamic_shapes=True\n+    torch.ops.aten.upsample_nearest2d.vec, supports_dynamic_shapes=True\n )\n-@enforce_tensor_types(\n-    {\n-        0: (TRTTensor,),\n-    }\n-)\n-def aten_ops_upsample_nearest2d(\n-    ctx: ConversionContext,\n-    target: Target,\n-    args: Tuple[Argument, ...],\n-    kwargs: Dict[str, Argument],\n-    name: str,\n-) -> Union[TRTTensor, Sequence[TRTTensor]]:\n-    return impl.upsample.upsample(\n-        ctx,\n-        target,\n-        SourceIR.ATEN,\n-        name,\n-        args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 4 else [args[2], args[3]],\n-        mode=\"nearest\",\n-        align_corners=False,\n-    )\n-\n-\n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_nearest3d.default, supports_dynamic_shapes=True\n+    torch.ops.aten.upsample_nearest3d.vec, supports_dynamic_shapes=True\n )\n @enforce_tensor_types(\n     {\n         0: (TRTTensor,),\n     }\n )\n-def aten_ops_upsample_nearest3d(\n+def aten_ops_upsample_nearest(\n     ctx: ConversionContext,\n     target: Target,\n     args: Tuple[Argument, ...],\n@@ -3348,78 +3137,28 @@ def aten_ops_upsample_nearest3d(\n         SourceIR.ATEN,\n         name,\n         args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 5 else [args[2], args[3], args[4]],\n+        size=args_bounds_check(args, 1),\n+        scale_factor=args_bounds_check(args, 2),\n         mode=\"nearest\",\n         align_corners=False,\n     )\n \n \n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_linear1d.default, supports_dynamic_shapes=True\n-)\n-@enforce_tensor_types(\n-    {\n-        0: (TRTTensor,),\n-    }\n+    torch.ops.aten.upsample_linear1d.vec, supports_dynamic_shapes=True\n )\n-def aten_ops_upsample_linear1d(\n-    ctx: ConversionContext,\n-    target: Target,\n-    args: Tuple[Argument, ...],\n-    kwargs: Dict[str, Argument],\n-    name: str,\n-) -> Union[TRTTensor, Sequence[TRTTensor]]:\n-    return impl.upsample.upsample(\n-        ctx,\n-        target,\n-        SourceIR.ATEN,\n-        name,\n-        args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 4 else [args[3]],\n-        mode=\"linear\",\n-        align_corners=args[2],\n-    )\n-\n-\n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_bilinear2d.default, supports_dynamic_shapes=True\n+    torch.ops.aten.upsample_bilinear2d.vec, supports_dynamic_shapes=True\n )\n-@enforce_tensor_types(\n-    {\n-        0: (TRTTensor,),\n-    }\n-)\n-def aten_ops_upsample_bilinear2d(\n-    ctx: ConversionContext,\n-    target: Target,\n-    args: Tuple[Argument, ...],\n-    kwargs: Dict[str, Argument],\n-    name: str,\n-) -> Union[TRTTensor, Sequence[TRTTensor]]:\n-    return impl.upsample.upsample(\n-        ctx,\n-        target,\n-        SourceIR.ATEN,\n-        name,\n-        args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 5 else [args[3], args[4]],\n-        mode=\"bilinear\",\n-        align_corners=args[2],\n-    )\n-\n-\n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_trilinear3d.default, supports_dynamic_shapes=True\n+    torch.ops.aten.upsample_trilinear3d.vec, supports_dynamic_shapes=True\n )\n @enforce_tensor_types(\n     {\n         0: (TRTTensor,),\n     }\n )\n-def aten_ops_upsample_trilinear3d(\n+def aten_ops_upsample_linear(\n     ctx: ConversionContext,\n     target: Target,\n     args: Tuple[Argument, ...],\n@@ -3432,15 +3171,15 @@ def aten_ops_upsample_trilinear3d(\n         SourceIR.ATEN,\n         name,\n         args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 6 else [args[3], args[4], args[5]],\n-        mode=\"trilinear\",\n+        size=args_bounds_check(args, 1),\n+        scale_factor=args_bounds_check(args, 3),\n+        mode=\"linear\",\n         align_corners=args[2],\n     )\n \n \n @dynamo_tensorrt_converter(\n-    torch.ops.aten.upsample_bicubic2d.default, supports_dynamic_shapes=True\n+    torch.ops.aten.upsample_bicubic2d.vec, supports_dynamic_shapes=True\n )\n @enforce_tensor_types(\n     {\n@@ -3460,8 +3199,8 @@ def aten_ops_upsample_bicubic2d(\n         SourceIR.ATEN,\n         name,\n         args[0],\n-        size=args[1],\n-        scale_factor=None if len(args) < 5 else [args[3], args[4]],\n+        size=args_bounds_check(args, 1),\n+        scale_factor=args_bounds_check(args, 3),\n         mode=\"bicubic\",\n         align_corners=args[2],\n     )\ndiff --git a/py/torch_tensorrt/dynamo/conversion/impl/upsample.py b/py/torch_tensorrt/dynamo/conversion/impl/upsample.py\nindex 5d1e281699..247179455c 100644\n--- a/py/torch_tensorrt/dynamo/conversion/impl/upsample.py\n+++ b/py/torch_tensorrt/dynamo/conversion/impl/upsample.py\n@@ -18,14 +18,14 @@ def upsample(\n     source_ir: Optional[SourceIR],\n     name: str,\n     input: TRTTensor,\n-    size: Sequence[int],\n+    size: Optional[Sequence[int]],\n     scale_factor: Optional[Sequence[float]],\n     mode: str,\n     align_corners: bool,\n ) -> TRTTensor:\n     layer = ctx.net.add_resize(input)\n \n-    if scale_factor is not None and all(s is not None for s in scale_factor):\n+    if scale_factor is not None:\n         layer.scales = [1.0, 1.0] + list(scale_factor)\n     else:\n         shape = list(input.shape)[:2] + list(size)\ndiff --git a/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py b/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py\nindex c472c31a84..825be75076 100644\n--- a/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py\n+++ b/py/torch_tensorrt/dynamo/lowering/_decomposition_groups.py\n@@ -164,6 +164,13 @@\n }\n torch_disabled_decompositions: Set[Union[OpOverload, OpOverloadPacket]] = {\n     aten._softmax.default,\n+    aten.upsample_nearest1d.vec,\n+    aten.upsample_nearest2d.vec,\n+    aten.upsample_nearest3d.vec,\n+    aten.upsample_linear1d.vec,\n+    aten.upsample_bilinear2d.vec,\n+    aten.upsample_trilinear3d.vec,\n+    aten.upsample_bicubic2d.vec,\n }\n \n \ndiff --git a/py/torch_tensorrt/dynamo/lowering/_decompositions.py b/py/torch_tensorrt/dynamo/lowering/_decompositions.py\nindex d195ad81b8..dda014890d 100644\n--- a/py/torch_tensorrt/dynamo/lowering/_decompositions.py\n+++ b/py/torch_tensorrt/dynamo/lowering/_decompositions.py\n@@ -3,7 +3,8 @@\n from typing import Any, Callable, Dict, List, Optional\n \n import torch\n-from torch._decomp import _decomp_table_to_post_autograd_aten, register_decomposition\n+from torch._decomp import register_decomposition\n+from torch._export.utils import _decomp_table_to_post_autograd_aten\n from torch._ops import OpOverload\n from torch_tensorrt.dynamo._defaults import default_device\n from torch_tensorrt.dynamo.conversion.converter_utils import get_positive_dim\n@@ -411,8 +412,15 @@ def get_decompositions(\n         return {**CORE_ATEN_DECOMPOSITIONS_FILTERED, **TORCH_TRT_DECOMPOSITIONS}\n     else:\n         # changes made here due to torch2.6 changes https://github.com/pytorch/pytorch/pull/135080\n+        decomp_table = _decomp_table_to_post_autograd_aten()\n+        DECOMP_TABLE_FILTERED: Dict[OpOverload, Callable[[Any], Any]] = {\n+            decomp: decomp_table[decomp]\n+            for decomp in decomp_table\n+            if decomp not in torch_disabled_decompositions\n+        }\n+\n         return {\n             **ENABLED_TORCH_DECOMPOSITIONS,\n-            **_decomp_table_to_post_autograd_aten(),\n+            **DECOMP_TABLE_FILTERED,\n             **TORCH_TRT_DECOMPOSITIONS,\n         }\n", "instance_id": "pytorch__TensorRT-3227", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the bug, including the specific error message, steps to reproduce, expected behavior, and relevant environment details. The issue of import order between `torchao` and `torch_tensorrt` causing a failure in `F.interpolate` during `run_decompositions` is well-articulated with a reproducible code snippet and logs. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases beyond the import order, nor does it provide insight into why the import order causes the issue at a deeper technical level (e.g., interaction between libraries or kernel registration). Additionally, while the expected behavior is stated, there is no discussion of constraints or performance implications of a potential fix. Overall, it is clear enough to understand the issue and start working on a solution, but some minor details are missing.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files (`aten_ops_converters.py`, `upsample.py`, `_decomposition_groups.py`, `_decompositions.py`) and involving modifications to core functionality related to upsampling operations in PyTorch and TensorRT integration. The changes require understanding the interaction between PyTorch's operator dispatch system, decomposition mechanisms, and TensorRT's conversion pipeline, which are complex and non-trivial concepts. Second, the problem demands knowledge of advanced PyTorch internals, such as dispatch keys (e.g., `CompositeImplicitAutograd`), operator overloading, and decomposition tables, as well as familiarity with TensorRT's dynamic shape support and converter implementations. Third, while the problem statement does not explicitly mention edge cases beyond import order, the code changes suggest potential complexities in handling dynamic shapes and scale factors, which could introduce subtle bugs if not handled carefully. Finally, the impact of these changes is significant as they affect core operations used in model compilation, potentially influencing performance and compatibility across different hardware and library versions. Solving this requires a deep understanding of the codebase architecture and careful testing to ensure no regressions are introduced, justifying a difficulty score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add 10D Simple Test Function from Linkletter et al. (2006)\nThe ten-dimensional simple function from Linkletter et al. (2006)[^1] is a linear model defined as follows:\r\n\r\n$$\r\n\\mathcal{M}(\\boldsymbol{x}) = 0.2 x_1 + 0.2 x_2 + 0.2 x_3 + 0.2 x_4 + \\epsilon\r\n$$\r\n\r\nwhere $\\boldsymbol{x} = \\{ x_1, x_2, x_3, x_4 \\}$ is the vector of input variables modeled as an independent uniform random variable in $[0, 1]^4$. $\\epsilon$ is a random error generated from $\\mathcal{N}(\\mu = 0.0, \\sigma^2=0.05)$. Although the model is defined to be of ten-dimensional, only four input variables are active.\r\n\r\nIn the paper, the response is standardized (with mean 0 and standard deviation 1.0), but we will skip this in the implementation of the function in UQTestFuns. The candidate name for this function is `LinkLetter2006Simple`.\r\n\r\nThe model is used as a test function in the context of sensitivity analysis.\r\n\r\n[^1]: C. Linkletter, D. Bingham, N. Hengartner, D. Higdon, and K. Q. Ye, \u201cVariable Selection for Gaussian Process Models in Computer Experiments,\u201d Technometrics, vol. 48, no. 4, pp. 478\u2013490, Nov. 2006, doi: [10.1198/004017006000000228](https://doi.org/10.1198/004017006000000228).\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 46c99ce..2e9c378 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -9,6 +9,9 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ### Added\n \n+- The ten-dimensional linear function from Linkletter et al. (2006) featuring\n+  only four active input variables out of ten; the function was used\n+  in the context of metamodeling and sensitivity analysis.\n - The eight-dimensional function from Dette and Pepelyshev (2010) featuring\n   curved and logarithm terms for metamodeling exercises.\n - The three-dimensional highly-curved function from Dette and Pepelyshev (2010)\ndiff --git a/docs/fundamentals/metamodeling.md b/docs/fundamentals/metamodeling.md\nindex c69fe16..9636a4f 100644\n--- a/docs/fundamentals/metamodeling.md\n+++ b/docs/fundamentals/metamodeling.md\n@@ -18,51 +18,52 @@ kernelspec:\n The table below listed the available test functions typically used\n in the comparison of metamodeling approaches.\n \n-|                                  Name                                   | Input Dimension |      Constructor       |\n-|:-----------------------------------------------------------------------:|:---------------:|:----------------------:|\n-|                  {ref}`Ackley <test-functions:ackley>`                  |        M        |       `Ackley()`       |\n-|  {ref}`Alemazkoor & Meidani (2018) 2D <test-functions:alemazkoor-2d>`   |        2        |    `Alemazkoor2D()`    |\n-| {ref}`Alemazkoor & Meidani (2018) 20D <test-functions:alemazkoor-20d>`  |       20        |   `Alemazkoor20D()`    |\n-|                {ref}`Borehole <test-functions:borehole>`                |        8        |      `Borehole()`      |\n-|        {ref}`Cheng and Sandu (2010) 2D <test-functions:cheng2d>`        |        2        |       `Cheng2D`        |\n-|           {ref}`Coffee Cup Model <test-functions:coffee-cup>`           |        2        |     `CoffeeCup()`      |\n-|      {ref}`Currin et al. (1988) Sine <test-functions:currin-sine>`      |        1        |     `CurrinSine()`     |\n-|           {ref}`Damped Cosine <test-functions:damped-cosine>`           |        1        |    `DampedCosine()`    |\n-|       {ref}`Damped Oscillator <test-functions:damped-oscillator>`       |        7        |  `DampedOscillator()`  |\n-|      {ref}`Dette & Pepelyshev (2010) 8D <test-functions:dette-8d>`      |        3        |      `Dette8D()`       |\n-|  {ref}`Dette & Pepelyshev (2010) Curved <test-functions:dette-curved>`  |        3        |    `DetteCurved()`     |\n-| {ref}`Dette & Pepelyshev (2010) Exponential <test-functions:dette-exp>` |        3        |      `DetteExp()`      |\n-|                   {ref}`Flood <test-functions:flood>`                   |        8        |       `Flood()`        |\n-|        {ref}`Forrester et al. (2008) <test-functions:forrester>`        |        1        |   `Forrester2008()`    |\n-|              {ref}`(1st) Franke <test-functions:franke-1>`              |        2        |      `Franke1()`       |\n-|              {ref}`(2nd) Franke <test-functions:franke-2>`              |        2        |      `Franke2()`       |\n-|              {ref}`(3rd) Franke <test-functions:franke-3>`              |        2        |      `Franke3()`       |\n-|              {ref}`(4th) Franke <test-functions:franke-4>`              |        2        |      `Franke4()`       |\n-|              {ref}`(5th) Franke <test-functions:franke-5>`              |        2        |      `Franke5()`       |\n-|              {ref}`(6th) Franke <test-functions:franke-6>`              |        2        |      `Franke6()`       |\n-|            {ref}`Friedman (6D) <test-functions:friedman-6d>`            |        6        |     `Friedman6D()`     |\n-|           {ref}`Friedman (10D) <test-functions:friedman-10d>`           |       10        |    `Friedman10D()`     |\n-|       {ref}`Genz (Corner Peak) <test-functions:genz-corner-peak>`       |        M        |   `GenzCornerPeak()`   |\n-|     {ref}`Gramacy (2007) 1D Sine <test-functions:gramacy-1d-sine>`      |        1        |   `Gramacy1DSine()`    |\n-|         {ref}`Higdon (2002) Sine <test-functions:higdon-sine>`          |        1        |     `HigdonSine()`     |\n-|    {ref}`Holsclaw et al. (2013) Sine <test-functions:holsclaw-sine>`    |        1        |    `HolsclawSine()`    |\n-|  {ref}`Lim et al. (2002) Non-Polynomial <test-functions:lim-non-poly>`  |        2        |     `LimNonPoly()`     |\n-|      {ref}`Lim et al. (2002) Polynomial <test-functions:lim-poly>`      |        2        |      `LimPoly()`       |\n-|               {ref}`McLain S1 <test-functions:mclain-s1>`               |        2        |      `McLainS1()`      |\n-|               {ref}`McLain S2 <test-functions:mclain-s2>`               |        2        |      `McLainS2()`      |\n-|               {ref}`McLain S3 <test-functions:mclain-s3>`               |        2        |      `McLainS3()`      |\n-|               {ref}`McLain S4 <test-functions:mclain-s4>`               |        2        |      `McLainS4()`      |\n-|               {ref}`McLain S5 <test-functions:mclain-s5>`               |        2        |      `McLainS5()`      |\n-|      {ref}`Oakley & O'Hagan (2002) 1D <test-functions:oakley-1d>`       |        1        |      `Oakley1D()`      |\n-|             {ref}`OTL Circuit <test-functions:otl-circuit>`             |     6 / 20      |     `OTLCircuit()`     |\n-|            {ref}`Piston Simulation <test-functions:piston>`             |     7 / 20      |       `Piston()`       |\n-|               {ref}`Robot Arm <test-functions:robot-arm>`               |        8        |      `RobotArm()`      |\n-|           {ref}`Solar Cell Model <test-functions:solar-cell>`           |        5        |     `SolarCell()`      |\n-|                  {ref}`Sulfur <test-functions:sulfur>`                  |        9        |       `Sulfur()`       |\n-|     {ref}`Undamped Oscillator <test-functions:undamped-oscillator>`     |        6        | `UndampedOscillator()` |\n-|       {ref}`Webster et al. (1996) 2D <test-functions:webster-2d>`       |        2        |     `Webster2D()`      |\n-|          {ref}`Welch et al. (1992) <test-functions:welch1992>`          |       20        |     `Welch1992()`      |\n-|             {ref}`Wing Weight <test-functions:wing-weight>`             |       10        |     `WingWeight()`     |\n+|                                   Name                                    | Input Dimension |      Constructor       |\n+|:-------------------------------------------------------------------------:|:---------------:|:----------------------:|\n+|                   {ref}`Ackley <test-functions:ackley>`                   |        M        |       `Ackley()`       |\n+|   {ref}`Alemazkoor & Meidani (2018) 2D <test-functions:alemazkoor-2d>`    |        2        |    `Alemazkoor2D()`    |\n+|  {ref}`Alemazkoor & Meidani (2018) 20D <test-functions:alemazkoor-20d>`   |       20        |   `Alemazkoor20D()`    |\n+|                 {ref}`Borehole <test-functions:borehole>`                 |        8        |      `Borehole()`      |\n+|         {ref}`Cheng and Sandu (2010) 2D <test-functions:cheng2d>`         |        2        |       `Cheng2D`        |\n+|            {ref}`Coffee Cup Model <test-functions:coffee-cup>`            |        2        |     `CoffeeCup()`      |\n+|       {ref}`Currin et al. (1988) Sine <test-functions:currin-sine>`       |        1        |     `CurrinSine()`     |\n+|            {ref}`Damped Cosine <test-functions:damped-cosine>`            |        1        |    `DampedCosine()`    |\n+|        {ref}`Damped Oscillator <test-functions:damped-oscillator>`        |        7        |  `DampedOscillator()`  |\n+|       {ref}`Dette & Pepelyshev (2010) 8D <test-functions:dette-8d>`       |        3        |      `Dette8D()`       |\n+|   {ref}`Dette & Pepelyshev (2010) Curved <test-functions:dette-curved>`   |        3        |    `DetteCurved()`     |\n+|  {ref}`Dette & Pepelyshev (2010) Exponential <test-functions:dette-exp>`  |        3        |      `DetteExp()`      |\n+|                    {ref}`Flood <test-functions:flood>`                    |        8        |       `Flood()`        |\n+|         {ref}`Forrester et al. (2008) <test-functions:forrester>`         |        1        |   `Forrester2008()`    |\n+|               {ref}`(1st) Franke <test-functions:franke-1>`               |        2        |      `Franke1()`       |\n+|               {ref}`(2nd) Franke <test-functions:franke-2>`               |        2        |      `Franke2()`       |\n+|               {ref}`(3rd) Franke <test-functions:franke-3>`               |        2        |      `Franke3()`       |\n+|               {ref}`(4th) Franke <test-functions:franke-4>`               |        2        |      `Franke4()`       |\n+|               {ref}`(5th) Franke <test-functions:franke-5>`               |        2        |      `Franke5()`       |\n+|               {ref}`(6th) Franke <test-functions:franke-6>`               |        2        |      `Franke6()`       |\n+|             {ref}`Friedman (6D) <test-functions:friedman-6d>`             |        6        |     `Friedman6D()`     |\n+|            {ref}`Friedman (10D) <test-functions:friedman-10d>`            |       10        |    `Friedman10D()`     |\n+|        {ref}`Genz (Corner Peak) <test-functions:genz-corner-peak>`        |        M        |   `GenzCornerPeak()`   |\n+|      {ref}`Gramacy (2007) 1D Sine <test-functions:gramacy-1d-sine>`       |        1        |   `Gramacy1DSine()`    |\n+|          {ref}`Higdon (2002) Sine <test-functions:higdon-sine>`           |        1        |     `HigdonSine()`     |\n+|     {ref}`Holsclaw et al. (2013) Sine <test-functions:holsclaw-sine>`     |        1        |    `HolsclawSine()`    |\n+|   {ref}`Lim et al. (2002) Non-Polynomial <test-functions:lim-non-poly>`   |        2        |     `LimNonPoly()`     |\n+|       {ref}`Lim et al. (2002) Polynomial <test-functions:lim-poly>`       |        2        |      `LimPoly()`       |\n+| {ref}`Linkletter et al. (2006) Linear <test-functions:linkletter-linear>` |       10        |  `LinkletterLinear()`  |\n+|                {ref}`McLain S1 <test-functions:mclain-s1>`                |        2        |      `McLainS1()`      |\n+|                {ref}`McLain S2 <test-functions:mclain-s2>`                |        2        |      `McLainS2()`      |\n+|                {ref}`McLain S3 <test-functions:mclain-s3>`                |        2        |      `McLainS3()`      |\n+|                {ref}`McLain S4 <test-functions:mclain-s4>`                |        2        |      `McLainS4()`      |\n+|                {ref}`McLain S5 <test-functions:mclain-s5>`                |        2        |      `McLainS5()`      |\n+|       {ref}`Oakley & O'Hagan (2002) 1D <test-functions:oakley-1d>`        |        1        |      `Oakley1D()`      |\n+|              {ref}`OTL Circuit <test-functions:otl-circuit>`              |     6 / 20      |     `OTLCircuit()`     |\n+|             {ref}`Piston Simulation <test-functions:piston>`              |     7 / 20      |       `Piston()`       |\n+|                {ref}`Robot Arm <test-functions:robot-arm>`                |        8        |      `RobotArm()`      |\n+|            {ref}`Solar Cell Model <test-functions:solar-cell>`            |        5        |     `SolarCell()`      |\n+|                   {ref}`Sulfur <test-functions:sulfur>`                   |        9        |       `Sulfur()`       |\n+|      {ref}`Undamped Oscillator <test-functions:undamped-oscillator>`      |        6        | `UndampedOscillator()` |\n+|        {ref}`Webster et al. (1996) 2D <test-functions:webster-2d>`        |        2        |     `Webster2D()`      |\n+|           {ref}`Welch et al. (1992) <test-functions:welch1992>`           |       20        |     `Welch1992()`      |\n+|              {ref}`Wing Weight <test-functions:wing-weight>`              |       10        |     `WingWeight()`     |\n \n In a Python terminal, you can list all the available functions relevant\n for metamodeling applications using ``list_functions()``\ndiff --git a/docs/fundamentals/sensitivity.md b/docs/fundamentals/sensitivity.md\nindex 7c67e2b..f96d9ba 100644\n--- a/docs/fundamentals/sensitivity.md\n+++ b/docs/fundamentals/sensitivity.md\n@@ -18,32 +18,33 @@ kernelspec:\n The table below listed the available test functions typically used\n in the comparison of sensitivity analysis methods.\n \n-|                               Name                               | Input Dimension |      Constructor       |\n-|:----------------------------------------------------------------:|:---------------:|:----------------------:|\n-|            {ref}`Borehole <test-functions:borehole>`             |        8        |      `Borehole()`      |\n-|   {ref}`Bratley et al. (1992) A <test-functions:bratley1992a>`   |        M        |    `Bratley1992a()`    |\n-|   {ref}`Bratley et al. (1992) B <test-functions:bratley1992b>`   |        M        |    `Bratley1992b()`    |\n-|   {ref}`Bratley et al. (1992) C <test-functions:bratley1992c>`   |        M        |    `Bratley1992c()`    |\n-|   {ref}`Bratley et al. (1992) D <test-functions:bratley1992d>`   |        M        |    `Bratley1992d()`    |\n-|   {ref}`Damped Oscillator <test-functions:damped-oscillator>`    |        7        |  `DampedOscillator()`  |\n-|               {ref}`Flood <test-functions:flood>`                |        8        |       `Flood()`        |\n-|        {ref}`Friedman (6D) <test-functions:friedman-6d>`         |        6        |     `Friedman6D()`     |\n-|   {ref}`Genz (Corner Peak) <test-functions:genz-corner-peak>`    |        M        |   `GenzCornerPeak()`   |\n-| {ref}`Genz (Discontinuous) <test-functions:genz-discontinuous>`  |        M        | `GenzDiscontinuous()`  |\n-|            {ref}`Ishigami <test-functions:ishigami>`             |        3        |      `Ishigami()`      |\n-|          {ref}`Moon (2010) 3D <test-functions:moon3d>`           |        3        |       `Moon3D()`       |\n-|     {ref}`Morris et al. (2006) <test-functions:morris2006>`      |        M        |     `Morris2006()`     |\n-|         {ref}`OTL Circuit <test-functions:otl-circuit>`          |     6 / 20      |     `OTLCircuit()`     |\n-|         {ref}`Piston Simulation <test-functions:piston>`         |     7 / 20      |       `Piston()`       |\n-|   {ref}`Simple Portfolio Model <test-functions:portfolio-3d>`    |        3        |    `Portfolio3D()`     |\n-|      {ref}`SaltelliLinear <test-functions:saltelli-linear>`      |        M        |   `SaltelliLinear()`   |\n-|             {ref}`Sobol'-G <test-functions:sobol-g>`             |        M        |       `SobolG()`       |\n-|          {ref}`Sobol'-G* <test-functions:sobol-g-star>`          |        M        |     `SobolGStar()`     |\n-|       {ref}`Sobol'-Levitan <test-functions:sobol-levitan>`       |        M        |    `SobolLevitan()`    |\n-|       {ref}`Solar Cell Model <test-functions:solar-cell>`        |        5        |     `SolarCell()`      |\n-|              {ref}`Sulfur <test-functions:sulfur>`               |        9        |       `Sulfur()`       |\n-|      {ref}`Welch et al. (1992) <test-functions:welch1992>`       |       20        |     `Welch1992()`      |\n-|         {ref}`Wing Weight <test-functions:wing-weight>`          |       10        |     `WingWeight()`     |\n+|                                    Name                                    | Input Dimension |      Constructor       |\n+|:--------------------------------------------------------------------------:|:---------------:|:----------------------:|\n+|                 {ref}`Borehole <test-functions:borehole>`                  |        8        |      `Borehole()`      |\n+|        {ref}`Bratley et al. (1992) A <test-functions:bratley1992a>`        |        M        |    `Bratley1992a()`    |\n+|        {ref}`Bratley et al. (1992) B <test-functions:bratley1992b>`        |        M        |    `Bratley1992b()`    |\n+|        {ref}`Bratley et al. (1992) C <test-functions:bratley1992c>`        |        M        |    `Bratley1992c()`    |\n+|        {ref}`Bratley et al. (1992) D <test-functions:bratley1992d>`        |        M        |    `Bratley1992d()`    |\n+|        {ref}`Damped Oscillator <test-functions:damped-oscillator>`         |        7        |  `DampedOscillator()`  |\n+|                    {ref}`Flood <test-functions:flood>`                     |        8        |       `Flood()`        |\n+|             {ref}`Friedman (6D) <test-functions:friedman-6d>`              |        6        |     `Friedman6D()`     |\n+|        {ref}`Genz (Corner Peak) <test-functions:genz-corner-peak>`         |        M        |   `GenzCornerPeak()`   |\n+|      {ref}`Genz (Discontinuous) <test-functions:genz-discontinuous>`       |        M        | `GenzDiscontinuous()`  |\n+|                 {ref}`Ishigami <test-functions:ishigami>`                  |        3        |      `Ishigami()`      |\n+| {ref}`Linkletter et al. (2006) Linear <test-functions:linkletter-linear>`  |       10        |  `LinkletterLinear()`  |\n+|               {ref}`Moon (2010) 3D <test-functions:moon3d>`                |        3        |       `Moon3D()`       |\n+|          {ref}`Morris et al. (2006) <test-functions:morris2006>`           |        M        |     `Morris2006()`     |\n+|              {ref}`OTL Circuit <test-functions:otl-circuit>`               |     6 / 20      |     `OTLCircuit()`     |\n+|              {ref}`Piston Simulation <test-functions:piston>`              |     7 / 20      |       `Piston()`       |\n+|        {ref}`Simple Portfolio Model <test-functions:portfolio-3d>`         |        3        |    `Portfolio3D()`     |\n+|           {ref}`SaltelliLinear <test-functions:saltelli-linear>`           |        M        |   `SaltelliLinear()`   |\n+|                  {ref}`Sobol'-G <test-functions:sobol-g>`                  |        M        |       `SobolG()`       |\n+|               {ref}`Sobol'-G* <test-functions:sobol-g-star>`               |        M        |     `SobolGStar()`     |\n+|            {ref}`Sobol'-Levitan <test-functions:sobol-levitan>`            |        M        |    `SobolLevitan()`    |\n+|            {ref}`Solar Cell Model <test-functions:solar-cell>`             |        5        |     `SolarCell()`      |\n+|                   {ref}`Sulfur <test-functions:sulfur>`                    |        9        |       `Sulfur()`       |\n+|           {ref}`Welch et al. (1992) <test-functions:welch1992>`            |       20        |     `Welch1992()`      |\n+|              {ref}`Wing Weight <test-functions:wing-weight>`               |       10        |     `WingWeight()`     |\n \n In a Python terminal, you can list all the available functions relevant\n for metamodeling applications using ``list_functions()``\ndiff --git a/docs/references.bib b/docs/references.bib\nindex aee4893..ad043d2 100644\n--- a/docs/references.bib\n+++ b/docs/references.bib\n@@ -1043,4 +1043,15 @@ @Article{Dette2010\n   doi     = {10.1198/tech.2010.09157},\n }\n \n+@Article{Linkletter2006,\n+  author  = {Linkletter, Crystal and Bingham, Derek and Hengartner, Nicholas and Higdon, David and Ye, Kenny Q.},\n+  journal = {Technometrics},\n+  title   = {Variable selection for {Gaussian} process models in computer experiments},\n+  year    = {2006},\n+  number  = {4},\n+  pages   = {478--490},\n+  volume  = {48},\n+  doi     = {10.1198/004017006000000228},\n+}\n+\n @Comment{jabref-meta: databaseType:bibtex;}\n", "instance_id": "damar-wicaksono__uqtestfuns-431", "clarity": 3, "difficulty": 0.3, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly specifies the goal of adding a 10-dimensional simple test function from Linkletter et al. (2006), including the mathematical formulation of the model, the input variables (with their distribution), the error term (with its statistical properties), and the context of use (sensitivity analysis and metamodeling). The statement also clarifies implementation details, such as skipping standardization of the response and providing a candidate name for the function (`LinkLetter2006Simple`). Additionally, it includes a reference to the original paper for further context. There are no significant ambiguities regarding the problem's intent, input/output expectations, or constraints. The code changes provided in the diff align with the problem statement, updating relevant documentation and references, further supporting the clarity of the task. All critical details are present, making this a well-articulated problem description.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the straightforward nature of the task and the limited scope of required changes. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided diffs show updates to documentation files (`CHANGELOG.md`, `metamodeling.md`, `sensitivity.md`, and `references.bib`), indicating that the core implementation of the function is likely in a separate file not shown in the diff. However, based on the problem statement, the implementation of the function itself appears to be a simple linear model with a random error term, which would likely be confined to a single file or module within the `UQTestFuns` library. The changes do not suggest significant architectural impact or complex interactions with other parts of the codebase. The amount of code change for the actual function implementation is expected to be minimal, focusing on defining the linear combination of four active variables out of ten and adding a normally distributed error term.\n\n2. **Number of Technical Concepts**: The problem requires basic knowledge of Python (given the context of the repository and documentation), simple linear algebra for implementing the model equation, and basic statistical concepts for generating the random error term from a normal distribution (likely using a library like `numpy`). These concepts are not complex for a typical software engineer familiar with scientific computing. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic uncertainty quantification (UQ) are necessary.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention specific edge cases or error conditions to handle. Given the simplicity of the model (linear function with bounded uniform inputs and a normally distributed error), edge cases are minimal and likely limited to ensuring input dimensions match the expected 10D format. Error handling, if required, would be straightforward, such as validating input array shapes, which does not add significant complexity.\n\n4. **Overall Complexity**: The logic of the problem is inherently simple, as it involves implementing a linear function with a small number of active variables and a random error component. The task does not require deep understanding of the broader codebase architecture beyond integrating the function into the existing library structure (e.g., following a standard interface for test functions in `UQTestFuns`). The documentation updates shown in the diff are trivial and do not contribute to the difficulty.\n\nIn summary, this task is easy because it involves a small, self-contained implementation of a mathematically simple function with minimal technical challenges or codebase-wide impact. A score of 0.30 reflects the need for some understanding of the library's structure to integrate the function correctly, but the overall effort and complexity remain low.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add the One-Dimensional Sine Function from Currin et al. (1988)\nThe one-dimensional sine function from Currin et al. (1988)[^Currin] was used as a test function for Gaussian process metamodeling exercise and it is defined as follows:\r\n\r\n$$\r\n\\mathcal{M}(x) = \\sin{\\left( 2 \\pi (x - 0.1) \\right)},\r\n$$\r\n\r\nwhere $x$ is a uniform random variable in $[0, 1]$.\r\n\r\n[^Currin]: C. Currin, T. Mitchell, M. Morris, and D. Ylvisaker,  \u201cA Bayesian Approach to the Design and Analysis of Computer Experiments,\u201d ORNL-6498, 814584, Jan. 1988. doi: [10.2172/814584](https://doi.org/10.2172/814584).\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 4a4a2dc..e7076f5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -9,6 +9,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n ### Added\n \n+- The one-dimensional sine function from Currin et al. (1988) for metamodeling\n+  exercise.\n - The two-dimensional, time-dependent (vector-valued) cooling cup model\n   for metamodeling exercise.\n - The 8-dimensional robot arm function for metamodeling exercises.\ndiff --git a/docs/_toc.yml b/docs/_toc.yml\nindex 342d705..6affa25 100644\n--- a/docs/_toc.yml\n+++ b/docs/_toc.yml\n@@ -62,6 +62,8 @@ parts:\n             title: Coffee Cup\n           - file: test-functions/convex-fail-domain\n             title: Convex Failure Domain\n+          - file: test-functions/currin-sine\n+            title: Currin Sine\n           - file: test-functions/damped-cosine\n             title: Damped Cosine\n           - file: test-functions/damped-oscillator\ndiff --git a/docs/fundamentals/metamodeling.md b/docs/fundamentals/metamodeling.md\nindex 0a96b1a..29011b3 100644\n--- a/docs/fundamentals/metamodeling.md\n+++ b/docs/fundamentals/metamodeling.md\n@@ -18,44 +18,45 @@ kernelspec:\n The table below listed the available test functions typically used\n in the comparison of metamodeling approaches.\n \n-|                                  Name                                   | Input Dimension |       Constructor       |\n-|:-----------------------------------------------------------------------:|:---------------:|:-----------------------:|\n-|                  {ref}`Ackley <test-functions:ackley>`                  |        M        |       `Ackley()`        |\n-|  {ref}`Alemazkoor & Meidani (2018) 2D <test-functions:alemazkoor-2d>`   |        2        |    `Alemazkoor2D()`     |\n-| {ref}`Alemazkoor & Meidani (2018) 20D <test-functions:alemazkoor-20d>`  |       20        |    `Alemazkoor20D()`    |\n-|                {ref}`Borehole <test-functions:borehole>`                |        8        |      `Borehole()`       |\n-|        {ref}`Cheng and Sandu (2010) 2D <test-functions:cheng2d>`        |        2        |        `Cheng2D`        |\n-|           {ref}`Coffee Cup Model <test-functions:coffee-cup>`           |        2        |      `CoffeeCup()`      |\n-|           {ref}`Damped Cosine <test-functions:damped-cosine>`           |        1        |    `DampedCosine()`     |\n-|       {ref}`Damped Oscillator <test-functions:damped-oscillator>`       |        7        |  `DampedOscillator()`   |\n-|                   {ref}`Flood <test-functions:flood>`                   |        8        |        `Flood()`        |\n-|        {ref}`Forrester et al. (2008) <test-functions:forrester>`        |        1        |    `Forrester2008()`    |\n-|              {ref}`(1st) Franke <test-functions:franke-1>`              |        2        |       `Franke1()`       |\n-|              {ref}`(2nd) Franke <test-functions:franke-2>`              |        2        |       `Franke2()`       |\n-|              {ref}`(3rd) Franke <test-functions:franke-3>`              |        2        |       `Franke3()`       |\n-|              {ref}`(4th) Franke <test-functions:franke-4>`              |        2        |       `Franke4()`       |\n-|              {ref}`(5th) Franke <test-functions:franke-5>`              |        2        |       `Franke5()`       |\n-|              {ref}`(6th) Franke <test-functions:franke-6>`              |        2        |       `Franke6()`       |\n-|            {ref}`Friedman (6D) <test-functions:friedman-6d>`            |        6        |     `Friedman6D()`      |\n-|           {ref}`Friedman (10D) <test-functions:friedman-10d>`           |       10        |     `Friedman10D()`     |\n-|     {ref}`Gramacy (2007) 1D Sine <test-functions:gramacy-1d-sine>`      |        1        |    `Gramacy1DSine()`    |\n-|  {ref}`Lim et al. (2002) Non-Polynomial <test-functions:lim-non-poly>`  |        2        |     `LimNonPoly()`      |\n-|      {ref}`Lim et al. (2002) Polynomial <test-functions:lim-poly>`      |        2        |       `LimPoly()`       |\n-|               {ref}`McLain S1 <test-functions:mclain-s1>`               |        2        |      `McLainS1()`       |\n-|               {ref}`McLain S2 <test-functions:mclain-s2>`               |        2        |      `McLainS2()`       |\n-|               {ref}`McLain S3 <test-functions:mclain-s3>`               |        2        |      `McLainS3()`       |\n-|               {ref}`McLain S4 <test-functions:mclain-s4>`               |        2        |      `McLainS4()`       |\n-|               {ref}`McLain S5 <test-functions:mclain-s5>`               |        2        |      `McLainS5()`       |\n-|      {ref}`Oakley & O'Hagan (2002) 1D <test-functions:oakley-1d>`       |        1        |      `Oakley1D()`       |\n-|             {ref}`OTL Circuit <test-functions:otl-circuit>`             |     6 / 20      |     `OTLCircuit()`      |\n-|            {ref}`Piston Simulation <test-functions:piston>`             |     7 / 20      |       `Piston()`        |\n-|               {ref}`Robot Arm <test-functions:robot-arm>`               |        8        |      `RobotArm()`       |\n-|           {ref}`Solar Cell Model <test-functions:solar-cell>`           |        5        |      `SolarCell()`      |\n-|                  {ref}`Sulfur <test-functions:sulfur>`                  |        9        |       `Sulfur()`        |\n-|     {ref}`Undamped Oscillator <test-functions:undamped-oscillator>`     |        6        | `UndampedOscillator()`  |\n-|       {ref}`Webster et al. (1996) 2D <test-functions:webster-2d>`       |        2        |      `Webster2D()`      |\n-|          {ref}`Welch et al. (1992) <test-functions:welch1992>`          |       20        |      `Welch1992()`      |\n-|             {ref}`Wing Weight <test-functions:wing-weight>`             |       10        |     `WingWeight()`      |\n+|                                  Name                                  | Input Dimension |      Constructor       |\n+|:----------------------------------------------------------------------:|:---------------:|:----------------------:|\n+|                 {ref}`Ackley <test-functions:ackley>`                  |        M        |       `Ackley()`       |\n+|  {ref}`Alemazkoor & Meidani (2018) 2D <test-functions:alemazkoor-2d>`  |        2        |    `Alemazkoor2D()`    |\n+| {ref}`Alemazkoor & Meidani (2018) 20D <test-functions:alemazkoor-20d>` |       20        |   `Alemazkoor20D()`    |\n+|               {ref}`Borehole <test-functions:borehole>`                |        8        |      `Borehole()`      |\n+|       {ref}`Cheng and Sandu (2010) 2D <test-functions:cheng2d>`        |        2        |       `Cheng2D`        |\n+|          {ref}`Coffee Cup Model <test-functions:coffee-cup>`           |        2        |     `CoffeeCup()`      |\n+|            {ref}`Currin Sine <test-functions:currin-sine>`             |        1        |     `CurrinSine()`     |\n+|          {ref}`Damped Cosine <test-functions:damped-cosine>`           |        1        |    `DampedCosine()`    |\n+|      {ref}`Damped Oscillator <test-functions:damped-oscillator>`       |        7        |  `DampedOscillator()`  |\n+|                  {ref}`Flood <test-functions:flood>`                   |        8        |       `Flood()`        |\n+|       {ref}`Forrester et al. (2008) <test-functions:forrester>`        |        1        |   `Forrester2008()`    |\n+|             {ref}`(1st) Franke <test-functions:franke-1>`              |        2        |      `Franke1()`       |\n+|             {ref}`(2nd) Franke <test-functions:franke-2>`              |        2        |      `Franke2()`       |\n+|             {ref}`(3rd) Franke <test-functions:franke-3>`              |        2        |      `Franke3()`       |\n+|             {ref}`(4th) Franke <test-functions:franke-4>`              |        2        |      `Franke4()`       |\n+|             {ref}`(5th) Franke <test-functions:franke-5>`              |        2        |      `Franke5()`       |\n+|             {ref}`(6th) Franke <test-functions:franke-6>`              |        2        |      `Franke6()`       |\n+|           {ref}`Friedman (6D) <test-functions:friedman-6d>`            |        6        |     `Friedman6D()`     |\n+|          {ref}`Friedman (10D) <test-functions:friedman-10d>`           |       10        |    `Friedman10D()`     |\n+|     {ref}`Gramacy (2007) 1D Sine <test-functions:gramacy-1d-sine>`     |        1        |   `Gramacy1DSine()`    |\n+| {ref}`Lim et al. (2002) Non-Polynomial <test-functions:lim-non-poly>`  |        2        |     `LimNonPoly()`     |\n+|     {ref}`Lim et al. (2002) Polynomial <test-functions:lim-poly>`      |        2        |      `LimPoly()`       |\n+|              {ref}`McLain S1 <test-functions:mclain-s1>`               |        2        |      `McLainS1()`      |\n+|              {ref}`McLain S2 <test-functions:mclain-s2>`               |        2        |      `McLainS2()`      |\n+|              {ref}`McLain S3 <test-functions:mclain-s3>`               |        2        |      `McLainS3()`      |\n+|              {ref}`McLain S4 <test-functions:mclain-s4>`               |        2        |      `McLainS4()`      |\n+|              {ref}`McLain S5 <test-functions:mclain-s5>`               |        2        |      `McLainS5()`      |\n+|      {ref}`Oakley & O'Hagan (2002) 1D <test-functions:oakley-1d>`      |        1        |      `Oakley1D()`      |\n+|            {ref}`OTL Circuit <test-functions:otl-circuit>`             |     6 / 20      |     `OTLCircuit()`     |\n+|            {ref}`Piston Simulation <test-functions:piston>`            |     7 / 20      |       `Piston()`       |\n+|              {ref}`Robot Arm <test-functions:robot-arm>`               |        8        |      `RobotArm()`      |\n+|          {ref}`Solar Cell Model <test-functions:solar-cell>`           |        5        |     `SolarCell()`      |\n+|                 {ref}`Sulfur <test-functions:sulfur>`                  |        9        |       `Sulfur()`       |\n+|    {ref}`Undamped Oscillator <test-functions:undamped-oscillator>`     |        6        | `UndampedOscillator()` |\n+|      {ref}`Webster et al. (1996) 2D <test-functions:webster-2d>`       |        2        |     `Webster2D()`      |\n+|         {ref}`Welch et al. (1992) <test-functions:welch1992>`          |       20        |     `Welch1992()`      |\n+|            {ref}`Wing Weight <test-functions:wing-weight>`             |       10        |     `WingWeight()`     |\n \n In a Python terminal, you can list all the available functions relevant\n for metamodeling applications using ``list_functions()``\ndiff --git a/docs/references.bib b/docs/references.bib\nindex 33fea4b..50d1300 100644\n--- a/docs/references.bib\n+++ b/docs/references.bib\n@@ -964,4 +964,14 @@ @Article{Richardson2020\n   doi     = {10.5334/jors.303},\n }\n \n+@TechReport{Currin1988,\n+  author      = {Currin, C. and Mitchell, T. and Morris, M. and Ylvisaker, D.},\n+  institution = {Oak Ridge National Laboratory},\n+  title       = {A {Bayesian} approach to the design and analysis of computer experiments},\n+  year        = {1988},\n+  address     = {Oak Ridge, Tennessee},\n+  number      = {ORNL-6498},\n+  doi         = {10.2172/814584},\n+}\n+\n @Comment{jabref-meta: databaseType:bibtex;}\n", "instance_id": "damar-wicaksono__uqtestfuns-411", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in defining the goal of adding a one-dimensional sine function from Currin et al. (1988) for use in Gaussian process metamodeling exercises. The mathematical definition of the function is provided, along with the input domain (x as a uniform random variable in [0, 1]). The reference to the original paper is also included, which adds credibility and context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify how the function should be integrated into the codebase (e.g., as a standalone function, class, or part of a larger module), nor does it mention any specific requirements for input/output formats or constraints beyond the mathematical definition. Additionally, there is no discussion of potential edge cases or performance considerations for the implementation. While the code changes provide some context on integration, the problem statement itself lacks these critical details, making it \"Mostly Clear\" but not fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is rated as Easy (0.25) based on the provided factors. First, the scope and depth of code changes appear minimal, as the diffs primarily involve documentation updates (CHANGELOG.md, table of contents, metamodeling documentation, and references) rather than complex code implementation. Although the actual implementation code for the sine function is not shown in the provided diff, the nature of the function (a simple one-dimensional sine calculation) suggests that the core logic would be straightforward to implement, likely in a single file or function with minimal impact on the broader system architecture. Second, the number of technical concepts required is low; the problem involves basic mathematical computation (sine function) and likely simple integration into an existing framework for test functions, requiring only fundamental programming skills and possibly familiarity with a library or framework for metamodeling (e.g., Python's math module). Third, potential edge cases and error handling requirements seem negligible, as the input domain is well-defined ([0, 1]), and the function is mathematically continuous and smooth, with no obvious complex edge cases or error conditions mentioned in the problem statement or implied by the changes. Overall, this task requires understanding some code logic and making simple modifications, fitting within the Easy range (0.2-0.4), with a score of 0.25 reflecting the simplicity of the mathematical implementation balanced by the need to integrate it into an existing structure as inferred from the documentation updates.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Rules to enforce case on the `others` keyword in selected assignments\n**Is your feature request related to a problem? Please describe.**\r\nI'd like a rule to enforce case on the `others` keyword in selected assignments. For example, I'd like\r\n```vhdl\r\narchitecture rtl of fifo is\r\n\r\nbegin\r\n\r\n  my_proc : process (all) is\r\n  begin\r\n\r\n    with CONDITION select A <=\r\n      '1' when X,\r\n      '0' when OTHERS;\r\n\r\n    with CONDITION select A <=\r\n      '1' when X,\r\n      '0' when OTHERS;\r\n\r\n  end process my_proc;\r\n\r\nend architecture rtl;\r\n```\r\ncorrected to\r\n```vhdl\r\narchitecture rtl of fifo is\r\n\r\nbegin\r\n\r\n  my_proc : process (all) is\r\n  begin\r\n\r\n    with CONDITION select A <=\r\n      '1' when X,\r\n      '0' when others;\r\n\r\n    with CONDITION select A <=\r\n      '1' when X,\r\n      '0' when others;\r\n\r\n  end process my_proc;\r\n\r\nend architecture rtl;\r\n\n", "patch": "diff --git a/docs/aggregate_rules.rst b/docs/aggregate_rules.rst\nindex 114455d7c..92b5ceb5f 100644\n--- a/docs/aggregate_rules.rst\n+++ b/docs/aggregate_rules.rst\n@@ -6,20 +6,4 @@ Aggregate Rules\n aggregate_500\n #############\n \n-|phase_6| |error| |case| |case_keyword|\n-\n-This rule checks the *others* keyword in aggregates has proper case.\n-\n-|configuring_uppercase_and_lowercase_rules_link|\n-\n-**Violation**\n-\n-.. code-block:: vhdl\n-\n-   signal counter : t_counter := (OTHERS => '1');\n-\n-**Fix**\n-\n-.. code-block:: vhdl\n-\n-   signal counter : t_counter := (others => '1');\n+This rule has been deprecated and replaced with rule `choice_500 <choice_rules.html#choice-500>`_.\ndiff --git a/docs/case_generate_alternative_rules.rst b/docs/case_generate_alternative_rules.rst\nindex 71955e9fb..35834b3b5 100644\n--- a/docs/case_generate_alternative_rules.rst\n+++ b/docs/case_generate_alternative_rules.rst\n@@ -32,20 +32,4 @@ This rule checks the *when* keyword has proper case.\n case_generate_alternative_501\n #############################\n \n-|phase_6| |error| |case| |case_keyword|\n-\n-This rule checks the *others* keyword has proper case.\n-\n-|configuring_uppercase_and_lowercase_rules_link|\n-\n-**Violation**\n-\n-.. code-block:: vhdl\n-\n-   when OTHERS =>\n-\n-**Fix**\n-\n-.. code-block:: vhdl\n-\n-   when others =>\n+This rule has been deprecated and replaced with rule `choice_500 <choice_rules.html#choice-500>`_.\ndiff --git a/docs/case_rules.rst b/docs/case_rules.rst\nindex 3a278790a..ec03b412e 100644\n--- a/docs/case_rules.rst\n+++ b/docs/case_rules.rst\n@@ -558,20 +558,4 @@ This rule checks the indentation of the label.\n case_500\n ########\n \n-|phase_6| |error| |case| |case_keyword|\n-\n-This rule checks the *others* keyword has proper case.\n-\n-|configuring_uppercase_and_lowercase_rules_link|\n-\n-**Violation**\n-\n-.. code-block:: vhdl\n-\n-   when OTHERS =>\n-\n-**Fix**\n-\n-.. code-block:: vhdl\n-\n-   when others =>\n+This rule has been deprecated and replaced with rule `choice_500 <choice_rules.html#choice-500>`_.\ndiff --git a/docs/choice_rules.rst b/docs/choice_rules.rst\nnew file mode 100644\nindex 000000000..244bce256\n--- /dev/null\n+++ b/docs/choice_rules.rst\n@@ -0,0 +1,25 @@\n+.. include:: includes.rst\n+\n+Choice Rules\n+------------\n+\n+choice_500\n+##########\n+\n+|phase_6| |error| |case| |case_keyword|\n+\n+This rule checks the **others** keyword has proper case.\n+\n+|configuring_uppercase_and_lowercase_rules_link|\n+\n+**Violation**\n+\n+.. code-block:: vhdl\n+\n+    '0' when OTHERS;\n+\n+**Fix**\n+\n+.. code-block:: vhdl\n+\n+   '0' when others;\ndiff --git a/docs/configuring_uppercase_and_lowercase_rules.rst b/docs/configuring_uppercase_and_lowercase_rules.rst\nindex f90e3520e..3997085fe 100644\n--- a/docs/configuring_uppercase_and_lowercase_rules.rst\n+++ b/docs/configuring_uppercase_and_lowercase_rules.rst\n@@ -221,8 +221,6 @@ For example, if you want to uppercase everything except the entity name, you cou\n Rules Enforcing Case\n ####################\n \n-* `aggregate_500 <aggregate_rules.html#aggregate-500>`_\n-\n * `alias_declaration_500 <alias_declaration_rules.html#alias-declaration-500>`_\n * `alias_declaration_501 <alias_declaration_rules.html#alias-declaration-501>`_\n * `alias_declaration_502 <alias_declaration_rules.html#alias-declaration-502>`_\n@@ -268,14 +266,14 @@ Rules Enforcing Case\n * `case_016 <case_rules.html#case-016>`_\n * `case_017 <case_rules.html#case-017>`_\n * `case_018 <case_rules.html#case-018>`_\n-* `case_500 <case_rules.html#case-500>`_\n \n * `case_generate_alternative_500 <case_generate_alternative_rules.html#case-generate-alternative-500>`_\n-* `case_generate_alternative_501 <case_generate_alternative_rules.html#case-generate-alternative-501>`_\n \n * `case_generate_statement_500 <case_generate_statement_rules.html#case-generate-statement-500>`_\n * `case_generate_statement_501 <case_generate_statement_rules.html#case-generate-statement-501>`_\n \n+* `choice_500 <choice_rules.html#choice-500>`_\n+\n * `component_004 <component_rules.html#component-004>`_\n * `component_006 <component_rules.html#component-006>`_\n * `component_008 <component_rules.html#component-008>`_\ndiff --git a/docs/rule_groups/case_keyword_rule_group.rst b/docs/rule_groups/case_keyword_rule_group.rst\nindex 5745beac5..34419e351 100644\n--- a/docs/rule_groups/case_keyword_rule_group.rst\n+++ b/docs/rule_groups/case_keyword_rule_group.rst\n@@ -5,7 +5,6 @@ Case::Keyword Rule Group\n Rules Enforcing Case::Keyword Rule Group\n ########################################\n \n-* `aggregate_500 <../aggregate_rules.html#aggregate-500>`_\n * `alias_declaration_500 <../alias_declaration_rules.html#alias-declaration-500>`_\n * `alias_declaration_501 <../alias_declaration_rules.html#alias-declaration-501>`_\n * `architecture_004 <../architecture_rules.html#architecture-004>`_\n@@ -32,11 +31,10 @@ Rules Enforcing Case::Keyword Rule Group\n * `case_016 <../case_rules.html#case-016>`_\n * `case_017 <../case_rules.html#case-017>`_\n * `case_018 <../case_rules.html#case-018>`_\n-* `case_500 <../case_rules.html#case-500>`_\n * `case_generate_alternative_500 <../case_generate_alternative_rules.html#case-generate-alternative-500>`_\n-* `case_generate_alternative_501 <../case_generate_alternative_rules.html#case-generate-alternative-501>`_\n * `case_generate_statement_500 <../case_generate_statement_rules.html#case-generate-statement-500>`_\n * `case_generate_statement_501 <../case_generate_statement_rules.html#case-generate-statement-501>`_\n+* `choice_500 <../choice_rules.html#choice-500>`_\n * `component_004 <../component_rules.html#component-004>`_\n * `component_006 <../component_rules.html#component-006>`_\n * `component_010 <../component_rules.html#component-010>`_\ndiff --git a/docs/rule_groups/case_rule_group.rst b/docs/rule_groups/case_rule_group.rst\nindex c0581aef4..cc7bf2f4d 100644\n--- a/docs/rule_groups/case_rule_group.rst\n+++ b/docs/rule_groups/case_rule_group.rst\n@@ -5,7 +5,6 @@ Case Rule Group\n Rules Enforcing Case Rule Group\n ###############################\n \n-* `aggregate_500 <../aggregate_rules.html#aggregate-500>`_\n * `alias_declaration_500 <../alias_declaration_rules.html#alias-declaration-500>`_\n * `alias_declaration_501 <../alias_declaration_rules.html#alias-declaration-501>`_\n * `alias_declaration_502 <../alias_declaration_rules.html#alias-declaration-502>`_\n@@ -46,11 +45,10 @@ Rules Enforcing Case Rule Group\n * `case_016 <../case_rules.html#case-016>`_\n * `case_017 <../case_rules.html#case-017>`_\n * `case_018 <../case_rules.html#case-018>`_\n-* `case_500 <../case_rules.html#case-500>`_\n * `case_generate_alternative_500 <../case_generate_alternative_rules.html#case-generate-alternative-500>`_\n-* `case_generate_alternative_501 <../case_generate_alternative_rules.html#case-generate-alternative-501>`_\n * `case_generate_statement_500 <../case_generate_statement_rules.html#case-generate-statement-500>`_\n * `case_generate_statement_501 <../case_generate_statement_rules.html#case-generate-statement-501>`_\n+* `choice_500 <../choice_rules.html#choice-500>`_\n * `component_004 <../component_rules.html#component-004>`_\n * `component_006 <../component_rules.html#component-006>`_\n * `component_008 <../component_rules.html#component-008>`_\ndiff --git a/docs/rules.rst b/docs/rules.rst\nindex a89f4d4bf..b4d530045 100644\n--- a/docs/rules.rst\n+++ b/docs/rules.rst\n@@ -20,6 +20,7 @@ The rules are divided into categories depending on the part of the VHDL code bei\n    case_rules.rst\n    case_generate_alternative_rules.rst\n    case_generate_statement_rules.rst\n+   choice_rules.rst\n    comment_rules.rst\n    component_rules.rst\n    concurrent_rules.rst\ndiff --git a/vsg/rules/__init__.py b/vsg/rules/__init__.py\nindex 341420cd4..0815ef612 100644\n--- a/vsg/rules/__init__.py\n+++ b/vsg/rules/__init__.py\n@@ -112,6 +112,7 @@\n from vsg.rules import case\n from vsg.rules import case_generate_alternative\n from vsg.rules import case_generate_statement\n+from vsg.rules import choice\n from vsg.rules import comment\n from vsg.rules import component\n from vsg.rules import concurrent\ndiff --git a/vsg/rules/aggregate/rule_500.py b/vsg/rules/aggregate/rule_500.py\nindex fa4814705..54d74f6f9 100644\n--- a/vsg/rules/aggregate/rule_500.py\n+++ b/vsg/rules/aggregate/rule_500.py\n@@ -1,34 +1,13 @@\n # -*- coding: utf-8 -*-\n \n-from vsg import token\n-from vsg.rules import token_case_in_range_bounded_by_tokens as Rule\n-\n-lTokens = []\n-lTokens.append(token.choice.others_keyword)\n-\n-oStart = token.aggregate.open_parenthesis\n-oEnd = token.aggregate.close_parenthesis\n+from vsg.deprecated_rule import Rule\n \n \n class rule_500(Rule):\n     \"\"\"\n-    This rule checks the *others* keyword in aggregates has proper case.\n-\n-    |configuring_uppercase_and_lowercase_rules_link|\n-\n-    **Violation**\n-\n-    .. code-block:: vhdl\n-\n-       signal counter : t_counter := (OTHERS => '1');\n-\n-    **Fix**\n-\n-    .. code-block:: vhdl\n-\n-       signal counter : t_counter := (others => '1');\n+    This rule has been deprecated and replaced with rule `choice_500 <choice_rules.html#choice-500>`_.\n     \"\"\"\n \n     def __init__(self):\n-        super().__init__(lTokens, oStart, oEnd)\n-        self.groups.append(\"case::keyword\")\n+        Rule.__init__(self)\n+        self.message.append(\"Rule \" + self.unique_id + \" has been merged into choice_500.\")\ndiff --git a/vsg/rules/case/rule_500.py b/vsg/rules/case/rule_500.py\nindex 06040d36a..54d74f6f9 100644\n--- a/vsg/rules/case/rule_500.py\n+++ b/vsg/rules/case/rule_500.py\n@@ -1,35 +1,13 @@\n # -*- coding: utf-8 -*-\n \n-from vsg import token\n-from vsg.rules import token_case_in_range_bounded_by_tokens as Rule\n-\n-lTokens = []\n-lTokens.append(token.choice.others_keyword)\n-\n-oStartToken = token.case_statement_alternative.when_keyword\n-\n-oEndToken = token.case_statement_alternative.assignment\n+from vsg.deprecated_rule import Rule\n \n \n class rule_500(Rule):\n     \"\"\"\n-    This rule checks the *others* keyword has proper case.\n-\n-    |configuring_uppercase_and_lowercase_rules_link|\n-\n-    **Violation**\n-\n-    .. code-block:: vhdl\n-\n-       when OTHERS =>\n-\n-    **Fix**\n-\n-    .. code-block:: vhdl\n-\n-       when others =>\n+    This rule has been deprecated and replaced with rule `choice_500 <choice_rules.html#choice-500>`_.\n     \"\"\"\n \n     def __init__(self):\n-        super().__init__(lTokens, oStartToken, oEndToken)\n-        self.groups.append(\"case::keyword\")\n+        Rule.__init__(self)\n+        self.message.append(\"Rule \" + self.unique_id + \" has been merged into choice_500.\")\ndiff --git a/vsg/rules/case_generate_alternative/rule_501.py b/vsg/rules/case_generate_alternative/rule_501.py\nindex ed1e1932e..af5b4fd4d 100644\n--- a/vsg/rules/case_generate_alternative/rule_501.py\n+++ b/vsg/rules/case_generate_alternative/rule_501.py\n@@ -1,35 +1,13 @@\n # -*- coding: utf-8 -*-\n \n-from vsg import token\n-from vsg.rules import token_case_in_range_bounded_by_tokens as Rule\n-\n-lTokens = []\n-lTokens.append(token.choice.others_keyword)\n-\n-oStartToken = token.case_generate_alternative.when_keyword\n-\n-oEndToken = token.case_generate_alternative.assignment\n+from vsg.deprecated_rule import Rule\n \n \n class rule_501(Rule):\n     \"\"\"\n-    This rule checks the *others* keyword has proper case.\n-\n-    |configuring_uppercase_and_lowercase_rules_link|\n-\n-    **Violation**\n-\n-    .. code-block:: vhdl\n-\n-       when OTHERS =>\n-\n-    **Fix**\n-\n-    .. code-block:: vhdl\n-\n-       when others =>\n+    This rule has been deprecated and replaced with rule `choice_500 <choice_rules.html#choice-500>`_.\n     \"\"\"\n \n     def __init__(self):\n-        super().__init__(lTokens, oStartToken, oEndToken)\n-        self.groups.append(\"case::keyword\")\n+        Rule.__init__(self)\n+        self.message.append(\"Rule \" + self.unique_id + \" has been merged into choice_500.\")\ndiff --git a/vsg/rules/choice/__init__.py b/vsg/rules/choice/__init__.py\nnew file mode 100644\nindex 000000000..c5c1c6e60\n--- /dev/null\n+++ b/vsg/rules/choice/__init__.py\n@@ -0,0 +1,3 @@\n+# -*- coding: utf-8 -*-\n+\n+from .rule_500 import rule_500\ndiff --git a/vsg/rules/choice/rule_500.py b/vsg/rules/choice/rule_500.py\nnew file mode 100644\nindex 000000000..3fe4eb27c\n--- /dev/null\n+++ b/vsg/rules/choice/rule_500.py\n@@ -0,0 +1,31 @@\n+# -*- coding: utf-8 -*-\n+\n+from vsg import token\n+from vsg.rules import token_case\n+\n+lTokens = []\n+lTokens.append(token.choice.others_keyword)\n+\n+\n+class rule_500(token_case):\n+    \"\"\"\n+    This rule checks the **others** keyword has proper case.\n+\n+    |configuring_uppercase_and_lowercase_rules_link|\n+\n+    **Violation**\n+\n+    .. code-block:: vhdl\n+\n+        '0' when OTHERS;\n+\n+    **Fix**\n+\n+    .. code-block:: vhdl\n+\n+       '0' when others;\n+    \"\"\"\n+\n+    def __init__(self):\n+        super().__init__(lTokens)\n+        self.groups.append(\"case::keyword\")\n", "instance_id": "jeremiah-c-leary__vhdl-style-guide-1346", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the goal of enforcing a specific case (lowercase) for the 'others' keyword in VHDL code within selected assignments. It provides a concrete example of the desired transformation from uppercase 'OTHERS' to lowercase 'others', which helps in understanding the intent. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the scope of where this rule should apply beyond the provided example (e.g., does it apply to all VHDL constructs or only specific ones like aggregates or case statements?). Additionally, constraints or edge cases, such as handling mixed-case keywords or configurations for case sensitivity, are not mentioned. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the 'Easy' category (0.2-0.4) due to several factors. First, the scope of code changes is moderate but manageable, involving updates to documentation across multiple files and refactoring of rules in the codebase. The changes primarily consist of deprecating existing rules related to the 'others' keyword in different contexts (aggregate, case, case_generate_alternative) and consolidating them into a new unified rule (choice_500). This requires understanding the structure of the rule system in the VHDL static analysis tool (vsg) and making systematic updates, but it does not impact the core architecture of the system. Second, the technical concepts involved are relatively straightforward, focusing on basic Python programming, understanding of token-based parsing in the vsg tool, and familiarity with documentation updates using reStructuredText. No complex algorithms, design patterns, or domain-specific knowledge beyond VHDL syntax rules are required. Third, the amount of code change is moderate, involving the creation of a new rule file, deprecation of old rules, and extensive documentation updates, but these are repetitive and follow a clear pattern. Finally, there are no explicit edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as the task is primarily about enforcing a stylistic rule rather than handling runtime errors or complex logic. Overall, while the task requires attention to detail and understanding of the rule system, it is not inherently complex, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug][compiler-v2] Enum constructors without arguments don't work in expressions\nThe following does not work as expected:\r\n\r\n```\r\n  enum MessageHolder has key, drop {\r\n        Empty,\r\n        Message{\r\n           message: string::String,\r\n        }\r\n    }\r\n\r\n   fun expect_value(): MessageHolder {\r\n       MessageHolder::Empty // <--- should work but doesn't, and gives misleading error message\r\n       // MessageHolder::Empty{} // <-- does work\r\n   }\r\n```\r\n\r\n\n", "patch": "diff --git a/third_party/move/move-model/src/builder/exp_builder.rs b/third_party/move/move-model/src/builder/exp_builder.rs\nindex 8a7ac190db9ff..7f326237adcc5 100644\n--- a/third_party/move/move-model/src/builder/exp_builder.rs\n+++ b/third_party/move/move-model/src/builder/exp_builder.rs\n@@ -31,7 +31,7 @@ use codespan_reporting::diagnostic::Severity;\n use itertools::Itertools;\n use move_binary_format::file_format::{self, Ability, AbilitySet};\n use move_compiler::{\n-    expansion::ast::{self as EA, wild_card},\n+    expansion::ast::{self as EA},\n     hlir::ast as HA,\n     naming::ast as NA,\n     parser::ast::{self as PA, CallKind, Field},\n@@ -1500,8 +1500,8 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                     )\n                 }\n             },\n-            EA::Exp_::Pack(maccess, generics, fields) => {\n-                if let Some(exp) = self.translate_pack(\n+            EA::Exp_::Pack(maccess, generics, fields) => self\n+                .translate_pack(\n                     &loc,\n                     maccess,\n                     generics,\n@@ -1509,12 +1509,8 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                     expected_type,\n                     context,\n                     false,\n-                ) {\n-                    exp\n-                } else {\n-                    self.new_error_exp()\n-                }\n-            },\n+                )\n+                .unwrap_or_else(|| self.new_error_exp()),\n             EA::Exp_::IfElse(cond, then, else_) => {\n                 let try_freeze_if_else = |et: &mut ExpTranslator,\n                                           expected_ty: &Type,\n@@ -2422,6 +2418,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                 }\n             },\n             EA::LValue_::PositionalUnpack(maccess, generics, args) => {\n+                let expected_type = &self.subs.specialize(expected_type);\n                 let Some((struct_id, variant)) = self.translate_constructor_name(\n                     expected_type,\n                     expected_order,\n@@ -2493,7 +2490,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                                 );\n                                 let field_name = Field(field_name);\n                                 fields\n-                                    .add(field_name, (field_offset, wild_card(*arg_loc)))\n+                                    .add(field_name, (field_offset, EA::wild_card(*arg_loc)))\n                                     .expect(\"duplicate keys\");\n                                 remaining -= 1;\n                                 field_offset += 1;\n@@ -2529,31 +2526,27 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n     ) -> Option<Pattern> {\n         // Translate constructor name\n         let expected_type = self.subs.specialize(expected_type);\n-        let Some((\n+        let (\n             QualifiedInstId {\n                 module_id,\n                 id,\n                 inst,\n             },\n             variant,\n-        )) = self.translate_constructor_name(\n+        ) = self.translate_constructor_name(\n             &expected_type,\n             expected_order,\n             context,\n             loc,\n             maccess,\n             generics,\n-        )\n-        else {\n-            // error reported by `translate_constructor_name`\n-            return None;\n-        };\n+        )?;\n+        let struct_name_loc = self.to_loc(&maccess.loc);\n         let struct_name = self\n             .parent\n             .parent\n             .get_struct_name(module_id.qualified(id))\n             .clone();\n-        let struct_name_loc = self.to_loc(&maccess.loc);\n         let ref_expected = expected_type.try_reference_kind();\n \n         // Process argument list\n@@ -2575,7 +2568,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                         } else {\n                             field_ty.clone()\n                         };\n-                        let lvalue = wild_card(dotdot.loc);\n+                        let lvalue = EA::wild_card(dotdot.loc);\n                         let translated = self.translate_lvalue(\n                             &lvalue,\n                             &expected_field_ty,\n@@ -2654,10 +2647,45 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         generics: &Option<Vec<EA::Type>>,\n     ) -> Option<(QualifiedInstId<StructId>, Option<Symbol>)> {\n         let expected_type = self.subs.specialize(expected_type).drop_reference();\n+\n+        let (struct_name, variant, struct_entry) =\n+            self.resolve_struct_access(&expected_type, maccess, true)?;\n+\n+        // Resolve type instantiation.\n+        let name_loc = self.to_loc(&maccess.loc);\n+        let instantiation = self.make_instantiation_or_report(\n+            &name_loc,\n+            true,\n+            struct_name.symbol,\n+            &struct_entry.type_params,\n+            generics,\n+        )?;\n+\n+        // Verify type derived from reference with expected type.\n+        let struct_id = struct_entry\n+            .module_id\n+            .qualified_inst(struct_entry.struct_id, instantiation);\n+        let ty = struct_id.to_type();\n+        let ty = self.check_type_with_order(expected_order, loc, &ty, &expected_type, context);\n+        // Convert the unified type back to struct id\n+        let mut struct_id = struct_id;\n+        if let Type::Struct(_, _, types) = ty {\n+            struct_id.inst = types;\n+        }\n+\n+        Some((struct_id, variant))\n+    }\n+\n+    fn resolve_struct_access(\n+        &mut self,\n+        expected_type: &Type,\n+        maccess: &EA::ModuleAccess,\n+        report_error: bool,\n+    ) -> Option<(QualifiedSymbol, Option<Symbol>, StructEntry)> {\n         // Determine whether expected type is known to have variants (at this\n         // point during inference). If so, they are used for name resolution.\n         let variant_struct_info = if let Type::Struct(mid, sid, _) = expected_type {\n-            let entry = self.parent.parent.lookup_struct_entry(mid.qualified(sid));\n+            let entry = self.parent.parent.lookup_struct_entry(mid.qualified(*sid));\n             if let StructLayout::Variants(variants) = &entry.layout {\n                 Some((\n                     entry,\n@@ -2687,41 +2715,22 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                 let (struct_name, variant) =\n                     self.parent.module_access_to_qualified_with_variant(maccess);\n                 let struct_name_loc = self.to_loc(&maccess.loc);\n-                let struct_entry =\n-                    self.get_struct_report_undeclared(&struct_name, &struct_name_loc)?;\n+                let struct_entry = if report_error {\n+                    self.get_struct_report_undeclared(&struct_name, &struct_name_loc)?\n+                } else {\n+                    self.parent.parent.struct_table.get(&struct_name).cloned()?\n+                };\n+\n                 (struct_name, variant, struct_entry)\n             },\n         };\n-\n-        // Resolve type instantiation.\n-        let instantiation = self.make_instantiation_or_report(\n-            loc,\n-            true,\n-            struct_name.symbol,\n-            &struct_entry.type_params,\n-            generics,\n-        )?;\n-\n-        // If this is a struct variant, check whether it exists.\n-        if let Some(v) = variant {\n-            if !self.check_variant_declared(&struct_name, &struct_entry, &struct_name_loc, v) {\n+        if let Some(variant) = variant.filter(|_| report_error) {\n+            if !self.check_variant_declared(&struct_name, &struct_entry, &struct_name_loc, variant)\n+            {\n                 return None;\n             }\n         }\n-\n-        // Verify type derived from reference with expected type.\n-        let struct_id = struct_entry\n-            .module_id\n-            .qualified_inst(struct_entry.struct_id, instantiation);\n-        let ty = struct_id.to_type();\n-        let ty = self.check_type_with_order(expected_order, loc, &ty, &expected_type, context);\n-        // Convert the unified type back to struct id\n-        let mut struct_id = struct_id;\n-        if let Type::Struct(_, _, types) = ty {\n-            struct_id.inst = types;\n-        }\n-\n-        Some((struct_id, variant))\n+        Some((struct_name, variant, struct_entry))\n     }\n \n     fn new_error_pat(&mut self, loc: &Loc) -> Pattern {\n@@ -2939,10 +2948,13 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         result\n     }\n \n-    /// Checks whether the given name can be resolve to a struct.\n-    fn can_resolve_to_struct(&self, maccess: &EA::ModuleAccess) -> bool {\n-        let (struct_name, _variant) = self.parent.module_access_to_qualified_with_variant(maccess);\n-        self.parent.parent.struct_table.contains_key(&struct_name)\n+    /// Checks whether the given name can be resolved to a struct or struct variant\n+    fn can_resolve_to_struct(&mut self, expected_type: &Type, maccess: &EA::ModuleAccess) -> bool {\n+        (maccess.value.is_valid_struct_constant_or_schema_name()\n+            || ModuleBuilder::is_variant(maccess))\n+            && self\n+                .resolve_struct_access(expected_type, maccess, false)\n+                .is_some()\n     }\n \n     fn translate_fun_call_special_cases(\n@@ -2964,10 +2976,8 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         }\n \n         // handles call of struct/variant with positional fields\n-        if maccess.value.is_valid_struct_constant_or_schema_name()\n-            && self.can_resolve_to_struct(maccess)\n-            || ModuleBuilder::is_variant(maccess)\n-        {\n+        let expected_type = &self.subs.specialize(expected_type);\n+        if self.can_resolve_to_struct(expected_type, maccess) {\n             self.check_language_version(loc, \"positional fields\", LanguageVersion::V2_0);\n             // translates StructName(e0, e1, ...) to pack<StructName> { 0: e0, 1: e1, ... }\n             let fields: EA::Fields<_> =\n@@ -3282,6 +3292,14 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         expected_type: &Type,\n         context: &ErrorMessageContext,\n     ) -> ExpData {\n+        let expected_type = &self.subs.specialize(expected_type);\n+        // Try to resolve as argument-less construction of struct variant\n+        if self.can_resolve_to_struct(expected_type, maccess) {\n+            return self\n+                .translate_pack(loc, maccess, type_args, None, expected_type, context, false)\n+                .unwrap_or_else(|| self.new_error_exp());\n+        }\n+\n         let global_var_sym = match &maccess.value {\n             EA::ModuleAccess_::ModuleAccess(..) => self.parent.module_access_to_qualified(maccess),\n             EA::ModuleAccess_::Name(name) => {\n@@ -3874,16 +3892,17 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         context: &ErrorMessageContext,\n     ) -> ExpData {\n         let (exp_ty, exp) = self.translate_exp_free(exp);\n+        let exp_ty = self.subs.specialize(&exp_ty);\n         let mut variants = vec![];\n         let mut struct_id = None;\n         for ty in tys {\n-            let loc = self.to_loc(&ty.loc);\n+            let ty_loc = self.to_loc(&ty.loc);\n             if let EA::Type_::Apply(maccess, generics) = &ty.value {\n                 if let Some((inferred_struct_id, variant)) = self.translate_constructor_name(\n                     &exp_ty,\n                     WideningOrder::LeftToRight,\n                     context,\n-                    &loc,\n+                    &ty_loc,\n                     maccess,\n                     &Some(generics.clone()),\n                 ) {\n@@ -3894,7 +3913,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                         variants.push(variant);\n                     } else {\n                         self.error(\n-                            &loc,\n+                            &ty_loc,\n                             &format!(\n                                 \"expected variant of enum type but found type `{}`\",\n                                 self.env().display(&inferred_struct_id)\n@@ -4518,25 +4537,21 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         expected_positional_constructor: bool,\n     ) -> Option<ExpData> {\n         // Resolve reference to struct\n-        let (struct_name, variant) = self.parent.module_access_to_qualified_with_variant(maccess);\n-        let struct_name_loc = self.to_loc(&maccess.loc);\n-        let struct_entry = self.get_struct_report_undeclared(&struct_name, &struct_name_loc)?;\n-\n-        // Resolve type instantiation\n-        let instantiation = self.make_instantiation_or_report(\n-            &self.to_loc(&maccess.loc),\n-            true,\n-            struct_name.symbol,\n-            &struct_entry.type_params,\n+        // Translate constructor name\n+        let expected_type = self.subs.specialize(expected_type);\n+        let (struct_inst_id, variant) = self.translate_constructor_name(\n+            &expected_type,\n+            WideningOrder::LeftToRight,\n+            context,\n+            loc,\n+            maccess,\n             generics,\n         )?;\n-\n-        // If this is a struct variant, check whether it exists.\n-        if let Some(v) = variant {\n-            if !self.check_variant_declared(&struct_name, &struct_entry, &struct_name_loc, v) {\n-                return None;\n-            }\n-        }\n+        let struct_name = self\n+            .parent\n+            .parent\n+            .get_struct_name(struct_inst_id.to_qualified_id())\n+            .clone();\n \n         // Process argument list.\n         // given pack<S>{ f_p(1): e_1, ... }, where p is a permutation of the fields\n@@ -4549,10 +4564,11 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n         // is equivalent.\n         let mut bindings = BTreeMap::new();\n         let mut args = BTreeMap::new();\n+        let struct_name_loc = self.to_loc(&maccess.loc);\n         let (field_decls, is_positional_constructor) =\n             self.get_field_decls_for_pack_unpack(&struct_name, &struct_name_loc, variant)?;\n         let field_decls = field_decls.clone();\n-        if is_positional_constructor != expected_positional_constructor {\n+        if fields.is_some() && is_positional_constructor != expected_positional_constructor {\n             let struct_name_display = struct_name.display(self.env());\n             let variant_name_display = variant\n                 .map(|v| format!(\"::{}\", v.display(self.symbol_pool())))\n@@ -4589,7 +4605,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n             let in_order_fields = self.in_order_fields(&field_decls, fields);\n             for (_, name, (exp_idx, field_exp)) in fields.iter() {\n                 let (def_idx, field_name, translated_field_exp) =\n-                    self.translate_exp_field(&field_decls, name, &instantiation, field_exp);\n+                    self.translate_exp_field(&field_decls, name, &struct_inst_id.inst, field_exp);\n                 if in_order_fields.contains(&def_idx) {\n                     args.insert(def_idx, translated_field_exp);\n                 } else {\n@@ -4614,9 +4630,6 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                 ),\n             )\n         }\n-        let struct_id = struct_entry\n-            .module_id\n-            .qualified_inst(struct_entry.struct_id, instantiation);\n         let bindings = bindings\n             .into_iter()\n             .sorted_by_key(|(i, _)| *i)\n@@ -4628,8 +4641,8 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n             .map(|(_, value)| value)\n             .collect_vec();\n \n-        let struct_ty = struct_id.to_type();\n-        let struct_ty = self.check_type(loc, &struct_ty, expected_type, context);\n+        let struct_ty = struct_inst_id.to_type();\n+        let struct_ty = self.check_type(loc, &struct_ty, &expected_type, context);\n         let mut field_args = args.into_iter().map(|e| e.into_exp()).collect_vec();\n         if variant.is_none() && field_args.is_empty() {\n             // The move compiler inserts a dummy field with the value of false\n@@ -4642,10 +4655,10 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n             field_args.push(ExpData::Value(id, Value::Bool(false)).into_exp());\n         }\n         let id = self.new_node_id_with_type_loc(&struct_ty, loc);\n-        self.set_node_instantiation(id, struct_id.inst);\n+        self.set_node_instantiation(id, struct_inst_id.inst);\n         let body = ExpData::Call(\n             id,\n-            Operation::Pack(struct_id.module_id, struct_id.id, variant),\n+            Operation::Pack(struct_inst_id.module_id, struct_inst_id.id, variant),\n             field_args,\n         );\n         // Fold the bindings and the body into result exp\n@@ -4730,7 +4743,7 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                     self.error(\n                         struct_name_loc,\n                         &format!(\n-                            \"struct `{}` has no variant named `{}`\",\n+                            \"enum `{}` has no variant named `{}`\",\n                             struct_name.display(self.env()),\n                             name.display(self.symbol_pool())\n                         ),\n@@ -4738,8 +4751,34 @@ impl<'env, 'translator, 'module_translator> ExpTranslator<'env, 'translator, 'mo\n                     None\n                 }\n             },\n-            _ => {\n-                self.bug(struct_name_loc, \"inconsistent struct definition\");\n+            (StructLayout::Singleton(..), Some(_)) => {\n+                self.error(\n+                    struct_name_loc,\n+                    &format!(\n+                        \"struct `{}` does not have variants\",\n+                        struct_name.display(self.env())\n+                    ),\n+                );\n+                None\n+            },\n+            (StructLayout::Variants(..), None) => {\n+                self.error(\n+                    struct_name_loc,\n+                    &format!(\n+                        \"enum `{}` must be used with one of its variants\",\n+                        struct_name.display(self.env())\n+                    ),\n+                );\n+                None\n+            },\n+            (StructLayout::None, _) => {\n+                self.error(\n+                    struct_name_loc,\n+                    &format!(\n+                        \"native struct `{}` has no fields\",\n+                        struct_name.display(self.env())\n+                    ),\n+                );\n                 None\n             },\n         }\n", "instance_id": "aptos-labs__aptos-core-14411", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: enum constructors without arguments fail to work in expressions and produce misleading error messages. It provides a specific example of the problematic code (`MessageHolder::Empty` not working as expected) and contrasts it with a working alternative (`MessageHolder::Empty{}`). However, it lacks critical details such as the expected behavior, the exact nature of the misleading error message, and any constraints or edge cases that might be relevant. Additionally, there is no mention of the broader context or potential impact on other parts of the system. While the goal is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, as evidenced by the extensive diff in `exp_builder.rs`, which spans multiple functions and logic paths related to expression translation and struct/enum handling in a compiler (likely for the Move language). The changes involve deep modifications to the way enum constructors are resolved and instantiated, requiring a thorough understanding of the compiler's internal representation of types, structs, and variants.\n\nSecond, the number of technical concepts involved is high. Solving this requires knowledge of Rust (given the codebase), compiler design (expression translation, type specialization, and instantiation), and domain-specific knowledge of the Move language's enum and struct semantics. The code changes touch on complex logic such as type inference, pattern matching, and error reporting, which are non-trivial to modify correctly.\n\nThird, the potential for edge cases and error handling is notable. The problem statement does not explicitly mention edge cases, but the code changes suggest handling of various scenarios like argument-less constructors, variant resolution, and positional field handling. The modifications also improve error messages (e.g., distinguishing between structs and enums in error reporting), indicating a need to carefully manage error conditions.\n\nFinally, while the changes are confined to a single file, they impact a critical part of the compiler's architecture (expression building and type resolution), which could have downstream effects on other components. This requires a deep understanding of the codebase to ensure no regressions are introduced. Given these factors, a difficulty score of 0.75 reflects the challenging nature of the problem, requiring advanced skills and careful consideration, though it does not reach the extreme complexity of system-level redesign or highly specialized domain problems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[compiler-v2] Generic TypeInfo verification broken\nConsider code like:\r\n\r\n```\r\nmodule app::m {\r\n    use aptos_stdlib::type_info;\r\n    \r\n    fun generic_type_info_verification_target<A>(): type_info::TypeInfo {\r\n       type_info::type_of<A>()\r\n    }\r\n    spec generic_type_info_verification_target {\r\n        ensures result == generic_type_info_verification_target<A>();\r\n    }\r\n}\r\n```\r\n\r\nThis yields in the following Boogie compilation error:\r\n\r\n```\r\nMove prover returns: [internal] boogie exited with compilation errors:\r\ntype_reflection.bpl(5059,26): Error: cannot refer to a global variable in this context: #0_info\r\ntype_reflection.bpl(5059,38): Error: cannot refer to a global variable in this context: #0_info\r\ntype_reflection.bpl(5059,50): Error: cannot refer to a global variable in this context: #0_info\r\n```\r\n\r\nThis seems to be a bug in the new specification function inferrer shared between compiler v1 and v2. The native function `type_info::type_of` cannot be called from a specification function as it refers to type reflection state `#0_info`. \n", "patch": "diff --git a/crates/aptos/CHANGELOG.md b/crates/aptos/CHANGELOG.md\nindex 9dd9d07837866..9395dabd2382c 100644\n--- a/crates/aptos/CHANGELOG.md\n+++ b/crates/aptos/CHANGELOG.md\n@@ -3,6 +3,7 @@\n All notable changes to the Aptos CLI will be captured in this file. This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html) and the format set out by [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n \n ## Unreleased\n+- Fixes a bug in the Move Prover leading to internal error in generated boogie (error 'global `#0_info` cannot be accessed')\n \n ## [3.5.1] - 2024/07/21\n - Upgraded indexer processors for localnet from 5244b84fa5ed872e5280dc8df032d744d62ad29d to fa1ce4947f4c2be57529f1c9732529e05a06cb7f. Upgraded Hasura metadata accordingly.\ndiff --git a/third_party/move/move-compiler-v2/src/env_pipeline/spec_rewriter.rs b/third_party/move/move-compiler-v2/src/env_pipeline/spec_rewriter.rs\nindex 5fd1f908654b8..1077b7b3af83e 100644\n--- a/third_party/move/move-compiler-v2/src/env_pipeline/spec_rewriter.rs\n+++ b/third_party/move/move-compiler-v2/src/env_pipeline/spec_rewriter.rs\n@@ -255,6 +255,7 @@ fn derive_spec_fun(env: &mut GlobalEnv, fun_id: QualifiedId<FunId>) -> Qualified\n         body,\n         callees: BTreeSet::new(),\n         is_recursive: RefCell::new(None),\n+        insts_using_generic_type_reflection: Default::default(),\n     };\n     env.add_spec_function_def(fun_id.module_id, decl)\n }\ndiff --git a/third_party/move/move-model/src/ast.rs b/third_party/move/move-model/src/ast.rs\nindex e06543c6fd668..9c488890f5322 100644\n--- a/third_party/move/move-model/src/ast.rs\n+++ b/third_party/move/move-model/src/ast.rs\n@@ -60,6 +60,8 @@ pub struct SpecFunDecl {\n     pub body: Option<Exp>,\n     pub callees: BTreeSet<QualifiedInstId<SpecFunId>>,\n     pub is_recursive: RefCell<Option<bool>>,\n+    /// The instantiations for which this function is known to use generic type reflection.\n+    pub insts_using_generic_type_reflection: RefCell<BTreeMap<Vec<Type>, bool>>,\n }\n \n // =================================================================================================\ndiff --git a/third_party/move/move-model/src/builder/module_builder.rs b/third_party/move/move-model/src/builder/module_builder.rs\nindex 9f381cc5337b2..6c673e6ea4a5a 100644\n--- a/third_party/move/move-model/src/builder/module_builder.rs\n+++ b/third_party/move/move-model/src/builder/module_builder.rs\n@@ -763,6 +763,7 @@ impl<'env, 'translator> ModuleBuilder<'env, 'translator> {\n             body: None,\n             callees: Default::default(),\n             is_recursive: Default::default(),\n+            insts_using_generic_type_reflection: Default::default(),\n         };\n         self.spec_funs.push(fun_decl);\n     }\ndiff --git a/third_party/move/move-model/src/model.rs b/third_party/move/move-model/src/model.rs\nindex 9d563f8a1fdda..c8e63797a45da 100644\n--- a/third_party/move/move-model/src/model.rs\n+++ b/third_party/move/move-model/src/model.rs\n@@ -1372,6 +1372,81 @@ impl GlobalEnv {\n         }\n     }\n \n+    /// Determines whether the given spec fun uses type reflection over type parameters.\n+    pub fn spec_fun_uses_generic_type_reflection(\n+        &self,\n+        fun_id: &QualifiedInstId<SpecFunId>,\n+    ) -> bool {\n+        fn uses_generic_type_reflection(\n+            env: &GlobalEnv,\n+            visited: &mut BTreeSet<QualifiedInstId<SpecFunId>>,\n+            fun: &QualifiedInstId<SpecFunId>,\n+        ) -> bool {\n+            if !visited.insert(fun.clone()) {\n+                return false;\n+            }\n+            let module = env.get_module(fun.module_id);\n+            let decl = module.get_spec_fun(fun.id);\n+            if let Some(def) = &decl.body {\n+                // Check called spec funs\n+                def.called_spec_funs(env).into_iter().any(|qid| {\n+                    let qid_inst = qid.instantiate(&fun.inst);\n+                    is_generic_type_reflection(env, &qid_inst)\n+                        || uses_generic_type_reflection(env, visited, &qid_inst)\n+                })\n+            } else {\n+                false\n+            }\n+        }\n+        fn is_generic_type_reflection(\n+            env: &GlobalEnv,\n+            fun_id: &QualifiedInstId<SpecFunId>,\n+        ) -> bool {\n+            static REFLECTION_FUNS: Lazy<BTreeSet<String>> = Lazy::new(|| {\n+                [\n+                    well_known::TYPE_INFO_SPEC.to_owned(),\n+                    well_known::TYPE_NAME_SPEC.to_owned(),\n+                    well_known::TYPE_NAME_GET_SPEC.to_owned(),\n+                ]\n+                .into_iter()\n+                .collect()\n+            });\n+            // The function must be at `extlib` or `stdlib`.\n+            let module = env.get_module(fun_id.module_id);\n+            let addr = module.get_name().addr();\n+            if addr == &env.get_extlib_address() || addr == &env.get_stdlib_address() {\n+                let fun = module.get_spec_fun(fun_id.id);\n+                let name = format!(\n+                    \"{}::{}\",\n+                    module.get_name().name().display(module.symbol_pool()),\n+                    fun.name.display(module.symbol_pool())\n+                );\n+                REFLECTION_FUNS.contains(&name)\n+                    && fun_id.inst.iter().any(|ty| ty.is_type_parameter())\n+            } else {\n+                false\n+            }\n+        }\n+        let module = self.get_module(fun_id.module_id);\n+        let decl = module.get_spec_fun(fun_id.id);\n+        let uses = decl\n+            .insts_using_generic_type_reflection\n+            .borrow()\n+            .get(&fun_id.inst)\n+            .cloned();\n+        if let Some(b) = uses {\n+            b\n+        } else {\n+            let b = uses_generic_type_reflection(self, &mut BTreeSet::new(), fun_id);\n+            module\n+                .get_spec_fun(fun_id.id)\n+                .insts_using_generic_type_reflection\n+                .borrow_mut()\n+                .insert(fun_id.inst.clone(), b);\n+            b\n+        }\n+    }\n+\n     /// Returns true if the type represents the well-known event handle type.\n     pub fn is_wellknown_event_handle_type(&self, ty: &Type) -> bool {\n         if let Type::Struct(mid, sid, _) = ty {\n@@ -4771,6 +4846,15 @@ impl GetNameString for QualifiedId<FunId> {\n     }\n }\n \n+impl GetNameString for QualifiedId<SpecFunId> {\n+    fn get_name_for_display(&self, env: &GlobalEnv) -> String {\n+        env.get_spec_fun(*self)\n+            .name\n+            .display(env.symbol_pool())\n+            .to_string()\n+    }\n+}\n+\n impl<'a, Id: Clone> fmt::Display for EnvDisplay<'a, QualifiedId<Id>>\n where\n     QualifiedId<Id>: GetNameString,\ndiff --git a/third_party/move/move-prover/boogie-backend/src/spec_translator.rs b/third_party/move/move-prover/boogie-backend/src/spec_translator.rs\nindex fc78a5f833fc4..9d84ff64e72e0 100644\n--- a/third_party/move/move-prover/boogie-backend/src/spec_translator.rs\n+++ b/third_party/move/move-prover/boogie-backend/src/spec_translator.rs\n@@ -274,9 +274,11 @@ impl<'env> SpecTranslator<'env> {\n             self.error(&fun.loc, \"function or tuple result type not yet supported\");\n             return;\n         }\n-        let recursive = self\n+        let qid = module_env.get_id().qualified(id);\n+        let recursive = self.env.is_spec_fun_recursive(qid);\n+        let type_reflection = self\n             .env\n-            .is_spec_fun_recursive(module_env.get_id().qualified(id));\n+            .spec_fun_uses_generic_type_reflection(&qid.instantiate(self.type_inst.clone()));\n         emitln!(\n             self.writer,\n             \"// {}spec fun {}\",\n@@ -307,6 +309,18 @@ impl<'env> SpecTranslator<'env> {\n                 boogie_type\n             }\n         };\n+        let type_info_params = if type_reflection {\n+            (0..fun.type_params.len())\n+                .map(|i| {\n+                    format!(\n+                        \"{}_info: $TypeParamInfo\",\n+                        boogie_type(self.env, &Type::TypeParameter(i as u16))\n+                    )\n+                })\n+                .collect_vec()\n+        } else {\n+            vec![]\n+        };\n         let result_type = ty_str_fn(bv_flag_result)(self.env, &self.inst(&fun.result_type));\n         // it is possible that the spec fun may refer to the same memory after monomorphization,\n         // (e.g., one via concrete type and the other via type parameter being instantiated).\n@@ -352,7 +366,10 @@ impl<'env> SpecTranslator<'env> {\n             });\n         self.writer.set_location(&fun.loc);\n         let boogie_name = boogie_spec_fun_name(module_env, id, &self.type_inst, bv_flag_result);\n-        let param_list = mem_params.chain(params).join(\", \");\n+        let param_list = type_info_params\n+            .into_iter()\n+            .chain(mem_params.chain(params))\n+            .join(\", \");\n         let attrs = if fun.uninterpreted || recursive {\n             \"\"\n         } else {\n@@ -1031,14 +1048,69 @@ impl<'env> SpecTranslator<'env> {\n         let inst = &self.get_node_instantiation(node_id);\n         let module_env = &self.env.get_module(module_id);\n         let fun_decl = module_env.get_spec_fun(fun_id);\n+        if self.try_translate_spec_fun_reflection_call(module_env, fun_decl, inst) {\n+            return;\n+        }\n+\n+        // regular path\n         let global_state = &self\n             .env\n             .get_extension::<GlobalNumberOperationState>()\n             .expect(\"global number operation state\");\n+        let is_vector_table_module = module_env.is_std_vector() || module_env.is_table();\n+        let bv_flag = if is_vector_table_module && !args.is_empty() {\n+            global_state.get_node_num_oper(args[0].node_id()) == Bitwise\n+        } else {\n+            global_state.get_node_num_oper(node_id) == Bitwise\n+        };\n+        let name = boogie_spec_fun_name(module_env, fun_id, inst, bv_flag);\n+        emit!(self.writer, \"{}(\", name);\n+        let mut first = true;\n+        let mut maybe_comma = || {\n+            if first {\n+                first = false;\n+            } else {\n+                emit!(self.writer, \", \");\n+            }\n+        };\n+        // Start with type info parameters\n+        if self\n+            .env\n+            .spec_fun_uses_generic_type_reflection(&module_id.qualified_inst(fun_id, inst.clone()))\n+        {\n+            for i in 0..fun_decl.type_params.len() {\n+                maybe_comma();\n+                emit!(\n+                    self.writer,\n+                    \"{}_info\",\n+                    boogie_type(self.env, &Type::TypeParameter(i as u16))\n+                )\n+            }\n+        }\n+        // Add memory parameters.\n+        let label_at = |i| memory_labels.as_ref().map(|labels| labels[i]);\n+        let mut i = 0;\n+        for memory in &fun_decl.used_memory {\n+            let memory = &memory.to_owned().instantiate(inst);\n+            maybe_comma();\n+            let memory = boogie_resource_memory_name(self.env, memory, &label_at(i));\n+            emit!(self.writer, &memory);\n+            i = usize::saturating_add(i, 1);\n+        }\n+        // Finally add argument expressions\n+        for exp in args {\n+            maybe_comma();\n+            self.translate_exp(exp);\n+        }\n+        emit!(self.writer, \")\");\n+    }\n \n-        // special casing for type reflection\n-        let mut processed = false;\n-\n+    fn try_translate_spec_fun_reflection_call(\n+        &self,\n+        module_env: &ModuleEnv,\n+        fun_decl: &SpecFunDecl,\n+        inst: &[Type],\n+    ) -> bool {\n         // TODO(mengxu): change it to a better address name instead of extlib\n         if self.env.get_extlib_address() == *module_env.get_name().addr() {\n             let qualified_name = format!(\n@@ -1053,7 +1125,7 @@ impl<'env> SpecTranslator<'env> {\n                     \"{}\",\n                     boogie_reflection_type_name(self.env, &inst[0], false)\n                 );\n-                processed = true;\n+                true\n             } else if qualified_name == TYPE_INFO_SPEC {\n                 assert_eq!(inst.len(), 1);\n                 // TODO(mengxu): by ignoring the first return value of this function, we are\n@@ -1061,7 +1133,7 @@ impl<'env> SpecTranslator<'env> {\n                 // invoking `type_info` on a primitive type like: `type_info<bool>`.\n                 let (_, info) = boogie_reflection_type_info(self.env, &inst[0]);\n                 emit!(self.writer, \"{}\", info);\n-                processed = true;\n+                true\n             } else if qualified_name == TYPE_SPEC_IS_STRUCT {\n                 assert_eq!(inst.len(), 1);\n                 emit!(\n@@ -1069,11 +1141,11 @@ impl<'env> SpecTranslator<'env> {\n                     \"{}\",\n                     boogie_reflection_type_is_struct(self.env, &inst[0])\n                 );\n-                processed = true;\n+                true\n+            } else {\n+                false\n             }\n-        }\n-\n-        if self.env.get_stdlib_address() == *module_env.get_name().addr() {\n+        } else if self.env.get_stdlib_address() == *module_env.get_name().addr() {\n             let qualified_name = format!(\n                 \"{}::{}\",\n                 module_env.get_name().name().display(self.env.symbol_pool()),\n@@ -1086,42 +1158,12 @@ impl<'env> SpecTranslator<'env> {\n                     \"{}\",\n                     boogie_reflection_type_name(self.env, &inst[0], true)\n                 );\n-                processed = true;\n-            }\n-        }\n-\n-        let is_vector_table_module = module_env.is_std_vector() || module_env.is_table();\n-        // regular path\n-        if !processed {\n-            let bv_flag = if is_vector_table_module && !args.is_empty() {\n-                global_state.get_node_num_oper(args[0].node_id()) == Bitwise\n+                true\n             } else {\n-                global_state.get_node_num_oper(node_id) == Bitwise\n-            };\n-            let name = boogie_spec_fun_name(module_env, fun_id, inst, bv_flag);\n-            emit!(self.writer, \"{}(\", name);\n-            let mut first = true;\n-            let mut maybe_comma = || {\n-                if first {\n-                    first = false;\n-                } else {\n-                    emit!(self.writer, \", \");\n-                }\n-            };\n-            let label_at = |i| memory_labels.as_ref().map(|labels| labels[i]);\n-            let mut i = 0;\n-            for memory in &fun_decl.used_memory {\n-                let memory = &memory.to_owned().instantiate(inst);\n-                maybe_comma();\n-                let memory = boogie_resource_memory_name(self.env, memory, &label_at(i));\n-                emit!(self.writer, &memory);\n-                i = usize::saturating_add(i, 1);\n+                false\n             }\n-            for exp in args {\n-                maybe_comma();\n-                self.translate_exp(exp);\n-            }\n-            emit!(self.writer, \")\");\n+        } else {\n+            false\n         }\n     }\n \n", "instance_id": "aptos-labs__aptos-core-14208", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a bug in the Move Prover related to generic type information verification in the context of the `type_info::type_of` function, which causes a Boogie compilation error due to improper access to a global variable (`#0_info`) in specification functions. It provides a specific code example that triggers the error and identifies the root cause as a limitation in the specification function inferrer shared between compiler v1 and v2. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define the expected behavior or constraints for the fix (e.g., whether the solution should prevent such calls entirely or handle them differently). Additionally, edge cases or alternative scenarios where this issue might manifest are not discussed, which could leave room for interpretation during implementation. Overall, the problem is valid and mostly clear, but it lacks comprehensive details on the desired outcome and potential complexities.", "difficulty_explanation": "The difficulty of this problem is rated as hard (0.75) due to several factors. First, the scope and depth of code changes are significant, as they span multiple files and modules within the Move compiler and prover system (`spec_rewriter.rs`, `ast.rs`, `module_builder.rs`, `model.rs`, `spec_translator.rs`). The changes involve not just isolated fixes but modifications to core components like type reflection handling and specification function translation to Boogie, which suggests a need to understand intricate interactions within the codebase. Second, the number of technical concepts involved is high, requiring deep knowledge of the Move language, its type system, generic type reflection, the Boogie intermediate representation, and the internals of the Move Prover. Specific concepts include handling type parameters, detecting recursive specification functions, and managing memory and instantiation contexts during translation. Third, while the problem statement does not explicitly mention edge cases, the code changes introduce logic to handle specific instantiations of generic type reflection, indicating a need to consider various type parameter scenarios and potential error conditions (e.g., ensuring correct behavior for different module addresses like `stdlib` and `extlib`). Finally, the impact on the system's architecture is notable, as the fix modifies how specification functions are processed and translated, which could affect other parts of the compiler or prover pipeline. This combination of factors\u2014deep technical requirements, cross-module changes, and potential for subtle bugs\u2014places this problem in the hard category, though not at the extreme end of very hard, as it does not appear to involve system-level redesign or highly domain-specific challenges beyond the Move ecosystem.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update to Pendulum 3\n**Describe the bug**\r\nI'm trying to use the package with Python 3.12 but the pendulum dependencie doesn't allow it. \r\nIs there any reason why it is locked at pendulum>=2.1,<2.2? I think it could be updated without any major change.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Use Python 3.12\r\n2. pip install masonite-orm\r\n3. See error\r\n\r\n**Expected behavior**\r\nBe able to use Masonite with Python 3.12\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Mac OS\r\n - Version 14.3.1\r\n - \r\n**What database are you using?**\r\n - Type: MySQL\r\n - Version 8\r\n - Masonite ORM v2.20.0\r\n\r\n**Additional context**\r\nAny other steps you are doing or any other related information that will help us debug the problem please put here.\r\n\n", "patch": "diff --git a/requirements.txt b/requirements.txt\nindex a2188f67..aba4fd5b 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,6 +1,6 @@\n inflection==0.3.1\n psycopg2-binary\n pyodbc\n-pendulum>=2.1,<2.2\n+pendulum>=2.1,<3.1\n cleo>=0.8.0,<0.9\n python-dotenv==0.14.0\n\\ No newline at end of file\ndiff --git a/setup.py b/setup.py\nindex 8c55fd2b..f1334781 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -30,7 +30,7 @@\n     # https://packaging.python.org/en/latest/requirements.html\n     install_requires=[\n         \"inflection>=0.3,<0.6\",\n-        \"pendulum>=2.1,<2.2\",\n+        \"ppendulum>=2.1,<3.1\",\n         \"faker>=4.1.0,<14.0\",\n         \"cleo>=0.8.0,<0.9\",\n     ],\n", "instance_id": "MasoniteFramework__orm-898", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the package (Masonite ORM) is incompatible with Python 3.12 due to a version constraint on the `pendulum` dependency. The goal is to update the dependency to allow compatibility with Python 3.12, and the steps to reproduce the issue are provided. However, there are minor ambiguities and missing details. For instance, the problem does not specify whether updating `pendulum` to a newer version (e.g., <3.1) could introduce breaking changes or compatibility issues with other parts of the codebase. Additionally, there are no examples of the exact error message encountered during installation, which could provide more context. Edge cases or potential risks of the update are also not mentioned. Despite these minor gaps, the overall intent and issue are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). The code changes required are minimal and straightforward, involving only updates to version constraints in two files (`requirements.txt` and `setup.py`). The scope of the change is limited to these configuration files and does not impact the core logic or architecture of the codebase. No deep understanding of the codebase, complex algorithms, or domain-specific knowledge is required beyond basic familiarity with Python dependency management. However, there is a slight increase in difficulty due to the need to verify that updating `pendulum` to a version less than 3.1 does not introduce breaking changes or compatibility issues with other dependencies or Python 3.12. This verification might involve checking release notes or running tests, but it remains a simple task. No significant edge cases or error handling modifications are mentioned or required based on the provided changes. Overall, this is a basic dependency update task with minimal technical complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add ability to resize table column widths\n### Context\n\nIt would be nice to be able to resize the column widths in the tables. If I have a long environemnt name, it expands my table to beyond my screen size. I'm able to scroll to view, but it would be nice if I could control column width so I can adjust the table to my window. \r\n\r\n<img width=\"1403\" alt=\"image\" src=\"https://github.com/nebari-dev/jupyterlab-new-launcher/assets/15659110/4dadd6a2-a014-4aba-9d4f-6a51e83f0e28\">\r\n\r\n\n\n### Value and/or benefit\n\neasier viewing for tables with long strings in them. \n\n### Anything else?\n\n_No response_\n", "patch": "diff --git a/src/components/base-table.tsx b/src/components/base-table.tsx\nnew file mode 100644\nindex 0000000..9cba6e5\n--- /dev/null\n+++ b/src/components/base-table.tsx\n@@ -0,0 +1,243 @@\n+/*\n+ * Copyright (c) Jupyter Development Team.\n+ * Distributed under the terms of the Modified BSD License.\n+ */\n+\n+import React, { ReactElement, ReactNode, useState } from 'react';\n+import { caretDownIcon, caretUpIcon } from '@jupyterlab/ui-components';\n+\n+export const TABLE_CLASS = 'jp-sortable-table';\n+\n+/**\n+ * A namespace for Table.\n+ */\n+export namespace Table {\n+  /**\n+   * The state which will be restored from layout tracker.\n+   */\n+  export interface ISortState {\n+    sortKey?: string | null;\n+    sortDirection: -1 | 1;\n+  }\n+  /**\n+   * The initialization options for the table.\n+   */\n+  export interface IOptions<T> extends Partial<ISortState> {\n+    rows: IRow<T>[];\n+    columns: IColumn<T>[];\n+    onRowClick?: React.MouseEventHandler<HTMLTableRowElement>;\n+    blankIndicator: () => ReactNode;\n+  }\n+  /**\n+   * Table row with data to display.\n+   */\n+  export interface IRow<T> {\n+    data: T;\n+    key: string;\n+  }\n+  /**\n+   * Column definition.\n+   */\n+  export interface IColumn<T> {\n+    id: string;\n+    label: string;\n+    renderCell(data: T): ReactNode;\n+    sort(a: T, b: T): number | undefined;\n+    isAvailable?(): boolean;\n+    isHidden?: boolean;\n+    minWidth?: number;\n+  }\n+}\n+\n+/**\n+ * Sortable table component for small datasets.\n+ *\n+ * For large datasets use `DataGrid` from `@lumino/datagrid`.\n+ */\n+export function Table<T>(props: Table.IOptions<T>) {\n+  const [sortState, setSortState] = useState<Table.ISortState>({\n+    sortKey: props.sortKey,\n+    sortDirection: props.sortDirection || 1\n+  });\n+\n+  const sort = (key: string) => {\n+    if (key === sortState.sortKey) {\n+      setSortState({\n+        sortKey: key,\n+        sortDirection: (sortState.sortDirection * -1) as -1 | 1\n+      });\n+    } else {\n+      setSortState({ sortKey: key, sortDirection: 1 });\n+    }\n+  };\n+\n+  let rows = props.rows;\n+  const sortedColumn = props.columns.filter(\n+    column => column.id === sortState.sortKey\n+  )[0];\n+\n+  if (sortedColumn) {\n+    const sorter = sortedColumn.sort.bind(sortedColumn);\n+    rows = props.rows.sort(\n+      (a, b) => (sorter(a.data, b.data) ?? 0) * sortState.sortDirection\n+    );\n+  }\n+\n+  const visibleColumns = props.columns.filter(\n+    column =>\n+      (column.isAvailable ? column.isAvailable() : true) && !column.isHidden\n+  );\n+\n+  const [isColumnResized, setColumnResized] = React.useState<\n+    Record<string, boolean>\n+  >({});\n+\n+  const elements = rows.map(row => {\n+    const cells = visibleColumns.map(column => (\n+      <td\n+        key={column.id + '-' + row.key}\n+        className={isColumnResized[column.id] ? 'jp-mod-col-resized' : ''}\n+      >\n+        {column.renderCell(row.data)}\n+      </td>\n+    ));\n+\n+    return (\n+      <tr\n+        key={row.key}\n+        data-key={row.key}\n+        onClick={props.onRowClick}\n+        className={'jp-sortable-table-tr'}\n+      >\n+        {cells}\n+      </tr>\n+    );\n+  });\n+\n+  const columnsHeaders = visibleColumns.map(column => (\n+    <SortableTH\n+      label={column.label}\n+      id={column.id}\n+      state={sortState}\n+      key={column.id}\n+      onSort={() => {\n+        sort(column.id);\n+      }}\n+      onResize={() => {\n+        setColumnResized({\n+          ...isColumnResized,\n+          [column.id]: true\n+        });\n+      }}\n+      minWidth={column.minWidth}\n+    />\n+  ));\n+\n+  return (\n+    <table className={TABLE_CLASS}>\n+      <thead>\n+        <tr className={'jp-sortable-table-tr'}>{columnsHeaders}</tr>\n+      </thead>\n+      <tbody>{elements}</tbody>\n+    </table>\n+  );\n+}\n+\n+function SortableTH(props: {\n+  id: string;\n+  label: string;\n+  state: Table.ISortState;\n+  onSort: () => void;\n+  onResize: () => void;\n+  minWidth?: number;\n+}): ReactElement {\n+  const isSortKey = props.id === props.state.sortKey;\n+  const sortIcon =\n+    !isSortKey || props.state.sortDirection === 1 ? caretUpIcon : caretDownIcon;\n+  const [columnWidth, setColumnWidth] = React.useState<number | null>(null);\n+\n+  const thRef = React.useRef<HTMLTableCellElement | null>(null);\n+  const [resizeOngoing, setResizeOngoing] = React.useState<boolean>(false);\n+  const [thLeft, setThLeft] = useState<number | null>(null);\n+\n+  requestAnimationFrame(() => {\n+    if (thRef.current) {\n+      const { left } = thRef.current.getBoundingClientRect();\n+      setThLeft(left);\n+    }\n+  });\n+\n+  React.useEffect(() => {\n+    const onPointerMove = (event: PointerEvent) => {\n+      if (resizeOngoing) {\n+        let left: number;\n+        if (thLeft === null) {\n+          if (!thRef.current) {\n+            throw Error('Cannot resize: no reference to the current table');\n+          }\n+          console.warn('Resize cache for column was not available');\n+          left = thRef.current.getBoundingClientRect().left;\n+        } else {\n+          left = thLeft;\n+        }\n+        setColumnWidth(event.clientX - left);\n+        props.onResize();\n+      }\n+    };\n+    const onPointerUp = (event: PointerEvent) => {\n+      if (resizeOngoing) {\n+        document.body.classList.remove(RESIZE_ACTIVE);\n+        setResizeOngoing(false);\n+        // do not sort when finishing resize\n+        event.stopImmediatePropagation();\n+      }\n+    };\n+    document.body.addEventListener('pointermove', onPointerMove);\n+    document.body.addEventListener('pointerup', onPointerUp);\n+    return () => {\n+      document.body.removeEventListener('pointermove', onPointerMove);\n+      document.body.removeEventListener('pointerup', onPointerUp);\n+    };\n+  });\n+\n+  const classes: string[] = [];\n+  if (isSortKey) {\n+    classes.push('jp-sorted-header');\n+  }\n+  if (resizeOngoing) {\n+    classes.push('jp-header-resizing');\n+  }\n+  const RESIZE_HANDLE = 'jp-sortable-table-resize-handle';\n+  const RESIZE_ACTIVE = 'jp-mod-resize-table';\n+\n+  return (\n+    <th\n+      key={props.id}\n+      ref={thRef}\n+      onClick={() => props.onSort()}\n+      className={classes.join(' ')}\n+      data-id={props.id}\n+      style={{\n+        width:\n+          columnWidth !== null\n+            ? `${Math.max(props.minWidth ?? 50, columnWidth)}px`\n+            : ''\n+      }}\n+      onPointerDown={event => {\n+        if (\n+          event.target instanceof HTMLElement &&\n+          event.target.className === RESIZE_HANDLE\n+        ) {\n+          document.body.classList.add(RESIZE_ACTIVE);\n+          setResizeOngoing(true);\n+        }\n+      }}\n+    >\n+      <div className=\"jp-sortable-table-th-wrapper\">\n+        <label>{props.label}</label>\n+        <sortIcon.react tag=\"span\" className=\"jp-sort-icon\" />\n+      </div>\n+      <div className={RESIZE_HANDLE}></div>\n+    </th>\n+  );\n+}\ndiff --git a/src/components/table.tsx b/src/components/table.tsx\nindex 614f529..fc05093 100644\n--- a/src/components/table.tsx\n+++ b/src/components/table.tsx\n@@ -5,12 +5,8 @@ import { ReadonlyJSONObject } from '@lumino/coreutils';\n import { Time } from '@jupyterlab/coreutils';\n import { ISettingRegistry } from '@jupyterlab/settingregistry';\n import { TranslationBundle } from '@jupyterlab/translation';\n-import {\n-  FilterBox,\n-  Table,\n-  UseSignal,\n-  MenuSvg\n-} from '@jupyterlab/ui-components';\n+import { FilterBox, UseSignal, MenuSvg } from '@jupyterlab/ui-components';\n+import { Table } from './base-table';\n import * as React from 'react';\n import { ISettingsLayout, CommandIDs, IKernelItem } from '../types';\n import { starIcon } from '../icons';\n@@ -41,6 +37,14 @@ function columnLabelFromKey(key: string): string {\n   return key[0].toUpperCase() + key.substring(1);\n }\n \n+function EllipsedCell(props: React.PropsWithChildren<{ title?: string }>) {\n+  return (\n+    <div className=\"jp-ellipsis-wrapper\" title={props.title}>\n+      <div className=\"jp-ellipsis\">{props.children}</div>\n+    </div>\n+  );\n+}\n+\n export function KernelTable(props: {\n   trans: TranslationBundle;\n   items: IKernelItem[];\n@@ -87,14 +91,17 @@ export function KernelTable(props: {\n           const kernelMeta = item.metadata?.kernel as\n             | ReadonlyJSONObject\n             | undefined;\n-          if (!kernelMeta) {\n-            return '-';\n-          }\n-          const value = kernelMeta[metadataKey];\n-          if (typeof value === 'string') {\n-            return value;\n-          }\n-          return JSON.stringify(value);\n+          const render = () => {\n+            if (!kernelMeta) {\n+              return '-';\n+            }\n+            const value = kernelMeta[metadataKey];\n+            if (typeof value === 'string') {\n+              return value;\n+            }\n+            return JSON.stringify(value);\n+          };\n+          return <EllipsedCell>{render()}</EllipsedCell>;\n         },\n         sort: (a: IKernelItem, b: IKernelItem) => {\n           const aKernelMeta = a.metadata?.kernel as\n@@ -140,23 +147,7 @@ export function KernelTable(props: {\n       id: 'kernel',\n       label: trans.__('Kernel'),\n       renderCell: (row: IKernelItem) => (\n-        <>\n-          <span\n-            className=\"jp-LauncherCard-icon\"\n-            onClick={() => props.onClick(row)}\n-          >\n-            {row.kernelIconUrl ? (\n-              <img\n-                src={row.kernelIconUrl}\n-                className=\"jp-Launcher-kernelIcon\"\n-                alt={row.label}\n-              />\n-            ) : (\n-              <div className=\"jp-LauncherCard-noKernelIcon\">\n-                {row.label[0].toUpperCase()}\n-              </div>\n-            )}\n-          </span>\n+        <EllipsedCell>\n           <span\n             className={KERNEL_ITEM_CLASS}\n             onClick={event => {\n@@ -171,9 +162,25 @@ export function KernelTable(props: {\n             }}\n             tabIndex={0}\n           >\n-            {row.label}\n+            <span\n+              className=\"jp-LauncherCard-icon\"\n+              onClick={() => props.onClick(row)}\n+            >\n+              {row.kernelIconUrl ? (\n+                <img\n+                  src={row.kernelIconUrl}\n+                  className=\"jp-Launcher-kernelIcon\"\n+                  alt={row.label}\n+                />\n+              ) : (\n+                <div className=\"jp-LauncherCard-noKernelIcon\">\n+                  {row.label[0].toUpperCase()}\n+                </div>\n+              )}\n+            </span>\n+            <span className=\"jp-TableKernelItem-label\">{row.label}</span>\n           </span>\n-        </>\n+        </EllipsedCell>\n       ),\n       sort: (a: IKernelItem, b: IKernelItem) => a.label.localeCompare(b.label)\n     },\n@@ -185,12 +192,20 @@ export function KernelTable(props: {\n         return (\n           <UseSignal signal={row.refreshLastUsed}>\n             {() => {\n-              return row.lastUsed ? (\n-                <span title={Time.format(row.lastUsed)}>\n-                  {Time.formatHuman(row.lastUsed)}\n-                </span>\n-              ) : (\n-                trans.__('Never')\n+              return (\n+                <EllipsedCell\n+                  title={\n+                    row.lastUsed\n+                      ? Time.format(row.lastUsed)\n+                      : trans.__(\n+                          'No information about last use of this kernel is available in the layout database'\n+                        )\n+                  }\n+                >\n+                  {row.lastUsed\n+                    ? Time.formatHuman(row.lastUsed)\n+                    : trans.__('Never')}\n+                </EllipsedCell>\n               );\n             }}\n           </UseSignal>\ndiff --git a/style/base.css b/style/base.css\nindex ac1d235..95aa6ff 100644\n--- a/style/base.css\n+++ b/style/base.css\n@@ -148,10 +148,17 @@\n }\n \n .jp-TableKernelItem {\n-  display: inline-block;\n   line-height: var(--jp-icon-size);\n+}\n+\n+.jp-TableKernelItem > .jp-LauncherCard-icon {\n+  margin-right: 4px;\n+  vertical-align: top;\n+  height: var(--jp-icon-size);\n+}\n+\n+.jp-TableKernelItem-label {\n   vertical-align: top;\n-  padding-left: 4px;\n }\n \n .jp-NewLauncher-table th[data-id='icon'],\n@@ -245,3 +252,95 @@\n   height: 52px;\n   width: 52px;\n }\n+\n+.jp-NewLauncher-table .jp-sortable-table-tr > td {\n+  white-space: nowrap;\n+}\n+\n+.jp-sortable-table-tr > td.jp-mod-col-resized > .jp-ellipsis-wrapper {\n+  white-space: nowrap;\n+  position: absolute;\n+  inset: 0;\n+}\n+\n+.jp-sortable-table-tr\n+  > td.jp-mod-col-resized\n+  > .jp-ellipsis-wrapper\n+  > .jp-ellipsis {\n+  white-space: nowrap;\n+  overflow: hidden;\n+  text-overflow: ellipsis;\n+\n+  /* TODO share values for padding and height from `.jp-sortable-table-tr > td` via variable; not top padding is a sum of bottom and top in plain td to align text. */\n+  padding: 5px 12px 1px;\n+}\n+\n+.jp-sortable-table-tr > td > .jp-starIconButton {\n+  white-space: nowrap;\n+}\n+\n+.jp-sortable-table-tr > th,\n+.jp-sortable-table-tr > td {\n+  padding: 3px 12px;\n+}\n+\n+td > .jp-starIconButton {\n+  position: relative;\n+  top: 1px;\n+}\n+\n+/* To contribute upstream  */\n+\n+.jp-sortable-table-resize-handle {\n+  width: 4px;\n+  position: absolute;\n+  right: 0;\n+  top: 0;\n+  height: 100%;\n+  cursor: ew-resize;\n+}\n+\n+.jp-sortable-table-tr > th {\n+  overflow: hidden;\n+  box-sizing: border-box;\n+  height: 24px;\n+  padding: 0;\n+  -webkit-user-select: none;\n+}\n+\n+.jp-sortable-table-resize-handle:hover {\n+  background: var(--jp-layout-color3);\n+}\n+\n+.jp-header-resizing > .jp-sortable-table-resize-handle {\n+  background: var(--jp-brand-color1) !important;\n+}\n+\n+body.jp-mod-resize-table * {\n+  cursor: ew-resize;\n+}\n+\n+body.jp-mod-resize-table .jp-sortable-table {\n+  pointer-events: none;\n+}\n+\n+/*\n+This will likely be needed upstream when no ellipsis wrapping is used.\n+.jp-sortable-table-tr > td > * {\n+  white-space: normal;\n+}\n+*/\n+\n+.jp-sortable-table-th-wrapper > label {\n+  overflow: hidden;\n+  font-weight: 500;\n+}\n+\n+.jp-sortable-table-th-wrapper {\n+  width: inherit;\n+  box-sizing: border-box;\n+  padding: 4px 12px 2px;\n+\n+  /* fill-available is only needed when heading does not collapse\n+  /* min-width: -webkit-fill-available; */\n+}\n", "instance_id": "nebari-dev__jupyterlab-launchpad-22", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in terms of the goal: adding the ability to resize table column widths to improve user experience when viewing tables with long strings. The context provided, including the screenshot, helps illustrate the issue of tables expanding beyond screen size. However, there are minor ambiguities and missing details. For instance, the statement does not specify whether resizing should persist across sessions, if there are minimum or maximum width constraints, or how resizing should interact with other UI elements (e.g., sorting). Additionally, there are no explicit mentions of edge cases, such as very narrow or wide columns, or how resizing should behave on different screen sizes or devices. Despite these gaps, the intent and value of the feature are evident, making the problem statement mostly clear but not comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving the creation of a new file (`base-table.tsx`) with over 200 lines of code and modifications to existing files (`table.tsx` and `base.css`). This indicates a substantial addition to the codebase rather than a minor tweak. The changes impact multiple components, requiring an understanding of React state management (e.g., `useState` for tracking column widths and resize states), event handling (e.g., pointer events for resizing), and CSS styling for visual feedback during resizing. \n\nSecond, the technical concepts involved are moderately complex. The solution requires knowledge of React hooks, DOM manipulation (e.g., `getBoundingClientRect` for positioning), and CSS for layout and user interaction (e.g., `cursor: ew-resize`, overflow handling). Additionally, the implementation touches on UI/UX considerations, such as ensuring resized columns display content properly with ellipsis and maintaining usability during resizing (e.g., disabling pointer events on the table during resize).\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes address some implicitly, such as enforcing a minimum width for columns and handling resize interactions. However, implementing a robust resizing feature inherently involves potential edge cases like extremely small or large column widths, rapid user interactions, or resizing on different screen resolutions, which add to the complexity.\n\nFinally, the changes do not appear to significantly impact the broader system architecture but do require a deep understanding of the existing table component structure and integration with JupyterLab's UI components. Given the combination of new feature implementation, multiple technical concepts, and implicit edge case handling across several files, a difficulty score of 0.65 is appropriate, reflecting a challenging but not extremely complex task.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "feature: Add a `--refresh` option for `meltano select` to refresh a new copy of the discovered catalog\nThis feature would add a `--refresh` option, or similar CLI arg to the `meltano select` CLI command. https://docs.meltano.com/reference/command-line-interface#select\r\n\r\n```\r\n# Refresh the cached catalog by re-running discovery\r\nmeltano select tap-github --refresh\r\n\r\n# Print the normal --list output *after* refreshing the catalog\r\nmeltano select tap-github --list --refresh\r\n```\r\n\r\nThis would give users the ability to force a refresh of the cached `catalog.json` file.\r\n\r\n## Related\r\n\r\n- https://github.com/meltano/meltano/issues/6915\r\n- https://github.com/meltano/meltano/issues/6763\r\n- https://github.com/meltano/meltano/discussions/6757#discussioncomment-3650870\r\n\nfeat: Ignore Catalog Cache\ncloses #6919\r\n\r\n* Adds `no_catalog_cache` to config extras, so that a single plugin can be run every time without the catalog cache.\r\n* Adds `--catalog-refresh` option to `meltano select` and `meltano el`\r\n\r\nAdding a new extra seems extreme, but I don't know how else to implement this where it could be used as part of a job for any tap. If there's a better way to do it, I'm happy to adjust this based on feedback.\nAuthentication for Meltano Analyze\nMigrated from GitLab: https://gitlab.com/meltano/meltano/-/issues/1\n\nOriginally created by @joshlambert on 2018-07-25 16:40:57\n\n---\n\nWe need to add user authentication to Meltano Analyze. As an MVC we should:\n1. Support OAuth with GitLab (others are a bonus, but no need to test for now)\n\nImplement social login (GitLab only for now) with [Flask Security](https://pythonhosted.org/Flask-Security/)\n", "patch": "diff --git a/docs/docs/concepts/plugins.mdx b/docs/docs/concepts/plugins.mdx\nindex 7bd15f34f5..59f224bb13 100644\n--- a/docs/docs/concepts/plugins.mdx\n+++ b/docs/docs/concepts/plugins.mdx\n@@ -154,6 +154,7 @@ Extractors support the following [extras](/guide/configuration#plugin-extras):\n - [`select`](#select-extra)\n - [`select_filter`](#select-filter-extra)\n - [`state`](#state-extra)\n+- [`use_cached_catalog`](#cache-catalog-extra)\n \n #### `catalog` extra\n \n@@ -631,6 +632,52 @@ export TAP_GITLAB__STATE=extract/tap-gitlab.state.json\n   </TabItem>\n </Tabs>\n \n+#### <a name=\"cache-catalog-extra\"></a>`use_cached_catalog` extra\n+\n+- Setting: `_use_cached_catalog`\n+- [Environment variable](/guide/configuration#configuring-settings): `<EXTRACTOR>__USE_CACHED_CATALOG`, e.g. `TAP_GITLAB__USE_CACHED_CATALOG`\n+- Default: `True`\n+\n+An extractor's `use_cached_catalog` [extra](/guide/configuration#plugin-extras) is a boolean flag that, when set to `False`, disables the use of a cached catalog file during the extractor's discovery process. By default, Meltano will cache the catalog file generated by an extractor to speed up subsequent runs. However, if the extractor's schema has changed in a way that would affect discovery output, you may want to bypass the cache to ensure the latest catalog is used.\n+\n+Setting this extra to `False` forces the extractor to perform discovery and generate a new catalog file every time it runs, which can be useful during development or when an extractor supports dynamic catalog discovery, such as in [`tap-salesforce`](https://github.com/MeltanoLabs/tap-salesforce).\n+\n+##### How to use\n+\n+Manage this extra:\n+\n+<Tabs className=\"meltano-tabs\" queryString=\"meltano-tabs\">\n+  <TabItem className=\"meltano-tab-content\" value=\"meltano.yml\" label=\"meltano.yml\" default>\n+\n+```yaml\n+extractors:\n+- name: tap-gitlab\n+  use_cached_catalog: false\n+```\n+\n+  </TabItem>\n+  <TabItem className=\"meltano-tab-content\" value=\"terminal\" label=\"terminal\">\n+\n+```bash\n+meltano config <extractor> set _use_cached_catalog false\n+\n+# For example:\n+meltano config tap-gitlab set _use_cached_catalog false\n+```\n+\n+  </TabItem>\n+  <TabItem className=\"meltano-tab-content\" value=\"env\" label=\"env\">\n+\n+```bash\n+export <EXTRACTOR>__USE_CACHED_CATALOG=false\n+\n+# For example:\n+export TAP_GITLAB__USE_CACHED_CATALOG=false\n+```\n+\n+  </TabItem>\n+</Tabs>\n+\n ### Loaders\n \n Loaders are [pip packages](https://pip.pypa.io/en/stable/) used by [`meltano elt`](/reference/command-line-interface#elt) as part of [data integration](/guide/integration).\ndiff --git a/docs/docs/reference/command-line-interface.md b/docs/docs/reference/command-line-interface.md\nindex d291623f02..f41c60ae84 100644\n--- a/docs/docs/reference/command-line-interface.md\n+++ b/docs/docs/reference/command-line-interface.md\n@@ -971,6 +971,7 @@ meltano --environment=<ENVIRONMENT> run tap-gitlab target-postgres\n meltano run tap-gitlab one-mapping another-mapping target-postgres\n meltano run tap-gitlab target-postgres simple-job\n meltano run --state-id-suffix=<STATE_ID_SUFFIX> tap-gitlab target-postgres\n+meltano run --refresh-catalog tap-salesforce target-postgres\n ```\n \n #### Parameters\n@@ -985,6 +986,7 @@ meltano run --state-id-suffix=<STATE_ID_SUFFIX> tap-gitlab target-postgres\n - `--state-id-suffix` define a custom suffix to generate a state ID with for each EL pair.\n - `--merge-state` will merge state with that of previous runs. See the [example in the Meltano repository](https://github.com/meltano/meltano/blob/main/integration/example-library/meltano-run-merge-states/index.md).\n - `--run-id` will use the provided UUID for the current run. This is useful when your workflow is managed by an external system and you want to track the run in Meltano.\n+- `--refresh-catalog` will force a refresh of the catalog, ignoring any existing cached catalog from previous runs.\n \n Examples:\n \ndiff --git a/integration/meltano-manifest/expected-manifests/meltano-manifest.cicd.json b/integration/meltano-manifest/expected-manifests/meltano-manifest.cicd.json\nindex 2107f39cc6..33e18d6b11 100644\n--- a/integration/meltano-manifest/expected-manifests/meltano-manifest.cicd.json\n+++ b/integration/meltano-manifest/expected-manifests/meltano-manifest.cicd.json\n@@ -199,6 +199,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_CLOUDWATCH_AWS_REGION_NAME\": \"us-west-2\",\n           \"TAP_CLOUDWATCH_BATCH_INCREMENT_S\": \"3600\",\n           \"TAP_CLOUDWATCH_LOG_GROUP_NAME\": \"API-Gateway-Execution-Logs_i32s35df22/prod\",\n@@ -207,7 +208,8 @@\n           \"TAP_CLOUDWATCH__METADATA\": \"{}\",\n           \"TAP_CLOUDWATCH__SCHEMA\": \"{}\",\n           \"TAP_CLOUDWATCH__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\"\n+          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_CLOUDWATCH__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"executable\": \"tap-cloudwatch\",\n         \"label\": \"Cloudwatch\",\n@@ -375,11 +377,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"plugins.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_MELTANOHUB__LOAD_SCHEMA\": \"tap_meltanohub\",\n           \"TAP_MELTANOHUB__METADATA\": \"{}\",\n           \"TAP_MELTANOHUB__SCHEMA\": \"{}\",\n           \"TAP_MELTANOHUB__SELECT\": \"[\\\"plugins.*\\\"]\",\n-          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_MELTANOHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Meltanohub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/meltanohub.png\",\n@@ -479,12 +483,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SPREADSHEETS_ANYWHERE_TABLES\": \"[{\\\"path\\\": \\\"https://ip-ranges.amazonaws.com\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"ip_prefix\\\"], \\\"name\\\": \\\"aws_ips\\\", \\\"pattern\\\": \\\"ip-ranges.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://www.gstatic.com/ipranges\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"gcp_ips\\\", \\\"pattern\\\": \\\"cloud.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"azure_ips\\\", \\\"pattern\\\": \\\"ServiceTags_Public_20230116.json\\\", \\\"json_path\\\": \\\"values\\\"}]\",\n           \"TAP_SPREADSHEETS_ANYWHERE__LOAD_SCHEMA\": \"tap_spreadsheets_anywhere\",\n           \"TAP_SPREADSHEETS_ANYWHERE__METADATA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SCHEMA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SPREADSHEETS_ANYWHERE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Spreadsheets Anywhere\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/spreadsheets-anywhere.png\",\n@@ -540,6 +546,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"private_channel\\\"]\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[\\\"C01SK13R9NJ\\\"]\",\n@@ -548,7 +555,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Slack\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/slack.png\",\n@@ -707,6 +715,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"public_channel\\\"]\",\n           \"TAP_SLACK_PUBLIC_AUTO_JOIN_CHANNELS\": \"false\",\n@@ -718,13 +727,15 @@\n           \"TAP_SLACK_PUBLIC__SCHEMA\": \"{}\",\n           \"TAP_SLACK_PUBLIC__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"TAP_SLACK_PUBLIC__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK_PUBLIC__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[]\",\n           \"TAP_SLACK_THREAD_LOOKBACK_DAYS\": \"1\",\n           \"TAP_SLACK__LOAD_SCHEMA\": \"tap_slack_public\",\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-slack\",\n         \"name\": \"tap-slack-public\",\n@@ -758,13 +769,15 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GOOGLE_ANALYTICS_REPORTS\": \"./extract/ga_reports/cli_events_report_definition.json\",\n           \"TAP_GOOGLE_ANALYTICS_VIEW_ID\": \"188392047\",\n           \"TAP_GOOGLE_ANALYTICS__LOAD_SCHEMA\": \"tap_google_analytics\",\n           \"TAP_GOOGLE_ANALYTICS__METADATA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SCHEMA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\"\n+          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_GOOGLE_ANALYTICS__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Google Analytics (Universal Analytics API - Deprecated)\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/google-analytics.png\",\n@@ -910,6 +923,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITLAB_API_URL\": \"https://gitlab.com\",\n           \"TAP_GITLAB_FETCH_MERGE_REQUEST_COMMITS\": \"false\",\n           \"TAP_GITLAB_FETCH_PIPELINES_EXTENDED\": \"false\",\n@@ -921,7 +935,8 @@\n           \"TAP_GITLAB__METADATA\": \"{}\",\n           \"TAP_GITLAB__SCHEMA\": \"{}\",\n           \"TAP_GITLAB__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n-          \"TAP_GITLAB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITLAB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITLAB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitLab\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/gitlab.png\",\n@@ -1030,11 +1045,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB__LOAD_SCHEMA\": \"tap_github\",\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitHub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/github.png\",\n@@ -1182,6 +1199,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_MELTANO_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1190,6 +1208,7 @@\n           \"TAP_GITHUB_MELTANO__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_MELTANO__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"TAP_GITHUB_MELTANO__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_MELTANO__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1197,7 +1216,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-meltano\",\n@@ -1253,6 +1273,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_METRICS_LOG_LEVEL\": \"DEBUG\",\n           \"TAP_GITHUB_SEARCHES\": \"[{\\\"name\\\": \\\"tap non-forks\\\", \\\"query\\\": \\\"tap- fork:false language:Python singer in:readme org:MeltanoLabs org:transferwise\\\"}, {\\\"name\\\": \\\"target non-forks\\\", \\\"query\\\": \\\"target- fork:false language:Python singer in:readme org:MeltanoLabs org:transferwise\\\"}]\",\n           \"TAP_GITHUB_SEARCH_METRICS_LOG_LEVEL\": \"DEBUG\",\n@@ -1265,6 +1286,7 @@\n           \"TAP_GITHUB_SEARCH__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_SEARCH__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"TAP_GITHUB_SEARCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_SEARCH__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"repositories.__filter__\\\": \\\"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\\\n\\\", \\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n           \"TAP_GITHUB_STREAM_MAPS_REPOSITORIES___FILTER__\": \"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\n\",\n@@ -1272,7 +1294,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-search\",\n@@ -1311,6 +1334,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"CICD_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1321,7 +1345,8 @@\n           \"TAP_SNOWFLAKE__METADATA\": \"{}\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Snowflake\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/snowflake.png\",\n@@ -1424,6 +1449,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"CICD_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1440,6 +1466,7 @@\n           \"TAP_SNOWFLAKE_METRICS__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_METRICS__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ROLE\": \"CICD\",\n           \"TAP_SNOWFLAKE_TABLES\": \"CICD_PROD._MELTANO_HUB.FACT_VARIANT_HUB_METRICS\",\n           \"TAP_SNOWFLAKE_USER\": \"CICD\",\n@@ -1449,7 +1476,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1498,6 +1526,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_DBNAME\": \"CICD_PROD\",\n@@ -1512,6 +1541,7 @@\n           \"TAP_SNOWFLAKE_AUDIT__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_AUDIT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"CICD_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_ROLE\": \"CICD\",\n@@ -1523,7 +1553,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1555,6 +1586,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"CICD_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1572,6 +1604,7 @@\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_SINGER_ACTIVITY__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_TABLES\": \"CICD_PROD._SLACK_NOTIFICATIONS.SLACK_ALERTS\",\n           \"TAP_SNOWFLAKE_USER\": \"CICD\",\n           \"TAP_SNOWFLAKE_WAREHOUSE\": \"CICD\",\n@@ -1580,7 +1613,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\ndiff --git a/integration/meltano-manifest/expected-manifests/meltano-manifest.jigsaw.json b/integration/meltano-manifest/expected-manifests/meltano-manifest.jigsaw.json\nindex 7af906b56a..aeb70e370d 100644\n--- a/integration/meltano-manifest/expected-manifests/meltano-manifest.jigsaw.json\n+++ b/integration/meltano-manifest/expected-manifests/meltano-manifest.jigsaw.json\n@@ -189,6 +189,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_CLOUDWATCH_AWS_REGION_NAME\": \"us-west-2\",\n           \"TAP_CLOUDWATCH_BATCH_INCREMENT_S\": \"3600\",\n           \"TAP_CLOUDWATCH_LOG_GROUP_NAME\": \"API-Gateway-Execution-Logs_i32s35df22/prod\",\n@@ -198,7 +199,8 @@\n           \"TAP_CLOUDWATCH__METADATA\": \"{}\",\n           \"TAP_CLOUDWATCH__SCHEMA\": \"{}\",\n           \"TAP_CLOUDWATCH__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\"\n+          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_CLOUDWATCH__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"executable\": \"tap-cloudwatch\",\n         \"label\": \"Cloudwatch\",\n@@ -366,11 +368,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"plugins.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_MELTANOHUB__LOAD_SCHEMA\": \"tap_meltanohub\",\n           \"TAP_MELTANOHUB__METADATA\": \"{}\",\n           \"TAP_MELTANOHUB__SCHEMA\": \"{}\",\n           \"TAP_MELTANOHUB__SELECT\": \"[\\\"plugins.*\\\"]\",\n-          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_MELTANOHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Meltanohub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/meltanohub.png\",\n@@ -470,12 +474,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SPREADSHEETS_ANYWHERE_TABLES\": \"[{\\\"path\\\": \\\"https://ip-ranges.amazonaws.com\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"ip_prefix\\\"], \\\"name\\\": \\\"aws_ips\\\", \\\"pattern\\\": \\\"ip-ranges.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://www.gstatic.com/ipranges\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"gcp_ips\\\", \\\"pattern\\\": \\\"cloud.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"azure_ips\\\", \\\"pattern\\\": \\\"ServiceTags_Public_20230116.json\\\", \\\"json_path\\\": \\\"values\\\"}]\",\n           \"TAP_SPREADSHEETS_ANYWHERE__LOAD_SCHEMA\": \"tap_spreadsheets_anywhere\",\n           \"TAP_SPREADSHEETS_ANYWHERE__METADATA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SCHEMA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SPREADSHEETS_ANYWHERE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Spreadsheets Anywhere\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/spreadsheets-anywhere.png\",\n@@ -532,6 +538,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"private_channel\\\"]\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[\\\"C01SK13R9NJ\\\"]\",\n@@ -541,7 +548,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Slack\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/slack.png\",\n@@ -701,6 +709,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"public_channel\\\"]\",\n           \"TAP_SLACK_PUBLIC_AUTO_JOIN_CHANNELS\": \"false\",\n@@ -713,6 +722,7 @@\n           \"TAP_SLACK_PUBLIC__SCHEMA\": \"{}\",\n           \"TAP_SLACK_PUBLIC__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"TAP_SLACK_PUBLIC__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK_PUBLIC__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[]\",\n           \"TAP_SLACK_START_DATE\": \"2022-09-01\",\n           \"TAP_SLACK_THREAD_LOOKBACK_DAYS\": \"1\",\n@@ -720,7 +730,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-slack\",\n         \"name\": \"tap-slack-public\",\n@@ -753,13 +764,15 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GOOGLE_ANALYTICS_REPORTS\": \"./extract/ga_reports/cli_events_report_definition.json\",\n           \"TAP_GOOGLE_ANALYTICS_VIEW_ID\": \"188392047\",\n           \"TAP_GOOGLE_ANALYTICS__LOAD_SCHEMA\": \"tap_google_analytics\",\n           \"TAP_GOOGLE_ANALYTICS__METADATA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SCHEMA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\"\n+          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_GOOGLE_ANALYTICS__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Google Analytics (Universal Analytics API - Deprecated)\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/google-analytics.png\",\n@@ -901,6 +914,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITLAB_API_URL\": \"https://gitlab.com\",\n           \"TAP_GITLAB_FETCH_MERGE_REQUEST_COMMITS\": \"false\",\n           \"TAP_GITLAB_FETCH_PIPELINES_EXTENDED\": \"false\",\n@@ -912,7 +926,8 @@\n           \"TAP_GITLAB__METADATA\": \"{}\",\n           \"TAP_GITLAB__SCHEMA\": \"{}\",\n           \"TAP_GITLAB__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n-          \"TAP_GITLAB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITLAB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITLAB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitLab\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/gitlab.png\",\n@@ -1018,11 +1033,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB__LOAD_SCHEMA\": \"tap_github\",\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitHub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/github.png\",\n@@ -1169,6 +1186,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_MELTANO_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1177,6 +1195,7 @@\n           \"TAP_GITHUB_MELTANO__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_MELTANO__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"TAP_GITHUB_MELTANO__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_MELTANO__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1184,7 +1203,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-meltano\",\n@@ -1233,6 +1253,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_METRICS_LOG_LEVEL\": \"DEBUG\",\n           \"TAP_GITHUB_SEARCHES\": \"[{\\\"name\\\": \\\"tap non-forks\\\", \\\"query\\\": \\\"tap- fork:false language:Python singer in:readme\\\"}, {\\\"name\\\": \\\"target non-forks\\\", \\\"query\\\": \\\"target- fork:false language:Python singer in:readme\\\"}]\",\n           \"TAP_GITHUB_SEARCH_METRICS_LOG_LEVEL\": \"DEBUG\",\n@@ -1246,6 +1267,7 @@\n           \"TAP_GITHUB_SEARCH__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_SEARCH__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"TAP_GITHUB_SEARCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_SEARCH__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2022-02-13\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"repositories.__filter__\\\": \\\"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\\\n\\\", \\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1254,7 +1276,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-search\",\n@@ -1285,13 +1308,15 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{}\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Snowflake\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/snowflake.png\",\n@@ -1386,6 +1411,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_METRICS_ACCOUNT\": \"epa06486\",\n@@ -1396,12 +1422,14 @@\n           \"TAP_SNOWFLAKE_METRICS__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_METRICS__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake_metrics\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{\\\"*\\\": {\\\"replication-method\\\": \\\"FULL_TABLE\\\"}}\",\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1442,6 +1470,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_INSECURE_MODE\": \"False\",\n@@ -1451,13 +1480,15 @@\n           \"TAP_SNOWFLAKE_AUDIT__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_AUDIT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake_audit\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{\\\"*\\\": {\\\"replication-method\\\": \\\"FULL_TABLE\\\"}}\",\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1481,6 +1512,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY_ACCOUNT\": \"epa06486\",\n@@ -1491,12 +1523,14 @@\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_SINGER_ACTIVITY__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake_singer_activity\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{\\\"*\\\": {\\\"replication-method\\\": \\\"FULL_TABLE\\\"}}\",\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\ndiff --git a/integration/meltano-manifest/expected-manifests/meltano-manifest.json b/integration/meltano-manifest/expected-manifests/meltano-manifest.json\nindex 2d276e6352..daac127b85 100644\n--- a/integration/meltano-manifest/expected-manifests/meltano-manifest.json\n+++ b/integration/meltano-manifest/expected-manifests/meltano-manifest.json\n@@ -180,6 +180,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_CLOUDWATCH_AWS_REGION_NAME\": \"us-west-2\",\n           \"TAP_CLOUDWATCH_BATCH_INCREMENT_S\": \"3600\",\n           \"TAP_CLOUDWATCH_LOG_GROUP_NAME\": \"API-Gateway-Execution-Logs_i32s35df22/prod\",\n@@ -189,7 +190,8 @@\n           \"TAP_CLOUDWATCH__METADATA\": \"{}\",\n           \"TAP_CLOUDWATCH__SCHEMA\": \"{}\",\n           \"TAP_CLOUDWATCH__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\"\n+          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_CLOUDWATCH__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"executable\": \"tap-cloudwatch\",\n         \"label\": \"Cloudwatch\",\n@@ -357,11 +359,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"plugins.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_MELTANOHUB__LOAD_SCHEMA\": \"tap_meltanohub\",\n           \"TAP_MELTANOHUB__METADATA\": \"{}\",\n           \"TAP_MELTANOHUB__SCHEMA\": \"{}\",\n           \"TAP_MELTANOHUB__SELECT\": \"[\\\"plugins.*\\\"]\",\n-          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_MELTANOHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Meltanohub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/meltanohub.png\",\n@@ -461,12 +465,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SPREADSHEETS_ANYWHERE_TABLES\": \"[{\\\"path\\\": \\\"https://ip-ranges.amazonaws.com\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"ip_prefix\\\"], \\\"name\\\": \\\"aws_ips\\\", \\\"pattern\\\": \\\"ip-ranges.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://www.gstatic.com/ipranges\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"gcp_ips\\\", \\\"pattern\\\": \\\"cloud.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"azure_ips\\\", \\\"pattern\\\": \\\"ServiceTags_Public_20230116.json\\\", \\\"json_path\\\": \\\"values\\\"}]\",\n           \"TAP_SPREADSHEETS_ANYWHERE__LOAD_SCHEMA\": \"tap_spreadsheets_anywhere\",\n           \"TAP_SPREADSHEETS_ANYWHERE__METADATA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SCHEMA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SPREADSHEETS_ANYWHERE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Spreadsheets Anywhere\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/spreadsheets-anywhere.png\",\n@@ -523,6 +529,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"private_channel\\\"]\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[\\\"C01SK13R9NJ\\\"]\",\n@@ -532,7 +539,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Slack\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/slack.png\",\n@@ -692,6 +700,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"public_channel\\\"]\",\n           \"TAP_SLACK_PUBLIC_AUTO_JOIN_CHANNELS\": \"false\",\n@@ -704,6 +713,7 @@\n           \"TAP_SLACK_PUBLIC__SCHEMA\": \"{}\",\n           \"TAP_SLACK_PUBLIC__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"TAP_SLACK_PUBLIC__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK_PUBLIC__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[]\",\n           \"TAP_SLACK_START_DATE\": \"2021-01-01\",\n           \"TAP_SLACK_THREAD_LOOKBACK_DAYS\": \"1\",\n@@ -711,7 +721,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-slack\",\n         \"name\": \"tap-slack-public\",\n@@ -744,13 +755,15 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GOOGLE_ANALYTICS_REPORTS\": \"./extract/ga_reports/cli_events_report_definition.json\",\n           \"TAP_GOOGLE_ANALYTICS_VIEW_ID\": \"188392047\",\n           \"TAP_GOOGLE_ANALYTICS__LOAD_SCHEMA\": \"tap_google_analytics\",\n           \"TAP_GOOGLE_ANALYTICS__METADATA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SCHEMA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\"\n+          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_GOOGLE_ANALYTICS__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Google Analytics (Universal Analytics API - Deprecated)\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/google-analytics.png\",\n@@ -892,6 +905,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITLAB_API_URL\": \"https://gitlab.com\",\n           \"TAP_GITLAB_FETCH_MERGE_REQUEST_COMMITS\": \"false\",\n           \"TAP_GITLAB_FETCH_PIPELINES_EXTENDED\": \"false\",\n@@ -903,7 +917,8 @@\n           \"TAP_GITLAB__METADATA\": \"{}\",\n           \"TAP_GITLAB__SCHEMA\": \"{}\",\n           \"TAP_GITLAB__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n-          \"TAP_GITLAB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITLAB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITLAB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitLab\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/gitlab.png\",\n@@ -1009,11 +1024,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB__LOAD_SCHEMA\": \"tap_github\",\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitHub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/github.png\",\n@@ -1160,6 +1177,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_MELTANO_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1168,6 +1186,7 @@\n           \"TAP_GITHUB_MELTANO__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_MELTANO__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"TAP_GITHUB_MELTANO__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_MELTANO__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1175,7 +1194,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-meltano\",\n@@ -1224,6 +1244,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_METRICS_LOG_LEVEL\": \"DEBUG\",\n           \"TAP_GITHUB_SEARCHES\": \"[{\\\"name\\\": \\\"tap non-forks\\\", \\\"query\\\": \\\"tap- fork:false language:Python singer in:readme\\\"}, {\\\"name\\\": \\\"target non-forks\\\", \\\"query\\\": \\\"target- fork:false language:Python singer in:readme\\\"}]\",\n           \"TAP_GITHUB_SEARCH_METRICS_LOG_LEVEL\": \"DEBUG\",\n@@ -1237,6 +1258,7 @@\n           \"TAP_GITHUB_SEARCH__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_SEARCH__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"TAP_GITHUB_SEARCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_SEARCH__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2022-02-13\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"repositories.__filter__\\\": \\\"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\\\n\\\", \\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1245,7 +1267,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-search\",\n@@ -1276,13 +1299,15 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{}\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Snowflake\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/snowflake.png\",\n@@ -1377,6 +1402,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_METRICS_ACCOUNT\": \"epa06486\",\n@@ -1387,12 +1413,14 @@\n           \"TAP_SNOWFLAKE_METRICS__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_METRICS__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake_metrics\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{\\\"*\\\": {\\\"replication-method\\\": \\\"FULL_TABLE\\\"}}\",\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1433,6 +1461,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_INSECURE_MODE\": \"False\",\n@@ -1442,13 +1471,15 @@\n           \"TAP_SNOWFLAKE_AUDIT__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_AUDIT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake_audit\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{\\\"*\\\": {\\\"replication-method\\\": \\\"FULL_TABLE\\\"}}\",\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1472,6 +1503,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY_ACCOUNT\": \"epa06486\",\n@@ -1482,12 +1514,14 @@\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_SINGER_ACTIVITY__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE__LOAD_SCHEMA\": \"tap_snowflake_singer_activity\",\n           \"TAP_SNOWFLAKE__METADATA\": \"{\\\"*\\\": {\\\"replication-method\\\": \\\"FULL_TABLE\\\"}}\",\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\ndiff --git a/integration/meltano-manifest/expected-manifests/meltano-manifest.prod.json b/integration/meltano-manifest/expected-manifests/meltano-manifest.prod.json\nindex f728375339..8eb6af19bd 100644\n--- a/integration/meltano-manifest/expected-manifests/meltano-manifest.prod.json\n+++ b/integration/meltano-manifest/expected-manifests/meltano-manifest.prod.json\n@@ -185,6 +185,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_CLOUDWATCH_AWS_REGION_NAME\": \"us-west-2\",\n           \"TAP_CLOUDWATCH_BATCH_INCREMENT_S\": \"3600\",\n           \"TAP_CLOUDWATCH_LOG_GROUP_NAME\": \"API-Gateway-Execution-Logs_i32s35df22/prod\",\n@@ -194,7 +195,8 @@\n           \"TAP_CLOUDWATCH__METADATA\": \"{}\",\n           \"TAP_CLOUDWATCH__SCHEMA\": \"{}\",\n           \"TAP_CLOUDWATCH__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\"\n+          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_CLOUDWATCH__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"executable\": \"tap-cloudwatch\",\n         \"label\": \"Cloudwatch\",\n@@ -362,11 +364,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"plugins.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_MELTANOHUB__LOAD_SCHEMA\": \"tap_meltanohub\",\n           \"TAP_MELTANOHUB__METADATA\": \"{}\",\n           \"TAP_MELTANOHUB__SCHEMA\": \"{}\",\n           \"TAP_MELTANOHUB__SELECT\": \"[\\\"plugins.*\\\"]\",\n-          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_MELTANOHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Meltanohub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/meltanohub.png\",\n@@ -466,12 +470,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SPREADSHEETS_ANYWHERE_TABLES\": \"[{\\\"path\\\": \\\"https://ip-ranges.amazonaws.com\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"ip_prefix\\\"], \\\"name\\\": \\\"aws_ips\\\", \\\"pattern\\\": \\\"ip-ranges.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://www.gstatic.com/ipranges\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"gcp_ips\\\", \\\"pattern\\\": \\\"cloud.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"azure_ips\\\", \\\"pattern\\\": \\\"ServiceTags_Public_20230116.json\\\", \\\"json_path\\\": \\\"values\\\"}]\",\n           \"TAP_SPREADSHEETS_ANYWHERE__LOAD_SCHEMA\": \"tap_spreadsheets_anywhere\",\n           \"TAP_SPREADSHEETS_ANYWHERE__METADATA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SCHEMA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SPREADSHEETS_ANYWHERE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Spreadsheets Anywhere\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/spreadsheets-anywhere.png\",\n@@ -528,6 +534,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"private_channel\\\"]\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[\\\"C01SK13R9NJ\\\"]\",\n@@ -537,7 +544,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Slack\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/slack.png\",\n@@ -697,6 +705,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"public_channel\\\"]\",\n           \"TAP_SLACK_PUBLIC_AUTO_JOIN_CHANNELS\": \"false\",\n@@ -709,6 +718,7 @@\n           \"TAP_SLACK_PUBLIC__SCHEMA\": \"{}\",\n           \"TAP_SLACK_PUBLIC__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"TAP_SLACK_PUBLIC__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK_PUBLIC__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[]\",\n           \"TAP_SLACK_START_DATE\": \"2021-01-01\",\n           \"TAP_SLACK_THREAD_LOOKBACK_DAYS\": \"7\",\n@@ -716,7 +726,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-slack\",\n         \"name\": \"tap-slack-public\",\n@@ -751,6 +762,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GOOGLE_ANALYTICS_REPORTS\": \"./extract/ga_reports/cli_events_report_definition.json\",\n           \"TAP_GOOGLE_ANALYTICS_START_DATE\": \"2022-02-20\",\n           \"TAP_GOOGLE_ANALYTICS_VIEW_ID\": \"188392047\",\n@@ -758,7 +770,8 @@\n           \"TAP_GOOGLE_ANALYTICS__METADATA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SCHEMA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\"\n+          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_GOOGLE_ANALYTICS__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Google Analytics (Universal Analytics API - Deprecated)\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/google-analytics.png\",\n@@ -905,6 +918,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITLAB_API_URL\": \"https://gitlab.com\",\n           \"TAP_GITLAB_FETCH_MERGE_REQUEST_COMMITS\": \"false\",\n           \"TAP_GITLAB_FETCH_PIPELINES_EXTENDED\": \"false\",\n@@ -917,7 +931,8 @@\n           \"TAP_GITLAB__METADATA\": \"{}\",\n           \"TAP_GITLAB__SCHEMA\": \"{}\",\n           \"TAP_GITLAB__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n-          \"TAP_GITLAB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITLAB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITLAB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitLab\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/gitlab.png\",\n@@ -1027,12 +1042,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2020-01-01\",\n           \"TAP_GITHUB__LOAD_SCHEMA\": \"tap_github\",\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitHub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/github.png\",\n@@ -1180,6 +1197,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_MELTANO_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_MELTANO_START_DATE\": \"2020-01-01\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n@@ -1189,6 +1207,7 @@\n           \"TAP_GITHUB_MELTANO__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_MELTANO__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"TAP_GITHUB_MELTANO__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_MELTANO__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_START_DATE\": \"2020-01-01\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n@@ -1197,7 +1216,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-meltano\",\n@@ -1246,6 +1266,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_METRICS_LOG_LEVEL\": \"DEBUG\",\n           \"TAP_GITHUB_SEARCHES\": \"[{\\\"name\\\": \\\"tap non-forks\\\", \\\"query\\\": \\\"tap- fork:false language:Python singer in:readme\\\"}, {\\\"name\\\": \\\"target non-forks\\\", \\\"query\\\": \\\"target- fork:false language:Python singer in:readme\\\"}]\",\n           \"TAP_GITHUB_SEARCH_METRICS_LOG_LEVEL\": \"DEBUG\",\n@@ -1259,6 +1280,7 @@\n           \"TAP_GITHUB_SEARCH__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_SEARCH__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"TAP_GITHUB_SEARCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_SEARCH__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2022-02-13\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"repositories.__filter__\\\": \\\"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\\\n\\\", \\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1267,7 +1289,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-search\",\n@@ -1306,6 +1329,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1316,7 +1340,8 @@\n           \"TAP_SNOWFLAKE__METADATA\": \"{}\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Snowflake\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/snowflake.png\",\n@@ -1419,6 +1444,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1435,6 +1461,7 @@\n           \"TAP_SNOWFLAKE_METRICS__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_METRICS__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ROLE\": \"REPORTER\",\n           \"TAP_SNOWFLAKE_TABLES\": \"PROD.MELTANO_HUB.FACT_VARIANT_HUB_METRICS\",\n           \"TAP_SNOWFLAKE_USER\": \"MELTANO\",\n@@ -1444,7 +1471,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1493,6 +1521,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_DBNAME\": \"PROD\",\n@@ -1507,6 +1536,7 @@\n           \"TAP_SNOWFLAKE_AUDIT__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_AUDIT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_ROLE\": \"REPORTER\",\n@@ -1518,7 +1548,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1550,6 +1581,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1567,6 +1599,7 @@\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_SINGER_ACTIVITY__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_TABLES\": \"PROD.SLACK_NOTIFICATIONS.SLACK_ALERTS\",\n           \"TAP_SNOWFLAKE_USER\": \"MELTANO\",\n           \"TAP_SNOWFLAKE_WAREHOUSE\": \"REPORTER\",\n@@ -1575,7 +1608,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\ndiff --git a/integration/meltano-manifest/expected-manifests/meltano-manifest.staging.json b/integration/meltano-manifest/expected-manifests/meltano-manifest.staging.json\nindex a31aa17de2..e44e2a61c5 100644\n--- a/integration/meltano-manifest/expected-manifests/meltano-manifest.staging.json\n+++ b/integration/meltano-manifest/expected-manifests/meltano-manifest.staging.json\n@@ -181,6 +181,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_CLOUDWATCH_AWS_REGION_NAME\": \"us-west-2\",\n           \"TAP_CLOUDWATCH_BATCH_INCREMENT_S\": \"3600\",\n           \"TAP_CLOUDWATCH_LOG_GROUP_NAME\": \"API-Gateway-Execution-Logs_i32s35df22/prod\",\n@@ -190,7 +191,8 @@\n           \"TAP_CLOUDWATCH__METADATA\": \"{}\",\n           \"TAP_CLOUDWATCH__SCHEMA\": \"{}\",\n           \"TAP_CLOUDWATCH__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\"\n+          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_CLOUDWATCH__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"executable\": \"tap-cloudwatch\",\n         \"label\": \"Cloudwatch\",\n@@ -358,11 +360,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"plugins.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_MELTANOHUB__LOAD_SCHEMA\": \"tap_meltanohub\",\n           \"TAP_MELTANOHUB__METADATA\": \"{}\",\n           \"TAP_MELTANOHUB__SCHEMA\": \"{}\",\n           \"TAP_MELTANOHUB__SELECT\": \"[\\\"plugins.*\\\"]\",\n-          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_MELTANOHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Meltanohub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/meltanohub.png\",\n@@ -462,12 +466,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SPREADSHEETS_ANYWHERE_TABLES\": \"[{\\\"path\\\": \\\"https://ip-ranges.amazonaws.com\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"ip_prefix\\\"], \\\"name\\\": \\\"aws_ips\\\", \\\"pattern\\\": \\\"ip-ranges.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://www.gstatic.com/ipranges\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"gcp_ips\\\", \\\"pattern\\\": \\\"cloud.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"azure_ips\\\", \\\"pattern\\\": \\\"ServiceTags_Public_20230116.json\\\", \\\"json_path\\\": \\\"values\\\"}]\",\n           \"TAP_SPREADSHEETS_ANYWHERE__LOAD_SCHEMA\": \"tap_spreadsheets_anywhere\",\n           \"TAP_SPREADSHEETS_ANYWHERE__METADATA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SCHEMA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SPREADSHEETS_ANYWHERE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Spreadsheets Anywhere\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/spreadsheets-anywhere.png\",\n@@ -524,6 +530,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"private_channel\\\"]\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[\\\"C01SK13R9NJ\\\"]\",\n@@ -533,7 +540,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Slack\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/slack.png\",\n@@ -693,6 +701,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"public_channel\\\"]\",\n           \"TAP_SLACK_PUBLIC_AUTO_JOIN_CHANNELS\": \"false\",\n@@ -705,6 +714,7 @@\n           \"TAP_SLACK_PUBLIC__SCHEMA\": \"{}\",\n           \"TAP_SLACK_PUBLIC__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"TAP_SLACK_PUBLIC__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK_PUBLIC__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[]\",\n           \"TAP_SLACK_START_DATE\": \"2021-01-01\",\n           \"TAP_SLACK_THREAD_LOOKBACK_DAYS\": \"7\",\n@@ -712,7 +722,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-slack\",\n         \"name\": \"tap-slack-public\",\n@@ -747,6 +758,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GOOGLE_ANALYTICS_REPORTS\": \"./extract/ga_reports/cli_events_report_definition.json\",\n           \"TAP_GOOGLE_ANALYTICS_START_DATE\": \"2022-02-20\",\n           \"TAP_GOOGLE_ANALYTICS_VIEW_ID\": \"188392047\",\n@@ -754,7 +766,8 @@\n           \"TAP_GOOGLE_ANALYTICS__METADATA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SCHEMA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\"\n+          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_GOOGLE_ANALYTICS__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Google Analytics (Universal Analytics API - Deprecated)\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/google-analytics.png\",\n@@ -901,6 +914,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITLAB_API_URL\": \"https://gitlab.com\",\n           \"TAP_GITLAB_FETCH_MERGE_REQUEST_COMMITS\": \"false\",\n           \"TAP_GITLAB_FETCH_PIPELINES_EXTENDED\": \"false\",\n@@ -913,7 +927,8 @@\n           \"TAP_GITLAB__METADATA\": \"{}\",\n           \"TAP_GITLAB__SCHEMA\": \"{}\",\n           \"TAP_GITLAB__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n-          \"TAP_GITLAB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITLAB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITLAB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitLab\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/gitlab.png\",\n@@ -1023,12 +1038,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2020-01-01\",\n           \"TAP_GITHUB__LOAD_SCHEMA\": \"tap_github\",\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitHub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/github.png\",\n@@ -1176,6 +1193,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_MELTANO_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_MELTANO_START_DATE\": \"2020-01-01\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n@@ -1185,6 +1203,7 @@\n           \"TAP_GITHUB_MELTANO__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_MELTANO__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"TAP_GITHUB_MELTANO__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_MELTANO__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_START_DATE\": \"2020-01-01\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n@@ -1193,7 +1212,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-meltano\",\n@@ -1242,6 +1262,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_METRICS_LOG_LEVEL\": \"DEBUG\",\n           \"TAP_GITHUB_SEARCHES\": \"[{\\\"name\\\": \\\"tap non-forks\\\", \\\"query\\\": \\\"tap- fork:false language:Python singer in:readme\\\"}, {\\\"name\\\": \\\"target non-forks\\\", \\\"query\\\": \\\"target- fork:false language:Python singer in:readme\\\"}]\",\n           \"TAP_GITHUB_SEARCH_METRICS_LOG_LEVEL\": \"DEBUG\",\n@@ -1255,6 +1276,7 @@\n           \"TAP_GITHUB_SEARCH__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_SEARCH__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"TAP_GITHUB_SEARCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_SEARCH__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2022-02-13\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"repositories.__filter__\\\": \\\"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\\\n\\\", \\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1263,7 +1285,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-search\",\n@@ -1302,6 +1325,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"STAGING_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1312,7 +1336,8 @@\n           \"TAP_SNOWFLAKE__METADATA\": \"{}\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Snowflake\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/snowflake.png\",\n@@ -1415,6 +1440,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"STAGING_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1431,6 +1457,7 @@\n           \"TAP_SNOWFLAKE_METRICS__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_METRICS__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ROLE\": \"STAGING\",\n           \"TAP_SNOWFLAKE_TABLES\": \"STAGING_PROD.MELTANO_HUB.FACT_VARIANT_HUB_METRICS\",\n           \"TAP_SNOWFLAKE_USER\": \"STAGING\",\n@@ -1440,7 +1467,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1489,6 +1517,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_DBNAME\": \"STAGING_PROD\",\n@@ -1503,6 +1532,7 @@\n           \"TAP_SNOWFLAKE_AUDIT__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_AUDIT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"STAGING_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_ROLE\": \"STAGING\",\n@@ -1514,7 +1544,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1546,6 +1577,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"STAGING_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1563,6 +1595,7 @@\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_SINGER_ACTIVITY__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_TABLES\": \"STAGING_PROD.SLACK_NOTIFICATIONS.SLACK_ALERTS\",\n           \"TAP_SNOWFLAKE_USER\": \"STAGING\",\n           \"TAP_SNOWFLAKE_WAREHOUSE\": \"STAGING\",\n@@ -1571,7 +1604,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\ndiff --git a/integration/meltano-manifest/expected-manifests/meltano-manifest.userdev.json b/integration/meltano-manifest/expected-manifests/meltano-manifest.userdev.json\nindex 06c2cce4f2..2ef6a1da4c 100644\n--- a/integration/meltano-manifest/expected-manifests/meltano-manifest.userdev.json\n+++ b/integration/meltano-manifest/expected-manifests/meltano-manifest.userdev.json\n@@ -192,6 +192,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_CLOUDWATCH_AWS_REGION_NAME\": \"us-west-2\",\n           \"TAP_CLOUDWATCH_BATCH_INCREMENT_S\": \"3600\",\n           \"TAP_CLOUDWATCH_LOG_GROUP_NAME\": \"API-Gateway-Execution-Logs_i32s35df22/prod\",\n@@ -201,7 +202,8 @@\n           \"TAP_CLOUDWATCH__METADATA\": \"{}\",\n           \"TAP_CLOUDWATCH__SCHEMA\": \"{}\",\n           \"TAP_CLOUDWATCH__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\"\n+          \"TAP_CLOUDWATCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_CLOUDWATCH__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"executable\": \"tap-cloudwatch\",\n         \"label\": \"Cloudwatch\",\n@@ -369,11 +371,13 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"plugins.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_MELTANOHUB__LOAD_SCHEMA\": \"tap_meltanohub\",\n           \"TAP_MELTANOHUB__METADATA\": \"{}\",\n           \"TAP_MELTANOHUB__SCHEMA\": \"{}\",\n           \"TAP_MELTANOHUB__SELECT\": \"[\\\"plugins.*\\\"]\",\n-          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_MELTANOHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_MELTANOHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Meltanohub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/meltanohub.png\",\n@@ -473,12 +477,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SPREADSHEETS_ANYWHERE_TABLES\": \"[{\\\"path\\\": \\\"https://ip-ranges.amazonaws.com\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"ip_prefix\\\"], \\\"name\\\": \\\"aws_ips\\\", \\\"pattern\\\": \\\"ip-ranges.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://www.gstatic.com/ipranges\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"gcp_ips\\\", \\\"pattern\\\": \\\"cloud.json\\\", \\\"json_path\\\": \\\"prefixes\\\"}, {\\\"path\\\": \\\"https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63\\\", \\\"format\\\": \\\"json\\\", \\\"start_date\\\": \\\"2020-01-01T00:00:00Z\\\", \\\"key_properties\\\": [\\\"id\\\"], \\\"name\\\": \\\"azure_ips\\\", \\\"pattern\\\": \\\"ServiceTags_Public_20230116.json\\\", \\\"json_path\\\": \\\"values\\\"}]\",\n           \"TAP_SPREADSHEETS_ANYWHERE__LOAD_SCHEMA\": \"tap_spreadsheets_anywhere\",\n           \"TAP_SPREADSHEETS_ANYWHERE__METADATA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SCHEMA\": \"{}\",\n           \"TAP_SPREADSHEETS_ANYWHERE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SPREADSHEETS_ANYWHERE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SPREADSHEETS_ANYWHERE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Spreadsheets Anywhere\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/spreadsheets-anywhere.png\",\n@@ -535,6 +541,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"private_channel\\\"]\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[\\\"C01SK13R9NJ\\\"]\",\n@@ -544,7 +551,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"users.*\\\", \\\"channels.*\\\", \\\"messages.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Slack\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/slack.png\",\n@@ -704,6 +712,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_AUTO_JOIN_CHANNELS\": \"false\",\n           \"TAP_SLACK_CHANNEL_TYPES\": \"[\\\"public_channel\\\"]\",\n           \"TAP_SLACK_PUBLIC_AUTO_JOIN_CHANNELS\": \"false\",\n@@ -716,6 +725,7 @@\n           \"TAP_SLACK_PUBLIC__SCHEMA\": \"{}\",\n           \"TAP_SLACK_PUBLIC__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n           \"TAP_SLACK_PUBLIC__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK_PUBLIC__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SLACK_SELECTED_CHANNELS\": \"[]\",\n           \"TAP_SLACK_START_DATE\": \"2022-09-01\",\n           \"TAP_SLACK_THREAD_LOOKBACK_DAYS\": \"1\",\n@@ -723,7 +733,8 @@\n           \"TAP_SLACK__METADATA\": \"{}\",\n           \"TAP_SLACK__SCHEMA\": \"{}\",\n           \"TAP_SLACK__SELECT\": \"[\\\"channels.*\\\", \\\"messages.*\\\", \\\"threads.*\\\"]\",\n-          \"TAP_SLACK__SELECT_FILTER\": \"[]\"\n+          \"TAP_SLACK__SELECT_FILTER\": \"[]\",\n+          \"TAP_SLACK__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-slack\",\n         \"name\": \"tap-slack-public\",\n@@ -758,6 +769,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GOOGLE_ANALYTICS_REPORTS\": \"./extract/ga_reports/cli_events_report_definition.json\",\n           \"TAP_GOOGLE_ANALYTICS_START_DATE\": \"2022-02-20\",\n           \"TAP_GOOGLE_ANALYTICS_VIEW_ID\": \"188392047\",\n@@ -765,7 +777,8 @@\n           \"TAP_GOOGLE_ANALYTICS__METADATA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SCHEMA\": \"{}\",\n           \"TAP_GOOGLE_ANALYTICS__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\"\n+          \"TAP_GOOGLE_ANALYTICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_GOOGLE_ANALYTICS__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Google Analytics (Universal Analytics API - Deprecated)\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/google-analytics.png\",\n@@ -912,6 +925,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITLAB_API_URL\": \"https://gitlab.com\",\n           \"TAP_GITLAB_FETCH_MERGE_REQUEST_COMMITS\": \"false\",\n           \"TAP_GITLAB_FETCH_PIPELINES_EXTENDED\": \"false\",\n@@ -924,7 +938,8 @@\n           \"TAP_GITLAB__METADATA\": \"{}\",\n           \"TAP_GITLAB__SCHEMA\": \"{}\",\n           \"TAP_GITLAB__SELECT\": \"[\\\"projects.*\\\", \\\"merge_requests.*\\\", \\\"issues.*\\\", \\\"!issues.description\\\", \\\"!issues.title\\\", \\\"!merge_requests.description\\\", \\\"!merge_requests.title\\\"]\",\n-          \"TAP_GITLAB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITLAB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITLAB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitLab\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/gitlab.png\",\n@@ -1034,12 +1049,14 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2021-05-01\",\n           \"TAP_GITHUB__LOAD_SCHEMA\": \"tap_github\",\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"*.*\\\", \\\"!traffic_*.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"GitHub\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/github.png\",\n@@ -1187,6 +1204,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_MELTANO_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_MELTANO_START_DATE\": \"2021-05-01\",\n           \"TAP_GITHUB_MELTANO_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n@@ -1196,6 +1214,7 @@\n           \"TAP_GITHUB_MELTANO__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_MELTANO__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n           \"TAP_GITHUB_MELTANO__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_MELTANO__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_ORGANIZATIONS\": \"[\\\"MeltanoLabs\\\", \\\"meltano\\\"]\",\n           \"TAP_GITHUB_START_DATE\": \"2021-05-01\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n@@ -1204,7 +1223,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"pull_requests.*\\\", \\\"issues.*\\\", \\\"!issues.body\\\", \\\"!issues.title\\\", \\\"!pull_requests.body\\\", \\\"!pull_requests.title\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-meltano\",\n@@ -1253,6 +1273,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_METRICS_LOG_LEVEL\": \"DEBUG\",\n           \"TAP_GITHUB_SEARCHES\": \"[{\\\"name\\\": \\\"tap non-forks\\\", \\\"query\\\": \\\"tap- fork:false language:Python singer in:readme\\\"}, {\\\"name\\\": \\\"target non-forks\\\", \\\"query\\\": \\\"target- fork:false language:Python singer in:readme\\\"}]\",\n           \"TAP_GITHUB_SEARCH_METRICS_LOG_LEVEL\": \"DEBUG\",\n@@ -1266,6 +1287,7 @@\n           \"TAP_GITHUB_SEARCH__SCHEMA\": \"{}\",\n           \"TAP_GITHUB_SEARCH__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n           \"TAP_GITHUB_SEARCH__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB_SEARCH__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_GITHUB_START_DATE\": \"2022-02-13\",\n           \"TAP_GITHUB_STREAM_MAPS\": \"{\\\"repositories.__filter__\\\": \\\"('tap-' in name or 'target-' in full_name) and name != 'singer-tap-template' and name != 'singer-target-template'\\\\n\\\", \\\"issues.__filter__\\\": \\\"record['type'] = 'issue'\\\"}\",\n           \"TAP_GITHUB_STREAM_MAPS_ISSUES___FILTER__\": \"record['type'] = 'issue'\",\n@@ -1274,7 +1296,8 @@\n           \"TAP_GITHUB__METADATA\": \"{}\",\n           \"TAP_GITHUB__SCHEMA\": \"{}\",\n           \"TAP_GITHUB__SELECT\": \"[\\\"repositories.*\\\", \\\"readme.*\\\", \\\"issues.*\\\", \\\"pull_requests.*\\\"]\",\n-          \"TAP_GITHUB__SELECT_FILTER\": \"[]\"\n+          \"TAP_GITHUB__SELECT_FILTER\": \"[]\",\n+          \"TAP_GITHUB__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-github\",\n         \"name\": \"tap-github-search\",\n@@ -1313,6 +1336,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*.*\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"USERDEV_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1323,7 +1347,8 @@\n           \"TAP_SNOWFLAKE__METADATA\": \"{}\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*.*\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"label\": \"Snowflake\",\n         \"logo_url\": \"https://hub.meltano.com/assets/logos/extractors/snowflake.png\",\n@@ -1426,6 +1451,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"USERDEV_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1442,6 +1468,7 @@\n           \"TAP_SNOWFLAKE_METRICS__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n           \"TAP_SNOWFLAKE_METRICS__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_METRICS__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ROLE\": \"PNADOLNY\",\n           \"TAP_SNOWFLAKE_TABLES\": \"USERDEV_PROD.PNADOLNY_MELTANO_HUB.FACT_VARIANT_HUB_METRICS\",\n           \"TAP_SNOWFLAKE_USER\": \"PNADOLNY\",\n@@ -1451,7 +1478,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*FACT_VARIANT_HUB_METRICS.NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.REPO\\\", \\\"*FACT_VARIANT_HUB_METRICS.PLUGIN_TYPE\\\", \\\"*FACT_VARIANT_HUB_METRICS.PIP_URL\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_PROJECTS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.ALL_EXECS_UNSTRUCT_BY_VARIANT\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_NAME\\\", \\\"*FACT_VARIANT_HUB_METRICS.SUCCESS_EXECS_UNSTRUCT_BY_VARIANT\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1500,6 +1528,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_AUDIT_DBNAME\": \"USERDEV_PROD\",\n@@ -1514,6 +1543,7 @@\n           \"TAP_SNOWFLAKE_AUDIT__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n           \"TAP_SNOWFLAKE_AUDIT__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_AUDIT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"USERDEV_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n           \"TAP_SNOWFLAKE_ROLE\": \"PNADOLNY\",\n@@ -1525,7 +1555,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*HUB_METRICS_AUDIT.UPDATED_DATE\\\", \\\"*HUB_METRICS_AUDIT.METRIC_TYPE\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\n@@ -1557,6 +1588,7 @@\n           \"MELTANO_EXTRACT__SCHEMA\": \"{}\",\n           \"MELTANO_EXTRACT__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"MELTANO_EXTRACT__SELECT_FILTER\": \"[]\",\n+          \"MELTANO_EXTRACT__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_ACCOUNT\": \"epa06486\",\n           \"TAP_SNOWFLAKE_DBNAME\": \"USERDEV_PROD\",\n           \"TAP_SNOWFLAKE_INSECURE_MODE\": \"False\",\n@@ -1574,6 +1606,7 @@\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n           \"TAP_SNOWFLAKE_SINGER_ACTIVITY__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE_SINGER_ACTIVITY__USE_CACHED_CATALOG\": \"true\",\n           \"TAP_SNOWFLAKE_TABLES\": \"USERDEV_PROD.PNADOLNY_SLACK_NOTIFICATIONS.SLACK_ALERTS\",\n           \"TAP_SNOWFLAKE_USER\": \"PNADOLNY\",\n           \"TAP_SNOWFLAKE_WAREHOUSE\": \"CORE\",\n@@ -1582,7 +1615,8 @@\n           \"TAP_SNOWFLAKE__METADATA___REPLICATION_METHOD\": \"FULL_TABLE\",\n           \"TAP_SNOWFLAKE__SCHEMA\": \"{}\",\n           \"TAP_SNOWFLAKE__SELECT\": \"[\\\"*SLACK_ALERTS.TITLE\\\", \\\"*SLACK_ALERTS.BODY\\\"]\",\n-          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\"\n+          \"TAP_SNOWFLAKE__SELECT_FILTER\": \"[]\",\n+          \"TAP_SNOWFLAKE__USE_CACHED_CATALOG\": \"true\"\n         },\n         \"inherit_from\": \"tap-snowflake\",\n         \"metadata\": {\ndiff --git a/src/meltano/cli/elt.py b/src/meltano/cli/elt.py\nindex 06d7061af9..44031d33f3 100644\n--- a/src/meltano/cli/elt.py\n+++ b/src/meltano/cli/elt.py\n@@ -57,6 +57,11 @@ class ELOptions:\n         help=\"Perform a full refresh (ignore state left behind by any previous runs).\",\n         is_flag=True,\n     )\n+    refresh_catalog = click.option(\n+        \"--refresh-catalog\",\n+        help=\"Invalidates catalog cache and forces running discovery before this run.\",\n+        is_flag=True,\n+    )\n     select = click.option(\n         \"--select\",\n         \"-s\",\n@@ -108,6 +113,7 @@ class ELOptions:\n @ELOptions.loader\n @ELOptions.dry\n @ELOptions.full_refresh\n+@ELOptions.refresh_catalog\n @ELOptions.select\n @ELOptions.exclude\n @ELOptions.catalog\n@@ -126,6 +132,7 @@ async def el(  # WPS408\n     loader: str,\n     dry: bool,\n     full_refresh: bool,\n+    refresh_catalog: bool,\n     select: list[str],\n     exclude: list[str],\n     catalog: str,\n@@ -153,6 +160,7 @@ async def el(  # WPS408\n         None,\n         dry,\n         full_refresh,\n+        refresh_catalog,\n         select,\n         exclude,\n         catalog,\n@@ -174,6 +182,7 @@ async def el(  # WPS408\n @ELOptions.transform\n @ELOptions.dry\n @ELOptions.full_refresh\n+@ELOptions.refresh_catalog\n @ELOptions.select\n @ELOptions.exclude\n @ELOptions.catalog\n@@ -193,6 +202,7 @@ async def elt(  # WPS408\n     transform: str,\n     dry: bool,\n     full_refresh: bool,\n+    refresh_catalog: bool,\n     select: list[str],\n     exclude: list[str],\n     catalog: str,\n@@ -221,6 +231,7 @@ async def elt(  # WPS408\n         transform,\n         dry,\n         full_refresh,\n+        refresh_catalog,\n         select,\n         exclude,\n         catalog,\n@@ -240,6 +251,7 @@ async def _run_el_command(\n     transform: str | None,\n     dry: bool,\n     full_refresh: bool,\n+    refresh_catalog: bool,\n     select: list[str],\n     exclude: list[str],\n     catalog: str,\n@@ -285,6 +297,7 @@ async def _run_el_command(\n             transform,\n             dry_run=dry,\n             full_refresh=full_refresh,\n+            refresh_catalog=refresh_catalog,\n             select_filter=select_filter,\n             catalog=catalog,\n             state=state,\n@@ -313,6 +326,7 @@ def _elt_context_builder(\n     transform,\n     dry_run=False,\n     full_refresh=False,\n+    refresh_catalog=False,\n     select_filter=None,\n     catalog=None,\n     state=None,\n@@ -333,6 +347,7 @@ def _elt_context_builder(\n         .with_dry_run(dry_run)\n         .with_only_transform(transform == \"only\")\n         .with_full_refresh(full_refresh)\n+        .with_refresh_catalog(refresh_catalog)\n         .with_select_filter(select_filter)\n         .with_catalog(catalog)\n         .with_state(state)\ndiff --git a/src/meltano/cli/run.py b/src/meltano/cli/run.py\nindex 4bd3aba289..e40ba6a0ff 100644\n--- a/src/meltano/cli/run.py\n+++ b/src/meltano/cli/run.py\n@@ -61,6 +61,11 @@ def convert(self, value, param, ctx):\n     ),\n     is_flag=True,\n )\n+@click.option(\n+    \"--refresh-catalog\",\n+    help=\"Invalidates catalog cache and forces running discovery before this run.\",\n+    is_flag=True,\n+)\n @click.option(\n     \"--no-state-update\",\n     help=\"Run without state saving. Applies to all pipelines.\",\n@@ -101,6 +106,7 @@ async def run(\n     project: Project,\n     dry_run: bool,\n     full_refresh: bool,\n+    refresh_catalog: bool,\n     no_state_update: bool,\n     force: bool,\n     state_id_suffix: str,\n@@ -143,6 +149,7 @@ async def run(\n             project,\n             blocks,\n             full_refresh=full_refresh,\n+            refresh_catalog=refresh_catalog,\n             no_state_update=no_state_update,\n             force=force,\n             state_id_suffix=state_id_suffix,\ndiff --git a/src/meltano/cli/select.py b/src/meltano/cli/select.py\nindex 426a0035ed..7cf723a943 100644\n--- a/src/meltano/cli/select.py\n+++ b/src/meltano/cli/select.py\n@@ -57,6 +57,11 @@ def selection_mark(selection):\n     is_flag=True,\n     help=\"Show all the tap attributes with their selected status.\",\n )\n+@click.option(\n+    \"--refresh-catalog\",\n+    is_flag=True,\n+    help=\"Invalidate the catalog cache and refresh the catalog.\",\n+)\n @click.option(\n     \"--rm\",\n     \"--remove\",\n@@ -85,7 +90,12 @@ async def select(\n     \"\"\"\n     try:\n         if flags[\"list\"]:\n-            await show(project, extractor, show_all=flags[\"all\"])\n+            await show(\n+                project,\n+                extractor,\n+                show_all=flags[\"all\"],\n+                refresh=flags[\"refresh_catalog\"],\n+            )\n         else:\n             update(\n                 project,\n@@ -112,13 +122,18 @@ def update(\n     select_service.update(entities_filter, attributes_filter, exclude, remove)\n \n \n-async def show(project, extractor, show_all=False):\n+async def show(\n+    project: Project,\n+    extractor: str,\n+    show_all: bool = False,\n+    refresh: bool = False,\n+) -> None:\n     \"\"\"Show selected.\"\"\"\n     _, Session = project_engine(project)  # noqa: N806\n     select_service = SelectService(project, extractor)\n \n     with closing(Session()) as session:\n-        list_all = await select_service.list_all(session)\n+        list_all = await select_service.list_all(session, refresh)\n \n     # legend\n     click.secho(\"Legend:\")\ndiff --git a/src/meltano/core/block/extract_load.py b/src/meltano/core/block/extract_load.py\nindex af552c85aa..c3803b3634 100644\n--- a/src/meltano/core/block/extract_load.py\n+++ b/src/meltano/core/block/extract_load.py\n@@ -52,6 +52,7 @@ def __init__(\n         session: Session | None = None,\n         job: Job | None = None,\n         full_refresh: bool | None = False,\n+        refresh_catalog: bool | None = False,\n         force: bool | None = False,\n         update_state: bool | None = True,\n         state_id_suffix: str | None = None,\n@@ -66,6 +67,7 @@ def __init__(\n             session: The session to use.\n             job: The job within this context should run.\n             full_refresh: Whether this is a full refresh.\n+            refresh_catalog: whether cached catalog should be ignored.\n             force: Whether to force the execution of the job if it is stale.\n             update_state: Whether to update the state of the job.\n             state_id_suffix: The state ID suffix to use.\n@@ -77,6 +79,7 @@ def __init__(\n         self.session = session\n         self.job = job\n         self.full_refresh = full_refresh\n+        self.refresh_catalog = refresh_catalog\n         self.force = force\n         self.update_state = update_state\n         self.state_id_suffix = state_id_suffix\n@@ -120,6 +123,7 @@ def __init__(self, project: Project):\n \n         self._job = None\n         self._full_refresh = False\n+        self._refresh_catalog = False\n         self._state_update = True\n         self._force = False\n         self._state_id_suffix = None\n@@ -167,6 +171,18 @@ def with_full_refresh(self, full_refresh: bool):\n         self._full_refresh = full_refresh\n         return self\n \n+    def with_refresh_catalog(self, refresh_catalog: bool):\n+        \"\"\"Set whether cached catalog should be ignored.\n+\n+        Args:\n+            refresh_catalog : whether cached catalog should be ignored.\n+\n+        Returns:\n+            self\n+        \"\"\"\n+        self._refresh_catalog = refresh_catalog\n+        return self\n+\n     def with_no_state_update(self, no_state_update: bool):\n         \"\"\"Set whether this run should not update state.\n \n@@ -309,6 +325,7 @@ def context(self) -> ELBContext:\n             session=self.session,\n             job=self._job,\n             full_refresh=self._full_refresh,\n+            refresh_catalog=self._refresh_catalog,\n             force=self._force,\n             update_state=self._state_update,\n             state_id_suffix=self._state_id_suffix,\ndiff --git a/src/meltano/core/block/parser.py b/src/meltano/core/block/parser.py\nindex 716e43b1ed..5428f9ad86 100644\n--- a/src/meltano/core/block/parser.py\n+++ b/src/meltano/core/block/parser.py\n@@ -72,6 +72,7 @@ def __init__(\n         project,\n         blocks: list[str],\n         full_refresh: bool | None = False,\n+        refresh_catalog: bool | None = False,\n         no_state_update: bool | None = False,\n         force: bool | None = False,\n         state_id_suffix: str | None = None,\n@@ -87,6 +88,7 @@ def __init__(\n             blocks: List of block names to parse.\n             full_refresh: Whether to perform a full refresh (applies to all\n                 found sets).\n+            refresh_catalog: Whether to ignore cached catalog.\n             no_state_update: Whether to run with or without state updates.\n             force: Whether to force a run if a job is already running (applies\n                 to all found sets).\n@@ -101,6 +103,7 @@ def __init__(\n         self.project = project\n \n         self._full_refresh = full_refresh\n+        self._refresh_catalog = refresh_catalog\n         self._no_state_update = no_state_update\n         self._force = force\n         self._state_id_suffix = state_id_suffix\n@@ -246,6 +249,7 @@ def _find_next_elb_set(  # noqa: WPS231, WPS213\n             ELBContextBuilder(self.project)\n             .with_force(self._force)\n             .with_full_refresh(self._full_refresh)\n+            .with_refresh_catalog(self._refresh_catalog)\n             .with_no_state_update(self._no_state_update)\n             .with_state_id_suffix(self._state_id_suffix)\n             .with_merge_state(self._merge_state)\ndiff --git a/src/meltano/core/elt_context.py b/src/meltano/core/elt_context.py\nindex cbdaea26ce..eece257959 100644\n--- a/src/meltano/core/elt_context.py\n+++ b/src/meltano/core/elt_context.py\n@@ -85,7 +85,7 @@ class ELTContext:  # noqa: WPS230\n \n     def __init__(\n         self,\n-        project,\n+        project: Project,\n         job: Job | None = None,\n         session=None,\n         extractor: PluginContext | None = None,\n@@ -95,6 +95,7 @@ def __init__(\n         only_transform: bool | None = False,\n         dry_run: bool | None = False,\n         full_refresh: bool | None = False,\n+        refresh_catalog: bool | None = False,\n         select_filter: list | None = None,\n         catalog: str | None = None,\n         state: str | None = None,\n@@ -114,6 +115,7 @@ def __init__(\n             only_transform: Flag. Only run transform.\n             dry_run: Flag. Don't actually run.\n             full_refresh: Flag. Ignore previous captured state.\n+            refresh_catalog: Flag. Ignore cached catalog.\n             select_filter: Select filters to apply to extractor.\n             catalog: Catalog to pass to extractor.\n             state: State to pass to extractor.\n@@ -132,6 +134,7 @@ def __init__(\n         self.only_transform = only_transform\n         self.dry_run = dry_run\n         self.full_refresh = full_refresh\n+        self.refresh_catalog = refresh_catalog\n         self.select_filter = select_filter or []\n         self.catalog = catalog\n         self.state = state\n@@ -220,6 +223,7 @@ def __init__(self, project: Project):\n         self._only_transform = False\n         self._dry_run = False\n         self._full_refresh = False\n+        self._refresh_catalog = False\n         self._select_filter = None\n         self._catalog = None\n         self._state = None\n@@ -340,6 +344,18 @@ def with_full_refresh(self, full_refresh: bool) -> ELTContextBuilder:\n         self._full_refresh = full_refresh\n         return self\n \n+    def with_refresh_catalog(self, refresh_catalog: bool) -> ELTContextBuilder:\n+        \"\"\"Ignore cached catalog.\n+\n+        Args:\n+            refresh_catalog: Whether ignore cached catalog.\n+\n+        Returns:\n+            Updated ELTContextBuilder instance.\n+        \"\"\"\n+        self._refresh_catalog = refresh_catalog\n+        return self\n+\n     def with_merge_state(self, merge_state: bool):\n         \"\"\"Set whether the state is to be merged or overwritten.\n \n@@ -502,6 +518,7 @@ def context(self) -> ELTContext:\n             only_transform=self._only_transform,\n             dry_run=self._dry_run,\n             full_refresh=self._full_refresh,\n+            refresh_catalog=self._refresh_catalog,\n             select_filter=self._select_filter,\n             catalog=self._catalog,\n             state=self._state,\ndiff --git a/src/meltano/core/plugin/singer/tap.py b/src/meltano/core/plugin/singer/tap.py\nindex fb0b2bdbeb..5d48c2558b 100644\n--- a/src/meltano/core/plugin/singer/tap.py\n+++ b/src/meltano/core/plugin/singer/tap.py\n@@ -185,6 +185,11 @@ class SingerTap(SingerPlugin):  # noqa: WPS214\n             value_processor=\"nest_object\",\n         ),\n         SettingDefinition(name=\"_select_filter\", kind=SettingKind.ARRAY, value=[]),\n+        SettingDefinition(\n+            name=\"_use_cached_catalog\",\n+            kind=SettingKind.BOOLEAN,\n+            value=True,\n+        ),\n     ]\n \n     def exec_args(self, plugin_invoker):\n@@ -375,7 +380,17 @@ async def discover_catalog(  # noqa: WPS231, WPS210,\n         \"\"\"\n         catalog_path = plugin_invoker.files[\"catalog\"]\n         catalog_cache_key_path = plugin_invoker.files[\"catalog_cache_key\"]\n-        if catalog_path.exists():\n+        elt_context = plugin_invoker.context\n+\n+        use_catalog_cache = True\n+        if (\n+            elt_context\n+            and elt_context.refresh_catalog\n+            or not plugin_invoker.plugin_config_extras[\"_use_cached_catalog\"]\n+        ):\n+            use_catalog_cache = False\n+\n+        if catalog_path.exists() and use_catalog_cache:\n             with suppress(FileNotFoundError):\n                 cached_key = catalog_cache_key_path.read_text()\n                 new_cache_key = self.catalog_cache_key(plugin_invoker)\ndiff --git a/src/meltano/core/select_service.py b/src/meltano/core/select_service.py\nindex 6b430c6cd6..6a92ef5686 100644\n--- a/src/meltano/core/select_service.py\n+++ b/src/meltano/core/select_service.py\n@@ -39,19 +39,22 @@ def current_select(self):\n         plugin_settings_service = PluginSettingsService(self.project, self.extractor)\n         return plugin_settings_service.get(\"_select\")\n \n-    async def load_catalog(self, session):\n+    async def load_catalog(self, session, refresh=False):\n         \"\"\"Load the catalog.\"\"\"\n         invoker = invoker_factory(self.project, self.extractor)\n \n+        if refresh:\n+            invoker.settings_service.config_override[\"_use_cached_catalog\"] = False\n+\n         async with invoker.prepared(session):\n             catalog_json = await invoker.dump(\"catalog\")\n \n         return json.loads(catalog_json)\n \n-    async def list_all(self, session) -> ListSelectedExecutor:\n+    async def list_all(self, session, refresh=False) -> ListSelectedExecutor:\n         \"\"\"List all select.\"\"\"\n         try:\n-            catalog = await self.load_catalog(session)\n+            catalog = await self.load_catalog(session, refresh)\n         except FileNotFoundError as err:\n             raise PluginExecutionError(\n                 \"Could not find catalog. Verify that the tap supports discovery \"  # noqa: EM101\ndiff --git a/src/meltano/schemas/meltano.schema.json b/src/meltano/schemas/meltano.schema.json\nindex e7da43799d..606e4c688a 100644\n--- a/src/meltano/schemas/meltano.schema.json\n+++ b/src/meltano/schemas/meltano.schema.json\n@@ -589,6 +589,11 @@\n               \"type\": \"string\"\n             },\n             \"default\": []\n+          },\n+          \"use_cached_catalog\": {\n+            \"type\": \"boolean\",\n+            \"description\": \"A boolean that determines if the catalog cache should be used or ignored.\",\n+            \"default\": true\n           }\n         }\n       },\n", "instance_id": "meltano__meltano-8580", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a `--refresh` option (or similar CLI argument) to the `meltano select` command to refresh the cached catalog. It provides specific examples of command usage and references related issues for context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the refresh mechanism should interact with existing caching logic or specify edge cases, such as what happens if the refresh fails or if there are concurrent operations. Additionally, the inclusion of unrelated content (e.g., \"Authentication for Meltano Analyze\") in the problem statement introduces noise, slightly detracting from the focus on the catalog refresh feature. Overall, while the intent and basic requirements are clear, some critical details about behavior and constraints are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is significant, as seen in the extensive diffs affecting multiple files, including CLI commands (`elt.py`, `run.py`, `select.py`), core logic (`extract_load.py`, `elt_context.py`, `tap.py`, `select_service.py`), and configuration schemas (`meltano.schema.json`). This requires understanding and modifying interactions across different parts of the Meltano codebase, such as plugin configuration, catalog discovery, and command-line argument handling. Second, the technical concepts involved include familiarity with Meltano's plugin system, catalog caching mechanism, asynchronous programming in Python (e.g., `async def` functions), and configuration management, which adds moderate complexity. Third, while edge cases like cache invalidation failures or concurrent access are not explicitly mentioned, implementing a robust refresh mechanism likely requires considering such scenarios, adding to the complexity of error handling. However, the problem does not appear to impact the system's core architecture fundamentally or require advanced domain-specific knowledge beyond Meltano's framework. Therefore, a difficulty score of 0.45 is assigned, reflecting a medium level of challenge that involves understanding multiple components and making coordinated changes across the codebase, but not reaching the level of deep architectural refactoring or highly intricate logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Confusion about `&` when mixing `cs.by_name` and `pl.col`\n### Checks\n\n- [X] I have checked that this issue has not already been reported.\n- [X] I have confirmed this bug exists on the [latest version](https://pypi.org/project/polars/) of Polars.\n\n### Reproducible example\n\n```python\r\nimport polars as pl\r\nimport polars.selectors as cs\r\n\r\ndf = pl.DataFrame({\r\n    \"a\": [1],\r\n    \"b\": [2]\r\n})\r\n\r\ndf.select(cs.by_name(\"a\") | pl.col(\"b\"))\r\n# shape: (1, 2)\r\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\r\n# \u2502 a   \u2506 b   \u2502\r\n# \u2502 --- \u2506 --- \u2502\r\n# \u2502 i64 \u2506 i64 \u2502\r\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\r\n# \u2502 1   \u2506 2   \u2502\r\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\ndf.select(cs.by_name(\"a\") & pl.col(\"b\"))\r\n# shape: (1, 2)\r\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\r\n# \u2502 a   \u2506 b   \u2502\r\n# \u2502 --- \u2506 --- \u2502\r\n# \u2502 i64 \u2506 i64 \u2502\r\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\r\n# \u2502 1   \u2506 2   \u2502\r\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\n\n### Log output\n\n_No response_\n\n### Issue description\n\nI don't understand some behavior of `&` to intersect selectors and expressions. \r\n\r\nIn the example above, `cs.by_name(\"a\") | pl.col(\"b\")` returns both columns, which makes sense since both match the condition that they are either \"a\" or \"b\". My issue is that `cs.by_name(\"a\") & pl.col(\"b\")` also returns both columns, but IMO it shouldn't since none of the columns is \"a\" and \"b\" at the same time. This comes from this `if` condition:\r\n\r\nhttps://github.com/pola-rs/polars/blob/7f0b3e00845c013222ac8b1094d693af582258c2/py-polars/polars/selectors.py#L386-L392\r\n\r\nI understand there are some issues with selectors, e.g. https://github.com/pola-rs/polars/issues/13757. Is this also an issue or am I misunderstanding?\n\n### Expected behavior\n\n`df.select(cs.by_name(\"a\") & pl.col(\"b\"))` should return 0 cols.\n\n### Installed versions\n\n<details>\r\n\r\n```\r\n--------Version info---------\r\nPolars:              1.12.0\r\nIndex type:          UInt32\r\nPlatform:            Linux-6.8.0-47-generic-x86_64-with-glibc2.39\r\nPython:              3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0]\r\nLTS CPU:             False\r\n\r\n----Optional dependencies----\r\nadbc_driver_manager  <not installed>\r\naltair               <not installed>\r\ncloudpickle          <not installed>\r\nconnectorx           <not installed>\r\ndeltalake            <not installed>\r\nfastexcel            <not installed>\r\nfsspec               <not installed>\r\ngevent               <not installed>\r\ngreat_tables         <not installed>\r\nmatplotlib           <not installed>\r\nnest_asyncio         <not installed>\r\nnumpy                2.1.2\r\nopenpyxl             <not installed>\r\npandas               <not installed>\r\npyarrow              <not installed>\r\npydantic             <not installed>\r\npyiceberg            <not installed>\r\nsqlalchemy           <not installed>\r\ntorch                <not installed>\r\nxlsx2csv             <not installed>\r\nxlsxwriter           <not installed>\r\n```\r\n\r\n</details>\r\n\n", "patch": "diff --git a/py-polars/polars/selectors.py b/py-polars/polars/selectors.py\nindex 4cb11506b3f6..9d3cedb47e85 100644\n--- a/py-polars/polars/selectors.py\n+++ b/py-polars/polars/selectors.py\n@@ -385,10 +385,6 @@ def __and__(self, other: Any) -> Expr: ...\n     def __and__(self, other: Any) -> SelectorType | Expr:\n         if is_column(other):\n             colname = other.meta.output_name()\n-            if self._attrs[\"name\"] == \"by_name\" and (\n-                params := self._attrs[\"params\"]\n-            ).get(\"require_all\", True):\n-                return by_name(*params[\"*names\"], colname)\n             other = by_name(colname)\n         if is_selector(other):\n             return _selector_proxy_(\n@@ -399,6 +395,12 @@ def __and__(self, other: Any) -> SelectorType | Expr:\n         else:\n             return self.as_expr().__and__(other)\n \n+    def __rand__(self, other: Any) -> Expr:\n+        if is_column(other):\n+            colname = other.meta.output_name()\n+            return by_name(colname) & self\n+        return self.as_expr().__rand__(other)\n+\n     @overload\n     def __or__(self, other: SelectorType) -> SelectorType: ...\n \n@@ -417,6 +419,11 @@ def __or__(self, other: Any) -> SelectorType | Expr:\n         else:\n             return self.as_expr().__or__(other)\n \n+    def __ror__(self, other: Any) -> Expr:\n+        if is_column(other):\n+            other = by_name(other.meta.output_name())\n+        return self.as_expr().__ror__(other)\n+\n     @overload\n     def __xor__(self, other: SelectorType) -> SelectorType: ...\n \n@@ -435,21 +442,6 @@ def __xor__(self, other: Any) -> SelectorType | Expr:\n         else:\n             return self.as_expr().__or__(other)\n \n-    def __rand__(self, other: Any) -> Expr:\n-        if is_column(other):\n-            colname = other.meta.output_name()\n-            if self._attrs[\"name\"] == \"by_name\" and (\n-                params := self._attrs[\"params\"]\n-            ).get(\"require_all\", True):\n-                return by_name(colname, *params[\"*names\"])\n-            other = by_name(colname)\n-        return self.as_expr().__rand__(other)\n-\n-    def __ror__(self, other: Any) -> Expr:\n-        if is_column(other):\n-            other = by_name(other.meta.output_name())\n-        return self.as_expr().__ror__(other)\n-\n     def __rxor__(self, other: Any) -> Expr:\n         if is_column(other):\n             other = by_name(other.meta.output_name())\n", "instance_id": "pola-rs__polars-19742", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the behavior of the `&` operator when used with `cs.by_name` and `pl.col` in the Polars library. It provides a reproducible example that demonstrates the unexpected behavior and clearly states the expected outcome (returning 0 columns for the `&` operation). The user also references specific lines of code in the repository, showing an attempt to understand the root cause. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases beyond the provided example, nor does it clarify if the issue applies to other selector operations or combinations. Additionally, while the user's misunderstanding or potential bug is articulated, the statement lacks a deeper exploration of whether this behavior is intentional in the library design or a genuine defect. Overall, it is valid and clear but misses some minor details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting the `selectors.py` file and specifically the logic for handling bitwise operations (`&`, `|`, etc.) on selectors and expressions in the Polars library. The changes involve modifying or removing conditional logic for the `__and__` and `__rand__` methods, which requires understanding how selectors are proxied and converted to expressions. Second, the technical concepts involved include Python's operator overloading, understanding of the Polars library's internal representation of selectors and columns, and the interaction between different types (`SelectorType` and `Expr`). This requires a moderate level of familiarity with the Polars codebase and its design patterns. Third, while the problem statement does not explicitly mention edge cases beyond the provided example, the code changes suggest potential impacts on other selector combinations or operations, necessitating careful consideration of how the modified logic might affect broader functionality. However, the changes do not appear to impact the overall architecture of the system significantly, nor do they require advanced domain-specific knowledge beyond the library's API. Therefore, I assign a difficulty score of 0.45, reflecting a medium level of complexity that requires understanding multiple concepts and making targeted but non-trivial modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Move continuous integration from Travis CI to Github Actions\nOur free travis CI credit ran out a long time ago. With this PR we move the our current CI Pipeline from travis to github actions.\r\n\r\nWith this new pipeline we test for Python versions 3.6 - 3.12. During the process some regressions in our unit tests on older Python versions were discovered such as using the `tuple` instead of `Tuple` in the type annotations. These small fixes are included in this PR.\r\n\r\nAn interesting read regarding costs etc.:\r\nhttps://docs.github.com/en/actions/administering-github-actions/usage-limits-billing-and-administration\r\n\r\nAccording to this, we fall under the free category.\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex cc65586..b80b260 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -6,12 +6,168 @@ nasaValve\n rel_research\n PyNomaly/loop_dev.py\n /PyNomaly.egg-info/\n-.pytest_cache\n-build\n-htmlcov/\n-*.egg\n *.pyc\n-.coverage\n *.coverage.*\n .coveragerc\n-venv/\n\\ No newline at end of file\n+\n+# Byte-compiled / optimized / DLL files\n+__pycache__/\n+*.py[cod]\n+*$py.class\n+\n+# C extensions\n+*.so\n+\n+# Distribution / packaging\n+.Python\n+build/\n+develop-eggs/\n+dist/\n+downloads/\n+eggs/\n+.eggs/\n+lib/\n+lib64/\n+parts/\n+sdist/\n+var/\n+wheels/\n+share/python-wheels/\n+*.egg-info/\n+.installed.cfg\n+*.egg\n+MANIFEST\n+\n+# PyInstaller\n+#  Usually these files are written by a python script from a template\n+#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n+*.manifest\n+*.spec\n+\n+# Installer logs\n+pip-log.txt\n+pip-delete-this-directory.txt\n+\n+# Unit test / coverage reports\n+htmlcov/\n+.tox/\n+.nox/\n+.coverage\n+.coverage.*\n+.cache\n+nosetests.xml\n+coverage.xml\n+*.cover\n+*.py,cover\n+.hypothesis/\n+.pytest_cache/\n+cover/\n+\n+# Translations\n+*.mo\n+*.pot\n+\n+# Django stuff:\n+*.log\n+local_settings.py\n+db.sqlite3\n+db.sqlite3-journal\n+\n+# Flask stuff:\n+instance/\n+.webassets-cache\n+\n+# Scrapy stuff:\n+.scrapy\n+\n+# Sphinx documentation\n+docs/_build/\n+\n+# PyBuilder\n+.pybuilder/\n+target/\n+\n+# Jupyter Notebook\n+.ipynb_checkpoints\n+\n+# IPython\n+profile_default/\n+ipython_config.py\n+\n+# pyenv\n+#   For a library or package, you might want to ignore these files since the code is\n+#   intended to run in multiple environments; otherwise, check them in:\n+# .python-version\n+\n+# pipenv\n+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n+#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n+#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n+#   install all needed dependencies.\n+#Pipfile.lock\n+\n+# poetry\n+#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n+#   This is especially recommended for binary packages to ensure reproducibility, and is more\n+#   commonly ignored for libraries.\n+#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n+#poetry.lock\n+\n+# pdm\n+#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n+#pdm.lock\n+#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n+#   in version control.\n+#   https://pdm.fming.dev/#use-with-ide\n+.pdm.toml\n+\n+# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n+__pypackages__/\n+\n+# Celery stuff\n+celerybeat-schedule\n+celerybeat.pid\n+\n+# SageMath parsed files\n+*.sage.py\n+\n+# Environments\n+.env\n+.venv\n+env/\n+venv/\n+ENV/\n+env.bak/\n+venv.bak/\n+\n+# Spyder project settings\n+.spyderproject\n+.spyproject\n+\n+# Rope project settings\n+.ropeproject\n+\n+# mkdocs documentation\n+/site\n+\n+# mypy\n+.mypy_cache/\n+.dmypy.json\n+dmypy.json\n+\n+# Pyre type checker\n+.pyre/\n+\n+# pytype static type analyzer\n+.pytype/\n+\n+# Cython debug symbols\n+cython_debug/\n+\n+# PyCharm\n+#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n+#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n+#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n+#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n+#.idea/\n+\ndiff --git a/readme.md b/readme.md\nindex 1a23f85..77b1724 100644\n--- a/readme.md\n+++ b/readme.md\n@@ -7,7 +7,7 @@ scores in the range of [0,1] that are directly interpretable as the probability\n [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n [![PyPi](https://img.shields.io/badge/pypi-0.3.3-blue.svg)](https://pypi.python.org/pypi/PyNomaly/0.3.3)\n ![](https://img.shields.io/pypi/dm/PyNomaly.svg?logoColor=blue)\n-[![Build Status](https://travis-ci.org/vc1492a/PyNomaly.svg?branch=main)](https://travis-ci.org/vc1492a/PyNomaly)\n+![Tests](https://github.com/vc1492a/PyNomaly/actions/workflows/tests.yml/badge.svg)\n [![Coverage Status](https://coveralls.io/repos/github/vc1492a/PyNomaly/badge.svg?branch=main)](https://coveralls.io/github/vc1492a/PyNomaly?branch=main)\n [![JOSS](http://joss.theoj.org/papers/f4d2cfe680768526da7c1f6a2c103266/status.svg)](http://joss.theoj.org/papers/f4d2cfe680768526da7c1f6a2c103266)\n \n", "instance_id": "vc1492a__PyNomaly-59", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to migrate the continuous integration (CI) pipeline from Travis CI to GitHub Actions. It specifies the motivation (expired Travis CI credits), the scope of testing (Python versions 3.6 to 3.12), and mentions minor fixes for regressions in unit tests. Additionally, it provides a reference to GitHub Actions documentation for cost and usage limits. However, there are minor ambiguities and missing details. For instance, it does not explicitly outline the specific steps or configurations required for the migration, nor does it detail the exact nature of the regressions or fixes beyond a single example (`tuple` vs. `Tuple` in type annotations). There is also no mention of potential challenges, edge cases, or specific requirements for compatibility across Python versions. While the goal is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes primarily involve updates to `.gitignore` (expanding the list of ignored files) and `readme.md` (updating the CI badge from Travis CI to GitHub Actions). However, the problem statement implies additional changes, such as setting up a GitHub Actions workflow file (not shown in the diff) and fixing regressions in unit tests. These changes likely affect a limited number of files and do not appear to impact the core architecture of the codebase. The overall amount of code change seems moderate, focusing on configuration and minor code fixes rather than deep structural modifications.\n\n2. **Technical Concepts Required:** Solving this problem requires familiarity with CI/CD pipelines, specifically Travis CI and GitHub Actions configurations. It also involves understanding Python type annotations and compatibility issues across different Python versions (3.6 to 3.12). While setting up a GitHub Actions workflow involves learning YAML syntax and GitHub-specific configurations, these are relatively straightforward concepts for someone with basic to intermediate experience in software development and DevOps. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Edge Cases and Error Handling:** The problem statement mentions regressions in unit tests on older Python versions, which implies the need to handle compatibility issues. However, no specific edge cases or complex error handling requirements are detailed. The fixes (e.g., updating type annotations) appear to be minor and localized, suggesting low complexity in this area.\n\n4. **Overall Complexity:** The task requires understanding CI migration and making targeted changes to configuration files and potentially test code. While it involves some learning (e.g., GitHub Actions syntax) and debugging (e.g., fixing Python version compatibility), it does not demand deep architectural changes or advanced technical expertise. The primary challenge lies in ensuring the new CI pipeline works correctly across all specified Python versions, which adds a slight layer of complexity but remains manageable.\n\nGiven these considerations, a difficulty score of 0.35 reflects an \"Easy\" task that requires moderate effort to understand CI tools and make targeted modifications, with minimal impact on the broader codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove Python 3.6 support\nRemoves all references to the deprecated 3.6 version, updating documentation and tests to account for the new baseline. This is a deliberately standalone PR, as any functional changes that'd come from it are better suited for evaluation in an isolated environment.\r\n\r\n## Contributor Checklist:\r\n\r\n* [x] I have created a new test or updated the unit tests to cover the new/changed functionality.\r\n* [x] I have updated `CHANGES.txt` and `RELEASE.txt` (and read the `README.rst`).\r\n* [x] I have updated the appropriate documentation\r\n\n", "patch": "diff --git a/CHANGES.txt b/CHANGES.txt\nindex 478b67d6d0..808e2b96c9 100644\n--- a/CHANGES.txt\n+++ b/CHANGES.txt\n@@ -78,6 +78,10 @@ RELEASE  VERSION/DATE TO BE FILLED IN LATER\n \n   From Thaddeus Crews:\n     - Ruff/Mypy: Excluded items now synced.\n+    - Ruff: Linter includes new rules - `FA`, `UP006`, `UP007`, and `UP037` - to\n+      detect and upgrade legacy type-hint syntax.\n+    - Removed \"SCons.Util.sctyping.py\", as the functionality can now be substituted\n+      via top-level `from __future__ import annotations`.\n \n   From Alex James:\n     - On Darwin, PermissionErrors are now handled while trying to access\ndiff --git a/RELEASE.txt b/RELEASE.txt\nindex ae1b90be55..8ca51871b7 100644\n--- a/RELEASE.txt\n+++ b/RELEASE.txt\n@@ -179,6 +179,12 @@ DEVELOPMENT\n \n - Ruff/Mypy: Excluded items now synced.\n \n+- Ruff: Linter includes new rules - `FA`, `UP006`, `UP007`, and `UP037` - to\n+  detect and upgrade legacy type-hint syntax.\n+\n+- Removed \"SCons.Util.sctyping.py\", as the functionality can now be substituted\n+  via top-level `from __future__ import annotations`.\n+\n Thanks to the following contributors listed below for their contributions to this release.\n ==========================================================================================\n .. code-block:: text\ndiff --git a/SCons/Action.py b/SCons/Action.py\nindex 567f66cef4..6022b19b38 100644\n--- a/SCons/Action.py\n+++ b/SCons/Action.py\n@@ -100,6 +100,8 @@\n \n \"\"\"\n \n+from __future__ import annotations\n+\n import inspect\n import os\n import pickle\n@@ -109,7 +111,7 @@\n from abc import ABC, abstractmethod\n from collections import OrderedDict\n from subprocess import DEVNULL, PIPE\n-from typing import List, Optional, Tuple\n+from typing import TYPE_CHECKING\n \n import SCons.Debug\n import SCons.Errors\n@@ -120,7 +122,9 @@\n from SCons.Debug import logInstanceCreation\n from SCons.Subst import SUBST_CMD, SUBST_RAW, SUBST_SIG\n from SCons.Util import is_String, is_List\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n class _null:\n     pass\n@@ -481,9 +485,7 @@ def _do_create_action(act, kw):\n     return None\n \n \n-# TODO: from __future__ import annotations once we get to Python 3.7 base,\n-#   to avoid quoting the defined-later classname\n-def _do_create_list_action(act, kw) -> \"ListAction\":\n+def _do_create_list_action(act, kw) -> ListAction:\n     \"\"\"A factory for list actions.\n \n     Convert the input list *act* into Actions and then wrap them in a\n@@ -529,7 +531,7 @@ def __call__(\n         show=_null,\n         execute=_null,\n         chdir=_null,\n-        executor: Optional[ExecutorType] = None,\n+        executor: Executor | None = None,\n     ):\n         raise NotImplementedError\n \n@@ -541,15 +543,15 @@ def no_batch_key(self, env, target, source):\n \n     batch_key = no_batch_key\n \n-    def genstring(self, target, source, env, executor: Optional[ExecutorType] = None) -> str:\n+    def genstring(self, target, source, env, executor: Executor | None = None) -> str:\n         return str(self)\n \n     @abstractmethod\n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         raise NotImplementedError\n \n     @abstractmethod\n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         raise NotImplementedError\n \n     def get_contents(self, target, source, env):\n@@ -601,10 +603,10 @@ def presub_lines(self, env):\n         self.presub_env = None      # don't need this any more\n         return lines\n \n-    def get_varlist(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_varlist(self, target, source, env, executor: Executor | None = None):\n         return self.varlist\n \n-    def get_targets(self, env, executor: Optional[ExecutorType]):\n+    def get_targets(self, env, executor: Executor | None):\n         \"\"\"\n         Returns the type of targets ($TARGETS, $CHANGED_TARGETS) used\n         by this action.\n@@ -658,7 +660,7 @@ def __call__(self, target, source, env,\n                                show=_null,\n                                execute=_null,\n                                chdir=_null,\n-                               executor: Optional[ExecutorType] = None):\n+                               executor: Executor | None = None):\n         if not is_List(target):\n             target = [target]\n         if not is_List(source):\n@@ -742,10 +744,10 @@ def __call__(self, target, source, env,\n     # an ABC like parent ActionBase, but things reach in and use it. It's\n     # not just unittests or we could fix it up with a concrete subclass there.\n \n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         raise NotImplementedError\n \n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         raise NotImplementedError\n \n \n@@ -1010,7 +1012,7 @@ def __str__(self) -> str:\n         return str(self.cmd_list)\n \n \n-    def process(self, target, source, env, executor=None, overrides: Optional[dict] = None) -> Tuple[List, bool, bool]:\n+    def process(self, target, source, env, executor: Executor | None = None, overrides: dict | None = None) -> tuple[list, bool, bool]:\n         if executor:\n             result = env.subst_list(self.cmd_list, SUBST_CMD, executor=executor, overrides=overrides)\n         else:\n@@ -1031,7 +1033,7 @@ def process(self, target, source, env, executor=None, overrides: Optional[dict]\n             pass\n         return result, ignore, silent\n \n-    def strfunction(self, target, source, env, executor: Optional[ExecutorType] = None, overrides: Optional[dict] = None) -> str:\n+    def strfunction(self, target, source, env, executor: Executor | None = None, overrides: dict | None = None) -> str:\n         if self.cmdstr is None:\n             return None\n         if self.cmdstr is not _null:\n@@ -1046,7 +1048,7 @@ def strfunction(self, target, source, env, executor: Optional[ExecutorType] = No\n             return ''\n         return _string_from_cmd_list(cmd_list[0])\n \n-    def execute(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def execute(self, target, source, env, executor: Executor | None = None):\n         \"\"\"Execute a command action.\n \n         This will handle lists of commands as well as individual commands,\n@@ -1108,7 +1110,7 @@ def execute(self, target, source, env, executor: Optional[ExecutorType] = None):\n                                                command=cmd_line)\n         return 0\n \n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         \"\"\"Return the signature contents of this action's command line.\n \n         This strips $(-$) and everything in between the string,\n@@ -1123,7 +1125,7 @@ def get_presig(self, target, source, env, executor: Optional[ExecutorType] = Non\n             return env.subst_target_source(cmd, SUBST_SIG, executor=executor)\n         return env.subst_target_source(cmd, SUBST_SIG, target, source)\n \n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         \"\"\"Return the implicit dependencies of this action's command line.\"\"\"\n         icd = env.get('IMPLICIT_COMMAND_DEPENDENCIES', True)\n         if is_String(icd) and icd[:1] == '$':\n@@ -1145,7 +1147,7 @@ def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType\n         # lightweight dependency scanning.\n         return self._get_implicit_deps_lightweight(target, source, env, executor)\n \n-    def _get_implicit_deps_lightweight(self, target, source, env, executor: Optional[ExecutorType]):\n+    def _get_implicit_deps_lightweight(self, target, source, env, executor: Executor | None):\n         \"\"\"\n         Lightweight dependency scanning involves only scanning the first entry\n         in an action string, even if it contains &&.\n@@ -1166,7 +1168,7 @@ def _get_implicit_deps_lightweight(self, target, source, env, executor: Optional\n                     res.append(env.fs.File(d))\n         return res\n \n-    def _get_implicit_deps_heavyweight(self, target, source, env, executor: Optional[ExecutorType],\n+    def _get_implicit_deps_heavyweight(self, target, source, env, executor: Executor | None,\n                                        icd_int):\n         \"\"\"\n         Heavyweight dependency scanning involves scanning more than just the\n@@ -1234,7 +1236,7 @@ def __init__(self, generator, kw) -> None:\n         self.varlist = kw.get('varlist', ())\n         self.targets = kw.get('targets', '$TARGETS')\n \n-    def _generate(self, target, source, env, for_signature, executor: Optional[ExecutorType] = None):\n+    def _generate(self, target, source, env, for_signature, executor: Executor | None = None):\n         # ensure that target is a list, to make it easier to write\n         # generator functions:\n         if not is_List(target):\n@@ -1265,11 +1267,11 @@ def __str__(self) -> str:\n     def batch_key(self, env, target, source):\n         return self._generate(target, source, env, 1).batch_key(env, target, source)\n \n-    def genstring(self, target, source, env, executor: Optional[ExecutorType] = None) -> str:\n+    def genstring(self, target, source, env, executor: Executor | None = None) -> str:\n         return self._generate(target, source, env, 1, executor).genstring(target, source, env)\n \n     def __call__(self, target, source, env, exitstatfunc=_null, presub=_null,\n-                 show=_null, execute=_null, chdir=_null, executor: Optional[ExecutorType] = None):\n+                 show=_null, execute=_null, chdir=_null, executor: Executor | None = None):\n         act = self._generate(target, source, env, 0, executor)\n         if act is None:\n             raise SCons.Errors.UserError(\n@@ -1281,7 +1283,7 @@ def __call__(self, target, source, env, exitstatfunc=_null, presub=_null,\n             target, source, env, exitstatfunc, presub, show, execute, chdir, executor\n         )\n \n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         \"\"\"Return the signature contents of this action's command line.\n \n         This strips $(-$) and everything in between the string,\n@@ -1289,13 +1291,13 @@ def get_presig(self, target, source, env, executor: Optional[ExecutorType] = Non\n         \"\"\"\n         return self._generate(target, source, env, 1, executor).get_presig(target, source, env)\n \n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         return self._generate(target, source, env, 1, executor).get_implicit_deps(target, source, env)\n \n-    def get_varlist(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_varlist(self, target, source, env, executor: Executor | None = None):\n         return self._generate(target, source, env, 1, executor).get_varlist(target, source, env, executor)\n \n-    def get_targets(self, env, executor: Optional[ExecutorType]):\n+    def get_targets(self, env, executor: Executor | None):\n         return self._generate(None, None, env, 1, executor).get_targets(env, executor)\n \n \n@@ -1341,22 +1343,22 @@ def _generate_cache(self, env):\n             raise SCons.Errors.UserError(\"$%s value %s cannot be used to create an Action.\" % (self.var, repr(c)))\n         return gen_cmd\n \n-    def _generate(self, target, source, env, for_signature, executor: Optional[ExecutorType] = None):\n+    def _generate(self, target, source, env, for_signature, executor: Executor | None = None):\n         return self._generate_cache(env)\n \n     def __call__(self, target, source, env, *args, **kw):\n         c = self.get_parent_class(env)\n         return c.__call__(self, target, source, env, *args, **kw)\n \n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         c = self.get_parent_class(env)\n         return c.get_presig(self, target, source, env)\n \n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         c = self.get_parent_class(env)\n         return c.get_implicit_deps(self, target, source, env)\n \n-    def get_varlist(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_varlist(self, target, source, env, executor: Executor | None = None):\n         c = self.get_parent_class(env)\n         return c.get_varlist(self, target, source, env, executor)\n \n@@ -1389,7 +1391,7 @@ def function_name(self):\n             except AttributeError:\n                 return \"unknown_python_function\"\n \n-    def strfunction(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def strfunction(self, target, source, env, executor: Executor | None = None):\n         if self.cmdstr is None:\n             return None\n         if self.cmdstr is not _null:\n@@ -1430,7 +1432,7 @@ def __str__(self) -> str:\n             return str(self.execfunction)\n         return \"%s(target, source, env)\" % name\n \n-    def execute(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def execute(self, target, source, env, executor: Executor | None = None):\n         exc_info = (None,None,None)\n         try:\n             if executor:\n@@ -1461,14 +1463,14 @@ def execute(self, target, source, env, executor: Optional[ExecutorType] = None):\n             # more information about this issue.\n             del exc_info\n \n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         \"\"\"Return the signature contents of this callable action.\"\"\"\n         try:\n             return self.gc(target, source, env)\n         except AttributeError:\n             return self.funccontents\n \n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         return []\n \n class ListAction(ActionBase):\n@@ -1485,7 +1487,7 @@ def list_of_actions(x):\n         self.varlist = ()\n         self.targets = '$TARGETS'\n \n-    def genstring(self, target, source, env, executor: Optional[ExecutorType] = None) -> str:\n+    def genstring(self, target, source, env, executor: Executor | None = None) -> str:\n         return '\\n'.join([a.genstring(target, source, env) for a in self.list])\n \n     def __str__(self) -> str:\n@@ -1495,7 +1497,7 @@ def presub_lines(self, env):\n         return SCons.Util.flatten_sequence(\n             [a.presub_lines(env) for a in self.list])\n \n-    def get_presig(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_presig(self, target, source, env, executor: Executor | None = None):\n         \"\"\"Return the signature contents of this action list.\n \n         Simple concatenation of the signatures of the elements.\n@@ -1503,7 +1505,7 @@ def get_presig(self, target, source, env, executor: Optional[ExecutorType] = Non\n         return b\"\".join([bytes(x.get_contents(target, source, env)) for x in self.list])\n \n     def __call__(self, target, source, env, exitstatfunc=_null, presub=_null,\n-                 show=_null, execute=_null, chdir=_null, executor: Optional[ExecutorType] = None):\n+                 show=_null, execute=_null, chdir=_null, executor: Executor | None = None):\n         if executor:\n             target = executor.get_all_targets()\n             source = executor.get_all_sources()\n@@ -1514,13 +1516,13 @@ def __call__(self, target, source, env, exitstatfunc=_null, presub=_null,\n                 return stat\n         return 0\n \n-    def get_implicit_deps(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_implicit_deps(self, target, source, env, executor: Executor | None = None):\n         result = []\n         for act in self.list:\n             result.extend(act.get_implicit_deps(target, source, env))\n         return result\n \n-    def get_varlist(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def get_varlist(self, target, source, env, executor: Executor | None = None):\n         result = OrderedDict()\n         for act in self.list:\n             for var in act.get_varlist(target, source, env, executor):\n@@ -1586,7 +1588,7 @@ def subst_kw(self, target, source, env):\n             kw[key] = self.subst(self.kw[key], target, source, env)\n         return kw\n \n-    def __call__(self, target, source, env, executor: Optional[ExecutorType] = None):\n+    def __call__(self, target, source, env, executor: Executor | None = None):\n         args = self.subst_args(target, source, env)\n         kw = self.subst_kw(target, source, env)\n         return self.parent.actfunc(*args, **kw)\ndiff --git a/SCons/ActionTests.py b/SCons/ActionTests.py\nindex 39798809e9..497de1869d 100644\n--- a/SCons/ActionTests.py\n+++ b/SCons/ActionTests.py\n@@ -27,6 +27,8 @@\n # contents, so try to minimize changes by defining them here, before we\n # even import anything.\n \n+from __future__ import annotations\n+\n def GlobalFunc() -> None:\n     pass\n \n@@ -43,13 +45,15 @@ def __call__(self) -> None:\n import unittest\n from unittest import mock\n from subprocess import PIPE\n-from typing import Optional\n+from typing import TYPE_CHECKING\n \n import SCons.Action\n import SCons.Environment\n import SCons.Errors\n from SCons.Action import scons_subproc_run\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n import TestCmd\n \n@@ -1701,11 +1705,11 @@ def __call__(self, target, source, env) -> int:\n         c = test.read(outfile, 'r')\n         assert c == \"class1b\\n\", c\n \n-        def build_it(target, source, env, executor: Optional[ExecutorType] = None, self=self) -> int:\n+        def build_it(target, source, env, executor: Executor | None = None, self=self) -> int:\n             self.build_it = 1\n             return 0\n \n-        def string_it(target, source, env, executor: Optional[ExecutorType] = None, self=self):\n+        def string_it(target, source, env, executor: Executor | None = None, self=self):\n             self.string_it = 1\n             return None\n \ndiff --git a/SCons/Builder.py b/SCons/Builder.py\nindex 3efcc8271d..c81d104143 100644\n--- a/SCons/Builder.py\n+++ b/SCons/Builder.py\n@@ -99,10 +99,11 @@\n \n \"\"\"\n \n+from __future__ import annotations\n+\n import os\n from collections import UserDict, UserList\n from contextlib import suppress\n-from typing import Optional\n \n import SCons.Action\n import SCons.Debug\n@@ -112,7 +113,7 @@\n import SCons.Warnings\n from SCons.Debug import logInstanceCreation\n from SCons.Errors import InternalError, UserError\n-from SCons.Util.sctyping import ExecutorType\n+from SCons.Executor import Executor\n \n class _Null:\n     pass\n@@ -591,7 +592,7 @@ def _execute(self, env, target, source, overwarn={}, executor_kw={}):\n         # build this particular list of targets from this particular list of\n         # sources.\n \n-        executor: Optional[ExecutorType] = None\n+        executor: Executor | None = None\n         key = None\n \n         if self.multi:\ndiff --git a/SCons/BuilderTests.py b/SCons/BuilderTests.py\nindex b66f52439e..adfa648280 100644\n--- a/SCons/BuilderTests.py\n+++ b/SCons/BuilderTests.py\n@@ -21,6 +21,8 @@\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n \n+from __future__ import annotations\n+\n import SCons.compat\n \n # Define a null function for use as a builder action.\n@@ -31,6 +33,7 @@ def Func() -> None:\n     pass\n \n from collections import UserList\n+from typing import TYPE_CHECKING\n import io\n import os.path\n import re\n@@ -45,7 +48,9 @@ def Func() -> None:\n import SCons.Errors\n import SCons.Subst\n import SCons.Util\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n sys.stdout = io.StringIO()\n \n@@ -185,9 +190,9 @@ def generate_build_env(self, env):\n         return env\n     def get_build_env(self):\n         return self.executor.get_build_env()\n-    def set_executor(self, executor: ExecutorType) -> None:\n+    def set_executor(self, executor: Executor) -> None:\n         self.executor = executor\n-    def get_executor(self, create: int=1) -> ExecutorType:\n+    def get_executor(self, create: int=1) -> Executor:\n         return self.executor\n \n class MyNode(MyNode_without_target_from_source):\ndiff --git a/SCons/Defaults.py b/SCons/Defaults.py\nindex a5d49fc76f..d971d060f6 100644\n--- a/SCons/Defaults.py\n+++ b/SCons/Defaults.py\n@@ -31,12 +31,14 @@\n from distutils.msvccompiler.\n \"\"\"\n \n+from __future__ import annotations\n+\n import os\n import shutil\n import stat\n import sys\n import time\n-from typing import List, Callable\n+from typing import Callable\n \n import SCons.Action\n import SCons.Builder\n@@ -467,8 +469,8 @@ def _stripixes(\n     prefix: str,\n     items,\n     suffix: str,\n-    stripprefixes: List[str],\n-    stripsuffixes: List[str],\n+    stripprefixes: list[str],\n+    stripsuffixes: list[str],\n     env,\n     literal_prefix: str = \"\",\n     c: Callable[[list], list] = None,\n@@ -547,7 +549,7 @@ def _stripixes(\n     return c(prefix, stripped, suffix, env)\n \n \n-def processDefines(defs) -> List[str]:\n+def processDefines(defs) -> list[str]:\n     \"\"\"Return list of strings for preprocessor defines from *defs*.\n \n     Resolves the different forms ``CPPDEFINES`` can be assembled in:\ndiff --git a/SCons/Environment.py b/SCons/Environment.py\nindex 0c14468406..62926f56a5 100644\n--- a/SCons/Environment.py\n+++ b/SCons/Environment.py\n@@ -30,6 +30,8 @@\n are construction variables used to initialize the Environment.\n \"\"\"\n \n+from __future__ import annotations\n+\n import copy\n import os\n import sys\n@@ -37,7 +39,7 @@\n import shlex\n from collections import UserDict, UserList, deque\n from subprocess import PIPE, DEVNULL\n-from typing import Callable, Collection, Optional, Sequence, Union\n+from typing import TYPE_CHECKING, Callable, Collection, Sequence\n \n import SCons.Action\n import SCons.Builder\n@@ -76,7 +78,9 @@\n     to_String_for_subst,\n     uniquer_hashables,\n )\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n class _Null:\n     pass\n@@ -698,7 +702,7 @@ def gvars(self):\n     def lvars(self):\n         return {}\n \n-    def subst(self, string, raw: int=0, target=None, source=None, conv=None, executor: Optional[ExecutorType] = None, overrides: Optional[dict] = None):\n+    def subst(self, string, raw: int=0, target=None, source=None, conv=None, executor: Executor | None = None, overrides: dict | None = None):\n         \"\"\"Recursively interpolates construction variables from the\n         Environment into the specified string, returning the expanded\n         result.  Construction variables are specified by a $ prefix\n@@ -724,7 +728,7 @@ def subst_kw(self, kw, raw: int=0, target=None, source=None):\n             nkw[k] = v\n         return nkw\n \n-    def subst_list(self, string, raw: int=0, target=None, source=None, conv=None, executor: Optional[ExecutorType] = None, overrides: Optional[dict] = None):\n+    def subst_list(self, string, raw: int=0, target=None, source=None, conv=None, executor: Executor | None = None, overrides: dict | None = None):\n         \"\"\"Calls through to SCons.Subst.scons_subst_list().\n \n         See the documentation for that function.\n@@ -901,7 +905,7 @@ def ParseFlags(self, *flags) -> dict:\n             'RPATH'         : [],\n         }\n \n-        def do_parse(arg: Union[str, Sequence]) -> None:\n+        def do_parse(arg: str | Sequence) -> None:\n             if not arg:\n                 return\n \n@@ -1798,7 +1802,7 @@ def default(self, obj):\n         raise ValueError(\"Unsupported serialization format: %s.\" % fmt)\n \n \n-    def FindIxes(self, paths: Sequence[str], prefix: str, suffix: str) -> Optional[str]:\n+    def FindIxes(self, paths: Sequence[str], prefix: str, suffix: str) -> str | None:\n         \"\"\"Search *paths* for a path that has *prefix* and *suffix*.\n \n         Returns on first match.\n@@ -2079,7 +2083,7 @@ def _find_toolpath_dir(self, tp):\n         return self.fs.Dir(self.subst(tp)).srcnode().get_abspath()\n \n     def Tool(\n-        self, tool: Union[str, Callable], toolpath: Optional[Collection[str]] = None, **kwargs\n+        self, tool: str | Callable, toolpath: Collection[str] | None = None, **kwargs\n     ) -> Callable:\n         \"\"\"Find and run tool module *tool*.\n \n@@ -2615,7 +2619,7 @@ class OverrideEnvironment(Base):\n     ``OverrideEnvironment``.\n     \"\"\"\n \n-    def __init__(self, subject, overrides: Optional[dict] = None) -> None:\n+    def __init__(self, subject, overrides: dict | None = None) -> None:\n         if SCons.Debug.track_instances: logInstanceCreation(self, 'Environment.OverrideEnvironment')\n         overrides = {} if overrides is None else overrides\n         # set these directly via __dict__ to avoid trapping by __setattr__\ndiff --git a/SCons/Errors.py b/SCons/Errors.py\nindex a2efc97088..af77971032 100644\n--- a/SCons/Errors.py\n+++ b/SCons/Errors.py\n@@ -26,11 +26,15 @@\n Used to handle internal and user errors in SCons.\n \"\"\"\n \n+from __future__ import annotations\n+\n import shutil\n-from typing import Optional\n+from typing import TYPE_CHECKING\n \n from SCons.Util.sctypes import to_String, is_String\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n # Note that not all Errors are defined here, some are at the point of use\n \n@@ -75,7 +79,7 @@ class BuildError(Exception):\n \n     def __init__(self,\n                  node=None, errstr: str=\"Unknown error\", status: int=2, exitstatus: int=2,\n-                 filename=None, executor: Optional[ExecutorType] = None, action=None, command=None,\n+                 filename=None, executor: Executor | None = None, action=None, command=None,\n                  exc_info=(None, None, None)) -> None:\n \n         # py3: errstr should be string and not bytes.\ndiff --git a/SCons/Executor.py b/SCons/Executor.py\nindex 1b054b4ba8..53eb5cbb2f 100644\n--- a/SCons/Executor.py\n+++ b/SCons/Executor.py\n@@ -23,8 +23,9 @@\n \n \"\"\"Execute actions with specific lists of target and source Nodes.\"\"\"\n \n+from __future__ import annotations\n+\n import collections\n-from typing import Dict\n \n import SCons.Errors\n import SCons.Memoize\n@@ -32,7 +33,6 @@\n from SCons.compat import NoSlotsPyPy\n import SCons.Debug\n from SCons.Debug import logInstanceCreation\n-from SCons.Util.sctyping import ExecutorType\n \n class Batch:\n     \"\"\"Remembers exact association between targets\n@@ -550,12 +550,12 @@ def get_implicit_deps(self):\n \n \n \n-_batch_executors: Dict[str, ExecutorType] = {}\n+_batch_executors: dict[str, Executor] = {}\n \n-def GetBatchExecutor(key: str) -> ExecutorType:\n+def GetBatchExecutor(key: str) -> Executor:\n     return _batch_executors[key]\n \n-def AddBatchExecutor(key: str, executor: ExecutorType) -> None:\n+def AddBatchExecutor(key: str, executor: Executor) -> None:\n     assert key not in _batch_executors\n     _batch_executors[key] = executor\n \ndiff --git a/SCons/Node/FS.py b/SCons/Node/FS.py\nindex 3cd7720c0f..bdecffcfbd 100644\n--- a/SCons/Node/FS.py\n+++ b/SCons/Node/FS.py\n@@ -30,6 +30,8 @@\n that can be used by scripts or modules looking for the canonical default.\n \"\"\"\n \n+from __future__ import annotations\n+\n import codecs\n import fnmatch\n import importlib.util\n@@ -40,7 +42,6 @@\n import sys\n import time\n from itertools import chain\n-from typing import Optional\n \n import SCons.Action\n import SCons.Debug\n@@ -1492,7 +1493,7 @@ def Repository(self, *dirs) -> None:\n                 d = self.Dir(d)\n             self.Top.addRepository(d)\n \n-    def PyPackageDir(self, modulename) -> Optional[Dir]:\n+    def PyPackageDir(self, modulename) -> Dir | None:\n         r\"\"\"Locate the directory of Python module *modulename*.\n \n         For example 'SCons' might resolve to\n@@ -3190,7 +3191,7 @@ def exists(self):\n     # SIGNATURE SUBSYSTEM\n     #\n \n-    def get_max_drift_csig(self) -> Optional[str]:\n+    def get_max_drift_csig(self) -> str | None:\n         \"\"\"\n         Returns the content signature currently stored for this node\n         if it's been unmodified longer than the max_drift value, or the\ndiff --git a/SCons/Node/FSTests.py b/SCons/Node/FSTests.py\nindex 83ceef28c7..9ae8c03e10 100644\n--- a/SCons/Node/FSTests.py\n+++ b/SCons/Node/FSTests.py\n@@ -21,6 +21,8 @@\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n \n+from __future__ import annotations\n+\n import SCons.compat\n import os\n import os.path\n@@ -29,7 +31,7 @@\n import unittest\n import shutil\n import stat\n-from typing import Optional\n+from typing import TYPE_CHECKING\n \n from TestCmd import TestCmd, IS_WINDOWS, IS_ROOT\n \n@@ -38,7 +40,9 @@\n import SCons.Util\n import SCons.Warnings\n import SCons.Environment\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n built_it = None\n \n@@ -320,7 +324,7 @@ class MkdirAction(Action):\n             def __init__(self, dir_made) -> None:\n                 self.dir_made = dir_made\n \n-            def __call__(self, target, source, env, executor: Optional[ExecutorType] = None) -> None:\n+            def __call__(self, target, source, env, executor: Executor | None = None) -> None:\n                 if executor:\n                     target = executor.get_all_targets()\n                     source = executor.get_all_sources()\n@@ -2491,7 +2495,7 @@ def collect(self, args):\n                             result += a\n                         return result\n \n-                    def signature(self, executor: ExecutorType):\n+                    def signature(self, executor: Executor):\n                         return self.val + 222\n \n                 self.module = M(val)\n@@ -3582,7 +3586,7 @@ class MkdirAction(Action):\n             def __init__(self, dir_made) -> None:\n                 self.dir_made = dir_made\n \n-            def __call__(self, target, source, env, executor: Optional[ExecutorType] = None) -> None:\n+            def __call__(self, target, source, env, executor: Executor | None = None) -> None:\n                 if executor:\n                     target = executor.get_all_targets()\n                     source = executor.get_all_sources()\ndiff --git a/SCons/Node/NodeTests.py b/SCons/Node/NodeTests.py\nindex 70c8551b13..6c7437d600 100644\n--- a/SCons/Node/NodeTests.py\n+++ b/SCons/Node/NodeTests.py\n@@ -21,17 +21,21 @@\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n \n+from __future__ import annotations\n+\n import SCons.compat\n \n import collections\n import re\n import unittest\n-from typing import Optional\n+from typing import TYPE_CHECKING\n \n import SCons.Errors\n import SCons.Node\n import SCons.Util\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n \n built_it = None\n@@ -64,7 +68,7 @@ class MyAction(MyActionBase):\n     def __init__(self) -> None:\n         self.order = 0\n \n-    def __call__(self, target, source, env, executor: Optional[ExecutorType] = None) -> int:\n+    def __call__(self, target, source, env, executor: Executor | None = None) -> int:\n         global built_it, built_target, built_source, built_args, built_order\n         if executor:\n             target = executor.get_all_targets()\ndiff --git a/SCons/Node/__init__.py b/SCons/Node/__init__.py\nindex 00bf4ac3b1..8ae991ecf7 100644\n--- a/SCons/Node/__init__.py\n+++ b/SCons/Node/__init__.py\n@@ -40,18 +40,19 @@\n \n \"\"\"\n \n+from __future__ import annotations\n+\n import collections\n import copy\n from itertools import chain, zip_longest\n-from typing import Optional\n \n import SCons.Debug\n import SCons.Executor\n import SCons.Memoize\n from SCons.compat import NoSlotsPyPy\n from SCons.Debug import logInstanceCreation, Trace\n+from SCons.Executor import Executor\n from SCons.Util import hash_signature, is_List, UniqueList, render_tree\n-from SCons.Util.sctyping import ExecutorType\n \n print_duplicate = 0\n \n@@ -636,11 +637,11 @@ def get_build_scanner_path(self, scanner):\n         \"\"\"Fetch the appropriate scanner path for this node.\"\"\"\n         return self.get_executor().get_build_scanner_path(scanner)\n \n-    def set_executor(self, executor: ExecutorType) -> None:\n+    def set_executor(self, executor: Executor) -> None:\n         \"\"\"Set the action executor for this node.\"\"\"\n         self.executor = executor\n \n-    def get_executor(self, create: int=1) -> ExecutorType:\n+    def get_executor(self, create: int=1) -> Executor:\n         \"\"\"Fetch the action executor for this node.  Create one if\n         there isn't already one, and requested to do so.\"\"\"\n         try:\ndiff --git a/SCons/SConf.py b/SCons/SConf.py\nindex d2e09be34e..c22355665c 100644\n--- a/SCons/SConf.py\n+++ b/SCons/SConf.py\n@@ -31,6 +31,8 @@\n libraries are installed, if some command line options are supported etc.\n \"\"\"\n \n+from __future__ import annotations\n+\n import SCons.compat\n \n import atexit\n@@ -39,7 +41,6 @@\n import re\n import sys\n import traceback\n-from typing import Tuple\n \n import SCons.Action\n import SCons.Builder\n@@ -265,7 +266,7 @@ def failed(self):\n             sys.excepthook(*self.exc_info())\n         return SCons.Taskmaster.Task.failed(self)\n \n-    def collect_node_states(self) -> Tuple[bool, bool, bool]:\n+    def collect_node_states(self) -> tuple[bool, bool, bool]:\n         # returns (is_up_to_date, cached_error, cachable)\n         # where is_up_to_date is True if the node(s) are up_to_date\n         #       cached_error  is True if the node(s) are up_to_date, but the\ndiff --git a/SCons/Script/Main.py b/SCons/Script/Main.py\nindex 04b420a3bf..7f04d00243 100644\n--- a/SCons/Script/Main.py\n+++ b/SCons/Script/Main.py\n@@ -31,6 +31,8 @@\n it goes here.\n \"\"\"\n \n+from __future__ import annotations\n+\n import SCons.compat\n \n import importlib.util\n@@ -42,7 +44,6 @@\n import traceback\n import platform\n import threading\n-from typing import Optional, List, TYPE_CHECKING\n \n import SCons.CacheDir\n import SCons.Debug\n@@ -552,7 +553,7 @@ def SetOption(name: str, value):\n     \"\"\"Set the value of an option - Public API.\"\"\"\n     return OptionsParser.values.set_option(name, value)\n \n-def DebugOptions(json: Optional[str] = None) -> None:\n+def DebugOptions(json: str | None = None) -> None:\n     \"\"\"Specify options to SCons debug logic - Public API.\n \n     Currently only *json* is supported, which changes the JSON file\n@@ -681,8 +682,8 @@ def _scons_internal_error() -> None:\n     sys.exit(2)\n \n def _SConstruct_exists(\n-    dirname: str, repositories: List[str], filelist: List[str]\n-) -> Optional[str]:\n+    dirname: str, repositories: list[str], filelist: list[str]\n+) -> str | None:\n     \"\"\"Check that an SConstruct file exists in a directory.\n \n     Arguments:\n@@ -1424,7 +1425,7 @@ def _exec_main(parser, values) -> None:\n         class SConsPdb(pdb.Pdb):\n             \"\"\"Specialization of Pdb to help find SConscript files.\"\"\"\n \n-            def lookupmodule(self, filename: str) -> Optional[str]:\n+            def lookupmodule(self, filename: str) -> str | None:\n                 \"\"\"Helper function for break/clear parsing -- SCons version.\n \n                 Translates (possibly incomplete) file or module name\ndiff --git a/SCons/Script/SConsOptions.py b/SCons/Script/SConsOptions.py\nindex 08531814f5..ef27b70669 100644\n--- a/SCons/Script/SConsOptions.py\n+++ b/SCons/Script/SConsOptions.py\n@@ -21,13 +21,14 @@\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n \n+from __future__ import annotations\n+\n import gettext\n import optparse\n import re\n import shutil\n import sys\n import textwrap\n-from typing import Optional\n \n import SCons.Node.FS\n import SCons.Platform.virtualenv\n@@ -318,7 +319,7 @@ class SConsBadOptionError(optparse.BadOptionError):\n     \"\"\"\n     # TODO why is 'parser' needed? Not called in current code base.\n \n-    def __init__(self, opt_str: str, parser: Optional[\"SConsOptionParser\"] = None) -> None:\n+    def __init__(self, opt_str: str, parser: SConsOptionParser | None = None) -> None:\n         self.opt_str = opt_str\n         self.parser = parser\n \ndiff --git a/SCons/Script/SConscript.py b/SCons/Script/SConscript.py\nindex a2ef3b9d57..7cc9bea5af 100644\n--- a/SCons/Script/SConscript.py\n+++ b/SCons/Script/SConscript.py\n@@ -23,6 +23,8 @@\n \n \"\"\"This module defines the Python API provided to SConscript files.\"\"\"\n \n+from __future__ import annotations\n+\n import SCons\n import SCons.Action\n import SCons.Builder\n@@ -45,7 +47,6 @@\n import sys\n import traceback\n import time\n-from typing import Tuple\n \n class SConscriptReturn(Exception):\n     pass\n@@ -386,7 +387,7 @@ class SConsEnvironment(SCons.Environment.Base):\n     # Private methods of an SConsEnvironment.\n     #\n     @staticmethod\n-    def _get_major_minor_revision(version_string: str) -> Tuple[int, int, int]:\n+    def _get_major_minor_revision(version_string: str) -> tuple[int, int, int]:\n         \"\"\"Split a version string into major, minor and (optionally)\n         revision parts.\n \n@@ -485,7 +486,7 @@ def Default(self, *targets) -> None:\n         SCons.Script._Set_Default_Targets(self, targets)\n \n     @staticmethod\n-    def GetSConsVersion() -> Tuple[int, int, int]:\n+    def GetSConsVersion() -> tuple[int, int, int]:\n         \"\"\"Return the current SCons version.\n \n         .. versionadded:: 4.8.0\ndiff --git a/SCons/Subst.py b/SCons/Subst.py\nindex b04ebe50cd..4d6b249c6c 100644\n--- a/SCons/Subst.py\n+++ b/SCons/Subst.py\n@@ -23,10 +23,11 @@\n \n \"\"\"SCons string substitution.\"\"\"\n \n+from __future__ import annotations\n+\n import collections\n import re\n from inspect import signature, Parameter\n-from typing import Optional\n \n import SCons.Errors\n from SCons.Util import is_String, is_Sequence\n@@ -807,7 +808,7 @@ def _remove_list(list):\n _space_sep = re.compile(r'[\\t ]+(?![^{]*})')\n \n \n-def scons_subst(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None, overrides: Optional[dict] = None):\n+def scons_subst(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None, overrides: dict | None = None):\n     \"\"\"Expand a string or list containing construction variable\n     substitutions.\n \n@@ -889,7 +890,7 @@ def scons_subst(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={\n \n     return result\n \n-def scons_subst_list(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None, overrides: Optional[dict] = None):\n+def scons_subst_list(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None, overrides: dict | None = None):\n     \"\"\"Substitute construction variables in a string (or list or other\n     object) and separate the arguments into a command list.\n \ndiff --git a/SCons/Tool/FortranCommon.py b/SCons/Tool/FortranCommon.py\nindex f221d15ec7..dc9dcddf6e 100644\n--- a/SCons/Tool/FortranCommon.py\n+++ b/SCons/Tool/FortranCommon.py\n@@ -23,9 +23,10 @@\n \n \"\"\"Routines for setting up Fortran, common to all dialects.\"\"\"\n \n+from __future__ import annotations\n+\n import re\n import os.path\n-from typing import Tuple, List\n \n import SCons.Scanner.Fortran\n import SCons.Tool\n@@ -96,7 +97,7 @@ def ShFortranEmitter(target, source, env) -> Tuple:\n     return SharedObjectEmitter(target, source, env)\n \n \n-def ComputeFortranSuffixes(suffixes: List[str], ppsuffixes: List[str]) -> None:\n+def ComputeFortranSuffixes(suffixes: list[str], ppsuffixes: list[str]) -> None:\n     \"\"\"Update the suffix lists to reflect the platform requirements.\n \n     If upper-cased suffixes can be distinguished from lower, those are\n@@ -119,7 +120,7 @@ def ComputeFortranSuffixes(suffixes: List[str], ppsuffixes: List[str]) -> None:\n \n def CreateDialectActions(\n     dialect: str,\n-) -> Tuple[CommandAction, CommandAction, CommandAction, CommandAction]:\n+) -> tuple[CommandAction, CommandAction, CommandAction, CommandAction]:\n     \"\"\"Create dialect specific actions.\"\"\"\n     CompAction = Action(f'${dialect}COM ', cmdstr=f'${dialect}COMSTR')\n     CompPPAction = Action(f'${dialect}PPCOM ', cmdstr=f'${dialect}PPCOMSTR')\n@@ -131,8 +132,8 @@ def CreateDialectActions(\n def DialectAddToEnv(\n     env,\n     dialect: str,\n-    suffixes: List[str],\n-    ppsuffixes: List[str],\n+    suffixes: list[str],\n+    ppsuffixes: list[str],\n     support_mods: bool = False,\n ) -> None:\n     \"\"\"Add dialect specific construction variables.\ndiff --git a/SCons/Tool/JavaCommon.py b/SCons/Tool/JavaCommon.py\nindex c7e62b88ce..0bcb0eaa5e 100644\n--- a/SCons/Tool/JavaCommon.py\n+++ b/SCons/Tool/JavaCommon.py\n@@ -23,11 +23,12 @@\n \n \"\"\"Common routines for processing Java. \"\"\"\n \n+from __future__ import annotations\n+\n import os\n import re\n import glob\n from pathlib import Path\n-from typing import List\n \n import SCons.Util\n \n@@ -491,7 +492,7 @@ def parse_java_file(fn, version=default_java_version):\n         return os.path.split(fn)\n \n \n-def get_java_install_dirs(platform, version=None) -> List[str]:\n+def get_java_install_dirs(platform, version=None) -> list[str]:\n     \"\"\" Find possible java jdk installation directories.\n \n     Returns a list for use as `default_paths` when looking up actual\n@@ -540,7 +541,7 @@ def win32getvnum(java):\n     return []\n \n \n-def get_java_include_paths(env, javac, version) -> List[str]:\n+def get_java_include_paths(env, javac, version) -> list[str]:\n     \"\"\"Find java include paths for JNI building.\n \n     Cannot be called in isolation - `javac` refers to an already detected\ndiff --git a/SCons/Tool/__init__.py b/SCons/Tool/__init__.py\nindex faa92a78b2..a7bc927ebc 100644\n--- a/SCons/Tool/__init__.py\n+++ b/SCons/Tool/__init__.py\n@@ -33,10 +33,11 @@\n tool specifications.\n \"\"\"\n \n+from __future__ import annotations\n+\n import sys\n import os\n import importlib.util\n-from typing import Optional\n \n import SCons.Builder\n import SCons.Errors\n@@ -824,7 +825,7 @@ def tool_list(platform, env):\n     return [x for x in tools if x]\n \n \n-def find_program_path(env, key_program, default_paths=None, add_path: bool=False) -> Optional[str]:\n+def find_program_path(env, key_program, default_paths=None, add_path: bool=False) -> str | None:\n     \"\"\"\n     Find the location of a tool using various means.\n \ndiff --git a/SCons/Tool/jar.py b/SCons/Tool/jar.py\nindex 1967294f08..13bdca0518 100644\n--- a/SCons/Tool/jar.py\n+++ b/SCons/Tool/jar.py\n@@ -28,8 +28,9 @@\n selection method.\n \"\"\"\n \n+from __future__ import annotations\n+\n import os\n-from typing import List\n \n import SCons.Node\n import SCons.Node.FS\n@@ -41,7 +42,7 @@\n from SCons.Tool.JavaCommon import get_java_install_dirs\n \n \n-def jarSources(target, source, env, for_signature) -> List[str]:\n+def jarSources(target, source, env, for_signature) -> list[str]:\n     \"\"\"Only include sources that are not a manifest file.\"\"\"\n     try:\n         env['JARCHDIR']\ndiff --git a/SCons/Tool/lex.py b/SCons/Tool/lex.py\nindex 527f91c294..5e77ea692d 100644\n--- a/SCons/Tool/lex.py\n+++ b/SCons/Tool/lex.py\n@@ -31,9 +31,10 @@\n selection method.\n \"\"\"\n \n+from __future__ import annotations\n+\n import os.path\n import sys\n-from typing import Optional\n \n import SCons.Action\n import SCons.Tool\n@@ -95,7 +96,7 @@ def lexEmitter(target, source, env) -> tuple:\n     return target, source\n \n \n-def get_lex_path(env, append_paths: bool=False) -> Optional[str]:\n+def get_lex_path(env, append_paths: bool=False) -> str | None:\n     \"\"\"\n     Returns the path to the lex tool, searching several possible names.\n \n@@ -162,7 +163,7 @@ def generate(env) -> None:\n     env['_LEX_TABLES'] = '${LEX_TABLES_FILE and \"--tables-file=\" + str(LEX_TABLES_FILE)}'\n \n \n-def exists(env) -> Optional[str]:\n+def exists(env) -> str | None:\n     if sys.platform == 'win32':\n         return get_lex_path(env)\n     else:\ndiff --git a/SCons/Tool/ninja/Methods.py b/SCons/Tool/ninja/Methods.py\nindex 5c56e49085..ff006c072e 100644\n--- a/SCons/Tool/ninja/Methods.py\n+++ b/SCons/Tool/ninja/Methods.py\n@@ -21,10 +21,12 @@\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n \n+from __future__ import annotations\n+\n import os\n import shlex\n import textwrap\n-from typing import Optional\n+from typing import TYPE_CHECKING\n \n import SCons\n from SCons.Subst import SUBST_CMD\n@@ -32,7 +34,9 @@\n from SCons.Tool.ninja.Globals import __NINJA_RULE_MAPPING\n from SCons.Tool.ninja.Utils import get_targets_sources, get_dependencies, get_order_only, get_outputs, get_inputs, \\\n     get_rule, get_path, generate_command, get_command_env, get_comstr\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n \n def register_custom_handler(env, name, handler) -> None:\n@@ -78,7 +82,7 @@ def set_build_node_callback(env, node, callback) -> None:\n         node.attributes.ninja_build_callback = callback\n \n \n-def get_generic_shell_command(env, node, action, targets, sources, executor: Optional[ExecutorType] = None):\n+def get_generic_shell_command(env, node, action, targets, sources, executor: Executor | None = None):\n     return (\n         \"GENERATED_CMD\",\n         {\n@@ -231,7 +235,7 @@ def gen_get_response_file_command(env, rule, tool, tool_is_dynamic: bool=False,\n     if \"$\" in tool:\n         tool_is_dynamic = True\n \n-    def get_response_file_command(env, node, action, targets, sources, executor: Optional[ExecutorType] = None):\n+    def get_response_file_command(env, node, action, targets, sources, executor: Executor | None = None):\n         if hasattr(action, \"process\"):\n             cmd_list, _, _ = action.process(targets, sources, env, executor=executor)\n             cmd_list = [str(c).replace(\"$\", \"$$\") for c in cmd_list[0]]\ndiff --git a/SCons/Tool/ninja/Utils.py b/SCons/Tool/ninja/Utils.py\nindex cdce92895c..24d439ef5e 100644\n--- a/SCons/Tool/ninja/Utils.py\n+++ b/SCons/Tool/ninja/Utils.py\n@@ -20,17 +20,22 @@\n # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+from __future__ import annotations\n+\n import os\n import shutil\n from os.path import join as joinpath\n from collections import OrderedDict\n-from typing import Optional\n+from typing import TYPE_CHECKING\n \n import SCons\n from SCons.Action import get_default_ENV, _string_from_cmd_list\n from SCons.Script import AddOption\n from SCons.Util import is_List, flatten_sequence\n-from SCons.Util.sctyping import ExecutorType\n+\n+if TYPE_CHECKING:\n+    from SCons.Executor import Executor\n \n class NinjaExperimentalWarning(SCons.Warnings.WarningOnByDefault):\n     pass\n@@ -349,7 +354,7 @@ def get_comstr(env, action, targets, sources):\n     return action.genstring(targets, sources, env)\n \n \n-def generate_command(env, node, action, targets, sources, executor: Optional[ExecutorType] = None):\n+def generate_command(env, node, action, targets, sources, executor: Executor | None = None):\n     # Actions like CommandAction have a method called process that is\n     # used by SCons to generate the cmd_line they need to run. So\n     # check if it's a thing like CommandAction and call it if we can.\ndiff --git a/SCons/Tool/yacc.py b/SCons/Tool/yacc.py\nindex 7a4ddfc54e..bfd82f6053 100644\n--- a/SCons/Tool/yacc.py\n+++ b/SCons/Tool/yacc.py\n@@ -34,9 +34,10 @@\n selection method.\n \"\"\"\n \n+from __future__ import annotations\n+\n import os.path\n import sys\n-from typing import Optional\n \n import SCons.Defaults\n import SCons.Tool\n@@ -68,7 +69,7 @@ def _yaccEmitter(target, source, env, ysuf, hsuf) -> tuple:\n \n     # If -d is specified on the command line, yacc will emit a .h\n     # or .hpp file with the same base name as the .c or .cpp output file.\n-    # if '-d' in flags: \n+    # if '-d' in flags:\n     # or  bison options -H, --header, --defines (obsolete)\n     if \"-d\" in flags or \"-H\" in flags or \"--header\" in flags or \"--defines\" in flags:\n         target.append(targetBase + env.subst(hsuf, target=target, source=source))\n@@ -76,7 +77,7 @@ def _yaccEmitter(target, source, env, ysuf, hsuf) -> tuple:\n     # If -g is specified on the command line, yacc will emit a graph\n     # file with the same base name as the .c or .cpp output file.\n     # TODO: should this be handled like -v? i.e. a side effect, not target\n-    # if \"-g\" in flags:  \n+    # if \"-g\" in flags:\n     # or bison option --graph\n     if \"-g\" in flags or \"--graph\" in flags:\n         target.append(targetBase + env.subst(\"$YACC_GRAPH_FILE_SUFFIX\"))\n@@ -134,7 +135,7 @@ def yyEmitter(target, source, env) -> tuple:\n     return _yaccEmitter(target, source, env, ['.yy'], '$YACCHXXFILESUFFIX')\n \n \n-def get_yacc_path(env, append_paths: bool=False) -> Optional[str]:\n+def get_yacc_path(env, append_paths: bool=False) -> str | None:\n     \"\"\"\n     Returns the path to the yacc tool, searching several possible names.\n \n@@ -200,7 +201,7 @@ def generate(env) -> None:\n     env['_YACC_GRAPH'] = '${YACC_GRAPH_FILE and \"--graph=\" + str(YACC_GRAPH_FILE)}'\n \n \n-def exists(env) -> Optional[str]:\n+def exists(env) -> str | None:\n     if 'YACC' in env:\n         return env.Detect(env['YACC'])\n \ndiff --git a/SCons/Util/UtilTests.py b/SCons/Util/UtilTests.py\nindex b1c01086e4..0f457c3ce0 100644\n--- a/SCons/Util/UtilTests.py\n+++ b/SCons/Util/UtilTests.py\n@@ -21,6 +21,8 @@\n # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n \n+from __future__ import annotations\n+\n import functools\n import hashlib\n import io\n@@ -30,7 +32,6 @@\n import unittest.mock\n import warnings\n from collections import UserDict, UserList, UserString, namedtuple\n-from typing import Union\n \n import TestCmd\n \n@@ -533,8 +534,8 @@ def test_get_native_path(self) -> None:\n \n     def test_PrependPath(self) -> None:\n         \"\"\"Test prepending to a path\"\"\"\n-        p1: Union[list, str] = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n-        p2: Union[list, str] = r'C:\\mydir\\num\\one;C:\\mydir\\num\\two'\n+        p1: list | str = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n+        p2: list | str = r'C:\\mydir\\num\\one;C:\\mydir\\num\\two'\n         # have to include the pathsep here so that the test will work on UNIX too.\n         p1 = PrependPath(p1, r'C:\\dir\\num\\two', sep=';')\n         p1 = PrependPath(p1, r'C:\\dir\\num\\three', sep=';')\n@@ -545,14 +546,14 @@ def test_PrependPath(self) -> None:\n         assert p2 == r'C:\\mydir\\num\\one;C:\\mydir\\num\\three;C:\\mydir\\num\\two', p2\n \n         # check (only) first one is kept if there are dupes in new\n-        p3: Union[list, str] = r'C:\\dir\\num\\one'\n+        p3: list | str = r'C:\\dir\\num\\one'\n         p3 = PrependPath(p3, r'C:\\dir\\num\\two;C:\\dir\\num\\three;C:\\dir\\num\\two', sep=';')\n         assert p3 == r'C:\\dir\\num\\two;C:\\dir\\num\\three;C:\\dir\\num\\one', p3\n \n     def test_AppendPath(self) -> None:\n         \"\"\"Test appending to a path.\"\"\"\n-        p1: Union[list, str] = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n-        p2: Union[list, str] = r'C:\\mydir\\num\\one;C:\\mydir\\num\\two'\n+        p1: list | str = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n+        p2: list | str = r'C:\\mydir\\num\\one;C:\\mydir\\num\\two'\n         # have to include the pathsep here so that the test will work on UNIX too.\n         p1 = AppendPath(p1, r'C:\\dir\\num\\two', sep=';')\n         p1 = AppendPath(p1, r'C:\\dir\\num\\three', sep=';')\n@@ -563,13 +564,13 @@ def test_AppendPath(self) -> None:\n         assert p2 == r'C:\\mydir\\num\\two;C:\\mydir\\num\\three;C:\\mydir\\num\\one', p2\n \n         # check (only) last one is kept if there are dupes in new\n-        p3: Union[list, str] = r'C:\\dir\\num\\one'\n+        p3: list | str = r'C:\\dir\\num\\one'\n         p3 = AppendPath(p3, r'C:\\dir\\num\\two;C:\\dir\\num\\three;C:\\dir\\num\\two', sep=';')\n         assert p3 == r'C:\\dir\\num\\one;C:\\dir\\num\\three;C:\\dir\\num\\two', p3\n \n     def test_PrependPathPreserveOld(self) -> None:\n         \"\"\"Test prepending to a path while preserving old paths\"\"\"\n-        p1: Union[list, str] = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n+        p1: list | str = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n         # have to include the pathsep here so that the test will work on UNIX too.\n         p1 = PrependPath(p1, r'C:\\dir\\num\\two', sep=';', delete_existing=False)\n         p1 = PrependPath(p1, r'C:\\dir\\num\\three', sep=';')\n@@ -577,7 +578,7 @@ def test_PrependPathPreserveOld(self) -> None:\n \n     def test_AppendPathPreserveOld(self) -> None:\n         \"\"\"Test appending to a path while preserving old paths\"\"\"\n-        p1: Union[list, str] = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n+        p1: list | str = r'C:\\dir\\num\\one;C:\\dir\\num\\two'\n         # have to include the pathsep here so that the test will work on UNIX too.\n         p1 = AppendPath(p1, r'C:\\dir\\num\\one', sep=';', delete_existing=False)\n         p1 = AppendPath(p1, r'C:\\dir\\num\\three', sep=';')\ndiff --git a/SCons/Util/__init__.py b/SCons/Util/__init__.py\nindex 28565da797..0398c1fad7 100644\n--- a/SCons/Util/__init__.py\n+++ b/SCons/Util/__init__.py\n@@ -49,6 +49,8 @@\n #       )\n # (issue filed on this upstream, for now just be aware)\n \n+from __future__ import annotations\n+\n import copy\n import hashlib\n import logging\n@@ -59,7 +61,7 @@\n from collections import UserDict, UserList, deque\n from contextlib import suppress\n from types import MethodType, FunctionType\n-from typing import Optional, Union, Any, List\n+from typing import Any\n from logging import Formatter\n \n # Util split into a package. Make sure things that used to work\n@@ -203,11 +205,11 @@ def __str__(self) -> str:\n     def __iter__(self):\n         return iter(self.data)\n \n-    def __call__(self, *args, **kwargs) -> 'NodeList':\n+    def __call__(self, *args, **kwargs) -> NodeList:\n         result = [x(*args, **kwargs) for x in self.data]\n         return self.__class__(result)\n \n-    def __getattr__(self, name) -> 'NodeList':\n+    def __getattr__(self, name) -> NodeList:\n         \"\"\"Returns a NodeList of `name` from each member.\"\"\"\n         result = [getattr(x, name) for x in self.data]\n         return self.__class__(result)\n@@ -254,8 +256,8 @@ def render_tree(\n     root,\n     child_func,\n     prune: bool = False,\n-    margin: List[bool] = [False],\n-    visited: Optional[dict] = None,\n+    margin: list[bool] = [False],\n+    visited: dict | None = None,\n ) -> str:\n     \"\"\"Render a tree of nodes into an ASCII tree view.\n \n@@ -323,8 +325,8 @@ def print_tree(\n     child_func,\n     prune: bool = False,\n     showtags: int = 0,\n-    margin: List[bool] = [False],\n-    visited: Optional[dict] = None,\n+    margin: list[bool] = [False],\n+    visited: dict | None = None,\n     lastChild: bool = False,\n     singleLineDraw: bool = False,\n ) -> None:\n@@ -694,7 +696,7 @@ def RegOpenKeyEx(root, key):\n \n if sys.platform == 'win32':\n \n-    def WhereIs(file, path=None, pathext=None, reject=None) -> Optional[str]:\n+    def WhereIs(file, path=None, pathext=None, reject=None) -> str | None:\n         if path is None:\n             try:\n                 path = os.environ['PATH']\n@@ -731,7 +733,7 @@ def WhereIs(file, path=None, pathext=None, reject=None) -> Optional[str]:\n \n elif os.name == 'os2':\n \n-    def WhereIs(file, path=None, pathext=None, reject=None) -> Optional[str]:\n+    def WhereIs(file, path=None, pathext=None, reject=None) -> str | None:\n         if path is None:\n             try:\n                 path = os.environ['PATH']\n@@ -763,7 +765,7 @@ def WhereIs(file, path=None, pathext=None, reject=None) -> Optional[str]:\n \n else:\n \n-    def WhereIs(file, path=None, pathext=None, reject=None) -> Optional[str]:\n+    def WhereIs(file, path=None, pathext=None, reject=None) -> str | None:\n         import stat  # pylint: disable=import-outside-toplevel\n \n         if path is None:\ndiff --git a/SCons/Util/envs.py b/SCons/Util/envs.py\nindex 2640ef5c19..9c97da6e9e 100644\n--- a/SCons/Util/envs.py\n+++ b/SCons/Util/envs.py\n@@ -9,10 +9,12 @@\n that don't need the specifics of the Environment class.\n \"\"\"\n \n+from __future__ import annotations\n+\n import re\n import os\n from types import MethodType, FunctionType\n-from typing import Union, Callable, Optional, Any\n+from typing import Callable, Any\n \n from .sctypes import is_List, is_Tuple, is_String\n \n@@ -22,8 +24,8 @@ def PrependPath(\n     newpath,\n     sep=os.pathsep,\n     delete_existing: bool = True,\n-    canonicalize: Optional[Callable] = None,\n-) -> Union[list, str]:\n+    canonicalize: Callable | None = None,\n+) -> list | str:\n     \"\"\"Prepend *newpath* path elements to *oldpath*.\n \n     Will only add any particular path once (leaving the first one it\n@@ -112,8 +114,8 @@ def AppendPath(\n     newpath,\n     sep=os.pathsep,\n     delete_existing: bool = True,\n-    canonicalize: Optional[Callable] = None,\n-) -> Union[list, str]:\n+    canonicalize: Callable | None = None,\n+) -> list | str:\n     \"\"\"Append *newpath* path elements to *oldpath*.\n \n     Will only add any particular path once (leaving the last one it\n@@ -239,7 +241,7 @@ class MethodWrapper:\n     a new underlying object being copied (without which we wouldn't need\n     to save that info).\n     \"\"\"\n-    def __init__(self, obj: Any, method: Callable, name: Optional[str] = None) -> None:\n+    def __init__(self, obj: Any, method: Callable, name: str | None = None) -> None:\n         if name is None:\n             name = method.__name__\n         self.object = obj\n@@ -275,7 +277,7 @@ def clone(self, new_object):\n #   is not needed, the remaining bit is now used inline in AddMethod.\n \n \n-def AddMethod(obj, function: Callable, name: Optional[str] = None) -> None:\n+def AddMethod(obj, function: Callable, name: str | None = None) -> None:\n     \"\"\"Add a method to an object.\n \n     Adds *function* to *obj* if *obj* is a class object.\n@@ -314,7 +316,7 @@ def AddMethod(obj, function: Callable, name: Optional[str] = None) -> None:\n             function.__code__, function.__globals__, name, function.__defaults__\n         )\n \n-    method: Union[MethodType, MethodWrapper, Callable]\n+    method: MethodType | MethodWrapper | Callable\n \n     if hasattr(obj, '__class__') and obj.__class__ is not type:\n         # obj is an instance, so it gets a bound method.\ndiff --git a/SCons/Util/filelock.py b/SCons/Util/filelock.py\nindex 8ebf3889fe..730f48607b 100644\n--- a/SCons/Util/filelock.py\n+++ b/SCons/Util/filelock.py\n@@ -30,9 +30,10 @@\n #   The lock attributes could probably be made opaque. Showed one visible\n #     in the example above, but not sure the benefit of that.\n \n+from __future__ import annotations\n+\n import os\n import time\n-from typing import Optional\n \n \n class SConsLockFailure(Exception):\n@@ -75,8 +76,8 @@ class FileLock:\n     def __init__(\n         self,\n         file: str,\n-        timeout: Optional[int] = None,\n-        delay: Optional[float] = 0.05,\n+        timeout: int | None = None,\n+        delay: float | None = 0.05,\n         writer: bool = False,\n     ) -> None:\n         if timeout is not None and delay is None:\n@@ -90,7 +91,7 @@ def __init__(\n         # Our simple first guess is just put it where the file is.\n         self.file = file\n         self.lockfile = f\"{file}.lock\"\n-        self.lock: Optional[int] = None\n+        self.lock: int | None = None\n         self.timeout = 999999 if timeout == 0 else timeout\n         self.delay = 0.0 if delay is None else delay\n         self.writer = writer\n@@ -128,7 +129,7 @@ def release_lock(self) -> None:\n             os.unlink(self.lockfile)\n             self.lock = None\n \n-    def __enter__(self) -> \"FileLock\":\n+    def __enter__(self) -> FileLock:\n         \"\"\"Context manager entry: acquire lock if not holding.\"\"\"\n         if not self.lock:\n             self.acquire_lock()\ndiff --git a/SCons/Util/hashes.py b/SCons/Util/hashes.py\nindex 566897abbe..016354c6e5 100644\n--- a/SCons/Util/hashes.py\n+++ b/SCons/Util/hashes.py\n@@ -8,10 +8,11 @@\n Routines for working with content and signature hashes.\n \"\"\"\n \n+from __future__ import annotations\n+\n import functools\n import hashlib\n import sys\n-from typing import Optional, Union\n \n from .sctypes import to_bytes\n \ndiff --git a/SCons/Util/sctypes.py b/SCons/Util/sctypes.py\nindex 765458e130..b95e395c80 100644\n--- a/SCons/Util/sctypes.py\n+++ b/SCons/Util/sctypes.py\n@@ -6,13 +6,13 @@\n \n Routines which check types and do type conversions.\n \"\"\"\n+from __future__ import annotations\n \n import codecs\n import os\n import pprint\n import re\n import sys\n-from typing import Optional, Union\n \n from collections import UserDict, UserList, UserString, deque\n from collections.abc import MappingView, Iterable\n@@ -56,20 +56,24 @@\n if sys.version_info >= (3, 13):\n     from typing import TypeAlias, TypeIs\n \n-    DictTypeRet: TypeAlias = TypeIs[Union[dict, UserDict]]\n-    ListTypeRet: TypeAlias = TypeIs[Union[list, UserList, deque]]\n-    SequenceTypeRet: TypeAlias = TypeIs[Union[list, tuple, deque, UserList, MappingView]]\n+    DictTypeRet: TypeAlias = TypeIs[dict | UserDict]\n+    ListTypeRet: TypeAlias = TypeIs[list | UserList | deque]\n+    SequenceTypeRet: TypeAlias = TypeIs[list | tuple | deque | UserList | MappingView]\n     TupleTypeRet: TypeAlias = TypeIs[tuple]\n-    StringTypeRet: TypeAlias = TypeIs[Union[str, UserString]]\n+    StringTypeRet: TypeAlias = TypeIs[str | UserString]\n elif sys.version_info >= (3, 10):\n     from typing import TypeAlias, TypeGuard\n \n-    DictTypeRet: TypeAlias = TypeGuard[Union[dict, UserDict]]\n-    ListTypeRet: TypeAlias = TypeGuard[Union[list, UserList, deque]]\n-    SequenceTypeRet: TypeAlias = TypeGuard[Union[list, tuple, deque, UserList, MappingView]]\n+    DictTypeRet: TypeAlias = TypeGuard[dict | UserDict]\n+    ListTypeRet: TypeAlias = TypeGuard[list | UserList | deque]\n+    SequenceTypeRet: TypeAlias = TypeGuard[list | tuple | deque | UserList | MappingView]\n     TupleTypeRet: TypeAlias = TypeGuard[tuple]\n-    StringTypeRet: TypeAlias = TypeGuard[Union[str, UserString]]\n+    StringTypeRet: TypeAlias = TypeGuard[str | UserString]\n else:\n+    # Because we have neither `TypeAlias` class nor `type` keyword pre-3.10,\n+    # the boolean fallback type has to be wrapped in the legacy `Union` class.\n+    from typing import Union\n+\n     DictTypeRet = Union[bool, bool]\n     ListTypeRet = Union[bool, bool]\n     SequenceTypeRet = Union[bool, bool]\n@@ -354,7 +358,7 @@ def get_os_env_bool(name: str, default: bool=False) -> bool:\n _get_env_var = re.compile(r'^\\$([_a-zA-Z]\\w*|{[_a-zA-Z]\\w*})$')\n \n \n-def get_environment_var(varstr) -> Optional[str]:\n+def get_environment_var(varstr) -> str | None:\n     \"\"\"Return undecorated construction variable string.\n \n     Determine if *varstr* looks like a reference\ndiff --git a/SCons/Util/sctyping.py b/SCons/Util/sctyping.py\ndeleted file mode 100644\nindex 5da5eb9fe7..0000000000\n--- a/SCons/Util/sctyping.py\n+++ /dev/null\n@@ -1,33 +0,0 @@\n-# SPDX-License-Identifier: MIT\n-#\n-# Copyright The SCons Foundation\n-\n-\"\"\"Various SCons type aliases.\n-\n-For representing complex types across the entire repo without risking\n-circular dependencies, we take advantage of TYPE_CHECKING to import\n-modules in an tool-only environment. This allows us to introduce\n-hinting that resolves as expected in IDEs without clashing at runtime.\n-\n-For consistency, it's recommended to ALWAYS use these aliases in a\n-type-hinting context, even if the type is actually expected to be\n-resolved in a given file.\n-\"\"\"\n-\n-from typing import Union, TYPE_CHECKING\n-\n-if TYPE_CHECKING:\n-    import SCons.Executor\n-\n-\n-# Because we don't have access to TypeAlias until 3.10, we have to utilize\n-# 'Union' for all aliases. As it expects at least two entries, anything that\n-# is only represented with a single type needs to list itself twice.\n-ExecutorType = Union[\"SCons.Executor.Executor\", \"SCons.Executor.Executor\"]\n-\n-\n-# Local Variables:\n-# tab-width:4\n-# indent-tabs-mode:nil\n-# End:\n-# vim: set expandtab tabstop=4 shiftwidth=4:\ndiff --git a/SCons/Variables/BoolVariable.py b/SCons/Variables/BoolVariable.py\nindex 815a4b7865..573a0244e7 100644\n--- a/SCons/Variables/BoolVariable.py\n+++ b/SCons/Variables/BoolVariable.py\n@@ -32,7 +32,9 @@\n         ...\n \"\"\"\n \n-from typing import Callable, Tuple, Union\n+from __future__ import annotations\n+\n+from typing import Callable\n \n import SCons.Errors\n \n@@ -42,7 +44,7 @@\n FALSE_STRINGS = ('n', 'no', 'false', 'f', '0', 'off', 'none')\n \n \n-def _text2bool(val: Union[str, bool]) -> bool:\n+def _text2bool(val: str | bool) -> bool:\n     \"\"\"Convert boolean-like string to boolean.\n \n     If *val* looks like it expresses a bool-like value, based on\n@@ -83,7 +85,7 @@ def _validator(key: str, val, env) -> None:\n         raise SCons.Errors.UserError(msg) from None\n \n # lint: W0622: Redefining built-in 'help' (redefined-builtin)\n-def BoolVariable(key, help: str, default) -> Tuple[str, str, str, Callable, Callable]:\n+def BoolVariable(key, help: str, default) -> tuple[str, str, str, Callable, Callable]:\n     \"\"\"Return a tuple describing a boolean SCons Variable.\n \n     The input parameters describe a boolean variable, using a string\ndiff --git a/SCons/Variables/EnumVariable.py b/SCons/Variables/EnumVariable.py\nindex 3698e470dc..f154a133b7 100644\n--- a/SCons/Variables/EnumVariable.py\n+++ b/SCons/Variables/EnumVariable.py\n@@ -43,7 +43,9 @@\n         ...\n \"\"\"\n \n-from typing import Callable, List, Optional, Tuple\n+from __future__ import annotations\n+\n+from typing import Callable\n \n import SCons.Errors\n \n@@ -69,10 +71,10 @@ def EnumVariable(\n     key,\n     help: str,\n     default: str,\n-    allowed_values: List[str],\n-    map: Optional[dict] = None,\n+    allowed_values: list[str],\n+    map: dict | None = None,\n     ignorecase: int = 0,\n-) -> Tuple[str, str, str, Callable, Callable]:\n+) -> tuple[str, str, str, Callable, Callable]:\n     \"\"\"Return a tuple describing an enumaration SCons Variable.\n \n     The input parameters describe a variable with only predefined values\ndiff --git a/SCons/Variables/ListVariable.py b/SCons/Variables/ListVariable.py\nindex 2c79ee7f15..4ea7dc304c 100644\n--- a/SCons/Variables/ListVariable.py\n+++ b/SCons/Variables/ListVariable.py\n@@ -53,9 +53,11 @@\n # Known Bug: This should behave like a Set-Type, but does not really,\n # since elements can occur twice.\n \n+from __future__ import annotations\n+\n import collections\n import functools\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable\n \n import SCons.Util\n \n@@ -75,7 +77,7 @@ class _ListVariable(collections.UserList):\n     \"\"\"\n \n     def __init__(\n-        self, initlist: Optional[list] = None, allowedElems: Optional[list] = None\n+        self, initlist: list | None = None, allowedElems: list | None = None\n     ) -> None:\n         if initlist is None:\n             initlist = []\n@@ -179,11 +181,11 @@ def _validator(key, val, env) -> None:\n def ListVariable(\n     key,\n     help: str,\n-    default: Union[str, List[str]],\n-    names: List[str],\n-    map: Optional[dict] = None,\n-    validator: Optional[Callable] = None,\n-) -> Tuple[str, str, str, None, Callable]:\n+    default: str | list[str],\n+    names: list[str],\n+    map: dict | None = None,\n+    validator: Callable | None = None,\n+) -> tuple[str, str, str, None, Callable]:\n     \"\"\"Return a tuple describing a list variable.\n \n     The input parameters describe a list variable, where the values\ndiff --git a/SCons/Variables/PackageVariable.py b/SCons/Variables/PackageVariable.py\nindex 7271cfbf8f..2ecedfe77a 100644\n--- a/SCons/Variables/PackageVariable.py\n+++ b/SCons/Variables/PackageVariable.py\n@@ -50,9 +50,11 @@\n         ...  # build with x11 ...\n \"\"\"\n \n+from __future__ import annotations\n+\n import os\n import functools\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable\n \n import SCons.Errors\n \n@@ -61,7 +63,7 @@\n ENABLE_STRINGS = ('1', 'yes', 'true',  'on', 'enable', 'search')\n DISABLE_STRINGS = ('0', 'no',  'false', 'off', 'disable')\n \n-def _converter(val: Union[str, bool], default: str) -> Union[str, bool]:\n+def _converter(val: str | bool, default: str) -> str | bool:\n     \"\"\"Convert a package variable.\n \n     Returns *val* if it looks like a path string, and ``False`` if it\n@@ -108,8 +110,8 @@ def _validator(key: str, val, env, searchfunc) -> None:\n \n # lint: W0622: Redefining built-in 'help' (redefined-builtin)\n def PackageVariable(\n-    key: str, help: str, default, searchfunc: Optional[Callable] = None\n-) -> Tuple[str, str, str, Callable, Callable]:\n+    key: str, help: str, default, searchfunc: Callable | None = None\n+) -> tuple[str, str, str, Callable, Callable]:\n     \"\"\"Return a tuple describing a package list SCons Variable.\n \n     The input parameters describe a 'package list' variable. Returns\ndiff --git a/SCons/Variables/PathVariable.py b/SCons/Variables/PathVariable.py\nindex 43904e62c7..f840a95d26 100644\n--- a/SCons/Variables/PathVariable.py\n+++ b/SCons/Variables/PathVariable.py\n@@ -71,10 +71,11 @@\n     )\n \"\"\"\n \n+from __future__ import annotations\n \n import os\n import os.path\n-from typing import Callable, Optional, Tuple\n+from typing import Callable\n \n import SCons.Errors\n import SCons.Util\n@@ -141,8 +142,8 @@ def PathExists(key: str, val, env) -> None:\n \n     # lint: W0622: Redefining built-in 'help' (redefined-builtin)\n     def __call__(\n-        self, key: str, help: str, default, validator: Optional[Callable] = None\n-    ) -> Tuple[str, str, str, Callable, None]:\n+        self, key: str, help: str, default, validator: Callable | None = None\n+    ) -> tuple[str, str, str, Callable, None]:\n         \"\"\"Return a tuple describing a path list SCons Variable.\n \n         The input parameters describe a 'path list' variable. Returns\ndiff --git a/SCons/Variables/__init__.py b/SCons/Variables/__init__.py\nindex 28325266fb..2d160072a5 100644\n--- a/SCons/Variables/__init__.py\n+++ b/SCons/Variables/__init__.py\n@@ -23,10 +23,12 @@\n \n \"\"\"Adds user-friendly customizable variables to an SCons build.\"\"\"\n \n+from __future__ import annotations\n+\n import os.path\n import sys\n from functools import cmp_to_key\n-from typing import Callable, Dict, List, Optional, Sequence, Union\n+from typing import Callable, Sequence\n \n import SCons.Errors\n import SCons.Util\n@@ -90,16 +92,16 @@ class Variables:\n \n     def __init__(\n         self,\n-        files: Optional[Union[str, Sequence[str]]] = None,\n-        args: Optional[dict] = None,\n+        files: str | Sequence[str | None] = None,\n+        args: dict | None = None,\n         is_global: bool = False,\n     ) -> None:\n-        self.options: List[Variable] = []\n+        self.options: list[Variable] = []\n         self.args = args if args is not None else {}\n         if not SCons.Util.is_Sequence(files):\n             files = [files] if files else []\n         self.files: Sequence[str] = files\n-        self.unknown: Dict[str, str] = {}\n+        self.unknown: dict[str, str] = {}\n \n     def __str__(self) -> str:\n         \"\"\"Provide a way to \"print\" a Variables object.\"\"\"\n@@ -113,11 +115,11 @@ def __str__(self) -> str:\n     # lint: W0622: Redefining built-in 'help'\n     def _do_add(\n         self,\n-        key: Union[str, List[str]],\n+        key: str | list[str],\n         help: str = \"\",\n         default=None,\n-        validator: Optional[Callable] = None,\n-        converter: Optional[Callable] = None,\n+        validator: Callable | None = None,\n+        converter: Callable | None = None,\n         **kwargs,\n     ) -> None:\n         \"\"\"Create a Variable and add it to the list.\n@@ -162,7 +164,7 @@ def keys(self) -> list:\n             yield option.key\n \n     def Add(\n-        self, key: Union[str, Sequence], *args, **kwargs,\n+        self, key: str | Sequence, *args, **kwargs,\n     ) -> None:\n         \"\"\"Add a Build Variable.\n \n@@ -218,7 +220,7 @@ def AddVariables(self, *optlist) -> None:\n         for opt in optlist:\n             self._do_add(*opt)\n \n-    def Update(self, env, args: Optional[dict] = None) -> None:\n+    def Update(self, env, args: dict | None = None) -> None:\n         \"\"\"Update an environment with the Build Variables.\n \n         Args:\n@@ -362,7 +364,7 @@ def Save(self, filename, env) -> None:\n             msg = f'Error writing options to file: {filename}\\n{exc}'\n             raise SCons.Errors.UserError(msg) from exc\n \n-    def GenerateHelpText(self, env, sort: Union[bool, Callable] = False) -> str:\n+    def GenerateHelpText(self, env, sort: bool | Callable = False) -> str:\n         \"\"\"Generate the help text for the Variables object.\n \n         Args:\n@@ -403,7 +405,7 @@ def FormatVariableHelpText(\n         help: str,\n         default,\n         actual,\n-        aliases: Optional[List[str]] = None,\n+        aliases: list[str | None] = None,\n     ) -> str:\n         \"\"\"Format the help text for a single variable.\n \ndiff --git a/SCons/Warnings.py b/SCons/Warnings.py\nindex d604659c40..b9f9cc16f4 100644\n--- a/SCons/Warnings.py\n+++ b/SCons/Warnings.py\n@@ -61,8 +61,10 @@\n framework and it will behave like an ordinary exception.\n \"\"\"\n \n+from __future__ import annotations\n+\n import sys\n-from typing import Callable, Sequence, Optional\n+from typing import Callable, Sequence\n \n import SCons.Errors\n \n@@ -75,7 +77,7 @@\n \n # Function to emit the warning. Initialized by SCons/Main.py for regular use;\n # the unit test will set to a capturing version for testing.\n-_warningOut: Optional[Callable] = None\n+_warningOut: Callable | None = None\n \n \n class SConsWarning(SCons.Errors.UserError):\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 1e00257be9..5f3a49e04f 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -84,6 +84,15 @@ extend-exclude = [\n     \"SCons/Tool/docbook/docbook-xsl-1.76.1/\",\n ]\n \n+[tool.ruff.lint]\n+extend-select = [\n+    \"FA\", # Future annotations\n+    \"UP006\", # Use {to} instead of {from} for type annotation\n+    \"UP007\", # Use `X | Y` for type annotations\n+    \"UP037\", # Remove quotes from type annotation\n+]\n+extend-safe-fixes = [\"FA\", \"UP006\", \"UP007\"]\n+\n [tool.ruff.format]\n quote-style = \"preserve\" # Equivalent to black's \"skip-string-normalization\"\n \n", "instance_id": "SCons__scons-4643", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement \"Remove Python 3.6 support\" is mostly clear in its intent to eliminate references to a deprecated Python version and update the codebase to a new baseline. It specifies that the change is standalone and mentions updates to documentation and tests, which provides some context. However, it lacks critical details such as the specific Python version to target as the new baseline, explicit mention of which parts of the codebase are affected (beyond general references to documentation and tests), and any specific constraints or compatibility issues to consider during the removal. There are no examples or detailed requirements provided for what \"removing support\" entails (e.g., specific syntax changes, library updates, or compatibility checks). This results in minor ambiguities that prevent a comprehensive score of 3.", "difficulty_explanation": "The difficulty of this task falls into the medium range due to several factors. First, the scope of code changes is significant, spanning multiple files (e.g., `SCons/Action.py`, `SCons/Executor.py`, `CHANGES.txt`, `RELEASE.txt`, and various test files), indicating a need to understand and modify interactions across different parts of the codebase. The changes primarily involve updating type hints and annotations to leverage Python's `from __future__ import annotations` feature, removing a custom typing module (`SCons.Util.sctyping.py`), and adjusting documentation. This requires a moderate understanding of Python's type system, particularly post-Python 3.6 features, and familiarity with the project's structure to ensure consistency across files.\n\nThe technical concepts involved include Python type annotations, future imports, and potentially understanding the implications of dropping support for an older Python version on the codebase's compatibility and testing framework. While these concepts are not overly complex for an experienced developer, they do require careful attention to detail to avoid introducing errors, especially in a large codebase like SCons. The amount of code change is substantial but not architectural; it involves repetitive updates to type hints and signatures rather than deep refactoring or redesign.\n\nEdge cases and error handling requirements are not explicitly mentioned in the problem statement, but the code changes suggest a need to ensure that type annotations are correctly updated without breaking existing functionality. There is a risk of subtle bugs if annotations are mismatched or if dependencies on Python 3.6-specific behaviors are overlooked. However, the problem does not appear to involve complex performance considerations or system-level changes, keeping it from reaching a higher difficulty score.\n\nOverall, I rate this as 0.45, reflecting a medium difficulty task that requires understanding multiple files and concepts but does not demand deep architectural changes or advanced domain-specific knowledge. It is more involved than a simple bug fix or feature addition but not as challenging as a core system refactor.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Implement `__matmul__`, typehint pd.DataFrame.eval `inplace`  and test conversion for `test_frame.py`\n**Describe the bug**\r\n\r\n- @ operator on dataframe is not typehinted properly.\r\n- `df.eval(\"...\", inplace=True)` returns Unknown\r\n- Convert test for types into check/assert_type framework\r\n\r\n**To Reproduce**\r\n```python\r\nimport pandas as pd\r\n\r\ndf1 = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\r\ndf2 = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\r\n\r\ncheck(assert_type(df1 @ df2, pd.DataFrame ),pd.DataFrame )\r\ncheck(assert_type(df1.eval(\"c = 0 + 1\", inplace=True), None), type(None))\r\n```\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: [e.g. Windows, Linux, MacOS] MacOS\r\n - OS Version [e.g. 22] 15\r\n - python version 3.12\r\n - version of type checker 1.13\r\n - version of installed `pandas-stubs` 2.2.3.20241009\n", "patch": "diff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex c179b755..ad194dd9 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -281,7 +281,12 @@ class DataFrame(NDFrame, OpsMixin):\n     def dot(self, other: DataFrame | ArrayLike) -> DataFrame: ...\n     @overload\n     def dot(self, other: Series) -> Series: ...\n-    def __matmul__(self, other): ...\n+    @overload\n+    def __matmul__(self, other: DataFrame) -> DataFrame: ...\n+    @overload\n+    def __matmul__(self, other: Series) -> Series: ...\n+    @overload\n+    def __matmul__(self, other: np.ndarray) -> DataFrame: ...\n     def __rmatmul__(self, other): ...\n     @overload\n     @classmethod\n@@ -620,7 +625,12 @@ class DataFrame(NDFrame, OpsMixin):\n     def query(\n         self, expr: _str, *, inplace: Literal[False] = ..., **kwargs\n     ) -> DataFrame: ...\n-    def eval(self, expr: _str, *, inplace: _bool = ..., **kwargs): ...\n+    @overload\n+    def eval(self, expr: _str, *, inplace: Literal[True], **kwargs) -> None: ...\n+    @overload\n+    def eval(\n+        self, expr: _str, *, inplace: Literal[False] = ..., **kwargs\n+    ) -> Scalar | np.ndarray | DataFrame | Series: ...\n     AstypeArgExt: TypeAlias = (\n         AstypeArg\n         | Literal[\n", "instance_id": "pandas-dev__pandas-stubs-1054", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issues to be addressed: implementing proper type hints for the `@` operator (matrix multiplication) on a DataFrame, fixing the return type of `df.eval()` with the `inplace` parameter, and converting tests to a specific framework. It provides a reproducible code snippet and specifies the environment details (OS, Python version, library versions), which are helpful. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the expected behavior or constraints for edge cases (e.g., incompatible matrix dimensions for `@` or invalid expressions in `eval`). Additionally, the requirement to \"convert test for types into check/assert_type framework\" lacks specificity about the scope of tests or the exact framework details. These gaps prevent it from being fully comprehensive, but the overall intent and requirements are understandable.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized to a single file (`pandas-stubs/core/frame.pyi`), specifically updating type hints for two methods (`__matmul__` and `eval`). The modifications involve adding overloads to handle different input types and return values, which is straightforward. There is no indication of broader architectural impact or interaction with other parts of the codebase beyond this stub file. The amount of code change is minimal, focusing on type annotations rather than logic implementation.\n\n2. **Technical Concepts Required**: The problem requires understanding of Python type hinting, specifically the use of `@overload` decorators from the `typing` module, and familiarity with pandas' DataFrame and Series types. These concepts are relatively basic for someone with experience in Python and type systems. No complex algorithms, design patterns, or domain-specific knowledge beyond pandas' API are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement and code changes do not explicitly address edge cases or error handling. The type hints added are purely declarative and do not involve runtime logic for handling errors (e.g., matrix dimension mismatches for `__matmul__` or invalid expressions in `eval`). This reduces the complexity of the task.\n\n4. **Overall Complexity**: The task is primarily about improving static type checking for a library stub file, which is a well-defined and constrained problem. It does not require deep understanding of the pandas internals or significant debugging. The mention of converting tests to a specific framework adds a minor layer of complexity, but since no test code changes are provided in the diff, I assume this part is either trivial or out of scope for the current evaluation.\n\nGiven these points, a score of 0.30 reflects a problem that is easy but requires some understanding of type hinting and pandas' API to implement correctly. It is not a trivial typo fix (which would be closer to 0.0-0.2), but it also does not approach medium difficulty due to the limited scope and lack of complex logic or cross-module dependencies.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Check incorrect when using a TimestampSeries and YearEnd from pandas.tseries.offsets\nIt seems type annotations are wrong between a Timestamp series and tools offered by the offsets library, using `pyright`\r\n\r\ndf[\"year_end\"] = df[\"date\"] + pd.tseries.offsets.YearEnd(0) is flagged with Operator \"+\" not supported for types \"TimestampSeries\" and \"YearEnd\"\r\n\r\n - OS: MacOS\r\n - OS Version 13.5\r\n - python 3.9.19\r\n - pylance version v2024.7.1\n", "patch": "diff --git a/pandas-stubs/core/indexes/datetimes.pyi b/pandas-stubs/core/indexes/datetimes.pyi\nindex b1f73409..b0de6f5a 100644\n--- a/pandas-stubs/core/indexes/datetimes.pyi\n+++ b/pandas-stubs/core/indexes/datetimes.pyi\n@@ -57,13 +57,13 @@ class DatetimeIndex(DatetimeTimedeltaMixin[Timestamp], DatetimeIndexProperties):\n     def __add__(self, other: TimedeltaSeries) -> TimestampSeries: ...\n     @overload\n     def __add__(\n-        self, other: timedelta | Timedelta | TimedeltaIndex\n+        self, other: timedelta | Timedelta | TimedeltaIndex | BaseOffset\n     ) -> DatetimeIndex: ...\n     @overload\n     def __sub__(self, other: TimedeltaSeries) -> TimestampSeries: ...\n     @overload\n     def __sub__(\n-        self, other: timedelta | Timedelta | TimedeltaIndex\n+        self, other: timedelta | Timedelta | TimedeltaIndex | BaseOffset\n     ) -> DatetimeIndex: ...\n     @overload\n     def __sub__(\ndiff --git a/pandas-stubs/core/series.pyi b/pandas-stubs/core/series.pyi\nindex 24bac1a1..24c47576 100644\n--- a/pandas-stubs/core/series.pyi\n+++ b/pandas-stubs/core/series.pyi\n@@ -2073,7 +2073,7 @@ class Series(IndexOpsMixin[S1], NDFrame):\n class TimestampSeries(Series[Timestamp]):\n     @property\n     def dt(self) -> TimestampProperties: ...  # type: ignore[override] # pyright: ignore[reportIncompatibleMethodOverride]\n-    def __add__(self, other: TimedeltaSeries | np.timedelta64 | timedelta) -> TimestampSeries: ...  # type: ignore[override] # pyright: ignore[reportIncompatibleMethodOverride]\n+    def __add__(self, other: TimedeltaSeries | np.timedelta64 | timedelta | BaseOffset) -> TimestampSeries: ...  # type: ignore[override] # pyright: ignore[reportIncompatibleMethodOverride]\n     def __radd__(self, other: TimedeltaSeries | np.timedelta64 | timedelta) -> TimestampSeries: ...  # type: ignore[override] # pyright: ignore[reportIncompatibleMethodOverride]\n     @overload  # type: ignore[override]\n     def __sub__(\n@@ -2082,7 +2082,9 @@ class TimestampSeries(Series[Timestamp]):\n     @overload\n     def __sub__(  # pyright: ignore[reportIncompatibleMethodOverride]\n         self,\n-        other: timedelta | TimedeltaSeries | TimedeltaIndex | np.timedelta64,\n+        other: (\n+            timedelta | TimedeltaSeries | TimedeltaIndex | np.timedelta64 | BaseOffset\n+        ),\n     ) -> TimestampSeries: ...\n     def __mul__(self, other: float | Series[int] | Series[float] | Sequence[float]) -> TimestampSeries: ...  # type: ignore[override] # pyright: ignore[reportIncompatibleMethodOverride]\n     def __truediv__(self, other: float | Series[int] | Series[float] | Sequence[float]) -> TimestampSeries: ...  # type: ignore[override] # pyright: ignore[reportIncompatibleMethodOverride]\n", "instance_id": "pandas-dev__pandas-stubs-1004", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a type annotation error when using a `TimestampSeries` with `YearEnd` from `pandas.tseries.offsets` in conjunction with the `pyright` type checker. It specifies the problematic operation (`df[\"date\"] + pd.tseries.offsets.YearEnd(0)`), the error message, and relevant environment details (OS, Python version, Pylance version). However, it lacks deeper context about the expected behavior or specific use cases, and it does not mention any edge cases or constraints that might need to be considered when addressing the type annotation issue. Additionally, there are no examples of correct or incorrect outputs beyond the error message. While the goal is understandable (fix the type annotation), the statement could benefit from more detailed requirements or examples to ensure comprehensive clarity.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are localized to type annotation updates in two files (`datetimes.pyi` and `series.pyi`) within the `pandas-stubs` repository. The modifications involve adding `BaseOffset` as a supported type for addition and subtraction operations in `DatetimeIndex` and `TimestampSeries`. This does not impact the broader architecture of the system or require extensive refactoring, as it is purely a type hint adjustment.\n\n2. **Number of Technical Concepts:** Solving this requires a basic understanding of Python type annotations, specifically with `pyright` or similar static type checkers, and familiarity with the `pandas` library's type system (e.g., `TimestampSeries`, `BaseOffset`). These concepts are not overly complex for someone with moderate experience in Python or type systems, though it does require some domain-specific knowledge of `pandas` internals or stub files.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not introduce or modify error handling logic. The focus is solely on type compatibility, so the complexity related to edge cases is minimal.\n\n4. **Overall Complexity:** The task is straightforward\u2014update type hints to include `BaseOffset` as a valid operand for certain operations. It does not require deep architectural changes, complex algorithms, or performance considerations. The primary challenge might be ensuring that the type annotation change aligns with the actual runtime behavior of `pandas`, but this is a relatively minor concern given the narrow scope.\n\nThus, a score of 0.3 reflects a problem that is easy to solve with some understanding of type annotations and the `pandas` library, involving minimal code changes and no significant impact on the broader codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "read_excel/index_col: string not accepted\nEither the documentation or the type hint for index_col in pandas.read_excel is wrong.\r\n\r\nThe documentation says a string may be used.\r\n\r\nThe type hint forbids that\r\n\r\nDocumentation:\r\nindex_col     int, **str**, list of int, default None\r\n\r\nType hints:\r\n`index_col: int | Sequence[int] | None = ...,`\nread_excel/index_col: string not accepted\nEither the documentation or the type hint for index_col in pandas.read_excel is wrong.\r\n\r\nThe documentation says a string may be used.\r\n\r\nThe type hint forbids that\r\n\r\nDocumentation:\r\nindex_col     int, **str**, list of int, default None\r\n\r\nType hints:\r\n`index_col: int | Sequence[int] | None = ...,`\n", "patch": "diff --git a/pandas-stubs/io/excel/_base.pyi b/pandas-stubs/io/excel/_base.pyi\nindex a3368d2e..5937fa5f 100644\n--- a/pandas-stubs/io/excel/_base.pyi\n+++ b/pandas-stubs/io/excel/_base.pyi\n@@ -49,7 +49,7 @@ def read_excel(\n     *,\n     header: int | Sequence[int] | None = ...,\n     names: ListLikeHashable | None = ...,\n-    index_col: int | Sequence[int] | None = ...,\n+    index_col: int | Sequence[int] | str | None = ...,\n     usecols: str | UsecolsArgType = ...,\n     dtype: str | Dtype | Mapping[str, str | Dtype] | None = ...,\n     engine: ExcelReadEngine | None = ...,\n@@ -91,7 +91,7 @@ def read_excel(\n     *,\n     header: int | Sequence[int] | None = ...,\n     names: ListLikeHashable | None = ...,\n-    index_col: int | Sequence[int] | None = ...,\n+    index_col: int | Sequence[int] | str | None = ...,\n     usecols: str | UsecolsArgType = ...,\n     dtype: str | Dtype | Mapping[str, str | Dtype] | None = ...,\n     engine: ExcelReadEngine | None = ...,\n@@ -134,7 +134,7 @@ def read_excel(  # type: ignore[misc]\n     *,\n     header: int | Sequence[int] | None = ...,\n     names: ListLikeHashable | None = ...,\n-    index_col: int | Sequence[int] | None = ...,\n+    index_col: int | Sequence[int] | str | None = ...,\n     usecols: str | UsecolsArgType = ...,\n     dtype: str | Dtype | Mapping[str, str | Dtype] | None = ...,\n     engine: ExcelReadEngine | None = ...,\n@@ -176,7 +176,7 @@ def read_excel(\n     *,\n     header: int | Sequence[int] | None = ...,\n     names: ListLikeHashable | None = ...,\n-    index_col: int | Sequence[int] | None = ...,\n+    index_col: int | Sequence[int] | str | None = ...,\n     usecols: str | UsecolsArgType = ...,\n     dtype: str | Dtype | Mapping[str, str | Dtype] | None = ...,\n     engine: ExcelReadEngine | None = ...,\n", "instance_id": "pandas-dev__pandas-stubs-914", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: there is a discrepancy between the documentation and the type hints for the `index_col` parameter in `pandas.read_excel`. The documentation indicates that a string is a valid input, while the type hint does not allow it. The goal of resolving this mismatch is evident. However, the statement lacks critical details, such as whether the intended resolution is to update the type hint (as done in the code changes) or to update the documentation instead. Additionally, there are no examples or edge cases provided to illustrate the issue (e.g., what kind of string input should be accepted and how it should behave). Despite these minor ambiguities, the problem is valid and the intent is understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to the type hints in a single file (`pandas-stubs/io/excel/_base.pyi`). The code changes are minimal, consisting of adding `str` as an acceptable type for the `index_col` parameter in four instances within the same file. This does not require deep understanding of the codebase, complex logic, or interaction between multiple modules. The technical concept involved is basic\u2014understanding Python type hints and union types (`|` operator)\u2014which is a fundamental skill for any developer working with Python. There are no edge cases or error handling considerations mentioned in the problem statement or evident in the code changes. The scope of the change is extremely limited, with no impact on the system's architecture or performance. Therefore, I assign a difficulty score of 0.1, placing it in the \"Very Easy\" category, as it is a simple fix akin to correcting a typo or updating a type annotation.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add IBM Cloud Object Storage Integration\nThe goal of this issue is to build a new handler to integrate IBM Cloud Object Storage with MindsDB.\r\n\r\n## Resources :bulb: \r\n\r\n:checkered_flag: To get started, please refer to the following resources:\r\n\r\n* :books:  [Building Application Handler docs](https://docs.mindsdb.com/contribute/app-handlers): This documentation will guide you through the process of building a custom handler for MindsDB. It contains essential information on the handler structure, methods, and best practices.\r\n* :book:  [Object Storage Python SDK](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-python) \r\n*  :point_right:  [S3 API Handler]https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/s3_handlerr)\r\n\r\n## Next Steps :male_detective: :female_detective: \r\n \r\n-   You can familiarize yourself with IBM Cloud Object Storage.\r\n-   Create a new handler that connects MindsDB to IBM Cloud Object Storage.\r\n-   Implement the core features mentioned above.\r\n-   Thoroughly test the integration to ensure proper functionality.\r\n-   Submit a pull request with the new IBM Cloud Object Storage handler.\r\n\r\n## :loudspeaker: Additional rewards :trophy:\r\n\r\nEach App integration brings :eight:  :zero:  : points for SWAG and entry into the draw for a  :point_down: :\r\n- :computer:  [Razer Blade 16 Laptop](https://www.razer.com/gaming-laptops/Razer-Blade-16/RZ09-0483SEJ3-R3U1)\r\n- :shirt: :bear:  [MindsDB Swag](https://store.covver.io/mindsdb/collections/swag-collection)\r\n:information_source: For more info check out https://mindsdb.com/hacktoberfest/ :point_left: \r\n\r\n\r\n\n", "patch": "diff --git a/mindsdb/integrations/handlers/ibm_cos_handler/README.md b/mindsdb/integrations/handlers/ibm_cos_handler/README.md\nnew file mode 100644\nindex 00000000000..ab330c4b34d\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/README.md\n@@ -0,0 +1,98 @@\n+---\n+title: IBM COS\n+sidebarTitle: IBM Cloud Object Storage\n+---\n+\n+This documentation describes the integration of MindsDB with [IBM COS](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-getting-started-cloud-object-storage), an object storage service that offers industry-leading scalability, data availability, security, and performance.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure that MindsDB is installed locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+\n+## Connection\n+\n+Establish a connection to your IBM COS buckets from MindsDB by executing the following SQL command:\n+\n+```sql\n+CREATE DATABASE ibm_datasource\n+WITH ENGINE = 'ibm_cos',\n+    PARAMETERS = {\n+        'cos_hmac_access_key_id': 'your-access-key-id',\n+        'cos_hmac_secret_access_key': 'your-secret-access-key',\n+        'cos_endpoint_url': 'https://s3.eu-gb.cloud-object-storage.appdomain.cloud',\n+        'bucket': 'your-bucket-name' -- Not required\n+    };\n+```\n+\n+<Note>\n+Note that sample parameter values are provided here for reference, and you should replace them with your connection parameters.\n+</Note>\n+\n+Required connection parameters include the following:\n+\n+- `cos_hmac_access_key_id`: The IBM COS access key that identifies the user or IAM role.\n+- `cos_hmac_secret_access_key`: The IBM COS secret access key that identifies the user or IAM role.\n+- `cos_endpoint_url`: The IBM COS resource ID for your cloud Object Storage.\n+\n+Optional connection parameters include the following:\n+\n+- `bucket`: The name of the IBM COS bucket. If not provided, all available buckets can be queried, however, this can affect performance, especially when listing all of the available objects.\n+\n+## Usage\n+\n+Retrieve data from a specified object (file) in a IBM COS bucket by providing the integration name and the object key:\n+\n+```sql\n+SELECT *\n+FROM ibm_datasource.`my-file.csv`;\n+LIMIT 10;\n+```\n+\n+<Tip>\n+If a bucket name is provided in the `CREATE DATABASE` command, querying will be limited to that bucket and the bucket name can be ommitted from the object key as shown in the example above. However, if the bucket name is not provided, the object key must include the bucket name, such as `ibm_datasource.`my-bucket/my-folder/my-file.csv`.\n+\n+Wrap the object key in backticks (\\`) to avoid any issues parsing the SQL statements provided. This is especially important when the object key contains spaces, special characters or prefixes, such as `my-folder/my-file.csv`.\n+\n+At the moment, the supported file formats are CSV, TSV, JSON, and Parquet.\n+</Tip>\n+\n+<Note>\n+The above examples utilize `ibm_datasource` as the datasource name, which is defined in the `CREATE DATABASE` command.\n+</Note>\n+\n+The special `files` table can be used to list all objects available in the specified bucket or all buckets if the bucket name is not provided:\n+\n+```sql\n+SELECT *\n+FROM ibm_datasource.files LIMIT 10\n+```\n+\n+The content of files can also be retrieved by explicitly requesting the `content` column. This column is empty by default to avoid unnecessary data transfer:\n+\n+```sql\n+SELECT path, content\n+FROM ibm_datasource.files LIMIT 10\n+```\n+\n+<Tip>\n+This table will return all objects regardless of the file format, however, only the supported file formats mentioned above can be queried.\n+</Tip>\n+\n+## Troubleshooting Guide\n+\n+<Warning>\n+`Database Connection Error`\n+\n+- **Symptoms**: Failure to connect MindsDB with the Amazon S3 bucket.\n+- **Checklist**: 1. Make sure the IBM COS bucket exists. 2. Confirm that provided IBM COS credentials are correct. Try making a direct connection to the IBM COS bucket using the IBM CLI. 3. Ensure a stable network between MindsDB and IBM COS.\n+  </Warning>\n+\n+<Warning>\n+`SQL statement cannot be parsed by mindsdb_sql`\n+\n+- **Symptoms**: SQL queries failing or not recognizing object names containing spaces, special characters or prefixes.\n+- **Checklist**: 1. Ensure object names with spaces, special characters or prefixes are enclosed in backticks. 2. Examples:\n+  _ Incorrect: SELECT _ FROM integration.travel/travel_data.csv\n+  - Incorrect: SELECT _ FROM integration.'travel/travel_data.csv'\n+    _ Correct: SELECT \\_ FROM integration.\\`travel/travel_data.csv\\`\n+    </Warning>\ndiff --git a/mindsdb/integrations/handlers/ibm_cos_handler/__about__.py b/mindsdb/integrations/handlers/ibm_cos_handler/__about__.py\nnew file mode 100644\nindex 00000000000..c0197eaec7e\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/__about__.py\n@@ -0,0 +1,9 @@\n+__title__ = \"MindsDB IBM COS Handler\"\n+__package_name__ = \"mindsdb_ibm_cos_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for IBM Cloud Object Storage\"\n+__author__ = \"Ton Hoang Nguyen (Bill)\"\n+__github__ = \"'https://github.com/mindsdb/mindsdb'\"\n+__pypi__ = \"'https://github.com/mindsdb/mindsdb'\"\n+__license__ = \"MIT\"\n+__copyright__ = \"Copyright 2024 - mindsdb\"\ndiff --git a/mindsdb/integrations/handlers/ibm_cos_handler/__init__.py b/mindsdb/integrations/handlers/ibm_cos_handler/__init__.py\nnew file mode 100644\nindex 00000000000..d3dac2794bd\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/__init__.py\n@@ -0,0 +1,30 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+from .connection_args import connection_args, connection_args_example\n+\n+try:\n+    from .ibm_cos_handler import IBMCloudObjectStorageHandler as Handler\n+\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = \"IBM Cloud Object Storage\"\n+name = \"ibm_cos\"\n+type = HANDLER_TYPE.DATA\n+icon_path = \"icon.svg\"\n+\n+__all__ = [\n+    \"Handler\",\n+    \"version\",\n+    \"name\",\n+    \"type\",\n+    \"title\",\n+    \"description\",\n+    \"connection_args\",\n+    \"connection_args_example\",\n+    \"import_error\",\n+    \"icon_path\",\n+]\ndiff --git a/mindsdb/integrations/handlers/ibm_cos_handler/connection_args.py b/mindsdb/integrations/handlers/ibm_cos_handler/connection_args.py\nnew file mode 100644\nindex 00000000000..9fa875fc015\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/connection_args.py\n@@ -0,0 +1,38 @@\n+from collections import OrderedDict\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+connection_args = OrderedDict(\n+    cos_hmac_access_key_id={\n+        \"type\": ARG_TYPE.PWD,\n+        \"description\": \"IBM COS HMAC Access Key ID.\",\n+        \"required\": True,\n+        \"label\": \"HMAC Access Key ID\",\n+        \"secret\": True,\n+    },\n+    cos_hmac_secret_access_key={\n+        \"type\": ARG_TYPE.PWD,\n+        \"description\": \"IBM COS HMAC Secret Access Key.\",\n+        \"required\": True,\n+        \"label\": \"HMAC Secret Access Key\",\n+        \"secret\": True,\n+    },\n+    cos_endpoint_url={\n+        \"type\": ARG_TYPE.STR,\n+        \"description\": \"IBM COS Endpoint URL (e.g., https://s3.eu-gb.cloud-object-storage.appdomain.cloud).\",\n+        \"required\": True,\n+        \"label\": \"Endpoint URL\",\n+    },\n+    bucket={\n+        \"type\": ARG_TYPE.STR,\n+        \"description\": \"IBM COS Bucket Name (Optional).\",\n+        \"required\": False,\n+        \"label\": \"Bucket Name\",\n+    },\n+)\n+\n+connection_args_example = OrderedDict(\n+    cos_hmac_access_key_id=\"YOUR_HMAC_ACCESS_KEY_ID\",\n+    cos_hmac_secret_access_key=\"YOUR_HMAC_SECRET_ACCESS_KEY\",\n+    cos_endpoint_url=\"https://s3.eu-gb.cloud-object-storage.appdomain.cloud\",\n+    bucket=\"YOUR_BUCKET_NAME\",\n+)\ndiff --git a/mindsdb/integrations/handlers/ibm_cos_handler/ibm_cos_handler.py b/mindsdb/integrations/handlers/ibm_cos_handler/ibm_cos_handler.py\nnew file mode 100644\nindex 00000000000..df4dfd50623\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/ibm_cos_handler.py\n@@ -0,0 +1,338 @@\n+from contextlib import contextmanager\n+from typing import Text, Dict, Optional, List\n+\n+import ibm_boto3\n+from ibm_botocore.client import ClientError\n+import pandas as pd\n+import duckdb\n+\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.parser.ast import Select, Identifier, Insert, Star, Constant\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE,\n+)\n+\n+from mindsdb.integrations.libs.api_handler import APIResource, APIHandler\n+from mindsdb.integrations.utilities.sql_utils import FilterCondition, FilterOperator\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class ListFilesTable(APIResource):\n+    def list(\n+        self,\n+        targets: List[str] = None,\n+        conditions: List[FilterCondition] = None,\n+        limit: int = None,\n+        *args,\n+        **kwargs,\n+    ) -> pd.DataFrame:\n+\n+        buckets = None\n+        for condition in conditions:\n+            if condition.column == \"bucket\":\n+                if condition.op == FilterOperator.IN:\n+                    buckets = condition.value\n+                elif condition.op == FilterOperator.EQUAL:\n+                    buckets = [condition.value]\n+                condition.applied = True\n+\n+        data = []\n+        for obj in self.handler.get_objects(limit=limit, buckets=buckets):\n+            path = obj[\"Key\"]\n+            if obj[\"Filename\"].split(\".\")[1] in self.handler.supported_file_formats:\n+                item = {\n+                    \"path\": path,\n+                    \"bucket\": obj[\"Bucket\"],\n+                    \"name\": path[path.rfind(\"/\") + 1:],\n+                    \"extension\": path[path.rfind(\".\") + 1:],\n+                }\n+\n+                data.append(item)\n+\n+        return pd.DataFrame(data=data, columns=self.get_columns())\n+\n+    def get_columns(self) -> List[str]:\n+        return [\"path\", \"name\", \"extension\", \"bucket\", \"content\"]\n+\n+\n+class FileTable(APIResource):\n+    def list(\n+        self, targets: List[str] = None, table_name=None, *args, **kwargs\n+    ) -> pd.DataFrame:\n+        return self.handler.read_as_table(table_name)\n+\n+    def add(self, data, table_name=None):\n+        df = pd.DataFrame(data)\n+        return self.handler.add_data_to_table(table_name, df)\n+\n+\n+class IBMCloudObjectStorageHandler(APIHandler):\n+\n+    name = \"ibm_cos\"\n+    supported_file_formats = [\"csv\", \"tsv\", \"json\", \"parquet\"]\n+\n+    def __init__(self, name: Text, connection_data: Optional[Dict] = None, **kwargs):\n+        super().__init__(name)\n+        self.connection_data = connection_data or {}\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+        self.thread_safe = True\n+        self._regions = {}\n+\n+        self.bucket = self.connection_data.get(\"bucket\")\n+        self._files_table = ListFilesTable(self)\n+\n+    def __del__(self):\n+        if self.is_connected is True:\n+            self.disconnect()\n+\n+    def connect(self):\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        required_params = [\n+            \"cos_hmac_access_key_id\",\n+            \"cos_hmac_secret_access_key\",\n+            \"cos_endpoint_url\",\n+        ]\n+        if not all(key in self.connection_data for key in required_params):\n+            raise ValueError(\n+                \"Required parameters (cos_hmac_access_key_id, cos_hmac_secret_access_key, cos_endpoint_url) must be provided.\"\n+            )\n+\n+        self.connection = self._connect_ibm_boto3()\n+        self.is_connected = True\n+\n+        return self.connection\n+\n+    def _connect_ibm_boto3(self) -> ibm_boto3.client:\n+        config = {\n+            \"aws_access_key_id\": self.connection_data[\"cos_hmac_access_key_id\"],\n+            \"aws_secret_access_key\": self.connection_data[\"cos_hmac_secret_access_key\"],\n+            \"endpoint_url\": self.connection_data[\"cos_endpoint_url\"],\n+        }\n+\n+        client = ibm_boto3.client(\"s3\", **config)\n+\n+        if self.bucket is not None:\n+            client.head_bucket(Bucket=self.bucket)\n+        else:\n+            client.list_buckets()\n+\n+        return client\n+\n+    def disconnect(self):\n+        if not self.is_connected:\n+            return\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def check_connection(self) -> StatusResponse:\n+        response = StatusResponse(False)\n+        need_to_close = self.is_connected is False\n+\n+        try:\n+            self._connect_ibm_boto3()\n+            response.success = True\n+        except (ClientError, ValueError) as e:\n+            logger.error(\n+                f\"Error connecting to IBM COS with the given credentials, {e}!\"\n+            )\n+            response.error_message = str(e)\n+\n+        if response.success and need_to_close:\n+            self.disconnect()\n+\n+        elif not response.success and self.is_connected:\n+            self.is_connected = False\n+\n+        return response\n+\n+    @contextmanager\n+    def _connect_duckdb(self):\n+        duckdb_conn = duckdb.connect(\":memory:\")\n+        duckdb_conn.execute(\"INSTALL httpfs\")\n+        duckdb_conn.execute(\"LOAD httpfs\")\n+\n+        duckdb_conn.execute(\n+            f\"SET s3_access_key_id='{self.connection_data['cos_hmac_access_key_id']}'\"\n+        )\n+        duckdb_conn.execute(\n+            f\"SET s3_secret_access_key='{self.connection_data['cos_hmac_secret_access_key']}'\"\n+        )\n+\n+        endpoint_url = self.connection_data[\"cos_endpoint_url\"]\n+        if endpoint_url.startswith(\"https://\"):\n+            endpoint_url = endpoint_url[len(\"https://\"):]\n+        elif endpoint_url.startswith(\"http://\"):\n+            endpoint_url = endpoint_url[len(\"http://\"):]\n+\n+        duckdb_conn.execute(f\"SET s3_endpoint='{endpoint_url}'\")\n+        duckdb_conn.execute(\"SET s3_url_style='path'\")\n+        duckdb_conn.execute(\"SET s3_use_ssl=true\")\n+\n+        try:\n+            yield duckdb_conn\n+        finally:\n+            duckdb_conn.close()\n+\n+    def _get_bucket(self, key):\n+        if self.bucket is not None:\n+            return self.bucket, key\n+\n+        ar = key.split(\"/\")\n+        return ar[0], \"/\".join(ar[1:])\n+\n+    def read_as_table(self, key) -> pd.DataFrame:\n+        bucket, key = self._get_bucket(key)\n+\n+        with self._connect_duckdb() as connection:\n+\n+            cursor = connection.execute(f\"SELECT * FROM 's3://{bucket}/{key}'\")\n+\n+            return cursor.fetchdf()\n+\n+    def _read_as_content(self, key) -> None:\n+        bucket, key = self._get_bucket(key)\n+\n+        client = self.connect()\n+\n+        obj = client.get_object(Bucket=bucket, Key=key)\n+        content = obj[\"Body\"].read()\n+        return content\n+\n+    def add_data_to_table(self, key, df) -> None:\n+\n+        bucket, key = self._get_bucket(key)\n+\n+        try:\n+            client = self.connect()\n+            client.head_object(Bucket=bucket, Key=key)\n+        except ClientError as e:\n+            logger.error(f\"Error querying the file {key} in the bucket {bucket}, {e}!\")\n+            raise e\n+\n+        with self._connect_duckdb() as connection:\n+            connection.execute(\n+                f\"CREATE TABLE tmp_table AS SELECT * FROM 's3://{bucket}/{key}'\"\n+            )\n+\n+            connection.execute(\"INSERT INTO tmp_table BY NAME SELECT * FROM df\")\n+\n+            connection.execute(f\"COPY tmp_table TO 's3://{bucket}/{key}'\")\n+\n+    def query(self, query: ASTNode) -> Response:\n+        self.connect()\n+        if isinstance(query, Select):\n+            table_name = query.from_table.parts[-1]\n+\n+            if table_name == \"files\":\n+                table = self._files_table\n+                df = table.select(query)\n+\n+                has_content = False\n+                for target in query.targets:\n+                    if (\n+                        isinstance(target, Identifier)\n+                        and target.parts[-1].lower() == \"content\"\n+                    ):\n+                        has_content = True\n+                        break\n+                if has_content:\n+                    df[\"content\"] = df[\"path\"].apply(self._read_as_content)\n+            else:\n+                extension = table_name.split(\".\")[-1]\n+                if extension not in self.supported_file_formats:\n+                    logger.error(f\"The file format {extension} is not supported!\")\n+                    raise ValueError(f\"The file format {extension} is not supported!\")\n+\n+                table = FileTable(self, table_name=table_name)\n+                df = table.select(query)\n+\n+            response = Response(RESPONSE_TYPE.TABLE, data_frame=df)\n+        elif isinstance(query, Insert):\n+            table_name = query.table.parts[-1]\n+            table = FileTable(self, table_name=table_name)\n+            table.insert(query)\n+            response = Response(RESPONSE_TYPE.OK)\n+        else:\n+            raise NotImplementedError\n+\n+        return response\n+\n+    def get_objects(self, limit=None, buckets=None) -> List[dict]:\n+        client = self.connect()\n+        if self.bucket is not None:\n+            add_bucket_to_name = False\n+            scan_buckets = [self.bucket]\n+        else:\n+            add_bucket_to_name = True\n+            resp = client.list_buckets()\n+            scan_buckets = [b[\"Name\"] for b in resp[\"Buckets\"]]\n+\n+        objects = []\n+        for bucket in scan_buckets:\n+            if buckets is not None and bucket not in buckets:\n+                continue\n+\n+            resp = client.list_objects_v2(Bucket=bucket)\n+            if \"Contents\" not in resp:\n+                continue\n+\n+            for obj in resp[\"Contents\"]:\n+                obj[\"Bucket\"] = bucket\n+                obj[\"Filename\"] = obj[\"Key\"]\n+                if add_bucket_to_name:\n+                    obj[\"Key\"] = f'{bucket}/{obj[\"Key\"]}'\n+                objects.append(obj)\n+            if limit is not None and len(objects) >= limit:\n+                break\n+\n+        return objects\n+\n+    def get_tables(self) -> Response:\n+        supported_names = [\n+            f\"{obj['Key']}\"\n+            for obj in self.get_objects()\n+            if obj[\"Key\"].split(\".\")[-1] in self.supported_file_formats\n+        ]\n+\n+        supported_names.insert(0, \"files\")\n+\n+        response = Response(\n+            RESPONSE_TYPE.TABLE,\n+            data_frame=pd.DataFrame(supported_names, columns=[\"table_name\"]),\n+        )\n+\n+        return response\n+\n+    def get_columns(self, table_name: str) -> Response:\n+        query = Select(\n+            targets=[Star()],\n+            from_table=Identifier(parts=[table_name]),\n+            limit=Constant(1),\n+        )\n+\n+        result = self.query(query)\n+\n+        response = Response(\n+            RESPONSE_TYPE.TABLE,\n+            data_frame=pd.DataFrame(\n+                {\n+                    \"column_name\": result.data_frame.columns,\n+                    \"data_type\": [\n+                        str(dtype) if str(dtype) != \"object\" else \"string\"\n+                        for dtype in result.data_frame.dtypes\n+                    ],\n+                }\n+            ),\n+        )\n+\n+        return response\ndiff --git a/mindsdb/integrations/handlers/ibm_cos_handler/icon.svg b/mindsdb/integrations/handlers/ibm_cos_handler/icon.svg\nnew file mode 100644\nindex 00000000000..f8574f62860\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/icon.svg\n@@ -0,0 +1,54 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<!-- Generator: Adobe Illustrator 23.0.1, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->\n+<svg version=\"1.1\" id=\"Livello_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\"\n+\t width=\"1000px\" height=\"401.149px\" viewBox=\"0 0 1000 401.149\" enable-background=\"new 0 0 1000 401.149\" xml:space=\"preserve\">\n+<g>\n+\t<g>\n+\t\t<polygon id=\"Rectangle-path\" fill=\"#1F70C1\" points=\"0,373.217 194.433,373.217 194.433,401.05 0,401.05 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_1_\" fill=\"#1F70C1\" points=\"0,319.83 194.433,319.83 194.433,347.761 0,347.761 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_2_\" fill=\"#1F70C1\" points=\"55.468,266.541 138.867,266.541 138.867,294.473 55.468,294.473 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_3_\" fill=\"#1F70C1\" points=\"55.468,213.253 138.867,213.253 138.867,241.185 55.468,241.185 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_4_\" fill=\"#1F70C1\" points=\"55.468,159.964 138.867,159.964 138.867,187.896 55.468,187.896 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_5_\" fill=\"#1F70C1\" points=\"55.468,106.577 138.867,106.577 138.867,134.509 55.468,134.509 \t\t\"/>\n+\t\t<rect id=\"Rectangle-path_6_\" y=\"53.288\" fill=\"#1F70C1\" width=\"194.433\" height=\"27.932\"/>\n+\t\t<rect id=\"Rectangle-path_7_\" fill=\"#1F70C1\" width=\"194.433\" height=\"27.932\"/>\n+\t</g>\n+\t<g>\n+\t\t<path id=\"Shape_16_\" fill=\"#1F70C1\" d=\"M222.167,347.761h299.029c5.051-8.617,8.815-18.027,11.094-27.932H222.167V347.761z\"/>\n+\t\t<path id=\"Shape_17_\" fill=\"#1F70C1\" d=\"M497.92,213.253H277.734v27.932h243.463C514.857,230.487,507.032,221.078,497.92,213.253z\"\n+\t\t\t/>\n+\t\t<path id=\"Shape_18_\" fill=\"#1F70C1\" d=\"M277.734,159.964v27.932H497.92c9.311-7.825,17.135-17.235,23.277-27.932H277.734z\"/>\n+\t\t<path id=\"Shape_19_\" fill=\"#1F70C1\" d=\"M521.197,53.288H222.167V81.22H532.29C529.715,71.315,525.951,61.906,521.197,53.288z\"/>\n+\t\t<path id=\"Shape_20_\" fill=\"#1F70C1\" d=\"M429.279,0H222.167v27.932h278.526C482.072,10.697,456.815,0,429.279,0z\"/>\n+\t\t<rect id=\"Rectangle-path_8_\" x=\"277.734\" y=\"106.577\" fill=\"#1F70C1\" width=\"83.3\" height=\"27.932\"/>\n+\t\t<path id=\"Shape_21_\" fill=\"#1F70C1\" d=\"M444.433,134.509h87.163c2.476-8.914,3.764-18.324,3.764-27.932h-90.927L444.433,134.509\n+\t\t\tL444.433,134.509z\"/>\n+\t\t<polygon id=\"Rectangle-path_9_\" fill=\"#1F70C1\" points=\"277.734,266.541 361.034,266.541 361.034,294.473 277.734,294.473 \t\t\"/>\n+\t\t<path id=\"Shape_22_\" fill=\"#1F70C1\" d=\"M444.433,266.541v27.932h90.927c0-9.608-1.288-19.017-3.764-27.932H444.433z\"/>\n+\t\t<path id=\"Shape_23_\" fill=\"#1F70C1\" d=\"M222.167,400.852l207.112,0.297c27.734,0,52.793-10.697,71.513-27.932H222.167V400.852z\"/>\n+\t</g>\n+\t<g>\n+\t\t<polygon id=\"Rectangle-path_10_\" fill=\"#1F70C1\" points=\"555.567,373.217 694.433,373.217 694.433,401.05 555.567,401.05 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_11_\" fill=\"#1F70C1\" points=\"555.567,319.83 694.433,319.83 694.433,347.761 555.567,347.761 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_12_\" fill=\"#1F70C1\" points=\"611.034,266.541 694.433,266.541 694.433,294.473 611.034,294.473 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_13_\" fill=\"#1F70C1\" points=\"611.034,213.253 694.433,213.253 694.433,241.185 611.034,241.185 \t\t\"/>\n+\t\t<polygon id=\"Shape_24_\" fill=\"#1F70C1\" points=\"733.063,53.288 555.567,53.288 555.567,81.22 742.67,81.22 \t\t\"/>\n+\t\t<polygon id=\"Shape_25_\" fill=\"#1F70C1\" points=\"714.639,0 555.567,0 555.567,27.932 724.247,27.932 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_14_\" fill=\"#1F70C1\" points=\"861.034,373.217 1000,373.217 1000,401.05 861.034,401.05 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_15_\" fill=\"#1F70C1\" points=\"861.034,319.83 1000,319.83 1000,347.761 861.034,347.761 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_16_\" fill=\"#1F70C1\" points=\"861.034,266.541 944.433,266.541 944.433,294.473 861.034,294.473 \t\t\"/>\n+\t\t<polygon id=\"Rectangle-path_17_\" fill=\"#1F70C1\" points=\"861.034,213.253 944.433,213.253 944.433,241.185 861.034,241.185 \t\t\"/>\n+\t\t<polygon id=\"Shape_26_\" fill=\"#1F70C1\" points=\"861.034,187.896 944.433,187.896 944.433,159.964 861.034,159.964 \n+\t\t\t861.034,159.964 785.559,159.964 777.734,182.548 769.909,159.964 694.433,159.964 694.433,159.964 611.034,159.964 \n+\t\t\t611.034,187.896 694.433,187.896 694.433,162.242 703.249,187.896 852.219,187.896 861.034,162.242 \t\t\"/>\n+\t\t<polygon id=\"Shape_27_\" fill=\"#1F70C1\" points=\"944.433,106.577 803.982,106.577 794.374,134.509 944.433,134.509 \t\t\"/>\n+\t\t<polygon id=\"Shape_28_\" fill=\"#1F70C1\" points=\"840.927,0 831.319,27.932 1000,27.932 1000,0 \t\t\"/>\n+\t\t<polygon id=\"Shape_29_\" fill=\"#1F70C1\" points=\"777.734,400.852 787.341,373.217 768.126,373.217 \t\t\"/>\n+\t\t<polygon id=\"Shape_30_\" fill=\"#1F70C1\" points=\"759.311,347.761 796.157,347.761 806.062,319.83 749.505,319.83 \t\t\"/>\n+\t\t<polygon id=\"Shape_31_\" fill=\"#1F70C1\" points=\"740.59,294.473 814.877,294.473 824.683,266.541 730.784,266.541 \t\t\"/>\n+\t\t<polygon id=\"Shape_32_\" fill=\"#1F70C1\" points=\"721.969,241.185 833.597,241.185 843.106,213.253 712.361,213.253 \t\t\"/>\n+\t\t<polygon id=\"Shape_33_\" fill=\"#1F70C1\" points=\"611.034,134.509 761.093,134.509 751.486,106.577 611.034,106.577 \t\t\"/>\n+\t\t<polygon id=\"Shape_34_\" fill=\"#1F70C1\" points=\"812.896,81.22 1000,81.22 1000,53.288 822.405,53.288 \t\t\"/>\n+\t</g>\n+</g>\n+</svg>\ndiff --git a/mindsdb/integrations/handlers/ibm_cos_handler/requirements.txt b/mindsdb/integrations/handlers/ibm_cos_handler/requirements.txt\nnew file mode 100644\nindex 00000000000..e64c5eb3812\n--- /dev/null\n+++ b/mindsdb/integrations/handlers/ibm_cos_handler/requirements.txt\n@@ -0,0 +1,1 @@\n+ibm-cos-sdk\n\\ No newline at end of file\n", "instance_id": "mindsdb__mindsdb-10072", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement for integrating IBM Cloud Object Storage with MindsDB is mostly clear, with a defined goal of building a new handler for the integration. It provides useful resources such as documentation links and references to a similar S3 API handler, which helps in understanding the expected structure and approach. The \"Next Steps\" section outlines the high-level tasks to be performed, such as creating and testing the handler. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the core features to be implemented (it only mentions \"core features mentioned above\" without listing them), nor does it specify input/output formats, supported file types, or specific performance requirements. Additionally, edge cases, error handling expectations, and testing criteria are not detailed in the statement itself, though some of these are addressed in the code changes (e.g., supported file formats like CSV, JSON). Overall, while the intent and general direction are clear, the lack of specific requirements and examples in the problem statement leaves room for interpretation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving the creation of an entirely new handler module for IBM Cloud Object Storage within the MindsDB framework. This includes multiple new files (e.g., handler implementation, connection arguments, documentation) and a substantial amount of code (over 300 lines in the main handler file alone), indicating a non-trivial implementation effort. Second, the problem requires understanding and integrating with multiple technical concepts, including the IBM COS SDK (via `ibm-cos-sdk`), AWS S3-compatible APIs (as IBM COS uses a similar interface), and the MindsDB handler architecture (e.g., `APIHandler`, `APIResource`). Additionally, it involves working with data processing libraries like `pandas` and `duckdb` for file handling and SQL query execution, as well as implementing file format support (CSV, TSV, JSON, Parquet). Third, the code changes impact the system's architecture by extending MindsDB's integration capabilities, requiring the developer to understand how handlers interact with the broader system (e.g., SQL parsing via `mindsdb_sql`, connection management). Fourth, there are notable edge cases and error handling requirements addressed in the code, such as handling unsupported file formats, bucket name parsing, connection errors, and SQL query parsing issues (e.g., special characters in object keys), which add complexity. While the problem does not require advanced domain-specific knowledge beyond typical cloud storage integration or system-level considerations (e.g., distributed systems), it does demand a deep understanding of the MindsDB codebase and careful implementation to ensure compatibility and performance. A score of 0.65 reflects the challenging nature of the task, balancing the complexity of the integration with the availability of reference materials (e.g., S3 handler) that mitigate some difficulty.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Streaming decryption\nAny plans to support streaming decryption from a file-like object? We want to stream large encrypted files from an sftp server and decrypt them on the fly as we write them somewhere else.\r\n\r\nIt looks like `decrypt_file` is basically doing that, we just need an API that takes the actual file-like fileobj.\r\nhttps://github.com/woodruffw/pyrage/blob/aab0f86bca80eb263b8d7b5fa4fdb06dfb52f095/src/lib.rs#L224\r\n\r\nMaybe using something like this https://github.com/omerbenamram/pyo3-file\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 0167c63..9ce2c31 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -940,6 +940,15 @@ dependencies = [\n  \"pyo3-build-config\",\n ]\n \n+[[package]]\n+name = \"pyo3-file\"\n+version = \"0.9.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"488563e2317157edd6e12c3ef23e10363bd079bf8630e3de719e368b4eb02a21\"\n+dependencies = [\n+ \"pyo3\",\n+]\n+\n [[package]]\n name = \"pyo3-macros\"\n version = \"0.22.3\"\n@@ -972,6 +981,7 @@ dependencies = [\n  \"age\",\n  \"age-core\",\n  \"pyo3\",\n+ \"pyo3-file\",\n ]\n \n [[package]]\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 15a8106..09296d5 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -25,3 +25,4 @@ pyo3 = { version = \"0.22.3\", features = [\n     \"abi3-py38\",\n     \"py-clone\",\n ] }\n+pyo3-file = \"0.9.0\"\ndiff --git a/pyrage-stubs/pyrage-stubs/__init__.pyi b/pyrage-stubs/pyrage-stubs/__init__.pyi\nindex 8119866..1282a58 100644\n--- a/pyrage-stubs/pyrage-stubs/__init__.pyi\n+++ b/pyrage-stubs/pyrage-stubs/__init__.pyi\n@@ -1,3 +1,4 @@\n+from io import BufferedIOBase\n from typing import Sequence, Union\n \n from pyrage import ssh, x25519, passphrase, plugin\n@@ -15,5 +16,7 @@ class IdentityError(Exception):\n \n def encrypt(plaintext: bytes, recipients: Sequence[Recipient]) -> bytes: ...\n def encrypt_file(infile: str, outfile: str, recipients: Sequence[Recipient]) -> None: ...\n+def encrypt_io(in_io: BufferedIOBase, recipients: Sequence[Recipient]) -> bytes: ...\n def decrypt(ciphertext: bytes, identities: Sequence[Identity]) -> bytes: ...\n def decrypt_file(infile: str, outfile: str, identities: Sequence[Identity]) -> None: ...\n+def decrypt_io(in_io: BufferedIOBase, out_io: BufferedIOBase, recipient: Sequence[Recipient]) -> None: ...\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 179316b..7c1198f 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -17,6 +17,7 @@ use pyo3::{\n     py_run,\n     types::PyBytes,\n };\n+use pyo3_file::PyFileLikeObject;\n \n mod passphrase;\n mod plugin;\n@@ -259,6 +260,64 @@ fn decrypt_file(\n     Ok(())\n }\n \n+fn from_pyobject(file: PyObject, read_only: bool) -> PyResult<PyFileLikeObject> {\n+    // is a file-like\n+    PyFileLikeObject::with_requirements(file, read_only, !read_only, false, false)\n+}\n+\n+#[pyfunction]\n+fn encrypt_io(\n+    reader: PyObject,\n+    writer: PyObject,\n+    recipients: Vec<Box<dyn PyrageRecipient>>,\n+) -> PyResult<()> {\n+    // This turns each `dyn PyrageRecipient` into a `dyn Recipient`, which\n+    // is what the underlying `age` API expects.\n+    let recipients = recipients.into_iter().map(|pr| pr.as_recipient()).collect();\n+    let reader = from_pyobject(reader, true)?;\n+    let writer = from_pyobject(writer, false)?;\n+    let mut reader = std::io::BufReader::new(reader);\n+    let mut writer = std::io::BufWriter::new(writer);\n+    let encryptor = Encryptor::with_recipients(recipients)\n+        .ok_or_else(|| EncryptError::new_err(\"expected at least one recipient\"))?;\n+    let mut writer = encryptor\n+        .wrap_output(&mut writer)\n+        .map_err(|e| EncryptError::new_err(e.to_string()))?;\n+    std::io::copy(&mut reader, &mut writer).map_err(|e| EncryptError::new_err(e.to_string()))?;\n+    writer\n+        .finish()\n+        .map_err(|e| EncryptError::new_err(e.to_string()))?;\n+    Ok(())\n+}\n+\n+#[pyfunction]\n+fn decrypt_io(\n+    reader: PyObject,\n+    writer: PyObject,\n+    identities: Vec<Box<dyn PyrageIdentity>>,\n+) -> PyResult<()> {\n+    let identities = identities.iter().map(|pi| pi.as_ref().as_identity());\n+    let reader = from_pyobject(reader, true)?;\n+    let writer = from_pyobject(writer, false)?;\n+    let reader = std::io::BufReader::new(reader);\n+    let mut writer = std::io::BufWriter::new(writer);\n+    let decryptor = match age::Decryptor::new_buffered(reader)\n+        .map_err(|e| DecryptError::new_err(e.to_string()))?\n+    {\n+        age::Decryptor::Recipients(d) => d,\n+        age::Decryptor::Passphrase(_) => {\n+            return Err(DecryptError::new_err(\n+                \"invalid ciphertext (encrypted with passphrase, not identities)\",\n+            ))\n+        }\n+    };\n+    let mut reader = decryptor\n+        .decrypt(identities)\n+        .map_err(|e| DecryptError::new_err(e.to_string()))?;\n+    std::io::copy(&mut reader, &mut writer)?;\n+    Ok(())\n+}\n+\n #[pymodule]\n fn pyrage(py: Python, m: &Bound<'_, PyModule>) -> PyResult<()> {\n     // HACK(ww): pyO3 modules are not packages, so we need this nasty\n@@ -298,9 +357,11 @@ fn pyrage(py: Python, m: &Bound<'_, PyModule>) -> PyResult<()> {\n     m.add(\"EncryptError\", py.get_type_bound::<EncryptError>())?;\n     m.add_wrapped(wrap_pyfunction!(encrypt))?;\n     m.add_wrapped(wrap_pyfunction!(encrypt_file))?;\n+    m.add_wrapped(wrap_pyfunction!(encrypt_io))?;\n     m.add(\"DecryptError\", py.get_type_bound::<DecryptError>())?;\n     m.add_wrapped(wrap_pyfunction!(decrypt))?;\n     m.add_wrapped(wrap_pyfunction!(decrypt_file))?;\n+    m.add_wrapped(wrap_pyfunction!(decrypt_io))?;\n \n     Ok(())\n }\n", "instance_id": "woodruffw__pyrage-77", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in its intent to add support for streaming decryption (and encryption) from a file-like object in Python, using the `pyo3-file` library as a reference. It identifies a specific function (`decrypt_file`) in the codebase as a starting point and suggests a potential solution. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input/output behavior for the new API (e.g., whether it should handle partial reads/writes or specific file-like object requirements). Additionally, edge cases such as handling invalid file objects, interrupted streams, or large file performance are not mentioned. While the goal is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving the addition of new functions (`encrypt_io` and `decrypt_io`) in a single file (`lib.rs`), along with updates to dependencies and type hints in other files. The changes do not significantly impact the overall architecture but require integrating with the existing `age` library for encryption/decryption and the `pyo3-file` library for handling Python file-like objects. Second, the technical concepts involved include Rust-Python interoperability (via `pyo3`), streaming I/O operations, and encryption/decryption logic, which are moderately complex but not overly advanced for someone familiar with Rust and Python bindings. Third, the problem requires understanding and extending existing functionality (`decrypt_file`) to a more generic I/O interface, which involves some logic around buffered reading/writing and error handling. Finally, while edge cases (e.g., invalid file objects, stream interruptions) are not explicitly mentioned in the problem statement, the code changes do include basic error handling, and a developer would need to consider additional scenarios like large file streaming or partial data handling. Overall, this problem requires a solid understanding of multiple concepts and careful implementation, but it does not demand deep architectural changes or highly advanced expertise, placing it at a medium difficulty level of 0.50.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Recreating UI components resets their state to when the page loaded instead of a clean state\n**Description**\r\n\r\nWhen recreating a deephaven UI component or re-opening a component via console button, the persisted state gets reset to whatever it was when the page loaded, not a clean state.\r\n\r\n**Steps to reproduce**\r\n\r\n1. Create a counter\r\n\r\n    ```py\r\n    from deephaven import ui\r\n    \r\n    @ui.component\r\n    def ui_counter():\r\n        count, set_count = ui.use_state(0)\r\n        return ui.button(f\"Pressed {count} times\", on_press=lambda: set_count(count + 1))\r\n    \r\n    c1 = ui_counter()\r\n    ```\r\n\r\n2. Click the button\r\n3. Refresh the page\r\n4. Click the button again\r\n5. Rerun the code to make the counter or click on the button in the console to re-open the counter\r\n\r\n**Expected results**\r\n\r\n6. Counter resets to 0\r\n\r\n**Actual results**\r\n\r\n6. Counter resets to 1. The state when the page was loaded.\r\n\n", "patch": "diff --git a/plugins/ui/src/js/src/widget/WidgetHandler.tsx b/plugins/ui/src/js/src/widget/WidgetHandler.tsx\nindex f390c98a3..918ba0bf1 100644\n--- a/plugins/ui/src/js/src/widget/WidgetHandler.tsx\n+++ b/plugins/ui/src/js/src/widget/WidgetHandler.tsx\n@@ -67,7 +67,10 @@ function WidgetHandler({\n   const [widget, setWidget] = useState<dh.Widget>();\n   const [document, setDocument] = useState<ReactNode>();\n   const [error, setError] = useState<WidgetError>();\n-  const [initialData] = useState(initialDataProp);\n+\n+  // We want to update the initial data if the widget changes, as we'll need to re-fetch the widget and want to start with a fresh state.\n+  // eslint-disable-next-line react-hooks/exhaustive-deps\n+  const initialData = useMemo(() => initialDataProp, [widget]);\n \n   // When we fetch a widget, the client is then responsible for the exported objects.\n   // These objects could stay alive even after the widget is closed if we wanted to,\ndiff --git a/plugins/ui/src/js/src/widget/WidgetTestUtils.ts b/plugins/ui/src/js/src/widget/WidgetTestUtils.ts\nindex 278c0e50b..a452944ff 100644\n--- a/plugins/ui/src/js/src/widget/WidgetTestUtils.ts\n+++ b/plugins/ui/src/js/src/widget/WidgetTestUtils.ts\n@@ -1,6 +1,7 @@\n import { WidgetDescriptor } from '@deephaven/dashboard';\n import { TestUtils } from '@deephaven/utils';\n import type { dh } from '@deephaven/jsapi-types';\n+import { WidgetMessageEvent } from './WidgetTypes';\n \n export function makeDocumentUpdatedJsonRpc(\n   document: Record<string, unknown> = {}\n@@ -12,12 +13,43 @@ export function makeDocumentUpdatedJsonRpc(\n   };\n }\n \n+export function makeJsonRpcResponseString(id: number, result = ''): string {\n+  return JSON.stringify({\n+    jsonrpc: '2.0',\n+    id,\n+    result,\n+  });\n+}\n+\n export function makeDocumentUpdatedJsonRpcString(\n   document: Record<string, unknown> = {}\n ): string {\n   return JSON.stringify(makeDocumentUpdatedJsonRpc(document));\n }\n \n+export function makeWidgetEvent(data = ''): WidgetMessageEvent {\n+  return new CustomEvent('message', {\n+    detail: {\n+      getDataAsBase64: () => '',\n+      getDataAsString: () => data,\n+      exportedObjects: [],\n+    },\n+  });\n+}\n+\n+export function makeWidgetEventJsonRpcResponse(\n+  id: number,\n+  response = ''\n+): WidgetMessageEvent {\n+  return makeWidgetEvent(makeJsonRpcResponseString(id, response));\n+}\n+\n+export function makeWidgetEventDocumentUpdated(\n+  document: Record<string, unknown> = {}\n+): WidgetMessageEvent {\n+  return makeWidgetEvent(makeDocumentUpdatedJsonRpcString(document));\n+}\n+\n export function makeWidgetDescriptor({\n   id = 'widget-id',\n   type = 'widget-type',\n@@ -34,10 +66,12 @@ export function makeWidget({\n   addEventListener = jest.fn(() => jest.fn()),\n   getDataAsString = () => makeDocumentUpdatedJsonRpcString(),\n   exportedObjects = [],\n+  sendMessage = jest.fn(),\n }: Partial<dh.Widget> = {}): dh.Widget {\n   return TestUtils.createMockProxy<dh.Widget>({\n     addEventListener,\n     getDataAsString,\n     exportedObjects,\n+    sendMessage,\n   });\n }\n", "instance_id": "deephaven__deephaven-plugins-486", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when recreating or re-opening a Deephaven UI component, the state resets to the value at page load rather than a clean state (e.g., counter resetting to 0). It provides a reproducible example in Python, steps to reproduce, expected results, and actual results, which helps in understanding the problem. However, there are minor ambiguities. The statement does not explicitly define what a \"clean state\" means beyond the counter example, nor does it mention potential edge cases or constraints (e.g., does this apply to all UI components or specific ones? Are there performance implications?). Additionally, it lacks context about the broader system or how state persistence is managed, which could be critical for a complete understanding. Thus, while the problem is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting two files (`WidgetHandler.tsx` and `WidgetTestUtils.ts`) in a React-based TypeScript codebase. The changes in `WidgetHandler.tsx` involve updating the initial data state to refresh when the widget changes using `useMemo`, which is a straightforward fix but requires understanding React hooks and the widget lifecycle. The modifications in `WidgetTestUtils.ts` are more extensive, adding utility functions for testing widget events and JSON-RPC responses, indicating a need to understand the testing framework and widget communication protocols. \n\nSecond, the technical concepts involved include React state management (hooks like `useState` and `useMemo`), widget lifecycle in the Deephaven framework, and JSON-RPC messaging for widget communication. These are moderately complex for someone unfamiliar with the specific framework or React, but not overly challenging for an experienced developer. \n\nThird, the problem does not explicitly mention edge cases or error handling requirements in the statement, and the code changes do not introduce significant new error handling logic. However, ensuring that the state reset works correctly across different widget types or usage scenarios might implicitly require considering edge cases (e.g., widgets with complex nested states), though this is not evident in the provided diff.\n\nFinally, the impact on the codebase appears limited to the widget state initialization and testing utilities, without broader architectural changes. Given these factors\u2014focused scope, moderate technical concepts, and minimal explicit edge case handling\u2014I rate the difficulty as 0.45, placing it in the medium category. It requires understanding multiple concepts and making targeted modifications, but it does not demand deep architectural changes or advanced domain-specific knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Missing urls.py File in Tests Causing API Test Failures\nWhile running API tests, the tests are failing because the URL configuration is missing or incomplete. In the current setup, the Django project does not include a urls.py file or a valid route configuration for the API endpoints, resulting in 404 Not Found errors during test execution.\r\nSteps to Reproduce:\r\nRun the test suite targeting API endpoints (e.g., using pytest or django.test).\r\nObserve the failure logs indicating that API routes are not found.\r\nExpected Behavior:\r\nAPI tests should pass successfully by routing to the correct endpoints defined in the urls.py file.\r\n\r\nActual Behavior:\r\nAPI tests fail due to missing or undefined URL patterns for the API views.\r\n\r\nSuggested Solution:\r\nCreate or update a urls.py file in the test setup to include the necessary routes for the API views being tested.\r\nEnsure that the test suite is correctly pointing to the URL configuration during test execution.\r\nExample Fix:\n\ud83d\udc1b Fix/run tests\n\n", "patch": "diff --git a/.github/dependabot.yml b/.github/dependabot.yml\nnew file mode 100644\nindex 0000000..d4493e0\n--- /dev/null\n+++ b/.github/dependabot.yml\n@@ -0,0 +1,13 @@\n+# To get started with Dependabot version updates, you'll need to specify which\n+# package ecosystems to update and where the package manifests are located.\n+# Please see the documentation for all configuration options:\n+# https://docs.github.com/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file\n+\n+version: 2\n+updates:\n+  - package-ecosystem: \"pip\"\n+    directory: \"/packages\" # Location of the requirements.txt and requirements-dev.txt file\n+    schedule:\n+      interval: \"weekly\"\n+\n+    target-branch: \"main\"\ndiff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nnew file mode 100644\nindex 0000000..f279cd4\n--- /dev/null\n+++ b/.github/workflows/ci.yml\n@@ -0,0 +1,44 @@\n+name: CI\n+\n+on: [push, pull_request]\n+\n+jobs:\n+  test:\n+    name: Python ${{ matrix.python-version }}\n+    runs-on: ubuntu-20.04\n+\n+    strategy:\n+      matrix:\n+        python-version:\n+          - '3.9'\n+          - '3.10'\n+          - '3.11'\n+          - '3.12'\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Set up Python ${{ matrix.python-version }}\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: ${{ matrix.python-version }}\n+\n+      - name: Install dependencies\n+        run: |\n+          python -m pip install --upgrade pip\n+          pip install coverage codecov pytest poetry\n+          pip install -r packages/requirements-dev.txt\n+\n+      - name: Run tests with coverage\n+        run: pytest --cov=django_notification --cov-report=xml\n+\n+      - name: Run Tox tests\n+        run: tox\n+\n+      - name: Run pre-commit hooks\n+        run: pre-commit run --all-files --config=.pre-commit-config-ci.yaml\n+\n+      - name: Upload coverage to Codecov\n+        run: codecov\n+        env:\n+          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\ndiff --git a/.github/workflows/release.yml b/.github/workflows/release.yml\nnew file mode 100644\nindex 0000000..5bbb84c\n--- /dev/null\n+++ b/.github/workflows/release.yml\n@@ -0,0 +1,35 @@\n+name: Release\n+\n+on:\n+  push:\n+    tags:\n+      - 'v*.*.*'\n+\n+jobs:\n+  release:\n+    name: Build and Release\n+    runs-on: ubuntu-latest\n+\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Set up Python\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.x'\n+\n+      - name: Install dependencies\n+        run: |\n+          python -m pip install --upgrade pip\n+          pip install poetry\n+\n+      - name: Build package\n+        run: |\n+          poetry build\n+\n+      - name: Publish to PyPI\n+        run: |\n+          poetry publish --username __token__ --password ${{ secrets.PYPI_TOKEN }}\n+        env:\n+          PYPI_TOKEN: ${{ secrets.PYPI_TOKEN }}\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex ff750cc..5e5aec2 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -85,14 +85,14 @@ repos:\n \n   - repo: local\n     hooks:\n-      - id: pytest\n-        name: Pytest\n-        entry: poetry run pytest -v\n-        language: system\n-        types: [ python ]\n-        stages: [ commit ]\n-        pass_filenames: false\n-        always_run: true\n+#      - id: pytest\n+#        name: Pytest\n+#        entry: poetry run pytest -v\n+#        language: system\n+#        types: [ python ]\n+#        stages: [ commit ]\n+#        pass_filenames: false\n+#        always_run: true\n \n       - id: pylint\n         name: pylint\ndiff --git a/django_notification/api/serializers/helper/get_serializer_cls.py b/django_notification/api/serializers/helper/get_serializer_cls.py\nindex 500955f..bab58d6 100644\n--- a/django_notification/api/serializers/helper/get_serializer_cls.py\n+++ b/django_notification/api/serializers/helper/get_serializer_cls.py\n@@ -6,15 +6,13 @@\n from django_notification.settings.conf import config\n \n \n-def group_serializer_class():\n-    \"\"\"\n-    Get the serializer class for the group field, either from config or the default.\n-    \"\"\"\n+def group_serializer_class() -> Type[BaseSerializer]:\n+    \"\"\"Get the serializer class for the group field, either from config or the\n+    default.\"\"\"\n     return config.group_serializer_class or GroupSerializer\n \n \n def user_serializer_class() -> Type[BaseSerializer]:\n-    \"\"\"\n-    Get the serializer class for the recipient and seen_by fields, either from config or the default.\n-    \"\"\"\n+    \"\"\"Get the serializer class for the recipient and seen_by fields, either\n+    from config or the default.\"\"\"\n     return config.user_serializer_class or UserSerializer\ndiff --git a/poetry.lock b/poetry.lock\nindex af0d530..fbbe386 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -1,7 +1,2021 @@\n # This file is automatically @generated by Poetry 1.8.3 and should not be changed by hand.\n-package = []\n+\n+[[package]]\n+name = \"alabaster\"\n+version = \"0.7.13\"\n+description = \"A configurable sidebar-enabled Sphinx theme\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"alabaster-0.7.13-py3-none-any.whl\", hash = \"sha256:1ee19aca801bbabb5ba3f5f258e4422dfa86f82f3e9cefb0859b283cdd7f62a3\"},\n+    {file = \"alabaster-0.7.13.tar.gz\", hash = \"sha256:a27a4a084d5e690e16e01e03ad2b2e552c61a65469419b907243193de1a84ae2\"},\n+]\n+\n+[[package]]\n+name = \"annotated-types\"\n+version = \"0.7.0\"\n+description = \"Reusable constraint types to use with typing.Annotated\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"annotated_types-0.7.0-py3-none-any.whl\", hash = \"sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53\"},\n+    {file = \"annotated_types-0.7.0.tar.gz\", hash = \"sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89\"},\n+]\n+\n+[package.dependencies]\n+typing-extensions = {version = \">=4.0.0\", markers = \"python_version < \\\"3.9\\\"\"}\n+\n+[[package]]\n+name = \"argcomplete\"\n+version = \"3.5.0\"\n+description = \"Bash tab completion for argparse\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"argcomplete-3.5.0-py3-none-any.whl\", hash = \"sha256:d4bcf3ff544f51e16e54228a7ac7f486ed70ebf2ecfe49a63a91171c76bf029b\"},\n+    {file = \"argcomplete-3.5.0.tar.gz\", hash = \"sha256:4349400469dccfb7950bb60334a680c58d88699bff6159df61251878dc6bf74b\"},\n+]\n+\n+[package.extras]\n+test = [\"coverage\", \"mypy\", \"pexpect\", \"ruff\", \"wheel\"]\n+\n+[[package]]\n+name = \"asgiref\"\n+version = \"3.8.1\"\n+description = \"ASGI specs, helper code, and adapters\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"asgiref-3.8.1-py3-none-any.whl\", hash = \"sha256:3e1e3ecc849832fe52ccf2cb6686b7a55f82bb1d6aee72a58826471390335e47\"},\n+    {file = \"asgiref-3.8.1.tar.gz\", hash = \"sha256:c343bd80a0bec947a9860adb4c432ffa7db769836c64238fc34bdc3fec84d590\"},\n+]\n+\n+[package.dependencies]\n+typing-extensions = {version = \">=4\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+tests = [\"mypy (>=0.800)\", \"pytest\", \"pytest-asyncio\"]\n+\n+[[package]]\n+name = \"astroid\"\n+version = \"3.2.4\"\n+description = \"An abstract syntax tree for Python with inference support.\"\n+optional = false\n+python-versions = \">=3.8.0\"\n+files = [\n+    {file = \"astroid-3.2.4-py3-none-any.whl\", hash = \"sha256:413658a61eeca6202a59231abb473f932038fbcbf1666587f66d482083413a25\"},\n+    {file = \"astroid-3.2.4.tar.gz\", hash = \"sha256:0e14202810b30da1b735827f78f5157be2bbd4a7a59b7707ca0bfc2fb4c0063a\"},\n+]\n+\n+[package.dependencies]\n+typing-extensions = {version = \">=4.0.0\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[[package]]\n+name = \"babel\"\n+version = \"2.16.0\"\n+description = \"Internationalization utilities\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"babel-2.16.0-py3-none-any.whl\", hash = \"sha256:368b5b98b37c06b7daf6696391c3240c938b37767d4584413e8438c5c435fa8b\"},\n+    {file = \"babel-2.16.0.tar.gz\", hash = \"sha256:d1f3554ca26605fe173f3de0c65f750f5a42f924499bf134de6423582298e316\"},\n+]\n+\n+[package.dependencies]\n+pytz = {version = \">=2015.7\", markers = \"python_version < \\\"3.9\\\"\"}\n+\n+[package.extras]\n+dev = [\"freezegun (>=1.0,<2.0)\", \"pytest (>=6.0)\", \"pytest-cov\"]\n+\n+[[package]]\n+name = \"backports-zoneinfo\"\n+version = \"0.2.1\"\n+description = \"Backport of the standard library zoneinfo module\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"backports.zoneinfo-0.2.1-cp36-cp36m-macosx_10_14_x86_64.whl\", hash = \"sha256:da6013fd84a690242c310d77ddb8441a559e9cb3d3d59ebac9aca1a57b2e18bc\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp36-cp36m-manylinux1_i686.whl\", hash = \"sha256:89a48c0d158a3cc3f654da4c2de1ceba85263fafb861b98b59040a5086259722\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp36-cp36m-manylinux1_x86_64.whl\", hash = \"sha256:1c5742112073a563c81f786e77514969acb58649bcdf6cdf0b4ed31a348d4546\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp36-cp36m-win32.whl\", hash = \"sha256:e8236383a20872c0cdf5a62b554b27538db7fa1bbec52429d8d106effbaeca08\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp36-cp36m-win_amd64.whl\", hash = \"sha256:8439c030a11780786a2002261569bdf362264f605dfa4d65090b64b05c9f79a7\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp37-cp37m-macosx_10_14_x86_64.whl\", hash = \"sha256:f04e857b59d9d1ccc39ce2da1021d196e47234873820cbeaad210724b1ee28ac\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_i686.whl\", hash = \"sha256:17746bd546106fa389c51dbea67c8b7c8f0d14b5526a579ca6ccf5ed72c526cf\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl\", hash = \"sha256:5c144945a7752ca544b4b78c8c41544cdfaf9786f25fe5ffb10e838e19a27570\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp37-cp37m-win32.whl\", hash = \"sha256:e55b384612d93be96506932a786bbcde5a2db7a9e6a4bb4bffe8b733f5b9036b\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp37-cp37m-win_amd64.whl\", hash = \"sha256:a76b38c52400b762e48131494ba26be363491ac4f9a04c1b7e92483d169f6582\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp38-cp38-macosx_10_14_x86_64.whl\", hash = \"sha256:8961c0f32cd0336fb8e8ead11a1f8cd99ec07145ec2931122faaac1c8f7fd987\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_i686.whl\", hash = \"sha256:e81b76cace8eda1fca50e345242ba977f9be6ae3945af8d46326d776b4cf78d1\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl\", hash = \"sha256:7b0a64cda4145548fed9efc10322770f929b944ce5cee6c0dfe0c87bf4c0c8c9\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp38-cp38-win32.whl\", hash = \"sha256:1b13e654a55cd45672cb54ed12148cd33628f672548f373963b0bff67b217328\"},\n+    {file = \"backports.zoneinfo-0.2.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:4a0f800587060bf8880f954dbef70de6c11bbe59c673c3d818921f042f9954a6\"},\n+    {file = \"backports.zoneinfo-0.2.1.tar.gz\", hash = \"sha256:fadbfe37f74051d024037f223b8e001611eac868b5c5b06144ef4d8b799862f2\"},\n+]\n+\n+[package.extras]\n+tzdata = [\"tzdata\"]\n+\n+[[package]]\n+name = \"bandit\"\n+version = \"1.7.9\"\n+description = \"Security oriented static analyser for python code.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"bandit-1.7.9-py3-none-any.whl\", hash = \"sha256:52077cb339000f337fb25f7e045995c4ad01511e716e5daac37014b9752de8ec\"},\n+    {file = \"bandit-1.7.9.tar.gz\", hash = \"sha256:7c395a436743018f7be0a4cbb0a4ea9b902b6d87264ddecf8cfdc73b4f78ff61\"},\n+]\n+\n+[package.dependencies]\n+colorama = {version = \">=0.3.9\", markers = \"platform_system == \\\"Windows\\\"\"}\n+PyYAML = \">=5.3.1\"\n+rich = \"*\"\n+stevedore = \">=1.20.0\"\n+tomli = {version = \">=1.1.0\", optional = true, markers = \"python_version < \\\"3.11\\\" and extra == \\\"toml\\\"\"}\n+\n+[package.extras]\n+baseline = [\"GitPython (>=3.1.30)\"]\n+sarif = [\"jschema-to-python (>=1.2.3)\", \"sarif-om (>=1.0.4)\"]\n+test = [\"beautifulsoup4 (>=4.8.0)\", \"coverage (>=4.5.4)\", \"fixtures (>=3.0.0)\", \"flake8 (>=4.0.0)\", \"pylint (==1.9.4)\", \"stestr (>=2.5.0)\", \"testscenarios (>=0.5.0)\", \"testtools (>=2.3.0)\"]\n+toml = [\"tomli (>=1.1.0)\"]\n+yaml = [\"PyYAML\"]\n+\n+[[package]]\n+name = \"black\"\n+version = \"24.8.0\"\n+description = \"The uncompromising code formatter.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"black-24.8.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:09cdeb74d494ec023ded657f7092ba518e8cf78fa8386155e4a03fdcc44679e6\"},\n+    {file = \"black-24.8.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:81c6742da39f33b08e791da38410f32e27d632260e599df7245cccee2064afeb\"},\n+    {file = \"black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:707a1ca89221bc8a1a64fb5e15ef39cd755633daa672a9db7498d1c19de66a42\"},\n+    {file = \"black-24.8.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:d6417535d99c37cee4091a2f24eb2b6d5ec42b144d50f1f2e436d9fe1916fe1a\"},\n+    {file = \"black-24.8.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:fb6e2c0b86bbd43dee042e48059c9ad7830abd5c94b0bc518c0eeec57c3eddc1\"},\n+    {file = \"black-24.8.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:837fd281f1908d0076844bc2b801ad2d369c78c45cf800cad7b61686051041af\"},\n+    {file = \"black-24.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:62e8730977f0b77998029da7971fa896ceefa2c4c4933fcd593fa599ecbf97a4\"},\n+    {file = \"black-24.8.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:72901b4913cbac8972ad911dc4098d5753704d1f3c56e44ae8dce99eecb0e3af\"},\n+    {file = \"black-24.8.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:7c046c1d1eeb7aea9335da62472481d3bbf3fd986e093cffd35f4385c94ae368\"},\n+    {file = \"black-24.8.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:649f6d84ccbae73ab767e206772cc2d7a393a001070a4c814a546afd0d423aed\"},\n+    {file = \"black-24.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:2b59b250fdba5f9a9cd9d0ece6e6d993d91ce877d121d161e4698af3eb9c1018\"},\n+    {file = \"black-24.8.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:6e55d30d44bed36593c3163b9bc63bf58b3b30e4611e4d88a0c3c239930ed5b2\"},\n+    {file = \"black-24.8.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:505289f17ceda596658ae81b61ebbe2d9b25aa78067035184ed0a9d855d18afd\"},\n+    {file = \"black-24.8.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:b19c9ad992c7883ad84c9b22aaa73562a16b819c1d8db7a1a1a49fb7ec13c7d2\"},\n+    {file = \"black-24.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:1f13f7f386f86f8121d76599114bb8c17b69d962137fc70efe56137727c7047e\"},\n+    {file = \"black-24.8.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:f490dbd59680d809ca31efdae20e634f3fae27fba3ce0ba3208333b713bc3920\"},\n+    {file = \"black-24.8.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:eab4dd44ce80dea27dc69db40dab62d4ca96112f87996bca68cd75639aeb2e4c\"},\n+    {file = \"black-24.8.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:3c4285573d4897a7610054af5a890bde7c65cb466040c5f0c8b732812d7f0e5e\"},\n+    {file = \"black-24.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:9e84e33b37be070ba135176c123ae52a51f82306def9f7d063ee302ecab2cf47\"},\n+    {file = \"black-24.8.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:73bbf84ed136e45d451a260c6b73ed674652f90a2b3211d6a35e78054563a9bb\"},\n+    {file = \"black-24.8.0-py3-none-any.whl\", hash = \"sha256:972085c618ee94f402da1af548a4f218c754ea7e5dc70acb168bfaca4c2542ed\"},\n+    {file = \"black-24.8.0.tar.gz\", hash = \"sha256:2500945420b6784c38b9ee885af039f5e7471ef284ab03fa35ecdde4688cd83f\"},\n+]\n+\n+[package.dependencies]\n+click = \">=8.0.0\"\n+mypy-extensions = \">=0.4.3\"\n+packaging = \">=22.0\"\n+pathspec = \">=0.9.0\"\n+platformdirs = \">=2\"\n+tomli = {version = \">=1.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\n+typing-extensions = {version = \">=4.0.1\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+colorama = [\"colorama (>=0.4.3)\"]\n+d = [\"aiohttp (>=3.7.4)\", \"aiohttp (>=3.7.4,!=3.9.0)\"]\n+jupyter = [\"ipython (>=7.8.0)\", \"tokenize-rt (>=3.2.0)\"]\n+uvloop = [\"uvloop (>=0.15.2)\"]\n+\n+[[package]]\n+name = \"cachetools\"\n+version = \"5.5.0\"\n+description = \"Extensible memoizing collections and decorators\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"cachetools-5.5.0-py3-none-any.whl\", hash = \"sha256:02134e8439cdc2ffb62023ce1debca2944c3f289d66bb17ead3ab3dede74b292\"},\n+    {file = \"cachetools-5.5.0.tar.gz\", hash = \"sha256:2cc24fb4cbe39633fb7badd9db9ca6295d766d9c2995f245725a46715d050f2a\"},\n+]\n+\n+[[package]]\n+name = \"certifi\"\n+version = \"2024.8.30\"\n+description = \"Python package for providing Mozilla's CA Bundle.\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"certifi-2024.8.30-py3-none-any.whl\", hash = \"sha256:922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8\"},\n+    {file = \"certifi-2024.8.30.tar.gz\", hash = \"sha256:bec941d2aa8195e248a60b31ff9f0558284cf01a52591ceda73ea9afffd69fd9\"},\n+]\n+\n+[[package]]\n+name = \"cfgv\"\n+version = \"3.4.0\"\n+description = \"Validate configuration and produce human readable error messages.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"cfgv-3.4.0-py2.py3-none-any.whl\", hash = \"sha256:b7265b1f29fd3316bfcd2b330d63d024f2bfd8bcb8b0272f8e19a504856c48f9\"},\n+    {file = \"cfgv-3.4.0.tar.gz\", hash = \"sha256:e52591d4c5f5dead8e0f673fb16db7949d2cfb3f7da4582893288f0ded8fe560\"},\n+]\n+\n+[[package]]\n+name = \"chardet\"\n+version = \"5.2.0\"\n+description = \"Universal encoding detector for Python 3\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"chardet-5.2.0-py3-none-any.whl\", hash = \"sha256:e1cf59446890a00105fe7b7912492ea04b6e6f06d4b742b2c788469e34c82970\"},\n+    {file = \"chardet-5.2.0.tar.gz\", hash = \"sha256:1b3b6ff479a8c414bc3fa2c0852995695c4a026dcd6d0633b2dd092ca39c1cf7\"},\n+]\n+\n+[[package]]\n+name = \"charset-normalizer\"\n+version = \"3.3.2\"\n+description = \"The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\"\n+optional = false\n+python-versions = \">=3.7.0\"\n+files = [\n+    {file = \"charset-normalizer-3.3.2.tar.gz\", hash = \"sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-win32.whl\", hash = \"sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73\"},\n+    {file = \"charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-win32.whl\", hash = \"sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab\"},\n+    {file = \"charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_s390x.whl\", hash = \"sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-win32.whl\", hash = \"sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7\"},\n+    {file = \"charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-win32.whl\", hash = \"sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4\"},\n+    {file = \"charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-win32.whl\", hash = \"sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25\"},\n+    {file = \"charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-win32.whl\", hash = \"sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f\"},\n+    {file = \"charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d\"},\n+    {file = \"charset_normalizer-3.3.2-py3-none-any.whl\", hash = \"sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc\"},\n+]\n+\n+[[package]]\n+name = \"click\"\n+version = \"8.1.7\"\n+description = \"Composable command line interface toolkit\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"click-8.1.7-py3-none-any.whl\", hash = \"sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28\"},\n+    {file = \"click-8.1.7.tar.gz\", hash = \"sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de\"},\n+]\n+\n+[package.dependencies]\n+colorama = {version = \"*\", markers = \"platform_system == \\\"Windows\\\"\"}\n+\n+[[package]]\n+name = \"click-option-group\"\n+version = \"0.5.6\"\n+description = \"Option groups missing in Click\"\n+optional = false\n+python-versions = \">=3.6,<4\"\n+files = [\n+    {file = \"click-option-group-0.5.6.tar.gz\", hash = \"sha256:97d06703873518cc5038509443742b25069a3c7562d1ea72ff08bfadde1ce777\"},\n+    {file = \"click_option_group-0.5.6-py3-none-any.whl\", hash = \"sha256:38a26d963ee3ad93332ddf782f9259c5bdfe405e73408d943ef5e7d0c3767ec7\"},\n+]\n+\n+[package.dependencies]\n+Click = \">=7.0,<9\"\n+\n+[package.extras]\n+docs = [\"Pallets-Sphinx-Themes\", \"m2r2\", \"sphinx\"]\n+tests = [\"pytest\"]\n+tests-cov = [\"coverage\", \"coveralls\", \"pytest\", \"pytest-cov\"]\n+\n+[[package]]\n+name = \"codecov\"\n+version = \"2.1.13\"\n+description = \"Hosted coverage reports for GitHub, Bitbucket and Gitlab\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+files = [\n+    {file = \"codecov-2.1.13-py2.py3-none-any.whl\", hash = \"sha256:c2ca5e51bba9ebb43644c43d0690148a55086f7f5e6fd36170858fa4206744d5\"},\n+    {file = \"codecov-2.1.13.tar.gz\", hash = \"sha256:2362b685633caeaf45b9951a9b76ce359cd3581dd515b430c6c3f5dfb4d92a8c\"},\n+]\n+\n+[package.dependencies]\n+coverage = \"*\"\n+requests = \">=2.7.9\"\n+\n+[[package]]\n+name = \"colorama\"\n+version = \"0.4.6\"\n+description = \"Cross-platform colored terminal text.\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"\n+files = [\n+    {file = \"colorama-0.4.6-py2.py3-none-any.whl\", hash = \"sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\"},\n+    {file = \"colorama-0.4.6.tar.gz\", hash = \"sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44\"},\n+]\n+\n+[[package]]\n+name = \"commitizen\"\n+version = \"3.29.0\"\n+description = \"Python commitizen client tool\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"commitizen-3.29.0-py3-none-any.whl\", hash = \"sha256:0c6c479dbee6d19292315c6fca3782cf5c1f7f1638bc4bb5ab4cfb67f4e11894\"},\n+    {file = \"commitizen-3.29.0.tar.gz\", hash = \"sha256:586b30c1976850d244b836cd4730771097ba362c9c1684d1f8c379176c2ea532\"},\n+]\n+\n+[package.dependencies]\n+argcomplete = \">=1.12.1,<3.6\"\n+charset-normalizer = \">=2.1.0,<4\"\n+colorama = \">=0.4.1,<0.5.0\"\n+decli = \">=0.6.0,<0.7.0\"\n+importlib_metadata = {version = \">=8.0.0,<9\", markers = \"python_version < \\\"3.10\\\"\"}\n+jinja2 = \">=2.10.3\"\n+packaging = \">=19\"\n+pyyaml = \">=3.08\"\n+questionary = \">=2.0,<3.0\"\n+termcolor = \">=1.1,<3\"\n+tomlkit = \">=0.5.3,<1.0.0\"\n+\n+[[package]]\n+name = \"coverage\"\n+version = \"7.6.1\"\n+description = \"Code coverage measurement for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"coverage-7.6.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:b06079abebbc0e89e6163b8e8f0e16270124c154dc6e4a47b413dd538859af16\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:cf4b19715bccd7ee27b6b120e7e9dd56037b9c0681dcc1adc9ba9db3d417fa36\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e61c0abb4c85b095a784ef23fdd4aede7a2628478e7baba7c5e3deba61070a02\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:fd21f6ae3f08b41004dfb433fa895d858f3f5979e7762d052b12aef444e29afc\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8f59d57baca39b32db42b83b2a7ba6f47ad9c394ec2076b084c3f029b7afca23\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:a1ac0ae2b8bd743b88ed0502544847c3053d7171a3cff9228af618a068ed9c34\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:e6a08c0be454c3b3beb105c0596ebdc2371fab6bb90c0c0297f4e58fd7e1012c\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:f5796e664fe802da4f57a168c85359a8fbf3eab5e55cd4e4569fbacecc903959\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-win32.whl\", hash = \"sha256:7bb65125fcbef8d989fa1dd0e8a060999497629ca5b0efbca209588a73356232\"},\n+    {file = \"coverage-7.6.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:3115a95daa9bdba70aea750db7b96b37259a81a709223c8448fa97727d546fe0\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:7dea0889685db8550f839fa202744652e87c60015029ce3f60e006f8c4462c93\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:ed37bd3c3b063412f7620464a9ac1314d33100329f39799255fb8d3027da50d3\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d85f5e9a5f8b73e2350097c3756ef7e785f55bd71205defa0bfdaf96c31616ff\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:9bc572be474cafb617672c43fe989d6e48d3c83af02ce8de73fff1c6bb3c198d\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0c0420b573964c760df9e9e86d1a9a622d0d27f417e1a949a8a66dd7bcee7bc6\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:1f4aa8219db826ce6be7099d559f8ec311549bfc4046f7f9fe9b5cea5c581c56\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:fc5a77d0c516700ebad189b587de289a20a78324bc54baee03dd486f0855d234\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:b48f312cca9621272ae49008c7f613337c53fadca647d6384cc129d2996d1133\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-win32.whl\", hash = \"sha256:1125ca0e5fd475cbbba3bb67ae20bd2c23a98fac4e32412883f9bcbaa81c314c\"},\n+    {file = \"coverage-7.6.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:8ae539519c4c040c5ffd0632784e21b2f03fc1340752af711f33e5be83a9d6c6\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:95cae0efeb032af8458fc27d191f85d1717b1d4e49f7cb226cf526ff28179778\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:5621a9175cf9d0b0c84c2ef2b12e9f5f5071357c4d2ea6ca1cf01814f45d2391\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:260933720fdcd75340e7dbe9060655aff3af1f0c5d20f46b57f262ab6c86a5e8\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:07e2ca0ad381b91350c0ed49d52699b625aab2b44b65e1b4e02fa9df0e92ad2d\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c44fee9975f04b33331cb8eb272827111efc8930cfd582e0320613263ca849ca\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:877abb17e6339d96bf08e7a622d05095e72b71f8afd8a9fefc82cf30ed944163\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:3e0cadcf6733c09154b461f1ca72d5416635e5e4ec4e536192180d34ec160f8a\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:c3c02d12f837d9683e5ab2f3d9844dc57655b92c74e286c262e0fc54213c216d\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-win32.whl\", hash = \"sha256:e05882b70b87a18d937ca6768ff33cc3f72847cbc4de4491c8e73880766718e5\"},\n+    {file = \"coverage-7.6.1-cp312-cp312-win_amd64.whl\", hash = \"sha256:b5d7b556859dd85f3a541db6a4e0167b86e7273e1cdc973e5b175166bb634fdb\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:a4acd025ecc06185ba2b801f2de85546e0b8ac787cf9d3b06e7e2a69f925b106\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:a6d3adcf24b624a7b778533480e32434a39ad8fa30c315208f6d3e5542aeb6e9\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d0c212c49b6c10e6951362f7c6df3329f04c2b1c28499563d4035d964ab8e08c\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6e81d7a3e58882450ec4186ca59a3f20a5d4440f25b1cff6f0902ad890e6748a\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:78b260de9790fd81e69401c2dc8b17da47c8038176a79092a89cb2b7d945d060\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:a78d169acd38300060b28d600344a803628c3fd585c912cacc9ea8790fe96862\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:2c09f4ce52cb99dd7505cd0fc8e0e37c77b87f46bc9c1eb03fe3bc9991085388\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:6878ef48d4227aace338d88c48738a4258213cd7b74fd9a3d4d7582bb1d8a155\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-win32.whl\", hash = \"sha256:44df346d5215a8c0e360307d46ffaabe0f5d3502c8a1cefd700b34baf31d411a\"},\n+    {file = \"coverage-7.6.1-cp313-cp313-win_amd64.whl\", hash = \"sha256:8284cf8c0dd272a247bc154eb6c95548722dce90d098c17a883ed36e67cdb129\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-macosx_10_13_x86_64.whl\", hash = \"sha256:d3296782ca4eab572a1a4eca686d8bfb00226300dcefdf43faa25b5242ab8a3e\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-macosx_11_0_arm64.whl\", hash = \"sha256:502753043567491d3ff6d08629270127e0c31d4184c4c8d98f92c26f65019962\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6a89ecca80709d4076b95f89f308544ec8f7b4727e8a547913a35f16717856cb\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a318d68e92e80af8b00fa99609796fdbcdfef3629c77c6283566c6f02c6d6704\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:13b0a73a0896988f053e4fbb7de6d93388e6dd292b0d87ee51d106f2c11b465b\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-musllinux_1_2_aarch64.whl\", hash = \"sha256:4421712dbfc5562150f7554f13dde997a2e932a6b5f352edcce948a815efee6f\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-musllinux_1_2_i686.whl\", hash = \"sha256:166811d20dfea725e2e4baa71fffd6c968a958577848d2131f39b60043400223\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-musllinux_1_2_x86_64.whl\", hash = \"sha256:225667980479a17db1048cb2bf8bfb39b8e5be8f164b8f6628b64f78a72cf9d3\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-win32.whl\", hash = \"sha256:170d444ab405852903b7d04ea9ae9b98f98ab6d7e63e1115e82620807519797f\"},\n+    {file = \"coverage-7.6.1-cp313-cp313t-win_amd64.whl\", hash = \"sha256:b9f222de8cded79c49bf184bdbc06630d4c58eec9459b939b4a690c82ed05657\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6db04803b6c7291985a761004e9060b2bca08da6d04f26a7f2294b8623a0c1a0\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:f1adfc8ac319e1a348af294106bc6a8458a0f1633cc62a1446aebc30c5fa186a\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a95324a9de9650a729239daea117df21f4b9868ce32e63f8b650ebe6cef5595b\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b43c03669dc4618ec25270b06ecd3ee4fa94c7f9b3c14bae6571ca00ef98b0d3\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8929543a7192c13d177b770008bc4e8119f2e1f881d563fc6b6305d2d0ebe9de\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:a09ece4a69cf399510c8ab25e0950d9cf2b42f7b3cb0374f95d2e2ff594478a6\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:9054a0754de38d9dbd01a46621636689124d666bad1936d76c0341f7d71bf569\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:0dbde0f4aa9a16fa4d754356a8f2e36296ff4d83994b2c9d8398aa32f222f989\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-win32.whl\", hash = \"sha256:da511e6ad4f7323ee5702e6633085fb76c2f893aaf8ce4c51a0ba4fc07580ea7\"},\n+    {file = \"coverage-7.6.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:3f1156e3e8f2872197af3840d8ad307a9dd18e615dc64d9ee41696f287c57ad8\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:abd5fd0db5f4dc9289408aaf34908072f805ff7792632250dcb36dc591d24255\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:547f45fa1a93154bd82050a7f3cddbc1a7a4dd2a9bf5cb7d06f4ae29fe94eaf8\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:645786266c8f18a931b65bfcefdbf6952dd0dea98feee39bd188607a9d307ed2\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:9e0b2df163b8ed01d515807af24f63de04bebcecbd6c3bfeff88385789fdf75a\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:609b06f178fe8e9f89ef676532760ec0b4deea15e9969bf754b37f7c40326dbc\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:702855feff378050ae4f741045e19a32d57d19f3e0676d589df0575008ea5004\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:2bdb062ea438f22d99cba0d7829c2ef0af1d768d1e4a4f528087224c90b132cb\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:9c56863d44bd1c4fe2abb8a4d6f5371d197f1ac0ebdee542f07f35895fc07f36\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-win32.whl\", hash = \"sha256:6e2cd258d7d927d09493c8df1ce9174ad01b381d4729a9d8d4e38670ca24774c\"},\n+    {file = \"coverage-7.6.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:06a737c882bd26d0d6ee7269b20b12f14a8704807a01056c80bb881a4b2ce6ca\"},\n+    {file = \"coverage-7.6.1-pp38.pp39.pp310-none-any.whl\", hash = \"sha256:e9a6e0eb86070e8ccaedfbd9d38fec54864f3125ab95419970575b42af7541df\"},\n+    {file = \"coverage-7.6.1.tar.gz\", hash = \"sha256:953510dfb7b12ab69d20135a0662397f077c59b1e6379a768e97c59d852ee51d\"},\n+]\n+\n+[package.dependencies]\n+tomli = {version = \"*\", optional = true, markers = \"python_full_version <= \\\"3.11.0a6\\\" and extra == \\\"toml\\\"\"}\n+\n+[package.extras]\n+toml = [\"tomli\"]\n+\n+[[package]]\n+name = \"decli\"\n+version = \"0.6.2\"\n+description = \"Minimal, easy-to-use, declarative cli tool\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"decli-0.6.2-py3-none-any.whl\", hash = \"sha256:2fc84106ce9a8f523ed501ca543bdb7e416c064917c12a59ebdc7f311a97b7ed\"},\n+    {file = \"decli-0.6.2.tar.gz\", hash = \"sha256:36f71eb55fd0093895efb4f416ec32b7f6e00147dda448e3365cf73ceab42d6f\"},\n+]\n+\n+[[package]]\n+name = \"dill\"\n+version = \"0.3.8\"\n+description = \"serialize all of Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"dill-0.3.8-py3-none-any.whl\", hash = \"sha256:c36ca9ffb54365bdd2f8eb3eff7d2a21237f8452b57ace88b1ac615b7e815bd7\"},\n+    {file = \"dill-0.3.8.tar.gz\", hash = \"sha256:3ebe3c479ad625c4553aca177444d89b486b1d84982eeacded644afc0cf797ca\"},\n+]\n+\n+[package.extras]\n+graph = [\"objgraph (>=1.7.2)\"]\n+profile = [\"gprof2dot (>=2022.7.29)\"]\n+\n+[[package]]\n+name = \"distlib\"\n+version = \"0.3.8\"\n+description = \"Distribution utilities\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"distlib-0.3.8-py2.py3-none-any.whl\", hash = \"sha256:034db59a0b96f8ca18035f36290806a9a6e6bd9d1ff91e45a7f172eb17e51784\"},\n+    {file = \"distlib-0.3.8.tar.gz\", hash = \"sha256:1530ea13e350031b6312d8580ddb6b27a104275a31106523b8f123787f494f64\"},\n+]\n+\n+[[package]]\n+name = \"django\"\n+version = \"4.2.16\"\n+description = \"A high-level Python web framework that encourages rapid development and clean, pragmatic design.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"Django-4.2.16-py3-none-any.whl\", hash = \"sha256:1ddc333a16fc139fd253035a1606bb24261951bbc3a6ca256717fa06cc41a898\"},\n+    {file = \"Django-4.2.16.tar.gz\", hash = \"sha256:6f1616c2786c408ce86ab7e10f792b8f15742f7b7b7460243929cb371e7f1dad\"},\n+]\n+\n+[package.dependencies]\n+asgiref = \">=3.6.0,<4\"\n+\"backports.zoneinfo\" = {version = \"*\", markers = \"python_version < \\\"3.9\\\"\"}\n+sqlparse = \">=0.3.1\"\n+tzdata = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+\n+[package.extras]\n+argon2 = [\"argon2-cffi (>=19.1.0)\"]\n+bcrypt = [\"bcrypt\"]\n+\n+[[package]]\n+name = \"django\"\n+version = \"5.1.1\"\n+description = \"A high-level Python web framework that encourages rapid development and clean, pragmatic design.\"\n+optional = false\n+python-versions = \">=3.10\"\n+files = [\n+    {file = \"Django-5.1.1-py3-none-any.whl\", hash = \"sha256:71603f27dac22a6533fb38d83072eea9ddb4017fead6f67f2562a40402d61c3f\"},\n+    {file = \"Django-5.1.1.tar.gz\", hash = \"sha256:021ffb7fdab3d2d388bc8c7c2434eb9c1f6f4d09e6119010bbb1694dda286bc2\"},\n+]\n+\n+[package.dependencies]\n+asgiref = \">=3.8.1,<4\"\n+sqlparse = \">=0.3.1\"\n+tzdata = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+\n+[package.extras]\n+argon2 = [\"argon2-cffi (>=19.1.0)\"]\n+bcrypt = [\"bcrypt\"]\n+\n+[[package]]\n+name = \"django-filter\"\n+version = \"24.3\"\n+description = \"Django-filter is a reusable Django application for allowing users to filter querysets dynamically.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"django_filter-24.3-py3-none-any.whl\", hash = \"sha256:c4852822928ce17fb699bcfccd644b3574f1a2d80aeb2b4ff4f16b02dd49dc64\"},\n+    {file = \"django_filter-24.3.tar.gz\", hash = \"sha256:d8ccaf6732afd21ca0542f6733b11591030fa98669f8d15599b358e24a2cd9c3\"},\n+]\n+\n+[package.dependencies]\n+Django = \">=4.2\"\n+\n+[[package]]\n+name = \"django-stubs\"\n+version = \"5.0.4\"\n+description = \"Mypy stubs for Django\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"django_stubs-5.0.4-py3-none-any.whl\", hash = \"sha256:c2502f5ecbae50c68f9a86d52b5b2447d8648fd205036dad0ccb41e19a445927\"},\n+    {file = \"django_stubs-5.0.4.tar.gz\", hash = \"sha256:78e3764488fdfd2695f12502136548ec22f8d4b1780541a835042b8238d11514\"},\n+]\n+\n+[package.dependencies]\n+asgiref = \"*\"\n+django = \"*\"\n+django-stubs-ext = \">=5.0.4\"\n+tomli = {version = \"*\", markers = \"python_version < \\\"3.11\\\"\"}\n+types-PyYAML = \"*\"\n+typing-extensions = \">=4.11.0\"\n+\n+[package.extras]\n+compatible-mypy = [\"mypy (>=1.11.0,<1.12.0)\"]\n+oracle = [\"oracledb\"]\n+redis = [\"redis\"]\n+\n+[[package]]\n+name = \"django-stubs-ext\"\n+version = \"5.0.4\"\n+description = \"Monkey-patching and extensions for django-stubs\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"django_stubs_ext-5.0.4-py3-none-any.whl\", hash = \"sha256:910cbaff3d1e8e806a5c27d5ddd4088535aae8371ea921b7fd680fdfa5f14e30\"},\n+    {file = \"django_stubs_ext-5.0.4.tar.gz\", hash = \"sha256:85da065224204774208be29c7d02b4482d5a69218a728465c2fbe41725fdc819\"},\n+]\n+\n+[package.dependencies]\n+django = \"*\"\n+typing-extensions = \"*\"\n+\n+[[package]]\n+name = \"djangorestframework\"\n+version = \"3.15.2\"\n+description = \"Web APIs for Django, made easy.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"djangorestframework-3.15.2-py3-none-any.whl\", hash = \"sha256:2b8871b062ba1aefc2de01f773875441a961fefbf79f5eed1e32b2f096944b20\"},\n+    {file = \"djangorestframework-3.15.2.tar.gz\", hash = \"sha256:36fe88cd2d6c6bec23dca9804bab2ba5517a8bb9d8f47ebc68981b56840107ad\"},\n+]\n+\n+[package.dependencies]\n+\"backports.zoneinfo\" = {version = \"*\", markers = \"python_version < \\\"3.9\\\"\"}\n+django = \">=4.2\"\n+\n+[[package]]\n+name = \"docformatter\"\n+version = \"1.7.5\"\n+description = \"Formats docstrings to follow PEP 257\"\n+optional = false\n+python-versions = \">=3.7,<4.0\"\n+files = [\n+    {file = \"docformatter-1.7.5-py3-none-any.whl\", hash = \"sha256:a24f5545ed1f30af00d106f5d85dc2fce4959295687c24c8f39f5263afaf9186\"},\n+    {file = \"docformatter-1.7.5.tar.gz\", hash = \"sha256:ffed3da0daffa2e77f80ccba4f0e50bfa2755e1c10e130102571c890a61b246e\"},\n+]\n+\n+[package.dependencies]\n+charset_normalizer = \">=3.0.0,<4.0.0\"\n+untokenize = \">=0.1.1,<0.2.0\"\n+\n+[package.extras]\n+tomli = [\"tomli (>=2.0.0,<3.0.0)\"]\n+\n+[[package]]\n+name = \"docutils\"\n+version = \"0.19\"\n+description = \"Docutils -- Python Documentation Utilities\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"docutils-0.19-py3-none-any.whl\", hash = \"sha256:5e1de4d849fee02c63b040a4a3fd567f4ab104defd8a5511fbbc24a8a017efbc\"},\n+    {file = \"docutils-0.19.tar.gz\", hash = \"sha256:33995a6753c30b7f577febfc2c50411fec6aac7f7ffeb7c4cfe5991072dcf9e6\"},\n+]\n+\n+[[package]]\n+name = \"dotty-dict\"\n+version = \"1.3.1\"\n+description = \"Dictionary wrapper for quick access to deeply nested keys.\"\n+optional = false\n+python-versions = \">=3.5,<4.0\"\n+files = [\n+    {file = \"dotty_dict-1.3.1-py3-none-any.whl\", hash = \"sha256:5022d234d9922f13aa711b4950372a06a6d64cb6d6db9ba43d0ba133ebfce31f\"},\n+    {file = \"dotty_dict-1.3.1.tar.gz\", hash = \"sha256:4b016e03b8ae265539757a53eba24b9bfda506fb94fbce0bee843c6f05541a15\"},\n+]\n+\n+[[package]]\n+name = \"exceptiongroup\"\n+version = \"1.2.2\"\n+description = \"Backport of PEP 654 (exception groups)\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"exceptiongroup-1.2.2-py3-none-any.whl\", hash = \"sha256:3111b9d131c238bec2f8f516e123e14ba243563fb135d3fe885990585aa7795b\"},\n+    {file = \"exceptiongroup-1.2.2.tar.gz\", hash = \"sha256:47c2edf7c6738fafb49fd34290706d1a1a2f4d1c6df275526b62cbb4aa5393cc\"},\n+]\n+\n+[package.extras]\n+test = [\"pytest (>=6)\"]\n+\n+[[package]]\n+name = \"filelock\"\n+version = \"3.16.0\"\n+description = \"A platform independent file lock.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"filelock-3.16.0-py3-none-any.whl\", hash = \"sha256:f6ed4c963184f4c84dd5557ce8fece759a3724b37b80c6c4f20a2f63a4dc6609\"},\n+    {file = \"filelock-3.16.0.tar.gz\", hash = \"sha256:81de9eb8453c769b63369f87f11131a7ab04e367f8d97ad39dc230daa07e3bec\"},\n+]\n+\n+[package.extras]\n+docs = [\"furo (>=2024.8.6)\", \"sphinx (>=8.0.2)\", \"sphinx-autodoc-typehints (>=2.4)\"]\n+testing = [\"covdefaults (>=2.3)\", \"coverage (>=7.6.1)\", \"diff-cover (>=9.1.1)\", \"pytest (>=8.3.2)\", \"pytest-asyncio (>=0.24)\", \"pytest-cov (>=5)\", \"pytest-mock (>=3.14)\", \"pytest-timeout (>=2.3.1)\", \"virtualenv (>=20.26.3)\"]\n+typing = [\"typing-extensions (>=4.12.2)\"]\n+\n+[[package]]\n+name = \"gitdb\"\n+version = \"4.0.11\"\n+description = \"Git Object Database\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"gitdb-4.0.11-py3-none-any.whl\", hash = \"sha256:81a3407ddd2ee8df444cbacea00e2d038e40150acfa3001696fe0dcf1d3adfa4\"},\n+    {file = \"gitdb-4.0.11.tar.gz\", hash = \"sha256:bf5421126136d6d0af55bc1e7c1af1c397a34f5b7bd79e776cd3e89785c2b04b\"},\n+]\n+\n+[package.dependencies]\n+smmap = \">=3.0.1,<6\"\n+\n+[[package]]\n+name = \"gitpython\"\n+version = \"3.1.43\"\n+description = \"GitPython is a Python library used to interact with Git repositories\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"GitPython-3.1.43-py3-none-any.whl\", hash = \"sha256:eec7ec56b92aad751f9912a73404bc02ba212a23adb2c7098ee668417051a1ff\"},\n+    {file = \"GitPython-3.1.43.tar.gz\", hash = \"sha256:35f314a9f878467f5453cc1fee295c3e18e52f1b99f10f6cf5b1682e968a9e7c\"},\n+]\n+\n+[package.dependencies]\n+gitdb = \">=4.0.1,<5\"\n+\n+[package.extras]\n+doc = [\"sphinx (==4.3.2)\", \"sphinx-autodoc-typehints\", \"sphinx-rtd-theme\", \"sphinxcontrib-applehelp (>=1.0.2,<=1.0.4)\", \"sphinxcontrib-devhelp (==1.0.2)\", \"sphinxcontrib-htmlhelp (>=2.0.0,<=2.0.1)\", \"sphinxcontrib-qthelp (==1.0.3)\", \"sphinxcontrib-serializinghtml (==1.1.5)\"]\n+test = [\"coverage[toml]\", \"ddt (>=1.1.1,!=1.4.3)\", \"mock\", \"mypy\", \"pre-commit\", \"pytest (>=7.3.1)\", \"pytest-cov\", \"pytest-instafail\", \"pytest-mock\", \"pytest-sugar\", \"typing-extensions\"]\n+\n+[[package]]\n+name = \"identify\"\n+version = \"2.6.1\"\n+description = \"File identification library for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"identify-2.6.1-py2.py3-none-any.whl\", hash = \"sha256:53863bcac7caf8d2ed85bd20312ea5dcfc22226800f6d6881f232d861db5a8f0\"},\n+    {file = \"identify-2.6.1.tar.gz\", hash = \"sha256:91478c5fb7c3aac5ff7bf9b4344f803843dc586832d5f110d672b19aa1984c98\"},\n+]\n+\n+[package.extras]\n+license = [\"ukkonen\"]\n+\n+[[package]]\n+name = \"idna\"\n+version = \"3.9\"\n+description = \"Internationalized Domain Names in Applications (IDNA)\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"idna-3.9-py3-none-any.whl\", hash = \"sha256:69297d5da0cc9281c77efffb4e730254dd45943f45bbfb461de5991713989b1e\"},\n+    {file = \"idna-3.9.tar.gz\", hash = \"sha256:e5c5dafde284f26e9e0f28f6ea2d6400abd5ca099864a67f576f3981c6476124\"},\n+]\n+\n+[package.extras]\n+all = [\"flake8 (>=7.1.1)\", \"mypy (>=1.11.2)\", \"pytest (>=8.3.2)\", \"ruff (>=0.6.2)\"]\n+\n+[[package]]\n+name = \"imagesize\"\n+version = \"1.4.1\"\n+description = \"Getting image size from png/jpeg/jpeg2000/gif file\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+files = [\n+    {file = \"imagesize-1.4.1-py2.py3-none-any.whl\", hash = \"sha256:0d8d18d08f840c19d0ee7ca1fd82490fdc3729b7ac93f49870406ddde8ef8d8b\"},\n+    {file = \"imagesize-1.4.1.tar.gz\", hash = \"sha256:69150444affb9cb0d5cc5a92b3676f0b2fb7cd9ae39e947a5e11a36b4497cd4a\"},\n+]\n+\n+[[package]]\n+name = \"importlib-metadata\"\n+version = \"8.5.0\"\n+description = \"Read metadata from Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"importlib_metadata-8.5.0-py3-none-any.whl\", hash = \"sha256:45e54197d28b7a7f1559e60b95e7c567032b602131fbd588f1497f47880aa68b\"},\n+    {file = \"importlib_metadata-8.5.0.tar.gz\", hash = \"sha256:71522656f0abace1d072b9e5481a48f07c138e00f079c38c8f883823f9c26bd7\"},\n+]\n+\n+[package.dependencies]\n+zipp = \">=3.20\"\n+\n+[package.extras]\n+check = [\"pytest-checkdocs (>=2.4)\", \"pytest-ruff (>=0.2.1)\"]\n+cover = [\"pytest-cov\"]\n+doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-lint\"]\n+enabler = [\"pytest-enabler (>=2.2)\"]\n+perf = [\"ipython\"]\n+test = [\"flufl.flake8\", \"importlib-resources (>=1.3)\", \"jaraco.test (>=5.4)\", \"packaging\", \"pyfakefs\", \"pytest (>=6,!=8.1.*)\", \"pytest-perf (>=0.9.2)\"]\n+type = [\"pytest-mypy\"]\n+\n+[[package]]\n+name = \"importlib-resources\"\n+version = \"6.4.5\"\n+description = \"Read resources from Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"importlib_resources-6.4.5-py3-none-any.whl\", hash = \"sha256:ac29d5f956f01d5e4bb63102a5a19957f1b9175e45649977264a1416783bb717\"},\n+    {file = \"importlib_resources-6.4.5.tar.gz\", hash = \"sha256:980862a1d16c9e147a59603677fa2aa5fd82b87f223b6cb870695bcfce830065\"},\n+]\n+\n+[package.dependencies]\n+zipp = {version = \">=3.1.0\", markers = \"python_version < \\\"3.10\\\"\"}\n+\n+[package.extras]\n+check = [\"pytest-checkdocs (>=2.4)\", \"pytest-ruff (>=0.2.1)\"]\n+cover = [\"pytest-cov\"]\n+doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-lint\"]\n+enabler = [\"pytest-enabler (>=2.2)\"]\n+test = [\"jaraco.test (>=5.4)\", \"pytest (>=6,!=8.1.*)\", \"zipp (>=3.17)\"]\n+type = [\"pytest-mypy\"]\n+\n+[[package]]\n+name = \"iniconfig\"\n+version = \"2.0.0\"\n+description = \"brain-dead simple config-ini parsing\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"iniconfig-2.0.0-py3-none-any.whl\", hash = \"sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\"},\n+    {file = \"iniconfig-2.0.0.tar.gz\", hash = \"sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3\"},\n+]\n+\n+[[package]]\n+name = \"isort\"\n+version = \"5.13.2\"\n+description = \"A Python utility / library to sort Python imports.\"\n+optional = false\n+python-versions = \">=3.8.0\"\n+files = [\n+    {file = \"isort-5.13.2-py3-none-any.whl\", hash = \"sha256:8ca5e72a8d85860d5a3fa69b8745237f2939afe12dbf656afbcb47fe72d947a6\"},\n+    {file = \"isort-5.13.2.tar.gz\", hash = \"sha256:48fdfcb9face5d58a4f6dde2e72a1fb8dcaf8ab26f95ab49fab84c2ddefb0109\"},\n+]\n+\n+[package.extras]\n+colors = [\"colorama (>=0.4.6)\"]\n+\n+[[package]]\n+name = \"jinja2\"\n+version = \"3.1.4\"\n+description = \"A very fast and expressive template engine.\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"jinja2-3.1.4-py3-none-any.whl\", hash = \"sha256:bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d\"},\n+    {file = \"jinja2-3.1.4.tar.gz\", hash = \"sha256:4a3aee7acbbe7303aede8e9648d13b8bf88a429282aa6122a993f0ac800cb369\"},\n+]\n+\n+[package.dependencies]\n+MarkupSafe = \">=2.0\"\n+\n+[package.extras]\n+i18n = [\"Babel (>=2.7)\"]\n+\n+[[package]]\n+name = \"markdown-it-py\"\n+version = \"3.0.0\"\n+description = \"Python port of markdown-it. Markdown parsing, done right!\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"markdown-it-py-3.0.0.tar.gz\", hash = \"sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb\"},\n+    {file = \"markdown_it_py-3.0.0-py3-none-any.whl\", hash = \"sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1\"},\n+]\n+\n+[package.dependencies]\n+mdurl = \">=0.1,<1.0\"\n+\n+[package.extras]\n+benchmarking = [\"psutil\", \"pytest\", \"pytest-benchmark\"]\n+code-style = [\"pre-commit (>=3.0,<4.0)\"]\n+compare = [\"commonmark (>=0.9,<1.0)\", \"markdown (>=3.4,<4.0)\", \"mistletoe (>=1.0,<2.0)\", \"mistune (>=2.0,<3.0)\", \"panflute (>=2.3,<3.0)\"]\n+linkify = [\"linkify-it-py (>=1,<3)\"]\n+plugins = [\"mdit-py-plugins\"]\n+profiling = [\"gprof2dot\"]\n+rtd = [\"jupyter_sphinx\", \"mdit-py-plugins\", \"myst-parser\", \"pyyaml\", \"sphinx\", \"sphinx-copybutton\", \"sphinx-design\", \"sphinx_book_theme\"]\n+testing = [\"coverage\", \"pytest\", \"pytest-cov\", \"pytest-regressions\"]\n+\n+[[package]]\n+name = \"markupsafe\"\n+version = \"2.1.5\"\n+description = \"Safely add untrusted strings to HTML/XML markup.\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:a17a92de5231666cfbe003f0e4b9b3a7ae3afb1ec2845aadc2bacc93ff85febc\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e61659ba32cf2cf1481e575d0462554625196a1f2fc06a1c777d3f48e8865d46\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ae2ad8ae6ebee9d2d94b17fb62763125f3f374c25618198f40cbb8b525411900\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fce659a462a1be54d2ffcacea5e3ba2d74daa74f30f5f143fe0c58636e355fdd\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-win32.whl\", hash = \"sha256:d9fad5155d72433c921b782e58892377c44bd6252b5af2f67f16b194987338a4\"},\n+    {file = \"MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl\", hash = \"sha256:bf50cd79a75d181c9181df03572cdce0fbb75cc353bc350712073108cba98de5\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-win32.whl\", hash = \"sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906\"},\n+    {file = \"MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl\", hash = \"sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-win32.whl\", hash = \"sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad\"},\n+    {file = \"MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl\", hash = \"sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:c8b29db45f8fe46ad280a7294f5c3ec36dbac9491f2d1c17345be8e69cc5928f\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ec6a563cff360b50eed26f13adc43e61bc0c04d94b8be985e6fb24b81f6dcfdf\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a549b9c31bec33820e885335b451286e2969a2d9e24879f83fe904a5ce59d70a\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:ce409136744f6521e39fd8e2a24c53fa18ad67aa5bc7c2cf83645cce5b5c4e50\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-win32.whl\", hash = \"sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371\"},\n+    {file = \"MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl\", hash = \"sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fa9db3f79de01457b03d4f01b34cf91bc0048eb2c3846ff26f66687c2f6d16ab\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ffee1f21e5ef0d712f9033568f8344d5da8cc2869dbd08d87c84656e6a2d2f68\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-win32.whl\", hash = \"sha256:daa4ee5a243f0f20d528d939d06670a298dd39b1ad5f8a72a4275124a7819eff\"},\n+    {file = \"MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl\", hash = \"sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:db0b55e0f3cc0be60c1f19efdde9a637c32740486004f20d1cff53c3c0ece4d2\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:bff1b4290a66b490a2f4719358c0cdcd9bafb6b8f061e45c7a2460866bf50c2e\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:bc1667f8b83f48511b94671e0e441401371dfd0f0a795c7daa4a3cd1dde55bea\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-win32.whl\", hash = \"sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf\"},\n+    {file = \"MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl\", hash = \"sha256:fa173ec60341d6bb97a89f5ea19c85c5643c1e7dedebc22f5181eb73573142c5\"},\n+    {file = \"MarkupSafe-2.1.5.tar.gz\", hash = \"sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b\"},\n+]\n+\n+[[package]]\n+name = \"mccabe\"\n+version = \"0.7.0\"\n+description = \"McCabe checker, plugin for flake8\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"mccabe-0.7.0-py2.py3-none-any.whl\", hash = \"sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e\"},\n+    {file = \"mccabe-0.7.0.tar.gz\", hash = \"sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325\"},\n+]\n+\n+[[package]]\n+name = \"mdurl\"\n+version = \"0.1.2\"\n+description = \"Markdown URL utilities\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"mdurl-0.1.2-py3-none-any.whl\", hash = \"sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8\"},\n+    {file = \"mdurl-0.1.2.tar.gz\", hash = \"sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\"},\n+]\n+\n+[[package]]\n+name = \"mypy\"\n+version = \"1.11.2\"\n+description = \"Optional static typing for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"mypy-1.11.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:d42a6dd818ffce7be66cce644f1dff482f1d97c53ca70908dff0b9ddc120b77a\"},\n+    {file = \"mypy-1.11.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:801780c56d1cdb896eacd5619a83e427ce436d86a3bdf9112527f24a66618fef\"},\n+    {file = \"mypy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:41ea707d036a5307ac674ea172875f40c9d55c5394f888b168033177fce47383\"},\n+    {file = \"mypy-1.11.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:6e658bd2d20565ea86da7d91331b0eed6d2eee22dc031579e6297f3e12c758c8\"},\n+    {file = \"mypy-1.11.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:478db5f5036817fe45adb7332d927daa62417159d49783041338921dcf646fc7\"},\n+    {file = \"mypy-1.11.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:75746e06d5fa1e91bfd5432448d00d34593b52e7e91a187d981d08d1f33d4385\"},\n+    {file = \"mypy-1.11.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:a976775ab2256aadc6add633d44f100a2517d2388906ec4f13231fafbb0eccca\"},\n+    {file = \"mypy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:cd953f221ac1379050a8a646585a29574488974f79d8082cedef62744f0a0104\"},\n+    {file = \"mypy-1.11.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:57555a7715c0a34421013144a33d280e73c08df70f3a18a552938587ce9274f4\"},\n+    {file = \"mypy-1.11.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:36383a4fcbad95f2657642a07ba22ff797de26277158f1cc7bd234821468b1b6\"},\n+    {file = \"mypy-1.11.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:e8960dbbbf36906c5c0b7f4fbf2f0c7ffb20f4898e6a879fcf56a41a08b0d318\"},\n+    {file = \"mypy-1.11.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:06d26c277962f3fb50e13044674aa10553981ae514288cb7d0a738f495550b36\"},\n+    {file = \"mypy-1.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:6e7184632d89d677973a14d00ae4d03214c8bc301ceefcdaf5c474866814c987\"},\n+    {file = \"mypy-1.11.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:3a66169b92452f72117e2da3a576087025449018afc2d8e9bfe5ffab865709ca\"},\n+    {file = \"mypy-1.11.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:969ea3ef09617aff826885a22ece0ddef69d95852cdad2f60c8bb06bf1f71f70\"},\n+    {file = \"mypy-1.11.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:37c7fa6121c1cdfcaac97ce3d3b5588e847aa79b580c1e922bb5d5d2902df19b\"},\n+    {file = \"mypy-1.11.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:4a8a53bc3ffbd161b5b2a4fff2f0f1e23a33b0168f1c0778ec70e1a3d66deb86\"},\n+    {file = \"mypy-1.11.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:2ff93107f01968ed834f4256bc1fc4475e2fecf6c661260066a985b52741ddce\"},\n+    {file = \"mypy-1.11.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:edb91dded4df17eae4537668b23f0ff6baf3707683734b6a818d5b9d0c0c31a1\"},\n+    {file = \"mypy-1.11.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:ee23de8530d99b6db0573c4ef4bd8f39a2a6f9b60655bf7a1357e585a3486f2b\"},\n+    {file = \"mypy-1.11.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:801ca29f43d5acce85f8e999b1e431fb479cb02d0e11deb7d2abb56bdaf24fd6\"},\n+    {file = \"mypy-1.11.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:af8d155170fcf87a2afb55b35dc1a0ac21df4431e7d96717621962e4b9192e70\"},\n+    {file = \"mypy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:f7821776e5c4286b6a13138cc935e2e9b6fde05e081bdebf5cdb2bb97c9df81d\"},\n+    {file = \"mypy-1.11.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:539c570477a96a4e6fb718b8d5c3e0c0eba1f485df13f86d2970c91f0673148d\"},\n+    {file = \"mypy-1.11.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:3f14cd3d386ac4d05c5a39a51b84387403dadbd936e17cb35882134d4f8f0d24\"},\n+    {file = \"mypy-1.11.2-py3-none-any.whl\", hash = \"sha256:b499bc07dbdcd3de92b0a8b29fdf592c111276f6a12fe29c30f6c417dd546d12\"},\n+    {file = \"mypy-1.11.2.tar.gz\", hash = \"sha256:7f9993ad3e0ffdc95c2a14b66dee63729f021968bff8ad911867579c65d13a79\"},\n+]\n+\n+[package.dependencies]\n+mypy-extensions = \">=1.0.0\"\n+tomli = {version = \">=1.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\n+typing-extensions = \">=4.6.0\"\n+\n+[package.extras]\n+dmypy = [\"psutil (>=4.0)\"]\n+install-types = [\"pip\"]\n+mypyc = [\"setuptools (>=50)\"]\n+reports = [\"lxml\"]\n+\n+[[package]]\n+name = \"mypy-extensions\"\n+version = \"1.0.0\"\n+description = \"Type system extensions for programs checked with the mypy type checker.\"\n+optional = false\n+python-versions = \">=3.5\"\n+files = [\n+    {file = \"mypy_extensions-1.0.0-py3-none-any.whl\", hash = \"sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d\"},\n+    {file = \"mypy_extensions-1.0.0.tar.gz\", hash = \"sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782\"},\n+]\n+\n+[[package]]\n+name = \"nodeenv\"\n+version = \"1.9.1\"\n+description = \"Node.js virtual environment builder\"\n+optional = false\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"\n+files = [\n+    {file = \"nodeenv-1.9.1-py2.py3-none-any.whl\", hash = \"sha256:ba11c9782d29c27c70ffbdda2d7415098754709be8a7056d79a737cd901155c9\"},\n+    {file = \"nodeenv-1.9.1.tar.gz\", hash = \"sha256:6ec12890a2dab7946721edbfbcd91f3319c6ccc9aec47be7c7e6b7011ee6645f\"},\n+]\n+\n+[[package]]\n+name = \"packaging\"\n+version = \"24.1\"\n+description = \"Core utilities for Python packages\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"packaging-24.1-py3-none-any.whl\", hash = \"sha256:5b8f2217dbdbd2f7f384c41c628544e6d52f2d0f53c6d0c3ea61aa5d1d7ff124\"},\n+    {file = \"packaging-24.1.tar.gz\", hash = \"sha256:026ed72c8ed3fcce5bf8950572258698927fd1dbda10a5e981cdf0ac37f4f002\"},\n+]\n+\n+[[package]]\n+name = \"pathspec\"\n+version = \"0.12.1\"\n+description = \"Utility library for gitignore style pattern matching of file paths.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pathspec-0.12.1-py3-none-any.whl\", hash = \"sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08\"},\n+    {file = \"pathspec-0.12.1.tar.gz\", hash = \"sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712\"},\n+]\n+\n+[[package]]\n+name = \"pbr\"\n+version = \"6.1.0\"\n+description = \"Python Build Reasonableness\"\n+optional = false\n+python-versions = \">=2.6\"\n+files = [\n+    {file = \"pbr-6.1.0-py2.py3-none-any.whl\", hash = \"sha256:a776ae228892d8013649c0aeccbb3d5f99ee15e005a4cbb7e61d55a067b28a2a\"},\n+    {file = \"pbr-6.1.0.tar.gz\", hash = \"sha256:788183e382e3d1d7707db08978239965e8b9e4e5ed42669bf4758186734d5f24\"},\n+]\n+\n+[[package]]\n+name = \"platformdirs\"\n+version = \"4.3.3\"\n+description = \"A small Python package for determining appropriate platform-specific dirs, e.g. a `user data dir`.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"platformdirs-4.3.3-py3-none-any.whl\", hash = \"sha256:50a5450e2e84f44539718293cbb1da0a0885c9d14adf21b77bae4e66fc99d9b5\"},\n+    {file = \"platformdirs-4.3.3.tar.gz\", hash = \"sha256:d4e0b7d8ec176b341fb03cb11ca12d0276faa8c485f9cd218f613840463fc2c0\"},\n+]\n+\n+[package.extras]\n+docs = [\"furo (>=2024.8.6)\", \"proselint (>=0.14)\", \"sphinx (>=8.0.2)\", \"sphinx-autodoc-typehints (>=2.4)\"]\n+test = [\"appdirs (==1.4.4)\", \"covdefaults (>=2.3)\", \"pytest (>=8.3.2)\", \"pytest-cov (>=5)\", \"pytest-mock (>=3.14)\"]\n+type = [\"mypy (>=1.11.2)\"]\n+\n+[[package]]\n+name = \"pluggy\"\n+version = \"1.5.0\"\n+description = \"plugin and hook calling mechanisms for python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pluggy-1.5.0-py3-none-any.whl\", hash = \"sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669\"},\n+    {file = \"pluggy-1.5.0.tar.gz\", hash = \"sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1\"},\n+]\n+\n+[package.extras]\n+dev = [\"pre-commit\", \"tox\"]\n+testing = [\"pytest\", \"pytest-benchmark\"]\n+\n+[[package]]\n+name = \"pre-commit\"\n+version = \"3.5.0\"\n+description = \"A framework for managing and maintaining multi-language pre-commit hooks.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pre_commit-3.5.0-py2.py3-none-any.whl\", hash = \"sha256:841dc9aef25daba9a0238cd27984041fa0467b4199fc4852e27950664919f660\"},\n+    {file = \"pre_commit-3.5.0.tar.gz\", hash = \"sha256:5804465c675b659b0862f07907f96295d490822a450c4c40e747d0b1c6ebcb32\"},\n+]\n+\n+[package.dependencies]\n+cfgv = \">=2.0.0\"\n+identify = \">=1.0.0\"\n+nodeenv = \">=0.11.1\"\n+pyyaml = \">=5.1\"\n+virtualenv = \">=20.10.0\"\n+\n+[[package]]\n+name = \"prompt-toolkit\"\n+version = \"3.0.36\"\n+description = \"Library for building powerful interactive command lines in Python\"\n+optional = false\n+python-versions = \">=3.6.2\"\n+files = [\n+    {file = \"prompt_toolkit-3.0.36-py3-none-any.whl\", hash = \"sha256:aa64ad242a462c5ff0363a7b9cfe696c20d55d9fc60c11fd8e632d064804d305\"},\n+    {file = \"prompt_toolkit-3.0.36.tar.gz\", hash = \"sha256:3e163f254bef5a03b146397d7c1963bd3e2812f0964bb9a24e6ec761fd28db63\"},\n+]\n+\n+[package.dependencies]\n+wcwidth = \"*\"\n+\n+[[package]]\n+name = \"pydantic\"\n+version = \"2.9.1\"\n+description = \"Data validation using Python type hints\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pydantic-2.9.1-py3-none-any.whl\", hash = \"sha256:7aff4db5fdf3cf573d4b3c30926a510a10e19a0774d38fc4967f78beb6deb612\"},\n+    {file = \"pydantic-2.9.1.tar.gz\", hash = \"sha256:1363c7d975c7036df0db2b4a61f2e062fbc0aa5ab5f2772e0ffc7191a4f4bce2\"},\n+]\n+\n+[package.dependencies]\n+annotated-types = \">=0.6.0\"\n+pydantic-core = \"2.23.3\"\n+typing-extensions = [\n+    {version = \">=4.6.1\", markers = \"python_version < \\\"3.13\\\"\"},\n+    {version = \">=4.12.2\", markers = \"python_version >= \\\"3.13\\\"\"},\n+]\n+\n+[package.extras]\n+email = [\"email-validator (>=2.0.0)\"]\n+timezone = [\"tzdata\"]\n+\n+[[package]]\n+name = \"pydantic-core\"\n+version = \"2.23.3\"\n+description = \"Core functionality for Pydantic validation and serialization\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:7f10a5d1b9281392f1bf507d16ac720e78285dfd635b05737c3911637601bae6\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:3c09a7885dd33ee8c65266e5aa7fb7e2f23d49d8043f089989726391dd7350c5\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6470b5a1ec4d1c2e9afe928c6cb37eb33381cab99292a708b8cb9aa89e62429b\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:9172d2088e27d9a185ea0a6c8cebe227a9139fd90295221d7d495944d2367700\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:86fc6c762ca7ac8fbbdff80d61b2c59fb6b7d144aa46e2d54d9e1b7b0e780e01\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f0cb80fd5c2df4898693aa841425ea1727b1b6d2167448253077d2a49003e0ed\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:03667cec5daf43ac4995cefa8aaf58f99de036204a37b889c24a80927b629cec\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:047531242f8e9c2db733599f1c612925de095e93c9cc0e599e96cf536aaf56ba\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:5499798317fff7f25dbef9347f4451b91ac2a4330c6669821c8202fd354c7bee\"},\n+    {file = \"pydantic_core-2.23.3-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:bbb5e45eab7624440516ee3722a3044b83fff4c0372efe183fd6ba678ff681fe\"},\n+    {file = \"pydantic_core-2.23.3-cp310-none-win32.whl\", hash = \"sha256:8b5b3ed73abb147704a6e9f556d8c5cb078f8c095be4588e669d315e0d11893b\"},\n+    {file = \"pydantic_core-2.23.3-cp310-none-win_amd64.whl\", hash = \"sha256:2b603cde285322758a0279995b5796d64b63060bfbe214b50a3ca23b5cee3e83\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:c889fd87e1f1bbeb877c2ee56b63bb297de4636661cc9bbfcf4b34e5e925bc27\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:ea85bda3189fb27503af4c45273735bcde3dd31c1ab17d11f37b04877859ef45\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a7f7f72f721223f33d3dc98a791666ebc6a91fa023ce63733709f4894a7dc611\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:2b2b55b0448e9da68f56b696f313949cda1039e8ec7b5d294285335b53104b61\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c24574c7e92e2c56379706b9a3f07c1e0c7f2f87a41b6ee86653100c4ce343e5\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f2b05e6ccbee333a8f4b8f4d7c244fdb7a979e90977ad9c51ea31261e2085ce0\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e2c409ce1c219c091e47cb03feb3c4ed8c2b8e004efc940da0166aaee8f9d6c8\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:d965e8b325f443ed3196db890d85dfebbb09f7384486a77461347f4adb1fa7f8\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:f56af3a420fb1ffaf43ece3ea09c2d27c444e7c40dcb7c6e7cf57aae764f2b48\"},\n+    {file = \"pydantic_core-2.23.3-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:5b01a078dd4f9a52494370af21aa52964e0a96d4862ac64ff7cea06e0f12d2c5\"},\n+    {file = \"pydantic_core-2.23.3-cp311-none-win32.whl\", hash = \"sha256:560e32f0df04ac69b3dd818f71339983f6d1f70eb99d4d1f8e9705fb6c34a5c1\"},\n+    {file = \"pydantic_core-2.23.3-cp311-none-win_amd64.whl\", hash = \"sha256:c744fa100fdea0d000d8bcddee95213d2de2e95b9c12be083370b2072333a0fa\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:e0ec50663feedf64d21bad0809f5857bac1ce91deded203efc4a84b31b2e4305\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:db6e6afcb95edbe6b357786684b71008499836e91f2a4a1e55b840955b341dbb\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:98ccd69edcf49f0875d86942f4418a4e83eb3047f20eb897bffa62a5d419c8fa\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:a678c1ac5c5ec5685af0133262103defb427114e62eafeda12f1357a12140162\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:01491d8b4d8db9f3391d93b0df60701e644ff0894352947f31fff3e52bd5c801\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:fcf31facf2796a2d3b7fe338fe8640aa0166e4e55b4cb108dbfd1058049bf4cb\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7200fd561fb3be06827340da066df4311d0b6b8eb0c2116a110be5245dceb326\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:dc1636770a809dee2bd44dd74b89cc80eb41172bcad8af75dd0bc182c2666d4c\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:67a5def279309f2e23014b608c4150b0c2d323bd7bccd27ff07b001c12c2415c\"},\n+    {file = \"pydantic_core-2.23.3-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:748bdf985014c6dd3e1e4cc3db90f1c3ecc7246ff5a3cd4ddab20c768b2f1dab\"},\n+    {file = \"pydantic_core-2.23.3-cp312-none-win32.whl\", hash = \"sha256:255ec6dcb899c115f1e2a64bc9ebc24cc0e3ab097775755244f77360d1f3c06c\"},\n+    {file = \"pydantic_core-2.23.3-cp312-none-win_amd64.whl\", hash = \"sha256:40b8441be16c1e940abebed83cd006ddb9e3737a279e339dbd6d31578b802f7b\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:6daaf5b1ba1369a22c8b050b643250e3e5efc6a78366d323294aee54953a4d5f\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:d015e63b985a78a3d4ccffd3bdf22b7c20b3bbd4b8227809b3e8e75bc37f9cb2\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a3fc572d9b5b5cfe13f8e8a6e26271d5d13f80173724b738557a8c7f3a8a3791\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:f6bd91345b5163ee7448bee201ed7dd601ca24f43f439109b0212e296eb5b423\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:fc379c73fd66606628b866f661e8785088afe2adaba78e6bbe80796baf708a63\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:fbdce4b47592f9e296e19ac31667daed8753c8367ebb34b9a9bd89dacaa299c9\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fc3cf31edf405a161a0adad83246568647c54404739b614b1ff43dad2b02e6d5\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:8e22b477bf90db71c156f89a55bfe4d25177b81fce4aa09294d9e805eec13855\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:0a0137ddf462575d9bce863c4c95bac3493ba8e22f8c28ca94634b4a1d3e2bb4\"},\n+    {file = \"pydantic_core-2.23.3-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:203171e48946c3164fe7691fc349c79241ff8f28306abd4cad5f4f75ed80bc8d\"},\n+    {file = \"pydantic_core-2.23.3-cp313-none-win32.whl\", hash = \"sha256:76bdab0de4acb3f119c2a4bff740e0c7dc2e6de7692774620f7452ce11ca76c8\"},\n+    {file = \"pydantic_core-2.23.3-cp313-none-win_amd64.whl\", hash = \"sha256:37ba321ac2a46100c578a92e9a6aa33afe9ec99ffa084424291d84e456f490c1\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:d063c6b9fed7d992bcbebfc9133f4c24b7a7f215d6b102f3e082b1117cddb72c\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:6cb968da9a0746a0cf521b2b5ef25fc5a0bee9b9a1a8214e0a1cfaea5be7e8a4\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:edbefe079a520c5984e30e1f1f29325054b59534729c25b874a16a5048028d16\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:cbaaf2ef20d282659093913da9d402108203f7cb5955020bd8d1ae5a2325d1c4\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:fb539d7e5dc4aac345846f290cf504d2fd3c1be26ac4e8b5e4c2b688069ff4cf\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:7e6f33503c5495059148cc486867e1d24ca35df5fc064686e631e314d959ad5b\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:04b07490bc2f6f2717b10c3969e1b830f5720b632f8ae2f3b8b1542394c47a8e\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:03795b9e8a5d7fda05f3873efc3f59105e2dcff14231680296b87b80bb327295\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:c483dab0f14b8d3f0df0c6c18d70b21b086f74c87ab03c59250dbf6d3c89baba\"},\n+    {file = \"pydantic_core-2.23.3-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:8b2682038e255e94baf2c473dca914a7460069171ff5cdd4080be18ab8a7fd6e\"},\n+    {file = \"pydantic_core-2.23.3-cp38-none-win32.whl\", hash = \"sha256:f4a57db8966b3a1d1a350012839c6a0099f0898c56512dfade8a1fe5fb278710\"},\n+    {file = \"pydantic_core-2.23.3-cp38-none-win_amd64.whl\", hash = \"sha256:13dd45ba2561603681a2676ca56006d6dee94493f03d5cadc055d2055615c3ea\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:82da2f4703894134a9f000e24965df73cc103e31e8c31906cc1ee89fde72cbd8\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:dd9be0a42de08f4b58a3cc73a123f124f65c24698b95a54c1543065baca8cf0e\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:89b731f25c80830c76fdb13705c68fef6a2b6dc494402987c7ea9584fe189f5d\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:c6de1ec30c4bb94f3a69c9f5f2182baeda5b809f806676675e9ef6b8dc936f28\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:bb68b41c3fa64587412b104294b9cbb027509dc2f6958446c502638d481525ef\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1c3980f2843de5184656aab58698011b42763ccba11c4a8c35936c8dd6c7068c\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:94f85614f2cba13f62c3c6481716e4adeae48e1eaa7e8bac379b9d177d93947a\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:510b7fb0a86dc8f10a8bb43bd2f97beb63cffad1203071dc434dac26453955cd\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:1eba2f7ce3e30ee2170410e2171867ea73dbd692433b81a93758ab2de6c64835\"},\n+    {file = \"pydantic_core-2.23.3-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:4b259fd8409ab84b4041b7b3f24dcc41e4696f180b775961ca8142b5b21d0e70\"},\n+    {file = \"pydantic_core-2.23.3-cp39-none-win32.whl\", hash = \"sha256:40d9bd259538dba2f40963286009bf7caf18b5112b19d2b55b09c14dde6db6a7\"},\n+    {file = \"pydantic_core-2.23.3-cp39-none-win_amd64.whl\", hash = \"sha256:5a8cd3074a98ee70173a8633ad3c10e00dcb991ecec57263aacb4095c5efb958\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:f399e8657c67313476a121a6944311fab377085ca7f490648c9af97fc732732d\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:6b5547d098c76e1694ba85f05b595720d7c60d342f24d5aad32c3049131fa5c4\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0dda0290a6f608504882d9f7650975b4651ff91c85673341789a476b1159f211\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:65b6e5da855e9c55a0c67f4db8a492bf13d8d3316a59999cfbaf98cc6e401961\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:09e926397f392059ce0afdcac920df29d9c833256354d0c55f1584b0b70cf07e\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:87cfa0ed6b8c5bd6ae8b66de941cece179281239d482f363814d2b986b79cedc\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:e61328920154b6a44d98cabcb709f10e8b74276bc709c9a513a8c37a18786cc4\"},\n+    {file = \"pydantic_core-2.23.3-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:ce3317d155628301d649fe5e16a99528d5680af4ec7aa70b90b8dacd2d725c9b\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:e89513f014c6be0d17b00a9a7c81b1c426f4eb9224b15433f3d98c1a071f8433\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:4f62c1c953d7ee375df5eb2e44ad50ce2f5aff931723b398b8bc6f0ac159791a\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2718443bc671c7ac331de4eef9b673063b10af32a0bb385019ad61dcf2cc8f6c\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a0d90e08b2727c5d01af1b5ef4121d2f0c99fbee692c762f4d9d0409c9da6541\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:2b676583fc459c64146debea14ba3af54e540b61762dfc0613dc4e98c3f66eeb\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:50e4661f3337977740fdbfbae084ae5693e505ca2b3130a6d4eb0f2281dc43b8\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:68f4cf373f0de6abfe599a38307f4417c1c867ca381c03df27c873a9069cda25\"},\n+    {file = \"pydantic_core-2.23.3-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:59d52cf01854cb26c46958552a21acb10dd78a52aa34c86f284e66b209db8cab\"},\n+    {file = \"pydantic_core-2.23.3.tar.gz\", hash = \"sha256:3cb0f65d8b4121c1b015c60104a685feb929a29d7cf204387c7f2688c7974690\"},\n+]\n+\n+[package.dependencies]\n+typing-extensions = \">=4.6.0,<4.7.0 || >4.7.0\"\n+\n+[[package]]\n+name = \"pygments\"\n+version = \"2.18.0\"\n+description = \"Pygments is a syntax highlighting package written in Python.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pygments-2.18.0-py3-none-any.whl\", hash = \"sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a\"},\n+    {file = \"pygments-2.18.0.tar.gz\", hash = \"sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199\"},\n+]\n+\n+[package.extras]\n+windows-terminal = [\"colorama (>=0.4.6)\"]\n+\n+[[package]]\n+name = \"pylint\"\n+version = \"3.2.7\"\n+description = \"python code static checker\"\n+optional = false\n+python-versions = \">=3.8.0\"\n+files = [\n+    {file = \"pylint-3.2.7-py3-none-any.whl\", hash = \"sha256:02f4aedeac91be69fb3b4bea997ce580a4ac68ce58b89eaefeaf06749df73f4b\"},\n+    {file = \"pylint-3.2.7.tar.gz\", hash = \"sha256:1b7a721b575eaeaa7d39db076b6e7743c993ea44f57979127c517c6c572c803e\"},\n+]\n+\n+[package.dependencies]\n+astroid = \">=3.2.4,<=3.3.0-dev0\"\n+colorama = {version = \">=0.4.5\", markers = \"sys_platform == \\\"win32\\\"\"}\n+dill = [\n+    {version = \">=0.2\", markers = \"python_version < \\\"3.11\\\"\"},\n+    {version = \">=0.3.7\", markers = \"python_version >= \\\"3.12\\\"\"},\n+    {version = \">=0.3.6\", markers = \"python_version >= \\\"3.11\\\" and python_version < \\\"3.12\\\"\"},\n+]\n+isort = \">=4.2.5,<5.13.0 || >5.13.0,<6\"\n+mccabe = \">=0.6,<0.8\"\n+platformdirs = \">=2.2.0\"\n+tomli = {version = \">=1.1.0\", markers = \"python_version < \\\"3.11\\\"\"}\n+tomlkit = \">=0.10.1\"\n+typing-extensions = {version = \">=3.10.0\", markers = \"python_version < \\\"3.10\\\"\"}\n+\n+[package.extras]\n+spelling = [\"pyenchant (>=3.2,<4.0)\"]\n+testutils = [\"gitpython (>3)\"]\n+\n+[[package]]\n+name = \"pylint-django\"\n+version = \"2.5.5\"\n+description = \"A Pylint plugin to help Pylint understand the Django web framework\"\n+optional = false\n+python-versions = \">=3.7,<4.0\"\n+files = [\n+    {file = \"pylint_django-2.5.5-py3-none-any.whl\", hash = \"sha256:5abd5c2228e0e5e2a4cb6d0b4fc1d1cef1e773d0be911412f4dd4fc1a1a440b7\"},\n+    {file = \"pylint_django-2.5.5.tar.gz\", hash = \"sha256:2f339e4bf55776958283395c5139c37700c91bd5ef1d8251ef6ac88b5abbba9b\"},\n+]\n+\n+[package.dependencies]\n+pylint = \">=2.0,<4\"\n+pylint-plugin-utils = \">=0.8\"\n+\n+[package.extras]\n+with-django = [\"Django (>=2.2)\"]\n+\n+[[package]]\n+name = \"pylint-plugin-utils\"\n+version = \"0.8.2\"\n+description = \"Utilities and helpers for writing Pylint plugins\"\n+optional = false\n+python-versions = \">=3.7,<4.0\"\n+files = [\n+    {file = \"pylint_plugin_utils-0.8.2-py3-none-any.whl\", hash = \"sha256:ae11664737aa2effbf26f973a9e0b6779ab7106ec0adc5fe104b0907ca04e507\"},\n+    {file = \"pylint_plugin_utils-0.8.2.tar.gz\", hash = \"sha256:d3cebf68a38ba3fba23a873809155562571386d4c1b03e5b4c4cc26c3eee93e4\"},\n+]\n+\n+[package.dependencies]\n+pylint = \">=1.7\"\n+\n+[[package]]\n+name = \"pyproject-api\"\n+version = \"1.7.1\"\n+description = \"API to interact with the python pyproject.toml based projects\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pyproject_api-1.7.1-py3-none-any.whl\", hash = \"sha256:2dc1654062c2b27733d8fd4cdda672b22fe8741ef1dde8e3a998a9547b071eeb\"},\n+    {file = \"pyproject_api-1.7.1.tar.gz\", hash = \"sha256:7ebc6cd10710f89f4cf2a2731710a98abce37ebff19427116ff2174c9236a827\"},\n+]\n+\n+[package.dependencies]\n+packaging = \">=24.1\"\n+tomli = {version = \">=2.0.1\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+docs = [\"furo (>=2024.5.6)\", \"sphinx-autodoc-typehints (>=2.2.1)\"]\n+testing = [\"covdefaults (>=2.3)\", \"pytest (>=8.2.2)\", \"pytest-cov (>=5)\", \"pytest-mock (>=3.14)\", \"setuptools (>=70.1)\"]\n+\n+[[package]]\n+name = \"pytest\"\n+version = \"8.3.3\"\n+description = \"pytest: simple powerful testing with Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pytest-8.3.3-py3-none-any.whl\", hash = \"sha256:a6853c7375b2663155079443d2e45de913a911a11d669df02a50814944db57b2\"},\n+    {file = \"pytest-8.3.3.tar.gz\", hash = \"sha256:70b98107bd648308a7952b06e6ca9a50bc660be218d53c257cc1fc94fda10181\"},\n+]\n+\n+[package.dependencies]\n+colorama = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n+exceptiongroup = {version = \">=1.0.0rc8\", markers = \"python_version < \\\"3.11\\\"\"}\n+iniconfig = \"*\"\n+packaging = \"*\"\n+pluggy = \">=1.5,<2\"\n+tomli = {version = \">=1\", markers = \"python_version < \\\"3.11\\\"\"}\n+\n+[package.extras]\n+dev = [\"argcomplete\", \"attrs (>=19.2)\", \"hypothesis (>=3.56)\", \"mock\", \"pygments (>=2.7.2)\", \"requests\", \"setuptools\", \"xmlschema\"]\n+\n+[[package]]\n+name = \"pytest-cov\"\n+version = \"5.0.0\"\n+description = \"Pytest plugin for measuring coverage.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pytest-cov-5.0.0.tar.gz\", hash = \"sha256:5837b58e9f6ebd335b0f8060eecce69b662415b16dc503883a02f45dfeb14857\"},\n+    {file = \"pytest_cov-5.0.0-py3-none-any.whl\", hash = \"sha256:4f0764a1219df53214206bf1feea4633c3b558a2925c8b59f144f682861ce652\"},\n+]\n+\n+[package.dependencies]\n+coverage = {version = \">=5.2.1\", extras = [\"toml\"]}\n+pytest = \">=4.6\"\n+\n+[package.extras]\n+testing = [\"fields\", \"hunter\", \"process-tests\", \"pytest-xdist\", \"virtualenv\"]\n+\n+[[package]]\n+name = \"pytest-django\"\n+version = \"4.9.0\"\n+description = \"A Django plugin for pytest.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"pytest_django-4.9.0-py3-none-any.whl\", hash = \"sha256:1d83692cb39188682dbb419ff0393867e9904094a549a7d38a3154d5731b2b99\"},\n+    {file = \"pytest_django-4.9.0.tar.gz\", hash = \"sha256:8bf7bc358c9ae6f6fc51b6cebb190fe20212196e6807121f11bd6a3b03428314\"},\n+]\n+\n+[package.dependencies]\n+pytest = \">=7.0.0\"\n+\n+[package.extras]\n+docs = [\"sphinx\", \"sphinx-rtd-theme\"]\n+testing = [\"Django\", \"django-configurations (>=2.0)\"]\n+\n+[[package]]\n+name = \"python-gitlab\"\n+version = \"4.11.1\"\n+description = \"A python wrapper for the GitLab API\"\n+optional = false\n+python-versions = \">=3.8.0\"\n+files = [\n+    {file = \"python_gitlab-4.11.1-py3-none-any.whl\", hash = \"sha256:f15fe4f4cbaa58c04e422ad30a2d4fc7976294dfdf1f90f81ab927e5b2238f33\"},\n+    {file = \"python_gitlab-4.11.1.tar.gz\", hash = \"sha256:7afa2f9c30618bc3daee95d2186a06e1cf18ca2fa97890eb55fd8ddc4e339812\"},\n+]\n+\n+[package.dependencies]\n+requests = \">=2.32.0\"\n+requests-toolbelt = \">=1.0.0\"\n+\n+[package.extras]\n+autocompletion = [\"argcomplete (>=1.10.0,<3)\"]\n+graphql = [\"gql[httpx] (>=3.5.0,<4)\"]\n+yaml = [\"PyYaml (>=6.0.1)\"]\n+\n+[[package]]\n+name = \"python-semantic-release\"\n+version = \"9.8.8\"\n+description = \"Automatic Semantic Versioning for Python projects\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"python_semantic_release-9.8.8-py3-none-any.whl\", hash = \"sha256:df43d02234ce4be3802bceca804c3a357aebf833a97624dc74563ec69da8a3ef\"},\n+    {file = \"python_semantic_release-9.8.8.tar.gz\", hash = \"sha256:19ac268bc417b2b8ea869caede307c50bc1fc2c6c70ee42e88403e321d13f494\"},\n+]\n+\n+[package.dependencies]\n+click = \">=8.0,<9.0\"\n+click-option-group = \">=0.5,<1.0\"\n+dotty-dict = \">=1.3,<2.0\"\n+gitpython = \">=3.0,<4.0\"\n+importlib-resources = \">=6.0,<7.0\"\n+jinja2 = \">=3.1,<4.0\"\n+pydantic = \">=2.0,<3.0\"\n+python-gitlab = \">=4.0,<5.0\"\n+requests = \">=2.25,<3.0\"\n+rich = \">=13.0,<14.0\"\n+shellingham = \">=1.5,<2.0\"\n+tomlkit = \">=0.11,<1.0\"\n+\n+[package.extras]\n+build = [\"build (>=1.2,<2.0)\"]\n+dev = [\"pre-commit (>=3.5,<4.0)\", \"ruff (==0.5.0)\", \"tox (>=4.11,<5.0)\"]\n+docs = [\"Sphinx (>=6.0,<7.0)\", \"furo (>=2024.1,<2025.0)\", \"sphinx-autobuild (==2024.2.4)\", \"sphinxcontrib-apidoc (==0.5.0)\"]\n+mypy = [\"mypy (==1.10.1)\", \"types-requests (>=2.32.0,<2.33.0)\"]\n+test = [\"coverage[toml] (>=7.0,<8.0)\", \"pytest (>=8.3,<9.0)\", \"pytest-clarity (>=1.0,<2.0)\", \"pytest-cov (>=5.0,<6.0)\", \"pytest-env (>=1.0,<2.0)\", \"pytest-lazy-fixtures (>=1.1.1,<1.2.0)\", \"pytest-mock (>=3.0,<4.0)\", \"pytest-pretty (>=1.2,<2.0)\", \"pytest-xdist (>=3.0,<4.0)\", \"requests-mock (>=1.10,<2.0)\", \"responses (>=0.25.0,<0.26.0)\"]\n+\n+[[package]]\n+name = \"pytz\"\n+version = \"2024.2\"\n+description = \"World timezone definitions, modern and historical\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"pytz-2024.2-py2.py3-none-any.whl\", hash = \"sha256:31c7c1817eb7fae7ca4b8c7ee50c72f93aa2dd863de768e1ef4245d426aa0725\"},\n+    {file = \"pytz-2024.2.tar.gz\", hash = \"sha256:2aa355083c50a0f93fa581709deac0c9ad65cca8a9e9beac660adcbd493c798a\"},\n+]\n+\n+[[package]]\n+name = \"pyyaml\"\n+version = \"6.0.2\"\n+description = \"YAML parser and emitter for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:0a9a2848a5b7feac301353437eb7d5957887edbf81d56e903999a75a3d743086\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:29717114e51c84ddfba879543fb232a6ed60086602313ca38cce623c1d62cfbf\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:8824b5a04a04a047e72eea5cec3bc266db09e35de6bdfe34c9436ac5ee27d237\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:7c36280e6fb8385e520936c3cb3b8042851904eba0e58d277dca80a5cfed590b\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ec031d5d2feb36d1d1a24380e4db6d43695f3748343d99434e6f5f9156aaa2ed\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:936d68689298c36b53b29f23c6dbb74de12b4ac12ca6cfe0e047bedceea56180\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:23502f431948090f597378482b4812b0caae32c22213aecf3b55325e049a6c68\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-win32.whl\", hash = \"sha256:2e99c6826ffa974fe6e27cdb5ed0021786b03fc98e5ee3c5bfe1fd5015f42b99\"},\n+    {file = \"PyYAML-6.0.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:a4d3091415f010369ae4ed1fc6b79def9416358877534caf6a0fdd2146c87a3e\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:cc1c1159b3d456576af7a3e4d1ba7e6924cb39de8f67111c735f6fc832082774\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:1e2120ef853f59c7419231f3bf4e7021f1b936f6ebd222406c3b60212205d2ee\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5d225db5a45f21e78dd9358e58a98702a0302f2659a3c6cd320564b75b86f47c\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5ac9328ec4831237bec75defaf839f7d4564be1e6b25ac710bd1a96321cc8317\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3ad2a3decf9aaba3d29c8f537ac4b243e36bef957511b4766cb0057d32b0be85\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:ff3824dc5261f50c9b0dfb3be22b4567a6f938ccce4587b38952d85fd9e9afe4\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:797b4f722ffa07cc8d62053e4cff1486fa6dc094105d13fea7b1de7d8bf71c9e\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-win32.whl\", hash = \"sha256:11d8f3dd2b9c1207dcaf2ee0bbbfd5991f571186ec9cc78427ba5bd32afae4b5\"},\n+    {file = \"PyYAML-6.0.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:e10ce637b18caea04431ce14fabcf5c64a1c61ec9c56b071a4b7ca131ca52d44\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-win32.whl\", hash = \"sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4\"},\n+    {file = \"PyYAML-6.0.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-win32.whl\", hash = \"sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183\"},\n+    {file = \"PyYAML-6.0.2-cp313-cp313-win_amd64.whl\", hash = \"sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:24471b829b3bf607e04e88d79542a9d48bb037c2267d7927a874e6c205ca7e9a\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d7fded462629cfa4b685c5416b949ebad6cec74af5e2d42905d41e257e0869f5\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d84a1718ee396f54f3a086ea0a66d8e552b2ab2017ef8b420e92edbc841c352d\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9056c1ecd25795207ad294bcf39f2db3d845767be0ea6e6a34d856f006006083\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:82d09873e40955485746739bcb8b4586983670466c23382c19cffecbf1fd8706\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-win32.whl\", hash = \"sha256:43fa96a3ca0d6b1812e01ced1044a003533c47f6ee8aca31724f78e93ccc089a\"},\n+    {file = \"PyYAML-6.0.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:01179a4a8559ab5de078078f37e5c1a30d76bb88519906844fd7bdea1b7729ff\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:688ba32a1cffef67fd2e9398a2efebaea461578b0923624778664cc1c914db5d\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:a8786accb172bd8afb8be14490a16625cbc387036876ab6ba70912730faf8e1f\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d8e03406cac8513435335dbab54c0d385e4a49e4945d2909a581c83647ca0290\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f753120cb8181e736c57ef7636e83f31b9c0d1722c516f7e86cf15b7aa57ff12\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3b1fdb9dc17f5a7677423d508ab4f243a726dea51fa5e70992e59a7411c89d19\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:0b69e4ce7a131fe56b7e4d770c67429700908fc0752af059838b1cfb41960e4e\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:a9f8c2e67970f13b16084e04f134610fd1d374bf477b17ec1599185cf611d725\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-win32.whl\", hash = \"sha256:6395c297d42274772abc367baaa79683958044e5d3835486c16da75d2a694631\"},\n+    {file = \"PyYAML-6.0.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:39693e1f8320ae4f43943590b49779ffb98acb81f788220ea932a6b6c51004d8\"},\n+    {file = \"pyyaml-6.0.2.tar.gz\", hash = \"sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e\"},\n+]\n+\n+[[package]]\n+name = \"questionary\"\n+version = \"2.0.1\"\n+description = \"Python library to build pretty command line user prompts \u2b50\ufe0f\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"questionary-2.0.1-py3-none-any.whl\", hash = \"sha256:8ab9a01d0b91b68444dff7f6652c1e754105533f083cbe27597c8110ecc230a2\"},\n+    {file = \"questionary-2.0.1.tar.gz\", hash = \"sha256:bcce898bf3dbb446ff62830c86c5c6fb9a22a54146f0f5597d3da43b10d8fc8b\"},\n+]\n+\n+[package.dependencies]\n+prompt_toolkit = \">=2.0,<=3.0.36\"\n+\n+[[package]]\n+name = \"requests\"\n+version = \"2.32.3\"\n+description = \"Python HTTP for Humans.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"requests-2.32.3-py3-none-any.whl\", hash = \"sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6\"},\n+    {file = \"requests-2.32.3.tar.gz\", hash = \"sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760\"},\n+]\n+\n+[package.dependencies]\n+certifi = \">=2017.4.17\"\n+charset-normalizer = \">=2,<4\"\n+idna = \">=2.5,<4\"\n+urllib3 = \">=1.21.1,<3\"\n+\n+[package.extras]\n+socks = [\"PySocks (>=1.5.6,!=1.5.7)\"]\n+use-chardet-on-py3 = [\"chardet (>=3.0.2,<6)\"]\n+\n+[[package]]\n+name = \"requests-toolbelt\"\n+version = \"1.0.0\"\n+description = \"A utility belt for advanced users of python-requests\"\n+optional = false\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+files = [\n+    {file = \"requests-toolbelt-1.0.0.tar.gz\", hash = \"sha256:7681a0a3d047012b5bdc0ee37d7f8f07ebe76ab08caeccfc3921ce23c88d5bc6\"},\n+    {file = \"requests_toolbelt-1.0.0-py2.py3-none-any.whl\", hash = \"sha256:cccfdd665f0a24fcf4726e690f65639d272bb0637b9b92dfd91a5568ccf6bd06\"},\n+]\n+\n+[package.dependencies]\n+requests = \">=2.0.1,<3.0.0\"\n+\n+[[package]]\n+name = \"rich\"\n+version = \"13.8.1\"\n+description = \"Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal\"\n+optional = false\n+python-versions = \">=3.7.0\"\n+files = [\n+    {file = \"rich-13.8.1-py3-none-any.whl\", hash = \"sha256:1760a3c0848469b97b558fc61c85233e3dafb69c7a071b4d60c38099d3cd4c06\"},\n+    {file = \"rich-13.8.1.tar.gz\", hash = \"sha256:8260cda28e3db6bf04d2d1ef4dbc03ba80a824c88b0e7668a0f23126a424844a\"},\n+]\n+\n+[package.dependencies]\n+markdown-it-py = \">=2.2.0\"\n+pygments = \">=2.13.0,<3.0.0\"\n+typing-extensions = {version = \">=4.0.0,<5.0\", markers = \"python_version < \\\"3.9\\\"\"}\n+\n+[package.extras]\n+jupyter = [\"ipywidgets (>=7.5.1,<9)\"]\n+\n+[[package]]\n+name = \"shellingham\"\n+version = \"1.5.4\"\n+description = \"Tool to Detect Surrounding Shell\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"shellingham-1.5.4-py2.py3-none-any.whl\", hash = \"sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686\"},\n+    {file = \"shellingham-1.5.4.tar.gz\", hash = \"sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de\"},\n+]\n+\n+[[package]]\n+name = \"smmap\"\n+version = \"5.0.1\"\n+description = \"A pure Python implementation of a sliding window memory map manager\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"smmap-5.0.1-py3-none-any.whl\", hash = \"sha256:e6d8668fa5f93e706934a62d7b4db19c8d9eb8cf2adbb75ef1b675aa332b69da\"},\n+    {file = \"smmap-5.0.1.tar.gz\", hash = \"sha256:dceeb6c0028fdb6734471eb07c0cd2aae706ccaecab45965ee83f11c8d3b1f62\"},\n+]\n+\n+[[package]]\n+name = \"snowballstemmer\"\n+version = \"2.2.0\"\n+description = \"This package provides 29 stemmers for 28 languages generated from Snowball algorithms.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"snowballstemmer-2.2.0-py2.py3-none-any.whl\", hash = \"sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a\"},\n+    {file = \"snowballstemmer-2.2.0.tar.gz\", hash = \"sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1\"},\n+]\n+\n+[[package]]\n+name = \"sphinx\"\n+version = \"6.2.1\"\n+description = \"Python documentation generator\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"Sphinx-6.2.1.tar.gz\", hash = \"sha256:6d56a34697bb749ffa0152feafc4b19836c755d90a7c59b72bc7dfd371b9cc6b\"},\n+    {file = \"sphinx-6.2.1-py3-none-any.whl\", hash = \"sha256:97787ff1fa3256a3eef9eda523a63dbf299f7b47e053cfcf684a1c2a8380c912\"},\n+]\n+\n+[package.dependencies]\n+alabaster = \">=0.7,<0.8\"\n+babel = \">=2.9\"\n+colorama = {version = \">=0.4.5\", markers = \"sys_platform == \\\"win32\\\"\"}\n+docutils = \">=0.18.1,<0.20\"\n+imagesize = \">=1.3\"\n+importlib-metadata = {version = \">=4.8\", markers = \"python_version < \\\"3.10\\\"\"}\n+Jinja2 = \">=3.0\"\n+packaging = \">=21.0\"\n+Pygments = \">=2.13\"\n+requests = \">=2.25.0\"\n+snowballstemmer = \">=2.0\"\n+sphinxcontrib-applehelp = \"*\"\n+sphinxcontrib-devhelp = \"*\"\n+sphinxcontrib-htmlhelp = \">=2.0.0\"\n+sphinxcontrib-jsmath = \"*\"\n+sphinxcontrib-qthelp = \"*\"\n+sphinxcontrib-serializinghtml = \">=1.1.5\"\n+\n+[package.extras]\n+docs = [\"sphinxcontrib-websupport\"]\n+lint = [\"docutils-stubs\", \"flake8 (>=3.5.0)\", \"flake8-simplify\", \"isort\", \"mypy (>=0.990)\", \"ruff\", \"sphinx-lint\", \"types-requests\"]\n+test = [\"cython\", \"filelock\", \"html5lib\", \"pytest (>=4.6)\"]\n+\n+[[package]]\n+name = \"sphinx-rtd-theme\"\n+version = \"2.0.0\"\n+description = \"Read the Docs theme for Sphinx\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl\", hash = \"sha256:ec93d0856dc280cf3aee9a4c9807c60e027c7f7b461b77aeffed682e68f0e586\"},\n+    {file = \"sphinx_rtd_theme-2.0.0.tar.gz\", hash = \"sha256:bd5d7b80622406762073a04ef8fadc5f9151261563d47027de09910ce03afe6b\"},\n+]\n+\n+[package.dependencies]\n+docutils = \"<0.21\"\n+sphinx = \">=5,<8\"\n+sphinxcontrib-jquery = \">=4,<5\"\n+\n+[package.extras]\n+dev = [\"bump2version\", \"sphinxcontrib-httpdomain\", \"transifex-client\", \"wheel\"]\n+\n+[[package]]\n+name = \"sphinxcontrib-applehelp\"\n+version = \"1.0.4\"\n+description = \"sphinxcontrib-applehelp is a Sphinx extension which outputs Apple help books\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"sphinxcontrib-applehelp-1.0.4.tar.gz\", hash = \"sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e\"},\n+    {file = \"sphinxcontrib_applehelp-1.0.4-py3-none-any.whl\", hash = \"sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228\"},\n+]\n+\n+[package.extras]\n+lint = [\"docutils-stubs\", \"flake8\", \"mypy\"]\n+test = [\"pytest\"]\n+\n+[[package]]\n+name = \"sphinxcontrib-devhelp\"\n+version = \"1.0.2\"\n+description = \"sphinxcontrib-devhelp is a sphinx extension which outputs Devhelp document.\"\n+optional = false\n+python-versions = \">=3.5\"\n+files = [\n+    {file = \"sphinxcontrib-devhelp-1.0.2.tar.gz\", hash = \"sha256:ff7f1afa7b9642e7060379360a67e9c41e8f3121f2ce9164266f61b9f4b338e4\"},\n+    {file = \"sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl\", hash = \"sha256:8165223f9a335cc1af7ffe1ed31d2871f325254c0423bc0c4c7cd1c1e4734a2e\"},\n+]\n+\n+[package.extras]\n+lint = [\"docutils-stubs\", \"flake8\", \"mypy\"]\n+test = [\"pytest\"]\n+\n+[[package]]\n+name = \"sphinxcontrib-htmlhelp\"\n+version = \"2.0.1\"\n+description = \"sphinxcontrib-htmlhelp is a sphinx extension which renders HTML help files\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"sphinxcontrib-htmlhelp-2.0.1.tar.gz\", hash = \"sha256:0cbdd302815330058422b98a113195c9249825d681e18f11e8b1f78a2f11efff\"},\n+    {file = \"sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl\", hash = \"sha256:c38cb46dccf316c79de6e5515e1770414b797162b23cd3d06e67020e1d2a6903\"},\n+]\n+\n+[package.extras]\n+lint = [\"docutils-stubs\", \"flake8\", \"mypy\"]\n+test = [\"html5lib\", \"pytest\"]\n+\n+[[package]]\n+name = \"sphinxcontrib-jquery\"\n+version = \"4.1\"\n+description = \"Extension to include jQuery on newer Sphinx releases\"\n+optional = false\n+python-versions = \">=2.7\"\n+files = [\n+    {file = \"sphinxcontrib-jquery-4.1.tar.gz\", hash = \"sha256:1620739f04e36a2c779f1a131a2dfd49b2fd07351bf1968ced074365933abc7a\"},\n+    {file = \"sphinxcontrib_jquery-4.1-py2.py3-none-any.whl\", hash = \"sha256:f936030d7d0147dd026a4f2b5a57343d233f1fc7b363f68b3d4f1cb0993878ae\"},\n+]\n+\n+[package.dependencies]\n+Sphinx = \">=1.8\"\n+\n+[[package]]\n+name = \"sphinxcontrib-jsmath\"\n+version = \"1.0.1\"\n+description = \"A sphinx extension which renders display math in HTML via JavaScript\"\n+optional = false\n+python-versions = \">=3.5\"\n+files = [\n+    {file = \"sphinxcontrib-jsmath-1.0.1.tar.gz\", hash = \"sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8\"},\n+    {file = \"sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl\", hash = \"sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178\"},\n+]\n+\n+[package.extras]\n+test = [\"flake8\", \"mypy\", \"pytest\"]\n+\n+[[package]]\n+name = \"sphinxcontrib-qthelp\"\n+version = \"1.0.3\"\n+description = \"sphinxcontrib-qthelp is a sphinx extension which outputs QtHelp document.\"\n+optional = false\n+python-versions = \">=3.5\"\n+files = [\n+    {file = \"sphinxcontrib-qthelp-1.0.3.tar.gz\", hash = \"sha256:4c33767ee058b70dba89a6fc5c1892c0d57a54be67ddd3e7875a18d14cba5a72\"},\n+    {file = \"sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl\", hash = \"sha256:bd9fc24bcb748a8d51fd4ecaade681350aa63009a347a8c14e637895444dfab6\"},\n+]\n+\n+[package.extras]\n+lint = [\"docutils-stubs\", \"flake8\", \"mypy\"]\n+test = [\"pytest\"]\n+\n+[[package]]\n+name = \"sphinxcontrib-serializinghtml\"\n+version = \"1.1.5\"\n+description = \"sphinxcontrib-serializinghtml is a sphinx extension which outputs \\\"serialized\\\" HTML files (json and pickle).\"\n+optional = false\n+python-versions = \">=3.5\"\n+files = [\n+    {file = \"sphinxcontrib-serializinghtml-1.1.5.tar.gz\", hash = \"sha256:aa5f6de5dfdf809ef505c4895e51ef5c9eac17d0f287933eb49ec495280b6952\"},\n+    {file = \"sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl\", hash = \"sha256:352a9a00ae864471d3a7ead8d7d79f5fc0b57e8b3f95e9867eb9eb28999b92fd\"},\n+]\n+\n+[package.extras]\n+lint = [\"docutils-stubs\", \"flake8\", \"mypy\"]\n+test = [\"pytest\"]\n+\n+[[package]]\n+name = \"sqlparse\"\n+version = \"0.5.1\"\n+description = \"A non-validating SQL parser.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"sqlparse-0.5.1-py3-none-any.whl\", hash = \"sha256:773dcbf9a5ab44a090f3441e2180efe2560220203dc2f8c0b0fa141e18b505e4\"},\n+    {file = \"sqlparse-0.5.1.tar.gz\", hash = \"sha256:bb6b4df465655ef332548e24f08e205afc81b9ab86cb1c45657a7ff173a3a00e\"},\n+]\n+\n+[package.extras]\n+dev = [\"build\", \"hatch\"]\n+doc = [\"sphinx\"]\n+\n+[[package]]\n+name = \"stevedore\"\n+version = \"5.3.0\"\n+description = \"Manage dynamic plugins for Python applications\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"stevedore-5.3.0-py3-none-any.whl\", hash = \"sha256:1efd34ca08f474dad08d9b19e934a22c68bb6fe416926479ba29e5013bcc8f78\"},\n+    {file = \"stevedore-5.3.0.tar.gz\", hash = \"sha256:9a64265f4060312828151c204efbe9b7a9852a0d9228756344dbc7e4023e375a\"},\n+]\n+\n+[package.dependencies]\n+pbr = \">=2.0.0\"\n+\n+[[package]]\n+name = \"termcolor\"\n+version = \"2.4.0\"\n+description = \"ANSI color formatting for output in terminal\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"termcolor-2.4.0-py3-none-any.whl\", hash = \"sha256:9297c0df9c99445c2412e832e882a7884038a25617c60cea2ad69488d4040d63\"},\n+    {file = \"termcolor-2.4.0.tar.gz\", hash = \"sha256:aab9e56047c8ac41ed798fa36d892a37aca6b3e9159f3e0c24bc64a9b3ac7b7a\"},\n+]\n+\n+[package.extras]\n+tests = [\"pytest\", \"pytest-cov\"]\n+\n+[[package]]\n+name = \"tomli\"\n+version = \"2.0.1\"\n+description = \"A lil' TOML parser\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"tomli-2.0.1-py3-none-any.whl\", hash = \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\"},\n+    {file = \"tomli-2.0.1.tar.gz\", hash = \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"},\n+]\n+\n+[[package]]\n+name = \"tomlkit\"\n+version = \"0.13.2\"\n+description = \"Style preserving TOML library\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"tomlkit-0.13.2-py3-none-any.whl\", hash = \"sha256:7a974427f6e119197f670fbbbeae7bef749a6c14e793db934baefc1b5f03efde\"},\n+    {file = \"tomlkit-0.13.2.tar.gz\", hash = \"sha256:fff5fe59a87295b278abd31bec92c15d9bc4a06885ab12bcea52c71119392e79\"},\n+]\n+\n+[[package]]\n+name = \"tox\"\n+version = \"4.18.1\"\n+description = \"tox is a generic virtualenv management and test command line tool\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"tox-4.18.1-py3-none-any.whl\", hash = \"sha256:35d472032ee1f73fe20c3e0e73d7073a4e85075c86ff02c576f9fc7c6a15a578\"},\n+    {file = \"tox-4.18.1.tar.gz\", hash = \"sha256:3c0c96bc3a568a5c7e66387a4cfcf8c875b52e09f4d47c9f7a277ec82f1a0b11\"},\n+]\n+\n+[package.dependencies]\n+cachetools = \">=5.5\"\n+chardet = \">=5.2\"\n+colorama = \">=0.4.6\"\n+filelock = \">=3.15.4\"\n+packaging = \">=24.1\"\n+platformdirs = \">=4.2.2\"\n+pluggy = \">=1.5\"\n+pyproject-api = \">=1.7.1\"\n+tomli = {version = \">=2.0.1\", markers = \"python_version < \\\"3.11\\\"\"}\n+virtualenv = \">=20.26.3\"\n+\n+[package.extras]\n+docs = [\"furo (>=2024.8.6)\", \"sphinx (>=8.0.2)\", \"sphinx-argparse-cli (>=1.17)\", \"sphinx-autodoc-typehints (>=2.4)\", \"sphinx-copybutton (>=0.5.2)\", \"sphinx-inline-tabs (>=2023.4.21)\", \"sphinxcontrib-towncrier (>=0.2.1a0)\", \"towncrier (>=24.8)\"]\n+testing = [\"build[virtualenv] (>=1.2.2)\", \"covdefaults (>=2.3)\", \"detect-test-pollution (>=1.2)\", \"devpi-process (>=1)\", \"diff-cover (>=9.1.1)\", \"distlib (>=0.3.8)\", \"flaky (>=3.8.1)\", \"hatch-vcs (>=0.4)\", \"hatchling (>=1.25)\", \"psutil (>=6)\", \"pytest (>=8.3.2)\", \"pytest-cov (>=5)\", \"pytest-mock (>=3.14)\", \"pytest-xdist (>=3.6.1)\", \"re-assert (>=1.1)\", \"setuptools (>=74.1.2)\", \"time-machine (>=2.15)\", \"wheel (>=0.44)\"]\n+\n+[[package]]\n+name = \"types-pyyaml\"\n+version = \"6.0.12.20240808\"\n+description = \"Typing stubs for PyYAML\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"types-PyYAML-6.0.12.20240808.tar.gz\", hash = \"sha256:b8f76ddbd7f65440a8bda5526a9607e4c7a322dc2f8e1a8c405644f9a6f4b9af\"},\n+    {file = \"types_PyYAML-6.0.12.20240808-py3-none-any.whl\", hash = \"sha256:deda34c5c655265fc517b546c902aa6eed2ef8d3e921e4765fe606fe2afe8d35\"},\n+]\n+\n+[[package]]\n+name = \"typing-extensions\"\n+version = \"4.12.2\"\n+description = \"Backported and Experimental Type Hints for Python 3.8+\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"typing_extensions-4.12.2-py3-none-any.whl\", hash = \"sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d\"},\n+    {file = \"typing_extensions-4.12.2.tar.gz\", hash = \"sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8\"},\n+]\n+\n+[[package]]\n+name = \"tzdata\"\n+version = \"2024.1\"\n+description = \"Provider of IANA time zone data\"\n+optional = false\n+python-versions = \">=2\"\n+files = [\n+    {file = \"tzdata-2024.1-py2.py3-none-any.whl\", hash = \"sha256:9068bc196136463f5245e51efda838afa15aaeca9903f49050dfa2679db4d252\"},\n+    {file = \"tzdata-2024.1.tar.gz\", hash = \"sha256:2674120f8d891909751c38abcdfd386ac0a5a1127954fbc332af6b5ceae07efd\"},\n+]\n+\n+[[package]]\n+name = \"untokenize\"\n+version = \"0.1.1\"\n+description = \"Transforms tokens into original source code (while preserving whitespace).\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"untokenize-0.1.1.tar.gz\", hash = \"sha256:3865dbbbb8efb4bb5eaa72f1be7f3e0be00ea8b7f125c69cbd1f5fda926f37a2\"},\n+]\n+\n+[[package]]\n+name = \"urllib3\"\n+version = \"2.2.3\"\n+description = \"HTTP library with thread-safe connection pooling, file post, and more.\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"urllib3-2.2.3-py3-none-any.whl\", hash = \"sha256:ca899ca043dcb1bafa3e262d73aa25c465bfb49e0bd9dd5d59f1d0acba2f8fac\"},\n+    {file = \"urllib3-2.2.3.tar.gz\", hash = \"sha256:e7d814a81dad81e6caf2ec9fdedb284ecc9c73076b62654547cc64ccdcae26e9\"},\n+]\n+\n+[package.extras]\n+brotli = [\"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\"]\n+h2 = [\"h2 (>=4,<5)\"]\n+socks = [\"pysocks (>=1.5.6,!=1.5.7,<2.0)\"]\n+zstd = [\"zstandard (>=0.18.0)\"]\n+\n+[[package]]\n+name = \"virtualenv\"\n+version = \"20.26.4\"\n+description = \"Virtual Python Environment builder\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"virtualenv-20.26.4-py3-none-any.whl\", hash = \"sha256:48f2695d9809277003f30776d155615ffc11328e6a0a8c1f0ec80188d7874a55\"},\n+    {file = \"virtualenv-20.26.4.tar.gz\", hash = \"sha256:c17f4e0f3e6036e9f26700446f85c76ab11df65ff6d8a9cbfad9f71aabfcf23c\"},\n+]\n+\n+[package.dependencies]\n+distlib = \">=0.3.7,<1\"\n+filelock = \">=3.12.2,<4\"\n+platformdirs = \">=3.9.1,<5\"\n+\n+[package.extras]\n+docs = [\"furo (>=2023.7.26)\", \"proselint (>=0.13)\", \"sphinx (>=7.1.2,!=7.3)\", \"sphinx-argparse (>=0.4)\", \"sphinxcontrib-towncrier (>=0.2.1a0)\", \"towncrier (>=23.6)\"]\n+test = [\"covdefaults (>=2.3)\", \"coverage (>=7.2.7)\", \"coverage-enable-subprocess (>=1)\", \"flaky (>=3.7)\", \"packaging (>=23.1)\", \"pytest (>=7.4)\", \"pytest-env (>=0.8.2)\", \"pytest-freezer (>=0.4.8)\", \"pytest-mock (>=3.11.1)\", \"pytest-randomly (>=3.12)\", \"pytest-timeout (>=2.1)\", \"setuptools (>=68)\", \"time-machine (>=2.10)\"]\n+\n+[[package]]\n+name = \"wcwidth\"\n+version = \"0.2.13\"\n+description = \"Measures the displayed width of unicode strings in a terminal\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"wcwidth-0.2.13-py2.py3-none-any.whl\", hash = \"sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859\"},\n+    {file = \"wcwidth-0.2.13.tar.gz\", hash = \"sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5\"},\n+]\n+\n+[[package]]\n+name = \"zipp\"\n+version = \"3.20.2\"\n+description = \"Backport of pathlib-compatible object wrapper for zip files\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"zipp-3.20.2-py3-none-any.whl\", hash = \"sha256:a817ac80d6cf4b23bf7f2828b7cabf326f15a001bea8b1f9b49631780ba28350\"},\n+    {file = \"zipp-3.20.2.tar.gz\", hash = \"sha256:bc9eb26f4506fda01b81bcde0ca78103b6e62f991b381fec825435c836edbc29\"},\n+]\n+\n+[package.extras]\n+check = [\"pytest-checkdocs (>=2.4)\", \"pytest-ruff (>=0.2.1)\"]\n+cover = [\"pytest-cov\"]\n+doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-lint\"]\n+enabler = [\"pytest-enabler (>=2.2)\"]\n+test = [\"big-O\", \"importlib-resources\", \"jaraco.functools\", \"jaraco.itertools\", \"jaraco.test\", \"more-itertools\", \"pytest (>=6,!=8.1.*)\", \"pytest-ignore-flaky\"]\n+type = [\"pytest-mypy\"]\n \n [metadata]\n lock-version = \"2.0\"\n python-versions = \">=3.8,<4.0\"\n-content-hash = \"4ac630389ce948fa5ca7cf671b57c0067b7918c937cb4c60b2991557c16ba864\"\n+content-hash = \"6ff61355ba75be473fe852cd6b929a15842525574c4186cc6242a68c268cae70\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 0db2e6d..c828e21 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,15 +1,325 @@\n+[build-system]\n+build-backend = \"poetry.core.masonry.api\"\n+requires = [ \"poetry-core\" ]\n+\n [tool.poetry]\n name = \"dj-notification-api\"\n-version = \"0.1.0\"\n-description = \"dj-notification-api is a Django package developed by Lazarus for efficiently handling notifications through various APIs\"\n-authors = [\"ARYAN-NIKNEZHAD <aryan513966@gmail.com>\", \"MEHRSHAD-MIRSHEKARY <mehrshad_mirshekary@email.com>\"]\n+version = \"1.0.0\"\n+description = \"A Django package developed by Lazarus for efficiently handling notifications through various APIs\"\n+authors = [ \"ARYAN-NIKNEZHAD <aryan513966@gmail.com>\", \"MEHRSHAD-MIRSHEKARY <mehrshad_mirshekary@email.com>\" ]\n license = \"MIT\"\n readme = \"README.md\"\n+keywords = [ \"django_notification\", \"django\", \"notification\", \"django-notification\" ]\n+\n+classifiers = [\n+  \"Development Status :: 3 - Alpha\",\n+  \"Environment :: Web Environment\",\n+  \"Framework :: Django\",\n+  \"Framework :: Django :: 4.2\",\n+  \"Framework :: Django :: 5.0\",\n+  \"Framework :: Django :: 5.1\",\n+  \"Intended Audience :: Developers\",\n+  \"License :: OSI Approved :: MIT License\",\n+  \"Operating System :: OS Independent\",\n+  \"Programming Language :: Python\",\n+  \"Programming Language :: Python :: 3\",\n+  \"Programming Language :: Python :: 3 :: Only\",\n+  \"Programming Language :: Python :: 3.8\",\n+  \"Programming Language :: Python :: 3.9\",\n+  \"Programming Language :: Python :: 3.10\",\n+  \"Programming Language :: Python :: 3.11\",\n+  \"Programming Language :: Python :: 3.12\",\n+  \"Topic :: Software Development :: Libraries :: Python Modules\",\n+]\n+\n+packages = [\n+  { include = \"django_notification\" },\n+]\n+\n+[tool.poetry.urls]\n+\"Documentation\" = \"https://django-notification.readthedocs.io\"\n+\"Source Code\" = \"https://github.com/Lazarus-org/dj-notification-api\"\n+\"Changelog\" = \"https://github.com/lazarus-org/dj-notification-api/blob/main/CHANGELOG.md\"\n+\"Issues\" = \"https://github.com/lazarus-org/dj-notification-api/issues\"\n \n [tool.poetry.dependencies]\n python = \">=3.8,<4.0\"\n+django = [\n+  { version = \">=4.2,<5.0\", python = \">=3.8,<3.10\" },\n+  { version = \">=4.2,<5.3\", python = \">=3.10\" },      # Django 4.2 and 5.x for Python 3.10+\n+]\n+djangorestframework = \"^3.15.2\"\n+django-filter = \"^24.3\"\n \n+[tool.poetry.group.dev.dependencies]\n+pytest = \"^8.3.2\"\n+pytest-django = \"^4.8.0\"\n+pytest-cov = \"^5.0.0\"\n+pylint = \"^3.2.6\"\n+pylint-django = \"^2.5.5\"\n+mypy = \"^1.11.1\"\n+isort = \"^5.13.2\"\n+black = \"^24.4.2\"\n+commitizen = \"^3.28.0\"\n+pre-commit = \"^3.5.0\"\n+bandit = { extras = [ \"toml\" ], version = \"^1.7.9\" }\n+tox = \"^4.16.0\"\n+django-stubs = \"^5.0.4\"\n+sphinx = \"^6.2.1\"\n+sphinx-rtd-theme = \"^2.0.0\"\n+docformatter = \"^1.7.5\"\n+codecov = \"^2.1.13\"\n+python-semantic-release = \"^9.8.8\"\n \n-[build-system]\n-requires = [\"poetry-core\"]\n-build-backend = \"poetry.core.masonry.api\"\n+[tool.black]\n+line-length = 88\n+exclude = '''\n+/(\n+    \\.git\n+  | \\.venv\n+  | \\.tox\n+  | build\n+  | dist\n+  | migrations\n+  | venv\n+  | env\n+  | __pycache__\n+  | node_modules\n+  | env\n+  | kernel\n+  | \\.mypy_cache\n+  | \\.pytest_cache\n+  | .*\\.egg-info\n+)/\n+'''\n+\n+[tool.isort]\n+profile = \"black\"\n+line_length = 88\n+skip = [\n+  \"venv\",\n+  \".venv\",\n+  \".tox\",\n+  \"build\",\n+  \"dist\",\n+  \".git\",\n+  \"__pycache__\",\n+  \"*.egg-info\",\n+  \".mypy_cache\",\n+  \".pytest_cache\",\n+  \"migrations\",\n+  \"node_modules\",\n+  \"env\",\n+  \"kernel\",\n+]\n+\n+[tool.pylint]\n+disable = [\n+  \"C0103\", # Invalid constant name\n+  \"C0114\", # Missing module docstring\n+  \"C0115\", # Missing class docstring\n+  \"C0116\", # Missing function or method docstring\n+  \"E1101\", # Instance of 'Foo' has no 'bar' member\n+  \"W0212\", # Access to a protected member\n+  \"C0301\", # Line too long\n+  \"C0411\", # Wrong import order\n+  \"W0611\", # Unused imports\n+  \"W0613\", # Unused arguments\n+  \"W0622\", # Redefining built-in names\n+  \"R0903\", # Too few public methods\n+  \"R0801\", # Duplicate code\n+  \"W0621\",\n+  \"C0415\",\n+  \"R1719\", # The if expression can be replaced with 'bool(test)'\n+  \"R1705\", # Unnecessary \"elif\" after \"return\"\n+  \"R0401\",\n+]\n+max-line-length = 88\n+ignore = [\n+  \"tests\",\n+  \"migrations/*\",\n+  \"venv/*\",\n+  \"build/*\",\n+  \"dist/*\",\n+  \".git/*\",\n+  \".tox/*\",\n+  \"__pycache__/*\",\n+  \"*.egg-info/*\",\n+  \".mypy_cache/*\",\n+  \".pytest_cache/*\",\n+]\n+django-settings-module = \"kernel.settings\"\n+load-plugins = [\n+  \"pylint_django\",\n+  \"pylint.extensions.docparams\",\n+]\n+\n+suggestion-mode = true\n+const-rgx = \"([A-Z_][A-Z0-9_]*)|(__.*__)\"\n+attr-rgx = \"[a-z_][a-z0-9_]{2,30}$\"\n+variable-rgx = \"[a-z_][a-z0-9_]{2,30}$\"\n+argument-rgx = \"[a-z_][a-z0-9_]{2,30}$\"\n+method-rgx = \"[a-z_][a-z0-9_]{2,30}$\"\n+function-rgx = \"[a-z_][a-z0-9_]{2,30}$\"\n+class-rgx = \"[A-Z_][a-zA-Z0-9]+$\"\n+module-rgx = \"(([a-z_][a-z0-9_]*)|(__.*__))$\"\n+\n+[tool.pytest.ini_options]\n+python_files = [ \"tests.py\", \"test_*.py\" ]\n+testpaths = [ \"django_notification/tests\" ]\n+addopts = \"--cov --cov-report=term-missing --cov-report=html --cov-fail-under=90\"\n+markers = [\n+  \"models: Marks tests related to all database models in the project.\",\n+  \"models_notification: Marks tests for the Notification model, including creation and model-specific methods.\",\n+  \"models_notification_recipient: Marks tests related to the NotificationRecipient model.\",\n+  \"models_notification_seen: Marks tests for the NotificationSeen model, which track when notifications have been seen.\",\n+  \"models_deleted_notification: Marks tests for handling deleted notifications, including soft logic.\",\n+  \"admin: Marks tests for Django admin functionalities, including access, rendering, and configurations.\",\n+  \"admin_notification: Marks tests for managing notifications in the Django admin, such as listing, filtering, and bulk actions.\",\n+  \"admin_deleted_notification: Marks tests for managing deleted notifications in the Django admin, including visibility and actions.\",\n+  \"api: Marks tests related to the Django REST Framework (DRF) API as a whole.\",\n+  \"api_views: Marks tests for DRF views, covering endpoints, request handling, and response generation.\",\n+  \"api_views_notification: Marks tests for views handling notification-related API operations, such as listing or managing notifications.\",\n+  \"api_views_activity: Marks tests for views related to activity-related operations in the API.\",\n+  \"api_paginations: Marks tests for pagination in the API, ensuring correct behavior of paginated responses across various endpoints.\",\n+  \"api_serializers: Marks tests for DRF serializers, including validation, data transformation, and response formatting.\",\n+  \"api_serializers_simple_notification: Marks tests for the SimpleNotificationSerializer, focusing on how notification data is serialized.\",\n+  \"api_serializers_group: Marks tests for serializers that handle group-related data, focusing on serialization.\",\n+  \"api_throttlings: Marks tests for DRF throttling mechanisms, ensuring the correct limiting of API requests.\",\n+  \"utils: Marks tests for general utility functions used across the project.\",\n+  \"utils_title_generator: Marks tests for the title generation utility, whichformats titles based on notification content.\",\n+  \"settings: Marks tests for settings and configurations in the project.\",\n+  \"settings_conf: Marks tests related to loading project-specific settings and configurations.\",\n+  \"settings_checks: Marks tests for settings validation, ensuring that required settings are correctly configured.\",\n+  \"validators: Marks tests for custom validators used throughout the project.\",\n+  \"config_validators: Marks tests for configuration validators that validate the config values in project settings.\",\n+  \"queryset: Marks tests for custom queryset methods, ensuring they perform for filtering and querying the database.\",\n+  \"decorators: Marks tests for custom Python decorators used in the project.\",\n+  \"decorators_action: Marks tests for action-related decorators.\",\n+]\n+\n+norecursedirs = [\n+  \"migrations\",\n+  \"env\",\n+  \"venv\",\n+  \".venv\",\n+  \"dist\",\n+  \"build\",\n+  \"kernel\",\n+]\n+\n+[tool.coverage.run]\n+source = [ \"django_notification\" ]\n+omit = [\n+  \"*/tests/*\",\n+  \"*/migrations/*\",\n+]\n+\n+[tool.coverage.report]\n+exclude_lines = [\n+  \"pragma: no cover\",\n+  \"def __repr__\",\n+  \"if self\\\\.debug\",\n+  \"raise AssertionError\",\n+  \"if 0:\",\n+  \"if __name__ == .__main__.:\",\n+]\n+\n+[tool.mypy]\n+mypy_path = \"stubs\"\n+disallow_untyped_calls = true\n+disallow_untyped_defs = true\n+ignore_missing_imports = true\n+explicit_package_bases = true\n+exclude = \"\"\"\n+^docs/source/conf.py|\n+^build/|\n+^tests/|\n+^stubs/|\n+^kernel/\n+\"\"\"\n+\n+[tool.bandit]\n+targets = [ \"./django_notification\" ]\n+exclude_dirs = [\n+  \"tests\",\n+  \"migrations\",\n+]\n+severity = \"medium\"\n+confidence = \"medium\"\n+max_lines = 500\n+progress = true\n+reports = true\n+output_format = \"screen\"\n+output_file = \"bandit_report.txt\"\n+include = [ \"B101\", \"B102\" ]\n+exclude_tests = [ \"B301\", \"B302\" ]\n+\n+[tool.bandit.plugins]\n+B104 = { check_typed_list = true }\n+\n+[tool.commitizen]\n+name = \"cz_conventional_commits\"\n+version = \"1.0.0\"\n+tag_format = \"v$version\"\n+\n+[tool.commitizen.settings]\n+increment_types = [ \"feat\", \"fix\" ]\n+\n+[tool.semantic_release]\n+assets = [  ]\n+build_command_env = [  ]\n+commit_message = \"{version}\\n\\nAutomatically generated by python-semantic-release\"\n+commit_parser = \"emoji\"\n+logging_use_named_masks = false\n+major_on_zero = true\n+allow_zero_version = true\n+no_git_verify = false\n+tag_format = \"v{version}\"\n+#version_variables = [ \"django_notification/version.py:__version__\" ]\n+\n+[tool.semantic_release.branches.main]\n+match = \"(main|master)\"\n+prerelease_token = \"rc\"\n+prerelease = false\n+\n+[tool.semantic_release.changelog]\n+template_dir = \"templates\"\n+changelog_file = \"CHANGELOG.md\"\n+exclude_commit_patterns = [  ]\n+\n+[tool.semantic_release.changelog.environment]\n+block_start_string = \"{%\"\n+block_end_string = \"%}\"\n+variable_start_string = \"{{\"\n+variable_end_string = \"}}\"\n+comment_start_string = \"{#\"\n+comment_end_string = \"#}\"\n+trim_blocks = false\n+lstrip_blocks = false\n+newline_sequence = \"\\n\"\n+keep_trailing_newline = false\n+extensions = [  ]\n+autoescape = true\n+\n+[tool.semantic_release.commit_author]\n+env = \"GIT_COMMIT_AUTHOR\"\n+default = \"semantic-release <semantic-release>\"\n+\n+[tool.semantic_release.commit_parser_options]\n+allowed_tags = [ \"build\", \"chore\", \"ci\", \"docs\", \"feat\", \"fix\", \"perf\", \"style\", \"refactor\", \"test\" ]\n+minor_tags = [ \"feat\" ]\n+patch_tags = [ \"fix\", \"perf\" ]\n+default_bump_level = 0\n+parse_emoji = true\n+\n+[tool.semantic_release.remote]\n+name = \"origin\"\n+token = \"\"\n+type = \"github\"\n+ignore_token_for_push = false\n+insecure = false\n+\n+[tool.semantic_release.publish]\n+dist_glob_patterns = [ \"dist/*\" ]\n+upload_to_vcs_release = true\ndiff --git a/tox.ini b/tox.ini\nnew file mode 100644\nindex 0000000..d9dde85\n--- /dev/null\n+++ b/tox.ini\n@@ -0,0 +1,50 @@\n+[tox]\n+requires =\n+    tox>=4.2\n+env_list =\n+    py312-django40\n+    py312-django50\n+    py312-django51\n+    py311-django40\n+    py311-django50\n+    py311-django51\n+    py310-django40\n+    py310-django50\n+    py310-django51\n+    py39-django40\n+\n+[testenv]\n+description = Run Pytest tests with multiple django versions\n+deps =\n+    pytest\n+    pytest-cov\n+    pytest-django\n+    django40: django<5.0,>=4.2\n+    django50: django<5.1,>=5\n+    django51: django<5.2,>=5.1\n+commands =\n+    pytest --cov=django_notification --cov-report=html\n+develop = True\n+\n+[testenv:bandit]\n+description = Run security checks\n+skip_install = true\n+deps =\n+    bandit\n+commands =\n+    bandit -r django_notification\n+\n+[testenv:pre-commit]\n+description = Run pre-commit hooks\n+skip_install = true\n+deps =\n+    pre-commit\n+commands =\n+    pre-commit run --all-files\n+\n+[gh-actions]\n+python =\n+    3.9: py39\n+    3.10: py310\n+    3.11: py311\n+    3.12: py312\n", "instance_id": "Lazarus-org__dj-notification-api-54", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: API tests are failing due to a missing or incomplete `urls.py` file in a Django project, resulting in 404 Not Found errors. It provides steps to reproduce, expected behavior, actual behavior, and a suggested solution (creating or updating `urls.py` for test setup). However, there are minor ambiguities and missing details. For instance, it does not specify which specific API endpoints or views need to be routed, nor does it mention any constraints or requirements for the test environment setup. Additionally, edge cases or potential challenges in integrating the URL configuration with the existing test suite are not addressed. The code changes provided do not directly relate to the `urls.py` issue, as they focus on CI workflows, dependency management, and minor code formatting, which introduces some confusion about the relevance of the diff to the stated problem. Overall, while the core issue is understandable, the lack of specific details about the URL configuration and the disconnect with the provided code changes prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of solving the stated problem\u2014adding or updating a `urls.py` file for API test routing\u2014falls into the easy category. It requires basic knowledge of Django's URL routing system and test configuration, likely involving a single file modification to define routes for the API endpoints being tested. The scope of the change is narrow, focusing on test setup rather than core application logic, and does not appear to impact the broader system architecture. However, there is a slight increase in complexity due to the need to understand the specific API views or endpoints that require routing, which may involve minimal cross-referencing with existing code. The provided code changes (e.g., CI workflows, dependency updates) are unrelated to the core issue and do not factor into this difficulty assessment, as they seem to be part of a broader repository update rather than a direct solution to the `urls.py` problem. No significant edge cases or error handling are mentioned in the problem statement, and the task does not require advanced technical concepts beyond standard Django practices. Therefore, a score of 0.3 reflects an easy task with minor complexity in identifying the exact routing needs.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Output to SARIF format\nThe SARIF format (https://sarifweb.azurewebsites.net/) is a standard schema for JSON that is written and read by many different static analysis  tools (https://github.com/oasis-tcs/sarif-spec/wiki/Known-Producers-and-Consumers#known-producers-of-sarif). For instance, it is written by the GCC and Clang static analyzers, and GitHub CodeQL, and it can be read by IDE extensions (e.g. [VSCode](https://marketplace.visualstudio.com/items?itemName=MS-SarifVSCode.sarif-viewer)) and also git forges (e.g., [GitHub Security](https://docs.github.com/en/code-security/code-scanning/integrating-with-code-scanning/sarif-support-for-code-scanning), [Azure DevOps](https://docs.kics.io/latest/integrations_azurepipelines/), and there is a request for integration into [GitLab](https://gitlab.com/gitlab-org/gitlab/-/issues/452042)). My use-case would primarily be the git forge integration, so that I can eventually include fortitude in a pipeline and have it scan PRs for issues.\r\n\r\nIt appears there is a Rust library that wrap the structure already - [serde-sarif](https://docs.rs/serde-sarif/latest/serde_sarif/index.html), so there shouldn't be a need to actually write the JSON formatter for this if we can include this dependency.\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex f0145d0..d25d6d8 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -103,6 +103,15 @@ version = \"2.6.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"b048fb63fd8b5923fc5aa7b340d8e156aec7ec02f0c78fa8a6ddc2613f6f71de\"\n \n+[[package]]\n+name = \"block-buffer\"\n+version = \"0.10.4\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"3078c7629b62d3f0439517fa394996acacc5cbc91c5a20d8c658e77abd503a71\"\n+dependencies = [\n+ \"generic-array\",\n+]\n+\n [[package]]\n name = \"bstr\"\n version = \"1.10.0\"\n@@ -135,6 +144,15 @@ version = \"1.0.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd\"\n \n+[[package]]\n+name = \"chrono\"\n+version = \"0.4.38\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"a21f936df1771bf62b77f047b726c4625ff2e8aa607c01ec06e5a05bd8463401\"\n+dependencies = [\n+ \"num-traits\",\n+]\n+\n [[package]]\n name = \"clap\"\n version = \"4.5.20\"\n@@ -224,6 +242,15 @@ dependencies = [\n  \"unicode-xid\",\n ]\n \n+[[package]]\n+name = \"cpufeatures\"\n+version = \"0.2.16\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"16b80225097f2e5ae4e7179dd2266824648f3e2f49d9134d584b76389d31c4c3\"\n+dependencies = [\n+ \"libc\",\n+]\n+\n [[package]]\n name = \"crossbeam-deque\"\n version = \"0.8.5\"\n@@ -249,6 +276,16 @@ version = \"0.8.20\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"22ec99545bb0ed0ea7bb9b8e1e9122ea386ff8a48c0922e43f36d45ab09e0e80\"\n \n+[[package]]\n+name = \"crypto-common\"\n+version = \"0.1.6\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"1bfb12502f3fc46cca1bb51ac28df9d618d813cdc3d2f25b9fe775a34af26bb3\"\n+dependencies = [\n+ \"generic-array\",\n+ \"typenum\",\n+]\n+\n [[package]]\n name = \"deranged\"\n version = \"0.3.11\"\n@@ -270,6 +307,27 @@ version = \"0.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"6184e33543162437515c2e2b48714794e37845ec9851711914eec9d308f6ebe8\"\n \n+[[package]]\n+name = \"digest\"\n+version = \"0.10.7\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292\"\n+dependencies = [\n+ \"block-buffer\",\n+ \"crypto-common\",\n+]\n+\n+[[package]]\n+name = \"displaydoc\"\n+version = \"0.2.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"97369cbbc041bc366949bc74d34658d6cda5621039731c6310521892a3a20ae0\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\n+\n [[package]]\n name = \"doc-comment\"\n version = \"0.3.3\"\n@@ -331,6 +389,15 @@ dependencies = [\n  \"num-traits\",\n ]\n \n+[[package]]\n+name = \"form_urlencoded\"\n+version = \"1.2.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"e13624c2627564efccf4934284bdd98cbaa14e79b0b5a141218e507b3a823456\"\n+dependencies = [\n+ \"percent-encoding\",\n+]\n+\n [[package]]\n name = \"fortitude\"\n version = \"0.5.1\"\n@@ -353,6 +420,7 @@ dependencies = [\n  \"pathdiff\",\n  \"predicates\",\n  \"pretty_assertions\",\n+ \"quick-junit\",\n  \"rayon\",\n  \"ruff_cache\",\n  \"ruff_diagnostics\",\n@@ -362,16 +430,18 @@ dependencies = [\n  \"serde\",\n  \"serde_json\",\n  \"shadow-rs\",\n+ \"similar\",\n  \"similar-asserts\",\n  \"strum\",\n  \"strum_macros\",\n  \"tempfile\",\n  \"test-case\",\n  \"textwrap\",\n- \"thiserror\",\n+ \"thiserror 1.0.66\",\n  \"toml\",\n  \"tree-sitter\",\n  \"tree-sitter-fortran\",\n+ \"url\",\n  \"walkdir\",\n ]\n \n@@ -385,6 +455,16 @@ dependencies = [\n  \"syn\",\n ]\n \n+[[package]]\n+name = \"generic-array\"\n+version = \"0.14.7\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"85649ca51fd72272d7821adaf274ad91c288277713d9c18820d8499a7ff69e9a\"\n+dependencies = [\n+ \"typenum\",\n+ \"version_check\",\n+]\n+\n [[package]]\n name = \"glob\"\n version = \"0.3.1\"\n@@ -416,6 +496,145 @@ version = \"0.5.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea\"\n \n+[[package]]\n+name = \"icu_collections\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"db2fa452206ebee18c4b5c2274dbf1de17008e874b4dc4f0aea9d01ca79e4526\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"yoke\",\n+ \"zerofrom\",\n+ \"zerovec\",\n+]\n+\n+[[package]]\n+name = \"icu_locid\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"13acbb8371917fc971be86fc8057c41a64b521c184808a698c02acc242dbf637\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"litemap\",\n+ \"tinystr\",\n+ \"writeable\",\n+ \"zerovec\",\n+]\n+\n+[[package]]\n+name = \"icu_locid_transform\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"01d11ac35de8e40fdeda00d9e1e9d92525f3f9d887cdd7aa81d727596788b54e\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"icu_locid\",\n+ \"icu_locid_transform_data\",\n+ \"icu_provider\",\n+ \"tinystr\",\n+ \"zerovec\",\n+]\n+\n+[[package]]\n+name = \"icu_locid_transform_data\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"fdc8ff3388f852bede6b579ad4e978ab004f139284d7b28715f773507b946f6e\"\n+\n+[[package]]\n+name = \"icu_normalizer\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"19ce3e0da2ec68599d193c93d088142efd7f9c5d6fc9b803774855747dc6a84f\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"icu_collections\",\n+ \"icu_normalizer_data\",\n+ \"icu_properties\",\n+ \"icu_provider\",\n+ \"smallvec\",\n+ \"utf16_iter\",\n+ \"utf8_iter\",\n+ \"write16\",\n+ \"zerovec\",\n+]\n+\n+[[package]]\n+name = \"icu_normalizer_data\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f8cafbf7aa791e9b22bec55a167906f9e1215fd475cd22adfcf660e03e989516\"\n+\n+[[package]]\n+name = \"icu_properties\"\n+version = \"1.5.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"93d6020766cfc6302c15dbbc9c8778c37e62c14427cb7f6e601d849e092aeef5\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"icu_collections\",\n+ \"icu_locid_transform\",\n+ \"icu_properties_data\",\n+ \"icu_provider\",\n+ \"tinystr\",\n+ \"zerovec\",\n+]\n+\n+[[package]]\n+name = \"icu_properties_data\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"67a8effbc3dd3e4ba1afa8ad918d5684b8868b3b26500753effea8d2eed19569\"\n+\n+[[package]]\n+name = \"icu_provider\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"6ed421c8a8ef78d3e2dbc98a973be2f3770cb42b606e3ab18d6237c4dfde68d9\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"icu_locid\",\n+ \"icu_provider_macros\",\n+ \"stable_deref_trait\",\n+ \"tinystr\",\n+ \"writeable\",\n+ \"yoke\",\n+ \"zerofrom\",\n+ \"zerovec\",\n+]\n+\n+[[package]]\n+name = \"icu_provider_macros\"\n+version = \"1.5.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"1ec89e9337638ecdc08744df490b221a7399bf8d164eb52a665454e60e075ad6\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\n+\n+[[package]]\n+name = \"idna\"\n+version = \"1.0.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"686f825264d630750a544639377bae737628043f20d38bbc029e8f29ea968a7e\"\n+dependencies = [\n+ \"idna_adapter\",\n+ \"smallvec\",\n+ \"utf8_iter\",\n+]\n+\n+[[package]]\n+name = \"idna_adapter\"\n+version = \"1.2.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"daca1df1c957320b2cf139ac61e7bd64fed304c5040df000a745aa1de3b4ef71\"\n+dependencies = [\n+ \"icu_normalizer\",\n+ \"icu_properties\",\n+]\n+\n [[package]]\n name = \"indexmap\"\n version = \"2.6.0\"\n@@ -450,6 +669,8 @@ dependencies = [\n  \"console\",\n  \"lazy_static\",\n  \"linked-hash-map\",\n+ \"pest\",\n+ \"pest_derive\",\n  \"regex\",\n  \"serde\",\n  \"similar\",\n@@ -581,6 +802,12 @@ version = \"0.4.14\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"78b3ae25bc7c8c38cec158d1f2757ee79e9b3740fbc7ccf0e59e4b08d793fa89\"\n \n+[[package]]\n+name = \"litemap\"\n+version = \"0.7.4\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"4ee93343901ab17bd981295f2cf0026d4ad018c7c31ba84549a4ddbb47a45104\"\n+\n [[package]]\n name = \"log\"\n version = \"0.4.22\"\n@@ -593,6 +820,15 @@ version = \"2.7.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"78ca9ab1a0babb1e7d5695e3530886289c18cf2f87ec19a575a0abdce112e3a3\"\n \n+[[package]]\n+name = \"newtype-uuid\"\n+version = \"1.1.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"4c8781e2ef64806278a55ad223f0bc875772fd40e1fe6e73e8adbf027817229d\"\n+dependencies = [\n+ \"uuid\",\n+]\n+\n [[package]]\n name = \"normalize-line-endings\"\n version = \"0.3.0\"\n@@ -659,6 +895,57 @@ version = \"0.2.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"d61c5ce1153ab5b689d0c074c4e7fc613e942dfb7dd9eea5ab202d2ad91fe361\"\n \n+[[package]]\n+name = \"percent-encoding\"\n+version = \"2.3.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"e3148f5046208a5d56bcfc03053e3ca6334e51da8dfb19b6cdc8b306fae3283e\"\n+\n+[[package]]\n+name = \"pest\"\n+version = \"2.7.14\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"879952a81a83930934cbf1786752d6dedc3b1f29e8f8fb2ad1d0a36f377cf442\"\n+dependencies = [\n+ \"memchr\",\n+ \"thiserror 1.0.66\",\n+ \"ucd-trie\",\n+]\n+\n+[[package]]\n+name = \"pest_derive\"\n+version = \"2.7.14\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"d214365f632b123a47fd913301e14c946c61d1c183ee245fa76eb752e59a02dd\"\n+dependencies = [\n+ \"pest\",\n+ \"pest_generator\",\n+]\n+\n+[[package]]\n+name = \"pest_generator\"\n+version = \"2.7.14\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"eb55586734301717aea2ac313f50b2eb8f60d2fc3dc01d190eefa2e625f60c4e\"\n+dependencies = [\n+ \"pest\",\n+ \"pest_meta\",\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\n+\n+[[package]]\n+name = \"pest_meta\"\n+version = \"2.7.14\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"b75da2a70cf4d9cb76833c990ac9cd3923c9a8905a8929789ce347c84564d03d\"\n+dependencies = [\n+ \"once_cell\",\n+ \"pest\",\n+ \"sha2\",\n+]\n+\n [[package]]\n name = \"portable-atomic\"\n version = \"1.9.0\"\n@@ -720,6 +1007,30 @@ dependencies = [\n  \"unicode-ident\",\n ]\n \n+[[package]]\n+name = \"quick-junit\"\n+version = \"0.5.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"3ed1a693391a16317257103ad06a88c6529ac640846021da7c435a06fffdacd7\"\n+dependencies = [\n+ \"chrono\",\n+ \"indexmap\",\n+ \"newtype-uuid\",\n+ \"quick-xml\",\n+ \"strip-ansi-escapes\",\n+ \"thiserror 2.0.3\",\n+ \"uuid\",\n+]\n+\n+[[package]]\n+name = \"quick-xml\"\n+version = \"0.37.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f22f29bdff3987b4d8632ef95fd6424ec7e4e0a57e2f4fc63e489e75357f6a03\"\n+dependencies = [\n+ \"memchr\",\n+]\n+\n [[package]]\n name = \"quote\"\n version = \"1.0.37\"\n@@ -934,6 +1245,17 @@ dependencies = [\n  \"serde\",\n ]\n \n+[[package]]\n+name = \"sha2\"\n+version = \"0.10.8\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"793db75ad2bcafc3ffa7c68b215fee268f537982cd901d132f89c6343f3a3dc8\"\n+dependencies = [\n+ \"cfg-if\",\n+ \"cpufeatures\",\n+ \"digest\",\n+]\n+\n [[package]]\n name = \"shadow-rs\"\n version = \"0.36.0\"\n@@ -971,18 +1293,39 @@ dependencies = [\n  \"similar\",\n ]\n \n+[[package]]\n+name = \"smallvec\"\n+version = \"1.13.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"3c5e1a9a646d36c3599cd173a41282daf47c44583ad367b8e6837255952e5c67\"\n+\n [[package]]\n name = \"smawk\"\n version = \"0.3.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"b7c388c1b5e93756d0c740965c41e8822f866621d41acbdf6336a6a168f8840c\"\n \n+[[package]]\n+name = \"stable_deref_trait\"\n+version = \"1.2.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"a8f112729512f8e442d81f95a8a7ddf2b7c6b8a1a6f509a95864142b30cab2d3\"\n+\n [[package]]\n name = \"streaming-iterator\"\n version = \"0.1.9\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"2b2231b7c3057d5e4ad0156fb3dc807d900806020c5ffa3ee6ff2c8c76fb8520\"\n \n+[[package]]\n+name = \"strip-ansi-escapes\"\n+version = \"0.2.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"55ff8ef943b384c414f54aefa961dd2bd853add74ec75e7ac74cf91dba62bcfa\"\n+dependencies = [\n+ \"vte\",\n+]\n+\n [[package]]\n name = \"strsim\"\n version = \"0.11.1\"\n@@ -1013,15 +1356,26 @@ dependencies = [\n \n [[package]]\n name = \"syn\"\n-version = \"2.0.86\"\n+version = \"2.0.87\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e89275301d38033efb81a6e60e3497e734dfcc62571f2854bf4b16690398824c\"\n+checksum = \"25aa4ce346d03a6dcd68dd8b4010bcb74e54e62c90c573f394c46eae99aba32d\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"unicode-ident\",\n ]\n \n+[[package]]\n+name = \"synstructure\"\n+version = \"0.13.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"c8af7666ab7b6390ab78131fb5b0fce11d6b7a6951602017c35fa82800708971\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\n+\n [[package]]\n name = \"tempfile\"\n version = \"3.13.0\"\n@@ -1091,7 +1445,16 @@ version = \"1.0.66\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"5d171f59dbaa811dbbb1aee1e73db92ec2b122911a48e1390dfe327a821ddede\"\n dependencies = [\n- \"thiserror-impl\",\n+ \"thiserror-impl 1.0.66\",\n+]\n+\n+[[package]]\n+name = \"thiserror\"\n+version = \"2.0.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"c006c85c7651b3cf2ada4584faa36773bd07bac24acfb39f3c431b36d7e667aa\"\n+dependencies = [\n+ \"thiserror-impl 2.0.3\",\n ]\n \n [[package]]\n@@ -1105,6 +1468,17 @@ dependencies = [\n  \"syn\",\n ]\n \n+[[package]]\n+name = \"thiserror-impl\"\n+version = \"2.0.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f077553d607adc1caf65430528a576c757a71ed73944b66ebb58ef2bbd243568\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\n+\n [[package]]\n name = \"time\"\n version = \"0.3.36\"\n@@ -1138,6 +1512,16 @@ dependencies = [\n  \"time-core\",\n ]\n \n+[[package]]\n+name = \"tinystr\"\n+version = \"0.7.6\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"9117f5d4db391c1cf6927e7bea3db74b9a1c1add8f7eda9ffd5364f40f57b82f\"\n+dependencies = [\n+ \"displaydoc\",\n+ \"zerovec\",\n+]\n+\n [[package]]\n name = \"toml\"\n version = \"0.8.19\"\n@@ -1201,6 +1585,18 @@ version = \"0.1.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"e8ddffe35a0e5eeeadf13ff7350af564c6e73993a24db62caee1822b185c2600\"\n \n+[[package]]\n+name = \"typenum\"\n+version = \"1.17.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"42ff0bf0c66b8238c6f3b578df37d0b7848e55df8577b3f74f92a69acceeb825\"\n+\n+[[package]]\n+name = \"ucd-trie\"\n+version = \"0.1.7\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"2896d95c02a80c6d6a5d6e953d479f5ddf2dfdb6a244441010e373ac0fb88971\"\n+\n [[package]]\n name = \"unicode-ident\"\n version = \"1.0.13\"\n@@ -1237,12 +1633,67 @@ version = \"0.2.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"ebc1c04c71510c7f702b52b7c350734c9ff1295c464a03335b00bb84fc54f853\"\n \n+[[package]]\n+name = \"url\"\n+version = \"2.5.4\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"32f8b686cadd1473f4bd0117a5d28d36b1ade384ea9b5069a1c40aefed7fda60\"\n+dependencies = [\n+ \"form_urlencoded\",\n+ \"idna\",\n+ \"percent-encoding\",\n+]\n+\n+[[package]]\n+name = \"utf16_iter\"\n+version = \"1.0.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"c8232dd3cdaed5356e0f716d285e4b40b932ac434100fe9b7e0e8e935b9e6246\"\n+\n+[[package]]\n+name = \"utf8_iter\"\n+version = \"1.0.4\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"b6c140620e7ffbb22c2dee59cafe6084a59b5ffc27a8859a5f0d494b5d52b6be\"\n+\n [[package]]\n name = \"utf8parse\"\n version = \"0.2.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"06abde3611657adf66d383f00b093d7faecc7fa57071cce2578660c9f1010821\"\n \n+[[package]]\n+name = \"uuid\"\n+version = \"1.11.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f8c5f0a0af699448548ad1a2fbf920fb4bee257eae39953ba95cb84891a0446a\"\n+\n+[[package]]\n+name = \"version_check\"\n+version = \"0.9.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"0b928f33d975fc6ad9f86c8f283853ad26bdd5b10b7f1542aa2fa15e2289105a\"\n+\n+[[package]]\n+name = \"vte\"\n+version = \"0.11.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f5022b5fbf9407086c180e9557be968742d839e68346af7792b8592489732197\"\n+dependencies = [\n+ \"utf8parse\",\n+ \"vte_generate_state_changes\",\n+]\n+\n+[[package]]\n+name = \"vte_generate_state_changes\"\n+version = \"0.1.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"2e369bee1b05d510a7b4ed645f5faa90619e05437111783ea5848f28d97d3c2e\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+]\n+\n [[package]]\n name = \"wait-timeout\"\n version = \"0.2.0\"\n@@ -1493,8 +1944,87 @@ dependencies = [\n  \"memchr\",\n ]\n \n+[[package]]\n+name = \"write16\"\n+version = \"1.0.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"d1890f4022759daae28ed4fe62859b1236caebfc61ede2f63ed4e695f3f6d936\"\n+\n+[[package]]\n+name = \"writeable\"\n+version = \"0.5.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"1e9df38ee2d2c3c5948ea468a8406ff0db0b29ae1ffde1bcf20ef305bcc95c51\"\n+\n [[package]]\n name = \"yansi\"\n version = \"1.0.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"cfe53a6657fd280eaa890a3bc59152892ffa3e30101319d168b781ed6529b049\"\n+\n+[[package]]\n+name = \"yoke\"\n+version = \"0.7.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"120e6aef9aa629e3d4f52dc8cc43a015c7724194c97dfaf45180d2daf2b77f40\"\n+dependencies = [\n+ \"serde\",\n+ \"stable_deref_trait\",\n+ \"yoke-derive\",\n+ \"zerofrom\",\n+]\n+\n+[[package]]\n+name = \"yoke-derive\"\n+version = \"0.7.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"2380878cad4ac9aac1e2435f3eb4020e8374b5f13c296cb75b4620ff8e229154\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+ \"synstructure\",\n+]\n+\n+[[package]]\n+name = \"zerofrom\"\n+version = \"0.1.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"cff3ee08c995dee1859d998dea82f7374f2826091dd9cd47def953cae446cd2e\"\n+dependencies = [\n+ \"zerofrom-derive\",\n+]\n+\n+[[package]]\n+name = \"zerofrom-derive\"\n+version = \"0.1.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"595eed982f7d355beb85837f651fa22e90b3c044842dc7f2c2842c086f295808\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+ \"synstructure\",\n+]\n+\n+[[package]]\n+name = \"zerovec\"\n+version = \"0.10.4\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"aa2b893d79df23bfb12d5461018d408ea19dfafe76c2c7ef6d4eba614f8ff079\"\n+dependencies = [\n+ \"yoke\",\n+ \"zerofrom\",\n+ \"zerovec-derive\",\n+]\n+\n+[[package]]\n+name = \"zerovec-derive\"\n+version = \"0.10.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"6eafa6dfb17584ea3e2bd6e76e0cc15ad7af12b09abdd1ca55961bed9b1063c6\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\ndiff --git a/fortitude/Cargo.toml b/fortitude/Cargo.toml\nindex 7f3efda..ab97801 100644\n--- a/fortitude/Cargo.toml\n+++ b/fortitude/Cargo.toml\n@@ -32,6 +32,7 @@ path-absolutize = { version = \"3.1.1\", features = [\n     \"use_unix_paths_on_wasm\",\n ]}\n pathdiff = { version = \"0.2.1\" }\n+quick-junit = { version = \"0.5.0\" }\n rayon = \"1.10.0\"\n ruff_cache = { git = \"https://github.com/astral-sh/ruff.git\", tag = \"0.7.1\", version = \"0.0.0\" }\n ruff_diagnostics = { git = \"https://github.com/astral-sh/ruff.git\", tag = \"0.7.1\", version = \"0.0.0\", features = [\"serde\"] }\n@@ -40,6 +41,7 @@ ruff_source_file = { git = \"https://github.com/astral-sh/ruff.git\", tag = \"0.7.1\n ruff_text_size = { git = \"https://github.com/astral-sh/ruff.git\", tag = \"0.7.1\", version = \"0.0.0\" }\n serde = { version = \"1.0.210\", features = [\"derive\"] }\n serde_json = { workspace = true }\n+similar = { version = \"2.4.0\", features = [\"inline\"] }\n similar-asserts = \"1.6.0\"\n shadow-rs = { version = \"0.36.0\", default-features = false }\n strum = { workspace = true }\n@@ -49,6 +51,7 @@ thiserror = { version = \"1.0.58\" }\n toml = \"0.8.19\"\n tree-sitter = \"~0.24.0\"\n tree-sitter-fortran = \"0.3.0\"\n+url = { version = \"2.5.0\" }\n walkdir = \"2.4.0\"\n \n [build-dependencies]\n@@ -58,7 +61,7 @@ shadow-rs = { version = \"0.36.0\", default-features = false }\n assert_cmd = \"2.0.16\"\n # Disable colored output in tests\n colored = { workspace = true, features = [\"no-color\"] }\n-insta = { version = \"1.41.1\", features = [\"filters\"] }\n+insta = { version = \"1.41.1\", features = [\"filters\", \"json\", \"redactions\"] }\n insta-cmd = \"0.6.0\"\n predicates = \"3.1.2\"\n pretty_assertions = \"1.4.1\"\ndiff --git a/fortitude/src/message/azure.rs b/fortitude/src/message/azure.rs\nnew file mode 100644\nindex 0000000..0f32c4c\n--- /dev/null\n+++ b/fortitude/src/message/azure.rs\n@@ -0,0 +1,57 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::io::Write;\n+\n+use crate::message::Emitter;\n+\n+use super::DiagnosticMessage;\n+\n+/// Generate error logging commands for Azure Pipelines format.\n+/// See [documentation](https://learn.microsoft.com/en-us/azure/devops/pipelines/scripts/logging-commands?view=azure-devops&tabs=bash#logissue-log-an-error-or-warning)\n+#[derive(Default)]\n+pub struct AzureEmitter;\n+\n+impl Emitter for AzureEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        for message in messages {\n+            let location = message.compute_start_location();\n+\n+            writeln!(\n+                writer,\n+                \"##vso[task.logissue type=error\\\n+                        ;sourcepath={filename};linenumber={line};columnnumber={col};{code}]{body}\",\n+                filename = message.filename(),\n+                line = location.row,\n+                col = location.column,\n+                code = message\n+                    .rule()\n+                    .map_or_else(String::new, |rule| format!(\"code={};\", rule.noqa_code())),\n+                body = message.body(),\n+            )?;\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::AzureEmitter;\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = AzureEmitter;\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/diff.rs b/fortitude/src/message/diff.rs\nnew file mode 100644\nindex 0000000..776e47b\n--- /dev/null\n+++ b/fortitude/src/message/diff.rs\n@@ -0,0 +1,207 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::fmt::{Display, Formatter};\n+use std::num::NonZeroUsize;\n+\n+use colored::{Color, ColoredString, Colorize, Styles};\n+\n+use ruff_text_size::{Ranged, TextRange, TextSize};\n+use similar::{ChangeTag, TextDiff};\n+\n+use ruff_diagnostics::{Applicability, Fix};\n+use ruff_source_file::{OneIndexed, SourceFile};\n+\n+use crate::message::DiagnosticMessage;\n+use crate::text_helpers::ShowNonprinting;\n+\n+/// Renders a diff that shows the code fixes.\n+///\n+/// The implementation isn't fully fledged out and only used by tests. Before using in production, try\n+/// * Improve layout\n+/// * Replace tabs with spaces for a consistent experience across terminals\n+/// * Replace zero-width whitespaces\n+/// * Print a simpler diff if only a single line has changed\n+/// * Compute the diff from the [`Edit`] because diff calculation is expensive.\n+pub(super) struct Diff<'a> {\n+    fix: &'a Fix,\n+    source_code: &'a SourceFile,\n+}\n+\n+impl<'a> Diff<'a> {\n+    pub(crate) fn from_message(message: &'a DiagnosticMessage) -> Option<Diff<'a>> {\n+        message.fix().map(|fix| Diff {\n+            source_code: message.source_file(),\n+            fix,\n+        })\n+    }\n+}\n+\n+impl Display for Diff<'_> {\n+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n+        // TODO(dhruvmanila): Add support for Notebook cells once it's user-facing\n+        let mut output = String::with_capacity(self.source_code.source_text().len());\n+        let mut last_end = TextSize::default();\n+\n+        for edit in self.fix.edits() {\n+            output.push_str(\n+                self.source_code\n+                    .slice(TextRange::new(last_end, edit.start())),\n+            );\n+            output.push_str(edit.content().unwrap_or_default());\n+            last_end = edit.end();\n+        }\n+\n+        output.push_str(&self.source_code.source_text()[usize::from(last_end)..]);\n+\n+        let diff = TextDiff::from_lines(self.source_code.source_text(), &output);\n+\n+        let message = match self.fix.applicability() {\n+            // TODO(zanieb): Adjust this messaging once it's user-facing\n+            Applicability::Safe => \"Safe fix\",\n+            Applicability::Unsafe => \"Unsafe fix\",\n+            Applicability::DisplayOnly => \"Display-only fix\",\n+        };\n+        writeln!(f, \"\u2139 {}\", message.blue())?;\n+\n+        let (largest_old, largest_new) = diff\n+            .ops()\n+            .last()\n+            .map(|op| (op.old_range().start, op.new_range().start))\n+            .unwrap_or_default();\n+\n+        let digit_with =\n+            calculate_print_width(OneIndexed::from_zero_indexed(largest_new.max(largest_old)));\n+\n+        for (idx, group) in diff.grouped_ops(3).iter().enumerate() {\n+            if idx > 0 {\n+                writeln!(f, \"{:-^1$}\", \"-\", 80)?;\n+            }\n+            for op in group {\n+                for change in diff.iter_inline_changes(op) {\n+                    let sign = match change.tag() {\n+                        ChangeTag::Delete => \"-\",\n+                        ChangeTag::Insert => \"+\",\n+                        ChangeTag::Equal => \" \",\n+                    };\n+\n+                    let line_style = LineStyle::from(change.tag());\n+\n+                    let old_index = change.old_index().map(OneIndexed::from_zero_indexed);\n+                    let new_index = change.new_index().map(OneIndexed::from_zero_indexed);\n+\n+                    write!(\n+                        f,\n+                        \"{} {} |{}\",\n+                        Line {\n+                            index: old_index,\n+                            width: digit_with\n+                        },\n+                        Line {\n+                            index: new_index,\n+                            width: digit_with\n+                        },\n+                        line_style.apply_to(sign).bold()\n+                    )?;\n+\n+                    for (emphasized, value) in change.iter_strings_lossy() {\n+                        let value = value.show_nonprinting();\n+                        if emphasized {\n+                            write!(f, \"{}\", line_style.apply_to(&value).underline().on_black())?;\n+                        } else {\n+                            write!(f, \"{}\", line_style.apply_to(&value))?;\n+                        }\n+                    }\n+                    if change.missing_newline() {\n+                        writeln!(f)?;\n+                    }\n+                }\n+            }\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+struct LineStyle {\n+    fgcolor: Option<Color>,\n+    style: Option<Styles>,\n+}\n+\n+impl LineStyle {\n+    fn apply_to(&self, input: &str) -> ColoredString {\n+        let mut colored = ColoredString::from(input);\n+        if let Some(color) = self.fgcolor {\n+            colored = colored.color(color);\n+        }\n+\n+        if let Some(style) = self.style {\n+            match style {\n+                Styles::Clear => colored.clear(),\n+                Styles::Bold => colored.bold(),\n+                Styles::Dimmed => colored.dimmed(),\n+                Styles::Underline => colored.underline(),\n+                Styles::Reversed => colored.reversed(),\n+                Styles::Italic => colored.italic(),\n+                Styles::Blink => colored.blink(),\n+                Styles::Hidden => colored.hidden(),\n+                Styles::Strikethrough => colored.strikethrough(),\n+            }\n+        } else {\n+            colored\n+        }\n+    }\n+}\n+\n+impl From<ChangeTag> for LineStyle {\n+    fn from(value: ChangeTag) -> Self {\n+        match value {\n+            ChangeTag::Equal => LineStyle {\n+                fgcolor: None,\n+                style: Some(Styles::Dimmed),\n+            },\n+            ChangeTag::Delete => LineStyle {\n+                fgcolor: Some(Color::Red),\n+                style: None,\n+            },\n+            ChangeTag::Insert => LineStyle {\n+                fgcolor: Some(Color::Green),\n+                style: None,\n+            },\n+        }\n+    }\n+}\n+\n+struct Line {\n+    index: Option<OneIndexed>,\n+    width: NonZeroUsize,\n+}\n+\n+impl Display for Line {\n+    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {\n+        match self.index {\n+            None => {\n+                for _ in 0..self.width.get() {\n+                    f.write_str(\" \")?;\n+                }\n+                Ok(())\n+            }\n+            Some(idx) => write!(f, \"{:<width$}\", idx, width = self.width.get()),\n+        }\n+    }\n+}\n+\n+/// Calculate the length of the string representation of `value`\n+pub(super) fn calculate_print_width(mut value: OneIndexed) -> NonZeroUsize {\n+    const TEN: OneIndexed = OneIndexed::from_zero_indexed(9);\n+\n+    let mut width = OneIndexed::ONE;\n+\n+    while value >= TEN {\n+        value = OneIndexed::new(value.get() / 10).unwrap_or(OneIndexed::MIN);\n+        width = width.checked_add(1).unwrap();\n+    }\n+\n+    width\n+}\ndiff --git a/fortitude/src/message/github.rs b/fortitude/src/message/github.rs\nnew file mode 100644\nindex 0000000..4d224ff\n--- /dev/null\n+++ b/fortitude/src/message/github.rs\n@@ -0,0 +1,73 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::io::Write;\n+\n+use crate::fs::relativize_path;\n+use crate::message::Emitter;\n+\n+use super::DiagnosticMessage;\n+\n+/// Generate error workflow command in GitHub Actions format.\n+/// See: [GitHub documentation](https://docs.github.com/en/actions/reference/workflow-commands-for-github-actions#setting-an-error-message)\n+#[derive(Default)]\n+pub struct GithubEmitter;\n+\n+impl Emitter for GithubEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        for message in messages {\n+            let source_location = message.compute_start_location();\n+            let location = source_location.clone();\n+\n+            let end_location = message.compute_end_location();\n+\n+            write!(\n+                writer,\n+                \"::error title=Fortitude{code},file={file},line={row},col={column},endLine={end_row},endColumn={end_column}::\",\n+                code = message.rule().map_or_else(String::new, |rule| format!(\" ({})\", rule.noqa_code())),\n+                file = message.filename(),\n+                row = source_location.row,\n+                column = source_location.column,\n+                end_row = end_location.row,\n+                end_column = end_location.column,\n+            )?;\n+\n+            write!(\n+                writer,\n+                \"{path}:{row}:{column}:\",\n+                path = relativize_path(message.filename()),\n+                row = location.row,\n+                column = location.column,\n+            )?;\n+\n+            if let Some(rule) = message.rule() {\n+                write!(writer, \" {}\", rule.noqa_code())?;\n+            }\n+\n+            writeln!(writer, \" {}\", message.body())?;\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::GithubEmitter;\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = GithubEmitter;\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/gitlab.rs b/fortitude/src/message/gitlab.rs\nnew file mode 100644\nindex 0000000..f324dfd\n--- /dev/null\n+++ b/fortitude/src/message/gitlab.rs\n@@ -0,0 +1,157 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::collections::hash_map::DefaultHasher;\n+use std::collections::HashSet;\n+use std::hash::{Hash, Hasher};\n+use std::io::Write;\n+\n+use serde::ser::SerializeSeq;\n+use serde::{Serialize, Serializer};\n+use serde_json::json;\n+\n+use crate::fs::{relativize_path, relativize_path_to};\n+use crate::message::Emitter;\n+\n+use super::DiagnosticMessage;\n+\n+/// Generate JSON with violations in GitLab CI format\n+//  https://docs.gitlab.com/ee/ci/testing/code_quality.html#implement-a-custom-tool\n+pub struct GitlabEmitter {\n+    project_dir: Option<String>,\n+}\n+\n+impl Default for GitlabEmitter {\n+    fn default() -> Self {\n+        Self {\n+            project_dir: std::env::var(\"CI_PROJECT_DIR\").ok(),\n+        }\n+    }\n+}\n+\n+impl Emitter for GitlabEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        serde_json::to_writer_pretty(\n+            writer,\n+            &SerializedMessages {\n+                messages,\n+                project_dir: self.project_dir.as_deref(),\n+            },\n+        )?;\n+\n+        Ok(())\n+    }\n+}\n+\n+struct SerializedMessages<'a> {\n+    messages: &'a [DiagnosticMessage],\n+    project_dir: Option<&'a str>,\n+}\n+\n+impl Serialize for SerializedMessages<'_> {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut s = serializer.serialize_seq(Some(self.messages.len()))?;\n+        let mut fingerprints = HashSet::<u64>::with_capacity(self.messages.len());\n+\n+        for message in self.messages {\n+            let start_location = message.compute_start_location();\n+            let end_location = message.compute_end_location();\n+\n+            let lines = json!({\n+                \"begin\": start_location.row,\n+                \"end\": end_location.row\n+            });\n+\n+            let path = self.project_dir.as_ref().map_or_else(\n+                || relativize_path(message.filename()),\n+                |project_dir| relativize_path_to(message.filename(), project_dir),\n+            );\n+\n+            let mut message_fingerprint = fingerprint(message, &path, 0);\n+\n+            // Make sure that we do not get a fingerprint that is already in use\n+            // by adding in the previously generated one.\n+            while fingerprints.contains(&message_fingerprint) {\n+                message_fingerprint = fingerprint(message, &path, message_fingerprint);\n+            }\n+            fingerprints.insert(message_fingerprint);\n+\n+            let description = if let Some(rule) = message.rule() {\n+                format!(\"({}) {}\", rule.noqa_code(), message.body())\n+            } else {\n+                message.body().to_string()\n+            };\n+\n+            let value = json!({\n+                \"description\": description,\n+                \"severity\": \"major\",\n+                \"fingerprint\": format!(\"{:x}\", message_fingerprint),\n+                \"location\": {\n+                    \"path\": path,\n+                    \"lines\": lines\n+                }\n+            });\n+\n+            s.serialize_element(&value)?;\n+        }\n+\n+        s.end()\n+    }\n+}\n+\n+/// Generate a unique fingerprint to identify a violation.\n+fn fingerprint(message: &DiagnosticMessage, project_path: &str, salt: u64) -> u64 {\n+    let mut hasher = DefaultHasher::new();\n+\n+    salt.hash(&mut hasher);\n+    message.name().hash(&mut hasher);\n+    project_path.hash(&mut hasher);\n+\n+    hasher.finish()\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::GitlabEmitter;\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = GitlabEmitter::default();\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(redact_fingerprint(&content));\n+    }\n+\n+    // Redact the fingerprint because the default hasher isn't stable across platforms.\n+    fn redact_fingerprint(content: &str) -> String {\n+        static FINGERPRINT_HAY_KEY: &str = r#\"\"fingerprint\": \"\"#;\n+\n+        let mut output = String::with_capacity(content.len());\n+        let mut last = 0;\n+\n+        for (start, _) in content.match_indices(FINGERPRINT_HAY_KEY) {\n+            let fingerprint_hash_start = start + FINGERPRINT_HAY_KEY.len();\n+            output.push_str(&content[last..fingerprint_hash_start]);\n+            output.push_str(\"<redacted>\");\n+            last = fingerprint_hash_start\n+                + content[fingerprint_hash_start..]\n+                    .find('\"')\n+                    .expect(\"Expected terminating quote\");\n+        }\n+\n+        output.push_str(&content[last..]);\n+\n+        output\n+    }\n+}\ndiff --git a/fortitude/src/message/grouped.rs b/fortitude/src/message/grouped.rs\nnew file mode 100644\nindex 0000000..23e9baa\n--- /dev/null\n+++ b/fortitude/src/message/grouped.rs\n@@ -0,0 +1,226 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::fmt::{Display, Formatter};\n+use std::io::Write;\n+use std::num::NonZeroUsize;\n+\n+use colored::Colorize;\n+\n+use ruff_source_file::OneIndexed;\n+\n+use crate::fs::relativize_path;\n+use crate::message::diff::calculate_print_width;\n+use crate::message::text::{MessageCodeFrame, RuleCodeAndBody};\n+use crate::message::{group_messages_by_filename, Emitter, MessageWithLocation};\n+use crate::settings::UnsafeFixes;\n+\n+use super::DiagnosticMessage;\n+\n+#[derive(Default)]\n+pub struct GroupedEmitter {\n+    show_fix_status: bool,\n+    show_source: bool,\n+    unsafe_fixes: UnsafeFixes,\n+}\n+\n+impl GroupedEmitter {\n+    #[must_use]\n+    pub fn with_show_fix_status(mut self, show_fix_status: bool) -> Self {\n+        self.show_fix_status = show_fix_status;\n+        self\n+    }\n+\n+    // Might only be used in tests?\n+    #[must_use]\n+    #[allow(dead_code)]\n+    pub fn with_show_source(mut self, show_source: bool) -> Self {\n+        self.show_source = show_source;\n+        self\n+    }\n+\n+    #[must_use]\n+    pub fn with_unsafe_fixes(mut self, unsafe_fixes: UnsafeFixes) -> Self {\n+        self.unsafe_fixes = unsafe_fixes;\n+        self\n+    }\n+}\n+\n+impl Emitter for GroupedEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        for (filename, messages) in group_messages_by_filename(messages) {\n+            // Compute the maximum number of digits in the row and column, for messages in\n+            // this file.\n+\n+            let mut max_row_length = OneIndexed::MIN;\n+            let mut max_column_length = OneIndexed::MIN;\n+\n+            for message in &messages {\n+                max_row_length = max_row_length.max(message.start_location.row);\n+                max_column_length = max_column_length.max(message.start_location.column);\n+            }\n+\n+            let row_length = calculate_print_width(max_row_length);\n+            let column_length = calculate_print_width(max_column_length);\n+\n+            // Print the filename.\n+            writeln!(writer, \"{}:\", relativize_path(filename).underline())?;\n+\n+            // Print each message.\n+            for message in messages {\n+                write!(\n+                    writer,\n+                    \"{}\",\n+                    DisplayGroupedMessage {\n+                        message,\n+                        show_fix_status: self.show_fix_status,\n+                        unsafe_fixes: self.unsafe_fixes,\n+                        show_source: self.show_source,\n+                        row_length,\n+                        column_length,\n+                    }\n+                )?;\n+            }\n+\n+            // Print a blank line between files, unless we're showing the source, in which case\n+            // we'll have already printed a blank line between messages.\n+            if !self.show_source {\n+                writeln!(writer)?;\n+            }\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+struct DisplayGroupedMessage<'a> {\n+    message: MessageWithLocation<'a>,\n+    show_fix_status: bool,\n+    unsafe_fixes: UnsafeFixes,\n+    show_source: bool,\n+    row_length: NonZeroUsize,\n+    column_length: NonZeroUsize,\n+}\n+\n+impl Display for DisplayGroupedMessage<'_> {\n+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n+        let MessageWithLocation {\n+            message,\n+            start_location,\n+        } = &self.message;\n+\n+        write!(\n+            f,\n+            \"  {row_padding}\",\n+            row_padding =\n+                \" \".repeat(self.row_length.get() - calculate_print_width(start_location.row).get())\n+        )?;\n+\n+        // Check if we're working on a jupyter notebook and translate positions with cell accordingly\n+        let (row, col) = (start_location.row, start_location.column);\n+\n+        writeln!(\n+            f,\n+            \"{row}{sep}{col}{col_padding} {code_and_body}\",\n+            sep = \":\".cyan(),\n+            col_padding = \" \".repeat(\n+                self.column_length.get() - calculate_print_width(start_location.column).get()\n+            ),\n+            code_and_body = RuleCodeAndBody {\n+                message,\n+                show_fix_status: self.show_fix_status,\n+                unsafe_fixes: self.unsafe_fixes\n+            },\n+        )?;\n+\n+        if self.show_source {\n+            use std::fmt::Write;\n+            let mut padded = PadAdapter::new(f);\n+            writeln!(padded, \"{}\", MessageCodeFrame { message })?;\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+/// Adapter that adds a '  ' at the start of every line without the need to copy the string.\n+/// Inspired by Rust's `debug_struct()` internal implementation that also uses a `PadAdapter`.\n+struct PadAdapter<'buf> {\n+    buf: &'buf mut (dyn std::fmt::Write + 'buf),\n+    on_newline: bool,\n+}\n+\n+impl<'buf> PadAdapter<'buf> {\n+    fn new(buf: &'buf mut (dyn std::fmt::Write + 'buf)) -> Self {\n+        Self {\n+            buf,\n+            on_newline: true,\n+        }\n+    }\n+}\n+\n+impl std::fmt::Write for PadAdapter<'_> {\n+    fn write_str(&mut self, s: &str) -> std::fmt::Result {\n+        for s in s.split_inclusive('\\n') {\n+            if self.on_newline {\n+                self.buf.write_str(\"  \")?;\n+            }\n+\n+            self.on_newline = s.ends_with('\\n');\n+            self.buf.write_str(s)?;\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::GroupedEmitter;\n+    use crate::settings::UnsafeFixes;\n+\n+    #[test]\n+    fn default() {\n+        let mut emitter = GroupedEmitter::default();\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+\n+    #[test]\n+    fn show_source() {\n+        let mut emitter = GroupedEmitter::default().with_show_source(true);\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+\n+    #[test]\n+    fn fix_status() {\n+        let mut emitter = GroupedEmitter::default()\n+            .with_show_fix_status(true)\n+            .with_show_source(true);\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+\n+    #[test]\n+    fn fix_status_unsafe() {\n+        let mut emitter = GroupedEmitter::default()\n+            .with_show_fix_status(true)\n+            .with_show_source(true)\n+            .with_unsafe_fixes(UnsafeFixes::Enabled);\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/json_lines.rs b/fortitude/src/message/json_lines.rs\nnew file mode 100644\nindex 0000000..01b15d1\n--- /dev/null\n+++ b/fortitude/src/message/json_lines.rs\n@@ -0,0 +1,43 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::io::Write;\n+\n+use crate::message::json::message_to_json_value;\n+use crate::message::Emitter;\n+\n+use super::DiagnosticMessage;\n+\n+#[derive(Default)]\n+pub struct JsonLinesEmitter;\n+\n+impl Emitter for JsonLinesEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        for message in messages {\n+            serde_json::to_writer(&mut *writer, &message_to_json_value(message))?;\n+            writer.write_all(b\"\\n\")?;\n+        }\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::json_lines::JsonLinesEmitter;\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = JsonLinesEmitter;\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/junit.rs b/fortitude/src/message/junit.rs\nnew file mode 100644\nindex 0000000..2aa9ea4\n--- /dev/null\n+++ b/fortitude/src/message/junit.rs\n@@ -0,0 +1,105 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::io::Write;\n+use std::path::Path;\n+\n+use quick_junit::{NonSuccessKind, Report, TestCase, TestCaseStatus, TestSuite, XmlString};\n+\n+use crate::message::{group_messages_by_filename, Emitter, MessageWithLocation};\n+\n+use super::DiagnosticMessage;\n+\n+#[derive(Default)]\n+pub struct JunitEmitter;\n+\n+impl Emitter for JunitEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        let mut report = Report::new(\"fortitude\");\n+\n+        if messages.is_empty() {\n+            let mut test_suite = TestSuite::new(\"fortitude\");\n+            test_suite.extra.insert(\n+                XmlString::new(\"package\"),\n+                XmlString::new(\"org.plasmafair.fortitude\"),\n+            );\n+            let mut case = TestCase::new(\"No errors found\", TestCaseStatus::success());\n+            case.set_classname(\"fortitude\");\n+            test_suite.add_test_case(case);\n+            report.add_test_suite(test_suite);\n+        } else {\n+            for (filename, messages) in group_messages_by_filename(messages) {\n+                let mut test_suite = TestSuite::new(filename);\n+                test_suite.extra.insert(\n+                    XmlString::new(\"package\"),\n+                    XmlString::new(\"org.plasmafair.fortitude\"),\n+                );\n+\n+                for message in messages {\n+                    let MessageWithLocation {\n+                        message,\n+                        start_location,\n+                    } = message;\n+                    let mut status = TestCaseStatus::non_success(NonSuccessKind::Failure);\n+                    status.set_message(message.body());\n+                    let location = start_location;\n+\n+                    status.set_description(format!(\n+                        \"line {row}, col {col}, {body}\",\n+                        row = location.row,\n+                        col = location.column,\n+                        body = message.body()\n+                    ));\n+                    let mut case = TestCase::new(\n+                        if let Some(rule) = message.rule() {\n+                            format!(\"org.plasmafair.fortitude.{}\", rule.noqa_code())\n+                        } else {\n+                            \"org.plasmafair.fortitude\".to_string()\n+                        },\n+                        status,\n+                    );\n+                    let file_path = Path::new(filename);\n+                    let file_stem = file_path.file_stem().unwrap().to_str().unwrap();\n+                    let classname = file_path.parent().unwrap().join(file_stem);\n+                    case.set_classname(classname.to_str().unwrap());\n+                    case.extra.insert(\n+                        XmlString::new(\"line\"),\n+                        XmlString::new(location.row.to_string()),\n+                    );\n+                    case.extra.insert(\n+                        XmlString::new(\"column\"),\n+                        XmlString::new(location.column.to_string()),\n+                    );\n+\n+                    test_suite.add_test_case(case);\n+                }\n+                report.add_test_suite(test_suite);\n+            }\n+        }\n+\n+        report.serialize(writer)?;\n+\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::JunitEmitter;\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = JunitEmitter;\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/mod.rs b/fortitude/src/message/mod.rs\nindex aa46498..ad6d228 100644\n--- a/fortitude/src/message/mod.rs\n+++ b/fortitude/src/message/mod.rs\n@@ -1,11 +1,31 @@\n+pub use azure::AzureEmitter;\n+pub use github::GithubEmitter;\n+pub use gitlab::GitlabEmitter;\n+pub use grouped::GroupedEmitter;\n pub use json::JsonEmitter;\n+pub use json_lines::JsonLinesEmitter;\n+pub use junit::JunitEmitter;\n+pub use pylint::PylintEmitter;\n+pub use rdjson::RdjsonEmitter;\n+pub use sarif::SarifEmitter;\n pub use text::TextEmitter;\n \n+mod azure;\n+mod diff;\n+mod github;\n+mod gitlab;\n+mod grouped;\n mod json;\n+mod json_lines;\n+mod junit;\n+mod pylint;\n+mod rdjson;\n+mod sarif;\n mod text;\n \n-use std::cmp::Ordering;\n+use std::collections::BTreeMap;\n use std::io::Write;\n+use std::{cmp::Ordering, ops::Deref};\n \n use crate::{registry::AsRule, rules::Rule};\n use ruff_diagnostics::{Diagnostic, DiagnosticKind, Fix};\n@@ -135,6 +155,35 @@ pub trait Emitter {\n     ) -> anyhow::Result<()>;\n }\n \n+struct MessageWithLocation<'a> {\n+    message: &'a DiagnosticMessage,\n+    start_location: SourceLocation,\n+}\n+\n+impl Deref for MessageWithLocation<'_> {\n+    type Target = DiagnosticMessage;\n+\n+    fn deref(&self) -> &Self::Target {\n+        self.message\n+    }\n+}\n+\n+fn group_messages_by_filename(\n+    messages: &[DiagnosticMessage],\n+) -> BTreeMap<&str, Vec<MessageWithLocation>> {\n+    let mut grouped_messages = BTreeMap::default();\n+    for message in messages {\n+        grouped_messages\n+            .entry(message.filename())\n+            .or_insert_with(Vec::new)\n+            .push(MessageWithLocation {\n+                message,\n+                start_location: message.compute_start_location(),\n+            });\n+    }\n+    grouped_messages\n+}\n+\n #[cfg(test)]\n mod tests {\n     use ruff_diagnostics::{Diagnostic, DiagnosticKind, Edit, Fix};\ndiff --git a/fortitude/src/message/pylint.rs b/fortitude/src/message/pylint.rs\nnew file mode 100644\nindex 0000000..e267d12\n--- /dev/null\n+++ b/fortitude/src/message/pylint.rs\n@@ -0,0 +1,61 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::io::Write;\n+\n+use crate::fs::relativize_path;\n+use crate::message::Emitter;\n+\n+use super::DiagnosticMessage;\n+\n+/// Generate violations in Pylint format.\n+/// See: [Flake8 documentation](https://flake8.pycqa.org/en/latest/internal/formatters.html#pylint-formatter)\n+#[derive(Default)]\n+pub struct PylintEmitter;\n+\n+impl Emitter for PylintEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        for message in messages {\n+            let row = message.compute_start_location().row;\n+\n+            let body = if let Some(rule) = message.rule() {\n+                format!(\n+                    \"[{code}] {body}\",\n+                    code = rule.noqa_code(),\n+                    body = message.body()\n+                )\n+            } else {\n+                message.body().to_string()\n+            };\n+\n+            writeln!(\n+                writer,\n+                \"{path}:{row}: {body}\",\n+                path = relativize_path(message.filename()),\n+            )?;\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::PylintEmitter;\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = PylintEmitter;\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/rdjson.rs b/fortitude/src/message/rdjson.rs\nnew file mode 100644\nindex 0000000..cb2d24f\n--- /dev/null\n+++ b/fortitude/src/message/rdjson.rs\n@@ -0,0 +1,142 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::io::Write;\n+\n+use serde::ser::SerializeSeq;\n+use serde::{Serialize, Serializer};\n+use serde_json::{json, Value};\n+\n+use ruff_diagnostics::Edit;\n+use ruff_source_file::SourceCode;\n+use ruff_text_size::Ranged;\n+\n+use crate::message::{Emitter, SourceLocation};\n+\n+use super::DiagnosticMessage;\n+\n+#[derive(Default)]\n+pub struct RdjsonEmitter;\n+\n+impl Emitter for RdjsonEmitter {\n+    fn emit(\n+        &mut self,\n+        writer: &mut dyn Write,\n+        messages: &[DiagnosticMessage],\n+    ) -> anyhow::Result<()> {\n+        serde_json::to_writer_pretty(\n+            writer,\n+            &json!({\n+                \"source\": {\n+                    \"name\": \"fortitude\",\n+                    \"url\": \"https://github.com/PlasmaFAIR/fortitude\",\n+                },\n+                \"severity\": \"warning\",\n+                \"diagnostics\": &ExpandedMessages{ messages }\n+            }),\n+        )?;\n+\n+        Ok(())\n+    }\n+}\n+\n+struct ExpandedMessages<'a> {\n+    messages: &'a [DiagnosticMessage],\n+}\n+\n+impl Serialize for ExpandedMessages<'_> {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut s = serializer.serialize_seq(Some(self.messages.len()))?;\n+\n+        for message in self.messages {\n+            let value = message_to_rdjson_value(message);\n+            s.serialize_element(&value)?;\n+        }\n+\n+        s.end()\n+    }\n+}\n+\n+fn message_to_rdjson_value(message: &DiagnosticMessage) -> Value {\n+    let source_code = message.source_file().to_source_code();\n+\n+    let start_location = source_code.source_location(message.start());\n+    let end_location = source_code.source_location(message.end());\n+\n+    if let Some(fix) = message.fix() {\n+        json!({\n+            \"message\": message.body(),\n+            \"location\": {\n+                \"path\": message.filename(),\n+                \"range\": rdjson_range(&start_location, &end_location),\n+            },\n+            \"code\": {\n+                \"value\": message.rule().map(|rule| rule.noqa_code().to_string()),\n+                // \"url\": message.rule().and_then(|rule| rule.url()),\n+            },\n+            \"suggestions\": rdjson_suggestions(fix.edits(), &source_code),\n+        })\n+    } else {\n+        json!({\n+            \"message\": message.body(),\n+            \"location\": {\n+                \"path\": message.filename(),\n+                \"range\": rdjson_range(&start_location, &end_location),\n+            },\n+            \"code\": {\n+                \"value\": message.rule().map(|rule| rule.noqa_code().to_string()),\n+                // \"url\": message.rule().and_then(|rule| rule.url()),\n+            },\n+        })\n+    }\n+}\n+\n+fn rdjson_suggestions(edits: &[Edit], source_code: &SourceCode) -> Value {\n+    Value::Array(\n+        edits\n+            .iter()\n+            .map(|edit| {\n+                let location = source_code.source_location(edit.start());\n+                let end_location = source_code.source_location(edit.end());\n+\n+                json!({\n+                    \"range\": rdjson_range(&location, &end_location),\n+                    \"text\": edit.content().unwrap_or_default(),\n+                })\n+            })\n+            .collect(),\n+    )\n+}\n+\n+fn rdjson_range(start: &SourceLocation, end: &SourceLocation) -> Value {\n+    json!({\n+        \"start\": {\n+            \"line\": start.row,\n+            \"column\": start.column,\n+        },\n+        \"end\": {\n+            \"line\": end.row,\n+            \"column\": end.column,\n+        },\n+    })\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use insta::assert_snapshot;\n+\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::RdjsonEmitter;\n+\n+    #[test]\n+    fn output() {\n+        let mut emitter = RdjsonEmitter;\n+        let content = capture_emitter_output(&mut emitter, &create_messages());\n+\n+        assert_snapshot!(content);\n+    }\n+}\ndiff --git a/fortitude/src/message/sarif.rs b/fortitude/src/message/sarif.rs\nnew file mode 100644\nindex 0000000..ba01bde\n--- /dev/null\n+++ b/fortitude/src/message/sarif.rs\n@@ -0,0 +1,214 @@\n+// Adapted from from ruff\n+// Copyright 2022 Charles Marsh\n+// SPDX-License-Identifier: MIT\n+\n+use std::collections::HashSet;\n+use std::io::Write;\n+\n+use anyhow::Result;\n+use serde::{Serialize, Serializer};\n+use serde_json::json;\n+\n+use ruff_source_file::OneIndexed;\n+\n+use crate::build::VERSION;\n+use crate::fs::normalize_path;\n+use crate::message::Emitter;\n+use crate::registry::{Category, RuleNamespace};\n+use crate::rules::Rule;\n+\n+use super::DiagnosticMessage;\n+\n+pub struct SarifEmitter;\n+\n+impl Emitter for SarifEmitter {\n+    fn emit(&mut self, writer: &mut dyn Write, messages: &[DiagnosticMessage]) -> Result<()> {\n+        let results = messages\n+            .iter()\n+            .map(SarifResult::from_message)\n+            .collect::<Result<Vec<_>>>()?;\n+\n+        let unique_rules: HashSet<_> = results.iter().filter_map(|result| result.rule).collect();\n+        let mut rules: Vec<SarifRule> = unique_rules.into_iter().map(SarifRule::from).collect();\n+        rules.sort_by(|a, b| a.code.cmp(&b.code));\n+\n+        let output = json!({\n+            \"$schema\": \"https://json.schemastore.org/sarif-2.1.0.json\",\n+            \"version\": \"2.1.0\",\n+            \"runs\": [{\n+                \"tool\": {\n+                    \"driver\": {\n+                        \"name\": \"fortitude\",\n+                        \"informationUri\": \"https://github.com/PlasmaFAIR/fortitude\",\n+                        \"rules\": rules,\n+                        \"version\": VERSION.to_string(),\n+                    }\n+                },\n+                \"results\": results,\n+            }],\n+        });\n+        serde_json::to_writer_pretty(writer, &output)?;\n+        Ok(())\n+    }\n+}\n+\n+#[derive(Debug, Clone)]\n+struct SarifRule<'a> {\n+    name: &'a str,\n+    code: String,\n+    linter: &'a str,\n+    summary: &'a str,\n+    explanation: Option<&'a str>,\n+    // url: Option<String>,\n+}\n+\n+impl From<Rule> for SarifRule<'_> {\n+    fn from(rule: Rule) -> Self {\n+        let code = rule.noqa_code().to_string();\n+        let (linter, _) = Category::parse_code(&code).unwrap();\n+        Self {\n+            name: rule.into(),\n+            code,\n+            linter: linter.name(),\n+            summary: rule.message_formats()[0],\n+            explanation: rule.explanation(),\n+            // url: rule.url(),\n+        }\n+    }\n+}\n+\n+impl Serialize for SarifRule<'_> {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        json!({\n+            \"id\": self.code,\n+            \"shortDescription\": {\n+                \"text\": self.summary,\n+            },\n+            \"fullDescription\": {\n+                \"text\": self.explanation,\n+            },\n+            \"help\": {\n+                \"text\": self.summary,\n+            },\n+            // \"helpUri\": self.url,\n+            \"properties\": {\n+                \"id\": self.code,\n+                \"kind\": self.linter,\n+                \"name\": self.name,\n+                \"problem.severity\": \"error\".to_string(),\n+            },\n+        })\n+        .serialize(serializer)\n+    }\n+}\n+\n+#[derive(Debug)]\n+struct SarifResult {\n+    rule: Option<Rule>,\n+    level: String,\n+    message: String,\n+    uri: String,\n+    start_line: OneIndexed,\n+    start_column: OneIndexed,\n+    end_line: OneIndexed,\n+    end_column: OneIndexed,\n+}\n+\n+impl SarifResult {\n+    #[cfg(not(target_arch = \"wasm32\"))]\n+    fn from_message(message: &DiagnosticMessage) -> Result<Self> {\n+        let start_location = message.compute_start_location();\n+        let end_location = message.compute_end_location();\n+        let path = normalize_path(message.filename());\n+        Ok(Self {\n+            rule: message.rule(),\n+            level: \"error\".to_string(),\n+            message: message.body().to_string(),\n+            uri: url::Url::from_file_path(&path)\n+                .map_err(|()| anyhow::anyhow!(\"Failed to convert path to URL: {}\", path.display()))?\n+                .to_string(),\n+            start_line: start_location.row,\n+            start_column: start_location.column,\n+            end_line: end_location.row,\n+            end_column: end_location.column,\n+        })\n+    }\n+\n+    #[cfg(target_arch = \"wasm32\")]\n+    #[allow(clippy::unnecessary_wraps)]\n+    fn from_message(message: &Message) -> Result<Self> {\n+        let start_location = message.compute_start_location();\n+        let end_location = message.compute_end_location();\n+        let path = normalize_path(message.filename());\n+        Ok(Self {\n+            rule: message.rule(),\n+            level: \"error\".to_string(),\n+            message: message.body().to_string(),\n+            uri: path.display().to_string(),\n+            start_line: start_location.row,\n+            start_column: start_location.column,\n+            end_line: end_location.row,\n+            end_column: end_location.column,\n+        })\n+    }\n+}\n+\n+impl Serialize for SarifResult {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        json!({\n+            \"level\": self.level,\n+            \"message\": {\n+                \"text\": self.message,\n+            },\n+            \"locations\": [{\n+                \"physicalLocation\": {\n+                    \"artifactLocation\": {\n+                        \"uri\": self.uri,\n+                    },\n+                    \"region\": {\n+                        \"startLine\": self.start_line,\n+                        \"startColumn\": self.start_column,\n+                        \"endLine\": self.end_line,\n+                        \"endColumn\": self.end_column,\n+                    }\n+                }\n+            }],\n+            \"ruleId\": self.rule.map(|rule| rule.noqa_code().to_string()),\n+        })\n+        .serialize(serializer)\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use crate::message::tests::{capture_emitter_output, create_messages};\n+    use crate::message::SarifEmitter;\n+\n+    fn get_output() -> String {\n+        let mut emitter = SarifEmitter {};\n+        capture_emitter_output(&mut emitter, &create_messages())\n+    }\n+\n+    #[test]\n+    fn valid_json() {\n+        let content = get_output();\n+        serde_json::from_str::<serde_json::Value>(&content).unwrap();\n+    }\n+\n+    #[test]\n+    fn test_results() {\n+        let content = get_output();\n+        let value = serde_json::from_str::<serde_json::Value>(&content).unwrap();\n+\n+        insta::assert_json_snapshot!(value, {\n+            \".runs[0].tool.driver.version\" => \"[VERSION]\",\n+            \".runs[0].results[].locations[].physicalLocation.artifactLocation.uri\" => \"[URI]\",\n+        });\n+    }\n+}\ndiff --git a/fortitude/src/message/text.rs b/fortitude/src/message/text.rs\nindex ed4547c..6e71a4b 100644\n--- a/fortitude/src/message/text.rs\n+++ b/fortitude/src/message/text.rs\n@@ -14,7 +14,7 @@ use ruff_text_size::{Ranged, TextRange};\n \n use crate::fs::relativize_path;\n // use crate::line_width::{IndentWidth, LineWidthBuilder};\n-// use crate::message::diff::Diff;\n+use crate::message::diff::Diff;\n use crate::message::Emitter;\n use crate::settings::UnsafeFixes;\n use crate::text_helpers::ShowNonprinting;\n@@ -104,11 +104,11 @@ impl Emitter for TextEmitter {\n                 }\n             }\n \n-            // if self.flags.intersects(EmitterFlags::SHOW_FIX_DIFF) {\n-            //     if let Some(diff) = Diff::from_message(message) {\n-            //         writeln!(writer, \"{diff}\")?;\n-            //     }\n-            // }\n+            if self.flags.intersects(EmitterFlags::SHOW_FIX_DIFF) {\n+                if let Some(diff) = Diff::from_message(message) {\n+                    writeln!(writer, \"{diff}\")?;\n+                }\n+            }\n         }\n \n         Ok(())\ndiff --git a/fortitude/src/printer.rs b/fortitude/src/printer.rs\nindex a3a9100..29f54f8 100644\n--- a/fortitude/src/printer.rs\n+++ b/fortitude/src/printer.rs\n@@ -4,7 +4,11 @@ use anyhow::Result;\n use bitflags::bitflags;\n use colored::Colorize;\n \n-use crate::message::{DiagnosticMessage, Emitter, JsonEmitter, TextEmitter};\n+use crate::message::{\n+    AzureEmitter, DiagnosticMessage, Emitter, GithubEmitter, GitlabEmitter, GroupedEmitter,\n+    JsonEmitter, JsonLinesEmitter, JunitEmitter, PylintEmitter, RdjsonEmitter, SarifEmitter,\n+    TextEmitter,\n+};\n use crate::settings::OutputFormat;\n \n bitflags! {\n@@ -77,6 +81,8 @@ impl Printer {\n         diagnostics: &[DiagnosticMessage],\n         writer: &mut dyn Write,\n     ) -> Result<()> {\n+        // TODO: implement tracking of fixables\n+\n         match self.format {\n             OutputFormat::Concise | OutputFormat::Full => {\n                 TextEmitter::default()\n@@ -87,9 +93,48 @@ impl Printer {\n                     .emit(writer, diagnostics)?;\n                 self.write_summary_text(writer, diagnostics, num_files)?;\n             }\n+            OutputFormat::Github => {\n+                GithubEmitter.emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::Gitlab => {\n+                GitlabEmitter::default().emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::Grouped => {\n+                GroupedEmitter::default()\n+                    .with_show_fix_status(true)\n+                    .with_unsafe_fixes(crate::settings::UnsafeFixes::Hint)\n+                    .emit(writer, diagnostics)?;\n+\n+                // if self.flags.intersects(Flags::SHOW_FIX_SUMMARY) {\n+                //     if !diagnostics.fixed.is_empty() {\n+                //         writeln!(writer)?;\n+                //         print_fix_summary(writer, &diagnostics.fixed)?;\n+                //         writeln!(writer)?;\n+                //     }\n+                // }\n+                self.write_summary_text(writer, diagnostics, num_files)?;\n+            }\n             OutputFormat::Json => {\n                 JsonEmitter.emit(writer, diagnostics)?;\n             }\n+            OutputFormat::Sarif => {\n+                SarifEmitter.emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::Azure => {\n+                AzureEmitter.emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::JsonLines => {\n+                JsonLinesEmitter.emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::Rdjson => {\n+                RdjsonEmitter.emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::Junit => {\n+                JunitEmitter.emit(writer, diagnostics)?;\n+            }\n+            OutputFormat::Pylint => {\n+                PylintEmitter.emit(writer, diagnostics)?;\n+            }\n         }\n \n         writer.flush()?;\ndiff --git a/fortitude/src/rules/style/line_length.rs b/fortitude/src/rules/style/line_length.rs\nindex 2a2cec4..ac9dd69 100644\n--- a/fortitude/src/rules/style/line_length.rs\n+++ b/fortitude/src/rules/style/line_length.rs\n@@ -1,10 +1,10 @@\n+/// Defines rules that govern line length.\n use crate::settings::Settings;\n use crate::{FromStartEndLineCol, TextRule};\n use lazy_regex::regex_is_match;\n use ruff_diagnostics::{Diagnostic, Violation};\n use ruff_macros::{derive_message_formats, violation};\n use ruff_source_file::SourceFile;\n-/// Defines rules that govern line length.\n \n /// ## What does it do?\n /// Checks line length isn't too long\ndiff --git a/fortitude/src/rules/style/whitespace.rs b/fortitude/src/rules/style/whitespace.rs\nindex 29437f1..7851a03 100644\n--- a/fortitude/src/rules/style/whitespace.rs\n+++ b/fortitude/src/rules/style/whitespace.rs\n@@ -1,10 +1,10 @@\n+/// Defines rules that enforce widely accepted whitespace rules.\n use ruff_diagnostics::{Diagnostic, Violation};\n use ruff_macros::{derive_message_formats, violation};\n use ruff_source_file::SourceFile;\n \n use crate::settings::Settings;\n use crate::{FromStartEndLineCol, TextRule};\n-/// Defines rules that enforce widely accepted whitespace rules.\n \n /// ## What does it do?\n /// Checks for tailing whitespace\ndiff --git a/fortitude/src/rules/typing/implicit_typing.rs b/fortitude/src/rules/typing/implicit_typing.rs\nindex 57901f6..31eee6b 100644\n--- a/fortitude/src/rules/typing/implicit_typing.rs\n+++ b/fortitude/src/rules/typing/implicit_typing.rs\n@@ -1,3 +1,4 @@\n+/// Defines rules that raise errors if implicit typing is in use.\n use crate::ast::FortitudeNode;\n use crate::settings::Settings;\n use crate::{AstRule, FromAstNode};\n@@ -6,8 +7,6 @@ use ruff_macros::{derive_message_formats, violation};\n use ruff_source_file::SourceFile;\n use tree_sitter::Node;\n \n-/// Defines rules that raise errors if implicit typing is in use.\n-\n fn implicit_statement_is_none(node: &Node) -> bool {\n     if let Some(child) = node.child(1) {\n         return child.kind() == \"none\";\ndiff --git a/fortitude/src/settings.rs b/fortitude/src/settings.rs\nindex 0a7ffb3..75de8a7 100644\n--- a/fortitude/src/settings.rs\n+++ b/fortitude/src/settings.rs\n@@ -126,15 +126,15 @@ pub enum OutputFormat {\n     #[default]\n     Full,\n     Json,\n-    // JsonLines,\n-    // Junit,\n-    // Grouped,\n-    // Github,\n-    // Gitlab,\n-    // Pylint,\n-    // Rdjson,\n-    // Azure,\n-    // Sarif,\n+    JsonLines,\n+    Junit,\n+    Grouped,\n+    Github,\n+    Gitlab,\n+    Pylint,\n+    Rdjson,\n+    Azure,\n+    Sarif,\n }\n \n impl Display for OutputFormat {\n@@ -143,15 +143,15 @@ impl Display for OutputFormat {\n             Self::Concise => write!(f, \"concise\"),\n             Self::Full => write!(f, \"full\"),\n             Self::Json => write!(f, \"json\"),\n-            // Self::JsonLines => write!(f, \"json_lines\"),\n-            // Self::Junit => write!(f, \"junit\"),\n-            // Self::Grouped => write!(f, \"grouped\"),\n-            // Self::Github => write!(f, \"github\"),\n-            // Self::Gitlab => write!(f, \"gitlab\"),\n-            // Self::Pylint => write!(f, \"pylint\"),\n-            // Self::Rdjson => write!(f, \"rdjson\"),\n-            // Self::Azure => write!(f, \"azure\"),\n-            // Self::Sarif => write!(f, \"sarif\"),\n+            Self::JsonLines => write!(f, \"json_lines\"),\n+            Self::Junit => write!(f, \"junit\"),\n+            Self::Grouped => write!(f, \"grouped\"),\n+            Self::Github => write!(f, \"github\"),\n+            Self::Gitlab => write!(f, \"gitlab\"),\n+            Self::Pylint => write!(f, \"pylint\"),\n+            Self::Rdjson => write!(f, \"rdjson\"),\n+            Self::Azure => write!(f, \"azure\"),\n+            Self::Sarif => write!(f, \"sarif\"),\n         }\n     }\n }\n", "instance_id": "PlasmaFAIR__fortitude-177", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to add support for outputting results in SARIF format, a standard JSON schema for static analysis tools, with a specific use case of integrating with git forges for PR scanning. It provides a reference to the SARIF format and mentions an existing Rust library (`serde-sarif`) that can be used, which reduces ambiguity regarding implementation details. However, there are minor details missing, such as specific requirements for which SARIF fields must be populated, how to handle edge cases (e.g., missing file paths or invalid data), and whether there are specific git forge integration requirements beyond basic SARIF output. Additionally, there are no examples of expected input/output or detailed constraints on how the integration should behave in a pipeline. Despite these omissions, the goal and general approach are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is significant, as evidenced by the extensive diff provided, which includes modifications across multiple files and the addition of several new modules (`sarif.rs`, `azure.rs`, `github.rs`, etc.) to support various output formats, including SARIF. This indicates that the task involves integrating a new feature into an existing codebase, requiring understanding of the current message emission system and extending it with new emitters. Second, the number of technical concepts involved is moderate, including familiarity with Rust's serialization libraries (e.g., `serde`), JSON schema handling, and potentially domain-specific knowledge of static analysis output formats like SARIF. The use of an existing library (`serde-sarif`) simplifies some aspects, but mapping diagnostic data to SARIF structures still requires careful design. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest the need to handle file paths, source locations, and rule metadata, which could involve error handling for invalid or missing data. Finally, the changes impact multiple parts of the codebase (e.g., adding dependencies, updating emitter logic), but do not appear to fundamentally alter the system's architecture. Given these considerations\u2014a moderate scope of changes across files, a reasonable number of concepts to grasp, and some implicit edge case handling\u2014I assign a difficulty score of 0.55, placing it in the medium category (0.4-0.6), as it requires a solid understanding of the codebase and careful implementation but does not reach the complexity of a hard or very hard problem.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Confusing features in dataset returned by `get_torch_loader`\nI'm trying to use the dataloaders returned by `pinder.core.loader.dataset.get_torch_loader` for all-atom representations of the protein complexes. When I iterate over the keys present in a sample from one of these datasets, I get:\r\n```\r\nfrom pinder.core.loader.dataset import PinderDataset, get_torch_loader\r\ntrain, val = [get_torch_loader(PinderDataset(split=split)) for split in [\"train\", \"val\"]]\r\nfor key in train.dataset[0].keys():\r\n    print(key, type(train.dataset[0][key]))\r\n    if type(train.dataset[0][key]) == dict:\r\n        for key2 in train.dataset[0][key].keys():\r\n            print(\"    \", key2, type(train.dataset[0][key][key2]), train.dataset[0][key][key2].shape)\r\n\r\n### ------- Output ------- ###\r\n\r\ntarget_complex <class 'dict'>\r\n     atom_types <class 'torch.Tensor'> torch.Size([2556, 12])\r\n     residue_types <class 'torch.Tensor'> torch.Size([2556, 1])\r\n     atom_coordinates <class 'torch.Tensor'> torch.Size([2556, 3])\r\n     residue_coordinates <class 'torch.Tensor'> torch.Size([2556, 3])\r\n     residue_ids <class 'torch.Tensor'> torch.Size([2556])\r\n     chain_ids <class 'torch.Tensor'> torch.Size([2556])\r\nfeature_complex <class 'dict'>\r\n     atom_types <class 'torch.Tensor'> torch.Size([2556, 12])\r\n     residue_types <class 'torch.Tensor'> torch.Size([2556, 1])\r\n     atom_coordinates <class 'torch.Tensor'> torch.Size([2556, 3])\r\n     residue_coordinates <class 'torch.Tensor'> torch.Size([2556, 3])\r\n     residue_ids <class 'torch.Tensor'> torch.Size([2556])\r\n     chain_ids <class 'torch.Tensor'> torch.Size([2556])\r\nid <class 'str'>\r\nsample_id <class 'str'>\r\ntarget_id <class 'str'>\r\n```\r\nI have the following questions:\r\n\r\n**What is the difference between `atom_coordinates` and `residue_coordinates`?**\r\n\r\nFor at least the first 10 examples in the train dataset, I find that\r\n```\r\nfor i in range(10):\r\n    print((train.dataset[i]['feature_complex']['residue_coordinates'] == train.dataset[i]['feature_complex']['atom_coordinates']).all())\r\n```\r\nreturns `True`.\r\n\r\n**Can the loader be modified to also return atom name information?**\r\n\r\nThe `atom_types` tensor provides only a one-hot encoding of element type, for example, running\r\n```\r\ntrain.dataset[0]['target_complex']['atom_types'].unique(dim=0)\r\n```\r\nreturns\r\n```\r\ntensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\r\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\nThis is confusing when compared to the `Structure` class, in which `atom_name` is encoded as elements of `constants.BB_ATOMS` and `constants.SC_ATOMS` (e.g. 'N', 'CA', 'C', 'O', 'CB', 'CG', etc.).\r\n\r\n**It would be useful/less confusing if the dataloader returned by `get_torch_loader` contained information that corresponded more closely to the data provided in the `Structure` class.**\n", "patch": "diff --git a/README.md b/README.md\nindex 66550ec..e8be078 100644\n--- a/README.md\n+++ b/README.md\n@@ -375,7 +375,8 @@ Each model decoy should have exactly two chains: {R, L} for {Receptor, Ligand},\n }\n ```\n \n-For more details on the implementations of the eval metrics, see the [eval docs](examples/eval/).\n+For more details on the implementations of the eval metrics, see the [eval tutorial](https://pinder-org.github.io/pinder/pinder-eval.html), [API docs](https://pinder-org.github.io/pinder/source/pinder.eval.dockq.html#) and [eval FAQ](https://pinder-org.github.io/pinder/faq.html#how-can-i-use-the-evaluation-harness-outside-of-a-pinder-context).\n+\n For more details on leaderboard generation, see the [MethodMetrics](src/pinder-eval/pinder/eval/dockq/method.py) implementation.\n \n \ndiff --git a/examples/pinder-loader.ipynb b/examples/pinder-loader.ipynb\nindex 47a563f..46af8a5 100644\n--- a/examples/pinder-loader.ipynb\n+++ b/examples/pinder-loader.ipynb\n@@ -435,7 +435,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 7,\n    \"id\": \"6e09ccc9-215f-4d8e-a08f-1de96bb42131\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -445,7 +445,7 @@\n        \"PinderLoader(split=test, monomers=holo, systems=180)\"\n       ]\n      },\n-     \"execution_count\": 6,\n+     \"execution_count\": 7,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -481,7 +481,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 8,\n    \"id\": \"f7bcbe6e-4bd3-435c-9ff0-cf611f6bf9cf\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -491,7 +491,7 @@\n        \"180\"\n       ]\n      },\n-     \"execution_count\": 7,\n+     \"execution_count\": 8,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -502,7 +502,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 9,\n    \"id\": \"fecd34da-c91d-400c-bbda-64d6cccb8e0b\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -521,7 +521,7 @@\n        \" pinder.core.loader.structure.Structure)\"\n       ]\n      },\n-     \"execution_count\": 8,\n+     \"execution_count\": 9,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -535,7 +535,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 10,\n    \"id\": \"bdb3327d-26d2-46fc-95ea-de2ea2fc0516\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -543,7 +543,7 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 180/180 [01:14<00:00,  2.43it/s]\\n\"\n+      \"100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 180/180 [01:12<00:00,  2.50it/s]\\n\"\n      ]\n     }\n    ],\n@@ -565,7 +565,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 11,\n    \"id\": \"77d0f734-508a-4b1e-a4f3-acb8590869a0\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -575,7 +575,7 @@\n        \"set()\"\n       ]\n      },\n-     \"execution_count\": 10,\n+     \"execution_count\": 11,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -601,7 +601,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 12,\n    \"id\": \"3644c869-da61-4620-a054-2a3ad163f3f8\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -611,7 +611,7 @@\n        \"True\"\n       ]\n      },\n-     \"execution_count\": 11,\n+     \"execution_count\": 12,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -637,7 +637,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 13,\n    \"id\": \"6b653c61-cebd-4ace-baf8-a27f1e011466\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -645,8 +645,8 @@\n      \"name\": \"stdout\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"[PosixPath('/var/folders/tt/x223wxwj6dzg3vjjgc_6y5bm0000gn/T/tmpbe5qjtfe/117e__A1_P00817--117e__B1_P00817/af__P00817.pdb')]\\n\",\n-      \"[PosixPath('/var/folders/tt/x223wxwj6dzg3vjjgc_6y5bm0000gn/T/tmpbe5qjtfe/1df0__A1_Q07009--1df0__B1_Q64537/af__Q07009.pdb'), PosixPath('/var/folders/tt/x223wxwj6dzg3vjjgc_6y5bm0000gn/T/tmpbe5qjtfe/1df0__A1_Q07009--1df0__B1_Q64537/af__Q64537.pdb')]\\n\"\n+      \"[PosixPath('/var/folders/tt/x223wxwj6dzg3vjjgc_6y5bm0000gn/T/tmpgrbzq_is/117e__A1_P00817--117e__B1_P00817/af__P00817.pdb')]\\n\",\n+      \"[PosixPath('/var/folders/tt/x223wxwj6dzg3vjjgc_6y5bm0000gn/T/tmpgrbzq_is/1df0__A1_Q07009--1df0__B1_Q64537/af__Q07009.pdb'), PosixPath('/var/folders/tt/x223wxwj6dzg3vjjgc_6y5bm0000gn/T/tmpgrbzq_is/1df0__A1_Q07009--1df0__B1_Q64537/af__Q64537.pdb')]\\n\"\n      ]\n     }\n    ],\n@@ -738,7 +738,8 @@\n     \"* `residue_types`\\n\",\n     \"* `residue_ids`\\n\",\n     \"\\n\",\n-    \"You can choose to use a different representation by overriding the default values of `transform` and `target_transform`.\\n\",\n+    \"**Note: You can choose to use a different representation by overriding the default values of `transform` and `target_transform`. The default transform is the structure2tensor_transform defined in pinder.core.loader.dataset. \\n\",\n+    \"It simply takes a `Structure` object and returns a dictionary with string keys and tensor values.**\\n\",\n     \" \\n\",\n     \"It leverages the `PinderLoader` to apply optional filters and/or transforms, provide an interface for sampling alternative monomers, and exposes `transform` and `target_transform` arguments used by the torch Dataset API. \\n\",\n     \"\\n\",\n@@ -747,24 +748,24 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 14,\n    \"id\": \"9658c6aa-5720-4c0f-98ae-b79d34ad2754\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"<pinder.core.loader.dataset.PinderDataset at 0x1ecf92350>\"\n+       \"<pinder.core.loader.dataset.PinderDataset at 0x1fc1f76a0>\"\n       ]\n      },\n-     \"execution_count\": 2,\n+     \"execution_count\": 14,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n    ],\n    \"source\": [\n     \"from pinder.core.loader import filters, transforms\\n\",\n-    \"from pinder.core.loader.dataset import PinderDataset\\n\",\n+    \"from pinder.core.loader.dataset import PinderDataset, structure2tensor_transform\\n\",\n     \"\\n\",\n     \"base_filters = [\\n\",\n     \"    filters.FilterByMissingHolo(),\\n\",\n@@ -792,6 +793,9 @@\n     \"    structure_transforms_target=structure_transforms,\\n\",\n     \"    # Apply to the feature complex\\n\",\n     \"    structure_transforms_feature=structure_transforms,\\n\",\n+    \"    # This is the default transform if not specified\\n\",\n+    \"    transform=structure2tensor_transform,\\n\",\n+    \"    target_transform=structure2tensor_transform,\\n\",\n     \")\\n\",\n     \"assert len(train_dataset) == len(get_index().query('split == \\\"train\\\"'))\\n\",\n     \"\\n\",\n@@ -834,20 +838,27 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 15,\n    \"id\": \"383244d7-1a85-4732-9dd4-3455fe4cec88\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"{'target_complex': {'atom_types': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n+       \"{'target_complex': {'atom_types': tensor([[0.],\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          ...,\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          [3.]]),\\n\",\n+       \"  'element_types': tensor([[3.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n        \"          ...,\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [0., 0., 1.,  ..., 0., 0., 0.]]),\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [2.]]),\\n\",\n        \"  'residue_types': tensor([[16.],\\n\",\n        \"          [16.],\\n\",\n        \"          [16.],\\n\",\n@@ -862,22 +873,351 @@\n        \"          [177.7620, 463.8650, 166.9020],\\n\",\n        \"          [177.4130, 465.0800, 167.7550],\\n\",\n        \"          [176.8000, 464.9490, 168.8150]]),\\n\",\n-       \"  'residue_coordinates': tensor([[131.7500, 429.3090, 163.5360],\\n\",\n-       \"          [132.6810, 428.2520, 163.1550],\\n\",\n-       \"          [133.5150, 428.6750, 161.9500],\\n\",\n-       \"          ...,\\n\",\n-       \"          [177.7620, 463.8650, 166.9020],\\n\",\n-       \"          [177.4130, 465.0800, 167.7550],\\n\",\n-       \"          [176.8000, 464.9490, 168.8150]]),\\n\",\n+       \"  'residue_coordinates': tensor([[132.6810, 428.2520, 163.1550],\\n\",\n+       \"          [133.5560, 429.2490, 159.5910],\\n\",\n+       \"          [133.8750, 432.8980, 160.6290],\\n\",\n+       \"          [136.1110, 431.8050, 163.5130],\\n\",\n+       \"          [138.3420, 429.8920, 161.0830],\\n\",\n+       \"          [138.5230, 432.9090, 158.7600],\\n\",\n+       \"          [139.4710, 435.1730, 161.6750],\\n\",\n+       \"          [142.1200, 432.6310, 162.6740],\\n\",\n+       \"          [143.5890, 432.7500, 159.1600],\\n\",\n+       \"          [143.6190, 436.5600, 159.1540],\\n\",\n+       \"          [145.2600, 436.8040, 162.5830],\\n\",\n+       \"          [147.8380, 434.1320, 161.7330],\\n\",\n+       \"          [148.7390, 435.8920, 158.4790],\\n\",\n+       \"          [149.0640, 439.2630, 160.2240],\\n\",\n+       \"          [151.2220, 437.7670, 162.9840],\\n\",\n+       \"          [153.4710, 435.8030, 160.6120],\\n\",\n+       \"          [153.9420, 438.9180, 158.4710],\\n\",\n+       \"          [156.1140, 440.2660, 161.3150],\\n\",\n+       \"          [158.3330, 437.1860, 161.4250],\\n\",\n+       \"          [161.8090, 436.4120, 160.1100],\\n\",\n+       \"          [161.1120, 432.8370, 158.9350],\\n\",\n+       \"          [157.4020, 432.0710, 159.3930],\\n\",\n+       \"          [156.5290, 428.3870, 159.0310],\\n\",\n+       \"          [154.0400, 428.1160, 156.1500],\\n\",\n+       \"          [153.3520, 424.6590, 154.7510],\\n\",\n+       \"          [152.8370, 424.8210, 150.9870],\\n\",\n+       \"          [150.9700, 422.4160, 148.7320],\\n\",\n+       \"          [153.5900, 422.6860, 145.9670],\\n\",\n+       \"          [157.3050, 423.3410, 145.6600],\\n\",\n+       \"          [158.4860, 426.6730, 144.2700],\\n\",\n+       \"          [160.7730, 427.0880, 141.2590],\\n\",\n+       \"          [163.9630, 429.0530, 141.9600],\\n\",\n+       \"          [166.4230, 427.4330, 139.5550],\\n\",\n+       \"          [167.6170, 430.8330, 138.3250],\\n\",\n+       \"          [170.5180, 431.9210, 140.5320],\\n\",\n+       \"          [169.7800, 435.6680, 140.3160],\\n\",\n+       \"          [167.6790, 437.8790, 142.6110],\\n\",\n+       \"          [167.3350, 435.1910, 145.2890],\\n\",\n+       \"          [168.9090, 437.3000, 148.0580],\\n\",\n+       \"          [166.1860, 439.9440, 148.5750],\\n\",\n+       \"          [164.1010, 438.1300, 151.1890],\\n\",\n+       \"          [163.0190, 438.8690, 154.7450],\\n\",\n+       \"          [164.6750, 435.6560, 155.9860],\\n\",\n+       \"          [168.1170, 436.6550, 154.6330],\\n\",\n+       \"          [169.8040, 437.0930, 158.0010],\\n\",\n+       \"          [173.2680, 438.4850, 158.5750],\\n\",\n+       \"          [174.9270, 440.8490, 156.1460],\\n\",\n+       \"          [175.9170, 440.8010, 152.5000],\\n\",\n+       \"          [179.6060, 441.0910, 153.4330],\\n\",\n+       \"          [180.8000, 439.9830, 156.8710],\\n\",\n+       \"          [184.2540, 439.9980, 158.4580],\\n\",\n+       \"          [185.5840, 437.2970, 160.7670],\\n\",\n+       \"          [186.4140, 439.9000, 163.4480],\\n\",\n+       \"          [182.9430, 441.4570, 163.7500],\\n\",\n+       \"          [181.4930, 442.0910, 167.2100],\\n\",\n+       \"          [178.0780, 440.5360, 167.9060],\\n\",\n+       \"          [176.1650, 440.9290, 171.1790],\\n\",\n+       \"          [173.0530, 438.9530, 172.1310],\\n\",\n+       \"          [170.3990, 440.6990, 174.2170],\\n\",\n+       \"          [166.7880, 440.0710, 175.2260],\\n\",\n+       \"          [164.4480, 442.1010, 173.0340],\\n\",\n+       \"          [160.9780, 443.6110, 173.3810],\\n\",\n+       \"          [158.8330, 443.9000, 170.2730],\\n\",\n+       \"          [160.1820, 442.9360, 166.8360],\\n\",\n+       \"          [163.5010, 444.7230, 166.1630],\\n\",\n+       \"          [164.6310, 442.4290, 163.3160],\\n\",\n+       \"          [166.6330, 444.2560, 160.6240],\\n\",\n+       \"          [166.1700, 447.5210, 162.5220],\\n\",\n+       \"          [168.1350, 449.9450, 164.6440],\\n\",\n+       \"          [168.0040, 449.0470, 168.3240],\\n\",\n+       \"          [168.4210, 451.0540, 171.5220],\\n\",\n+       \"          [169.0440, 450.0360, 175.1300],\\n\",\n+       \"          [166.1650, 450.3680, 177.6100],\\n\",\n+       \"          [166.7800, 449.8300, 181.3340],\\n\",\n+       \"          [163.8480, 448.9150, 183.5760],\\n\",\n+       \"          [163.3500, 443.9770, 187.9750],\\n\",\n+       \"          [165.0240, 442.4930, 184.8870],\\n\",\n+       \"          [168.1120, 443.0980, 182.7810],\\n\",\n+       \"          [168.1280, 445.9210, 180.2050],\\n\",\n+       \"          [166.7230, 445.0130, 176.8030],\\n\",\n+       \"          [167.0690, 446.1330, 173.1930],\\n\",\n+       \"          [164.0790, 447.8390, 171.5780],\\n\",\n+       \"          [163.4630, 449.0630, 168.0350],\\n\",\n+       \"          [164.4160, 452.7320, 168.0130],\\n\",\n+       \"          [166.7460, 455.3190, 166.5790],\\n\",\n+       \"          [167.5460, 459.0090, 166.8440],\\n\",\n+       \"          [170.2010, 459.9620, 169.3810],\\n\",\n+       \"          [169.5150, 456.8220, 171.4570],\\n\",\n+       \"          [170.7440, 454.3880, 168.7860],\\n\",\n+       \"          [172.8900, 451.6810, 170.3850],\\n\",\n+       \"          [173.2420, 449.3000, 167.4490],\\n\",\n+       \"          [171.4700, 447.3670, 164.7150],\\n\",\n+       \"          [169.7840, 443.9650, 164.9670],\\n\",\n+       \"          [170.8880, 441.4120, 162.3680],\\n\",\n+       \"          [169.3320, 438.1640, 163.6510],\\n\",\n+       \"          [166.7170, 436.9060, 166.1030],\\n\",\n+       \"          [166.2830, 433.5290, 167.7900],\\n\",\n+       \"          [162.5490, 433.4260, 168.5040],\\n\",\n+       \"          [162.6440, 430.3540, 170.7620],\\n\",\n+       \"          [164.9490, 432.1120, 173.2350],\\n\",\n+       \"          [163.6530, 435.5760, 172.2210],\\n\",\n+       \"          [167.2540, 436.7450, 171.8300],\\n\",\n+       \"          [168.3710, 439.3650, 169.3060],\\n\",\n+       \"          [171.8910, 439.6680, 167.8930],\\n\",\n+       \"          [173.1360, 443.2500, 167.5870],\\n\",\n+       \"          [176.0990, 444.7950, 165.8030],\\n\",\n+       \"          [177.1670, 447.8660, 167.8270],\\n\",\n+       \"          [177.4160, 451.2430, 166.1360],\\n\",\n+       \"          [181.1790, 451.0810, 166.7480],\\n\",\n+       \"          [181.3560, 448.4350, 163.9980],\\n\",\n+       \"          [180.6020, 448.8920, 160.3060],\\n\",\n+       \"          [177.8440, 446.8990, 158.6170],\\n\",\n+       \"          [176.7540, 446.3160, 155.0150],\\n\",\n+       \"          [173.3820, 444.6900, 154.4770],\\n\",\n+       \"          [169.6430, 444.9660, 153.9760],\\n\",\n+       \"          [167.6080, 446.9450, 156.5140],\\n\",\n+       \"          [163.9630, 447.9270, 156.8800],\\n\",\n+       \"          [163.1520, 451.2160, 155.1430],\\n\",\n+       \"          [160.3870, 453.6570, 156.0420],\\n\",\n+       \"          [158.9330, 453.6920, 152.5150],\\n\",\n+       \"          [159.8050, 453.2460, 148.8360],\\n\",\n+       \"          [161.2560, 456.7540, 148.4440],\\n\",\n+       \"          [164.9420, 455.8330, 148.8520],\\n\",\n+       \"          [167.3780, 455.9500, 145.9260],\\n\",\n+       \"          [171.0380, 455.0610, 145.5060],\\n\",\n+       \"          [173.5830, 457.5340, 146.9270],\\n\",\n+       \"          [171.2360, 458.9430, 149.5740],\\n\",\n+       \"          [171.8150, 459.8640, 153.2130],\\n\",\n+       \"          [169.5910, 457.9090, 155.6000],\\n\",\n+       \"          [168.4960, 458.3960, 159.2100],\\n\",\n+       \"          [167.0870, 455.9020, 161.7110],\\n\",\n+       \"          [163.6430, 457.0390, 162.8610],\\n\",\n+       \"          [161.8700, 456.3550, 166.1660],\\n\",\n+       \"          [160.7490, 452.9260, 164.8950],\\n\",\n+       \"          [164.2440, 451.8750, 163.8110],\\n\",\n+       \"          [163.4490, 452.3040, 160.1090],\\n\",\n+       \"          [165.6820, 454.0140, 157.5590],\\n\",\n+       \"          [164.2940, 457.2310, 156.0670],\\n\",\n+       \"          [165.6810, 459.7290, 153.5780],\\n\",\n+       \"          [167.3870, 462.7790, 155.0760],\\n\",\n+       \"          [170.2060, 465.8340, 161.8360],\\n\",\n+       \"          [172.1620, 462.7680, 160.6890],\\n\",\n+       \"          [174.0280, 461.7040, 163.8180],\\n\",\n+       \"          [175.2110, 458.5660, 161.9930],\\n\",\n+       \"          [176.5410, 458.2410, 158.4450],\\n\",\n+       \"          [174.5550, 455.7530, 156.3540],\\n\",\n+       \"          [174.3580, 455.6030, 152.5600],\\n\",\n+       \"          [171.8590, 453.6260, 150.4910],\\n\",\n+       \"          [173.6300, 451.2730, 148.0810],\\n\",\n+       \"          [170.4820, 450.1750, 146.2000],\\n\",\n+       \"          [166.8520, 451.1980, 145.7810],\\n\",\n+       \"          [164.0730, 450.1880, 148.1490],\\n\",\n+       \"          [162.6710, 446.7750, 147.1980],\\n\",\n+       \"          [159.1420, 445.8080, 148.2240],\\n\",\n+       \"          [158.3160, 442.3640, 149.5950],\\n\",\n+       \"          [155.0300, 443.1470, 151.4050],\\n\",\n+       \"          [152.6800, 446.0960, 151.8500],\\n\",\n+       \"          [155.0610, 447.4890, 154.4910],\\n\",\n+       \"          [158.2010, 445.3460, 154.0190],\\n\",\n+       \"          [160.8090, 447.5340, 152.2870],\\n\",\n+       \"          [164.4130, 446.3150, 152.1360],\\n\",\n+       \"          [167.3360, 448.5380, 151.1380],\\n\",\n+       \"          [171.0310, 447.6630, 150.9500],\\n\",\n+       \"          [172.8160, 450.1440, 153.2260],\\n\",\n+       \"          [176.3550, 450.4540, 154.5850],\\n\",\n+       \"          [176.6310, 452.0780, 158.0240],\\n\",\n+       \"          [180.1600, 453.0970, 159.0310],\\n\",\n+       \"          [179.9110, 454.9710, 162.3160],\\n\",\n+       \"          [178.7370, 458.4030, 163.4610],\\n\",\n+       \"          [179.1350, 461.9150, 162.0830],\\n\",\n+       \"          [180.3160, 464.9940, 163.9950],\\n\",\n+       \"          [195.1700, 454.0210, 194.1160],\\n\",\n+       \"          [196.4220, 450.5570, 193.1610],\\n\",\n+       \"          [195.0530, 447.5260, 191.3340],\\n\",\n+       \"          [195.7530, 445.4320, 194.4410],\\n\",\n+       \"          [195.6760, 446.8960, 197.9710],\\n\",\n+       \"          [199.1230, 447.9300, 199.1640],\\n\",\n+       \"          [198.4070, 447.0140, 202.7840],\\n\",\n+       \"          [198.1130, 443.3080, 203.5090],\\n\",\n+       \"          [195.0350, 441.7530, 205.1060],\\n\",\n+       \"          [194.7040, 440.1720, 208.5300],\\n\",\n+       \"          [191.7420, 437.9460, 207.6670],\\n\",\n+       \"          [190.9460, 435.0990, 205.3140],\\n\",\n+       \"          [188.0870, 435.6760, 202.8880],\\n\",\n+       \"          [184.8300, 433.7320, 203.1830],\\n\",\n+       \"          [184.1270, 431.8260, 199.9570],\\n\",\n+       \"          [180.5450, 430.8500, 200.7460],\\n\",\n+       \"          [178.2470, 433.3320, 199.0220],\\n\",\n+       \"          [178.2770, 433.9260, 195.2680],\\n\",\n+       \"          [176.3170, 437.2030, 195.0280],\\n\",\n+       \"          [177.7690, 440.7330, 194.9660],\\n\",\n+       \"          [181.3350, 439.3910, 195.1330],\\n\",\n+       \"          [182.5820, 441.3460, 192.0940],\\n\",\n+       \"          [182.3530, 444.9560, 193.3610],\\n\",\n+       \"          [186.0870, 445.2240, 194.0290],\\n\",\n+       \"          [188.7230, 447.5570, 192.6190],\\n\",\n+       \"          [190.8760, 444.5870, 191.5490],\\n\",\n+       \"          [188.2670, 443.5230, 188.9590],\\n\",\n+       \"          [190.1360, 444.3010, 185.7360],\\n\",\n+       \"          [188.8140, 444.1990, 182.1980],\\n\",\n+       \"          [185.1300, 444.1560, 181.3470],\\n\",\n+       \"          [182.1290, 442.2030, 182.6060],\\n\",\n+       \"          [181.3330, 441.2700, 178.9860],\\n\",\n+       \"          [184.3690, 440.6570, 176.7700],\\n\",\n+       \"          [184.4240, 439.7520, 173.0740],\\n\",\n+       \"          [187.1540, 437.5560, 171.6020],\\n\",\n+       \"          [187.5170, 439.9750, 168.6500],\\n\",\n+       \"          [188.4180, 443.0500, 170.7240],\\n\",\n+       \"          [191.4360, 445.2100, 169.8910],\\n\",\n+       \"          [193.7480, 445.6510, 172.8940],\\n\",\n+       \"          [196.8200, 447.8990, 173.0280],\\n\",\n+       \"          [199.4910, 448.6120, 175.6310],\\n\",\n+       \"          [201.2080, 451.9090, 176.3780],\\n\",\n+       \"          [203.6110, 453.4740, 178.8680],\\n\",\n+       \"          [201.6130, 454.9210, 181.7570],\\n\",\n+       \"          [202.0690, 458.3840, 183.2730],\\n\",\n+       \"          [200.4970, 458.3690, 186.7180],\\n\",\n+       \"          [197.7960, 455.8130, 187.5240],\\n\",\n+       \"          [195.1460, 455.8470, 184.7720],\\n\",\n+       \"          [193.6160, 452.5740, 185.9790],\\n\",\n+       \"          [189.8600, 452.3800, 185.3150],\\n\",\n+       \"          [189.9440, 455.8580, 183.7680],\\n\",\n+       \"          [189.7400, 457.4310, 180.3410],\\n\",\n+       \"          [193.1630, 457.9480, 178.8100],\\n\",\n+       \"          [194.6770, 460.5170, 176.4640],\\n\",\n+       \"          [197.9720, 460.6430, 174.6010],\\n\",\n+       \"          [200.7710, 462.5710, 176.3210],\\n\",\n+       \"          [203.7970, 463.7400, 174.3330],\\n\",\n+       \"          [207.1150, 464.5820, 175.9770],\\n\",\n+       \"          [210.1390, 466.5260, 174.8060],\\n\",\n+       \"          [210.8850, 463.9390, 172.1200],\\n\",\n+       \"          [213.6440, 461.8050, 173.6150],\\n\",\n+       \"          [211.1280, 459.2430, 174.9140],\\n\",\n+       \"          [208.0390, 457.7610, 173.2930],\\n\",\n+       \"          [204.4770, 458.9110, 173.9310],\\n\",\n+       \"          [202.5240, 457.8000, 177.0030],\\n\",\n+       \"          [198.9190, 457.7220, 178.2440],\\n\",\n+       \"          [197.5730, 459.9040, 181.0500],\\n\",\n+       \"          [194.2310, 460.2350, 182.8060],\\n\",\n+       \"          [192.1140, 462.6430, 180.7850],\\n\",\n+       \"          [189.0250, 463.1720, 178.7000],\\n\",\n+       \"          [187.1240, 465.8360, 176.8220],\\n\",\n+       \"          [188.2850, 466.3320, 173.2420],\\n\",\n+       \"          [191.5580, 464.4520, 173.8600],\\n\",\n+       \"          [189.9950, 461.1280, 174.9010],\\n\",\n+       \"          [191.8440, 458.2970, 173.1540],\\n\",\n+       \"          [190.4670, 455.2690, 174.9900],\\n\",\n+       \"          [189.7380, 453.5820, 178.3070],\\n\",\n+       \"          [192.3390, 451.8830, 180.5000],\\n\",\n+       \"          [191.2020, 448.5340, 181.9050],\\n\",\n+       \"          [194.3800, 447.0280, 183.4050],\\n\",\n+       \"          [197.8700, 447.9650, 184.5850],\\n\",\n+       \"          [200.9840, 445.8270, 184.9660],\\n\",\n+       \"          [202.8290, 447.6220, 187.7590],\\n\",\n+       \"          [206.2120, 445.9180, 187.3400],\\n\",\n+       \"          [206.6040, 447.1310, 183.7460],\\n\",\n+       \"          [204.4430, 450.2380, 184.1830],\\n\",\n+       \"          [202.2310, 449.1370, 181.2890],\\n\",\n+       \"          [198.6660, 450.3790, 180.7720],\\n\",\n+       \"          [196.2310, 448.2490, 178.7680],\\n\",\n+       \"          [194.0550, 450.4240, 176.5260],\\n\",\n+       \"          [190.8490, 449.7090, 174.6070],\\n\",\n+       \"          [190.4360, 452.3080, 171.8240],\\n\",\n+       \"          [187.3830, 454.5590, 171.7980],\\n\",\n+       \"          [186.4670, 452.9730, 168.4480],\\n\",\n+       \"          [185.9050, 449.6480, 170.2580],\\n\",\n+       \"          [182.8890, 448.8910, 172.4330],\\n\",\n+       \"          [183.5680, 448.6030, 176.1630],\\n\",\n+       \"          [181.3330, 447.5400, 179.0590],\\n\",\n+       \"          [182.7340, 447.5270, 182.5670],\\n\",\n+       \"          [183.3280, 449.2260, 185.8920],\\n\",\n+       \"          [184.8760, 452.6980, 185.6990],\\n\",\n+       \"          [185.7140, 455.4170, 188.2030],\\n\",\n+       \"          [182.9420, 457.9530, 188.8300],\\n\",\n+       \"          [183.2930, 461.5650, 189.9350],\\n\",\n+       \"          [182.7580, 462.1320, 193.6660],\\n\",\n+       \"          [180.0620, 464.7490, 193.1030],\\n\",\n+       \"          [176.4120, 463.9030, 192.5790],\\n\",\n+       \"          [177.1550, 460.1730, 192.6930],\\n\",\n+       \"          [173.8180, 459.5980, 194.4390],\\n\",\n+       \"          [172.0770, 461.1030, 191.4040],\\n\",\n+       \"          [173.6950, 458.5410, 189.0910],\\n\",\n+       \"          [171.0060, 456.0190, 188.1270],\\n\",\n+       \"          [170.8210, 453.3390, 185.4210],\\n\",\n+       \"          [170.0180, 454.8270, 182.0350],\\n\",\n+       \"          [171.8690, 458.1080, 182.6590],\\n\",\n+       \"          [174.2320, 459.6520, 180.1210],\\n\",\n+       \"          [177.8640, 459.9130, 181.2230],\\n\",\n+       \"          [180.8780, 461.8410, 179.9490],\\n\",\n+       \"          [184.5650, 461.8070, 180.8430],\\n\",\n+       \"          [186.0650, 464.7990, 182.6440],\\n\",\n+       \"          [189.5780, 466.2740, 182.6170],\\n\",\n+       \"          [190.7120, 463.6340, 185.1390],\\n\",\n+       \"          [189.2980, 460.6630, 183.2290],\\n\",\n+       \"          [186.3560, 460.3000, 185.6240],\\n\",\n+       \"          [182.7840, 459.6590, 184.5210],\\n\",\n+       \"          [180.2130, 462.3320, 185.3590],\\n\",\n+       \"          [176.5320, 462.8410, 184.6140],\\n\",\n+       \"          [175.8580, 465.0300, 181.5790],\\n\",\n+       \"          [178.1930, 465.7110, 174.9000],\\n\",\n+       \"          [180.9150, 464.1190, 172.7670],\\n\",\n+       \"          [181.2510, 460.5420, 174.1130],\\n\",\n+       \"          [178.4430, 457.9660, 174.2140],\\n\",\n+       \"          [178.5420, 456.4790, 177.7180],\\n\",\n+       \"          [175.4730, 455.1590, 179.5500],\\n\",\n+       \"          [175.4670, 454.0690, 183.1870],\\n\",\n+       \"          [174.1210, 450.5430, 183.6830],\\n\",\n+       \"          [173.9850, 450.6340, 187.5030],\\n\",\n+       \"          [174.3640, 452.9240, 190.5240],\\n\",\n+       \"          [177.5140, 454.2200, 192.1820],\\n\",\n+       \"          [179.3060, 451.6570, 194.3540],\\n\",\n+       \"          [181.4890, 453.2110, 197.0570],\\n\",\n+       \"          [184.7390, 451.3110, 197.6330],\\n\",\n+       \"          [186.4870, 454.0230, 199.6690],\\n\",\n+       \"          [185.5270, 457.3810, 201.1790],\\n\",\n+       \"          [186.7910, 459.0050, 197.9610],\\n\",\n+       \"          [186.6010, 456.0430, 195.5320],\\n\",\n+       \"          [183.3680, 455.3620, 193.6270],\\n\",\n+       \"          [182.9110, 452.9430, 190.7280],\\n\",\n+       \"          [180.0190, 452.5230, 188.3060],\\n\",\n+       \"          [179.0910, 450.0250, 185.5960],\\n\",\n+       \"          [178.9850, 451.7840, 182.2280],\\n\",\n+       \"          [179.0510, 451.0080, 178.5100],\\n\",\n+       \"          [181.0530, 453.1550, 176.0780],\\n\",\n+       \"          [180.1320, 452.7170, 172.4060],\\n\",\n+       \"          [182.0570, 455.4530, 170.6120],\\n\",\n+       \"          [181.8140, 459.1820, 170.0770],\\n\",\n+       \"          [178.5990, 461.0580, 169.3130],\\n\",\n+       \"          [177.7620, 463.8650, 166.9020]]),\\n\",\n        \"  'residue_ids': tensor([  4.,   4.,   4.,  ..., 182., 182., 182.]),\\n\",\n        \"  'chain_ids': tensor([0., 0., 0.,  ..., 1., 1., 1.])},\\n\",\n-       \" 'feature_complex': {'atom_types': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n+       \" 'feature_complex': {'atom_types': tensor([[0.],\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          ...,\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          [3.]]),\\n\",\n+       \"  'element_types': tensor([[3.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n        \"          ...,\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [0., 0., 1.,  ..., 0., 0., 0.]]),\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [2.]]),\\n\",\n        \"  'residue_types': tensor([[16.],\\n\",\n        \"          [16.],\\n\",\n        \"          [16.],\\n\",\n@@ -892,13 +1232,335 @@\n        \"          [177.7620, 463.8650, 166.9020],\\n\",\n        \"          [177.4130, 465.0800, 167.7550],\\n\",\n        \"          [176.8000, 464.9490, 168.8150]]),\\n\",\n-       \"  'residue_coordinates': tensor([[131.7500, 429.3090, 163.5360],\\n\",\n-       \"          [132.6810, 428.2520, 163.1550],\\n\",\n-       \"          [133.5150, 428.6750, 161.9500],\\n\",\n-       \"          ...,\\n\",\n-       \"          [177.7620, 463.8650, 166.9020],\\n\",\n-       \"          [177.4130, 465.0800, 167.7550],\\n\",\n-       \"          [176.8000, 464.9490, 168.8150]]),\\n\",\n+       \"  'residue_coordinates': tensor([[132.6810, 428.2520, 163.1550],\\n\",\n+       \"          [133.5560, 429.2490, 159.5910],\\n\",\n+       \"          [133.8750, 432.8980, 160.6290],\\n\",\n+       \"          [136.1110, 431.8050, 163.5130],\\n\",\n+       \"          [138.3420, 429.8920, 161.0830],\\n\",\n+       \"          [138.5230, 432.9090, 158.7600],\\n\",\n+       \"          [139.4710, 435.1730, 161.6750],\\n\",\n+       \"          [142.1200, 432.6310, 162.6740],\\n\",\n+       \"          [143.5890, 432.7500, 159.1600],\\n\",\n+       \"          [143.6190, 436.5600, 159.1540],\\n\",\n+       \"          [145.2600, 436.8040, 162.5830],\\n\",\n+       \"          [147.8380, 434.1320, 161.7330],\\n\",\n+       \"          [148.7390, 435.8920, 158.4790],\\n\",\n+       \"          [149.0640, 439.2630, 160.2240],\\n\",\n+       \"          [151.2220, 437.7670, 162.9840],\\n\",\n+       \"          [153.4710, 435.8030, 160.6120],\\n\",\n+       \"          [153.9420, 438.9180, 158.4710],\\n\",\n+       \"          [156.1140, 440.2660, 161.3150],\\n\",\n+       \"          [158.3330, 437.1860, 161.4250],\\n\",\n+       \"          [161.8090, 436.4120, 160.1100],\\n\",\n+       \"          [161.1120, 432.8370, 158.9350],\\n\",\n+       \"          [157.4020, 432.0710, 159.3930],\\n\",\n+       \"          [156.5290, 428.3870, 159.0310],\\n\",\n+       \"          [154.0400, 428.1160, 156.1500],\\n\",\n+       \"          [153.3520, 424.6590, 154.7510],\\n\",\n+       \"          [152.8370, 424.8210, 150.9870],\\n\",\n+       \"          [150.9700, 422.4160, 148.7320],\\n\",\n+       \"          [153.5900, 422.6860, 145.9670],\\n\",\n+       \"          [157.3050, 423.3410, 145.6600],\\n\",\n+       \"          [158.4860, 426.6730, 144.2700],\\n\",\n+       \"          [160.7730, 427.0880, 141.2590],\\n\",\n+       \"          [163.9630, 429.0530, 141.9600],\\n\",\n+       \"          [166.4230, 427.4330, 139.5550],\\n\",\n+       \"          [167.6170, 430.8330, 138.3250],\\n\",\n+       \"          [170.5180, 431.9210, 140.5320],\\n\",\n+       \"          [169.7800, 435.6680, 140.3160],\\n\",\n+       \"          [167.6790, 437.8790, 142.6110],\\n\",\n+       \"          [167.3350, 435.1910, 145.2890],\\n\",\n+       \"          [168.9090, 437.3000, 148.0580],\\n\",\n+       \"          [166.1860, 439.9440, 148.5750],\\n\",\n+       \"          [164.1010, 438.1300, 151.1890],\\n\",\n+       \"          [163.0190, 438.8690, 154.7450],\\n\",\n+       \"          [164.6750, 435.6560, 155.9860],\\n\",\n+       \"          [168.1170, 436.6550, 154.6330],\\n\",\n+       \"          [169.8040, 437.0930, 158.0010],\\n\",\n+       \"          [173.2680, 438.4850, 158.5750],\\n\",\n+       \"          [174.9270, 440.8490, 156.1460],\\n\",\n+       \"          [175.9170, 440.8010, 152.5000],\\n\",\n+       \"          [179.6060, 441.0910, 153.4330],\\n\",\n+       \"          [180.8000, 439.9830, 156.8710],\\n\",\n+       \"          [184.2540, 439.9980, 158.4580],\\n\",\n+       \"          [185.5840, 437.2970, 160.7670],\\n\",\n+       \"          [186.4140, 439.9000, 163.4480],\\n\",\n+       \"          [182.9430, 441.4570, 163.7500],\\n\",\n+       \"          [181.4930, 442.0910, 167.2100],\\n\",\n+       \"          [178.0780, 440.5360, 167.9060],\\n\",\n+       \"          [176.1650, 440.9290, 171.1790],\\n\",\n+       \"          [173.0530, 438.9530, 172.1310],\\n\",\n+       \"          [170.3990, 440.6990, 174.2170],\\n\",\n+       \"          [166.7880, 440.0710, 175.2260],\\n\",\n+       \"          [164.4480, 442.1010, 173.0340],\\n\",\n+       \"          [160.9780, 443.6110, 173.3810],\\n\",\n+       \"          [158.8330, 443.9000, 170.2730],\\n\",\n+       \"          [160.1820, 442.9360, 166.8360],\\n\",\n+       \"          [163.5010, 444.7230, 166.1630],\\n\",\n+       \"          [164.6310, 442.4290, 163.3160],\\n\",\n+       \"          [166.6330, 444.2560, 160.6240],\\n\",\n+       \"          [166.1700, 447.5210, 162.5220],\\n\",\n+       \"          [168.1350, 449.9450, 164.6440],\\n\",\n+       \"          [168.0040, 449.0470, 168.3240],\\n\",\n+       \"          [168.4210, 451.0540, 171.5220],\\n\",\n+       \"          [169.0440, 450.0360, 175.1300],\\n\",\n+       \"          [166.1650, 450.3680, 177.6100],\\n\",\n+       \"          [166.7800, 449.8300, 181.3340],\\n\",\n+       \"          [163.8480, 448.9150, 183.5760],\\n\",\n+       \"          [163.3500, 443.9770, 187.9750],\\n\",\n+       \"          [165.0240, 442.4930, 184.8870],\\n\",\n+       \"          [168.1120, 443.0980, 182.7810],\\n\",\n+       \"          [168.1280, 445.9210, 180.2050],\\n\",\n+       \"          [166.7230, 445.0130, 176.8030],\\n\",\n+       \"          [167.0690, 446.1330, 173.1930],\\n\",\n+       \"          [164.0790, 447.8390, 171.5780],\\n\",\n+       \"          [163.4630, 449.0630, 168.0350],\\n\",\n+       \"          [164.4160, 452.7320, 168.0130],\\n\",\n+       \"          [166.7460, 455.3190, 166.5790],\\n\",\n+       \"          [167.5460, 459.0090, 166.8440],\\n\",\n+       \"          [170.2010, 459.9620, 169.3810],\\n\",\n+       \"          [169.5150, 456.8220, 171.4570],\\n\",\n+       \"          [170.7440, 454.3880, 168.7860],\\n\",\n+       \"          [172.8900, 451.6810, 170.3850],\\n\",\n+       \"          [173.2420, 449.3000, 167.4490],\\n\",\n+       \"          [171.4700, 447.3670, 164.7150],\\n\",\n+       \"          [169.7840, 443.9650, 164.9670],\\n\",\n+       \"          [170.8880, 441.4120, 162.3680],\\n\",\n+       \"          [169.3320, 438.1640, 163.6510],\\n\",\n+       \"          [166.7170, 436.9060, 166.1030],\\n\",\n+       \"          [166.2830, 433.5290, 167.7900],\\n\",\n+       \"          [162.5490, 433.4260, 168.5040],\\n\",\n+       \"          [162.6440, 430.3540, 170.7620],\\n\",\n+       \"          [164.9490, 432.1120, 173.2350],\\n\",\n+       \"          [163.6530, 435.5760, 172.2210],\\n\",\n+       \"          [167.2540, 436.7450, 171.8300],\\n\",\n+       \"          [168.3710, 439.3650, 169.3060],\\n\",\n+       \"          [171.8910, 439.6680, 167.8930],\\n\",\n+       \"          [173.1360, 443.2500, 167.5870],\\n\",\n+       \"          [176.0990, 444.7950, 165.8030],\\n\",\n+       \"          [177.1670, 447.8660, 167.8270],\\n\",\n+       \"          [177.4160, 451.2430, 166.1360],\\n\",\n+       \"          [181.1790, 451.0810, 166.7480],\\n\",\n+       \"          [181.3560, 448.4350, 163.9980],\\n\",\n+       \"          [180.6020, 448.8920, 160.3060],\\n\",\n+       \"          [177.8440, 446.8990, 158.6170],\\n\",\n+       \"          [176.7540, 446.3160, 155.0150],\\n\",\n+       \"          [173.3820, 444.6900, 154.4770],\\n\",\n+       \"          [169.6430, 444.9660, 153.9760],\\n\",\n+       \"          [167.6080, 446.9450, 156.5140],\\n\",\n+       \"          [163.9630, 447.9270, 156.8800],\\n\",\n+       \"          [163.1520, 451.2160, 155.1430],\\n\",\n+       \"          [160.3870, 453.6570, 156.0420],\\n\",\n+       \"          [158.9330, 453.6920, 152.5150],\\n\",\n+       \"          [159.8050, 453.2460, 148.8360],\\n\",\n+       \"          [161.2560, 456.7540, 148.4440],\\n\",\n+       \"          [164.9420, 455.8330, 148.8520],\\n\",\n+       \"          [167.3780, 455.9500, 145.9260],\\n\",\n+       \"          [171.0380, 455.0610, 145.5060],\\n\",\n+       \"          [173.5830, 457.5340, 146.9270],\\n\",\n+       \"          [171.2360, 458.9430, 149.5740],\\n\",\n+       \"          [171.8150, 459.8640, 153.2130],\\n\",\n+       \"          [169.5910, 457.9090, 155.6000],\\n\",\n+       \"          [168.4960, 458.3960, 159.2100],\\n\",\n+       \"          [167.0870, 455.9020, 161.7110],\\n\",\n+       \"          [163.6430, 457.0390, 162.8610],\\n\",\n+       \"          [161.8700, 456.3550, 166.1660],\\n\",\n+       \"          [160.7490, 452.9260, 164.8950],\\n\",\n+       \"          [164.2440, 451.8750, 163.8110],\\n\",\n+       \"          [163.4490, 452.3040, 160.1090],\\n\",\n+       \"          [165.6820, 454.0140, 157.5590],\\n\",\n+       \"          [164.2940, 457.2310, 156.0670],\\n\",\n+       \"          [165.6810, 459.7290, 153.5780],\\n\",\n+       \"          [167.3870, 462.7790, 155.0760],\\n\",\n+       \"          [170.2060, 465.8340, 161.8360],\\n\",\n+       \"          [172.1620, 462.7680, 160.6890],\\n\",\n+       \"          [174.0280, 461.7040, 163.8180],\\n\",\n+       \"          [175.2110, 458.5660, 161.9930],\\n\",\n+       \"          [176.5410, 458.2410, 158.4450],\\n\",\n+       \"          [174.5550, 455.7530, 156.3540],\\n\",\n+       \"          [174.3580, 455.6030, 152.5600],\\n\",\n+       \"          [171.8590, 453.6260, 150.4910],\\n\",\n+       \"          [173.6300, 451.2730, 148.0810],\\n\",\n+       \"          [170.4820, 450.1750, 146.2000],\\n\",\n+       \"          [166.8520, 451.1980, 145.7810],\\n\",\n+       \"          [164.0730, 450.1880, 148.1490],\\n\",\n+       \"          [162.6710, 446.7750, 147.1980],\\n\",\n+       \"          [159.1420, 445.8080, 148.2240],\\n\",\n+       \"          [158.3160, 442.3640, 149.5950],\\n\",\n+       \"          [155.0300, 443.1470, 151.4050],\\n\",\n+       \"          [152.6800, 446.0960, 151.8500],\\n\",\n+       \"          [155.0610, 447.4890, 154.4910],\\n\",\n+       \"          [158.2010, 445.3460, 154.0190],\\n\",\n+       \"          [160.8090, 447.5340, 152.2870],\\n\",\n+       \"          [164.4130, 446.3150, 152.1360],\\n\",\n+       \"          [167.3360, 448.5380, 151.1380],\\n\",\n+       \"          [171.0310, 447.6630, 150.9500],\\n\",\n+       \"          [172.8160, 450.1440, 153.2260],\\n\",\n+       \"          [176.3550, 450.4540, 154.5850],\\n\",\n+       \"          [176.6310, 452.0780, 158.0240],\\n\",\n+       \"          [180.1600, 453.0970, 159.0310],\\n\",\n+       \"          [179.9110, 454.9710, 162.3160],\\n\",\n+       \"          [178.7370, 458.4030, 163.4610],\\n\",\n+       \"          [179.1350, 461.9150, 162.0830],\\n\",\n+       \"          [180.3160, 464.9940, 163.9950],\\n\",\n+       \"          [195.1700, 454.0210, 194.1160],\\n\",\n+       \"          [196.4220, 450.5570, 193.1610],\\n\",\n+       \"          [195.0530, 447.5260, 191.3340],\\n\",\n+       \"          [195.7530, 445.4320, 194.4410],\\n\",\n+       \"          [195.6760, 446.8960, 197.9710],\\n\",\n+       \"          [199.1230, 447.9300, 199.1640],\\n\",\n+       \"          [198.4070, 447.0140, 202.7840],\\n\",\n+       \"          [198.1130, 443.3080, 203.5090],\\n\",\n+       \"          [195.0350, 441.7530, 205.1060],\\n\",\n+       \"          [194.7040, 440.1720, 208.5300],\\n\",\n+       \"          [191.7420, 437.9460, 207.6670],\\n\",\n+       \"          [190.9460, 435.0990, 205.3140],\\n\",\n+       \"          [188.0870, 435.6760, 202.8880],\\n\",\n+       \"          [184.8300, 433.7320, 203.1830],\\n\",\n+       \"          [184.1270, 431.8260, 199.9570],\\n\",\n+       \"          [180.5450, 430.8500, 200.7460],\\n\",\n+       \"          [178.2470, 433.3320, 199.0220],\\n\",\n+       \"          [178.2770, 433.9260, 195.2680],\\n\",\n+       \"          [176.3170, 437.2030, 195.0280],\\n\",\n+       \"          [177.7690, 440.7330, 194.9660],\\n\",\n+       \"          [181.3350, 439.3910, 195.1330],\\n\",\n+       \"          [182.5820, 441.3460, 192.0940],\\n\",\n+       \"          [182.3530, 444.9560, 193.3610],\\n\",\n+       \"          [186.0870, 445.2240, 194.0290],\\n\",\n+       \"          [188.7230, 447.5570, 192.6190],\\n\",\n+       \"          [190.8760, 444.5870, 191.5490],\\n\",\n+       \"          [188.2670, 443.5230, 188.9590],\\n\",\n+       \"          [190.1360, 444.3010, 185.7360],\\n\",\n+       \"          [188.8140, 444.1990, 182.1980],\\n\",\n+       \"          [185.1300, 444.1560, 181.3470],\\n\",\n+       \"          [182.1290, 442.2030, 182.6060],\\n\",\n+       \"          [181.3330, 441.2700, 178.9860],\\n\",\n+       \"          [184.3690, 440.6570, 176.7700],\\n\",\n+       \"          [184.4240, 439.7520, 173.0740],\\n\",\n+       \"          [187.1540, 437.5560, 171.6020],\\n\",\n+       \"          [187.5170, 439.9750, 168.6500],\\n\",\n+       \"          [188.4180, 443.0500, 170.7240],\\n\",\n+       \"          [191.4360, 445.2100, 169.8910],\\n\",\n+       \"          [193.7480, 445.6510, 172.8940],\\n\",\n+       \"          [196.8200, 447.8990, 173.0280],\\n\",\n+       \"          [199.4910, 448.6120, 175.6310],\\n\",\n+       \"          [201.2080, 451.9090, 176.3780],\\n\",\n+       \"          [203.6110, 453.4740, 178.8680],\\n\",\n+       \"          [201.6130, 454.9210, 181.7570],\\n\",\n+       \"          [202.0690, 458.3840, 183.2730],\\n\",\n+       \"          [200.4970, 458.3690, 186.7180],\\n\",\n+       \"          [197.7960, 455.8130, 187.5240],\\n\",\n+       \"          [195.1460, 455.8470, 184.7720],\\n\",\n+       \"          [193.6160, 452.5740, 185.9790],\\n\",\n+       \"          [189.8600, 452.3800, 185.3150],\\n\",\n+       \"          [189.9440, 455.8580, 183.7680],\\n\",\n+       \"          [189.7400, 457.4310, 180.3410],\\n\",\n+       \"          [193.1630, 457.9480, 178.8100],\\n\",\n+       \"          [194.6770, 460.5170, 176.4640],\\n\",\n+       \"          [197.9720, 460.6430, 174.6010],\\n\",\n+       \"          [200.7710, 462.5710, 176.3210],\\n\",\n+       \"          [203.7970, 463.7400, 174.3330],\\n\",\n+       \"          [207.1150, 464.5820, 175.9770],\\n\",\n+       \"          [210.1390, 466.5260, 174.8060],\\n\",\n+       \"          [210.8850, 463.9390, 172.1200],\\n\",\n+       \"          [213.6440, 461.8050, 173.6150],\\n\",\n+       \"          [211.1280, 459.2430, 174.9140],\\n\",\n+       \"          [208.0390, 457.7610, 173.2930],\\n\",\n+       \"          [204.4770, 458.9110, 173.9310],\\n\",\n+       \"          [202.5240, 457.8000, 177.0030],\\n\",\n+       \"          [198.9190, 457.7220, 178.2440],\\n\",\n+       \"          [197.5730, 459.9040, 181.0500],\\n\",\n+       \"          [194.2310, 460.2350, 182.8060],\\n\",\n+       \"          [192.1140, 462.6430, 180.7850],\\n\",\n+       \"          [189.0250, 463.1720, 178.7000],\\n\",\n+       \"          [187.1240, 465.8360, 176.8220],\\n\",\n+       \"          [188.2850, 466.3320, 173.2420],\\n\",\n+       \"          [191.5580, 464.4520, 173.8600],\\n\",\n+       \"          [189.9950, 461.1280, 174.9010],\\n\",\n+       \"          [191.8440, 458.2970, 173.1540],\\n\",\n+       \"          [190.4670, 455.2690, 174.9900],\\n\",\n+       \"          [189.7380, 453.5820, 178.3070],\\n\",\n+       \"          [192.3390, 451.8830, 180.5000],\\n\",\n+       \"          [191.2020, 448.5340, 181.9050],\\n\",\n+       \"          [194.3800, 447.0280, 183.4050],\\n\",\n+       \"          [197.8700, 447.9650, 184.5850],\\n\",\n+       \"          [200.9840, 445.8270, 184.9660],\\n\",\n+       \"          [202.8290, 447.6220, 187.7590],\\n\",\n+       \"          [206.2120, 445.9180, 187.3400],\\n\",\n+       \"          [206.6040, 447.1310, 183.7460],\\n\",\n+       \"          [204.4430, 450.2380, 184.1830],\\n\",\n+       \"          [202.2310, 449.1370, 181.2890],\\n\",\n+       \"          [198.6660, 450.3790, 180.7720],\\n\",\n+       \"          [196.2310, 448.2490, 178.7680],\\n\",\n+       \"          [194.0550, 450.4240, 176.5260],\\n\",\n+       \"          [190.8490, 449.7090, 174.6070],\\n\",\n+       \"          [190.4360, 452.3080, 171.8240],\\n\",\n+       \"          [187.3830, 454.5590, 171.7980],\\n\",\n+       \"          [186.4670, 452.9730, 168.4480],\\n\",\n+       \"          [185.9050, 449.6480, 170.2580],\\n\",\n+       \"          [182.8890, 448.8910, 172.4330],\\n\",\n+       \"          [183.5680, 448.6030, 176.1630],\\n\",\n+       \"          [181.3330, 447.5400, 179.0590],\\n\",\n+       \"          [182.7340, 447.5270, 182.5670],\\n\",\n+       \"          [183.3280, 449.2260, 185.8920],\\n\",\n+       \"          [184.8760, 452.6980, 185.6990],\\n\",\n+       \"          [185.7140, 455.4170, 188.2030],\\n\",\n+       \"          [182.9420, 457.9530, 188.8300],\\n\",\n+       \"          [183.2930, 461.5650, 189.9350],\\n\",\n+       \"          [182.7580, 462.1320, 193.6660],\\n\",\n+       \"          [180.0620, 464.7490, 193.1030],\\n\",\n+       \"          [176.4120, 463.9030, 192.5790],\\n\",\n+       \"          [177.1550, 460.1730, 192.6930],\\n\",\n+       \"          [173.8180, 459.5980, 194.4390],\\n\",\n+       \"          [172.0770, 461.1030, 191.4040],\\n\",\n+       \"          [173.6950, 458.5410, 189.0910],\\n\",\n+       \"          [171.0060, 456.0190, 188.1270],\\n\",\n+       \"          [170.8210, 453.3390, 185.4210],\\n\",\n+       \"          [170.0180, 454.8270, 182.0350],\\n\",\n+       \"          [171.8690, 458.1080, 182.6590],\\n\",\n+       \"          [174.2320, 459.6520, 180.1210],\\n\",\n+       \"          [177.8640, 459.9130, 181.2230],\\n\",\n+       \"          [180.8780, 461.8410, 179.9490],\\n\",\n+       \"          [184.5650, 461.8070, 180.8430],\\n\",\n+       \"          [186.0650, 464.7990, 182.6440],\\n\",\n+       \"          [189.5780, 466.2740, 182.6170],\\n\",\n+       \"          [190.7120, 463.6340, 185.1390],\\n\",\n+       \"          [189.2980, 460.6630, 183.2290],\\n\",\n+       \"          [186.3560, 460.3000, 185.6240],\\n\",\n+       \"          [182.7840, 459.6590, 184.5210],\\n\",\n+       \"          [180.2130, 462.3320, 185.3590],\\n\",\n+       \"          [176.5320, 462.8410, 184.6140],\\n\",\n+       \"          [175.8580, 465.0300, 181.5790],\\n\",\n+       \"          [178.1930, 465.7110, 174.9000],\\n\",\n+       \"          [180.9150, 464.1190, 172.7670],\\n\",\n+       \"          [181.2510, 460.5420, 174.1130],\\n\",\n+       \"          [178.4430, 457.9660, 174.2140],\\n\",\n+       \"          [178.5420, 456.4790, 177.7180],\\n\",\n+       \"          [175.4730, 455.1590, 179.5500],\\n\",\n+       \"          [175.4670, 454.0690, 183.1870],\\n\",\n+       \"          [174.1210, 450.5430, 183.6830],\\n\",\n+       \"          [173.9850, 450.6340, 187.5030],\\n\",\n+       \"          [174.3640, 452.9240, 190.5240],\\n\",\n+       \"          [177.5140, 454.2200, 192.1820],\\n\",\n+       \"          [179.3060, 451.6570, 194.3540],\\n\",\n+       \"          [181.4890, 453.2110, 197.0570],\\n\",\n+       \"          [184.7390, 451.3110, 197.6330],\\n\",\n+       \"          [186.4870, 454.0230, 199.6690],\\n\",\n+       \"          [185.5270, 457.3810, 201.1790],\\n\",\n+       \"          [186.7910, 459.0050, 197.9610],\\n\",\n+       \"          [186.6010, 456.0430, 195.5320],\\n\",\n+       \"          [183.3680, 455.3620, 193.6270],\\n\",\n+       \"          [182.9110, 452.9430, 190.7280],\\n\",\n+       \"          [180.0190, 452.5230, 188.3060],\\n\",\n+       \"          [179.0910, 450.0250, 185.5960],\\n\",\n+       \"          [178.9850, 451.7840, 182.2280],\\n\",\n+       \"          [179.0510, 451.0080, 178.5100],\\n\",\n+       \"          [181.0530, 453.1550, 176.0780],\\n\",\n+       \"          [180.1320, 452.7170, 172.4060],\\n\",\n+       \"          [182.0570, 455.4530, 170.6120],\\n\",\n+       \"          [181.8140, 459.1820, 170.0770],\\n\",\n+       \"          [178.5990, 461.0580, 169.3130],\\n\",\n+       \"          [177.7620, 463.8650, 166.9020]]),\\n\",\n        \"  'residue_ids': tensor([  4.,   4.,   4.,  ..., 182., 182., 182.]),\\n\",\n        \"  'chain_ids': tensor([0., 0., 0.,  ..., 1., 1., 1.])},\\n\",\n        \" 'id': '8phr__X4_UNDEFINED--8phr__W4_UNDEFINED',\\n\",\n@@ -906,7 +1568,7 @@\n        \" 'target_id': '8phr__X4_UNDEFINED-R--8phr__W4_UNDEFINED-L'}\"\n       ]\n      },\n-     \"execution_count\": 3,\n+     \"execution_count\": 15,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -918,7 +1580,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 16,\n    \"id\": \"4a6f64b4-f027-4286-b444-68cd9f6b59a4\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -928,7 +1590,7 @@\n        \"torch.Size([1316, 3])\"\n       ]\n      },\n-     \"execution_count\": 10,\n+     \"execution_count\": 16,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -945,7 +1607,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 17,\n    \"id\": \"01025b2b-1962-43b3-8c4e-a33b6a3d6231\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -956,7 +1618,7 @@\n       \"Help on class PinderDataset in module pinder.core.loader.dataset:\\n\",\n       \"\\n\",\n       \"class PinderDataset(torch.utils.data.dataset.Dataset)\\n\",\n-      \" |  PinderDataset(split: 'str | None' = None, index: 'pd.DataFrame | None' = None, metadata: 'pd.DataFrame | None' = None, monomer_priority: 'str' = 'holo', base_filters: 'list[PinderFilterBase]' = [], sub_filters: 'list[PinderFilterSubBase]' = [], structure_filters: 'list[StructureFilter]' = [], structure_transforms_target: 'list[StructureTransform]' = [], structure_transforms_feature: 'list[StructureTransform]' = [], transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1e6b9a4d0>, target_transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1e6b9a4d0>, ids: 'list[str] | None' = None, fallback_to_holo: 'bool' = True, use_canonical_apo: 'bool' = True, crop_equal_monomer_shapes: 'bool' = True, index_query: 'str | None' = None, metadata_query: 'str | None' = None, pre_specified_monomers: 'dict[str, str] | pd.DataFrame | None' = None, **kwargs: 'Any') -> 'None'\\n\",\n+      \" |  PinderDataset(split: 'str | None' = None, index: 'pd.DataFrame | None' = None, metadata: 'pd.DataFrame | None' = None, monomer_priority: 'str' = 'holo', base_filters: 'list[PinderFilterBase]' = [], sub_filters: 'list[PinderFilterSubBase]' = [], structure_filters: 'list[StructureFilter]' = [], structure_transforms_target: 'list[StructureTransform]' = [], structure_transforms_feature: 'list[StructureTransform]' = [], transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1f9f84d30>, target_transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1f9f84d30>, ids: 'list[str] | None' = None, fallback_to_holo: 'bool' = True, use_canonical_apo: 'bool' = True, crop_equal_monomer_shapes: 'bool' = True, index_query: 'str | None' = None, metadata_query: 'str | None' = None, pre_specified_monomers: 'dict[str, str] | pd.DataFrame | None' = None, **kwargs: 'Any') -> 'None'\\n\",\n       \" |  \\n\",\n       \" |  Method resolution order:\\n\",\n       \" |      PinderDataset\\n\",\n@@ -968,7 +1630,7 @@\n       \" |  \\n\",\n       \" |  __getitem__(self, idx: 'int') -> 'dict[str, dict[str, torch.Tensor] | torch.Tensor]'\\n\",\n       \" |  \\n\",\n-      \" |  __init__(self, split: 'str | None' = None, index: 'pd.DataFrame | None' = None, metadata: 'pd.DataFrame | None' = None, monomer_priority: 'str' = 'holo', base_filters: 'list[PinderFilterBase]' = [], sub_filters: 'list[PinderFilterSubBase]' = [], structure_filters: 'list[StructureFilter]' = [], structure_transforms_target: 'list[StructureTransform]' = [], structure_transforms_feature: 'list[StructureTransform]' = [], transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1e6b9a4d0>, target_transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1e6b9a4d0>, ids: 'list[str] | None' = None, fallback_to_holo: 'bool' = True, use_canonical_apo: 'bool' = True, crop_equal_monomer_shapes: 'bool' = True, index_query: 'str | None' = None, metadata_query: 'str | None' = None, pre_specified_monomers: 'dict[str, str] | pd.DataFrame | None' = None, **kwargs: 'Any') -> 'None'\\n\",\n+      \" |  __init__(self, split: 'str | None' = None, index: 'pd.DataFrame | None' = None, metadata: 'pd.DataFrame | None' = None, monomer_priority: 'str' = 'holo', base_filters: 'list[PinderFilterBase]' = [], sub_filters: 'list[PinderFilterSubBase]' = [], structure_filters: 'list[StructureFilter]' = [], structure_transforms_target: 'list[StructureTransform]' = [], structure_transforms_feature: 'list[StructureTransform]' = [], transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1f9f84d30>, target_transform: 'Callable[[Structure], torch.Tensor | dict[str, torch.Tensor]]' = <function structure2tensor_transform at 0x1f9f84d30>, ids: 'list[str] | None' = None, fallback_to_holo: 'bool' = True, use_canonical_apo: 'bool' = True, crop_equal_monomer_shapes: 'bool' = True, index_query: 'str | None' = None, metadata_query: 'str | None' = None, pre_specified_monomers: 'dict[str, str] | pd.DataFrame | None' = None, **kwargs: 'Any') -> 'None'\\n\",\n       \" |      Initialize self.  See help(type(self)) for accurate signature.\\n\",\n       \" |  \\n\",\n       \" |  __len__(self) -> 'int'\\n\",\n@@ -1039,7 +1701,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 18,\n    \"id\": \"fcb50bed-7e0a-4d4b-a1d7-6bc003161aab\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1047,10 +1709,11 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"2024-09-23 15:44:49,788 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=8, items=8\\n\",\n-      \"2024-09-23 15:44:50,077 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.29s\\n\",\n-      \"2024-09-23 15:44:50,350 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n-      \"2024-09-23 15:44:50,645 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.30s\\n\"\n+      \"2024-09-25 16:23:26,740 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n+      \"2024-09-25 16:23:27,432 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n+      \"2024-09-25 16:23:27,691 | pinder.core.loader.filters:436 | WARNING : Skipping FilterSubRmsds on 3muw__B33_P03316--3muw__B6_P03316 as holo monomers have been filtered out\\n\",\n+      \"2024-09-25 16:23:27,789 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n+      \"2024-09-25 16:23:28,201 | pinder.core.loader.structure:595 | ERROR : no common residues found! 2in2__A1_P03303--2rs3__C39_P03303-L\\n\"\n      ]\n     }\n    ],\n@@ -1110,7 +1773,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 27,\n    \"id\": \"0de517fc-6e7f-4682-af48-980d27638b21\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1119,7 +1782,7 @@\n      \"output_type\": \"stream\",\n      \"text\": [\n       \"Processing...\\n\",\n-      \"2024-09-23 15:45:00,968 | pinder.core.loader.dataset:541 | INFO : Finished processing, only 5 systems\\n\",\n+      \"2024-09-25 16:29:35,300 | pinder.core.loader.dataset:543 | INFO : Finished processing, only 5 systems\\n\",\n       \"Done!\\n\"\n      ]\n     },\n@@ -1129,7 +1792,7 @@\n        \"PPIDataset(5)\"\n       ]\n      },\n-     \"execution_count\": 6,\n+     \"execution_count\": 27,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1157,7 +1820,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 28,\n    \"id\": \"5392a214-7426-4be2-bc81-a00702c82c55\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1178,12 +1841,12 @@\n        \"    chain=[1],\\n\",\n        \"  },\\n\",\n        \"  ligand_atom={\\n\",\n-       \"    x=[1198, 12],\\n\",\n+       \"    x=[1198, 1],\\n\",\n        \"    pos=[1198, 3],\\n\",\n        \"    edge_index=[2, 11980],\\n\",\n        \"  },\\n\",\n        \"  receptor_atom={\\n\",\n-       \"    x=[1358, 12],\\n\",\n+       \"    x=[1358, 1],\\n\",\n        \"    pos=[1358, 3],\\n\",\n        \"    edge_index=[2, 13580],\\n\",\n        \"  },\\n\",\n@@ -1194,7 +1857,7 @@\n        \")\"\n       ]\n      },\n-     \"execution_count\": 7,\n+     \"execution_count\": 28,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1220,7 +1883,66 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 29,\n+   \"id\": \"7948dcbd-fcb9-422e-89be-fbf3d1c4f13c\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"{'ligand_residue': 0,\\n\",\n+       \" 'receptor_residue': 0,\\n\",\n+       \" 'ligand_atom': 1,\\n\",\n+       \" 'receptor_atom': 1,\\n\",\n+       \" 'pdb': 0}\"\n+      ]\n+     },\n+     \"execution_count\": 29,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"data_item.num_node_features\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 30,\n+   \"id\": \"ff827dac-c7e5-4e81-99ff-ff9e67c3cbbe\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"{'x': tensor([[0.],\\n\",\n+       \"        [1.],\\n\",\n+       \"        [2.],\\n\",\n+       \"        ...,\\n\",\n+       \"        [2.],\\n\",\n+       \"        [3.],\\n\",\n+       \"        [8.]]), 'pos': tensor([[194.1170, 453.4250, 194.9300],\\n\",\n+       \"        [195.1700, 454.0210, 194.1160],\\n\",\n+       \"        [196.0630, 452.9460, 193.5050],\\n\",\n+       \"        ...,\\n\",\n+       \"        [177.4130, 465.0800, 167.7550],\\n\",\n+       \"        [176.8000, 464.9490, 168.8150],\\n\",\n+       \"        [176.7910, 463.7350, 165.7380]]), 'edge_index': tensor([[   1,    4,    2,  ..., 1186, 1187, 1182],\\n\",\n+       \"        [   0,    0,    0,  ..., 1197, 1197, 1197]])}\"\n+      ]\n+     },\n+     \"execution_count\": 30,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"data_item[\\\"ligand_atom\\\"]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 31,\n    \"id\": \"6fa927a9-cde9-4f32-acf9-f5c989b32966\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1241,12 +1963,12 @@\n        \"    chain=[1],\\n\",\n        \"  },\\n\",\n        \"  ligand_atom={\\n\",\n-       \"    x=[1198, 12],\\n\",\n+       \"    x=[1198, 1],\\n\",\n        \"    pos=[1198, 3],\\n\",\n        \"    edge_index=[2, 11980],\\n\",\n        \"  },\\n\",\n        \"  receptor_atom={\\n\",\n-       \"    x=[1358, 12],\\n\",\n+       \"    x=[1358, 1],\\n\",\n        \"    pos=[1358, 3],\\n\",\n        \"    edge_index=[2, 13580],\\n\",\n        \"  },\\n\",\n@@ -1257,7 +1979,7 @@\n        \")\"\n       ]\n      },\n-     \"execution_count\": 8,\n+     \"execution_count\": 31,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1271,7 +1993,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 32,\n    \"id\": \"75e13a76-cf9f-47dd-90c2-370c0652074f\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1327,7 +2049,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 33,\n    \"id\": \"ce872929-1706-4a09-ad67-63457511099f\",\n    \"metadata\": {\n     \"scrolled\": true\n@@ -1350,19 +2072,19 @@\n        \"    chain=[1],\\n\",\n        \"  },\\n\",\n        \"  ligand_atom={\\n\",\n-       \"    x=[1032, 12],\\n\",\n+       \"    x=[1032, 1],\\n\",\n        \"    pos=[1032, 3],\\n\",\n        \"    edge_index=[2, 10320],\\n\",\n        \"  },\\n\",\n        \"  receptor_atom={\\n\",\n-       \"    x=[1441, 12],\\n\",\n+       \"    x=[1441, 1],\\n\",\n        \"    pos=[1441, 3],\\n\",\n        \"    edge_index=[2, 14410],\\n\",\n        \"  }\\n\",\n        \")\"\n       ]\n      },\n-     \"execution_count\": 10,\n+     \"execution_count\": 33,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1389,8 +2111,8 @@\n     \"expected_num_feats = {\\n\",\n     \"    'ligand_residue': 0,\\n\",\n     \"    'receptor_residue': 0,\\n\",\n-    \"    'ligand_atom': 12,\\n\",\n-    \"    'receptor_atom': 12\\n\",\n+    \"    'ligand_atom': 1,\\n\",\n+    \"    'receptor_atom': 1\\n\",\n     \"}\\n\",\n     \"for k, v in expected_num_feats.items():\\n\",\n     \"    assert holo_data.num_node_features[k] == v\\n\",\n@@ -1404,7 +2126,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 34,\n    \"id\": \"b2913a8f-2992-46de-ab74-1fca594d6088\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1425,19 +2147,19 @@\n        \"    chain=[1],\\n\",\n        \"  },\\n\",\n        \"  ligand_atom={\\n\",\n-       \"    x=[1350, 12],\\n\",\n+       \"    x=[1350, 1],\\n\",\n        \"    pos=[1350, 3],\\n\",\n        \"    edge_index=[2, 13500],\\n\",\n        \"  },\\n\",\n        \"  receptor_atom={\\n\",\n-       \"    x=[1710, 12],\\n\",\n+       \"    x=[1710, 1],\\n\",\n        \"    pos=[1710, 3],\\n\",\n        \"    edge_index=[2, 17100],\\n\",\n        \"  }\\n\",\n        \")\"\n       ]\n      },\n-     \"execution_count\": 11,\n+     \"execution_count\": 34,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1457,8 +2179,8 @@\n     \"expected_num_feats = {\\n\",\n     \"    'ligand_residue': 0,\\n\",\n     \"    'receptor_residue': 0,\\n\",\n-    \"    'ligand_atom': 12,\\n\",\n-    \"    'receptor_atom': 12\\n\",\n+    \"    'ligand_atom': 1,\\n\",\n+    \"    'receptor_atom': 1,\\n\",\n     \"}\\n\",\n     \"for k, v in expected_num_feats.items():\\n\",\n     \"    assert apo_data.num_node_features[k] == v\\n\",\n@@ -1483,17 +2205,17 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 35,\n    \"id\": \"29ca7612-8a1b-4cf7-a790-cc2b5b26e7e5\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"<torch_geometric.loader.dataloader.DataLoader at 0x1fb60f550>\"\n+       \"<torch_geometric.loader.dataloader.DataLoader at 0x20265ebf0>\"\n       ]\n      },\n-     \"execution_count\": 12,\n+     \"execution_count\": 35,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1516,7 +2238,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 36,\n    \"id\": \"66f9a0ef-1c1f-47bd-ba86-f2c3340c34ec\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1526,7 +2248,7 @@\n        \"PPIDataset(5)\"\n       ]\n      },\n-     \"execution_count\": 13,\n+     \"execution_count\": 36,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1572,7 +2294,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": 37,\n    \"id\": \"bb468005-5ffb-45e8-9f03-f073cb6f7527\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -1696,7 +2418,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 38,\n    \"id\": \"def48617-4288-452f-af9e-55f3289d7788\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1719,7 +2441,7 @@\n        \" ))\"\n       ]\n      },\n-     \"execution_count\": 15,\n+     \"execution_count\": 38,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1744,7 +2466,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n+   \"execution_count\": 39,\n    \"id\": \"c13a072b-ca2b-47bb-9ed1-2a88885aad93\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -1755,20 +2477,20 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 17,\n+   \"execution_count\": 40,\n    \"id\": \"84f1a37c-4ac5-4086-bfc8-260bc8f2de8d\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"(array([[-19.331,  10.568,   4.591],\\n\",\n-       \"        [-18.082,  10.305,   3.883],\\n\",\n-       \"        [-16.867,  10.692,   4.727],\\n\",\n+       \"(array([[-12.6210985,  -9.128864 ,  17.258345 ],\\n\",\n+       \"        [-13.660538 ,  -8.840154 ,  16.265753 ],\\n\",\n+       \"        [-13.38884  ,  -7.5286074,  15.524057 ],\\n\",\n        \"        ...,\\n\",\n-       \"        [ 13.363,  11.96 ,  -6.092],\\n\",\n-       \"        [ 15.081,   9.028,  -5.759],\\n\",\n-       \"        [ 12.459,  10.17 ,  -6.767]], dtype=float32),\\n\",\n+       \"        [  7.1754823, -19.776093 ,  21.191608 ],\\n\",\n+       \"        [  9.592421 , -19.601936 ,  19.197937 ],\\n\",\n+       \"        [  9.432583 , -17.757698 ,  20.458267 ]], dtype=float32),\\n\",\n        \" array([[-13.215324 , -11.076905 ,  15.214827 ],\\n\",\n        \"        [-14.133494 , -10.301853 ,  14.386162 ],\\n\",\n        \"        [-13.614427 ,  -8.882867 ,  14.150835 ],\\n\",\n@@ -1778,7 +2500,7 @@\n        \"        [  7.0213795, -15.329907 ,  18.152586 ]], dtype=float32))\"\n       ]\n      },\n-     \"execution_count\": 17,\n+     \"execution_count\": 40,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1800,17 +2522,17 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 18,\n+   \"execution_count\": 41,\n    \"id\": \"8c495bfe-3aa8-4544-b94e-3bcb6e149dba\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"<torch.utils.data.sampler.WeightedRandomSampler at 0x1fb60c5b0>\"\n+       \"<torch.utils.data.sampler.WeightedRandomSampler at 0x200146500>\"\n       ]\n      },\n-     \"execution_count\": 18,\n+     \"execution_count\": 41,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1855,7 +2577,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 19,\n+   \"execution_count\": 42,\n    \"id\": \"cc7f7dc6-f01b-4d52-a4b5-e0dd19d68fe4\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1863,30 +2585,29 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"2024-09-23 15:45:18,411 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=2, items=2\\n\",\n-      \"2024-09-23 15:45:18,707 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.30s\\n\"\n+      \"2024-09-25 16:31:53,018 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=1, items=1\\n\"\n      ]\n     },\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"(tensor([[[37.2095,  8.0406, 72.5705],\\n\",\n-       \"          [36.1588,  7.1319, 72.1171],\\n\",\n-       \"          [35.5337,  7.6365, 70.8136],\\n\",\n+       \"(tensor([[[-44.9000, -13.0073,   3.9172],\\n\",\n+       \"          [-44.8239, -14.3579,   3.3580],\\n\",\n+       \"          [-44.6508, -14.3793,   1.8236],\\n\",\n        \"          ...,\\n\",\n-       \"          [40.5402,  3.1015, 96.2705],\\n\",\n-       \"          [39.7635,  4.0274, 94.4056],\\n\",\n-       \"          [39.5253,  3.4914, 95.5608]]]),\\n\",\n-       \" tensor([[[29.6114,  7.0679, 73.2040],\\n\",\n-       \"          [30.3889,  6.7263, 72.0241],\\n\",\n-       \"          [31.5403,  7.6852, 71.7204],\\n\",\n+       \"          [-41.4551, -36.3519,   2.2509],\\n\",\n+       \"          [-42.4746, -36.6704,   1.1575],\\n\",\n+       \"          [-43.0579, -35.4178,   0.6238]]]),\\n\",\n+       \" tensor([[[-47.9769,  -9.1210,   3.2415],\\n\",\n+       \"          [-47.0982,  -9.0194,   2.0802],\\n\",\n+       \"          [-46.2697, -10.2855,   1.8961],\\n\",\n        \"          ...,\\n\",\n-       \"          [35.5366, -4.1661, 95.5403],\\n\",\n-       \"          [34.4548, -4.3722, 96.2721],\\n\",\n-       \"          [35.4055, -4.2196, 94.2242]]]))\"\n+       \"          [-35.2793, -42.3302,   2.4468],\\n\",\n+       \"          [-34.2672, -43.3448,   1.9357],\\n\",\n+       \"          [-34.0432, -43.2068,   0.4709]]]))\"\n       ]\n      },\n-     \"execution_count\": 19,\n+     \"execution_count\": 42,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1917,17 +2638,17 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 20,\n+   \"execution_count\": 43,\n    \"id\": \"fe172bdd-09eb-45ed-bc51-28202e5158b6\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"<torch.utils.data.dataloader.DataLoader at 0x1fb6404c0>\"\n+       \"<torch.utils.data.dataloader.DataLoader at 0x1ff3c7850>\"\n       ]\n      },\n-     \"execution_count\": 20,\n+     \"execution_count\": 43,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1974,7 +2695,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 21,\n+   \"execution_count\": 44,\n    \"id\": \"b7602385-2893-4c09-afc3-6a10433c7ada\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1982,30 +2703,29 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"2024-09-23 15:45:25,616 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=2, items=2\\n\",\n-      \"2024-09-23 15:45:25,901 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.28s\\n\"\n+      \"2024-09-25 16:31:59,644 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=10, items=15\\n\"\n      ]\n     },\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"(tensor([[[226.8320, 169.1410, 206.0350],\\n\",\n-       \"          [226.2280, 168.7600, 207.3040],\\n\",\n-       \"          [227.3330, 168.7310, 208.3410],\\n\",\n+       \"(tensor([[[ 70.4514,  -6.9500,  29.3370],\\n\",\n+       \"          [ 69.9283,  -6.6501,  30.6817],\\n\",\n+       \"          [ 68.9000,  -5.5045,  30.7376],\\n\",\n        \"          ...,\\n\",\n-       \"          [205.7270, 143.0570, 181.4730],\\n\",\n-       \"          [206.4940, 142.1170, 181.2560],\\n\",\n-       \"          [204.8250, 144.8850, 182.9280]]]),\\n\",\n-       \" tensor([[[226.8320, 169.1410, 206.0350],\\n\",\n-       \"          [226.2280, 168.7600, 207.3040],\\n\",\n-       \"          [227.3330, 168.7310, 208.3410],\\n\",\n+       \"          [ 67.3974, -52.2529,  39.1974],\\n\",\n+       \"          [ 65.9296, -51.8409,  39.2738],\\n\",\n+       \"          [ 68.1239, -51.1915,  38.3785]]]),\\n\",\n+       \" tensor([[[ 68.1820,  -6.3490,  28.9990],\\n\",\n+       \"          [ 68.8730,  -5.0680,  28.9240],\\n\",\n+       \"          [ 68.3080,  -4.1750,  27.8270],\\n\",\n        \"          ...,\\n\",\n-       \"          [205.7270, 143.0570, 181.4730],\\n\",\n-       \"          [206.4940, 142.1170, 181.2560],\\n\",\n-       \"          [204.8250, 144.8850, 182.9280]]]))\"\n+       \"          [ 68.4140, -52.5670,  40.9670],\\n\",\n+       \"          [ 67.2560, -52.0200,  41.7820],\\n\",\n+       \"          [ 69.1350, -51.4480,  40.2400]]]))\"\n       ]\n      },\n-     \"execution_count\": 21,\n+     \"execution_count\": 44,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -2035,7 +2755,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 22,\n+   \"execution_count\": 45,\n    \"id\": \"cce6f303-99e7-4d42-805d-72f39d9c61b4\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -2074,7 +2794,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 23,\n+   \"execution_count\": 46,\n    \"id\": \"4fbf640e-3348-439c-a8d0-ae4bb2c86dc3\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -2082,48 +2802,45 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"2024-09-23 15:45:27,403 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=10, items=169\\n\",\n-      \"2024-09-23 15:45:30,832 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      3.43s\\n\",\n-      \"2024-09-23 15:45:31,248 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n-      \"2024-09-23 15:45:31,445 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.20s\\n\"\n+      \"2024-09-25 16:32:03,705 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\"\n      ]\n     },\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"(tensor([[[ -20.7939,   16.2259,   28.9029],\\n\",\n-       \"          [ -20.3926,   16.6965,   27.5845],\\n\",\n-       \"          [ -19.1855,   15.8943,   27.0868],\\n\",\n-       \"          ...,\\n\",\n-       \"          [  -3.5845,    7.8611,    8.0106],\\n\",\n-       \"          [  -6.1668,    6.1222,    8.1268],\\n\",\n-       \"          [  -5.3776,    5.6234,    9.3411]],\\n\",\n-       \" \\n\",\n-       \"         [[   5.1565,   -8.1926,   18.2980],\\n\",\n-       \"          [   4.2819,   -7.6345,   19.3660],\\n\",\n-       \"          [   3.1236,   -8.5983,   19.6553],\\n\",\n+       \"(tensor([[[ 239.4760,  376.4950,  244.9000],\\n\",\n+       \"          [ 238.9000,  377.8010,  245.2000],\\n\",\n+       \"          [ 237.8190,  377.6950,  246.2690],\\n\",\n        \"          ...,\\n\",\n        \"          [-100.0000, -100.0000, -100.0000],\\n\",\n        \"          [-100.0000, -100.0000, -100.0000],\\n\",\n-       \"          [-100.0000, -100.0000, -100.0000]]]),\\n\",\n-       \" tensor([[[ -20.9630,   15.4670,   28.5250],\\n\",\n-       \"          [ -20.3050,   16.5060,   27.7270],\\n\",\n-       \"          [ -18.9740,   15.9710,   27.1630],\\n\",\n-       \"          ...,\\n\",\n-       \"          [  -0.7240,   -5.2800,    4.0790],\\n\",\n-       \"          [  -0.9060,   -6.4480,    4.9780],\\n\",\n-       \"          [  -2.0090,   -6.1390,    5.9730]],\\n\",\n+       \"          [-100.0000, -100.0000, -100.0000]],\\n\",\n        \" \\n\",\n-       \"         [[   5.0550,   -8.1610,   18.0120],\\n\",\n-       \"          [   4.3920,   -7.8030,   19.2620],\\n\",\n-       \"          [   3.3010,   -8.8210,   19.5290],\\n\",\n+       \"         [[  31.4372, -103.9317,   46.1102],\\n\",\n+       \"          [  32.4872, -104.3322,   45.1591],\\n\",\n+       \"          [  32.0485, -104.2013,   43.6919],\\n\",\n+       \"          ...,\\n\",\n+       \"          [  10.1052, -129.8667,   21.4974],\\n\",\n+       \"          [  13.3537, -129.0618,   23.8115],\\n\",\n+       \"          [  12.3266, -127.9412,   23.6833]]]),\\n\",\n+       \" tensor([[[ 239.4760,  376.4950,  244.9000],\\n\",\n+       \"          [ 238.9000,  377.8010,  245.2000],\\n\",\n+       \"          [ 237.8190,  377.6950,  246.2690],\\n\",\n        \"          ...,\\n\",\n        \"          [-100.0000, -100.0000, -100.0000],\\n\",\n        \"          [-100.0000, -100.0000, -100.0000],\\n\",\n-       \"          [-100.0000, -100.0000, -100.0000]]]))\"\n+       \"          [-100.0000, -100.0000, -100.0000]],\\n\",\n+       \" \\n\",\n+       \"         [[  31.9020, -105.5020,   46.4300],\\n\",\n+       \"          [  31.2640, -104.3600,   45.7100],\\n\",\n+       \"          [  31.5600, -104.3840,   44.1990],\\n\",\n+       \"          ...,\\n\",\n+       \"          [   8.6080, -130.5800,   20.6600],\\n\",\n+       \"          [   7.2110, -130.2610,   21.1240],\\n\",\n+       \"          [   7.2070, -128.8110,   21.6200]]]))\"\n       ]\n      },\n-     \"execution_count\": 23,\n+     \"execution_count\": 46,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\ndiff --git a/examples/pinder-mlsb.ipynb b/examples/pinder-mlsb.ipynb\nindex fde36cd..a51a6dd 100644\n--- a/examples/pinder-mlsb.ipynb\n+++ b/examples/pinder-mlsb.ipynb\n@@ -232,20 +232,20 @@\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"({'native': 'pdbs/8ir7__S1_P51765--8ir7__AA1_Q7DGD4.pdb',\\n\",\n-       \"  'holo_R': 'pdbs/8ir7__S1_P51765-R.pdb',\\n\",\n-       \"  'holo_L': 'pdbs/8ir7__AA1_Q7DGD4-L.pdb',\\n\",\n-       \"  'predicted_R': 'pdbs/af__P51765.pdb',\\n\",\n-       \"  'predicted_L': 'pdbs/af__Q7DGD4.pdb',\\n\",\n+       \"({'native': 'pdbs/6iat__C36_Q859I3--6iat__C40_Q859I3.pdb',\\n\",\n+       \"  'holo_R': 'pdbs/6iat__C36_Q859I3-R.pdb',\\n\",\n+       \"  'holo_L': 'pdbs/6iat__C40_Q859I3-L.pdb',\\n\",\n+       \"  'predicted_R': '',\\n\",\n+       \"  'predicted_L': '',\\n\",\n        \"  'apo_R': '',\\n\",\n        \"  'apo_L': '',\\n\",\n        \"  'apo_R_alt': [],\\n\",\n        \"  'apo_L_alt': []},\\n\",\n-       \" {'native': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/8ir7__S1_P51765--8ir7__AA1_Q7DGD4.pdb'),\\n\",\n-       \"  'holo_R': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/8ir7__S1_P51765-R.pdb'),\\n\",\n-       \"  'holo_L': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/8ir7__AA1_Q7DGD4-L.pdb'),\\n\",\n-       \"  'predicted_R': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/af__P51765.pdb'),\\n\",\n-       \"  'predicted_L': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/af__Q7DGD4.pdb'),\\n\",\n+       \" {'native': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6iat__C36_Q859I3--6iat__C40_Q859I3.pdb'),\\n\",\n+       \"  'holo_R': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6iat__C36_Q859I3-R.pdb'),\\n\",\n+       \"  'holo_L': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6iat__C40_Q859I3-L.pdb'),\\n\",\n+       \"  'predicted_R': '',\\n\",\n+       \"  'predicted_L': '',\\n\",\n        \"  'apo_R': '',\\n\",\n        \"  'apo_L': '',\\n\",\n        \"  'apo_R_alt': [],\\n\",\n@@ -309,11 +309,11 @@\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"{'native': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6rao__H2_Q6HAD2--6rao__F1_Q6HAD0.pdb'),\\n\",\n-       \" 'holo_R_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6rao__H2_Q6HAD2-R.pdb'),\\n\",\n-       \" 'holo_L_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6rao__F1_Q6HAD0-L.pdb'),\\n\",\n-       \" 'predicted_R_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/af__Q6HAD2.pdb'),\\n\",\n-       \" 'predicted_L_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/af__Q6HAD0.pdb'),\\n\",\n+       \"{'native': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/7mu1__A37_Q9WZP2--7mu1__A48_Q9WZP2.pdb'),\\n\",\n+       \" 'holo_R_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/7mu1__A37_Q9WZP2-R.pdb'),\\n\",\n+       \" 'holo_L_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/7mu1__A48_Q9WZP2-L.pdb'),\\n\",\n+       \" 'predicted_R_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/af__Q9WZP2.pdb'),\\n\",\n+       \" 'predicted_L_pdb': PosixPath('/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/af__Q9WZP2.pdb'),\\n\",\n        \" 'apo_R_pdb': '',\\n\",\n        \" 'apo_L_pdb': '',\\n\",\n        \" 'apo_R_pdbs': [''],\\n\",\n@@ -359,7 +359,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 5,\n    \"id\": \"575049d7-e550-4b59-98db-0d7b045e391a\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -367,23 +367,22 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"2024-09-05 16:50:41,522 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=7, items=7\\n\",\n-      \"2024-09-05 16:50:41,943 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.42s\\n\"\n+      \"2024-09-25 16:32:28,261 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=7, items=7\\n\"\n      ]\n     },\n     {\n      \"data\": {\n       \"text/plain\": [\n        \"Structure(\\n\",\n-       \"    filepath=/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/6rao__H2_Q6HAD2-R.pdb,\\n\",\n+       \"    filepath=/Users/danielkovtun/.local/share/pinder/2024-02/pdbs/7mu1__A37_Q9WZP2-R.pdb,\\n\",\n        \"    uniprot_map=None,\\n\",\n-       \"    pinder_id='6rao__H2_Q6HAD2-R',\\n\",\n-       \"    atom_array=<class 'biotite.structure.AtomArray'> with shape (1737,),\\n\",\n+       \"    pinder_id='7mu1__A37_Q9WZP2-R',\\n\",\n+       \"    atom_array=<class 'biotite.structure.AtomArray'> with shape (2141,),\\n\",\n        \"    pdb_engine='fastpdb',\\n\",\n        \")\"\n       ]\n      },\n-     \"execution_count\": 6,\n+     \"execution_count\": 5,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -402,26 +401,26 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 7,\n+   \"execution_count\": 6,\n    \"id\": \"a31f5f1d-7fac-4cde-a1f1-e151afeb17e7\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"array([[ 30.417, -28.254,  74.132],\\n\",\n-       \"       [ 31.12 , -28.716,  75.321],\\n\",\n-       \"       [ 32.127, -29.81 ,  74.98 ],\\n\",\n-       \"       [ 32.824, -29.725,  73.966],\\n\",\n-       \"       [ 30.125, -29.23 ,  76.361],\\n\",\n-       \"       [ 29.491, -30.413,  75.911],\\n\",\n-       \"       [ 32.233, -30.777,  75.9  ],\\n\",\n-       \"       [ 32.949, -32.052,  75.825],\\n\",\n-       \"       [ 34.468, -31.925,  75.898],\\n\",\n-       \"       [ 35.158, -32.93 ,  76.097]], dtype=float32)\"\n+       \"array([[150.746, 157.31 , 247.742],\\n\",\n+       \"       [150.862, 158.701, 248.154],\\n\",\n+       \"       [150.855, 158.698, 249.688],\\n\",\n+       \"       [150.926, 157.633, 250.298],\\n\",\n+       \"       [149.724, 159.536, 247.537],\\n\",\n+       \"       [149.737, 161.039, 247.82 ],\\n\",\n+       \"       [148.383, 161.934, 247.037],\\n\",\n+       \"       [146.994, 161.391, 248.026],\\n\",\n+       \"       [150.77 , 159.87 , 250.317],\\n\",\n+       \"       [150.906, 159.979, 251.762]], dtype=float32)\"\n       ]\n      },\n-     \"execution_count\": 7,\n+     \"execution_count\": 6,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -432,17 +431,17 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 8,\n+   \"execution_count\": 7,\n    \"id\": \"3c9bceba-819b-4d32-b95a-86239ad6e1bf\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"'SLLERGLSKLTLNAWKDREGKIPAGSMSAMYNPETIQLDYQTRFDTEDTINTASQSNRYVISEPVGLNLTLLFDSQMPGNTTPIETQLAMLKSLCAVDAATGSPYFLRITWGKMRWENKGWFAGRARDLSVTYTLFDRDATPLRATVQLSLVADESFVIQQSLKTQSAPDRALVSVPDLASLPLLALSAGGVLASSVDYLSLAWDNDLDNLDDFQTGDFLRATK'\"\n+       \"'MEFLKRSFAPLTEKQWQEIDNRAREIFKTQLYGRKFVDVEGPYGWEYAAHPLGEVEVLSDENEVVKWGLRKSLPLIELRATFTLDLWELDNLERGKPNVDLSSLEETVRKVAEFEDEVIFRGCEKSGVKGLLSFEERKIECGSTPKDLLEAIVRALSIFSKDGIEGPYTLVINTDRWINFLKEEAGHYPLEKRVEECLRGGKIITTPRIEDALVVSERGGDFKLILGQDLSIGYEDREKDAVRLFITETFTFQVVNPEALILLK'\"\n       ]\n      },\n-     \"execution_count\": 8,\n+     \"execution_count\": 7,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -453,17 +452,17 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 9,\n+   \"execution_count\": 8,\n    \"id\": \"0c11d67a-c186-4710-bf4d-3589b1fe483f\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"'>6rao__H2_Q6HAD2-R\\\\nSLLERGLSKLTLNAWKDREGKIPAGSMSAMYNPETIQLDYQTRFDTEDTINTASQSNRYVISEPVGLNLTLLFDSQMPGNTTPIETQLAMLKSLCAVDAATGSPYFLRITWGKMRWENKGWFAGRARDLSVTYTLFDRDATPLRATVQLSLVADESFVIQQSLKTQSAPDRALVSVPDLASLPLLALSAGGVLASSVDYLSLAWDNDLDNLDDFQTGDFLRATK'\"\n+       \"'>7mu1__A37_Q9WZP2-R\\\\nMEFLKRSFAPLTEKQWQEIDNRAREIFKTQLYGRKFVDVEGPYGWEYAAHPLGEVEVLSDENEVVKWGLRKSLPLIELRATFTLDLWELDNLERGKPNVDLSSLEETVRKVAEFEDEVIFRGCEKSGVKGLLSFEERKIECGSTPKDLLEAIVRALSIFSKDGIEGPYTLVINTDRWINFLKEEAGHYPLEKRVEECLRGGKIITTPRIEDALVVSERGGDFKLILGQDLSIGYEDREKDAVRLFITETFTFQVVNPEALILLK'\"\n       ]\n      },\n-     \"execution_count\": 9,\n+     \"execution_count\": 8,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -474,7 +473,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 10,\n+   \"execution_count\": 9,\n    \"id\": \"3fd4c712-625c-4f80-ace4-b4c1a1b8a600\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -516,7 +515,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n+   \"execution_count\": 10,\n    \"id\": \"ee6840bb-f6db-4911-8bef-a900bf4a9c8e\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -690,7 +689,7 @@\n        \")\"\n       ]\n      },\n-     \"execution_count\": 11,\n+     \"execution_count\": 10,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -736,7 +735,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 12,\n+   \"execution_count\": 11,\n    \"id\": \"16e07a40-bc85-412d-bd52-858e4ce8d05b\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -759,7 +758,7 @@\n        \" ))\"\n       ]\n      },\n-     \"execution_count\": 12,\n+     \"execution_count\": 11,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -782,7 +781,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 13,\n+   \"execution_count\": 12,\n    \"id\": \"764dae70-980e-4cd3-9393-78a26bcf6136\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -792,7 +791,7 @@\n        \"'PLGSMFERFTDRARRVVVLAQEEARMLNHNYIGTEHILLGLIHEGEGVAAKSLESLGISLEGVRSQVEEIIGQGQQAPSGHIPFTPRAKKVLELSLREALQLGHNYIGTEHILLGLIREGEGVAAQVLVKLGAELTRVRQQVIQLLSGY'\"\n       ]\n      },\n-     \"execution_count\": 13,\n+     \"execution_count\": 12,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -803,7 +802,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n+   \"execution_count\": 13,\n    \"id\": \"b0d94de4-68bb-456f-9ac5-a6c93ad79613\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -817,7 +816,7 @@\n        \"       [-14.842, -18.494, -12.077]], dtype=float32)\"\n       ]\n      },\n-     \"execution_count\": 14,\n+     \"execution_count\": 13,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -836,7 +835,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 15,\n+   \"execution_count\": 14,\n    \"id\": \"a2965877-9130-4b0b-a9a0-e3352cd4d68f\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -852,7 +851,7 @@\n        \"])\"\n       ]\n      },\n-     \"execution_count\": 15,\n+     \"execution_count\": 14,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -886,7 +885,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 2,\n+   \"execution_count\": 15,\n    \"id\": \"3e71a783-0929-413a-80e4-8a6ca6f46c74\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -896,7 +895,7 @@\n        \"PinderLoader(split=val, monomers=holo, systems=1958)\"\n       ]\n      },\n-     \"execution_count\": 2,\n+     \"execution_count\": 15,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -955,7 +954,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 17,\n+   \"execution_count\": 16,\n    \"id\": \"2fc9dc84-d7ae-4765-82f1-4df75a45fd7f\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -971,7 +970,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 19,\n+   \"execution_count\": 17,\n    \"id\": \"4c3200cc-0f42-4737-91a6-a490f2af55ab\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -979,7 +978,7 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"  0%|\u2589                                                                                                                                                                                                | 9/1958 [00:02<07:35,  4.28it/s]\\n\"\n+      \"  0%|\u2588                                                                                                                                                                                                                                               | 9/1958 [00:02<07:39,  4.24it/s]\\n\"\n      ]\n     }\n    ],\n@@ -998,7 +997,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 20,\n+   \"execution_count\": 18,\n    \"id\": \"0625b00a-1208-48e6-908d-08de0be3f221\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1008,7 +1007,7 @@\n        \"10\"\n       ]\n      },\n-     \"execution_count\": 20,\n+     \"execution_count\": 18,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1019,17 +1018,17 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 3,\n+   \"execution_count\": 19,\n    \"id\": \"d9154449-15c6-4095-b3be-6e64721324dc\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"<pinder.core.loader.dataset.PinderDataset at 0x1fdbac640>\"\n+       \"<pinder.core.loader.dataset.PinderDataset at 0x216abf490>\"\n       ]\n      },\n-     \"execution_count\": 3,\n+     \"execution_count\": 19,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1090,6 +1089,7 @@\n     \"Each of the `target_complex` and `feature_complex` values are dictionaries with structural properties encoded by the `pinder.core.loader.geodata.structure2tensor` function by default:\\n\",\n     \"* `atom_coordinates`\\n\",\n     \"* `atom_types`\\n\",\n+    \"* `element_types`\\n\",\n     \"* `chain_ids`\\n\",\n     \"* `residue_coordinates`\\n\",\n     \"* `residue_types`\\n\",\n@@ -1100,20 +1100,27 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 4,\n+   \"execution_count\": 20,\n    \"id\": \"38ec8e3d-ba0f-4ff2-a8d1-44be72e32ad6\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"{'target_complex': {'atom_types': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n+       \"{'target_complex': {'atom_types': tensor([[0.],\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          ...,\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          [3.]]),\\n\",\n+       \"  'element_types': tensor([[3.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n        \"          ...,\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [0., 0., 1.,  ..., 0., 0., 0.]]),\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [2.]]),\\n\",\n        \"  'residue_types': tensor([[16.],\\n\",\n        \"          [16.],\\n\",\n        \"          [16.],\\n\",\n@@ -1128,22 +1135,351 @@\n        \"          [177.7620, 463.8650, 166.9020],\\n\",\n        \"          [177.4130, 465.0800, 167.7550],\\n\",\n        \"          [176.8000, 464.9490, 168.8150]]),\\n\",\n-       \"  'residue_coordinates': tensor([[131.7500, 429.3090, 163.5360],\\n\",\n-       \"          [132.6810, 428.2520, 163.1550],\\n\",\n-       \"          [133.5150, 428.6750, 161.9500],\\n\",\n-       \"          ...,\\n\",\n-       \"          [177.7620, 463.8650, 166.9020],\\n\",\n-       \"          [177.4130, 465.0800, 167.7550],\\n\",\n-       \"          [176.8000, 464.9490, 168.8150]]),\\n\",\n+       \"  'residue_coordinates': tensor([[132.6810, 428.2520, 163.1550],\\n\",\n+       \"          [133.5560, 429.2490, 159.5910],\\n\",\n+       \"          [133.8750, 432.8980, 160.6290],\\n\",\n+       \"          [136.1110, 431.8050, 163.5130],\\n\",\n+       \"          [138.3420, 429.8920, 161.0830],\\n\",\n+       \"          [138.5230, 432.9090, 158.7600],\\n\",\n+       \"          [139.4710, 435.1730, 161.6750],\\n\",\n+       \"          [142.1200, 432.6310, 162.6740],\\n\",\n+       \"          [143.5890, 432.7500, 159.1600],\\n\",\n+       \"          [143.6190, 436.5600, 159.1540],\\n\",\n+       \"          [145.2600, 436.8040, 162.5830],\\n\",\n+       \"          [147.8380, 434.1320, 161.7330],\\n\",\n+       \"          [148.7390, 435.8920, 158.4790],\\n\",\n+       \"          [149.0640, 439.2630, 160.2240],\\n\",\n+       \"          [151.2220, 437.7670, 162.9840],\\n\",\n+       \"          [153.4710, 435.8030, 160.6120],\\n\",\n+       \"          [153.9420, 438.9180, 158.4710],\\n\",\n+       \"          [156.1140, 440.2660, 161.3150],\\n\",\n+       \"          [158.3330, 437.1860, 161.4250],\\n\",\n+       \"          [161.8090, 436.4120, 160.1100],\\n\",\n+       \"          [161.1120, 432.8370, 158.9350],\\n\",\n+       \"          [157.4020, 432.0710, 159.3930],\\n\",\n+       \"          [156.5290, 428.3870, 159.0310],\\n\",\n+       \"          [154.0400, 428.1160, 156.1500],\\n\",\n+       \"          [153.3520, 424.6590, 154.7510],\\n\",\n+       \"          [152.8370, 424.8210, 150.9870],\\n\",\n+       \"          [150.9700, 422.4160, 148.7320],\\n\",\n+       \"          [153.5900, 422.6860, 145.9670],\\n\",\n+       \"          [157.3050, 423.3410, 145.6600],\\n\",\n+       \"          [158.4860, 426.6730, 144.2700],\\n\",\n+       \"          [160.7730, 427.0880, 141.2590],\\n\",\n+       \"          [163.9630, 429.0530, 141.9600],\\n\",\n+       \"          [166.4230, 427.4330, 139.5550],\\n\",\n+       \"          [167.6170, 430.8330, 138.3250],\\n\",\n+       \"          [170.5180, 431.9210, 140.5320],\\n\",\n+       \"          [169.7800, 435.6680, 140.3160],\\n\",\n+       \"          [167.6790, 437.8790, 142.6110],\\n\",\n+       \"          [167.3350, 435.1910, 145.2890],\\n\",\n+       \"          [168.9090, 437.3000, 148.0580],\\n\",\n+       \"          [166.1860, 439.9440, 148.5750],\\n\",\n+       \"          [164.1010, 438.1300, 151.1890],\\n\",\n+       \"          [163.0190, 438.8690, 154.7450],\\n\",\n+       \"          [164.6750, 435.6560, 155.9860],\\n\",\n+       \"          [168.1170, 436.6550, 154.6330],\\n\",\n+       \"          [169.8040, 437.0930, 158.0010],\\n\",\n+       \"          [173.2680, 438.4850, 158.5750],\\n\",\n+       \"          [174.9270, 440.8490, 156.1460],\\n\",\n+       \"          [175.9170, 440.8010, 152.5000],\\n\",\n+       \"          [179.6060, 441.0910, 153.4330],\\n\",\n+       \"          [180.8000, 439.9830, 156.8710],\\n\",\n+       \"          [184.2540, 439.9980, 158.4580],\\n\",\n+       \"          [185.5840, 437.2970, 160.7670],\\n\",\n+       \"          [186.4140, 439.9000, 163.4480],\\n\",\n+       \"          [182.9430, 441.4570, 163.7500],\\n\",\n+       \"          [181.4930, 442.0910, 167.2100],\\n\",\n+       \"          [178.0780, 440.5360, 167.9060],\\n\",\n+       \"          [176.1650, 440.9290, 171.1790],\\n\",\n+       \"          [173.0530, 438.9530, 172.1310],\\n\",\n+       \"          [170.3990, 440.6990, 174.2170],\\n\",\n+       \"          [166.7880, 440.0710, 175.2260],\\n\",\n+       \"          [164.4480, 442.1010, 173.0340],\\n\",\n+       \"          [160.9780, 443.6110, 173.3810],\\n\",\n+       \"          [158.8330, 443.9000, 170.2730],\\n\",\n+       \"          [160.1820, 442.9360, 166.8360],\\n\",\n+       \"          [163.5010, 444.7230, 166.1630],\\n\",\n+       \"          [164.6310, 442.4290, 163.3160],\\n\",\n+       \"          [166.6330, 444.2560, 160.6240],\\n\",\n+       \"          [166.1700, 447.5210, 162.5220],\\n\",\n+       \"          [168.1350, 449.9450, 164.6440],\\n\",\n+       \"          [168.0040, 449.0470, 168.3240],\\n\",\n+       \"          [168.4210, 451.0540, 171.5220],\\n\",\n+       \"          [169.0440, 450.0360, 175.1300],\\n\",\n+       \"          [166.1650, 450.3680, 177.6100],\\n\",\n+       \"          [166.7800, 449.8300, 181.3340],\\n\",\n+       \"          [163.8480, 448.9150, 183.5760],\\n\",\n+       \"          [163.3500, 443.9770, 187.9750],\\n\",\n+       \"          [165.0240, 442.4930, 184.8870],\\n\",\n+       \"          [168.1120, 443.0980, 182.7810],\\n\",\n+       \"          [168.1280, 445.9210, 180.2050],\\n\",\n+       \"          [166.7230, 445.0130, 176.8030],\\n\",\n+       \"          [167.0690, 446.1330, 173.1930],\\n\",\n+       \"          [164.0790, 447.8390, 171.5780],\\n\",\n+       \"          [163.4630, 449.0630, 168.0350],\\n\",\n+       \"          [164.4160, 452.7320, 168.0130],\\n\",\n+       \"          [166.7460, 455.3190, 166.5790],\\n\",\n+       \"          [167.5460, 459.0090, 166.8440],\\n\",\n+       \"          [170.2010, 459.9620, 169.3810],\\n\",\n+       \"          [169.5150, 456.8220, 171.4570],\\n\",\n+       \"          [170.7440, 454.3880, 168.7860],\\n\",\n+       \"          [172.8900, 451.6810, 170.3850],\\n\",\n+       \"          [173.2420, 449.3000, 167.4490],\\n\",\n+       \"          [171.4700, 447.3670, 164.7150],\\n\",\n+       \"          [169.7840, 443.9650, 164.9670],\\n\",\n+       \"          [170.8880, 441.4120, 162.3680],\\n\",\n+       \"          [169.3320, 438.1640, 163.6510],\\n\",\n+       \"          [166.7170, 436.9060, 166.1030],\\n\",\n+       \"          [166.2830, 433.5290, 167.7900],\\n\",\n+       \"          [162.5490, 433.4260, 168.5040],\\n\",\n+       \"          [162.6440, 430.3540, 170.7620],\\n\",\n+       \"          [164.9490, 432.1120, 173.2350],\\n\",\n+       \"          [163.6530, 435.5760, 172.2210],\\n\",\n+       \"          [167.2540, 436.7450, 171.8300],\\n\",\n+       \"          [168.3710, 439.3650, 169.3060],\\n\",\n+       \"          [171.8910, 439.6680, 167.8930],\\n\",\n+       \"          [173.1360, 443.2500, 167.5870],\\n\",\n+       \"          [176.0990, 444.7950, 165.8030],\\n\",\n+       \"          [177.1670, 447.8660, 167.8270],\\n\",\n+       \"          [177.4160, 451.2430, 166.1360],\\n\",\n+       \"          [181.1790, 451.0810, 166.7480],\\n\",\n+       \"          [181.3560, 448.4350, 163.9980],\\n\",\n+       \"          [180.6020, 448.8920, 160.3060],\\n\",\n+       \"          [177.8440, 446.8990, 158.6170],\\n\",\n+       \"          [176.7540, 446.3160, 155.0150],\\n\",\n+       \"          [173.3820, 444.6900, 154.4770],\\n\",\n+       \"          [169.6430, 444.9660, 153.9760],\\n\",\n+       \"          [167.6080, 446.9450, 156.5140],\\n\",\n+       \"          [163.9630, 447.9270, 156.8800],\\n\",\n+       \"          [163.1520, 451.2160, 155.1430],\\n\",\n+       \"          [160.3870, 453.6570, 156.0420],\\n\",\n+       \"          [158.9330, 453.6920, 152.5150],\\n\",\n+       \"          [159.8050, 453.2460, 148.8360],\\n\",\n+       \"          [161.2560, 456.7540, 148.4440],\\n\",\n+       \"          [164.9420, 455.8330, 148.8520],\\n\",\n+       \"          [167.3780, 455.9500, 145.9260],\\n\",\n+       \"          [171.0380, 455.0610, 145.5060],\\n\",\n+       \"          [173.5830, 457.5340, 146.9270],\\n\",\n+       \"          [171.2360, 458.9430, 149.5740],\\n\",\n+       \"          [171.8150, 459.8640, 153.2130],\\n\",\n+       \"          [169.5910, 457.9090, 155.6000],\\n\",\n+       \"          [168.4960, 458.3960, 159.2100],\\n\",\n+       \"          [167.0870, 455.9020, 161.7110],\\n\",\n+       \"          [163.6430, 457.0390, 162.8610],\\n\",\n+       \"          [161.8700, 456.3550, 166.1660],\\n\",\n+       \"          [160.7490, 452.9260, 164.8950],\\n\",\n+       \"          [164.2440, 451.8750, 163.8110],\\n\",\n+       \"          [163.4490, 452.3040, 160.1090],\\n\",\n+       \"          [165.6820, 454.0140, 157.5590],\\n\",\n+       \"          [164.2940, 457.2310, 156.0670],\\n\",\n+       \"          [165.6810, 459.7290, 153.5780],\\n\",\n+       \"          [167.3870, 462.7790, 155.0760],\\n\",\n+       \"          [170.2060, 465.8340, 161.8360],\\n\",\n+       \"          [172.1620, 462.7680, 160.6890],\\n\",\n+       \"          [174.0280, 461.7040, 163.8180],\\n\",\n+       \"          [175.2110, 458.5660, 161.9930],\\n\",\n+       \"          [176.5410, 458.2410, 158.4450],\\n\",\n+       \"          [174.5550, 455.7530, 156.3540],\\n\",\n+       \"          [174.3580, 455.6030, 152.5600],\\n\",\n+       \"          [171.8590, 453.6260, 150.4910],\\n\",\n+       \"          [173.6300, 451.2730, 148.0810],\\n\",\n+       \"          [170.4820, 450.1750, 146.2000],\\n\",\n+       \"          [166.8520, 451.1980, 145.7810],\\n\",\n+       \"          [164.0730, 450.1880, 148.1490],\\n\",\n+       \"          [162.6710, 446.7750, 147.1980],\\n\",\n+       \"          [159.1420, 445.8080, 148.2240],\\n\",\n+       \"          [158.3160, 442.3640, 149.5950],\\n\",\n+       \"          [155.0300, 443.1470, 151.4050],\\n\",\n+       \"          [152.6800, 446.0960, 151.8500],\\n\",\n+       \"          [155.0610, 447.4890, 154.4910],\\n\",\n+       \"          [158.2010, 445.3460, 154.0190],\\n\",\n+       \"          [160.8090, 447.5340, 152.2870],\\n\",\n+       \"          [164.4130, 446.3150, 152.1360],\\n\",\n+       \"          [167.3360, 448.5380, 151.1380],\\n\",\n+       \"          [171.0310, 447.6630, 150.9500],\\n\",\n+       \"          [172.8160, 450.1440, 153.2260],\\n\",\n+       \"          [176.3550, 450.4540, 154.5850],\\n\",\n+       \"          [176.6310, 452.0780, 158.0240],\\n\",\n+       \"          [180.1600, 453.0970, 159.0310],\\n\",\n+       \"          [179.9110, 454.9710, 162.3160],\\n\",\n+       \"          [178.7370, 458.4030, 163.4610],\\n\",\n+       \"          [179.1350, 461.9150, 162.0830],\\n\",\n+       \"          [180.3160, 464.9940, 163.9950],\\n\",\n+       \"          [195.1700, 454.0210, 194.1160],\\n\",\n+       \"          [196.4220, 450.5570, 193.1610],\\n\",\n+       \"          [195.0530, 447.5260, 191.3340],\\n\",\n+       \"          [195.7530, 445.4320, 194.4410],\\n\",\n+       \"          [195.6760, 446.8960, 197.9710],\\n\",\n+       \"          [199.1230, 447.9300, 199.1640],\\n\",\n+       \"          [198.4070, 447.0140, 202.7840],\\n\",\n+       \"          [198.1130, 443.3080, 203.5090],\\n\",\n+       \"          [195.0350, 441.7530, 205.1060],\\n\",\n+       \"          [194.7040, 440.1720, 208.5300],\\n\",\n+       \"          [191.7420, 437.9460, 207.6670],\\n\",\n+       \"          [190.9460, 435.0990, 205.3140],\\n\",\n+       \"          [188.0870, 435.6760, 202.8880],\\n\",\n+       \"          [184.8300, 433.7320, 203.1830],\\n\",\n+       \"          [184.1270, 431.8260, 199.9570],\\n\",\n+       \"          [180.5450, 430.8500, 200.7460],\\n\",\n+       \"          [178.2470, 433.3320, 199.0220],\\n\",\n+       \"          [178.2770, 433.9260, 195.2680],\\n\",\n+       \"          [176.3170, 437.2030, 195.0280],\\n\",\n+       \"          [177.7690, 440.7330, 194.9660],\\n\",\n+       \"          [181.3350, 439.3910, 195.1330],\\n\",\n+       \"          [182.5820, 441.3460, 192.0940],\\n\",\n+       \"          [182.3530, 444.9560, 193.3610],\\n\",\n+       \"          [186.0870, 445.2240, 194.0290],\\n\",\n+       \"          [188.7230, 447.5570, 192.6190],\\n\",\n+       \"          [190.8760, 444.5870, 191.5490],\\n\",\n+       \"          [188.2670, 443.5230, 188.9590],\\n\",\n+       \"          [190.1360, 444.3010, 185.7360],\\n\",\n+       \"          [188.8140, 444.1990, 182.1980],\\n\",\n+       \"          [185.1300, 444.1560, 181.3470],\\n\",\n+       \"          [182.1290, 442.2030, 182.6060],\\n\",\n+       \"          [181.3330, 441.2700, 178.9860],\\n\",\n+       \"          [184.3690, 440.6570, 176.7700],\\n\",\n+       \"          [184.4240, 439.7520, 173.0740],\\n\",\n+       \"          [187.1540, 437.5560, 171.6020],\\n\",\n+       \"          [187.5170, 439.9750, 168.6500],\\n\",\n+       \"          [188.4180, 443.0500, 170.7240],\\n\",\n+       \"          [191.4360, 445.2100, 169.8910],\\n\",\n+       \"          [193.7480, 445.6510, 172.8940],\\n\",\n+       \"          [196.8200, 447.8990, 173.0280],\\n\",\n+       \"          [199.4910, 448.6120, 175.6310],\\n\",\n+       \"          [201.2080, 451.9090, 176.3780],\\n\",\n+       \"          [203.6110, 453.4740, 178.8680],\\n\",\n+       \"          [201.6130, 454.9210, 181.7570],\\n\",\n+       \"          [202.0690, 458.3840, 183.2730],\\n\",\n+       \"          [200.4970, 458.3690, 186.7180],\\n\",\n+       \"          [197.7960, 455.8130, 187.5240],\\n\",\n+       \"          [195.1460, 455.8470, 184.7720],\\n\",\n+       \"          [193.6160, 452.5740, 185.9790],\\n\",\n+       \"          [189.8600, 452.3800, 185.3150],\\n\",\n+       \"          [189.9440, 455.8580, 183.7680],\\n\",\n+       \"          [189.7400, 457.4310, 180.3410],\\n\",\n+       \"          [193.1630, 457.9480, 178.8100],\\n\",\n+       \"          [194.6770, 460.5170, 176.4640],\\n\",\n+       \"          [197.9720, 460.6430, 174.6010],\\n\",\n+       \"          [200.7710, 462.5710, 176.3210],\\n\",\n+       \"          [203.7970, 463.7400, 174.3330],\\n\",\n+       \"          [207.1150, 464.5820, 175.9770],\\n\",\n+       \"          [210.1390, 466.5260, 174.8060],\\n\",\n+       \"          [210.8850, 463.9390, 172.1200],\\n\",\n+       \"          [213.6440, 461.8050, 173.6150],\\n\",\n+       \"          [211.1280, 459.2430, 174.9140],\\n\",\n+       \"          [208.0390, 457.7610, 173.2930],\\n\",\n+       \"          [204.4770, 458.9110, 173.9310],\\n\",\n+       \"          [202.5240, 457.8000, 177.0030],\\n\",\n+       \"          [198.9190, 457.7220, 178.2440],\\n\",\n+       \"          [197.5730, 459.9040, 181.0500],\\n\",\n+       \"          [194.2310, 460.2350, 182.8060],\\n\",\n+       \"          [192.1140, 462.6430, 180.7850],\\n\",\n+       \"          [189.0250, 463.1720, 178.7000],\\n\",\n+       \"          [187.1240, 465.8360, 176.8220],\\n\",\n+       \"          [188.2850, 466.3320, 173.2420],\\n\",\n+       \"          [191.5580, 464.4520, 173.8600],\\n\",\n+       \"          [189.9950, 461.1280, 174.9010],\\n\",\n+       \"          [191.8440, 458.2970, 173.1540],\\n\",\n+       \"          [190.4670, 455.2690, 174.9900],\\n\",\n+       \"          [189.7380, 453.5820, 178.3070],\\n\",\n+       \"          [192.3390, 451.8830, 180.5000],\\n\",\n+       \"          [191.2020, 448.5340, 181.9050],\\n\",\n+       \"          [194.3800, 447.0280, 183.4050],\\n\",\n+       \"          [197.8700, 447.9650, 184.5850],\\n\",\n+       \"          [200.9840, 445.8270, 184.9660],\\n\",\n+       \"          [202.8290, 447.6220, 187.7590],\\n\",\n+       \"          [206.2120, 445.9180, 187.3400],\\n\",\n+       \"          [206.6040, 447.1310, 183.7460],\\n\",\n+       \"          [204.4430, 450.2380, 184.1830],\\n\",\n+       \"          [202.2310, 449.1370, 181.2890],\\n\",\n+       \"          [198.6660, 450.3790, 180.7720],\\n\",\n+       \"          [196.2310, 448.2490, 178.7680],\\n\",\n+       \"          [194.0550, 450.4240, 176.5260],\\n\",\n+       \"          [190.8490, 449.7090, 174.6070],\\n\",\n+       \"          [190.4360, 452.3080, 171.8240],\\n\",\n+       \"          [187.3830, 454.5590, 171.7980],\\n\",\n+       \"          [186.4670, 452.9730, 168.4480],\\n\",\n+       \"          [185.9050, 449.6480, 170.2580],\\n\",\n+       \"          [182.8890, 448.8910, 172.4330],\\n\",\n+       \"          [183.5680, 448.6030, 176.1630],\\n\",\n+       \"          [181.3330, 447.5400, 179.0590],\\n\",\n+       \"          [182.7340, 447.5270, 182.5670],\\n\",\n+       \"          [183.3280, 449.2260, 185.8920],\\n\",\n+       \"          [184.8760, 452.6980, 185.6990],\\n\",\n+       \"          [185.7140, 455.4170, 188.2030],\\n\",\n+       \"          [182.9420, 457.9530, 188.8300],\\n\",\n+       \"          [183.2930, 461.5650, 189.9350],\\n\",\n+       \"          [182.7580, 462.1320, 193.6660],\\n\",\n+       \"          [180.0620, 464.7490, 193.1030],\\n\",\n+       \"          [176.4120, 463.9030, 192.5790],\\n\",\n+       \"          [177.1550, 460.1730, 192.6930],\\n\",\n+       \"          [173.8180, 459.5980, 194.4390],\\n\",\n+       \"          [172.0770, 461.1030, 191.4040],\\n\",\n+       \"          [173.6950, 458.5410, 189.0910],\\n\",\n+       \"          [171.0060, 456.0190, 188.1270],\\n\",\n+       \"          [170.8210, 453.3390, 185.4210],\\n\",\n+       \"          [170.0180, 454.8270, 182.0350],\\n\",\n+       \"          [171.8690, 458.1080, 182.6590],\\n\",\n+       \"          [174.2320, 459.6520, 180.1210],\\n\",\n+       \"          [177.8640, 459.9130, 181.2230],\\n\",\n+       \"          [180.8780, 461.8410, 179.9490],\\n\",\n+       \"          [184.5650, 461.8070, 180.8430],\\n\",\n+       \"          [186.0650, 464.7990, 182.6440],\\n\",\n+       \"          [189.5780, 466.2740, 182.6170],\\n\",\n+       \"          [190.7120, 463.6340, 185.1390],\\n\",\n+       \"          [189.2980, 460.6630, 183.2290],\\n\",\n+       \"          [186.3560, 460.3000, 185.6240],\\n\",\n+       \"          [182.7840, 459.6590, 184.5210],\\n\",\n+       \"          [180.2130, 462.3320, 185.3590],\\n\",\n+       \"          [176.5320, 462.8410, 184.6140],\\n\",\n+       \"          [175.8580, 465.0300, 181.5790],\\n\",\n+       \"          [178.1930, 465.7110, 174.9000],\\n\",\n+       \"          [180.9150, 464.1190, 172.7670],\\n\",\n+       \"          [181.2510, 460.5420, 174.1130],\\n\",\n+       \"          [178.4430, 457.9660, 174.2140],\\n\",\n+       \"          [178.5420, 456.4790, 177.7180],\\n\",\n+       \"          [175.4730, 455.1590, 179.5500],\\n\",\n+       \"          [175.4670, 454.0690, 183.1870],\\n\",\n+       \"          [174.1210, 450.5430, 183.6830],\\n\",\n+       \"          [173.9850, 450.6340, 187.5030],\\n\",\n+       \"          [174.3640, 452.9240, 190.5240],\\n\",\n+       \"          [177.5140, 454.2200, 192.1820],\\n\",\n+       \"          [179.3060, 451.6570, 194.3540],\\n\",\n+       \"          [181.4890, 453.2110, 197.0570],\\n\",\n+       \"          [184.7390, 451.3110, 197.6330],\\n\",\n+       \"          [186.4870, 454.0230, 199.6690],\\n\",\n+       \"          [185.5270, 457.3810, 201.1790],\\n\",\n+       \"          [186.7910, 459.0050, 197.9610],\\n\",\n+       \"          [186.6010, 456.0430, 195.5320],\\n\",\n+       \"          [183.3680, 455.3620, 193.6270],\\n\",\n+       \"          [182.9110, 452.9430, 190.7280],\\n\",\n+       \"          [180.0190, 452.5230, 188.3060],\\n\",\n+       \"          [179.0910, 450.0250, 185.5960],\\n\",\n+       \"          [178.9850, 451.7840, 182.2280],\\n\",\n+       \"          [179.0510, 451.0080, 178.5100],\\n\",\n+       \"          [181.0530, 453.1550, 176.0780],\\n\",\n+       \"          [180.1320, 452.7170, 172.4060],\\n\",\n+       \"          [182.0570, 455.4530, 170.6120],\\n\",\n+       \"          [181.8140, 459.1820, 170.0770],\\n\",\n+       \"          [178.5990, 461.0580, 169.3130],\\n\",\n+       \"          [177.7620, 463.8650, 166.9020]]),\\n\",\n        \"  'residue_ids': tensor([  4.,   4.,   4.,  ..., 182., 182., 182.]),\\n\",\n        \"  'chain_ids': tensor([0., 0., 0.,  ..., 1., 1., 1.])},\\n\",\n-       \" 'feature_complex': {'atom_types': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n+       \" 'feature_complex': {'atom_types': tensor([[0.],\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n        \"          ...,\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [1., 0., 0.,  ..., 0., 0., 0.],\\n\",\n-       \"          [0., 0., 1.,  ..., 0., 0., 0.]]),\\n\",\n+       \"          [1.],\\n\",\n+       \"          [2.],\\n\",\n+       \"          [3.]]),\\n\",\n+       \"  'element_types': tensor([[3.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          ...,\\n\",\n+       \"          [0.],\\n\",\n+       \"          [0.],\\n\",\n+       \"          [2.]]),\\n\",\n        \"  'residue_types': tensor([[16.],\\n\",\n        \"          [16.],\\n\",\n        \"          [16.],\\n\",\n@@ -1155,16 +1491,338 @@\n        \"          [132.6810, 428.2520, 163.1550],\\n\",\n        \"          [133.5150, 428.6750, 161.9500],\\n\",\n        \"          ...,\\n\",\n-       \"          [198.0186, 449.2866, 161.9663],\\n\",\n-       \"          [196.8790, 450.1770, 161.4823],\\n\",\n-       \"          [195.8437, 450.2865, 162.1400]]),\\n\",\n-       \"  'residue_coordinates': tensor([[131.7500, 429.3090, 163.5360],\\n\",\n-       \"          [132.6810, 428.2520, 163.1550],\\n\",\n-       \"          [133.5150, 428.6750, 161.9500],\\n\",\n-       \"          ...,\\n\",\n-       \"          [198.0186, 449.2866, 161.9663],\\n\",\n-       \"          [196.8790, 450.1770, 161.4823],\\n\",\n-       \"          [195.8437, 450.2865, 162.1400]]),\\n\",\n+       \"          [186.8651, 447.7225, 152.6899],\\n\",\n+       \"          [185.4418, 447.8365, 153.2255],\\n\",\n+       \"          [185.1119, 447.2595, 154.2621]]),\\n\",\n+       \"  'residue_coordinates': tensor([[132.6810, 428.2520, 163.1550],\\n\",\n+       \"          [133.5560, 429.2490, 159.5910],\\n\",\n+       \"          [133.8750, 432.8980, 160.6290],\\n\",\n+       \"          [136.1110, 431.8050, 163.5130],\\n\",\n+       \"          [138.3420, 429.8920, 161.0830],\\n\",\n+       \"          [138.5230, 432.9090, 158.7600],\\n\",\n+       \"          [139.4710, 435.1730, 161.6750],\\n\",\n+       \"          [142.1200, 432.6310, 162.6740],\\n\",\n+       \"          [143.5890, 432.7500, 159.1600],\\n\",\n+       \"          [143.6190, 436.5600, 159.1540],\\n\",\n+       \"          [145.2600, 436.8040, 162.5830],\\n\",\n+       \"          [147.8380, 434.1320, 161.7330],\\n\",\n+       \"          [148.7390, 435.8920, 158.4790],\\n\",\n+       \"          [149.0640, 439.2630, 160.2240],\\n\",\n+       \"          [151.2220, 437.7670, 162.9840],\\n\",\n+       \"          [153.4710, 435.8030, 160.6120],\\n\",\n+       \"          [153.9420, 438.9180, 158.4710],\\n\",\n+       \"          [156.1140, 440.2660, 161.3150],\\n\",\n+       \"          [158.3330, 437.1860, 161.4250],\\n\",\n+       \"          [161.8090, 436.4120, 160.1100],\\n\",\n+       \"          [161.1120, 432.8370, 158.9350],\\n\",\n+       \"          [157.4020, 432.0710, 159.3930],\\n\",\n+       \"          [156.5290, 428.3870, 159.0310],\\n\",\n+       \"          [154.0400, 428.1160, 156.1500],\\n\",\n+       \"          [153.3520, 424.6590, 154.7510],\\n\",\n+       \"          [152.8370, 424.8210, 150.9870],\\n\",\n+       \"          [150.9700, 422.4160, 148.7320],\\n\",\n+       \"          [153.5900, 422.6860, 145.9670],\\n\",\n+       \"          [157.3050, 423.3410, 145.6600],\\n\",\n+       \"          [158.4860, 426.6730, 144.2700],\\n\",\n+       \"          [160.7730, 427.0880, 141.2590],\\n\",\n+       \"          [163.9630, 429.0530, 141.9600],\\n\",\n+       \"          [166.4230, 427.4330, 139.5550],\\n\",\n+       \"          [167.6170, 430.8330, 138.3250],\\n\",\n+       \"          [170.5180, 431.9210, 140.5320],\\n\",\n+       \"          [169.7800, 435.6680, 140.3160],\\n\",\n+       \"          [167.6790, 437.8790, 142.6110],\\n\",\n+       \"          [167.3350, 435.1910, 145.2890],\\n\",\n+       \"          [168.9090, 437.3000, 148.0580],\\n\",\n+       \"          [166.1860, 439.9440, 148.5750],\\n\",\n+       \"          [164.1010, 438.1300, 151.1890],\\n\",\n+       \"          [163.0190, 438.8690, 154.7450],\\n\",\n+       \"          [164.6750, 435.6560, 155.9860],\\n\",\n+       \"          [168.1170, 436.6550, 154.6330],\\n\",\n+       \"          [169.8040, 437.0930, 158.0010],\\n\",\n+       \"          [173.2680, 438.4850, 158.5750],\\n\",\n+       \"          [174.9270, 440.8490, 156.1460],\\n\",\n+       \"          [175.9170, 440.8010, 152.5000],\\n\",\n+       \"          [179.6060, 441.0910, 153.4330],\\n\",\n+       \"          [180.8000, 439.9830, 156.8710],\\n\",\n+       \"          [184.2540, 439.9980, 158.4580],\\n\",\n+       \"          [185.5840, 437.2970, 160.7670],\\n\",\n+       \"          [186.4140, 439.9000, 163.4480],\\n\",\n+       \"          [182.9430, 441.4570, 163.7500],\\n\",\n+       \"          [181.4930, 442.0910, 167.2100],\\n\",\n+       \"          [178.0780, 440.5360, 167.9060],\\n\",\n+       \"          [176.1650, 440.9290, 171.1790],\\n\",\n+       \"          [173.0530, 438.9530, 172.1310],\\n\",\n+       \"          [170.3990, 440.6990, 174.2170],\\n\",\n+       \"          [166.7880, 440.0710, 175.2260],\\n\",\n+       \"          [164.4480, 442.1010, 173.0340],\\n\",\n+       \"          [160.9780, 443.6110, 173.3810],\\n\",\n+       \"          [158.8330, 443.9000, 170.2730],\\n\",\n+       \"          [160.1820, 442.9360, 166.8360],\\n\",\n+       \"          [163.5010, 444.7230, 166.1630],\\n\",\n+       \"          [164.6310, 442.4290, 163.3160],\\n\",\n+       \"          [166.6330, 444.2560, 160.6240],\\n\",\n+       \"          [166.1700, 447.5210, 162.5220],\\n\",\n+       \"          [168.1350, 449.9450, 164.6440],\\n\",\n+       \"          [168.0040, 449.0470, 168.3240],\\n\",\n+       \"          [168.4210, 451.0540, 171.5220],\\n\",\n+       \"          [169.0440, 450.0360, 175.1300],\\n\",\n+       \"          [166.1650, 450.3680, 177.6100],\\n\",\n+       \"          [166.7800, 449.8300, 181.3340],\\n\",\n+       \"          [163.8480, 448.9150, 183.5760],\\n\",\n+       \"          [163.3500, 443.9770, 187.9750],\\n\",\n+       \"          [165.0240, 442.4930, 184.8870],\\n\",\n+       \"          [168.1120, 443.0980, 182.7810],\\n\",\n+       \"          [168.1280, 445.9210, 180.2050],\\n\",\n+       \"          [166.7230, 445.0130, 176.8030],\\n\",\n+       \"          [167.0690, 446.1330, 173.1930],\\n\",\n+       \"          [164.0790, 447.8390, 171.5780],\\n\",\n+       \"          [163.4630, 449.0630, 168.0350],\\n\",\n+       \"          [164.4160, 452.7320, 168.0130],\\n\",\n+       \"          [166.7460, 455.3190, 166.5790],\\n\",\n+       \"          [167.5460, 459.0090, 166.8440],\\n\",\n+       \"          [170.2010, 459.9620, 169.3810],\\n\",\n+       \"          [169.5150, 456.8220, 171.4570],\\n\",\n+       \"          [170.7440, 454.3880, 168.7860],\\n\",\n+       \"          [172.8900, 451.6810, 170.3850],\\n\",\n+       \"          [173.2420, 449.3000, 167.4490],\\n\",\n+       \"          [171.4700, 447.3670, 164.7150],\\n\",\n+       \"          [169.7840, 443.9650, 164.9670],\\n\",\n+       \"          [170.8880, 441.4120, 162.3680],\\n\",\n+       \"          [169.3320, 438.1640, 163.6510],\\n\",\n+       \"          [166.7170, 436.9060, 166.1030],\\n\",\n+       \"          [166.2830, 433.5290, 167.7900],\\n\",\n+       \"          [162.5490, 433.4260, 168.5040],\\n\",\n+       \"          [162.6440, 430.3540, 170.7620],\\n\",\n+       \"          [164.9490, 432.1120, 173.2350],\\n\",\n+       \"          [163.6530, 435.5760, 172.2210],\\n\",\n+       \"          [167.2540, 436.7450, 171.8300],\\n\",\n+       \"          [168.3710, 439.3650, 169.3060],\\n\",\n+       \"          [171.8910, 439.6680, 167.8930],\\n\",\n+       \"          [173.1360, 443.2500, 167.5870],\\n\",\n+       \"          [176.0990, 444.7950, 165.8030],\\n\",\n+       \"          [177.1670, 447.8660, 167.8270],\\n\",\n+       \"          [177.4160, 451.2430, 166.1360],\\n\",\n+       \"          [181.1790, 451.0810, 166.7480],\\n\",\n+       \"          [181.3560, 448.4350, 163.9980],\\n\",\n+       \"          [180.6020, 448.8920, 160.3060],\\n\",\n+       \"          [177.8440, 446.8990, 158.6170],\\n\",\n+       \"          [176.7540, 446.3160, 155.0150],\\n\",\n+       \"          [173.3820, 444.6900, 154.4770],\\n\",\n+       \"          [169.6430, 444.9660, 153.9760],\\n\",\n+       \"          [167.6080, 446.9450, 156.5140],\\n\",\n+       \"          [163.9630, 447.9270, 156.8800],\\n\",\n+       \"          [163.1520, 451.2160, 155.1430],\\n\",\n+       \"          [160.3870, 453.6570, 156.0420],\\n\",\n+       \"          [158.9330, 453.6920, 152.5150],\\n\",\n+       \"          [159.8050, 453.2460, 148.8360],\\n\",\n+       \"          [161.2560, 456.7540, 148.4440],\\n\",\n+       \"          [164.9420, 455.8330, 148.8520],\\n\",\n+       \"          [167.3780, 455.9500, 145.9260],\\n\",\n+       \"          [171.0380, 455.0610, 145.5060],\\n\",\n+       \"          [173.5830, 457.5340, 146.9270],\\n\",\n+       \"          [171.2360, 458.9430, 149.5740],\\n\",\n+       \"          [171.8150, 459.8640, 153.2130],\\n\",\n+       \"          [169.5910, 457.9090, 155.6000],\\n\",\n+       \"          [168.4960, 458.3960, 159.2100],\\n\",\n+       \"          [167.0870, 455.9020, 161.7110],\\n\",\n+       \"          [163.6430, 457.0390, 162.8610],\\n\",\n+       \"          [161.8700, 456.3550, 166.1660],\\n\",\n+       \"          [160.7490, 452.9260, 164.8950],\\n\",\n+       \"          [164.2440, 451.8750, 163.8110],\\n\",\n+       \"          [163.4490, 452.3040, 160.1090],\\n\",\n+       \"          [165.6820, 454.0140, 157.5590],\\n\",\n+       \"          [164.2940, 457.2310, 156.0670],\\n\",\n+       \"          [165.6810, 459.7290, 153.5780],\\n\",\n+       \"          [167.3870, 462.7790, 155.0760],\\n\",\n+       \"          [170.2060, 465.8340, 161.8360],\\n\",\n+       \"          [172.1620, 462.7680, 160.6890],\\n\",\n+       \"          [174.0280, 461.7040, 163.8180],\\n\",\n+       \"          [175.2110, 458.5660, 161.9930],\\n\",\n+       \"          [176.5410, 458.2410, 158.4450],\\n\",\n+       \"          [174.5550, 455.7530, 156.3540],\\n\",\n+       \"          [174.3580, 455.6030, 152.5600],\\n\",\n+       \"          [171.8590, 453.6260, 150.4910],\\n\",\n+       \"          [173.6300, 451.2730, 148.0810],\\n\",\n+       \"          [170.4820, 450.1750, 146.2000],\\n\",\n+       \"          [166.8520, 451.1980, 145.7810],\\n\",\n+       \"          [164.0730, 450.1880, 148.1490],\\n\",\n+       \"          [162.6710, 446.7750, 147.1980],\\n\",\n+       \"          [159.1420, 445.8080, 148.2240],\\n\",\n+       \"          [158.3160, 442.3640, 149.5950],\\n\",\n+       \"          [155.0300, 443.1470, 151.4050],\\n\",\n+       \"          [152.6800, 446.0960, 151.8500],\\n\",\n+       \"          [155.0610, 447.4890, 154.4910],\\n\",\n+       \"          [158.2010, 445.3460, 154.0190],\\n\",\n+       \"          [160.8090, 447.5340, 152.2870],\\n\",\n+       \"          [164.4130, 446.3150, 152.1360],\\n\",\n+       \"          [167.3360, 448.5380, 151.1380],\\n\",\n+       \"          [171.0310, 447.6630, 150.9500],\\n\",\n+       \"          [172.8160, 450.1440, 153.2260],\\n\",\n+       \"          [176.3550, 450.4540, 154.5850],\\n\",\n+       \"          [176.6310, 452.0780, 158.0240],\\n\",\n+       \"          [180.1600, 453.0970, 159.0310],\\n\",\n+       \"          [179.9110, 454.9710, 162.3160],\\n\",\n+       \"          [178.7370, 458.4030, 163.4610],\\n\",\n+       \"          [179.1350, 461.9150, 162.0830],\\n\",\n+       \"          [180.3160, 464.9940, 163.9950],\\n\",\n+       \"          [195.4340, 461.9335, 182.1035],\\n\",\n+       \"          [199.2382, 461.9114, 182.0227],\\n\",\n+       \"          [201.9684, 459.5313, 180.8911],\\n\",\n+       \"          [203.4010, 459.6168, 184.4221],\\n\",\n+       \"          [201.2254, 460.1810, 187.5138],\\n\",\n+       \"          [201.1625, 463.8214, 188.5711],\\n\",\n+       \"          [200.9260, 462.9902, 192.2737],\\n\",\n+       \"          [204.0461, 461.5012, 193.8212],\\n\",\n+       \"          [204.0666, 458.1489, 195.6112],\\n\",\n+       \"          [204.6119, 457.4490, 199.2917],\\n\",\n+       \"          [205.8504, 453.8792, 198.8491],\\n\",\n+       \"          [208.7257, 452.0784, 197.1862],\\n\",\n+       \"          [207.7980, 449.4805, 194.5819],\\n\",\n+       \"          [208.4135, 445.7754, 195.1886],\\n\",\n+       \"          [210.6612, 444.3414, 192.4637],\\n\",\n+       \"          [210.1697, 440.6754, 193.3148],\\n\",\n+       \"          [207.5283, 439.2731, 190.9761],\\n\",\n+       \"          [207.8597, 439.3451, 187.1904],\\n\",\n+       \"          [204.2639, 438.5888, 186.1246],\\n\",\n+       \"          [201.5404, 441.1361, 185.3079],\\n\",\n+       \"          [203.9241, 444.0497, 185.9199],\\n\",\n+       \"          [203.2558, 445.7530, 182.5636],\\n\",\n+       \"          [199.5896, 446.7999, 182.9544],\\n\",\n+       \"          [200.4436, 450.4320, 183.6891],\\n\",\n+       \"          [199.5210, 453.6368, 181.8842],\\n\",\n+       \"          [203.2026, 454.6235, 181.6131],\\n\",\n+       \"          [203.8955, 451.7040, 179.2375],\\n\",\n+       \"          [204.5535, 453.5907, 175.9981],\\n\",\n+       \"          [205.0174, 452.1659, 172.5296],\\n\",\n+       \"          [204.0168, 448.6487, 171.5671],\\n\",\n+       \"          [204.5046, 445.2215, 173.1233],\\n\",\n+       \"          [205.9230, 444.0098, 169.7873],\\n\",\n+       \"          [208.0113, 446.5703, 167.8936],\\n\",\n+       \"          [209.7071, 446.1658, 164.5108],\\n\",\n+       \"          [212.9677, 447.9385, 163.6932],\\n\",\n+       \"          [211.5590, 448.9689, 160.2797],\\n\",\n+       \"          [208.5749, 450.9334, 161.6230],\\n\",\n+       \"          [207.8057, 454.4637, 160.4340],\\n\",\n+       \"          [207.4872, 456.9118, 163.3431],\\n\",\n+       \"          [206.4332, 460.5624, 163.0761],\\n\",\n+       \"          [206.0788, 463.4231, 165.5477],\\n\",\n+       \"          [203.4709, 466.1752, 165.5817],\\n\",\n+       \"          [202.2743, 469.0646, 167.7365],\\n\",\n+       \"          [199.6173, 467.7885, 170.1333],\\n\",\n+       \"          [196.2579, 469.4412, 170.8270],\\n\",\n+       \"          [194.9515, 468.0990, 174.1181],\\n\",\n+       \"          [196.1949, 464.7333, 175.3846],\\n\",\n+       \"          [195.9083, 462.1365, 172.5968],\\n\",\n+       \"          [198.1073, 459.6493, 174.4645],\\n\",\n+       \"          [197.1774, 456.0215, 173.7158],\\n\",\n+       \"          [194.3844, 457.2024, 171.4133],\\n\",\n+       \"          [193.6677, 457.3959, 167.7107],\\n\",\n+       \"          [194.6959, 460.7277, 166.2377],\\n\",\n+       \"          [193.3968, 462.9168, 163.4242],\\n\",\n+       \"          [194.8157, 465.9834, 161.7135],\\n\",\n+       \"          [193.5975, 469.3353, 163.0516],\\n\",\n+       \"          [194.0021, 472.4931, 160.9684],\\n\",\n+       \"          [193.9675, 475.9664, 162.5033],\\n\",\n+       \"          [193.4755, 479.4159, 161.0357],\\n\",\n+       \"          [196.7072, 479.1398, 159.0498],\\n\",\n+       \"          [199.2380, 481.0833, 161.1038],\\n\",\n+       \"          [200.4355, 477.9101, 162.8583],\\n\",\n+       \"          [201.1263, 474.4388, 161.5022],\\n\",\n+       \"          [198.7341, 471.4980, 161.7173],\\n\",\n+       \"          [198.3870, 469.4143, 164.8834],\\n\",\n+       \"          [196.9634, 466.0464, 165.9660],\\n\",\n+       \"          [193.8728, 465.6269, 168.1384],\\n\",\n+       \"          [192.0453, 462.6648, 169.6378],\\n\",\n+       \"          [189.6005, 461.3958, 167.0345],\\n\",\n+       \"          [188.5610, 458.5793, 164.7638],\\n\",\n+       \"          [185.9219, 457.6053, 162.2491],\\n\",\n+       \"          [186.6821, 458.7163, 158.6997],\\n\",\n+       \"          [189.3552, 461.1924, 159.8627],\\n\",\n+       \"          [191.6279, 458.6501, 161.5795],\\n\",\n+       \"          [195.2363, 459.3693, 160.6062],\\n\",\n+       \"          [197.1180, 457.1342, 163.0350],\\n\",\n+       \"          [197.6513, 456.0197, 166.6202],\\n\",\n+       \"          [199.5711, 457.9891, 169.2464],\\n\",\n+       \"          [201.9258, 455.8545, 171.3396],\\n\",\n+       \"          [204.0223, 458.4030, 173.2702],\\n\",\n+       \"          [204.0654, 462.0508, 174.3389],\\n\",\n+       \"          [206.9754, 464.2809, 175.3246],\\n\",\n+       \"          [205.3117, 466.7347, 177.6991],\\n\",\n+       \"          [208.0995, 469.3304, 177.8176],\\n\",\n+       \"          [207.9500, 469.9574, 174.0591],\\n\",\n+       \"          [204.2863, 468.9828, 173.6828],\\n\",\n+       \"          [205.2161, 466.4121, 171.0357],\\n\",\n+       \"          [203.0047, 463.4515, 170.1065],\\n\",\n+       \"          [204.5949, 460.3627, 168.5532],\\n\",\n+       \"          [202.3941, 458.9509, 165.7863],\\n\",\n+       \"          [202.4135, 455.6142, 163.9592],\\n\",\n+       \"          [200.5412, 455.9816, 160.6380],\\n\",\n+       \"          [197.4668, 453.8614, 159.9738],\\n\",\n+       \"          [199.3793, 452.3290, 157.0461],\\n\",\n+       \"          [201.8117, 450.7604, 159.5502],\\n\",\n+       \"          [200.9911, 447.7578, 161.7207],\\n\",\n+       \"          [200.6235, 448.4547, 165.4404],\\n\",\n+       \"          [200.1784, 446.1145, 168.4132],\\n\",\n+       \"          [199.8529, 447.5739, 171.8820],\\n\",\n+       \"          [197.7348, 448.8400, 174.7466],\\n\",\n+       \"          [195.1268, 451.4528, 173.8197],\\n\",\n+       \"          [192.3477, 453.2565, 175.6605],\\n\",\n+       \"          [188.9569, 451.5234, 175.5765],\\n\",\n+       \"          [185.5207, 453.1102, 175.8322],\\n\",\n+       \"          [183.9646, 452.9510, 179.3080],\\n\",\n+       \"          [180.7992, 451.2673, 178.0513],\\n\",\n+       \"          [180.4683, 447.5265, 177.5932],\\n\",\n+       \"          [184.0987, 446.9803, 178.5930],\\n\",\n+       \"          [183.1029, 443.7189, 180.2918],\\n\",\n+       \"          [181.8425, 442.4591, 176.9255],\\n\",\n+       \"          [185.2580, 443.0276, 175.3308],\\n\",\n+       \"          [186.8817, 439.6114, 174.8690],\\n\",\n+       \"          [189.8907, 438.4270, 172.8485],\\n\",\n+       \"          [189.0413, 438.0299, 169.1817],\\n\",\n+       \"          [186.5213, 440.8977, 169.1051],\\n\",\n+       \"          [186.4875, 443.5345, 166.3745],\\n\",\n+       \"          [187.2138, 447.0861, 167.5288],\\n\",\n+       \"          [186.7567, 450.5166, 165.9644],\\n\",\n+       \"          [187.8187, 454.0125, 166.9865],\\n\",\n+       \"          [185.1741, 456.5018, 168.1070],\\n\",\n+       \"          [185.0113, 460.3016, 167.8790],\\n\",\n+       \"          [187.2229, 460.5884, 170.9843],\\n\",\n+       \"          [189.9023, 458.1826, 169.7563],\\n\",\n+       \"          [188.6959, 455.3909, 172.0528],\\n\",\n+       \"          [188.3371, 451.7683, 170.9875],\\n\",\n+       \"          [184.8400, 450.2790, 171.0850],\\n\",\n+       \"          [183.3118, 446.9541, 170.0985],\\n\",\n+       \"          [181.7845, 446.9279, 166.6165],\\n\",\n+       \"          [183.4827, 449.0777, 160.0576],\\n\",\n+       \"          [186.3409, 451.0179, 158.4578],\\n\",\n+       \"          [189.4116, 450.1909, 160.6042],\\n\",\n+       \"          [190.7996, 446.6886, 161.1856],\\n\",\n+       \"          [191.3849, 446.4283, 164.9391],\\n\",\n+       \"          [191.1396, 443.1729, 166.9038],\\n\",\n+       \"          [191.2964, 442.9523, 170.6910],\\n\",\n+       \"          [193.9518, 440.5240, 171.9330],\\n\",\n+       \"          [192.9439, 440.5845, 175.6207],\\n\",\n+       \"          [190.2838, 441.8338, 178.0451],\\n\",\n+       \"          [189.7749, 445.3017, 179.4824],\\n\",\n+       \"          [192.2175, 446.2191, 182.2554],\\n\",\n+       \"          [190.9081, 448.9066, 184.6113],\\n\",\n+       \"          [193.6014, 451.3525, 185.7371],\\n\",\n+       \"          [191.2417, 453.9911, 187.1604],\\n\",\n+       \"          [187.5045, 454.2755, 187.8168],\\n\",\n+       \"          [187.1856, 455.8765, 184.3634],\\n\",\n+       \"          [190.3867, 454.6044, 182.6770],\\n\",\n+       \"          [190.3628, 451.2543, 180.8545],\\n\",\n+       \"          [193.0865, 449.8934, 178.5754],\\n\",\n+       \"          [193.0575, 446.9304, 176.2036],\\n\",\n+       \"          [195.6519, 445.1075, 174.1079],\\n\",\n+       \"          [194.7845, 445.4580, 170.4237],\\n\",\n+       \"          [196.3712, 445.1063, 166.9902],\\n\",\n+       \"          [195.6411, 447.6097, 164.2096],\\n\",\n+       \"          [196.5773, 446.4442, 160.7041],\\n\",\n+       \"          [195.1365, 449.0989, 158.4050],\\n\",\n+       \"          [191.7716, 450.0980, 157.0157],\\n\",\n+       \"          [189.1553, 447.6691, 155.7140],\\n\",\n+       \"          [186.8651, 447.7225, 152.6899]]),\\n\",\n        \"  'residue_ids': tensor([  4.,   4.,   4.,  ..., 182., 182., 182.]),\\n\",\n        \"  'chain_ids': tensor([0., 0., 0.,  ..., 1., 1., 1.])},\\n\",\n        \" 'id': '8phr__X4_UNDEFINED--8phr__W4_UNDEFINED',\\n\",\n@@ -1172,7 +1830,7 @@\n        \" 'target_id': '8phr__X4_UNDEFINED-R--8phr__W4_UNDEFINED-L'}\"\n       ]\n      },\n-     \"execution_count\": 4,\n+     \"execution_count\": 20,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\n@@ -1184,7 +1842,7 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 5,\n+   \"execution_count\": 21,\n    \"id\": \"edd68e94-0782-47a5-a9cb-d6f8d1db00f1\",\n    \"metadata\": {},\n    \"outputs\": [\n@@ -1192,10 +1850,8 @@\n      \"name\": \"stderr\",\n      \"output_type\": \"stream\",\n      \"text\": [\n-      \"2024-09-23 15:43:35,437 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n-      \"2024-09-23 15:43:35,771 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.33s\\n\",\n-      \"2024-09-23 15:43:35,997 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=7, items=7\\n\",\n-      \"2024-09-23 15:43:36,256 | pinder.core.utils.cloud.process_many:23 | INFO : runtime succeeded:      0.26s\\n\"\n+      \"2024-09-25 16:33:12,882 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=5, items=5\\n\",\n+      \"2024-09-25 16:33:13,349 | pinder.core.utils.cloud:375 | INFO : Gsutil process_many=download_to_filename, threads=7, items=7\\n\"\n      ]\n     }\n    ],\n@@ -1232,150 +1888,154 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 6,\n+   \"execution_count\": 22,\n    \"id\": \"608249db-4ba3-4391-b8da-79a8550ba974\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"{'target_complex': {'atom_types': tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n+       \"{'target_complex': {'atom_types': tensor([[[ 0.],\\n\",\n+       \"           [ 1.],\\n\",\n+       \"           [ 2.],\\n\",\n        \"           ...,\\n\",\n-       \"           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.]],\\n\",\n+       \"           [-1.],\\n\",\n+       \"           [-1.],\\n\",\n+       \"           [-1.]],\\n\",\n        \"  \\n\",\n-       \"          [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n+       \"          [[ 0.],\\n\",\n+       \"           [ 1.],\\n\",\n+       \"           [ 2.],\\n\",\n        \"           ...,\\n\",\n-       \"           [-1., -1., -1.,  ..., -1., -1., -1.],\\n\",\n-       \"           [-1., -1., -1.,  ..., -1., -1., -1.],\\n\",\n-       \"           [-1., -1., -1.,  ..., -1., -1., -1.]]]),\\n\",\n-       \"  'residue_types': tensor([[[15.],\\n\",\n-       \"           [15.],\\n\",\n-       \"           [15.],\\n\",\n-       \"           ...,\\n\",\n-       \"           [15.],\\n\",\n-       \"           [15.],\\n\",\n-       \"           [15.]],\\n\",\n-       \"  \\n\",\n-       \"          [[12.],\\n\",\n-       \"           [12.],\\n\",\n-       \"           [12.],\\n\",\n+       \"           [ 1.],\\n\",\n+       \"           [ 2.],\\n\",\n+       \"           [ 3.]]]),\\n\",\n+       \"  'residue_types': tensor([[[ 2.],\\n\",\n+       \"           [ 2.],\\n\",\n+       \"           [ 2.],\\n\",\n        \"           ...,\\n\",\n        \"           [-1.],\\n\",\n        \"           [-1.],\\n\",\n-       \"           [-1.]]]),\\n\",\n-       \"  'atom_coordinates': tensor([[[ 194.5170,  179.2660,  -91.8800],\\n\",\n-       \"           [ 194.9840,  179.8980,  -90.6150],\\n\",\n-       \"           [ 193.8100,  180.4120,  -89.7870],\\n\",\n-       \"           ...,\\n\",\n-       \"           [ 167.9860,  256.5550,  -26.2400],\\n\",\n-       \"           [ 168.9990,  256.8070,  -25.2210],\\n\",\n-       \"           [ 170.4040,  256.5630,  -25.7840]],\\n\",\n+       \"           [-1.]],\\n\",\n        \"  \\n\",\n-       \"          [[ -90.1810,  -82.1340, -226.8990],\\n\",\n-       \"           [ -90.2500,  -81.9540, -225.4530],\\n\",\n-       \"           [ -90.5700,  -80.5080, -225.0850],\\n\",\n+       \"          [[13.],\\n\",\n+       \"           [13.],\\n\",\n+       \"           [13.],\\n\",\n+       \"           ...,\\n\",\n+       \"           [13.],\\n\",\n+       \"           [13.],\\n\",\n+       \"           [13.]]]),\\n\",\n+       \"  'atom_coordinates': tensor([[[ 289.1240,  316.4350,  305.0190],\\n\",\n+       \"           [ 290.4980,  316.8960,  304.8630],\\n\",\n+       \"           [ 291.3100,  316.6540,  306.1310],\\n\",\n        \"           ...,\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n-       \"           [-100.0000, -100.0000, -100.0000]]]),\\n\",\n-       \"  'residue_coordinates': tensor([[[ 194.5170,  179.2660,  -91.8800],\\n\",\n-       \"           [ 194.9840,  179.8980,  -90.6150],\\n\",\n-       \"           [ 193.8100,  180.4120,  -89.7870],\\n\",\n-       \"           ...,\\n\",\n-       \"           [ 167.9860,  256.5550,  -26.2400],\\n\",\n-       \"           [ 168.9990,  256.8070,  -25.2210],\\n\",\n-       \"           [ 170.4040,  256.5630,  -25.7840]],\\n\",\n+       \"           [-100.0000, -100.0000, -100.0000]],\\n\",\n        \"  \\n\",\n-       \"          [[ -90.1810,  -82.1340, -226.8990],\\n\",\n-       \"           [ -90.2500,  -81.9540, -225.4530],\\n\",\n-       \"           [ -90.5700,  -80.5080, -225.0850],\\n\",\n+       \"          [[ 169.2570,  175.1550,  105.7850],\\n\",\n+       \"           [ 170.4880,  175.1580,  106.5660],\\n\",\n+       \"           [ 170.5260,  176.3570,  107.5090],\\n\",\n+       \"           ...,\\n\",\n+       \"           [ 158.9320,  185.6340,  166.0330],\\n\",\n+       \"           [ 158.8780,  185.2340,  164.5630],\\n\",\n+       \"           [ 158.0010,  184.4790,  164.1430]]]),\\n\",\n+       \"  'residue_coordinates': tensor([[[ 290.4980,  316.8960,  304.8630],\\n\",\n+       \"           [ 291.8970,  317.5740,  308.3580],\\n\",\n+       \"           [ 295.3220,  319.0930,  308.9830],\\n\",\n        \"           ...,\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n-       \"           [-100.0000, -100.0000, -100.0000]]]),\\n\",\n-       \"  'residue_ids': tensor([[  1.,   1.,   1.,  ..., 280., 280., 280.],\\n\",\n-       \"          [  1.,   1.,   1.,  ..., -99., -99., -99.]]),\\n\",\n-       \"  'chain_ids': tensor([[ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\\n\",\n-       \"          [ 0.,  0.,  0.,  ..., -1., -1., -1.]])},\\n\",\n-       \" 'feature_complex': {'atom_types': tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           ...,\\n\",\n-       \"           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.]],\\n\",\n+       \"           [-100.0000, -100.0000, -100.0000]],\\n\",\n        \"  \\n\",\n-       \"          [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n-       \"           [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\\n\",\n+       \"          [[ 170.4880,  175.1580,  106.5660],\\n\",\n+       \"           [ 171.8100,  177.5960,  109.1870],\\n\",\n+       \"           [ 174.0830,  180.6050,  108.7410],\\n\",\n        \"           ...,\\n\",\n-       \"           [-1., -1., -1.,  ..., -1., -1., -1.],\\n\",\n-       \"           [-1., -1., -1.,  ..., -1., -1., -1.],\\n\",\n-       \"           [-1., -1., -1.,  ..., -1., -1., -1.]]]),\\n\",\n-       \"  'residue_types': tensor([[[15.],\\n\",\n-       \"           [15.],\\n\",\n-       \"           [15.],\\n\",\n+       \"           [ 153.4260,  188.1030,  169.2780],\\n\",\n+       \"           [ 155.3650,  185.4840,  167.3250],\\n\",\n+       \"           [ 158.9320,  185.6340,  166.0330]]]),\\n\",\n+       \"  'residue_ids': tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ..., -9.9000e+01,\\n\",\n+       \"           -9.9000e+01, -9.9000e+01],\\n\",\n+       \"          [ 2.1500e+02,  2.1500e+02,  2.1500e+02,  ...,  1.1670e+03,\\n\",\n+       \"            1.1670e+03,  1.1670e+03]]),\\n\",\n+       \"  'chain_ids': tensor([[ 0.,  0.,  0.,  ..., -1., -1., -1.],\\n\",\n+       \"          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.]])},\\n\",\n+       \" 'feature_complex': {'atom_types': tensor([[[ 0.],\\n\",\n+       \"           [ 1.],\\n\",\n+       \"           [ 2.],\\n\",\n        \"           ...,\\n\",\n-       \"           [15.],\\n\",\n-       \"           [15.],\\n\",\n-       \"           [15.]],\\n\",\n+       \"           [-1.],\\n\",\n+       \"           [-1.],\\n\",\n+       \"           [-1.]],\\n\",\n        \"  \\n\",\n-       \"          [[12.],\\n\",\n-       \"           [12.],\\n\",\n-       \"           [12.],\\n\",\n+       \"          [[ 0.],\\n\",\n+       \"           [ 1.],\\n\",\n+       \"           [ 2.],\\n\",\n+       \"           ...,\\n\",\n+       \"           [ 1.],\\n\",\n+       \"           [ 2.],\\n\",\n+       \"           [ 3.]]]),\\n\",\n+       \"  'residue_types': tensor([[[ 2.],\\n\",\n+       \"           [ 2.],\\n\",\n+       \"           [ 2.],\\n\",\n        \"           ...,\\n\",\n        \"           [-1.],\\n\",\n        \"           [-1.],\\n\",\n-       \"           [-1.]]]),\\n\",\n-       \"  'atom_coordinates': tensor([[[ 194.5170,  179.2660,  -91.8800],\\n\",\n-       \"           [ 194.9840,  179.8980,  -90.6150],\\n\",\n-       \"           [ 193.8100,  180.4120,  -89.7870],\\n\",\n-       \"           ...,\\n\",\n-       \"           [ 178.4873,  215.5139,  -38.6835],\\n\",\n-       \"           [ 178.2736,  215.6114,  -40.1233],\\n\",\n-       \"           [ 179.1470,  216.7183,  -40.7253]],\\n\",\n+       \"           [-1.]],\\n\",\n        \"  \\n\",\n-       \"          [[ -90.1810,  -82.1340, -226.8990],\\n\",\n-       \"           [ -90.2500,  -81.9540, -225.4530],\\n\",\n-       \"           [ -90.5700,  -80.5080, -225.0850],\\n\",\n+       \"          [[13.],\\n\",\n+       \"           [13.],\\n\",\n+       \"           [13.],\\n\",\n+       \"           ...,\\n\",\n+       \"           [13.],\\n\",\n+       \"           [13.],\\n\",\n+       \"           [13.]]]),\\n\",\n+       \"  'atom_coordinates': tensor([[[ 289.1240,  316.4350,  305.0190],\\n\",\n+       \"           [ 290.4980,  316.8960,  304.8630],\\n\",\n+       \"           [ 291.3100,  316.6540,  306.1310],\\n\",\n        \"           ...,\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n-       \"           [-100.0000, -100.0000, -100.0000]]]),\\n\",\n-       \"  'residue_coordinates': tensor([[[ 194.5170,  179.2660,  -91.8800],\\n\",\n-       \"           [ 194.9840,  179.8980,  -90.6150],\\n\",\n-       \"           [ 193.8100,  180.4120,  -89.7870],\\n\",\n-       \"           ...,\\n\",\n-       \"           [ 178.4873,  215.5139,  -38.6835],\\n\",\n-       \"           [ 178.2736,  215.6114,  -40.1233],\\n\",\n-       \"           [ 179.1470,  216.7183,  -40.7253]],\\n\",\n+       \"           [-100.0000, -100.0000, -100.0000]],\\n\",\n        \"  \\n\",\n-       \"          [[ -90.1810,  -82.1340, -226.8990],\\n\",\n-       \"           [ -90.2500,  -81.9540, -225.4530],\\n\",\n-       \"           [ -90.5700,  -80.5080, -225.0850],\\n\",\n+       \"          [[ 169.2570,  175.1550,  105.7850],\\n\",\n+       \"           [ 170.4880,  175.1580,  106.5660],\\n\",\n+       \"           [ 170.5260,  176.3570,  107.5090],\\n\",\n+       \"           ...,\\n\",\n+       \"           [ 109.5580,  141.3806,  149.9941],\\n\",\n+       \"           [ 110.3494,  140.1194,  150.3209],\\n\",\n+       \"           [ 111.2073,  140.1200,  151.2038]]]),\\n\",\n+       \"  'residue_coordinates': tensor([[[ 290.4980,  316.8960,  304.8630],\\n\",\n+       \"           [ 291.8970,  317.5740,  308.3580],\\n\",\n+       \"           [ 295.3220,  319.0930,  308.9830],\\n\",\n        \"           ...,\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n        \"           [-100.0000, -100.0000, -100.0000],\\n\",\n-       \"           [-100.0000, -100.0000, -100.0000]]]),\\n\",\n-       \"  'residue_ids': tensor([[  1.,   1.,   1.,  ..., 280., 280., 280.],\\n\",\n-       \"          [  1.,   1.,   1.,  ..., -99., -99., -99.]]),\\n\",\n-       \"  'chain_ids': tensor([[ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\\n\",\n-       \"          [ 0.,  0.,  0.,  ..., -1., -1., -1.]])},\\n\",\n-       \" 'id': ['2fte__A43_P49861--2fte__G44_P49861',\\n\",\n-       \"  '3jc8__JB1_Q306N5--3jc8__NB1_Q306N4'],\\n\",\n-       \" 'sample_id': ['2fte__A43_P49861-R--2fte__G44_P49861-L',\\n\",\n-       \"  '3jc8__JB1_Q306N5-R--af__Q306N4'],\\n\",\n-       \" 'target_id': ['2fte__A43_P49861-R--2fte__G44_P49861-L',\\n\",\n-       \"  '3jc8__JB1_Q306N5-R--3jc8__NB1_Q306N4-L']}\"\n+       \"           [-100.0000, -100.0000, -100.0000]],\\n\",\n+       \"  \\n\",\n+       \"          [[ 170.4880,  175.1580,  106.5660],\\n\",\n+       \"           [ 171.8100,  177.5960,  109.1870],\\n\",\n+       \"           [ 174.0830,  180.6050,  108.7410],\\n\",\n+       \"           ...,\\n\",\n+       \"           [ 106.3922,  144.9902,  154.8819],\\n\",\n+       \"           [ 109.4120,  143.4231,  153.1913],\\n\",\n+       \"           [ 109.5580,  141.3806,  149.9941]]]),\\n\",\n+       \"  'residue_ids': tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ..., -9.9000e+01,\\n\",\n+       \"           -9.9000e+01, -9.9000e+01],\\n\",\n+       \"          [ 2.1500e+02,  2.1500e+02,  2.1500e+02,  ...,  1.1670e+03,\\n\",\n+       \"            1.1670e+03,  1.1670e+03]]),\\n\",\n+       \"  'chain_ids': tensor([[ 0.,  0.,  0.,  ..., -1., -1., -1.],\\n\",\n+       \"          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.]])},\\n\",\n+       \" 'id': ['7wl3__A23_A0A866W289--7wl3__A24_A0A866W289',\\n\",\n+       \"  '8ae1__A1_Q9RRB6--8ae1__B1_Q9RRB6'],\\n\",\n+       \" 'sample_id': ['7wl3__A23_A0A866W289-R--7wl3__A24_A0A866W289-L',\\n\",\n+       \"  '8ae1__A1_Q9RRB6-R--8ae1__B1_Q9RRB6-L'],\\n\",\n+       \" 'target_id': ['7wl3__A23_A0A866W289-R--7wl3__A24_A0A866W289-L',\\n\",\n+       \"  '8ae1__A1_Q9RRB6-R--8ae1__B1_Q9RRB6-L']}\"\n       ]\n      },\n-     \"execution_count\": 6,\n+     \"execution_count\": 22,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }\ndiff --git a/src/pinder-core/pinder/core/loader/dataset.py b/src/pinder-core/pinder/core/loader/dataset.py\nindex f817af3..6a0c89e 100644\n--- a/src/pinder-core/pinder/core/loader/dataset.py\n+++ b/src/pinder-core/pinder/core/loader/dataset.py\n@@ -52,10 +52,12 @@\n \n \n def structure2tensor_transform(structure: Structure) -> dict[str, torch.Tensor]:\n+    calpha = structure.filter(\"atom_name\", mask=[\"CA\"])\n     props: dict[str, torch.Tensor] = structure2tensor(\n         atom_coordinates=structure.coords,\n-        atom_types=structure.atom_array.element,\n-        residue_coordinates=structure.coords,\n+        atom_types=structure.atom_array.atom_name,\n+        element_types=structure.atom_array.element,\n+        residue_coordinates=calpha.coords,\n         residue_types=structure.atom_array.res_name,\n         residue_ids=structure.atom_array.res_id,\n         chain_ids=structure.atom_array.chain_id,\ndiff --git a/src/pinder-core/pinder/core/loader/geodata.py b/src/pinder-core/pinder/core/loader/geodata.py\nindex 5d95aba..8e75fae 100644\n--- a/src/pinder-core/pinder/core/loader/geodata.py\n+++ b/src/pinder-core/pinder/core/loader/geodata.py\n@@ -30,6 +30,7 @@\n def structure2tensor(\n     atom_coordinates: NDArray[np.double] | None = None,\n     atom_types: NDArray[np.str_] | None = None,\n+    element_types: NDArray[np.str_] | None = None,\n     residue_coordinates: NDArray[np.double] | None = None,\n     residue_ids: NDArray[np.int_] | None = None,\n     residue_types: NDArray[np.str_] | None = None,\n@@ -38,14 +39,16 @@ def structure2tensor(\n ) -> dict[str, torch.Tensor]:\n     property_dict = {}\n     if atom_types is not None:\n-        types_array_ele = np.zeros(\n-            (len(atom_types), len(set(list(pc.ELE2NUM.values()))))\n-        )\n+        unknown_name_idx = max(pc.ALL_ATOM_POSNS.values()) + 1\n+        types_array_at = np.zeros((len(atom_types), 1))\n         for i, name in enumerate(atom_types):\n-            types_array_ele[i, pc.ELE2NUM.get(name, \"C\")] = 1.0\n-\n-        property_dict[\"atom_types\"] = torch.tensor(types_array_ele).type(dtype)\n-\n+            types_array_at[i] = pc.ALL_ATOM_POSNS.get(name, unknown_name_idx)\n+        property_dict[\"atom_types\"] = torch.tensor(types_array_at).type(dtype)\n+    if element_types is not None:\n+        types_array_ele = np.zeros((len(element_types), 1))\n+        for i, name in enumerate(element_types):\n+            types_array_ele[i] = pc.ELE2NUM.get(name, pc.ELE2NUM[\"other\"])\n+        property_dict[\"element_types\"] = torch.tensor(types_array_ele).type(dtype)\n     if residue_types is not None:\n         unknown_name_idx = max(pc.AA_TO_INDEX.values()) + 1\n         types_array_res = np.zeros((len(residue_types), 1))\n@@ -119,14 +122,16 @@ def from_structure_pair(\n         lig_calpha = ligand.filter(\"atom_name\", mask=[\"CA\"])\n         rec_props = structure2tensor(\n             atom_coordinates=receptor.coords,\n-            atom_types=receptor.atom_array.element,\n+            atom_types=receptor.atom_array.atom_name,\n+            element_types=receptor.atom_array.element,\n             residue_coordinates=rec_calpha.coords,\n             residue_types=rec_calpha.atom_array.res_name,\n             residue_ids=rec_calpha.atom_array.res_id,\n         )\n         lig_props = structure2tensor(\n             atom_coordinates=ligand.coords,\n-            atom_types=ligand.atom_array.element,\n+            atom_types=ligand.atom_array.atom_name,\n+            element_types=ligand.atom_array.element,\n             residue_coordinates=lig_calpha.coords,\n             residue_types=lig_calpha.atom_array.res_name,\n             residue_ids=lig_calpha.atom_array.res_id,\n", "instance_id": "pinder-org__pinder-21", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in expressing the user's confusion about the features returned by `get_torch_loader` in the `pinder` library, specifically regarding the differences between `atom_coordinates` and `residue_coordinates`, and the lack of atom name information in the `atom_types` tensor. The goal of modifying the loader to include atom name information and align more closely with the `Structure` class is evident. The provided code snippets and output examples help illustrate the issue. However, there are minor ambiguities: the problem does not explicitly define the expected format or structure of the output for atom name information, nor does it specify how closely the dataloader should correspond to the `Structure` class. Additionally, edge cases or constraints (e.g., performance implications or handling of missing data) are not mentioned, which could impact the solution design.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, it requires understanding the internal structure of the `pinder` library, specifically the `get_torch_loader` function and the `structure2tensor_transform` logic, as seen in the code changes. The modifications involve altering how data is represented (e.g., separating `atom_types` into atom name and element type information, and adjusting `residue_coordinates` to use C-alpha atoms only), which necessitates changes in the `structure2tensor` function across a couple of files. This involves moderate complexity in handling tensor transformations and ensuring compatibility with downstream processes. Additionally, the problem touches on domain-specific knowledge of protein structure data (e.g., understanding atom names, residue coordinates, and one-hot encodings), which adds to the conceptual load. However, the scope of changes is relatively contained, primarily affecting data representation rather than core system architecture, and does not appear to introduce significant edge case handling or performance challenges beyond ensuring data consistency. Thus, a score of 0.45 reflects the need for understanding multiple concepts and making targeted, moderately complex modifications without requiring deep architectural refactoring or advanced technical feats.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug] test_rgb2lch fails with AssertionError on ARM\nThe following test fails on the latest stable version (0.1.3) **only on ARM platforms** (`aarch64-linux` and `aarch64-darwin`):\r\n```\r\n=================================== FAILURES ===================================\r\n_____________________________ test_rgb2lch[pair0] ______________________________\r\n\r\npair = ((0, 0, 0), (0, 0, 0))\r\n\r\n    @pytest.mark.parametrize(\"pair\", tests)\r\n    def test_rgb2lch(pair):\r\n        rgb, lch = pair\r\n        alch = convert(*rgb, src=cs.rgb, dst=cs.lch)\r\n>       assert alch[0] >= 0\r\nE       assert -2.220446049250313e-16 >= 0\r\n\r\ntests/test_colorspace.py:101: AssertionError\r\n=========================== short test summary info ============================\r\nFAILED tests/test_colorspace.py::test_rgb2lch[pair0] - assert -2.220446049250313e-16 >= 0\r\n======================== 1 failed, 1443 passed in 0.74s ========================\r\n```\r\n\n", "patch": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex ac0f7f6..86e74cc 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -81,7 +81,7 @@ jobs:\n           CIBW_ARCHS_MACOS: x86_64 arm64 universal2\n           CIBW_ARCHS_LINUX: auto aarch64\n           # CIBW_TEST_REQUIRES: pytest colormath==2.0.2\n-          # CIBW_TEST_COMMAND: python -m pytest {project}/tests -v -s\n+          # CIBW_TEST_COMMAND: python -m pytest {project}/tests\n \n       - uses: actions/upload-artifact@v3\n         with:\n@@ -120,7 +120,7 @@ jobs:\n           name: artifact\n           path: dist\n \n-      - uses: pypa/gh-action-pypi-publish@master\n+      - uses: pypa/gh-action-pypi-publish@release/v1\n         with:\n           user: ${{ secrets.PYPI_USERNAME }}\n           password: ${{ secrets.PYPI_PASSWORD }}\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 5bc6d99..b4fb583 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -4,27 +4,22 @@ repos:\n     hooks:\n       - id: validate-pyproject\n \n-  - repo: https://github.com/psf/black\n-    rev: 22.12.0\n-    hooks:\n-      - id: black\n-        language_version: python\n-\n   - repo: https://github.com/PyCQA/isort\n-    rev: 5.12.0\n+    rev: 5.13.2\n     hooks:\n       - id: isort\n         language_version: python\n \n-  - repo: https://github.com/charliermarsh/ruff-pre-commit\n-    rev: v0.0.238\n+  - repo: https://github.com/astral-sh/ruff-pre-commit\n+    rev: v0.3.5\n     hooks:\n       - id: ruff\n         args: [\"--fix\"]\n         exclude: tests/.*\n+      - id: ruff-format\n \n   - repo: https://github.com/pre-commit/mirrors-mypy\n-    rev: v0.991\n+    rev: v1.9.0\n     hooks:\n       - id: mypy\n         language_version: python\ndiff --git a/CHANGES.md b/CHANGES.md\nindex 5db9151..99416ad 100644\n--- a/CHANGES.md\n+++ b/CHANGES.md\n@@ -1,4 +1,9 @@\n \n+## 0.1.4 (2024-06-18)\n+\n+* limit numpy to **~1.0**\n+* relaxes tests scenario, failing on some ARM arch\n+\n ## 0.1.3 (2024-03-13)\n \n * no changes since `0.1.2`\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nnew file mode 100644\nindex 0000000..4556d12\n--- /dev/null\n+++ b/CONTRIBUTING.md\n@@ -0,0 +1,25 @@\n+# Contributing\n+\n+Issues and pull requests are more than welcome.\n+\n+### dev install\n+\n+```bash\n+git clone https://github.com/vincentsarago/color-operations.git\n+cd color-operations\n+python -m pip install -e \".[test,dev]\"\n+```\n+\n+You can then run the tests with the following command:\n+\n+```sh\n+python -m pytest --cov color_operations --cov-report term-missing\n+```\n+\n+### pre-commit\n+\n+This repo is set to use `pre-commit` to run *isort*, *flake8*, *pydocstring*, *black* (\"uncompromising Python code formatter\") and mypy when committing new code.\n+\n+```bash\n+$ pre-commit install\n+```\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 2bbf8d3..e7552ca 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -31,8 +31,11 @@ default_section = \"THIRDPARTY\"\n no_strict_optional = \"True\"\n \n [tool.ruff]\n+line-length = 90\n+\n+[tool.ruff.lint]\n select = [\n-    \"D1\",  # pydocstyle errors\n+    \"D1\", # pydocstyle errors\n     \"E\",  # pycodestyle errors\n     \"W\",  # pycodestyle warnings\n     \"F\",  # flake8\n@@ -44,3 +47,6 @@ ignore = [\n     \"B008\",  # do not perform function calls in argument defaults\n     \"B905\",  # ignore zip() without an explicit strict= parameter, only support with python >3.10\n ]\n+\n+[tool.ruff.lint.mccabe]\n+max-complexity = 14\ndiff --git a/setup.py b/setup.py\nindex 74d0aa0..047268c 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -91,7 +91,7 @@\n     packages=find_packages(exclude=[\"tests\"]),\n     include_package_data=True,\n     zip_safe=False,\n-    install_requires=[\"numpy\"],\n+    install_requires=[\"numpy~=1.0\"],\n     ext_modules=ext_modules,\n     include_dirs=include_dirs,\n     extras_require={\n@@ -100,5 +100,8 @@\n             \"colormath==2.0.2\",\n             \"pytest-cov\",\n         ],\n+        \"dev\": [\n+            \"pre-commit\",\n+        ],\n     },\n )\n", "instance_id": "vincentsarago__color-operations-10", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a specific test (`test_rgb2lch`) fails on ARM platforms with an AssertionError due to a very small negative value being compared against zero. The goal is implicitly to fix this failing test, and the input/output expectations are indirectly provided through the test case shown in the error message. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly state whether the fix should address the underlying conversion logic, adjust the test assertion to account for floating-point precision, or apply a platform-specific workaround. Additionally, edge cases or constraints related to floating-point precision or ARM-specific behavior are not mentioned. Despite these minor gaps, the problem is valid and provides enough context to understand the issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to several factors. First, the scope of code changes appears limited based on the provided diffs, which primarily update configuration files, dependencies, and CI settings rather than directly addressing the test failure with code modifications in the core logic. However, the actual fix for the test failure is not shown in the provided changes, suggesting that the solution might involve a small modification to either the test assertion (e.g., adding a tolerance for floating-point comparisons) or the color conversion logic in the `convert` function. This would require understanding basic floating-point precision issues and possibly platform-specific behavior on ARM, which are relatively straightforward concepts for an experienced developer. The number of technical concepts involved is minimal, likely limited to floating-point arithmetic and pytest assertions. The codebase impact seems low, as the fix would likely be localized to a single test or function. Edge cases related to floating-point precision are implied but not complex to handle. However, the lack of direct code changes addressing the bug in the provided diff slightly increases the difficulty due to the need to infer the appropriate fix. Additionally, understanding ARM-specific behavior might require minor research, though it is not a significant barrier. Overall, I assign a difficulty score of 0.30, reflecting an easy problem with minor investigative effort required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Convers\u00e3o de C\u00f3digo do IBGE de Estado para UF\n**Seu pedido de recurso est\u00e1 relacionado a um problema? Por favor, descreva.**\r\n\r\nDado um c\u00f3digo do IBGE de dois d\u00edgitos que representa um estado brasileiro, quero obter a sigla de Unidade Federativa (UF) correspondente. Isso \u00e9 \u00fatil para convers\u00e3o de c\u00f3digos num\u00e9ricos do IBGE em siglas de estados para sistemas e documentos.\r\n\r\nPor exemplo, converter `\"12\"` para `\"AM\"` (Amazonas).\r\n\r\n**Descreva a solu\u00e7\u00e3o que voc\u00ea gostaria**\r\n\r\n* Uma fun\u00e7\u00e3o `convert_code_to_uf`, que recebe um c\u00f3digo do IBGE (string de 2 d\u00edgitos) e retorna a sigla de UF correspondente.\r\n* A fun\u00e7\u00e3o deve verificar se o c\u00f3digo do IBGE \u00e9 v\u00e1lido e retornar a sigla da UF correspondente.\r\n* Se o c\u00f3digo do IBGE n\u00e3o for v\u00e1lido, a fun\u00e7\u00e3o deve retornar `None`.\r\n* A fun\u00e7\u00e3o deve lidar com todos os estados e o Distrito Federal do Brasil.\r\n* Os c\u00f3digos do IBGE devem ser representados como strings de dois d\u00edgitos.\r\n* J\u00e1 existe um enum dos c\u00f3digos do IBGE e suas correspond\u00eancias com UF no arquivo `brutils/data/enums/ibge_uf.py`, por\u00e9m o enum n\u00e3o mapeia para o c\u00f3digo. Criar um novo Enum no mesmo arquivo que mapeie c\u00f3digo e UF.\r\n\r\n**Descreva alternativas que voc\u00ea considerou**\r\n\r\n1. Seguir at\u00e9 o passo 8 do [guia de contribui\u00e7\u00e3o](https://github.com/brazilian-utils/brutils-python/blob/main/CONTRIBUTING.md#primeira-contribui%C3%A7%C3%A3o).\r\n\r\n2. Como parte do passo 8, criar o arquivo: `brutils-python/brutils/ibge/uf.py`.\r\n\r\n    ```python\r\n    def convert_code_to_uf(code): # type: (str) -> str | None\r\n        \"\"\"\r\n        Converts a given IBGE code (2-digit string) to its corresponding UF (state abbreviation).\r\n\r\n        This function takes a 2-digit IBGE code and returns the corresponding UF code. \r\n        It handles all Brazilian states and the Federal District. \r\n\r\n        Args:\r\n            code (str): The 2-digit IBGE code to be converted.\r\n\r\n        Returns:\r\n            str or None: The UF code corresponding to the IBGE code, \r\n                or None if the IBGE code is invalid.\r\n\r\n        Example:\r\n            >>> convert_code_to_uf('12')\r\n            \"AM\"\r\n            >>> convert_code_to_uf('33')\r\n            \"RJ\"\r\n            >>> convert_code_to_uf('99')\r\n            None\r\n        \"\"\"\r\n        # implementar a l\u00f3gica da fun\u00e7\u00e3o aqui\r\n    ```\r\n\r\n    Importar a nova fun\u00e7\u00e3o no arquivo `brutils-python/brutils/__init__.py`:\r\n\r\n    ```python\r\n    # IBGE Imports    \r\n    from brutils.ibge.uf import (\r\n        convert_code_to_uf,\r\n    )\r\n    ```\r\n\r\n    E adicionar o nome da nova fun\u00e7\u00e3o na lista `__all__` do mesmo arquivo `brutils-python/brutils/__init__.py`:\r\n\r\n    ```python\r\n    __all__ = [\r\n        ...\r\n        # IBGE\r\n        'convert_code_to_uf',\r\n    ]\r\n    ```\r\n\r\n3. Como parte do passo 9, criar o arquivo de teste: `brutils-python/tests/ibge/test_uf.py`.\r\n\r\n    ```python\r\n    from unittest import TestCase\r\n    from brutils.ibge.uf import convert_code_to_uf\r\n\r\n    class TestUF(TestCase):\r\n        def test_convert_code_to_uf(self):\r\n            # Testes para c\u00f3digos v\u00e1lidos\r\n            self.assertEqual(convert_code_to_uf('12'), \"AM\")  # Amazonas\r\n            self.assertEqual(convert_code_to_uf('33'), \"RJ\")  # Rio de Janeiro\r\n            self.assertEqual(convert_code_to_uf('31'), \"MG\")  # Minas Gerais\r\n            self.assertEqual(convert_code_to_uf('52'), \"GO\")  # Goi\u00e1s\r\n\r\n            # Testes para c\u00f3digos inv\u00e1lidos\r\n            self.assertIsNone(convert_code_to_uf('99'))  # C\u00f3digo n\u00e3o existe\r\n            self.assertIsNone(convert_code_to_uf('00'))  # C\u00f3digo n\u00e3o existe\r\n            self.assertIsNone(convert_code_to_uf(''))   # C\u00f3digo vazio\r\n            self.assertIsNone(convert_code_to_uf('AB'))  # C\u00f3digo n\u00e3o num\u00e9rico\r\n\r\n            # implementar mais casos de teste aqui se necess\u00e1rio\r\n    ```\r\n\r\n4. Seguir os passos seguintes do [guia de contribui\u00e7\u00e3o](https://github.com/brazilian-utils/brutils-python/blob/main/CONTRIBUTING.md#primeira-contribui%C3%A7%C3%A3o).\r\n\r\n**Contexto adicional**\r\n\r\n* A lista de c\u00f3digos do IBGE e suas correspond\u00eancias com as UFs \u00e9 definida pelo Instituto Brasileiro de Geografia e Estat\u00edstica (IBGE). Para mais detalhes, consulte o [site do IBGE](https://www.ibge.gov.br/).\r\n* A lista de c\u00f3digos e suas correspond\u00eancias pode ser encontrada em: [https://www.ibge.gov.br/explicacoes-para-publico/explicacoes-para-publico/33180-tabela-de-codigos-de-unidade-federativa-uf.html](https://www.ibge.gov.br/explicacoes-para-publico/explicacoes-para-publico/33180-tabela-de-codigos-de-unidade-federativa-uf.html).\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 3efcab1..b1546ed 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -6,6 +6,9 @@ The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n ## [Unreleased]\n+### Added\n+\n+- Utilit\u00e1rio `convert_code_to_uf` [#397](https://github.com/brazilian-utils/brutils-python/pull/410)\n \n ## [2.2.0] - 2024-09-12\n \ndiff --git a/README.md b/README.md\nindex ae47d91..73769b0 100644\n--- a/README.md\n+++ b/README.md\n@@ -87,6 +87,8 @@ False\n   - [is_valid_voter_id](#is_valid_voter_id)\n   - [format_voter_id](#format_voter_id)\n   - [generate_voter_id](#generate_voter_id)\n+- [IBGE](#ibge)\n+  - [convert_code_to_uf](#convert_code_to_uf)\n \n ## CPF\n \n@@ -1084,6 +1086,29 @@ Exemplo:\n '950125640248'\n ```\n \n+## IBGE\n+### convert_code_to_uf\n+Converte um determinado c\u00f3digo do IBGE (string de 2 d\u00edgitos) para sua UF (abreviatura estadual) correspondente.\n+\n+Args:\n+  * code (str): O c\u00f3digo IBGE de 2 d\u00edgitos a ser convertido.\n+\n+Retorna:\n+  * str or None: O c\u00f3digo UF correspondente ao c\u00f3digo IBGE, ou None se o\n+                 c\u00f3digo IBGE for inv\u00e1lido.\n+\n+Exemplo:\n+\n+```python\n+>>> from brutils.ibge.uf import convert_code_to_uf\n+>>> convert_code_to_uf(\"12\")\n+'AC'\n+>>> convert_code_to_uf(\"33\")\n+'RJ'\n+>>> convert_code_to_uf(\"99\")\n+>>>\n+```\n+\n \n # Novos Utilit\u00e1rios e Reportar Bugs\n \ndiff --git a/README_EN.md b/README_EN.md\nindex 921e65a..7917b67 100644\n--- a/README_EN.md\n+++ b/README_EN.md\n@@ -87,6 +87,8 @@ False\n   - [is_valid_voter_id](#is_valid_voter_id)\n   - [format_voter_id](#format_voter_id)\n   - [generate_voter_id](#generate_voter_id)\n+- [IBGE](#ibge)\n+  - [convert_code_to_uf](#convert_code_to_uf)\n \n ## CPF\n \n@@ -1087,6 +1089,29 @@ Example:\n '950125640248'\n ```\n \n+## IBGE\n+### convert_code_to_uf\n+Converts a given IBGE code (2-digit string) to its corresponding UF (state abbreviation).\n+\n+Args:\n+  * code (str): The 2-digit IBGE code to be converted.\n+\n+Retorna:\n+  * str or None: The UF code corresponding to the IBGE code, or None if the\n+                 IBGE code is invalid.\n+\n+Exemplo:\n+\n+```python\n+>>> from brutils.ibge.uf import convert_code_to_uf\n+>>> convert_code_to_uf(\"12\")\n+'AC'\n+>>> convert_code_to_uf(\"33\")\n+'RJ'\n+>>> convert_code_to_uf(\"99\")\n+>>>\n+```\n+\n # Feature Request and Bug Report\n \n If you want to suggest new features or report bugs, simply create\ndiff --git a/brutils/__init__.py b/brutils/__init__.py\nindex cbfa194..6959a0d 100644\n--- a/brutils/__init__.py\n+++ b/brutils/__init__.py\n@@ -45,6 +45,11 @@\n # Email Import\n from brutils.email import is_valid as is_valid_email\n \n+# IBGE Imports\n+from brutils.ibge.uf import (\n+    convert_code_to_uf,\n+)\n+\n # Legal Process Imports\n from brutils.legal_process import (\n     format_legal_process,\n@@ -165,4 +170,6 @@\n     \"format_voter_id\",\n     \"generate_voter_id\",\n     \"is_valid_voter_id\",\n+    # IBGE\n+    \"convert_code_to_uf\",\n ]\ndiff --git a/brutils/data/enums/__init__.py b/brutils/data/enums/__init__.py\nindex 5a6997e..705fc82 100644\n--- a/brutils/data/enums/__init__.py\n+++ b/brutils/data/enums/__init__.py\n@@ -1,1 +1,1 @@\n-from .uf import UF\n+from .uf import CODE_TO_UF, UF\ndiff --git a/brutils/data/enums/uf.py b/brutils/data/enums/uf.py\nindex aef6010..9bdb971 100644\n--- a/brutils/data/enums/uf.py\n+++ b/brutils/data/enums/uf.py\n@@ -29,3 +29,33 @@ class UF(BetterEnum):\n     SP = \"S\u00e3o Paulo\"\n     SE = \"Sergipe\"\n     TO = \"Tocantins\"\n+\n+\n+class CODE_TO_UF(BetterEnum):\n+    AC = \"12\"\n+    AL = \"27\"\n+    AP = \"16\"\n+    AM = \"13\"\n+    BA = \"29\"\n+    CE = \"23\"\n+    DF = \"53\"\n+    ES = \"32\"\n+    GO = \"52\"\n+    MA = \"21\"\n+    MT = \"51\"\n+    MS = \"52\"\n+    MG = \"31\"\n+    PA = \"15\"\n+    PB = \"25\"\n+    PR = \"41\"\n+    PE = \"26\"\n+    PI = \"22\"\n+    RJ = \"33\"\n+    RN = \"24\"\n+    RS = \"43\"\n+    RO = \"11\"\n+    RR = \"14\"\n+    SC = \"42\"\n+    SP = \"35\"\n+    SE = \"28\"\n+    TO = \"17\"\ndiff --git a/brutils/ibge/__init__.py b/brutils/ibge/__init__.py\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/brutils/ibge/uf.py b/brutils/ibge/uf.py\nnew file mode 100644\nindex 0000000..30dc741\n--- /dev/null\n+++ b/brutils/ibge/uf.py\n@@ -0,0 +1,32 @@\n+from brutils.data.enums.uf import CODE_TO_UF\n+\n+\n+def convert_code_to_uf(code):  # type: (str) -> str | None\n+    \"\"\"\n+    Converts a given IBGE code (2-digit string) to its corresponding UF (state abbreviation).\n+\n+    This function takes a 2-digit IBGE code and returns the corresponding UF code.\n+    It handles all Brazilian states and the Federal District.\n+\n+    Args:\n+        code (str): The 2-digit IBGE code to be converted.\n+\n+    Returns:\n+        str or None: The UF code corresponding to the IBGE code,\n+            or None if the IBGE code is invalid.\n+\n+    Example:\n+        >>> convert_code_to_uf('12')\n+        'AC'\n+        >>> convert_code_to_uf('33')\n+        'RJ'\n+        >>> convert_code_to_uf('99')\n+        >>>\n+    \"\"\"\n+\n+    result = None\n+\n+    if code in CODE_TO_UF.values:\n+        result = CODE_TO_UF(code).name\n+\n+    return result\n", "instance_id": "brazilian-utils__brutils-python-410", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-defined. The goal is clearly articulated: to create a function that converts a 2-digit IBGE code to its corresponding UF (state abbreviation) for Brazilian states. The input (a string of 2 digits) and output (a string representing the UF or None if invalid) are explicitly specified, along with constraints such as handling all Brazilian states and the Federal District. Examples are provided to illustrate the expected behavior (e.g., '12' -> 'AM', '99' -> None). The statement also includes additional context, such as referencing the IBGE source for codes and providing links for further information. The proposed solution structure, including file locations and test cases, is detailed, and alternatives considered are mentioned. There are no significant ambiguities, and even minor details like error handling for invalid inputs are addressed in the test cases. Therefore, this problem statement merits a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes are straightforward and localized. They involve creating a new Enum mapping IBGE codes to UFs in an existing file (`brutils/data/enums/uf.py`), implementing a simple function in a new file (`brutils/ibge/uf.py`), and updating imports and documentation in a few other files (`brutils/__init__.py`, `README.md`, `CHANGELOG.md`). The changes do not impact the broader system architecture and are limited to adding a small, self-contained utility. The amount of code change is minimal, mostly boilerplate and mapping data.\n\n2. **Number of Technical Concepts**: The problem requires basic Python knowledge, including working with Enums (using a custom `BetterEnum` class, which is already in use in the codebase), string handling, and dictionary-like lookups. No advanced algorithms, design patterns, or domain-specific knowledge beyond understanding the IBGE code-to-UF mapping (which is provided) are needed. The concepts involved are fundamental and accessible to junior developers.\n\n3. **Edge Cases and Error Handling**: The problem statement and test cases cover essential edge cases, such as invalid codes (e.g., '99'), empty strings, and non-numeric inputs. The implementation of error handling is simple\u2014returning `None` for invalid inputs\u2014and does not require complex logic.\n\n4. **Overall Complexity**: The logic of the solution is trivial: check if the input code exists in the Enum values and return the corresponding UF name or `None`. There are no performance considerations, intricate interactions with other parts of the codebase, or challenging debugging required.\n\nGiven these factors, a difficulty score of 0.25 is appropriate. This task requires understanding some code logic (e.g., how Enums work in the project) and making simple modifications across a few files, but it remains a basic feature addition that a developer with minimal experience could handle with guidance. It does not approach the complexity of medium or hard tasks, as it lacks depth in technical challenges or architectural impact.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Lack of exact version pinning of ruamel.yaml library\n**Describe the bug**\r\nThe `ruamel.yaml` library contains a bug having to do with string values containing colons, ':'\r\n\r\nhttps://sourceforge.net/p/ruamel-yaml/tickets/469/\r\n\r\nThis bug was introduced in a patch version of ruamel.yaml (0.17.29). However, due to semgrep not pinning the ruamel.yaml version on the patch version (`cli/setup.py` contains the range `ruamel.yaml>=0.16.0,<0.18`), semgrep's behavior in parsing YAML files changed _in the same version_, depending on the moment in time semgrep was installed.\r\n\r\nThis was probably also the root cause of:\r\nhttps://github.com/returntocorp/semgrep/issues/7944\r\nand\r\nhttps://github.com/returntocorp/semgrep-rules/pull/2941\r\nbecause both seem to be related to the same error, but are workarounds that do not address the actual issue.\r\n\r\n\r\n**To Reproduce**\r\nInstall the current latest version of semgrep:\r\n```\r\npip install semgrep==1.24.1\r\n```\r\n\r\nTake a \"bad\" rule from https://github.com/returntocorp/semgrep-rules/pull/2941 before it was modified:\r\n```\r\nrules:\r\n- id: python.lang.security.audit.network.bind.avoid-bind-to-all-interfaces\r\n  languages:\r\n  - python\r\n  message: Running `socket.bind` to 0.0.0.0, ::, or empty string could unexpectedly\r\n    expose the server publicly as it binds to all available interfaces. Consider instead\r\n    getting correct address from an environment variable or configuration file.\r\n  metadata:\r\n    category: security\r\n    confidence: HIGH\r\n    cwe:\r\n    - 'CWE-200: Exposure of Sensitive Information to an Unauthorized Actor'\r\n    cwe2021-top25: true\r\n    impact: MEDIUM\r\n    license: Commons Clause License Condition v1.0[LGPL-2.1-only]\r\n    likelihood: HIGH\r\n    owasp:\r\n    - A01:2021 - Broken Access Control\r\n    references:\r\n    - https://owasp.org/Top10/A01_2021-Broken_Access_Control\r\n    semgrep.dev:\r\n      rule:\r\n        origin: community\r\n        rule_id: OrU3og\r\n        url: https://semgrep.dev/playground/r/5PTY5G/python.lang.security.audit.network.bind.avoid-bind-to-all-interfaces\r\n        version_id: 5PTY5G\r\n    shortlink: https://sg.run/rdln\r\n    source: https://semgrep.dev/r/python.lang.security.audit.network.bind.avoid-bind-to-all-interfaces\r\n    subcategory:\r\n    - vuln\r\n    technology:\r\n    - python\r\n  pattern-either:\r\n  - pattern: '$S = socket.socket(...)\r\n\r\n      ...\r\n\r\n      $S.bind((\"0.0.0.0\", ...))\r\n\r\n      '\r\n  - pattern: '$S = socket.socket(...)\r\n\r\n      ...\r\n\r\n      $S.bind((\"::\", ...))\r\n\r\n      '\r\n  - pattern: '$S = socket.socket(...)\r\n\r\n      ...\r\n\r\n      $S.bind((\"\", ...))\r\n\r\n      '\r\n  severity: INFO\r\n```\r\n\r\nValidate with semgrep:\r\n```\r\n$ semgrep --config bad-rule.yml --validate\r\nConfiguration is invalid - found 1 configuration error(s), and 0 rule(s).\r\n[ERROR] Invalid YAML file bad-rule.yml:\r\n\twhile parsing a block mapping\r\n\t  in \"<file>\", line 2, column 3\r\n\texpected <block end>, but found '<scalar>'\r\n\t  in \"<file>\", line 5, column 47\r\n```\r\n\r\nThis hits the ruamel.yaml bug. Now, when we force a downgrade of the patch version of ruamel.yaml _right_ before the bug was introduced:\r\n```\r\npip install ruamel.yaml==0.17.28\r\n```\r\n.. and validate again, it works fine:\r\n```\r\n$ semgrep --config bad-rule.yml --validate\r\nConfiguration is valid - found 0 configuration error(s), and 1 rule(s).\r\n```\r\n\r\nIn our situation, we experienced different behavior overnight due to the way pinning happens on ruamel.yaml's minor range inside semgrep, which caused some headaches trying to figure out what went wrong while using the exact same version of semgrep, only a new installation (we install semgrep during CI/CD runs), but fortunately we now have a good understanding of the issue.\r\n\r\n**Expected behavior**\r\nFor important dependencies like these, it might make sense to pin them on the exact (patch) version, to avoid changing behavior between semgrep installs of the exact same version.\r\n\r\n**Screenshots**\r\nNone, but can be made on request.\r\n\r\n**What is the priority of the bug to you?**\r\n\r\n- [ ] P0: blocking your adoption of Semgrep or workflow\r\n- [x] P1: important to fix or quite annoying\r\n- [ ] P2: regular bug that should get fixed\r\n\r\n**Environment**\r\npip install in virtualenv\r\n\r\n**Use case**\r\nNo more sudden parsing errors with rules.\r\n\nChange iter_files interface to parse the AST lazily\nThis will enable later to skip parsing entire files if we don't find\ncertain regexps in it\n\ntest plan:\nmake test\n", "patch": "diff --git a/.github/workflows/.codemapignore b/.github/workflows/.codemapignore\nindex 33545517c9ce..73ae5a917fa0 100644\n--- a/.github/workflows/.codemapignore\n+++ b/.github/workflows/.codemapignore\n@@ -5,16 +5,15 @@ build-test-osx-x86.yml\n build-test-osx-arm64.yml\n build-test-manylinux-x86.yml\n build-test-manylinux-aarch64.yml\n-build-test-javascript.yml\n+build-test-windows-x86.yml\n tests.yml\n test-e2e-semgrep-ci.yml\n nightly.yml\n-cron-parsing-stats.yml\n check-semgrep-pro-version.yml\n+build-test-docker.yml\n+push-docker.yml\n start-release.yml\n release.yml\n release-homebrew.yml\n trigger-semgrep-comparison-argo.yml\n-build-test-docker.yml\n-push-docker.yml\n sync-with-PRO.yml\ndiff --git a/.github/workflows/README.md b/.github/workflows/README.md\nindex 90de3db69ef0..8c0bb11872bc 100644\n--- a/.github/workflows/README.md\n+++ b/.github/workflows/README.md\n@@ -5,7 +5,7 @@ See https://docs.github.com/en/actions/learn-github-actions/understanding-github\n for more information on GHA or our Notion page on \"Github actions\".\n \n Most of those workflows have the 'workflow_dispatch:' directive so you can\n-also trigger them manually here: https://github.com/returntocorp/semgrep/actions\n+also trigger them manually here: https://github.com/semgrep/semgrep/actions\n \n Note that many workflows are now written using Jsonnet\n (see https://jsonnet.org/learning/tutorial.html for a great intro to Jsonnet)\ndiff --git a/.github/workflows/libs/actions.libsonnet b/.github/workflows/libs/actions.libsonnet\nindex 6c935c6edbf9..f72b9d1a9af4 100644\n--- a/.github/workflows/libs/actions.libsonnet\n+++ b/.github/workflows/libs/actions.libsonnet\n@@ -75,16 +75,15 @@\n           tar czf artifacts.tgz artifacts\n         ||| % path,\n   },\n-  // TODO: switch to v4 for upload/download below\n   upload_artifact_step: function(artifact_name, path='artifacts.tgz') {\n-       uses: 'actions/upload-artifact@v3',\n+       uses: 'actions/upload-artifact@v4',\n        with: {\n           path: path,\n           name: artifact_name,\n         },\n   },\n   download_artifact_step(artifact_name): {\n-      uses: 'actions/download-artifact@v3',\n+      uses: 'actions/download-artifact@v4',\n       with: {\n         name: artifact_name,\n       },\ndiff --git a/.github/workflows/libs/semgrep.libsonnet b/.github/workflows/libs/semgrep.libsonnet\nindex 6a1ff35c3209..0024339eb414 100644\n--- a/.github/workflows/libs/semgrep.libsonnet\n+++ b/.github/workflows/libs/semgrep.libsonnet\n@@ -362,6 +362,10 @@ local setup_nix_step =\n   depot_project_id: 'fhmxj6w9z8',\n   opam_switch: opam_switch,\n   opam_setup: opam_setup,\n+  // coupling: cli/setup.py, the matrix in run-cli-tests.libsonnet,\n+  // build-test-manylinux-x86.jsonnet in pro, tests.jsonnet in OSS\n+  // TODO? could switch to higher like 3.11\n+  default_python_version: '3.9',\n   containers: containers,\n \n   github_bot: github_bot,\ndiff --git a/.github/workflows/libs/util.libsonnet b/.github/workflows/libs/util.libsonnet\nindex a95c94cae7f1..e7232592a28e 100644\n--- a/.github/workflows/libs/util.libsonnet\n+++ b/.github/workflows/libs/util.libsonnet\n@@ -56,7 +56,7 @@\n   },\n   // workflow_inputs is a JSON object that looks like [{ \"name\": \"name_1\", \"value\": \"value_1\" }, ...]\n   // \"value_1\" should be a string that can include GHA expression for the value, e.g.,\n-  // \"returntocorp/semgrep:${{ needs.setup-docker-tag.outputs.docker-tag }}\".\n+  // \"semgrep/semgrep:${{ needs.setup-docker-tag.outputs.docker-tag }}\".\n   // This will be converted into env vars to avoid accidental script injections\n   // and reconstructed as the JSON body in the POST request to Argo.\n   trigger_argo_workflow: function(trigger_url, workflow_inputs) {\ndiff --git a/.github/workflows/push-docker.jsonnet b/.github/workflows/push-docker.jsonnet\nindex 53aba5ea8b30..29654a4813c6 100644\n--- a/.github/workflows/push-docker.jsonnet\n+++ b/.github/workflows/push-docker.jsonnet\n@@ -52,7 +52,7 @@ local job = {\n       uses: 'iarekylew00t/regctl-installer@v1',\n     },\n     {\n-      uses: 'actions/download-artifact@v3',\n+      uses: 'actions/download-artifact@v4',\n       with: {\n         path: '/tmp/artifacts',\n       },\ndiff --git a/.github/workflows/push-docker.yml b/.github/workflows/push-docker.yml\nindex b9f0a93bb3e1..e10df81da72e 100644\n--- a/.github/workflows/push-docker.yml\n+++ b/.github/workflows/push-docker.yml\n@@ -4,7 +4,7 @@ jobs:\n     runs-on: ubuntu-22.04\n     steps:\n       - uses: iarekylew00t/regctl-installer@v1\n-      - uses: actions/download-artifact@v3\n+      - uses: actions/download-artifact@v4\n         with:\n           path: /tmp/artifacts\n       - env:\ndiff --git a/.github/workflows/release.jsonnet b/.github/workflows/release.jsonnet\nindex a8e21a2c877e..f7714aa3d06d 100644\n--- a/.github/workflows/release.jsonnet\n+++ b/.github/workflows/release.jsonnet\n@@ -232,7 +232,7 @@ local park_pypi_packages_job = {\n \n local download_step(name) = {\n   name: 'Download %s' % name,\n-  uses: 'actions/download-artifact@v3',\n+  uses: 'actions/download-artifact@v4',\n   with: {\n     name: name,\n     path: name,\n@@ -290,7 +290,7 @@ local create_release_job = {\n         GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}',\n       },\n       run: |||\n-        while ! gh release --repo returntocorp/semgrep list -L 5 | grep -q \"%s\"; do\n+        while ! gh release --repo semgrep/semgrep list -L 5 | grep -q \"%s\"; do\n           echo \"release not yet ready, sleeping for 5 seconds\"\n           sleep 5\n         done\n@@ -302,7 +302,7 @@ local create_release_job = {\n       env: {\n         GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}',\n       },\n-      run: 'gh release --repo returntocorp/semgrep edit %s --draft=false' % version,\n+      run: 'gh release --repo semgrep/semgrep edit %s --draft=false' % version,\n     },\n   ],\n } + unless_dry_run;\n@@ -327,7 +327,7 @@ local create_release_interfaces_job = {\n       env: {\n         GITHUB_TOKEN: semgrep.github_bot.token_ref,\n       },\n-      run: 'gh release --repo returntocorp/semgrep-interfaces upload %s cli/src/semgrep/semgrep_interfaces/rule_schema_v1.yaml' % version,\n+      run: 'gh release --repo semgrep/semgrep-interfaces upload %s cli/src/semgrep/semgrep_interfaces/rule_schema_v1.yaml' % version,\n     },\n     {\n       name: 'Publish Release Semgrep Interfaces',\n@@ -335,7 +335,7 @@ local create_release_interfaces_job = {\n       env: {\n         GITHUB_TOKEN: semgrep.github_bot.token_ref,\n       },\n-      run: 'gh release --repo returntocorp/semgrep-interfaces edit %s --draft=false' % version,\n+      run: 'gh release --repo semgrep/semgrep-interfaces edit %s --draft=false' % version,\n     },\n   ],\n } + unless_dry_run;\ndiff --git a/.github/workflows/release.yml b/.github/workflows/release.yml\nindex faf0f1e5a0b3..0da2799392aa 100644\n--- a/.github/workflows/release.yml\n+++ b/.github/workflows/release.yml\n@@ -65,7 +65,7 @@ jobs:\n         id: wait-draft-release\n         name: Wait for Draft Release if not Ready\n         run: |\n-          while ! gh release --repo returntocorp/semgrep list -L 5 | grep -q \"${{ steps.get-version.outputs.VERSION }}\"; do\n+          while ! gh release --repo semgrep/semgrep list -L 5 | grep -q \"${{ steps.get-version.outputs.VERSION }}\"; do\n             echo \"release not yet ready, sleeping for 5 seconds\"\n             sleep 5\n           done\n@@ -73,7 +73,7 @@ jobs:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         id: publish_release\n         name: Publish Release\n-        run: gh release --repo returntocorp/semgrep edit ${{ steps.get-version.outputs.VERSION }} --draft=false\n+        run: gh release --repo semgrep/semgrep edit ${{ steps.get-version.outputs.VERSION }} --draft=false\n   create-release-interfaces:\n     if: ${{ ! inputs.dry-run }}\n     needs:\n@@ -108,12 +108,12 @@ jobs:\n           GITHUB_TOKEN: ${{ steps.token.outputs.token }}\n         id: upload-semgrep-schema-files\n         name: Upload Schema Files\n-        run: gh release --repo returntocorp/semgrep-interfaces upload ${{ steps.get-version.outputs.VERSION }} cli/src/semgrep/semgrep_interfaces/rule_schema_v1.yaml\n+        run: gh release --repo semgrep/semgrep-interfaces upload ${{ steps.get-version.outputs.VERSION }} cli/src/semgrep/semgrep_interfaces/rule_schema_v1.yaml\n       - env:\n           GITHUB_TOKEN: ${{ steps.token.outputs.token }}\n         id: publish_release_semgrep_interfaces\n         name: Publish Release Semgrep Interfaces\n-        run: gh release --repo returntocorp/semgrep-interfaces edit ${{ steps.get-version.outputs.VERSION }} --draft=false\n+        run: gh release --repo semgrep/semgrep-interfaces edit ${{ steps.get-version.outputs.VERSION }} --draft=false\n   park-pypi-packages:\n     defaults:\n       run:\n@@ -188,22 +188,22 @@ jobs:\n     runs-on: ubuntu-latest\n     steps:\n       - name: Download manylinux-x86-wheel\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4\n         with:\n           name: manylinux-x86-wheel\n           path: manylinux-x86-wheel\n       - name: Download manylinux-aarch64-wheel\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4\n         with:\n           name: manylinux-aarch64-wheel\n           path: manylinux-aarch64-wheel\n       - name: Download osx-x86-wheel\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4\n         with:\n           name: osx-x86-wheel\n           path: osx-x86-wheel\n       - name: Download osx-arm64-wheel\n-        uses: actions/download-artifact@v3\n+        uses: actions/download-artifact@v4\n         with:\n           name: osx-arm64-wheel\n           path: osx-arm64-wheel\ndiff --git a/.github/workflows/semgrep.jsonnet b/.github/workflows/semgrep.jsonnet\nindex 59a1f379cfcc..66b455873d7c 100644\n--- a/.github/workflows/semgrep.jsonnet\n+++ b/.github/workflows/semgrep.jsonnet\n@@ -1,6 +1,6 @@\n // This workflow dogfoods 'semgrep ci', which includes running rules\n // for Code, Supply Chain, and Secrets set in the Semgrep WebApp.\n-// We're also dogfooding the returntocorp/semgrep:canary docker image!\n+// We're also dogfooding the semgrep/semgrep:canary docker image!\n // (see https://www.notion.so/semgrep/returntocorp-semgrep-canary-docker-canary).\n \n local actions = import 'libs/actions.libsonnet';\ndiff --git a/.github/workflows/start-release.jsonnet b/.github/workflows/start-release.jsonnet\nindex 90c6e91d7e1e..7f9e50f0781d 100644\n--- a/.github/workflows/start-release.jsonnet\n+++ b/.github/workflows/start-release.jsonnet\n@@ -86,7 +86,7 @@ local bump_job(repository) = {\n // Wait PR checks helpers\n // ----------------------------------------------------------------------------\n \n-local len_checks = \"$(gh pr -R returntocorp/semgrep view %s --json statusCheckRollup --jq '.statusCheckRollup | length')\" % pr_number;\n+local len_checks = \"$(gh pr -R semgrep/semgrep view %s --json statusCheckRollup --jq '.statusCheckRollup | length')\" % pr_number;\n \n local wait_pr_checks_to_register(while_cond) = |||\n   LEN_CHECKS=%s;\n@@ -97,7 +97,7 @@ local wait_pr_checks_to_register(while_cond) = |||\n   done\n   echo \"checks are valid\"\n   echo ${LEN_CHECKS}\n-  gh pr -R returntocorp/semgrep view %s --json statusCheckRollup\n+  gh pr -R semgrep/semgrep view %s --json statusCheckRollup\n ||| % [len_checks, while_cond, len_checks, pr_number];\n \n local wait_pr_checks_to_register_step(while_cond) = {\n@@ -115,7 +115,7 @@ local wait_pr_checks_to_complete_step = {\n     GITHUB_TOKEN: '${{ secrets.GITHUB_TOKEN }}',\n   },\n   // Wait for PR checks to finish\n-  run: 'gh pr -R returntocorp/semgrep checks %s --interval 90 --watch' % pr_number,\n+  run: 'gh pr -R semgrep/semgrep checks %s --interval 90 --watch' % pr_number,\n };\n \n local get_current_num_checks_step = {\n@@ -365,7 +365,7 @@ local create_draft_release_job = {\n         token: semgrep.github_bot.token_ref,\n         prerelease: false,\n         draft: true,\n-        repository: 'returntocorp/semgrep-interfaces',\n+        repository: 'semgrep/semgrep-interfaces',\n       },\n     },\n   ],\n@@ -446,7 +446,7 @@ local notify_success_job = {\n       run: |||\n         # POST a webhook to Zapier to allow for public notifications to our users via Twitter\n         curl \"${{ secrets.ZAPIER_WEBHOOK_URL }}\" \\\n-          -d '{\"version\":\"${VERSION}\",\"changelog_url\":\"https://github.com/returntocorp/semgrep/releases/tag/v${VERSION}\"}'\n+          -d '{\"version\":\"${VERSION}\",\"changelog_url\":\"https://github.com/semgrep/semgrep/releases/tag/v${VERSION}\"}'\n       |||\n     },\n     {\ndiff --git a/.github/workflows/start-release.yml b/.github/workflows/start-release.yml\nindex f1fedc661d88..397eae39162e 100644\n--- a/.github/workflows/start-release.yml\n+++ b/.github/workflows/start-release.yml\n@@ -252,7 +252,7 @@ jobs:\n           draft: true\n           name: Release v${{ github.event.inputs.semgrep-version }}\n           prerelease: false\n-          repository: returntocorp/semgrep-interfaces\n+          repository: semgrep/semgrep-interfaces\n           tag_name: v${{ github.event.inputs.semgrep-version }}\n           token: ${{ steps.token.outputs.token }}\n   create-tag:\n@@ -350,7 +350,7 @@ jobs:\n         run: |\n           # POST a webhook to Zapier to allow for public notifications to our users via Twitter\n           curl \"${{ secrets.ZAPIER_WEBHOOK_URL }}\" \\\n-            -d '{\"version\":\"${VERSION}\",\"changelog_url\":\"https://github.com/returntocorp/semgrep/releases/tag/v${VERSION}\"}'\n+            -d '{\"version\":\"${VERSION}\",\"changelog_url\":\"https://github.com/semgrep/semgrep/releases/tag/v${VERSION}\"}'\n       - name: Notify Success on Slack\n         run: |\n           curl --request POST \\\n@@ -458,26 +458,26 @@ jobs:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         name: Wait for checks to register\n         run: |\n-          LEN_CHECKS=$(gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n+          LEN_CHECKS=$(gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n           while [ ${LEN_CHECKS} = \"0\" ]; do\n             echo \"No checks available yet\"\n             sleep 1\n-            LEN_CHECKS=$(gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n+            LEN_CHECKS=$(gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n           done\n           echo \"checks are valid\"\n           echo ${LEN_CHECKS}\n-          gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup\n+          gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup\n       - env:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         id: wait-checks\n         name: Wait for checks to complete\n-        run: gh pr -R returntocorp/semgrep checks \"${{ needs.release-setup.outputs.pr-number }}\" --interval 90 --watch\n+        run: gh pr -R semgrep/semgrep checks \"${{ needs.release-setup.outputs.pr-number }}\" --interval 90 --watch\n       - env:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         id: num-checks\n         name: Get Current Num Checks\n         run: |\n-          LEN_CHECKS=$(gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n+          LEN_CHECKS=$(gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n           echo \"num-checks=${LEN_CHECKS}\" >> $GITHUB_OUTPUT\n   wait-for-release-checks:\n     if: ${{ ! inputs.dry-run }}\n@@ -491,20 +491,20 @@ jobs:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         name: Wait for checks to register\n         run: |\n-          LEN_CHECKS=$(gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n+          LEN_CHECKS=$(gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n           while [ ${LEN_CHECKS} = \"${{ needs.wait-for-pr-checks.outputs.num-checks }}\" ]; do\n             echo \"No checks available yet\"\n             sleep 1\n-            LEN_CHECKS=$(gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n+            LEN_CHECKS=$(gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup --jq '.statusCheckRollup | length');\n           done\n           echo \"checks are valid\"\n           echo ${LEN_CHECKS}\n-          gh pr -R returntocorp/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup\n+          gh pr -R semgrep/semgrep view \"${{ needs.release-setup.outputs.pr-number }}\" --json statusCheckRollup\n       - env:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         id: wait-checks\n         name: Wait for checks to complete\n-        run: gh pr -R returntocorp/semgrep checks \"${{ needs.release-setup.outputs.pr-number }}\" --interval 90 --watch\n+        run: gh pr -R semgrep/semgrep checks \"${{ needs.release-setup.outputs.pr-number }}\" --interval 90 --watch\n name: start-release\n on:\n   workflow_dispatch:\ndiff --git a/.github/workflows/trigger-doc-update.yml b/.github/workflows/trigger-doc-update.yml\nindex 30d5aff57fb2..861b6e46cf43 100644\n--- a/.github/workflows/trigger-doc-update.yml\n+++ b/.github/workflows/trigger-doc-update.yml\n@@ -23,4 +23,4 @@ jobs:\n         env:\n           GITHUB_TOKEN: ${{ steps.generate_token.outputs.token }}\n         run: |\n-          gh api repos/returntocorp/semgrep-docs/dispatches -f event_type=new_release --verbose\n+          gh api repos/semgrep/semgrep-docs/dispatches -f event_type=new_release --verbose\ndiff --git a/Dockerfile b/Dockerfile\nindex 5c8574b127b0..b67a81326451 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -159,7 +159,7 @@ RUN apk upgrade --no-cache && \\\n #\n # history: we used to install here various utilities needed by some of our\n # scripts under scripts/. Indeed, those scripts are run from CI jobs using the\n-# returntocorp/semgrep docker image as the container because they rely on semgrep\n+# semgrep/semgrep docker image as the container because they rely on semgrep\n # or semgrep-core. Those scripts must also perform different\n # tasks that require utilities other than semgrep (e.g., compute parsing\n # statistics and then run 'jq' to filter the JSON). It is convenient to add\n@@ -241,7 +241,7 @@ ENV SEMGREP_IN_DOCKER=1 \\\n     SEMGREP_USER_AGENT_APPEND=\"Docker\"\n \n # The command we tell people to run for testing semgrep in Docker is\n-#   docker run --rm -v \"${PWD}:/src\" returntocorp/semgrep semgrep --config=auto\n+#   docker run --rm -v \"${PWD}:/src\" semgrep/semgrep semgrep --config=auto\n # (see https://semgrep.dev/docs/getting-started/ ), hence this WORKDIR directive\n WORKDIR /src\n \n@@ -261,11 +261,9 @@ RUN printf \"[safe]\\n\tdirectory = /src\"  > ~semgrep/.gitconfig && \\\n \n # Note that we just use CMD below. Why not using ENTRYPOINT [\"semgrep\"] ?\n # so that people can simply run\n-# `docker run --rm -v \"${PWD}:/src\" returntocorp/semgrep --help` instead of\n-# `docker run --rm -v \"${PWD}:/src\" returntocorp/semgrep semgrep --help`?\n-# (It's even worse now that we've switched company name with\n-# `docker run --rm -v \"${PWD}:/src\" semgrep/semgrep semgrep --help`, we now\n-# have three semgrep, hmmm).\n+# `docker run --rm -v \"${PWD}:/src\" semgrep/semgrep --help` instead of\n+# `docker run --rm -v \"${PWD}:/src\" semgrep/semgrep semgrep --help`?\n+# (Yes, that's 3 semgrep in a row, hmmm)\n #\n # This is mainly to play well with CI providers like Gitlab. Indeed,\n # gitlab CI sets up all CI jobs by first running other commands in the\n@@ -311,7 +309,7 @@ RUN rm -rf /root/.semgrep\n # We can't make this the default in the semgrep-cli stage above because of\n # permissions errors on the mounted volume when using instructions for running\n # semgrep with docker:\n-#   `docker run -v \"${PWD}:/src\" -i returntocorp/semgrep semgrep`\n+#   `docker run -v \"${PWD}:/src\" -i semgrep/semgrep semgrep`\n \n #coupling: the 'nonroot' name is used in release.jsonnet\n FROM semgrep-cli AS nonroot\ndiff --git a/Makefile b/Makefile\nindex c09559882f3e..5518a42f1037 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -633,7 +633,7 @@ DOCKER_IMAGE=semgrep/semgrep:develop\n \n # If you get parsing errors while running this command, maybe you have an old\n # cached version of the docker image. You can invalidate the cache with\n-#   'docker rmi returntocorp/semgrep:develop`\n+#   'docker rmi semgrep/semgrep:develop`\n check_with_docker:\n \tdocker run --rm -v \"${PWD}:/src\" $(DOCKER_IMAGE) semgrep $(SEMGREP_ARGS)\n \ndiff --git a/README.md b/README.md\nindex 1a95d500f2c7..088ed509e63d 100644\n--- a/README.md\n+++ b/README.md\n@@ -33,8 +33,8 @@\n   <a href=\"https://hub.docker.com/r/semgrep/semgrep\">\n     <img src=\"https://img.shields.io/docker/pulls/semgrep/semgrep.svg?style=flat-square\" alt=\"Docker Pulls\" />\n   </a>\n-  <a href=\"https://hub.docker.com/r/returntocorp/semgrep\">\n-    <img src=\"https://img.shields.io/docker/pulls/returntocorp/semgrep.svg?style=flat-square\" alt=\"Docker Pulls (Old)\" />\n+  <a href=\"https://hub.docker.com/r/semgrep/semgrep\">\n+    <img src=\"https://img.shields.io/docker/pulls/semgrep/semgrep.svg?style=flat-square\" alt=\"Docker Pulls (Old)\" />\n   </a>\n   <a href=\"https://twitter.com/intent/follow?screen_name=semgrep\">\n     <img src=\"https://img.shields.io/twitter/follow/semgrep?label=Follow%20semgrep&style=social&color=blue\" alt=\"Follow @semgrep on Twitter\" />\ndiff --git a/changelog.d/CODE-7871.fixed b/changelog.d/CODE-7871.fixed\nnew file mode 100644\nindex 000000000000..7cf7a3bd8163\n--- /dev/null\n+++ b/changelog.d/CODE-7871.fixed\n@@ -0,0 +1,12 @@\n+Fixed bug affecting taint tracking through static fields when mixing accesses\n+using the class name and using an instance object, e.g.:\n+\n+    class C {\n+        static String s;\n+    }\n+\n+    ...\n+\n+            C o = new C();\n+            C.s = taint;\n+            sink(o.s); // finding !\ndiff --git a/changelog.d/python.changed b/changelog.d/python.changed\nnew file mode 100644\nindex 000000000000..348c0fccdd45\n--- /dev/null\n+++ b/changelog.d/python.changed\n@@ -0,0 +1,2 @@\n+The minimum Python version for semgrep is now 3.9.\n+We are dropping support for Python 3.8\ndiff --git a/cli/Pipfile.lock b/cli/Pipfile.lock\nindex 4ec29866fbd3..5ca70f02350c 100644\n--- a/cli/Pipfile.lock\n+++ b/cli/Pipfile.lock\n@@ -31,11 +31,11 @@\n         },\n         \"bracex\": {\n             \"hashes\": [\n-                \"sha256:0725da5045e8d37ea9592ab3614d8b561e22c3c5fde3964699be672e072ab611\",\n-                \"sha256:d2fcf4b606a82ac325471affe1706dd9bbaa3536c91ef86a31f6b766f3dad1d0\"\n+                \"sha256:12c50952415bfa773d2d9ccb8e79651b8cdb1f31a42f6091b804f6ba2b4a66b6\",\n+                \"sha256:13e5732fec27828d6af308628285ad358047cec36801598368cb28bc631dbaf6\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==2.5\"\n+            \"version\": \"==2.5.post1\"\n         },\n         \"certifi\": {\n             \"hashes\": [\n@@ -47,99 +47,114 @@\n         },\n         \"charset-normalizer\": {\n             \"hashes\": [\n-                \"sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027\",\n-                \"sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\",\n-                \"sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786\",\n-                \"sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8\",\n-                \"sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09\",\n-                \"sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185\",\n-                \"sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574\",\n-                \"sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e\",\n-                \"sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519\",\n-                \"sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898\",\n-                \"sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269\",\n-                \"sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3\",\n-                \"sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f\",\n-                \"sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6\",\n-                \"sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8\",\n-                \"sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a\",\n-                \"sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73\",\n-                \"sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc\",\n-                \"sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714\",\n-                \"sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2\",\n-                \"sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc\",\n-                \"sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce\",\n-                \"sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d\",\n-                \"sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e\",\n-                \"sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6\",\n-                \"sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269\",\n-                \"sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96\",\n-                \"sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d\",\n-                \"sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a\",\n-                \"sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4\",\n-                \"sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77\",\n-                \"sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d\",\n-                \"sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0\",\n-                \"sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed\",\n-                \"sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068\",\n-                \"sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac\",\n-                \"sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25\",\n-                \"sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8\",\n-                \"sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab\",\n-                \"sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26\",\n-                \"sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2\",\n-                \"sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db\",\n-                \"sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f\",\n-                \"sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5\",\n-                \"sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99\",\n-                \"sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c\",\n-                \"sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d\",\n-                \"sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811\",\n-                \"sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa\",\n-                \"sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a\",\n-                \"sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03\",\n-                \"sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b\",\n-                \"sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04\",\n-                \"sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c\",\n-                \"sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001\",\n-                \"sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458\",\n-                \"sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389\",\n-                \"sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99\",\n-                \"sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985\",\n-                \"sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537\",\n-                \"sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238\",\n-                \"sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f\",\n-                \"sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d\",\n-                \"sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796\",\n-                \"sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a\",\n-                \"sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143\",\n-                \"sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8\",\n-                \"sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c\",\n-                \"sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5\",\n-                \"sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5\",\n-                \"sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711\",\n-                \"sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4\",\n-                \"sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6\",\n-                \"sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c\",\n-                \"sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7\",\n-                \"sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4\",\n-                \"sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b\",\n-                \"sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae\",\n-                \"sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12\",\n-                \"sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c\",\n-                \"sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae\",\n-                \"sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8\",\n-                \"sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887\",\n-                \"sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b\",\n-                \"sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4\",\n-                \"sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f\",\n-                \"sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5\",\n-                \"sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33\",\n-                \"sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519\",\n-                \"sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\"\n+                \"sha256:0099d79bdfcf5c1f0c2c72f91516702ebf8b0b8ddd8905f97a8aecf49712c621\",\n+                \"sha256:0713f3adb9d03d49d365b70b84775d0a0d18e4ab08d12bc46baa6132ba78aaf6\",\n+                \"sha256:07afec21bbbbf8a5cc3651aa96b980afe2526e7f048fdfb7f1014d84acc8b6d8\",\n+                \"sha256:0b309d1747110feb25d7ed6b01afdec269c647d382c857ef4663bbe6ad95a912\",\n+                \"sha256:0d99dd8ff461990f12d6e42c7347fd9ab2532fb70e9621ba520f9e8637161d7c\",\n+                \"sha256:0de7b687289d3c1b3e8660d0741874abe7888100efe14bd0f9fd7141bcbda92b\",\n+                \"sha256:1110e22af8ca26b90bd6364fe4c763329b0ebf1ee213ba32b68c73de5752323d\",\n+                \"sha256:130272c698667a982a5d0e626851ceff662565379baf0ff2cc58067b81d4f11d\",\n+                \"sha256:136815f06a3ae311fae551c3df1f998a1ebd01ddd424aa5603a4336997629e95\",\n+                \"sha256:14215b71a762336254351b00ec720a8e85cada43b987da5a042e4ce3e82bd68e\",\n+                \"sha256:1db4e7fefefd0f548d73e2e2e041f9df5c59e178b4c72fbac4cc6f535cfb1565\",\n+                \"sha256:1ffd9493de4c922f2a38c2bf62b831dcec90ac673ed1ca182fe11b4d8e9f2a64\",\n+                \"sha256:2006769bd1640bdf4d5641c69a3d63b71b81445473cac5ded39740a226fa88ab\",\n+                \"sha256:20587d20f557fe189b7947d8e7ec5afa110ccf72a3128d61a2a387c3313f46be\",\n+                \"sha256:223217c3d4f82c3ac5e29032b3f1c2eb0fb591b72161f86d93f5719079dae93e\",\n+                \"sha256:27623ba66c183eca01bf9ff833875b459cad267aeeb044477fedac35e19ba907\",\n+                \"sha256:285e96d9d53422efc0d7a17c60e59f37fbf3dfa942073f666db4ac71e8d726d0\",\n+                \"sha256:2de62e8801ddfff069cd5c504ce3bc9672b23266597d4e4f50eda28846c322f2\",\n+                \"sha256:2f6c34da58ea9c1a9515621f4d9ac379871a8f21168ba1b5e09d74250de5ad62\",\n+                \"sha256:309a7de0a0ff3040acaebb35ec45d18db4b28232f21998851cfa709eeff49d62\",\n+                \"sha256:35c404d74c2926d0287fbd63ed5d27eb911eb9e4a3bb2c6d294f3cfd4a9e0c23\",\n+                \"sha256:3710a9751938947e6327ea9f3ea6332a09bf0ba0c09cae9cb1f250bd1f1549bc\",\n+                \"sha256:3d59d125ffbd6d552765510e3f31ed75ebac2c7470c7274195b9161a32350284\",\n+                \"sha256:40d3ff7fc90b98c637bda91c89d51264a3dcf210cade3a2c6f838c7268d7a4ca\",\n+                \"sha256:425c5f215d0eecee9a56cdb703203dda90423247421bf0d67125add85d0c4455\",\n+                \"sha256:43193c5cda5d612f247172016c4bb71251c784d7a4d9314677186a838ad34858\",\n+                \"sha256:44aeb140295a2f0659e113b31cfe92c9061622cadbc9e2a2f7b8ef6b1e29ef4b\",\n+                \"sha256:47334db71978b23ebcf3c0f9f5ee98b8d65992b65c9c4f2d34c2eaf5bcaf0594\",\n+                \"sha256:4796efc4faf6b53a18e3d46343535caed491776a22af773f366534056c4e1fbc\",\n+                \"sha256:4a51b48f42d9358460b78725283f04bddaf44a9358197b889657deba38f329db\",\n+                \"sha256:4b67fdab07fdd3c10bb21edab3cbfe8cf5696f453afce75d815d9d7223fbe88b\",\n+                \"sha256:4ec9dd88a5b71abfc74e9df5ebe7921c35cbb3b641181a531ca65cdb5e8e4dea\",\n+                \"sha256:4f9fc98dad6c2eaa32fc3af1417d95b5e3d08aff968df0cd320066def971f9a6\",\n+                \"sha256:54b6a92d009cbe2fb11054ba694bc9e284dad30a26757b1e372a1fdddaf21920\",\n+                \"sha256:55f56e2ebd4e3bc50442fbc0888c9d8c94e4e06a933804e2af3e89e2f9c1c749\",\n+                \"sha256:5726cf76c982532c1863fb64d8c6dd0e4c90b6ece9feb06c9f202417a31f7dd7\",\n+                \"sha256:5d447056e2ca60382d460a604b6302d8db69476fd2015c81e7c35417cfabe4cd\",\n+                \"sha256:5ed2e36c3e9b4f21dd9422f6893dec0abf2cca553af509b10cd630f878d3eb99\",\n+                \"sha256:5ff2ed8194587faf56555927b3aa10e6fb69d931e33953943bc4f837dfee2242\",\n+                \"sha256:62f60aebecfc7f4b82e3f639a7d1433a20ec32824db2199a11ad4f5e146ef5ee\",\n+                \"sha256:63bc5c4ae26e4bc6be6469943b8253c0fd4e4186c43ad46e713ea61a0ba49129\",\n+                \"sha256:6b40e8d38afe634559e398cc32b1472f376a4099c75fe6299ae607e404c033b2\",\n+                \"sha256:6b493a043635eb376e50eedf7818f2f322eabbaa974e948bd8bdd29eb7ef2a51\",\n+                \"sha256:6dba5d19c4dfab08e58d5b36304b3f92f3bd5d42c1a3fa37b5ba5cdf6dfcbcee\",\n+                \"sha256:6fd30dc99682dc2c603c2b315bded2799019cea829f8bf57dc6b61efde6611c8\",\n+                \"sha256:707b82d19e65c9bd28b81dde95249b07bf9f5b90ebe1ef17d9b57473f8a64b7b\",\n+                \"sha256:7706f5850360ac01d80c89bcef1640683cc12ed87f42579dab6c5d3ed6888613\",\n+                \"sha256:7782afc9b6b42200f7362858f9e73b1f8316afb276d316336c0ec3bd73312742\",\n+                \"sha256:79983512b108e4a164b9c8d34de3992f76d48cadc9554c9e60b43f308988aabe\",\n+                \"sha256:7f683ddc7eedd742e2889d2bfb96d69573fde1d92fcb811979cdb7165bb9c7d3\",\n+                \"sha256:82357d85de703176b5587dbe6ade8ff67f9f69a41c0733cf2425378b49954de5\",\n+                \"sha256:84450ba661fb96e9fd67629b93d2941c871ca86fc38d835d19d4225ff946a631\",\n+                \"sha256:86f4e8cca779080f66ff4f191a685ced73d2f72d50216f7112185dc02b90b9b7\",\n+                \"sha256:8cda06946eac330cbe6598f77bb54e690b4ca93f593dee1568ad22b04f347c15\",\n+                \"sha256:8ce7fd6767a1cc5a92a639b391891bf1c268b03ec7e021c7d6d902285259685c\",\n+                \"sha256:8ff4e7cdfdb1ab5698e675ca622e72d58a6fa2a8aa58195de0c0061288e6e3ea\",\n+                \"sha256:9289fd5dddcf57bab41d044f1756550f9e7cf0c8e373b8cdf0ce8773dc4bd417\",\n+                \"sha256:92a7e36b000bf022ef3dbb9c46bfe2d52c047d5e3f3343f43204263c5addc250\",\n+                \"sha256:92db3c28b5b2a273346bebb24857fda45601aef6ae1c011c0a997106581e8a88\",\n+                \"sha256:95c3c157765b031331dd4db3c775e58deaee050a3042fcad72cbc4189d7c8dca\",\n+                \"sha256:980b4f289d1d90ca5efcf07958d3eb38ed9c0b7676bf2831a54d4f66f9c27dfa\",\n+                \"sha256:9ae4ef0b3f6b41bad6366fb0ea4fc1d7ed051528e113a60fa2a65a9abb5b1d99\",\n+                \"sha256:9c98230f5042f4945f957d006edccc2af1e03ed5e37ce7c373f00a5a4daa6149\",\n+                \"sha256:9fa2566ca27d67c86569e8c85297aaf413ffab85a8960500f12ea34ff98e4c41\",\n+                \"sha256:a14969b8691f7998e74663b77b4c36c0337cb1df552da83d5c9004a93afdb574\",\n+                \"sha256:a8aacce6e2e1edcb6ac625fb0f8c3a9570ccc7bfba1f63419b3769ccf6a00ed0\",\n+                \"sha256:a8e538f46104c815be19c975572d74afb53f29650ea2025bbfaef359d2de2f7f\",\n+                \"sha256:aa41e526a5d4a9dfcfbab0716c7e8a1b215abd3f3df5a45cf18a12721d31cb5d\",\n+                \"sha256:aa693779a8b50cd97570e5a0f343538a8dbd3e496fa5dcb87e29406ad0299654\",\n+                \"sha256:ab22fbd9765e6954bc0bcff24c25ff71dcbfdb185fcdaca49e81bac68fe724d3\",\n+                \"sha256:ab2e5bef076f5a235c3774b4f4028a680432cded7cad37bba0fd90d64b187d19\",\n+                \"sha256:ab973df98fc99ab39080bfb0eb3a925181454d7c3ac8a1e695fddfae696d9e90\",\n+                \"sha256:af73657b7a68211996527dbfeffbb0864e043d270580c5aef06dc4b659a4b578\",\n+                \"sha256:b197e7094f232959f8f20541ead1d9862ac5ebea1d58e9849c1bf979255dfac9\",\n+                \"sha256:b295729485b06c1a0683af02a9e42d2caa9db04a373dc38a6a58cdd1e8abddf1\",\n+                \"sha256:b8831399554b92b72af5932cdbbd4ddc55c55f631bb13ff8fe4e6536a06c5c51\",\n+                \"sha256:b8dcd239c743aa2f9c22ce674a145e0a25cb1566c495928440a181ca1ccf6719\",\n+                \"sha256:bcb4f8ea87d03bc51ad04add8ceaf9b0f085ac045ab4d74e73bbc2dc033f0236\",\n+                \"sha256:bd7af3717683bea4c87acd8c0d3d5b44d56120b26fd3f8a692bdd2d5260c620a\",\n+                \"sha256:bf4475b82be41b07cc5e5ff94810e6a01f276e37c2d55571e3fe175e467a1a1c\",\n+                \"sha256:c3e446d253bd88f6377260d07c895816ebf33ffffd56c1c792b13bff9c3e1ade\",\n+                \"sha256:c57516e58fd17d03ebe67e181a4e4e2ccab1168f8c2976c6a334d4f819fe5944\",\n+                \"sha256:c94057af19bc953643a33581844649a7fdab902624d2eb739738a30e2b3e60fc\",\n+                \"sha256:cab5d0b79d987c67f3b9e9c53f54a61360422a5a0bc075f43cab5621d530c3b6\",\n+                \"sha256:ce031db0408e487fd2775d745ce30a7cd2923667cf3b69d48d219f1d8f5ddeb6\",\n+                \"sha256:cee4373f4d3ad28f1ab6290684d8e2ebdb9e7a1b74fdc39e4c211995f77bec27\",\n+                \"sha256:d5b054862739d276e09928de37c79ddeec42a6e1bfc55863be96a36ba22926f6\",\n+                \"sha256:dbe03226baf438ac4fda9e2d0715022fd579cb641c4cf639fa40d53b2fe6f3e2\",\n+                \"sha256:dc15e99b2d8a656f8e666854404f1ba54765871104e50c8e9813af8a7db07f12\",\n+                \"sha256:dcaf7c1524c0542ee2fc82cc8ec337f7a9f7edee2532421ab200d2b920fc97cf\",\n+                \"sha256:dd4eda173a9fcccb5f2e2bd2a9f423d180194b1bf17cf59e3269899235b2a114\",\n+                \"sha256:dd9a8bd8900e65504a305bf8ae6fa9fbc66de94178c420791d0293702fce2df7\",\n+                \"sha256:de7376c29d95d6719048c194a9cf1a1b0393fbe8488a22008610b0361d834ecf\",\n+                \"sha256:e7fdd52961feb4c96507aa649550ec2a0d527c086d284749b2f582f2d40a2e0d\",\n+                \"sha256:e91f541a85298cf35433bf66f3fab2a4a2cff05c127eeca4af174f6d497f0d4b\",\n+                \"sha256:e9e3c4c9e1ed40ea53acf11e2a386383c3304212c965773704e4603d589343ed\",\n+                \"sha256:ee803480535c44e7f5ad00788526da7d85525cfefaf8acf8ab9a310000be4b03\",\n+                \"sha256:f09cb5a7bbe1ecae6e87901a2eb23e0256bb524a79ccc53eb0b7629fbe7677c4\",\n+                \"sha256:f19c1585933c82098c2a520f8ec1227f20e339e33aca8fa6f956f6691b784e67\",\n+                \"sha256:f1a2f519ae173b5b6a2c9d5fa3116ce16e48b3462c8b96dfdded11055e3d6365\",\n+                \"sha256:f28f891ccd15c514a0981f3b9db9aa23d62fe1a99997512b0491d2ed323d229a\",\n+                \"sha256:f3e73a4255342d4eb26ef6df01e3962e73aa29baa3124a8e824c5d3364a65748\",\n+                \"sha256:f606a1881d2663630ea5b8ce2efe2111740df4b687bd78b34a8131baa007f79b\",\n+                \"sha256:fe9f97feb71aa9896b81973a7bbada8c49501dc73e58a10fcef6663af95e5079\",\n+                \"sha256:ffc519621dce0c767e96b9c53f09c5d215578e10b02c285809f76509a3931482\"\n             ],\n             \"markers\": \"python_full_version >= '3.7.0'\",\n-            \"version\": \"==3.3.2\"\n+            \"version\": \"==3.4.0\"\n         },\n         \"click\": {\n             \"hashes\": [\n@@ -175,11 +190,11 @@\n         },\n         \"deprecated\": {\n             \"hashes\": [\n-                \"sha256:6fac8b097794a90302bdbb17b9b815e732d3c4720583ff1b198499d78470466c\",\n-                \"sha256:e5323eb936458dccc2582dc6f9c322c852a775a27065ff2b0c4970b9d53d01b3\"\n+                \"sha256:353bc4a8ac4bfc96800ddab349d89c25dec1079f65fd53acdcc1e0b975b21320\",\n+                \"sha256:683e561a90de76239796e6b6feac66b99030d2dd3fcf61ef996330f14bbb9b0d\"\n             ],\n             \"markers\": \"python_version >= '2.7' and python_version not in '3.0, 3.1, 3.2, 3.3'\",\n-            \"version\": \"==1.2.14\"\n+            \"version\": \"==1.2.15\"\n         },\n         \"exceptiongroup\": {\n             \"hashes\": [\n@@ -191,10 +206,10 @@\n         },\n         \"face\": {\n             \"hashes\": [\n-                \"sha256:344fe31562d0f6f444a45982418f3793d4b14f9abb98ccca1509d22e0a3e7e35\",\n-                \"sha256:d5d692f90bc8f5987b636e47e36384b9bbda499aaf0a77aa0b0bbe834c76923d\"\n+                \"sha256:0e2c17b426fa4639a4e77d1de9580f74a98f4869ba4c7c8c175b810611622cd3\",\n+                \"sha256:611e29a01ac5970f0077f9c577e746d48c082588b411b33a0dd55c4d872949f6\"\n             ],\n-            \"version\": \"==22.0.0\"\n+            \"version\": \"==24.0.0\"\n         },\n         \"glom\": {\n             \"hashes\": [\n@@ -205,19 +220,19 @@\n         },\n         \"googleapis-common-protos\": {\n             \"hashes\": [\n-                \"sha256:2972e6c496f435b92590fd54045060867f3fe9be2c82ab148fc8885035479a63\",\n-                \"sha256:334a29d07cddc3aa01dee4988f9afd9b2916ee2ff49d6b757155dc0d197852c0\"\n+                \"sha256:c3e7b33d15fdca5374cc0a7346dd92ffa847425cc4ea941d970f13680052ec8c\",\n+                \"sha256:d7abcd75fabb2e0ec9f74466401f6c119a0b498e27370e9be4c94cb7e382b8ed\"\n             ],\n             \"markers\": \"python_version >= '3.7'\",\n-            \"version\": \"==1.65.0\"\n+            \"version\": \"==1.66.0\"\n         },\n         \"idna\": {\n             \"hashes\": [\n-                \"sha256:050b4e5baadcd44d760cedbd2b8e639f2ff89bbc7a5730fcc662954303377aac\",\n-                \"sha256:d838c2c0ed6fced7693d5e8ab8e734d5f8fda53a039c0164afb0b82e771e3603\"\n+                \"sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9\",\n+                \"sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3\"\n             ],\n             \"markers\": \"python_version >= '3.6'\",\n-            \"version\": \"==3.8\"\n+            \"version\": \"==3.10\"\n         },\n         \"importlib-metadata\": {\n             \"hashes\": [\n@@ -330,34 +345,34 @@\n         },\n         \"packaging\": {\n             \"hashes\": [\n-                \"sha256:026ed72c8ed3fcce5bf8950572258698927fd1dbda10a5e981cdf0ac37f4f002\",\n-                \"sha256:5b8f2217dbdbd2f7f384c41c628544e6d52f2d0f53c6d0c3ea61aa5d1d7ff124\"\n+                \"sha256:09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759\",\n+                \"sha256:c228a6dc5e932d346bc5739379109d49e8853dd8223571c7c5b55260edc0b97f\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==24.1\"\n+            \"version\": \"==24.2\"\n         },\n         \"peewee\": {\n             \"hashes\": [\n-                \"sha256:cea5592c6f4da1592b7cff8eaf655be6648a1f5857469e30037bf920c03fb8fb\"\n+                \"sha256:6aefc700bd530fc6ac23fa19c9c5b47041751d92985b799169c8e318e97eabaa\"\n             ],\n-            \"version\": \"==3.17.6\"\n+            \"version\": \"==3.17.7\"\n         },\n         \"protobuf\": {\n             \"hashes\": [\n-                \"sha256:051e97ce9fa6067a4546e75cb14f90cf0232dcb3e3d508c448b8d0e4265b61c1\",\n-                \"sha256:0dc4a62cc4052a036ee2204d26fe4d835c62827c855c8a03f29fe6da146b380d\",\n-                \"sha256:3319e073562e2515c6ddc643eb92ce20809f5d8f10fead3332f71c63be6a7040\",\n-                \"sha256:4c8a70fdcb995dcf6c8966cfa3a29101916f7225e9afe3ced4395359955d3835\",\n-                \"sha256:7e372cbbda66a63ebca18f8ffaa6948455dfecc4e9c1029312f6c2edcd86c4e1\",\n-                \"sha256:90bf6fd378494eb698805bbbe7afe6c5d12c8e17fca817a646cd6a1818c696ca\",\n-                \"sha256:ac79a48d6b99dfed2729ccccee547b34a1d3d63289c71cef056653a846a2240f\",\n-                \"sha256:ba3d8504116a921af46499471c63a85260c1a5fc23333154a427a310e015d26d\",\n-                \"sha256:bfbebc1c8e4793cfd58589acfb8a1026be0003e852b9da7db5a4285bde996978\",\n-                \"sha256:db9fd45183e1a67722cafa5c1da3e85c6492a5383f127c86c4c4aa4845867dc4\",\n-                \"sha256:eecd41bfc0e4b1bd3fa7909ed93dd14dd5567b98c941d6c1ad08fdcab3d6884b\"\n+                \"sha256:0aebecb809cae990f8129ada5ca273d9d670b76d9bfc9b1809f0a9c02b7dbf41\",\n+                \"sha256:4be0571adcbe712b282a330c6e89eae24281344429ae95c6d85e79e84780f5ea\",\n+                \"sha256:5e61fd921603f58d2f5acb2806a929b4675f8874ff5f330b7d6f7e2e784bbcd8\",\n+                \"sha256:7a183f592dc80aa7c8da7ad9e55091c4ffc9497b3054452d629bb85fa27c2a45\",\n+                \"sha256:7f8249476b4a9473645db7f8ab42b02fe1488cbe5fb72fddd445e0665afd8584\",\n+                \"sha256:919ad92d9b0310070f8356c24b855c98df2b8bd207ebc1c0c6fcc9ab1e007f3d\",\n+                \"sha256:98d8d8aa50de6a2747efd9cceba361c9034050ecce3e09136f90de37ddba66e1\",\n+                \"sha256:abe32aad8561aa7cc94fc7ba4fdef646e576983edb94a73381b03c53728a626f\",\n+                \"sha256:b0234dd5a03049e4ddd94b93400b67803c823cfc405689688f59b34e0742381a\",\n+                \"sha256:b2fde3d805354df675ea4c7c6338c1aecd254dfc9925e88c6d31a2bcb97eb173\",\n+                \"sha256:fe14e16c22be926d3abfcb500e60cab068baf10b542b8c858fa27e098123e331\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==4.25.4\"\n+            \"version\": \"==4.25.5\"\n         },\n         \"pygments\": {\n             \"hashes\": [\n@@ -423,67 +438,58 @@\n         },\n         \"ruamel.yaml\": {\n             \"hashes\": [\n-                \"sha256:6024b986f06765d482b5b07e086cc4b4cd05dd22ddcbc758fa23d54873cf313d\",\n-                \"sha256:b16b6c3816dff0a93dca12acf5e70afd089fa5acb80604afd1ffa8b465b7722c\"\n+                \"sha256:57b53ba33def16c4f3d807c0ccbc00f8a6081827e81ba2491691b76882d0c636\",\n+                \"sha256:8b27e6a217e786c6fbe5634d8f3f11bc63e0f80f6a5890f28863d9c45aac311b\"\n             ],\n-            \"markers\": \"python_version >= '3'\",\n-            \"version\": \"==0.17.40\"\n+            \"markers\": \"python_version >= '3.7'\",\n+            \"version\": \"==0.18.6\"\n         },\n         \"ruamel.yaml.clib\": {\n             \"hashes\": [\n-                \"sha256:024cfe1fc7c7f4e1aff4a81e718109e13409767e4f871443cbff3dba3578203d\",\n-                \"sha256:03d1162b6d1df1caa3a4bd27aa51ce17c9afc2046c31b0ad60a0a96ec22f8001\",\n-                \"sha256:07238db9cbdf8fc1e9de2489a4f68474e70dffcb32232db7c08fa61ca0c7c462\",\n-                \"sha256:09b055c05697b38ecacb7ac50bdab2240bfca1a0c4872b0fd309bb07dc9aa3a9\",\n-                \"sha256:1707814f0d9791df063f8c19bb51b0d1278b8e9a2353abbb676c2f685dee6afe\",\n-                \"sha256:1758ce7d8e1a29d23de54a16ae867abd370f01b5a69e1a3ba75223eaa3ca1a1b\",\n-                \"sha256:184565012b60405d93838167f425713180b949e9d8dd0bbc7b49f074407c5a8b\",\n-                \"sha256:1b617618914cb00bf5c34d4357c37aa15183fa229b24767259657746c9077615\",\n-                \"sha256:1dc67314e7e1086c9fdf2680b7b6c2be1c0d8e3a8279f2e993ca2a7545fecf62\",\n-                \"sha256:25ac8c08322002b06fa1d49d1646181f0b2c72f5cbc15a85e80b4c30a544bb15\",\n-                \"sha256:25c515e350e5b739842fc3228d662413ef28f295791af5e5110b543cf0b57d9b\",\n-                \"sha256:305889baa4043a09e5b76f8e2a51d4ffba44259f6b4c72dec8ca56207d9c6fe1\",\n-                \"sha256:3213ece08ea033eb159ac52ae052a4899b56ecc124bb80020d9bbceeb50258e9\",\n-                \"sha256:3f215c5daf6a9d7bbed4a0a4f760f3113b10e82ff4c5c44bec20a68c8014f675\",\n-                \"sha256:46d378daaac94f454b3a0e3d8d78cafd78a026b1d71443f4966c696b48a6d899\",\n-                \"sha256:4ecbf9c3e19f9562c7fdd462e8d18dd902a47ca046a2e64dba80699f0b6c09b7\",\n-                \"sha256:53a300ed9cea38cf5a2a9b069058137c2ca1ce658a874b79baceb8f892f915a7\",\n-                \"sha256:56f4252222c067b4ce51ae12cbac231bce32aee1d33fbfc9d17e5b8d6966c312\",\n-                \"sha256:5c365d91c88390c8d0a8545df0b5857172824b1c604e867161e6b3d59a827eaa\",\n-                \"sha256:700e4ebb569e59e16a976857c8798aee258dceac7c7d6b50cab63e080058df91\",\n-                \"sha256:75e1ed13e1f9de23c5607fe6bd1aeaae21e523b32d83bb33918245361e9cc51b\",\n-                \"sha256:77159f5d5b5c14f7c34073862a6b7d34944075d9f93e681638f6d753606c6ce6\",\n-                \"sha256:7f67a1ee819dc4562d444bbafb135832b0b909f81cc90f7aa00260968c9ca1b3\",\n-                \"sha256:840f0c7f194986a63d2c2465ca63af8ccbbc90ab1c6001b1978f05119b5e7334\",\n-                \"sha256:84b554931e932c46f94ab306913ad7e11bba988104c5cff26d90d03f68258cd5\",\n-                \"sha256:87ea5ff66d8064301a154b3933ae406b0863402a799b16e4a1d24d9fbbcbe0d3\",\n-                \"sha256:955eae71ac26c1ab35924203fda6220f84dce57d6d7884f189743e2abe3a9fbe\",\n-                \"sha256:a1a45e0bb052edf6a1d3a93baef85319733a888363938e1fc9924cb00c8df24c\",\n-                \"sha256:a5aa27bad2bb83670b71683aae140a1f52b0857a2deff56ad3f6c13a017a26ed\",\n-                \"sha256:a6a9ffd280b71ad062eae53ac1659ad86a17f59a0fdc7699fd9be40525153337\",\n-                \"sha256:a75879bacf2c987c003368cf14bed0ffe99e8e85acfa6c0bfffc21a090f16880\",\n-                \"sha256:aa2267c6a303eb483de8d02db2871afb5c5fc15618d894300b88958f729ad74f\",\n-                \"sha256:aab7fd643f71d7946f2ee58cc88c9b7bfc97debd71dcc93e03e2d174628e7e2d\",\n-                \"sha256:b16420e621d26fdfa949a8b4b47ade8810c56002f5389970db4ddda51dbff248\",\n-                \"sha256:b42169467c42b692c19cf539c38d4602069d8c1505e97b86387fcf7afb766e1d\",\n-                \"sha256:bba64af9fa9cebe325a62fa398760f5c7206b215201b0ec825005f1b18b9bccf\",\n-                \"sha256:beb2e0404003de9a4cab9753a8805a8fe9320ee6673136ed7f04255fe60bb512\",\n-                \"sha256:bef08cd86169d9eafb3ccb0a39edb11d8e25f3dae2b28f5c52fd997521133069\",\n-                \"sha256:c2a72e9109ea74e511e29032f3b670835f8a59bbdc9ce692c5b4ed91ccf1eedb\",\n-                \"sha256:c58ecd827313af6864893e7af0a3bb85fd529f862b6adbefe14643947cfe2942\",\n-                \"sha256:c69212f63169ec1cfc9bb44723bf2917cbbd8f6191a00ef3410f5a7fe300722d\",\n-                \"sha256:cabddb8d8ead485e255fe80429f833172b4cadf99274db39abc080e068cbcc31\",\n-                \"sha256:d176b57452ab5b7028ac47e7b3cf644bcfdc8cacfecf7e71759f7f51a59e5c92\",\n-                \"sha256:da09ad1c359a728e112d60116f626cc9f29730ff3e0e7db72b9a2dbc2e4beed5\",\n-                \"sha256:e2b4c44b60eadec492926a7270abb100ef9f72798e18743939bdbf037aab8c28\",\n-                \"sha256:e79e5db08739731b0ce4850bed599235d601701d5694c36570a99a0c5ca41a9d\",\n-                \"sha256:ebc06178e8821efc9692ea7544aa5644217358490145629914d8020042c24aa1\",\n-                \"sha256:edaef1c1200c4b4cb914583150dcaa3bc30e592e907c01117c08b13a07255ec2\",\n-                \"sha256:f481f16baec5290e45aebdc2a5168ebc6d35189ae6fea7a58787613a25f6e875\",\n-                \"sha256:fff3573c2db359f091e1589c3d7c5fc2f86f5bdb6f24252c2d8e539d4e45f412\"\n+                \"sha256:040ae85536960525ea62868b642bdb0c2cc6021c9f9d507810c0c604e66f5a7b\",\n+                \"sha256:0467c5965282c62203273b838ae77c0d29d7638c8a4e3a1c8bdd3602c10904e4\",\n+                \"sha256:0b7e75b4965e1d4690e93021adfcecccbca7d61c7bddd8e22406ef2ff20d74ef\",\n+                \"sha256:11f891336688faf5156a36293a9c362bdc7c88f03a8a027c2c1d8e0bcde998e5\",\n+                \"sha256:20b0f8dc160ba83b6dcc0e256846e1a02d044e13f7ea74a3d1d56ede4e48c632\",\n+                \"sha256:22353049ba4181685023b25b5b51a574bce33e7f51c759371a7422dcae5402a6\",\n+                \"sha256:32621c177bbf782ca5a18ba4d7af0f1082a3f6e517ac2a18b3974d4edf349680\",\n+                \"sha256:3bc2a80e6420ca8b7d3590791e2dfc709c88ab9152c00eeb511c9875ce5778bf\",\n+                \"sha256:3eac5a91891ceb88138c113f9db04f3cebdae277f5d44eaa3651a4f573e6a5da\",\n+                \"sha256:4a6679521a58256a90b0d89e03992c15144c5f3858f40d7c18886023d7943db6\",\n+                \"sha256:4c8c5d82f50bb53986a5e02d1b3092b03622c02c2eb78e29bec33fd9593bae1a\",\n+                \"sha256:5a0e060aace4c24dcaf71023bbd7d42674e3b230f7e7b97317baf1e953e5b519\",\n+                \"sha256:6442cb36270b3afb1b4951f060eccca1ce49f3d087ca1ca4563a6eb479cb3de6\",\n+                \"sha256:6c8fbb13ec503f99a91901ab46e0b07ae7941cd527393187039aec586fdfd36f\",\n+                \"sha256:749c16fcc4a2b09f28843cda5a193e0283e47454b63ec4b81eaa2242f50e4ccd\",\n+                \"sha256:7dd5adc8b930b12c8fc5b99e2d535a09889941aa0d0bd06f4749e9a9397c71d2\",\n+                \"sha256:811ea1594b8a0fb466172c384267a4e5e367298af6b228931f273b111f17ef52\",\n+                \"sha256:932205970b9f9991b34f55136be327501903f7c66830e9760a8ffb15b07f05cd\",\n+                \"sha256:943f32bc9dedb3abff9879edc134901df92cfce2c3d5c9348f172f62eb2d771d\",\n+                \"sha256:95c3829bb364fdb8e0332c9931ecf57d9be3519241323c5274bd82f709cebc0c\",\n+                \"sha256:96777d473c05ee3e5e3c3e999f5d23c6f4ec5b0c38c098b3a5229085f74236c6\",\n+                \"sha256:a274fb2cb086c7a3dea4322ec27f4cb5cc4b6298adb583ab0e211a4682f241eb\",\n+                \"sha256:a606ef75a60ecf3d924613892cc603b154178ee25abb3055db5062da811fd969\",\n+                \"sha256:ab007f2f5a87bd08ab1499bdf96f3d5c6ad4dcfa364884cb4549aa0154b13a28\",\n+                \"sha256:bb43a269eb827806502c7c8efb7ae7e9e9d0573257a46e8e952f4d4caba4f31e\",\n+                \"sha256:bc5f1e1c28e966d61d2519f2a3d451ba989f9ea0f2307de7bc45baa526de9e45\",\n+                \"sha256:bd0a08f0bab19093c54e18a14a10b4322e1eacc5217056f3c063bd2f59853ce4\",\n+                \"sha256:beffaed67936fbbeffd10966a4eb53c402fafd3d6833770516bf7314bc6ffa12\",\n+                \"sha256:bf165fef1f223beae7333275156ab2022cffe255dcc51c27f066b4370da81e31\",\n+                \"sha256:cf12567a7b565cbf65d438dec6cfbe2917d3c1bdddfce84a9930b7d35ea59642\",\n+                \"sha256:d84318609196d6bd6da0edfa25cedfbabd8dbde5140a0a23af29ad4b8f91fb1e\",\n+                \"sha256:d85252669dc32f98ebcd5d36768f5d4faeaeaa2d655ac0473be490ecdae3c285\",\n+                \"sha256:e143ada795c341b56de9418c58d028989093ee611aa27ffb9b7f609c00d813ed\",\n+                \"sha256:e188d2699864c11c36cdfdada94d781fd5d6b0071cd9c427bceb08ad3d7c70e1\",\n+                \"sha256:e2f1c3765db32be59d18ab3953f43ab62a761327aafc1594a2a1fbe038b8b8a7\",\n+                \"sha256:e5b8daf27af0b90da7bb903a876477a9e6d7270be6146906b276605997c7e9a3\",\n+                \"sha256:e7e3736715fbf53e9be2a79eb4db68e4ed857017344d697e8b9749444ae57475\",\n+                \"sha256:e8c4ebfcfd57177b572e2040777b8abc537cdef58a2120e830124946aa9b42c5\",\n+                \"sha256:f66efbc1caa63c088dead1c4170d148eabc9b80d95fb75b6c92ac0aad2437d76\",\n+                \"sha256:fc4b630cd3fa2cf7fce38afa91d7cfe844a9f75d7f0f36393fa98815e911d987\",\n+                \"sha256:fd5415dded15c3822597455bc02bcd66e81ef8b7a48cb71a33628fc9fdde39df\"\n             ],\n             \"markers\": \"python_version < '3.13' and platform_python_implementation == 'CPython'\",\n-            \"version\": \"==0.2.8\"\n+            \"version\": \"==0.2.12\"\n         },\n         \"semgrep\": {\n             \"editable\": true,\n@@ -492,19 +498,19 @@\n         },\n         \"setuptools\": {\n             \"hashes\": [\n-                \"sha256:5f4c08aa4d3ebcb57a50c33b1b07e94315d7fc7230f7115e47fc99776c8ce308\",\n-                \"sha256:95b40ed940a1c67eb70fc099094bd6e99c6ee7c23aa2306f4d2697ba7916f9c6\"\n+                \"sha256:8199222558df7c86216af4f84c30e9b34a61d8ba19366cc914424cdbd28252f6\",\n+                \"sha256:ce74b49e8f7110f9bf04883b730f4765b774ef3ef28f722cce7c273d253aaf7d\"\n             ],\n-            \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==74.1.2\"\n+            \"markers\": \"python_version >= '3.9'\",\n+            \"version\": \"==75.6.0\"\n         },\n         \"tomli\": {\n             \"hashes\": [\n-                \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\",\n-                \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"\n+                \"sha256:2ebe24485c53d303f690b0ec092806a085f07af5a5aa1464f3931eec36caaa38\",\n+                \"sha256:d46d457a85337051c36524bc5349dd91b1877838e2979ac5ced3e710ed8a60ed\"\n             ],\n-            \"markers\": \"python_version >= '3.7'\",\n-            \"version\": \"==2.0.1\"\n+            \"markers\": \"python_version >= '3.8'\",\n+            \"version\": \"==2.0.2\"\n         },\n         \"typing-extensions\": {\n             \"hashes\": [\n@@ -516,11 +522,11 @@\n         },\n         \"urllib3\": {\n             \"hashes\": [\n-                \"sha256:a448b2f64d686155468037e1ace9f2d2199776e17f0a46610480d311f73e3472\",\n-                \"sha256:dd505485549a7a552833da5e6063639d0d177c04f23bc3864e41e5dc5f612168\"\n+                \"sha256:ca899ca043dcb1bafa3e262d73aa25c465bfb49e0bd9dd5d59f1d0acba2f8fac\",\n+                \"sha256:e7d814a81dad81e6caf2ec9fdedb284ecc9c73076b62654547cc64ccdcae26e9\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==2.2.2\"\n+            \"version\": \"==2.2.3\"\n         },\n         \"wcmatch\": {\n             \"hashes\": [\n@@ -532,87 +538,82 @@\n         },\n         \"wrapt\": {\n             \"hashes\": [\n-                \"sha256:0d2691979e93d06a95a26257adb7bfd0c93818e89b1406f5a28f36e0d8c1e1fc\",\n-                \"sha256:14d7dc606219cdd7405133c713f2c218d4252f2a469003f8c46bb92d5d095d81\",\n-                \"sha256:1a5db485fe2de4403f13fafdc231b0dbae5eca4359232d2efc79025527375b09\",\n-                \"sha256:1acd723ee2a8826f3d53910255643e33673e1d11db84ce5880675954183ec47e\",\n-                \"sha256:1ca9b6085e4f866bd584fb135a041bfc32cab916e69f714a7d1d397f8c4891ca\",\n-                \"sha256:1dd50a2696ff89f57bd8847647a1c363b687d3d796dc30d4dd4a9d1689a706f0\",\n-                \"sha256:2076fad65c6736184e77d7d4729b63a6d1ae0b70da4868adeec40989858eb3fb\",\n-                \"sha256:2a88e6010048489cda82b1326889ec075a8c856c2e6a256072b28eaee3ccf487\",\n-                \"sha256:3ebf019be5c09d400cf7b024aa52b1f3aeebeff51550d007e92c3c1c4afc2a40\",\n-                \"sha256:418abb18146475c310d7a6dc71143d6f7adec5b004ac9ce08dc7a34e2babdc5c\",\n-                \"sha256:43aa59eadec7890d9958748db829df269f0368521ba6dc68cc172d5d03ed8060\",\n-                \"sha256:44a2754372e32ab315734c6c73b24351d06e77ffff6ae27d2ecf14cf3d229202\",\n-                \"sha256:490b0ee15c1a55be9c1bd8609b8cecd60e325f0575fc98f50058eae366e01f41\",\n-                \"sha256:49aac49dc4782cb04f58986e81ea0b4768e4ff197b57324dcbd7699c5dfb40b9\",\n-                \"sha256:5eb404d89131ec9b4f748fa5cfb5346802e5ee8836f57d516576e61f304f3b7b\",\n-                \"sha256:5f15814a33e42b04e3de432e573aa557f9f0f56458745c2074952f564c50e664\",\n-                \"sha256:5f370f952971e7d17c7d1ead40e49f32345a7f7a5373571ef44d800d06b1899d\",\n-                \"sha256:66027d667efe95cc4fa945af59f92c5a02c6f5bb6012bff9e60542c74c75c362\",\n-                \"sha256:66dfbaa7cfa3eb707bbfcd46dab2bc6207b005cbc9caa2199bcbc81d95071a00\",\n-                \"sha256:685f568fa5e627e93f3b52fda002c7ed2fa1800b50ce51f6ed1d572d8ab3e7fc\",\n-                \"sha256:6906c4100a8fcbf2fa735f6059214bb13b97f75b1a61777fcf6432121ef12ef1\",\n-                \"sha256:6a42cd0cfa8ffc1915aef79cb4284f6383d8a3e9dcca70c445dcfdd639d51267\",\n-                \"sha256:6dcfcffe73710be01d90cae08c3e548d90932d37b39ef83969ae135d36ef3956\",\n-                \"sha256:6f6eac2360f2d543cc875a0e5efd413b6cbd483cb3ad7ebf888884a6e0d2e966\",\n-                \"sha256:72554a23c78a8e7aa02abbd699d129eead8b147a23c56e08d08dfc29cfdddca1\",\n-                \"sha256:73870c364c11f03ed072dda68ff7aea6d2a3a5c3fe250d917a429c7432e15228\",\n-                \"sha256:73aa7d98215d39b8455f103de64391cb79dfcad601701a3aa0dddacf74911d72\",\n-                \"sha256:75ea7d0ee2a15733684badb16de6794894ed9c55aa5e9903260922f0482e687d\",\n-                \"sha256:7bd2d7ff69a2cac767fbf7a2b206add2e9a210e57947dd7ce03e25d03d2de292\",\n-                \"sha256:807cc8543a477ab7422f1120a217054f958a66ef7314f76dd9e77d3f02cdccd0\",\n-                \"sha256:8e9723528b9f787dc59168369e42ae1c3b0d3fadb2f1a71de14531d321ee05b0\",\n-                \"sha256:9090c9e676d5236a6948330e83cb89969f433b1943a558968f659ead07cb3b36\",\n-                \"sha256:9153ed35fc5e4fa3b2fe97bddaa7cbec0ed22412b85bcdaf54aeba92ea37428c\",\n-                \"sha256:9159485323798c8dc530a224bd3ffcf76659319ccc7bbd52e01e73bd0241a0c5\",\n-                \"sha256:941988b89b4fd6b41c3f0bfb20e92bd23746579736b7343283297c4c8cbae68f\",\n-                \"sha256:94265b00870aa407bd0cbcfd536f17ecde43b94fb8d228560a1e9d3041462d73\",\n-                \"sha256:98b5e1f498a8ca1858a1cdbffb023bfd954da4e3fa2c0cb5853d40014557248b\",\n-                \"sha256:9b201ae332c3637a42f02d1045e1d0cccfdc41f1f2f801dafbaa7e9b4797bfc2\",\n-                \"sha256:a0ea261ce52b5952bf669684a251a66df239ec6d441ccb59ec7afa882265d593\",\n-                \"sha256:a33a747400b94b6d6b8a165e4480264a64a78c8a4c734b62136062e9a248dd39\",\n-                \"sha256:a452f9ca3e3267cd4d0fcf2edd0d035b1934ac2bd7e0e57ac91ad6b95c0c6389\",\n-                \"sha256:a86373cf37cd7764f2201b76496aba58a52e76dedfaa698ef9e9688bfd9e41cf\",\n-                \"sha256:ac83a914ebaf589b69f7d0a1277602ff494e21f4c2f743313414378f8f50a4cf\",\n-                \"sha256:aefbc4cb0a54f91af643660a0a150ce2c090d3652cf4052a5397fb2de549cd89\",\n-                \"sha256:b3646eefa23daeba62643a58aac816945cadc0afaf21800a1421eeba5f6cfb9c\",\n-                \"sha256:b47cfad9e9bbbed2339081f4e346c93ecd7ab504299403320bf85f7f85c7d46c\",\n-                \"sha256:b935ae30c6e7400022b50f8d359c03ed233d45b725cfdd299462f41ee5ffba6f\",\n-                \"sha256:bb2dee3874a500de01c93d5c71415fcaef1d858370d405824783e7a8ef5db440\",\n-                \"sha256:bc57efac2da352a51cc4658878a68d2b1b67dbe9d33c36cb826ca449d80a8465\",\n-                \"sha256:bf5703fdeb350e36885f2875d853ce13172ae281c56e509f4e6eca049bdfb136\",\n-                \"sha256:c31f72b1b6624c9d863fc095da460802f43a7c6868c5dda140f51da24fd47d7b\",\n-                \"sha256:c5cd603b575ebceca7da5a3a251e69561bec509e0b46e4993e1cac402b7247b8\",\n-                \"sha256:d2efee35b4b0a347e0d99d28e884dfd82797852d62fcd7ebdeee26f3ceb72cf3\",\n-                \"sha256:d462f28826f4657968ae51d2181a074dfe03c200d6131690b7d65d55b0f360f8\",\n-                \"sha256:d5e49454f19ef621089e204f862388d29e6e8d8b162efce05208913dde5b9ad6\",\n-                \"sha256:da4813f751142436b075ed7aa012a8778aa43a99f7b36afe9b742d3ed8bdc95e\",\n-                \"sha256:db2e408d983b0e61e238cf579c09ef7020560441906ca990fe8412153e3b291f\",\n-                \"sha256:db98ad84a55eb09b3c32a96c576476777e87c520a34e2519d3e59c44710c002c\",\n-                \"sha256:dbed418ba5c3dce92619656802cc5355cb679e58d0d89b50f116e4a9d5a9603e\",\n-                \"sha256:dcdba5c86e368442528f7060039eda390cc4091bfd1dca41e8046af7c910dda8\",\n-                \"sha256:decbfa2f618fa8ed81c95ee18a387ff973143c656ef800c9f24fb7e9c16054e2\",\n-                \"sha256:e4fdb9275308292e880dcbeb12546df7f3e0f96c6b41197e0cf37d2826359020\",\n-                \"sha256:eb1b046be06b0fce7249f1d025cd359b4b80fc1c3e24ad9eca33e0dcdb2e4a35\",\n-                \"sha256:eb6e651000a19c96f452c85132811d25e9264d836951022d6e81df2fff38337d\",\n-                \"sha256:ed867c42c268f876097248e05b6117a65bcd1e63b779e916fe2e33cd6fd0d3c3\",\n-                \"sha256:edfad1d29c73f9b863ebe7082ae9321374ccb10879eeabc84ba3b69f2579d537\",\n-                \"sha256:f2058f813d4f2b5e3a9eb2eb3faf8f1d99b81c3e51aeda4b168406443e8ba809\",\n-                \"sha256:f6b2d0c6703c988d334f297aa5df18c45e97b0af3679bb75059e0e0bd8b1069d\",\n-                \"sha256:f8212564d49c50eb4565e502814f694e240c55551a5f1bc841d4fcaabb0a9b8a\",\n-                \"sha256:ffa565331890b90056c01db69c0fe634a776f8019c143a5ae265f9c6bc4bd6d4\"\n+                \"sha256:0229b247b0fc7dee0d36176cbb79dbaf2a9eb7ecc50ec3121f40ef443155fb1d\",\n+                \"sha256:0698d3a86f68abc894d537887b9bbf84d29bcfbc759e23f4644be27acf6da301\",\n+                \"sha256:0a0a1a1ec28b641f2a3a2c35cbe86c00051c04fffcfcc577ffcdd707df3f8635\",\n+                \"sha256:0b48554952f0f387984da81ccfa73b62e52817a4386d070c75e4db7d43a28c4a\",\n+                \"sha256:0f2a28eb35cf99d5f5bd12f5dd44a0f41d206db226535b37b0c60e9da162c3ed\",\n+                \"sha256:140ea00c87fafc42739bd74a94a5a9003f8e72c27c47cd4f61d8e05e6dec8721\",\n+                \"sha256:16187aa2317c731170a88ef35e8937ae0f533c402872c1ee5e6d079fcf320801\",\n+                \"sha256:17fcf043d0b4724858f25b8826c36e08f9fb2e475410bece0ec44a22d533da9b\",\n+                \"sha256:18b956061b8db634120b58f668592a772e87e2e78bc1f6a906cfcaa0cc7991c1\",\n+                \"sha256:2399408ac33ffd5b200480ee858baa58d77dd30e0dd0cab6a8a9547135f30a88\",\n+                \"sha256:2a0c23b8319848426f305f9cb0c98a6e32ee68a36264f45948ccf8e7d2b941f8\",\n+                \"sha256:2dfb7cff84e72e7bf975b06b4989477873dcf160b2fd89959c629535df53d4e0\",\n+                \"sha256:2f495b6754358979379f84534f8dd7a43ff8cff2558dcdea4a148a6e713a758f\",\n+                \"sha256:33539c6f5b96cf0b1105a0ff4cf5db9332e773bb521cc804a90e58dc49b10578\",\n+                \"sha256:3c34f6896a01b84bab196f7119770fd8466c8ae3dfa73c59c0bb281e7b588ce7\",\n+                \"sha256:498fec8da10e3e62edd1e7368f4b24aa362ac0ad931e678332d1b209aec93045\",\n+                \"sha256:4d63f4d446e10ad19ed01188d6c1e1bb134cde8c18b0aa2acfd973d41fcc5ada\",\n+                \"sha256:4e4b4385363de9052dac1a67bfb535c376f3d19c238b5f36bddc95efae15e12d\",\n+                \"sha256:4e547b447073fc0dbfcbff15154c1be8823d10dab4ad401bdb1575e3fdedff1b\",\n+                \"sha256:4f643df3d4419ea3f856c5c3f40fec1d65ea2e89ec812c83f7767c8730f9827a\",\n+                \"sha256:4f763a29ee6a20c529496a20a7bcb16a73de27f5da6a843249c7047daf135977\",\n+                \"sha256:5ae271862b2142f4bc687bdbfcc942e2473a89999a54231aa1c2c676e28f29ea\",\n+                \"sha256:5d8fd17635b262448ab8f99230fe4dac991af1dabdbb92f7a70a6afac8a7e346\",\n+                \"sha256:69c40d4655e078ede067a7095544bcec5a963566e17503e75a3a3e0fe2803b13\",\n+                \"sha256:69d093792dc34a9c4c8a70e4973a3361c7a7578e9cd86961b2bbf38ca71e4e22\",\n+                \"sha256:6a9653131bda68a1f029c52157fd81e11f07d485df55410401f745007bd6d339\",\n+                \"sha256:6ff02a91c4fc9b6a94e1c9c20f62ea06a7e375f42fe57587f004d1078ac86ca9\",\n+                \"sha256:714c12485aa52efbc0fc0ade1e9ab3a70343db82627f90f2ecbc898fdf0bb181\",\n+                \"sha256:7264cbb4a18dc4acfd73b63e4bcfec9c9802614572025bdd44d0721983fc1d9c\",\n+                \"sha256:73a96fd11d2b2e77d623a7f26e004cc31f131a365add1ce1ce9a19e55a1eef90\",\n+                \"sha256:74bf625b1b4caaa7bad51d9003f8b07a468a704e0644a700e936c357c17dd45a\",\n+                \"sha256:81b1289e99cf4bad07c23393ab447e5e96db0ab50974a280f7954b071d41b489\",\n+                \"sha256:8425cfce27b8b20c9b89d77fb50e368d8306a90bf2b6eef2cdf5cd5083adf83f\",\n+                \"sha256:875d240fdbdbe9e11f9831901fb8719da0bd4e6131f83aa9f69b96d18fae7504\",\n+                \"sha256:879591c2b5ab0a7184258274c42a126b74a2c3d5a329df16d69f9cee07bba6ea\",\n+                \"sha256:89fc28495896097622c3fc238915c79365dd0ede02f9a82ce436b13bd0ab7569\",\n+                \"sha256:8a5e7cc39a45fc430af1aefc4d77ee6bad72c5bcdb1322cfde852c15192b8bd4\",\n+                \"sha256:8f8909cdb9f1b237786c09a810e24ee5e15ef17019f7cecb207ce205b9b5fcce\",\n+                \"sha256:914f66f3b6fc7b915d46c1cc424bc2441841083de01b90f9e81109c9759e43ab\",\n+                \"sha256:92a3d214d5e53cb1db8b015f30d544bc9d3f7179a05feb8f16df713cecc2620a\",\n+                \"sha256:948a9bd0fb2c5120457b07e59c8d7210cbc8703243225dbd78f4dfc13c8d2d1f\",\n+                \"sha256:9c900108df470060174108012de06d45f514aa4ec21a191e7ab42988ff42a86c\",\n+                \"sha256:9f2939cd4a2a52ca32bc0b359015718472d7f6de870760342e7ba295be9ebaf9\",\n+                \"sha256:a4192b45dff127c7d69b3bdfb4d3e47b64179a0b9900b6351859f3001397dabf\",\n+                \"sha256:a8fc931382e56627ec4acb01e09ce66e5c03c384ca52606111cee50d931a342d\",\n+                \"sha256:ad47b095f0bdc5585bced35bd088cbfe4177236c7df9984b3cc46b391cc60627\",\n+                \"sha256:b1ca5f060e205f72bec57faae5bd817a1560fcfc4af03f414b08fa29106b7e2d\",\n+                \"sha256:ba1739fb38441a27a676f4de4123d3e858e494fac05868b7a281c0a383c098f4\",\n+                \"sha256:baa7ef4e0886a6f482e00d1d5bcd37c201b383f1d314643dfb0367169f94f04c\",\n+                \"sha256:bb90765dd91aed05b53cd7a87bd7f5c188fcd95960914bae0d32c5e7f899719d\",\n+                \"sha256:bc7f729a72b16ee21795a943f85c6244971724819819a41ddbaeb691b2dd85ad\",\n+                \"sha256:bdf62d25234290db1837875d4dceb2151e4ea7f9fff2ed41c0fde23ed542eb5b\",\n+                \"sha256:c30970bdee1cad6a8da2044febd824ef6dc4cc0b19e39af3085c763fdec7de33\",\n+                \"sha256:d2c63b93548eda58abf5188e505ffed0229bf675f7c3090f8e36ad55b8cbc371\",\n+                \"sha256:d751300b94e35b6016d4b1e7d0e7bbc3b5e1751e2405ef908316c2a9024008a1\",\n+                \"sha256:da427d311782324a376cacb47c1a4adc43f99fd9d996ffc1b3e8529c4074d393\",\n+                \"sha256:daba396199399ccabafbfc509037ac635a6bc18510ad1add8fd16d4739cdd106\",\n+                \"sha256:e185ec6060e301a7e5f8461c86fb3640a7beb1a0f0208ffde7a65ec4074931df\",\n+                \"sha256:e4a557d97f12813dc5e18dad9fa765ae44ddd56a672bb5de4825527c847d6379\",\n+                \"sha256:e5ed16d95fd142e9c72b6c10b06514ad30e846a0d0917ab406186541fe68b451\",\n+                \"sha256:e711fc1acc7468463bc084d1b68561e40d1eaa135d8c509a65dd534403d83d7b\",\n+                \"sha256:f28b29dc158ca5d6ac396c8e0a2ef45c4e97bb7e65522bfc04c989e6fe814575\",\n+                \"sha256:f335579a1b485c834849e9075191c9898e0731af45705c2ebf70e0cd5d58beed\",\n+                \"sha256:fce6fee67c318fdfb7f285c29a82d84782ae2579c0e1b385b7f36c6e8074fffb\",\n+                \"sha256:fd136bb85f4568fffca995bd3c8d52080b1e5b225dbf1c2b17b66b4c5fa02838\"\n             ],\n-            \"markers\": \"python_version >= '3.6'\",\n-            \"version\": \"==1.16.0\"\n+            \"markers\": \"python_version >= '3.8'\",\n+            \"version\": \"==1.17.0\"\n         },\n         \"zipp\": {\n             \"hashes\": [\n-                \"sha256:9960cd8967c8f85a56f920d5d507274e74f9ff813a0ab8889a5b5be2daf44064\",\n-                \"sha256:c22b14cc4763c5a5b04134207736c107db42e9d3ef2d9779d465f5f1bcba572b\"\n+                \"sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4\",\n+                \"sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\"\n             ],\n-            \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==3.20.1\"\n+            \"markers\": \"python_version >= '3.9'\",\n+            \"version\": \"==3.21.0\"\n         }\n     },\n     \"develop\": {\n@@ -650,99 +651,114 @@\n         },\n         \"charset-normalizer\": {\n             \"hashes\": [\n-                \"sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027\",\n-                \"sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087\",\n-                \"sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786\",\n-                \"sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8\",\n-                \"sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09\",\n-                \"sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185\",\n-                \"sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574\",\n-                \"sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e\",\n-                \"sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519\",\n-                \"sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898\",\n-                \"sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269\",\n-                \"sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3\",\n-                \"sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f\",\n-                \"sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6\",\n-                \"sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8\",\n-                \"sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a\",\n-                \"sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73\",\n-                \"sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc\",\n-                \"sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714\",\n-                \"sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2\",\n-                \"sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc\",\n-                \"sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce\",\n-                \"sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d\",\n-                \"sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e\",\n-                \"sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6\",\n-                \"sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269\",\n-                \"sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96\",\n-                \"sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d\",\n-                \"sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a\",\n-                \"sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4\",\n-                \"sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77\",\n-                \"sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d\",\n-                \"sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0\",\n-                \"sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed\",\n-                \"sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068\",\n-                \"sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac\",\n-                \"sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25\",\n-                \"sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8\",\n-                \"sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab\",\n-                \"sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26\",\n-                \"sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2\",\n-                \"sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db\",\n-                \"sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f\",\n-                \"sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5\",\n-                \"sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99\",\n-                \"sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c\",\n-                \"sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d\",\n-                \"sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811\",\n-                \"sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa\",\n-                \"sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a\",\n-                \"sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03\",\n-                \"sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b\",\n-                \"sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04\",\n-                \"sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c\",\n-                \"sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001\",\n-                \"sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458\",\n-                \"sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389\",\n-                \"sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99\",\n-                \"sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985\",\n-                \"sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537\",\n-                \"sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238\",\n-                \"sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f\",\n-                \"sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d\",\n-                \"sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796\",\n-                \"sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a\",\n-                \"sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143\",\n-                \"sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8\",\n-                \"sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c\",\n-                \"sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5\",\n-                \"sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5\",\n-                \"sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711\",\n-                \"sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4\",\n-                \"sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6\",\n-                \"sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c\",\n-                \"sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7\",\n-                \"sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4\",\n-                \"sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b\",\n-                \"sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae\",\n-                \"sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12\",\n-                \"sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c\",\n-                \"sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae\",\n-                \"sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8\",\n-                \"sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887\",\n-                \"sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b\",\n-                \"sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4\",\n-                \"sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f\",\n-                \"sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5\",\n-                \"sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33\",\n-                \"sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519\",\n-                \"sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561\"\n+                \"sha256:0099d79bdfcf5c1f0c2c72f91516702ebf8b0b8ddd8905f97a8aecf49712c621\",\n+                \"sha256:0713f3adb9d03d49d365b70b84775d0a0d18e4ab08d12bc46baa6132ba78aaf6\",\n+                \"sha256:07afec21bbbbf8a5cc3651aa96b980afe2526e7f048fdfb7f1014d84acc8b6d8\",\n+                \"sha256:0b309d1747110feb25d7ed6b01afdec269c647d382c857ef4663bbe6ad95a912\",\n+                \"sha256:0d99dd8ff461990f12d6e42c7347fd9ab2532fb70e9621ba520f9e8637161d7c\",\n+                \"sha256:0de7b687289d3c1b3e8660d0741874abe7888100efe14bd0f9fd7141bcbda92b\",\n+                \"sha256:1110e22af8ca26b90bd6364fe4c763329b0ebf1ee213ba32b68c73de5752323d\",\n+                \"sha256:130272c698667a982a5d0e626851ceff662565379baf0ff2cc58067b81d4f11d\",\n+                \"sha256:136815f06a3ae311fae551c3df1f998a1ebd01ddd424aa5603a4336997629e95\",\n+                \"sha256:14215b71a762336254351b00ec720a8e85cada43b987da5a042e4ce3e82bd68e\",\n+                \"sha256:1db4e7fefefd0f548d73e2e2e041f9df5c59e178b4c72fbac4cc6f535cfb1565\",\n+                \"sha256:1ffd9493de4c922f2a38c2bf62b831dcec90ac673ed1ca182fe11b4d8e9f2a64\",\n+                \"sha256:2006769bd1640bdf4d5641c69a3d63b71b81445473cac5ded39740a226fa88ab\",\n+                \"sha256:20587d20f557fe189b7947d8e7ec5afa110ccf72a3128d61a2a387c3313f46be\",\n+                \"sha256:223217c3d4f82c3ac5e29032b3f1c2eb0fb591b72161f86d93f5719079dae93e\",\n+                \"sha256:27623ba66c183eca01bf9ff833875b459cad267aeeb044477fedac35e19ba907\",\n+                \"sha256:285e96d9d53422efc0d7a17c60e59f37fbf3dfa942073f666db4ac71e8d726d0\",\n+                \"sha256:2de62e8801ddfff069cd5c504ce3bc9672b23266597d4e4f50eda28846c322f2\",\n+                \"sha256:2f6c34da58ea9c1a9515621f4d9ac379871a8f21168ba1b5e09d74250de5ad62\",\n+                \"sha256:309a7de0a0ff3040acaebb35ec45d18db4b28232f21998851cfa709eeff49d62\",\n+                \"sha256:35c404d74c2926d0287fbd63ed5d27eb911eb9e4a3bb2c6d294f3cfd4a9e0c23\",\n+                \"sha256:3710a9751938947e6327ea9f3ea6332a09bf0ba0c09cae9cb1f250bd1f1549bc\",\n+                \"sha256:3d59d125ffbd6d552765510e3f31ed75ebac2c7470c7274195b9161a32350284\",\n+                \"sha256:40d3ff7fc90b98c637bda91c89d51264a3dcf210cade3a2c6f838c7268d7a4ca\",\n+                \"sha256:425c5f215d0eecee9a56cdb703203dda90423247421bf0d67125add85d0c4455\",\n+                \"sha256:43193c5cda5d612f247172016c4bb71251c784d7a4d9314677186a838ad34858\",\n+                \"sha256:44aeb140295a2f0659e113b31cfe92c9061622cadbc9e2a2f7b8ef6b1e29ef4b\",\n+                \"sha256:47334db71978b23ebcf3c0f9f5ee98b8d65992b65c9c4f2d34c2eaf5bcaf0594\",\n+                \"sha256:4796efc4faf6b53a18e3d46343535caed491776a22af773f366534056c4e1fbc\",\n+                \"sha256:4a51b48f42d9358460b78725283f04bddaf44a9358197b889657deba38f329db\",\n+                \"sha256:4b67fdab07fdd3c10bb21edab3cbfe8cf5696f453afce75d815d9d7223fbe88b\",\n+                \"sha256:4ec9dd88a5b71abfc74e9df5ebe7921c35cbb3b641181a531ca65cdb5e8e4dea\",\n+                \"sha256:4f9fc98dad6c2eaa32fc3af1417d95b5e3d08aff968df0cd320066def971f9a6\",\n+                \"sha256:54b6a92d009cbe2fb11054ba694bc9e284dad30a26757b1e372a1fdddaf21920\",\n+                \"sha256:55f56e2ebd4e3bc50442fbc0888c9d8c94e4e06a933804e2af3e89e2f9c1c749\",\n+                \"sha256:5726cf76c982532c1863fb64d8c6dd0e4c90b6ece9feb06c9f202417a31f7dd7\",\n+                \"sha256:5d447056e2ca60382d460a604b6302d8db69476fd2015c81e7c35417cfabe4cd\",\n+                \"sha256:5ed2e36c3e9b4f21dd9422f6893dec0abf2cca553af509b10cd630f878d3eb99\",\n+                \"sha256:5ff2ed8194587faf56555927b3aa10e6fb69d931e33953943bc4f837dfee2242\",\n+                \"sha256:62f60aebecfc7f4b82e3f639a7d1433a20ec32824db2199a11ad4f5e146ef5ee\",\n+                \"sha256:63bc5c4ae26e4bc6be6469943b8253c0fd4e4186c43ad46e713ea61a0ba49129\",\n+                \"sha256:6b40e8d38afe634559e398cc32b1472f376a4099c75fe6299ae607e404c033b2\",\n+                \"sha256:6b493a043635eb376e50eedf7818f2f322eabbaa974e948bd8bdd29eb7ef2a51\",\n+                \"sha256:6dba5d19c4dfab08e58d5b36304b3f92f3bd5d42c1a3fa37b5ba5cdf6dfcbcee\",\n+                \"sha256:6fd30dc99682dc2c603c2b315bded2799019cea829f8bf57dc6b61efde6611c8\",\n+                \"sha256:707b82d19e65c9bd28b81dde95249b07bf9f5b90ebe1ef17d9b57473f8a64b7b\",\n+                \"sha256:7706f5850360ac01d80c89bcef1640683cc12ed87f42579dab6c5d3ed6888613\",\n+                \"sha256:7782afc9b6b42200f7362858f9e73b1f8316afb276d316336c0ec3bd73312742\",\n+                \"sha256:79983512b108e4a164b9c8d34de3992f76d48cadc9554c9e60b43f308988aabe\",\n+                \"sha256:7f683ddc7eedd742e2889d2bfb96d69573fde1d92fcb811979cdb7165bb9c7d3\",\n+                \"sha256:82357d85de703176b5587dbe6ade8ff67f9f69a41c0733cf2425378b49954de5\",\n+                \"sha256:84450ba661fb96e9fd67629b93d2941c871ca86fc38d835d19d4225ff946a631\",\n+                \"sha256:86f4e8cca779080f66ff4f191a685ced73d2f72d50216f7112185dc02b90b9b7\",\n+                \"sha256:8cda06946eac330cbe6598f77bb54e690b4ca93f593dee1568ad22b04f347c15\",\n+                \"sha256:8ce7fd6767a1cc5a92a639b391891bf1c268b03ec7e021c7d6d902285259685c\",\n+                \"sha256:8ff4e7cdfdb1ab5698e675ca622e72d58a6fa2a8aa58195de0c0061288e6e3ea\",\n+                \"sha256:9289fd5dddcf57bab41d044f1756550f9e7cf0c8e373b8cdf0ce8773dc4bd417\",\n+                \"sha256:92a7e36b000bf022ef3dbb9c46bfe2d52c047d5e3f3343f43204263c5addc250\",\n+                \"sha256:92db3c28b5b2a273346bebb24857fda45601aef6ae1c011c0a997106581e8a88\",\n+                \"sha256:95c3c157765b031331dd4db3c775e58deaee050a3042fcad72cbc4189d7c8dca\",\n+                \"sha256:980b4f289d1d90ca5efcf07958d3eb38ed9c0b7676bf2831a54d4f66f9c27dfa\",\n+                \"sha256:9ae4ef0b3f6b41bad6366fb0ea4fc1d7ed051528e113a60fa2a65a9abb5b1d99\",\n+                \"sha256:9c98230f5042f4945f957d006edccc2af1e03ed5e37ce7c373f00a5a4daa6149\",\n+                \"sha256:9fa2566ca27d67c86569e8c85297aaf413ffab85a8960500f12ea34ff98e4c41\",\n+                \"sha256:a14969b8691f7998e74663b77b4c36c0337cb1df552da83d5c9004a93afdb574\",\n+                \"sha256:a8aacce6e2e1edcb6ac625fb0f8c3a9570ccc7bfba1f63419b3769ccf6a00ed0\",\n+                \"sha256:a8e538f46104c815be19c975572d74afb53f29650ea2025bbfaef359d2de2f7f\",\n+                \"sha256:aa41e526a5d4a9dfcfbab0716c7e8a1b215abd3f3df5a45cf18a12721d31cb5d\",\n+                \"sha256:aa693779a8b50cd97570e5a0f343538a8dbd3e496fa5dcb87e29406ad0299654\",\n+                \"sha256:ab22fbd9765e6954bc0bcff24c25ff71dcbfdb185fcdaca49e81bac68fe724d3\",\n+                \"sha256:ab2e5bef076f5a235c3774b4f4028a680432cded7cad37bba0fd90d64b187d19\",\n+                \"sha256:ab973df98fc99ab39080bfb0eb3a925181454d7c3ac8a1e695fddfae696d9e90\",\n+                \"sha256:af73657b7a68211996527dbfeffbb0864e043d270580c5aef06dc4b659a4b578\",\n+                \"sha256:b197e7094f232959f8f20541ead1d9862ac5ebea1d58e9849c1bf979255dfac9\",\n+                \"sha256:b295729485b06c1a0683af02a9e42d2caa9db04a373dc38a6a58cdd1e8abddf1\",\n+                \"sha256:b8831399554b92b72af5932cdbbd4ddc55c55f631bb13ff8fe4e6536a06c5c51\",\n+                \"sha256:b8dcd239c743aa2f9c22ce674a145e0a25cb1566c495928440a181ca1ccf6719\",\n+                \"sha256:bcb4f8ea87d03bc51ad04add8ceaf9b0f085ac045ab4d74e73bbc2dc033f0236\",\n+                \"sha256:bd7af3717683bea4c87acd8c0d3d5b44d56120b26fd3f8a692bdd2d5260c620a\",\n+                \"sha256:bf4475b82be41b07cc5e5ff94810e6a01f276e37c2d55571e3fe175e467a1a1c\",\n+                \"sha256:c3e446d253bd88f6377260d07c895816ebf33ffffd56c1c792b13bff9c3e1ade\",\n+                \"sha256:c57516e58fd17d03ebe67e181a4e4e2ccab1168f8c2976c6a334d4f819fe5944\",\n+                \"sha256:c94057af19bc953643a33581844649a7fdab902624d2eb739738a30e2b3e60fc\",\n+                \"sha256:cab5d0b79d987c67f3b9e9c53f54a61360422a5a0bc075f43cab5621d530c3b6\",\n+                \"sha256:ce031db0408e487fd2775d745ce30a7cd2923667cf3b69d48d219f1d8f5ddeb6\",\n+                \"sha256:cee4373f4d3ad28f1ab6290684d8e2ebdb9e7a1b74fdc39e4c211995f77bec27\",\n+                \"sha256:d5b054862739d276e09928de37c79ddeec42a6e1bfc55863be96a36ba22926f6\",\n+                \"sha256:dbe03226baf438ac4fda9e2d0715022fd579cb641c4cf639fa40d53b2fe6f3e2\",\n+                \"sha256:dc15e99b2d8a656f8e666854404f1ba54765871104e50c8e9813af8a7db07f12\",\n+                \"sha256:dcaf7c1524c0542ee2fc82cc8ec337f7a9f7edee2532421ab200d2b920fc97cf\",\n+                \"sha256:dd4eda173a9fcccb5f2e2bd2a9f423d180194b1bf17cf59e3269899235b2a114\",\n+                \"sha256:dd9a8bd8900e65504a305bf8ae6fa9fbc66de94178c420791d0293702fce2df7\",\n+                \"sha256:de7376c29d95d6719048c194a9cf1a1b0393fbe8488a22008610b0361d834ecf\",\n+                \"sha256:e7fdd52961feb4c96507aa649550ec2a0d527c086d284749b2f582f2d40a2e0d\",\n+                \"sha256:e91f541a85298cf35433bf66f3fab2a4a2cff05c127eeca4af174f6d497f0d4b\",\n+                \"sha256:e9e3c4c9e1ed40ea53acf11e2a386383c3304212c965773704e4603d589343ed\",\n+                \"sha256:ee803480535c44e7f5ad00788526da7d85525cfefaf8acf8ab9a310000be4b03\",\n+                \"sha256:f09cb5a7bbe1ecae6e87901a2eb23e0256bb524a79ccc53eb0b7629fbe7677c4\",\n+                \"sha256:f19c1585933c82098c2a520f8ec1227f20e339e33aca8fa6f956f6691b784e67\",\n+                \"sha256:f1a2f519ae173b5b6a2c9d5fa3116ce16e48b3462c8b96dfdded11055e3d6365\",\n+                \"sha256:f28f891ccd15c514a0981f3b9db9aa23d62fe1a99997512b0491d2ed323d229a\",\n+                \"sha256:f3e73a4255342d4eb26ef6df01e3962e73aa29baa3124a8e824c5d3364a65748\",\n+                \"sha256:f606a1881d2663630ea5b8ce2efe2111740df4b687bd78b34a8131baa007f79b\",\n+                \"sha256:fe9f97feb71aa9896b81973a7bbada8c49501dc73e58a10fcef6663af95e5079\",\n+                \"sha256:ffc519621dce0c767e96b9c53f09c5d215578e10b02c285809f76509a3931482\"\n             ],\n             \"markers\": \"python_full_version >= '3.7.0'\",\n-            \"version\": \"==3.3.2\"\n+            \"version\": \"==3.4.0\"\n         },\n         \"colorama\": {\n             \"hashes\": [\n@@ -754,10 +770,10 @@\n         },\n         \"distlib\": {\n             \"hashes\": [\n-                \"sha256:034db59a0b96f8ca18035f36290806a9a6e6bd9d1ff91e45a7f172eb17e51784\",\n-                \"sha256:1530ea13e350031b6312d8580ddb6b27a104275a31106523b8f123787f494f64\"\n+                \"sha256:47f8c22fd27c27e25a65601af709b38e4f0a45ea4fc2e710f65755fa8caaaf87\",\n+                \"sha256:a60f20dea646b8a33f3e7772f74dc0b2d0772d2837ee1342a00645c81edf9403\"\n             ],\n-            \"version\": \"==0.3.8\"\n+            \"version\": \"==0.3.9\"\n         },\n         \"docutils\": {\n             \"hashes\": [\n@@ -777,11 +793,11 @@\n         },\n         \"filelock\": {\n             \"hashes\": [\n-                \"sha256:81de9eb8453c769b63369f87f11131a7ab04e367f8d97ad39dc230daa07e3bec\",\n-                \"sha256:f6ed4c963184f4c84dd5557ce8fece759a3724b37b80c6c4f20a2f63a4dc6609\"\n+                \"sha256:2082e5703d51fbf98ea75855d9d5527e33d8ff23099bec374a134febee6946b0\",\n+                \"sha256:c249fbfcd5db47e5e2d6d62198e565475ee65e4831e2561c8e313fa7eb961435\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==3.16.0\"\n+            \"version\": \"==3.16.1\"\n         },\n         \"freezegun\": {\n             \"hashes\": [\n@@ -793,11 +809,11 @@\n         },\n         \"idna\": {\n             \"hashes\": [\n-                \"sha256:050b4e5baadcd44d760cedbd2b8e639f2ff89bbc7a5730fcc662954303377aac\",\n-                \"sha256:d838c2c0ed6fced7693d5e8ab8e734d5f8fda53a039c0164afb0b82e771e3603\"\n+                \"sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9\",\n+                \"sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3\"\n             ],\n             \"markers\": \"python_version >= '3.6'\",\n-            \"version\": \"==3.8\"\n+            \"version\": \"==3.10\"\n         },\n         \"importlib-metadata\": {\n             \"hashes\": [\n@@ -882,19 +898,19 @@\n         },\n         \"packaging\": {\n             \"hashes\": [\n-                \"sha256:026ed72c8ed3fcce5bf8950572258698927fd1dbda10a5e981cdf0ac37f4f002\",\n-                \"sha256:5b8f2217dbdbd2f7f384c41c628544e6d52f2d0f53c6d0c3ea61aa5d1d7ff124\"\n+                \"sha256:09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759\",\n+                \"sha256:c228a6dc5e932d346bc5739379109d49e8853dd8223571c7c5b55260edc0b97f\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==24.1\"\n+            \"version\": \"==24.2\"\n         },\n         \"platformdirs\": {\n             \"hashes\": [\n-                \"sha256:9e5e27a08aa095dd127b9f2e764d74254f482fef22b0970773bfba79d091ab8c\",\n-                \"sha256:eb1c8582560b34ed4ba105009a4badf7f6f85768b30126f351328507b2beb617\"\n+                \"sha256:357fb2acbc885b0419afd3ce3ed34564c13c9b95c89360cd9563f73aa5e2b907\",\n+                \"sha256:73e575e1408ab8103900836b97580d5307456908a03e92031bab39e4554cc3fb\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==4.3.2\"\n+            \"version\": \"==4.3.6\"\n         },\n         \"pluggy\": {\n             \"hashes\": [\n@@ -922,11 +938,11 @@\n         },\n         \"pyproject-api\": {\n             \"hashes\": [\n-                \"sha256:2dc1654062c2b27733d8fd4cdda672b22fe8741ef1dde8e3a998a9547b071eeb\",\n-                \"sha256:7ebc6cd10710f89f4cf2a2731710a98abce37ebff19427116ff2174c9236a827\"\n+                \"sha256:3d7d347a047afe796fd5d1885b1e391ba29be7169bd2f102fcd378f04273d228\",\n+                \"sha256:77b8049f2feb5d33eefcc21b57f1e279636277a8ac8ad6b5871037b243778496\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==1.7.1\"\n+            \"version\": \"==1.8.0\"\n         },\n         \"pytest\": {\n             \"hashes\": [\n@@ -974,12 +990,12 @@\n         },\n         \"pytest-split\": {\n             \"hashes\": [\n-                \"sha256:9e197df601828d76a1ab615158d9c6253ec9f96e46c1d3ea27187aa5ac0ef9de\",\n-                \"sha256:ca52527e4d9024f6ec3aba723527bd276d12096024999b1f5b8445a38da1e81c\"\n+                \"sha256:466096b086a7147bcd423c6e6c2e57fc62af1c5ea2e256b4ed50fc030fc3dddc\",\n+                \"sha256:adf80ba9fef7be89500d571e705b4f963dfa05038edf35e4925817e6b34ea66f\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version < '4.0' and python_full_version >= '3.8.1'\",\n-            \"version\": \"==0.9.0\"\n+            \"version\": \"==0.10.0\"\n         },\n         \"pytest-xdist\": {\n             \"hashes\": [\n@@ -1033,20 +1049,20 @@\n         },\n         \"tomli\": {\n             \"hashes\": [\n-                \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\",\n-                \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"\n+                \"sha256:2ebe24485c53d303f690b0ec092806a085f07af5a5aa1464f3931eec36caaa38\",\n+                \"sha256:d46d457a85337051c36524bc5349dd91b1877838e2979ac5ced3e710ed8a60ed\"\n             ],\n-            \"markers\": \"python_version >= '3.7'\",\n-            \"version\": \"==2.0.1\"\n+            \"markers\": \"python_version >= '3.8'\",\n+            \"version\": \"==2.0.2\"\n         },\n         \"tox\": {\n             \"hashes\": [\n-                \"sha256:35d472032ee1f73fe20c3e0e73d7073a4e85075c86ff02c576f9fc7c6a15a578\",\n-                \"sha256:3c0c96bc3a568a5c7e66387a4cfcf8c875b52e09f4d47c9f7a277ec82f1a0b11\"\n+                \"sha256:452bc32bb031f2282881a2118923176445bac783ab97c874b8770ab4c3b76c38\",\n+                \"sha256:86075e00e555df6e82e74cfc333917f91ecb47ffbc868dcafbd2672e332f4a2c\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==4.18.1\"\n+            \"version\": \"==4.23.2\"\n         },\n         \"types-colorama\": {\n             \"hashes\": [\n@@ -1090,30 +1106,30 @@\n         },\n         \"types-python-dateutil\": {\n             \"hashes\": [\n-                \"sha256:27c8cc2d058ccb14946eebcaaa503088f4f6dbc4fb6093d3d456a49aef2753f6\",\n-                \"sha256:9706c3b68284c25adffc47319ecc7947e5bb86b3773f843c73906fd598bc176e\"\n+                \"sha256:250e1d8e80e7bbc3a6c99b907762711d1a1cdd00e978ad39cb5940f6f0a87f3d\",\n+                \"sha256:58cb85449b2a56d6684e41aeefb4c4280631246a0da1a719bdbe6f3fb0317446\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==2.9.0.20240906\"\n+            \"version\": \"==2.9.0.20241003\"\n         },\n         \"types-requests\": {\n             \"hashes\": [\n-                \"sha256:1d1e79faeaf9d42def77f3c304893dea17a97cae98168ac69f3cb465516ee8da\",\n-                \"sha256:ff33935f061b5e81ec87997e91050f7b4af4f82027a7a7a9d9aaea04a963fdf8\"\n+                \"sha256:0d9cad2f27515d0e3e3da7134a1b6f28fb97129d86b867f24d9c726452634d95\",\n+                \"sha256:4195d62d6d3e043a4eaaf08ff8a62184584d2e8684e9d2aa178c7915a7da3747\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==2.32.0.20240907\"\n+            \"version\": \"==2.32.0.20241016\"\n         },\n         \"types-setuptools\": {\n             \"hashes\": [\n-                \"sha256:0abdb082552ca966c1e5fc244e4853adc62971f6cd724fb1d8a3713b580e5a65\",\n-                \"sha256:15b38c8e63ca34f42f6063ff4b1dd662ea20086166d5ad6a102e670a52574120\"\n+                \"sha256:78cb5fef4a6056d2f37114d27da90f4655a306e4e38042d7034a8a880bc3f5dd\",\n+                \"sha256:f9e1ebd17a56f606e16395c4ee4efa1cdc394b9a2a0ee898a624058b4b62ef8f\"\n             ],\n             \"index\": \"pypi\",\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==74.1.0.20240907\"\n+            \"version\": \"==75.3.0.20241112\"\n         },\n         \"typing-extensions\": {\n             \"hashes\": [\n@@ -1125,27 +1141,27 @@\n         },\n         \"urllib3\": {\n             \"hashes\": [\n-                \"sha256:a448b2f64d686155468037e1ace9f2d2199776e17f0a46610480d311f73e3472\",\n-                \"sha256:dd505485549a7a552833da5e6063639d0d177c04f23bc3864e41e5dc5f612168\"\n+                \"sha256:ca899ca043dcb1bafa3e262d73aa25c465bfb49e0bd9dd5d59f1d0acba2f8fac\",\n+                \"sha256:e7d814a81dad81e6caf2ec9fdedb284ecc9c73076b62654547cc64ccdcae26e9\"\n             ],\n             \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==2.2.2\"\n+            \"version\": \"==2.2.3\"\n         },\n         \"virtualenv\": {\n             \"hashes\": [\n-                \"sha256:48f2695d9809277003f30776d155615ffc11328e6a0a8c1f0ec80188d7874a55\",\n-                \"sha256:c17f4e0f3e6036e9f26700446f85c76ab11df65ff6d8a9cbfad9f71aabfcf23c\"\n+                \"sha256:142c6be10212543b32c6c45d3d3893dff89112cc588b7d0879ae5a1ec03a47ba\",\n+                \"sha256:f11f1b8a29525562925f745563bfd48b189450f61fb34c4f9cc79dd5aa32a1f4\"\n             ],\n-            \"markers\": \"python_version >= '3.7'\",\n-            \"version\": \"==20.26.4\"\n+            \"markers\": \"python_version >= '3.8'\",\n+            \"version\": \"==20.27.1\"\n         },\n         \"zipp\": {\n             \"hashes\": [\n-                \"sha256:9960cd8967c8f85a56f920d5d507274e74f9ff813a0ab8889a5b5be2daf44064\",\n-                \"sha256:c22b14cc4763c5a5b04134207736c107db42e9d3ef2d9779d465f5f1bcba572b\"\n+                \"sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4\",\n+                \"sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\"\n             ],\n-            \"markers\": \"python_version >= '3.8'\",\n-            \"version\": \"==3.20.1\"\n+            \"markers\": \"python_version >= '3.9'\",\n+            \"version\": \"==3.21.0\"\n         }\n     }\n }\ndiff --git a/cli/setup.py b/cli/setup.py\nindex dda7459fa671..d94bff58f9f9 100644\n--- a/cli/setup.py\n+++ b/cli/setup.py\n@@ -33,8 +33,13 @@ def get_tag(self):\n             # For more information about python compatibility tags, check out:\n             # https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/\n \n-            # We support Python 3.8+\n-            python = \"cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311\"\n+            # We support Python 3.9+\n+            # coupling: if you drop support for some python, you'll probably\n+            # have to update 'python_requires' at the end of this file\n+            # and a few workflows as show for example in this PR:\n+            # https://github.com/semgrep/semgrep-proprietary/pull/2606/files\n+            # coupling: semgrep.libsonnet default_python_version\n+            python = \"cp39.cp310.cp311.py39.py310.py311\"\n \n             # We don't require a specific Python ABI\n             abi = \"none\"\n@@ -123,7 +128,7 @@ def find_executable(env_name, exec_name):\n     \"peewee~=3.14\",\n     \"requests~=2.22\",\n     \"rich~=13.5.2\",\n-    \"ruamel.yaml>=0.16.0,<0.18\",\n+    \"ruamel.yaml>=0.18.5\",\n     \"tomli~=2.0.1\",\n     \"typing-extensions~=4.2\",\n     \"urllib3~=2.0\",\n@@ -158,13 +163,12 @@ def find_executable(env_name, exec_name):\n         \"License :: OSI Approved :: GNU Lesser General Public License v2 (LGPLv2)\",\n         \"Operating System :: MacOS\",\n         \"Operating System :: POSIX :: Linux\",\n-        \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Topic :: Security\",\n         \"Topic :: Software Development :: Quality Assurance\",\n     ],\n-    python_requires=\">=3.8\",\n+    python_requires=\">=3.9\",\n     zip_safe=False,\n )\ndiff --git a/cli/src/semdep/package_restrictions.py b/cli/src/semdep/package_restrictions.py\nindex 0f26948eb915..6f1711caeef4 100644\n--- a/cli/src/semdep/package_restrictions.py\n+++ b/cli/src/semdep/package_restrictions.py\n@@ -2,12 +2,12 @@\n from typing import List\n from typing import Tuple\n \n+import semgrep.semgrep_interfaces.semgrep_output_v1 as out\n from semdep.external.packaging.specifiers import InvalidSpecifier  # type: ignore\n from semdep.external.packaging.specifiers import SpecifierSet  # type: ignore\n from semdep.golang_version import compare_golang_specifier\n from semdep.maven_version import compare_maven_specifier\n from semgrep.error import SemgrepError\n-from semgrep.semgrep_interfaces.semgrep_output_v1 import DependencyPattern\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Ecosystem\n from semgrep.semgrep_interfaces.semgrep_output_v1 import FoundDependency\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Gomod\n@@ -51,9 +51,9 @@ def is_in_range(ecosystem: Ecosystem, range: str, version: str) -> bool:\n \n # compare vulnerable range to version in lockfile\n def dependencies_range_match_any(\n-    search_for_ranges: List[DependencyPattern],\n+    search_for_ranges: List[out.ScaPattern],\n     have_deps: List[FoundDependency],\n-) -> Iterator[Tuple[DependencyPattern, FoundDependency]]:\n+) -> Iterator[Tuple[out.ScaPattern, FoundDependency]]:\n     for have_dep in have_deps:\n         for target_range in search_for_ranges:\n             if (\ndiff --git a/cli/src/semgrep/config_resolver.py b/cli/src/semgrep/config_resolver.py\nindex 97f18a14a174..c913390432fe 100644\n--- a/cli/src/semgrep/config_resolver.py\n+++ b/cli/src/semgrep/config_resolver.py\n@@ -573,7 +573,8 @@ def from_config_list(\n \n         for i, config in enumerate(configs):\n             try:\n-                # Patch config_id to fix https://github.com/returntocorp/semgrep/issues/1912\n+                # Patch config_id to fix\n+                # https://github.com/semgrep/semgrep/issues/1912\n                 resolved_config, config_errors = resolve_config(\n                     config, project_url, force_jsonschema=force_jsonschema\n                 )\ndiff --git a/cli/src/semgrep/dependency_aware_rule.py b/cli/src/semgrep/dependency_aware_rule.py\nindex 2b3b574f5c7e..ade74db7d0a3 100644\n--- a/cli/src/semgrep/dependency_aware_rule.py\n+++ b/cli/src/semgrep/dependency_aware_rule.py\n@@ -18,12 +18,10 @@\n from semgrep.rule import Rule\n from semgrep.rule_match import RuleMatch\n from semgrep.semgrep_interfaces.semgrep_output_v1 import DependencyMatch\n-from semgrep.semgrep_interfaces.semgrep_output_v1 import DependencyPattern\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Direct\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Ecosystem\n from semgrep.semgrep_interfaces.semgrep_output_v1 import FoundDependency\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Pypi\n-from semgrep.semgrep_interfaces.semgrep_output_v1 import ScaInfo\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Transitive\n from semgrep.semgrep_interfaces.semgrep_output_v1 import Transitivity\n from semgrep.subproject import find_closest_subproject\n@@ -36,7 +34,7 @@\n SCA_FINDING_SCHEMA = 20220913\n \n \n-def parse_depends_on_yaml(entries: List[Dict[str, str]]) -> Iterator[DependencyPattern]:\n+def parse_depends_on_yaml(entries: List[Dict[str, str]]) -> Iterator[out.ScaPattern]:\n     \"\"\"\n     Convert the entries in the Yaml to ProjectDependsOnEntry objects that specify\n     namespace, package name, and semver ranges\n@@ -65,7 +63,7 @@ def parse_depends_on_yaml(entries: List[Dict[str, str]]) -> Iterator[DependencyP\n         if ecosystem == Ecosystem(Pypi()):\n             package = package.lower()\n \n-        yield DependencyPattern(\n+        yield out.ScaPattern(\n             ecosystem=ecosystem, package=package, semver_range=semver_range\n         )\n \n@@ -136,7 +134,7 @@ def generate_unreachable_sca_findings(\n                         ),\n                     ),\n                     extra={\n-                        \"sca_info\": ScaInfo(\n+                        \"sca_info\": out.ScaMatch(\n                             sca_finding_schema=SCA_FINDING_SCHEMA,\n                             reachable=False,\n                             reachability_rule=rule.should_run_on_semgrep_core,\n@@ -229,7 +227,7 @@ def generate_reachable_sca_findings(\n                     # ! deepcopy is necessary here since we might iterate over the\n                     # ! same match for multiple dependencies\n                     new_match = copy.deepcopy(match)\n-                    new_match.extra[\"sca_info\"] = ScaInfo(\n+                    new_match.extra[\"sca_info\"] = out.ScaMatch(\n                         sca_finding_schema=SCA_FINDING_SCHEMA,\n                         reachable=True,\n                         reachability_rule=rule.should_run_on_semgrep_core,\ndiff --git a/cli/src/semgrep/rule_lang.py b/cli/src/semgrep/rule_lang.py\nindex 79b5579b3022..99f432de0b6f 100644\n--- a/cli/src/semgrep/rule_lang.py\n+++ b/cli/src/semgrep/rule_lang.py\n@@ -387,7 +387,7 @@ def _validation_error_message(error: jsonschema.exceptions.ValidationError) -> s\n     return contexts[0].message\n \n \n-DUMMY_POSITION = out.Position(line=1, col=0)\n+DUMMY_POSITION = out.Position(line=1, col=0, offset=0)\n \n \n def safe_relative_to(a: Path, b: Path) -> Path:\ndiff --git a/cli/src/semgrep/semgrep_interfaces b/cli/src/semgrep/semgrep_interfaces\nindex 60809032a2e3..3171f2052e3a 160000\n--- a/cli/src/semgrep/semgrep_interfaces\n+++ b/cli/src/semgrep/semgrep_interfaces\n@@ -1,1 +1,1 @@\n-Subproject commit 60809032a2e39742f42910d46b3e5dd305b8b8cf\n+Subproject commit 3171f2052e3a2611688f84ca2664891b37e8c154\ndiff --git a/flake.lock b/flake.lock\nindex ba13a276f008..f44224c21f10 100644\n--- a/flake.lock\n+++ b/flake.lock\n@@ -52,11 +52,11 @@\n     },\n     \"nixpkgs\": {\n       \"locked\": {\n-        \"lastModified\": 1732837521,\n-        \"narHash\": \"sha256-jNRNr49UiuIwaarqijgdTR2qLPifxsVhlJrKzQ8XUIE=\",\n+        \"lastModified\": 1733581040,\n+        \"narHash\": \"sha256-Qn3nPMSopRQJgmvHzVqPcE3I03zJyl8cSbgnnltfFDY=\",\n         \"owner\": \"nixos\",\n         \"repo\": \"nixpkgs\",\n-        \"rev\": \"970e93b9f82e2a0f3675757eb0bfc73297cc6370\",\n+        \"rev\": \"22c3f2cf41a0e70184334a958e6b124fb0ce3e01\",\n         \"type\": \"github\"\n       },\n       \"original\": {\n@@ -129,11 +129,11 @@\n     \"opam-repository_2\": {\n       \"flake\": false,\n       \"locked\": {\n-        \"lastModified\": 1733107127,\n-        \"narHash\": \"sha256-gYyaYnbLTLJyRYqeGIMJbZ/J2tSxMS+BTzfEU82dhno=\",\n+        \"lastModified\": 1733656817,\n+        \"narHash\": \"sha256-ddoiIYY6AhWLUxkgtXhPbB7Du0OcHPyR4iEF/Tm8BsY=\",\n         \"owner\": \"ocaml\",\n         \"repo\": \"opam-repository\",\n-        \"rev\": \"5cfbed0a5094c19375b0d6e7b243c13f677ec0d1\",\n+        \"rev\": \"211ce1c3d5d6eb57dee3ca21cb1e4a16da41d01f\",\n         \"type\": \"github\"\n       },\n       \"original\": {\ndiff --git a/languages/javascript/generic/js_to_generic.ml b/languages/javascript/generic/js_to_generic.ml\nindex 0afaff434ed7..a35ab88f99b7 100644\n--- a/languages/javascript/generic/js_to_generic.ml\n+++ b/languages/javascript/generic/js_to_generic.ml\n@@ -305,7 +305,7 @@ and expr (x : expr) =\n       let e = G.Lambda def |> G.e in\n       (* Since the attrs aren't included in the AST, at least update the range\n        * to include them. See\n-       * https://github.com/returntocorp/semgrep/issues/7353 *)\n+       * https://github.com/semgrep/semgrep/issues/7353 *)\n       let attrs_any = List_.map (fun attr -> G.At attr) more_attrs in\n       H.set_e_range_with_anys (G.Dk (G.FuncDef def) :: attrs_any) e;\n       e\ndiff --git a/libs/ast_generic/AST_generic.ml b/libs/ast_generic/AST_generic.ml\nindex b154cc5e1bc1..ba0c181f2a05 100644\n--- a/libs/ast_generic/AST_generic.ml\n+++ b/libs/ast_generic/AST_generic.ml\n@@ -2275,7 +2275,8 @@ let empty_id_info ?(hidden = false) ?(case_insensitive = false)\n     id_resolved_alternatives = ref [];\n     id_type = ref None;\n     id_svalue = ref None;\n-    id_flags = ref (IdFlags.make ~hidden ~case_insensitive ~final:false);\n+    id_flags =\n+      ref (IdFlags.make ~hidden ~case_insensitive ~final:false ~static:false);\n     id_info_id = id;\n   }\n \ndiff --git a/libs/ast_generic/IdFlags.ml b/libs/ast_generic/IdFlags.ml\nindex c33f36ab8b62..77ade21056fc 100644\n--- a/libs/ast_generic/IdFlags.ml\n+++ b/libs/ast_generic/IdFlags.ml\n@@ -16,6 +16,7 @@ open Ppx_hash_lib.Std.Hash.Builtin\n let bitmask_HIDDEN = 0x01\n let bitmask_CASE_INSENSITIVE = 0x02\n let bitmask_FINAL = 0x04\n+let bitmask_STATIC = 0x08\n \n type t = int [@@deriving show, eq, ord, hash]\n \n@@ -29,11 +30,14 @@ let is_case_insensitive, set_case_insensitive =\n   make_flag bitmask_CASE_INSENSITIVE\n \n let is_final, set_final = make_flag bitmask_FINAL\n+let is_static, set_static = make_flag bitmask_STATIC\n+let union x y = Int.logor x y\n let to_int x = x\n \n-let make ~hidden ~case_insensitive ~final =\n+let make ~hidden ~case_insensitive ~final ~static =\n   let flags = empty in\n   let flags = if hidden then set_hidden flags else flags in\n   let flags = if case_insensitive then set_case_insensitive flags else flags in\n   let flags = if final then set_final flags else flags in\n+  let flags = if static then set_static flags else flags in\n   flags\ndiff --git a/libs/ast_generic/IdFlags.mli b/libs/ast_generic/IdFlags.mli\nindex 22e404febe81..e65d759520c5 100644\n--- a/libs/ast_generic/IdFlags.mli\n+++ b/libs/ast_generic/IdFlags.mli\n@@ -4,7 +4,8 @@ type t [@@deriving show, eq, ord, hash]\n val empty : t\n (** No flags set *)\n \n-val make : hidden:bool -> case_insensitive:bool -> final:bool -> t\n+val make :\n+  hidden:bool -> case_insensitive:bool -> final:bool -> static:bool -> t\n \n val is_hidden : t -> bool\n (**\n@@ -42,9 +43,20 @@ val set_case_insensitive : t -> t\n \n val is_final : t -> bool\n (** Flag 'is_final' is used within variable definitions/assignments to record\n- * whether the variable being defined is \"effectively final\" (as in Java).\n- * This is used by e.g. taint analysis to propagate taint via globals and\n- * private class attributes. *)\n+    whether the variable being defined is \"effectively final\" (as in Java).\n+    This is used by e.g. taint analysis to propagate taint via globals and\n+    private class attributes. *)\n \n val set_final : t -> t\n+\n+val is_static : t -> bool\n+(** Flag 'is_static' indicates that the identifier is \"static\",\n+    e.g. to mark a static class field/variable. *)\n+\n+val set_static : t -> t\n+\n+val union : t -> t -> t\n+(** Union two sets of flags. *)\n+\n val to_int : t -> int\n+(** Get the underlying representation of a set of flags. *)\ndiff --git a/libs/commons/UFile.ml b/libs/commons/UFile.ml\nindex 8088101c077c..def3742576c4 100644\n--- a/libs/commons/UFile.ml\n+++ b/libs/commons/UFile.ml\n@@ -217,39 +217,72 @@ let filemtime file =\n   if !Common.jsoo then failwith \"JSOO:filemtime\"\n   else (UUnix.stat !!file).st_mtime\n \n-(* TODO? there's also USys.is_directory? *)\n-let is_directory file = (UUnix.stat !!file).st_kind =*= Unix.S_DIR\n-let is_file file = (UUnix.stat !!file).st_kind =*= Unix.S_REG\n-let is_symlink file = (UUnix.lstat !!file).st_kind =*= Unix.S_LNK\n+let is_dir ~follow_symlinks path =\n+  let stat = if follow_symlinks then UUnix.stat else UUnix.lstat in\n+  match (stat !!path).st_kind with\n+  | S_DIR -> true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n+\n+let is_reg ~follow_symlinks path =\n+  let stat = if follow_symlinks then UUnix.stat else UUnix.lstat in\n+  match (stat !!path).st_kind with\n+  | S_REG -> true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n+\n+let is_dir_or_reg ~follow_symlinks path =\n+  let stat = if follow_symlinks then UUnix.stat else UUnix.lstat in\n+  match (stat !!path).st_kind with\n+  | S_DIR\n+  | S_REG ->\n+      true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n+\n+let is_lnk path =\n+  match (UUnix.lstat !!path).st_kind with\n+  | S_LNK -> true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n+\n+let is_lnk_or_reg path =\n+  match (UUnix.lstat !!path).st_kind with\n+  | S_LNK\n+  | S_REG ->\n+      true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n+\n+(* This function isn't very useful but we offer it for completeness. *)\n+let is_dir_or_lnk path =\n+  match (UUnix.lstat !!path).st_kind with\n+  | S_LNK\n+  | S_DIR ->\n+      true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n+\n+let is_dir_or_lnk_or_reg path =\n+  match (UUnix.lstat !!path).st_kind with\n+  | S_DIR\n+  | S_LNK\n+  | S_REG ->\n+      true\n+  | _ -> false\n+  | exception UUnix.Unix_error _ -> false\n \n let is_executable file =\n   let stat = UUnix.stat !!file in\n   let perms = stat.st_perm in\n   stat.st_kind =*= Unix.S_REG && perms land 0o011 <> 0\n \n-let lfile_exists filename =\n-  try\n-    match (UUnix.lstat !!filename).st_kind with\n-    | Unix.S_REG\n-    | Unix.S_LNK ->\n-        true\n-    | _ -> false\n-  with\n-  | UUnix.Unix_error (Unix.ENOENT, _, _) -> false\n-\n-(* Helps avoid the `Fatal error: exception Unix_error: No such file or directory stat` *)\n-let dir_exists path =\n-  try\n-    match (UUnix.lstat !!path).st_kind with\n-    | S_DIR -> true\n-    | _ -> false\n-  with\n-  | UUnix.Unix_error (Unix.ENOENT, _, _) -> false\n-\n let rec make_directories dir =\n   try UUnix.mkdir !!dir 0o755 with\n   (* The directory already exists *)\n-  | UUnix.Unix_error ((EEXIST | EISDIR), _, _) when is_directory dir -> ()\n+  | UUnix.Unix_error ((EEXIST | EISDIR), _, _)\n+    when is_dir ~follow_symlinks:false dir ->\n+      ()\n   (* parent doesn't exist *)\n   | UUnix.Unix_error (ENOENT, _, _) ->\n       let parent = Fpath.parent dir in\ndiff --git a/libs/commons/UFile.mli b/libs/commons/UFile.mli\nindex 91844c302f5c..b4addcc8216b 100644\n--- a/libs/commons/UFile.mli\n+++ b/libs/commons/UFile.mli\n@@ -85,63 +85,52 @@ val find_first_match_with_whole_line :\n (*****************************************************************************)\n (* File properties *)\n (*****************************************************************************)\n-val is_executable : Fpath.t -> bool\n-val filesize : Fpath.t -> int\n-val filemtime : Fpath.t -> float\n-\n-(*\n-   TODO: the current interface for checking the existence and kind of files\n-   is confusing and raises exceptions that we usually want to ignore.\n-   Provide a new interface that is convenient, unambiguous,\n-   and covers the most common situations.\n \n-   Design guideline: make common tasks easy and uncommon ones possible.\n+(* Check if the file is executable by others or by the group.\n+   If the file is only executable by the user owning the file ('u'),\n+   this function reports it as not executable.\n+   For example, the following commands create a file that's executable by its\n+   owner (and by root) on which is_executable fails:\n \n-   Here are some ideas:\n+     echo > foo\n+     chmod 700 foo\n+     ./foo && echo 'success'\n \n-   - input 1: follow symlinks?\n-   - input 2: which kind of files we accept: regular file, a folder (dir),\n-              or a symlink? (don't care about the exotic kinds)\n-   - input 3: is the file readable or writable? -> out of scope\n-   - result: bool, turn all Unix_error exceptions into 'false'\n-\n-     val is_dir : follow_symlinks:bool -> Fpath.t -> bool\n-     val is_reg : follow_symlinks:bool -> Fpath.t -> bool\n-\n-     (* follow_symlinks is implicitly false when one of the allowed file\n-        kinds is symlink *)\n-     val is_lnk : Fpath.t -> bool\n-     val is_lnk_or_reg : Fpath.t -> bool\n-     val is_dir_or_lnk_or_reg : Fpath.t -> bool\n-\n-   For other file kinds (pipes, sockets, ...) or unusual combinations\n-   (dir or lnk), use the Unix module directly.\n+   TODO: is this intentional? Please explain.\n *)\n+val is_executable : Fpath.t -> bool\n+val filesize : Fpath.t -> int\n+val filemtime : Fpath.t -> float\n \n-(* raise Unix_error if the directory does not exist *)\n-val is_directory : Fpath.t -> bool\n-\n-(* Check if a file is a regular file or a symbolic link that references\n-   a regular file.\n+(*\n+   Functions for testing whether a file exists and is of the expected kind,\n+   without raising exceptions.\n \n-   Raise Unix_error if the file doesn't exist or if the symlink is broken.\n-*)\n-val is_file : Fpath.t -> bool\n+   The goal is to deal with the 3 common file types (dir, reg, lnk)\n+   and focus only on files that are usable. If a file is not usable\n+   due for example to missing permissions, all these functions will return a\n+   negative answer ('false') rather than raising an exception.\n+   A design principle is \"make common tasks easy and uncommon tasks possible\".\n+   Here, we're focusing on the former.\n \n-(* Check if a file is a symlink. It may be a broken symlink.\n+   dir = directory = folder\n+   reg = regular files\n+   lnk = symbolic link\n \n-   Raise Unix_error if the file (symlink) doesn't exist.\n-*)\n-val is_symlink : Fpath.t -> bool\n+   The functions whose name contains 'lnk' never follow symlinks.\n \n-(*\n-   Check if a file is a regular file or a symlink, possibly a broken symlink.\n-   Return false if the file doesn't exist or is of the wrong kind.\n+   For more exotic file kinds or for classifying files by kind,\n+   use UUnix.stat or UUnix.lstat directly.\n *)\n-val lfile_exists : Fpath.t -> bool\n-\n-(* no raised Unix_error if the directory does not exist *)\n-val dir_exists : Fpath.t -> bool\n+val is_dir : follow_symlinks:bool -> Fpath.t -> bool\n+val is_reg : follow_symlinks:bool -> Fpath.t -> bool\n+val is_lnk : Fpath.t -> bool\n+val is_dir_or_reg : follow_symlinks:bool -> Fpath.t -> bool\n+val is_dir_or_lnk : Fpath.t -> bool\n+val is_lnk_or_reg : Fpath.t -> bool\n+val is_dir_or_lnk_or_reg : Fpath.t -> bool\n+\n+(* Turn a file kind into a JSON string node and vice-versa *)\n val file_kind_to_yojson : Unix.file_kind -> Yojson.Safe.t\n val file_kind_of_yojson : Yojson.Safe.t -> (Unix.file_kind, string) result\n \ndiff --git a/libs/commons/UTmp.mli b/libs/commons/UTmp.mli\nindex bf48d33970dd..bf4a465f9a16 100644\n--- a/libs/commons/UTmp.mli\n+++ b/libs/commons/UTmp.mli\n@@ -66,7 +66,7 @@ val with_temp_file :\n  * rely on the cached lookup table for the previous file with that filename,\n  * and chaos ensues.\n  *\n- * See https://github.com/returntocorp/semgrep/issues/5277 for more info.\n+ * See https://github.com/semgrep/semgrep/issues/5277 for more info.\n  *)\n val register_temp_file_cleanup_hook : (Fpath.t -> unit) -> unit\n \ndiff --git a/libs/commons/Uri_.ml b/libs/commons/Uri_.ml\nnew file mode 100644\nindex 000000000000..c8d140359a7b\n--- /dev/null\n+++ b/libs/commons/Uri_.ml\n@@ -0,0 +1,28 @@\n+(* Yoann Padioleau\n+ *\n+ * Copyright (C) 2024 Semgrep Inc.\n+ *\n+ * This library is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU Lesser General Public License\n+ * version 2.1 as published by the Free Software Foundation, with the\n+ * special exception on linking described in file LICENSE.\n+ *\n+ * This library is distributed in the hope that it will be useful, but\n+ * WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the file\n+ * LICENSE for more details.\n+ *)\n+\n+(*****************************************************************************)\n+(* Prelude *)\n+(*****************************************************************************)\n+(* Extensions to Uri.ml\n+ *)\n+\n+(*****************************************************************************)\n+(* API *)\n+(*****************************************************************************)\n+\n+let of_string_opt (str : string) : Uri.t option =\n+  let uri = Uri.of_string str in\n+  if Uri.equal uri Uri.empty then None else Some uri\ndiff --git a/libs/commons/Uri_.mli b/libs/commons/Uri_.mli\nnew file mode 100644\nindex 000000000000..02638580d2fc\n--- /dev/null\n+++ b/libs/commons/Uri_.mli\n@@ -0,0 +1,4 @@\n+(* like Uri.of_string but instead of silently returning an\n+ * empty uri in case of error, we return None here.\n+ *)\n+val of_string_opt : string -> Uri.t option\ndiff --git a/libs/commons2/common2.ml b/libs/commons2/common2.ml\nindex fd483a211e61..6fa0dd50d107 100644\n--- a/libs/commons2/common2.ml\n+++ b/libs/commons2/common2.ml\n@@ -2565,7 +2565,7 @@ let sanity_check_files_and_adjust ext files =\n            if not (file =~ \".*\\\\.\" ^ ext) then (\n              pr2 (\"warning: seems not a .\" ^ ext ^ \" file\");\n              false)\n-           else if UFile.is_directory (Fpath.v file) then (\n+           else if UFile.is_dir ~follow_symlinks:true (Fpath.v file) then (\n              pr2 (spf \"warning: %s is a directory\" file);\n              false)\n            else true)\n@@ -4966,7 +4966,7 @@ let _ = example\n let inits_of_absolute_dir dir =\n   if not (is_absolute dir) then\n     failwith (spf \"inits_of_absolute_dir: %s is not an absolute path\" dir);\n-  if not (UFile.is_directory (Fpath.v dir)) then\n+  if not (UFile.is_dir ~follow_symlinks:true (Fpath.v dir)) then\n     failwith (spf \"inits_of_absolute_dir: %s is not a directory\" dir);\n   let dir = chop_dirsymbol dir in\n \ndiff --git a/libs/gitignore/Parse_gitignore.ml b/libs/gitignore/Parse_gitignore.ml\nindex b0a5a5adf396..fee3d9d4413d 100644\n--- a/libs/gitignore/Parse_gitignore.ml\n+++ b/libs/gitignore/Parse_gitignore.ml\n@@ -141,10 +141,8 @@ let rec expand_includes ~orig_semgrepignore_path lines =\n         let include_path =\n           get_include_path ~orig_semgrepignore_path relative_include_path\n         in\n-        if\n-          Sys.file_exists (Fpath.to_string include_path)\n-          && UFile.is_file include_path\n-        then include_path |> UFile.read_file |> read_lines_from_string\n+        if UFile.is_reg ~follow_symlinks:true include_path then\n+          include_path |> UFile.read_file |> read_lines_from_string\n         else\n           (* ignore silently\n              (why: git also ignores .gitignore files that are broken\ndiff --git a/libs/lib_parsing/Parsing_stat.ml b/libs/lib_parsing/Parsing_stat.ml\nindex 1c0a7bcda27e..3ba1876bab36 100644\n--- a/libs/lib_parsing/Parsing_stat.ml\n+++ b/libs/lib_parsing/Parsing_stat.ml\n@@ -196,7 +196,7 @@ let regression_information ~ext (xs : Fpath.t list) (newscore : Common2.score) :\n       let xs = Fpath_.to_strings xs in\n       let dirname_opt =\n         match xs with\n-        | [ x ] when UFile.is_directory (Fpath.v x) -> Some x\n+        | [ x ] when UFile.is_dir ~follow_symlinks:true (Fpath.v x) -> Some x\n         | _ -> None\n       in\n       (* TODO Config_pfff.regression_data_dir *)\ndiff --git a/libs/process_limits/Memory_limit.mli b/libs/process_limits/Memory_limit.mli\nindex 94e4987892a2..325db6fc227b 100644\n--- a/libs/process_limits/Memory_limit.mli\n+++ b/libs/process_limits/Memory_limit.mli\n@@ -29,7 +29,7 @@ exception ExceededMemoryLimit of string\n    on Linux/x86_64 where we'd get a 'Stack_overflow' exception.\n    See:\n    - https://discuss.ocaml.org/t/is-there-any-value-in-having-a-maximum-stack-size/8214/10\n-   - https://github.com/returntocorp/semgrep/issues/3640\n+   - https://github.com/semgrep/semgrep/issues/3640\n \n    The limits set by this function call should be lower than the system\n    limits so as to get exceptions instead of segfaults.\ndiff --git a/libs/spacegrep/src/bin/Spacecat_main.ml b/libs/spacegrep/src/bin/Spacecat_main.ml\nindex 85d7ffbb0298..fc69140df617 100644\n--- a/libs/spacegrep/src/bin/Spacecat_main.ml\n+++ b/libs/spacegrep/src/bin/Spacecat_main.ml\n@@ -64,10 +64,9 @@ let man =\n       \"Read an arbitrary program from stdin and print the parse tree\\n\\\n       \\      to stdout.\";\n     `S Manpage.s_authors;\n-    `P \"Martin Jambon <martin@returntocorp.com>\";\n+    `P \"Martin Jambon <martin@semgrep.com>\";\n     `S Manpage.s_bugs;\n-    `P\n-      \"Check out bug reports at https://github.com/returntocorp/semgrep/issues.\";\n+    `P \"Check out bug reports at https://github.com/semgrep/semgrep/issues.\";\n     `S Manpage.s_see_also;\n     `P \"spacegrep(1)\";\n   ]\ndiff --git a/libs/spacegrep/src/bin/Spacegrep_main.ml b/libs/spacegrep/src/bin/Spacegrep_main.ml\nindex eaadd3b3d795..c33fb70c9191 100644\n--- a/libs/spacegrep/src/bin/Spacegrep_main.ml\n+++ b/libs/spacegrep/src/bin/Spacegrep_main.ml\n@@ -487,10 +487,9 @@ let man =\n     `S Manpage.s_description;\n     `P \"Match a pattern against an arbitrary program read from stdin.\";\n     `S Manpage.s_authors;\n-    `P \"Martin Jambon <martin@returntocorp.com>\";\n+    `P \"Martin Jambon <martin@semgrep.com>\";\n     `S Manpage.s_bugs;\n-    `P\n-      \"Check out bug reports at https://github.com/returntocorp/semgrep/issues.\";\n+    `P \"Check out bug reports at https://github.com/semgrep/semgrep/issues.\";\n     `S Manpage.s_see_also;\n     `P \"semgrep, spacecat\";\n   ]\ndiff --git a/libs/spacegrep/src/lib/Pre_match.ml b/libs/spacegrep/src/lib/Pre_match.ml\nindex f40140d2ff16..2d3664a28548 100644\n--- a/libs/spacegrep/src/lib/Pre_match.ml\n+++ b/libs/spacegrep/src/lib/Pre_match.ml\n@@ -6,7 +6,7 @@\n    optimization that operates directly on the source files rather than the\n    AST. It would be better since it turns out parsing is usually more\n    expensive than matching, and that would allow skipping the parsing step.\n-   See https://github.com/returntocorp/semgrep/issues/3461\n+   See https://github.com/semgrep/semgrep/issues/3461\n *)\n \n (*\ndiff --git a/libs/tracing/Tracing.mli b/libs/tracing/Tracing.mli\nindex 3a3d6d889fb4..f0466bfcf8a3 100644\n--- a/libs/tracing/Tracing.mli\n+++ b/libs/tracing/Tracing.mli\n@@ -34,6 +34,15 @@ type config = {\n \n type user_data = Trace_core.user_data\n \n+(*****************************************************************************)\n+(* Constants *)\n+(*****************************************************************************)\n+module Attributes : sig\n+  val version : string\n+  val instance_id : string\n+  val deployment_environment_name : string\n+end\n+\n (*****************************************************************************)\n (* Levels *)\n (*****************************************************************************)\n@@ -136,20 +145,13 @@ val trace_data_only :\n (*****************************************************************************)\n \n val configure_tracing :\n-  ?attrs:(string * user_data) list ->\n-  ?env:string ->\n-  ?version:string ->\n-  string ->\n-  Uri.t ->\n-  unit\n-(** [configure_tracing ~env:\"prod\" ~version:\"v1.0.0\" service_name\n-    tracing_endpoint] Before instrumenting anything, configure some settings.\n-    This should only be run once in a program, because it creates a backend with\n-    threads, HTTP connections, etc. when called. [~env] is the environment (e.g.\n-    production, staging, development). [~version] is the version of the service.\n-    [service_name] is the name of the service. [~attrs] can be used to set\n-    additional global attributes, which are tags that will be applied to all\n-    outgoing traces/metrics/logs etc.\n+  ?attrs:(string * user_data) list -> string -> Uri.t -> unit\n+(** [configure_tracing service_name tracing_endpoint] Before instrumenting\n+    anything, configure some settings. This should only be run once in a\n+    program, because it creates a backend with threads, HTTP connections, etc.\n+    when called. [service_name] is the name of the service. [~attrs] can be used\n+    to set additional global attributes (such as [\"service.version\"]), which are\n+    tags that will be applied to all outgoing traces/metrics/logs etc.\n \n     NOTE: this will set the active trace endpoint to\n     whatever is passed. This endpoint will be used when restarting tracing via\ndiff --git a/libs/tracing/js/Tracing.ml b/libs/tracing/js/Tracing.ml\nindex 0ce8e0a63a5e..7207fefcc29f 100644\n--- a/libs/tracing/js/Tracing.ml\n+++ b/libs/tracing/js/Tracing.ml\n@@ -36,6 +36,14 @@ type config = {\n }\n [@@deriving show]\n \n+(*****************************************************************************)\n+(* Constants *)\n+(*****************************************************************************)\n+module Attributes = struct\n+  let version = \"version\"\n+  let instance_id = \"instance_id\"\n+  let deployment_environment_name = \"deployment.environment.name\"\n+end\n (*****************************************************************************)\n (* Levels *)\n (*****************************************************************************)\n@@ -94,11 +102,9 @@ let otel_reporter : Logs.reporter = Logs.nop_reporter\n let stop_tracing ~exit_active_spans:_ () = ()\n let restart_tracing () = ()\n \n-let configure_tracing ?(attrs = []) ?(env = \"\") ?(version = \"\")\n-    (_service_name : string) (_endpoint : Uri.t) =\n+let configure_tracing ?(attrs = []) (_service_name : string) (_endpoint : Uri.t)\n+    =\n   ignore attrs;\n-  ignore env;\n-  ignore version;\n   ()\n \n let with_tracing (_fname : string)\ndiff --git a/libs/tracing/unix/Tracing.ml b/libs/tracing/unix/Tracing.ml\nindex 2f495145e880..4bf5c2dd2469 100644\n--- a/libs/tracing/unix/Tracing.ml\n+++ b/libs/tracing/unix/Tracing.ml\n@@ -97,6 +97,15 @@ let trace_level_var = \"SEMGREP_TRACE_LEVEL\"\n let parent_span_id_var = \"SEMGREP_TRACE_PARENT_SPAN_ID\"\n let parent_trace_id_var = \"SEMGREP_TRACE_PARENT_TRACE_ID\"\n \n+(* Service related attributes *)\n+module Attributes = struct\n+  open Opentelemetry.Conventions\n+\n+  let version = Attributes.Service.version\n+  let instance_id = Attributes.Service.instance_id\n+  let deployment_environment_name = \"deployment.environment.name\"\n+end\n+\n (*****************************************************************************)\n (* Helpers *)\n (*****************************************************************************)\n@@ -267,6 +276,10 @@ let otel_reporter : Logs.reporter =\n   in\n   { Logs.report }\n \n+(*****************************************************************************)\n+(* Metrics *)\n+(*****************************************************************************)\n+\n (*****************************************************************************)\n (* Span/Event entrypoints *)\n (*****************************************************************************)\n@@ -440,15 +453,11 @@ let setup_otel trace_endpoint =\n     Opentelemetry_trace.setup ()\n \n (* Set according to README of https://github.com/imandra-ai/ocaml-opentelemetry/ *)\n-let configure_tracing ?(attrs : (string * user_data) list = []) ?(env = \"prod\")\n-    ?version service_name trace_endpoint =\n+let configure_tracing ?(attrs : (string * user_data) list = []) service_name\n+    trace_endpoint =\n   Otel.Globals.service_name := service_name;\n   Otel.Globals.default_span_kind := Otel.Span.Span_kind_internal;\n-  version\n-  |> Option.iter (fun version ->\n-         add_global_attribute Otel.Conventions.Attributes.Service.version\n-           (`String version));\n-  add_global_attribute \"deployment.environment.name\" (`String env);\n+  let attrs = attrs @ Otel.GC_metrics.get_runtime_attributes () in\n   List.iter\n     (fun (key, value) -> Otel.Globals.add_global_attribute key value)\n     attrs;\ndiff --git a/scripts/build-semgrep-proprietary b/scripts/build-semgrep-proprietary\ndeleted file mode 100755\nindex cafa9853d777..000000000000\n--- a/scripts/build-semgrep-proprietary\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-#! /usr/bin/env bash\n-#\n-# CI job that checks whether semgrep-proprietary can compile using the\n-# current semgrep commit as a submodule.\n-#\n-set -eu\n-\n-# Try to not print the GitHub access token since it's a secret.\n-access=\"\"\n-if [[ -n \"${GITHUB_TOKEN+x}\" ]]; then\n-  echo \"Using GITHUB_TOKEN from environment.\"\n-  access=\"$GITHUB_TOKEN:@\"\n-fi\n-\n-semgrep_pro_url=\"https://${access}github.com/returntocorp/semgrep-proprietary.git\"\n-semgrep_root=$(git rev-parse --show-toplevel)\n-semgrep_commit=$(git rev-parse --short=7 HEAD)\n-\n-cat <<EOF\n-We're attempting to build semgrep-proprietary using the current version\n-of semgrep. We're assuming we're in a folder within the semgrep repo.\n-\n-Here's what we found about semgrep:\n-\n-semgrep repo root: $semgrep_root\n-semgrep commit: $semgrep_commit\n-semgrep-proprietary remote URL: <hidden>\n-semgrep git status:\n-$(cd \"$semgrep_root\" && git status)\n-EOF\n-\n-# Use a folder where it's safe to delete any existing semgrep-proprietary.\n-workdir=$semgrep_root/../tmp-semgrep-pro\n-mkdir -p \"$workdir\"\n-(\n-  cd \"$workdir\"\n-  rm -rf semgrep-proprietary\n-  git clone \"$semgrep_pro_url\" --depth 1\n-  (\n-    cd semgrep-proprietary\n-    echo \"semgrep-proprietary work folder: $(pwd)\"\n-    echo \"Git remote:\"\n-    git remote -v\n-\n-    # Fetch all submodules (although we don't need the semgrep submodule)\n-    git submodule update --init --recursive --depth 1\n-\n-    # Replace the semgrep submodule\n-    rm -rf semgrep\n-    ln -s \"$semgrep_root\" semgrep\n-\n-    # Build semgrep-proprietary\n-    make\n-\n-    # Run the tests\n-    make test\n-  )\n-)\ndiff --git a/scripts/compare.py b/scripts/compare.py\nindex 707769489a42..fe3ba7f6bb87 100755\n--- a/scripts/compare.py\n+++ b/scripts/compare.py\n@@ -66,7 +66,7 @@ def docker_cmd(use_podman: bool, dir: str, version: str) -> Sequence[str]:\n             *([\"--security-opt\", \"label=disable\"] if use_podman else []),\n             \"-v\",\n             f\"{dir}:/src\",\n-            f\"returntocorp/semgrep:{version}\",\n+            f\"semgrep/semgrep:{version}\",\n             \"semgrep\",\n             \"scan\",\n             \"--config\",\ndiff --git a/scripts/release/get_semgrep_runs_zip.py b/scripts/release/get_semgrep_runs_zip.py\ndeleted file mode 100644\nindex a4c682d59fcd..000000000000\n--- a/scripts/release/get_semgrep_runs_zip.py\n+++ /dev/null\n@@ -1,73 +0,0 @@\n-import subprocess\n-from typing import Any\n-from typing import Mapping\n-\n-import click\n-import requests\n-\n-API_ROOT = \"https://api.github.com/repos/returntocorp/semgrep-rules\"\n-ACCEPT = \"application/vnd.github.v3+json\"\n-HEADERS = {\"accept\": ACCEPT}\n-TIMEOUT = 30\n-ZIP_LOC = \"/tmp/semgrep_runs_output.zip\"\n-FILE_LOC = \"/tmp/semgrep_runs_output.tar.gz\"\n-\n-JsonObject = Mapping[str, Any]\n-\n-\n-def err(content: str, **kwargs: Any) -> None:\n-    click.secho(content, err=True, **kwargs)\n-\n-\n-def _gh_get(path: str) -> JsonObject:\n-    r = requests.get(f\"{API_ROOT}/{path}\", headers=HEADERS, timeout=TIMEOUT)\n-    r.raise_for_status()\n-    return r.json()\n-\n-\n-def _get_last_rules_commit() -> str:\n-    res = _gh_get(\"branches/develop\")\n-    return res[\"commit\"][\"sha\"]\n-\n-\n-def _get_action_run() -> JsonObject:\n-    res = _gh_get(f\"actions/runs?branch=develop&event=push\")\n-    # Runs are in reverse time order, so this gets the most recent run\n-    return next(r for r in res[\"workflow_runs\"] if r[\"name\"] == \"semgrep-rules-test\")\n-\n-\n-def _get_artifact_url(run_id: int) -> str:\n-    res = _gh_get(f\"actions/runs/{run_id}/artifacts\")\n-    return next(a for a in res[\"artifacts\"] if a[\"name\"] == \"semgrep_runs\")[\n-        \"archive_download_url\"\n-    ]\n-\n-\n-def _get_runs_artifact(url: str, access_token: str) -> None:\n-    res = requests.get(\n-        url, headers={\"Authorization\": f\"bearer {access_token}\"}, timeout=30\n-    )\n-    res.raise_for_status()\n-    with open(ZIP_LOC, \"wb\") as fd:\n-        fd.write(res.content)\n-\n-\n-def _unzip_artifact() -> None:\n-    subprocess.run([\"unzip\", ZIP_LOC], cwd=\"/tmp\")\n-\n-\n-@click.command()\n-@click.argument(\"access_token\")\n-def main(access_token: str):\n-    run_id: int = _get_action_run()[\"id\"]\n-    url: str = _get_artifact_url(run_id)\n-    err(f\"Downloading {url}\")\n-    _get_runs_artifact(url, access_token)\n-    err(f\"Download successful\")\n-    err(f\"Unzipping archive\")\n-    _unzip_artifact()\n-    err(f\"Done; artifact is at {FILE_LOC}\")\n-\n-\n-if __name__ == \"__main__\":\n-    main()\ndiff --git a/scripts/release/towncrier.toml b/scripts/release/towncrier.toml\nindex 78132c945b93..3bdf1d673318 100644\n--- a/scripts/release/towncrier.toml\n+++ b/scripts/release/towncrier.toml\n@@ -2,7 +2,7 @@\n     directory = \"../../changelog.d\"\n     filename = \"../../CHANGELOG.md\"\n     name = \"semgrep\"\n-    title_format = \"## [{version}](https://github.com/returntocorp/semgrep/releases/tag/v{version}) - {project_date}\"\n+    title_format = \"## [{version}](https://github.com/semgrep/semgrep/releases/tag/v{version}) - {project_date}\"\n     start_string = \"<!-- insertion point -->\\n\"\n     underlines = [\"\", \"\"]\n     [[tool.towncrier.type]]\ndiff --git a/scripts/validate-docker-release.sh b/scripts/validate-docker-release.sh\nindex 6464f00d0a74..2c4c05434474 100755\n--- a/scripts/validate-docker-release.sh\n+++ b/scripts/validate-docker-release.sh\n@@ -10,7 +10,7 @@ docker_tag=\"${version/v/}\"\n echo \"Validating release with docker tag: $docker_tag\"\n \n echo \"if 1 == 1: pass\" \\\n-    | docker run -i returntocorp/semgrep:\"$docker_tag\" semgrep -l python -e '$X == $X' - \\\n+    | docker run -i semgrep/semgrep:\"$docker_tag\" semgrep -l python -e '$X == $X' - \\\n     | grep -q \"1 == 1\"\n \n echo \"Docker image OK!\"\ndiff --git a/scripts/validate-wheel.sh b/scripts/validate-wheel.sh\nindex 202e55c10b52..7e5d76a3d9cc 100755\n--- a/scripts/validate-wheel.sh\n+++ b/scripts/validate-wheel.sh\n@@ -8,12 +8,12 @@ if [ -z \"$1\" ]; then\n     exit 1\n fi\n \n-# temporary workaround for https://github.com/returntocorp/semgrep/pull/8336/commits/565ba8443bd1ca4f2cd18fabb6ee61af971c5dda,\n+# temporary workaround for https://github.com/semgrep/semgrep/pull/8336/commits/565ba8443bd1ca4f2cd18fabb6ee61af971c5dda,\n # which forces amd64 builds to use the `any` platform compatibility tag and\n # causes the glob that we're calling this script with not match anything\n # this will be fixed in the near future as we factor python wheel building out\n # of the main Dockerfile and update amd64 to use the same approach as arm64\n-# (see: https://github.com/returntocorp/semgrep/pull/8371)\n+# (see: https://github.com/semgrep/semgrep/pull/8371)\n if [ ! -f \"$1\" ]; then\n     echo \"not a file: $1\"\n     exit 0\ndiff --git a/semgrep.opam b/semgrep.opam\nindex 219669baaa62..d023a5e11c1d 100644\n--- a/semgrep.opam\n+++ b/semgrep.opam\n@@ -1,6 +1,6 @@\n # This file is generated by dune, edit dune-project instead\n opam-version: \"2.0\"\n-version: \"1.97.0\"\n+version: \"1.98.0\"\n synopsis:\n   \"Like grep but for code: fast and syntax-aware semantic code pattern for many languages\"\n description: \"\"\"\ndiff --git a/src/core/Core_error.ml b/src/core/Core_error.ml\nindex 68dda19c3261..8df6f06a5c40 100644\n--- a/src/core/Core_error.ml\n+++ b/src/core/Core_error.ml\n@@ -79,7 +79,7 @@ end)\n \n let please_file_issue_text =\n   \"An error occurred while invoking the Semgrep engine. Please help us fix \\\n-   this by creating an issue at https://github.com/returntocorp/semgrep\"\n+   this by creating an issue at https://github.com/semgrep/semgrep\"\n \n let mk_error ?rule_id ?(msg = \"\") ?(loc : Tok.location option)\n     (err : Out.error_type) : t =\ndiff --git a/src/core/Core_match.ml b/src/core/Core_match.ml\nindex f4608f4dd91b..ec46151808fd 100644\n--- a/src/core/Core_match.ml\n+++ b/src/core/Core_match.ml\n@@ -16,9 +16,9 @@\n (*****************************************************************************)\n (* Prelude *)\n (*****************************************************************************)\n-(* Type to represent a semgrep \"match\" (a.k.a. a finding).\n+(* Type to represent a semgrep \"match\" (a.k.a. finding).\n  *\n- * Note that the \"core\" match are translated at some point in\n+ * Note that the \"core\" matches are translated at some point in\n  * Semgrep_output_v1.core_match, then processed in pysemgrep (or osemgrep)\n  * and translated again in Semgrep_output_v1.cli_match, and\n  * translated even further by pysemgrep (or osemgrep) ci in\n@@ -53,6 +53,18 @@ type t = {\n    * as separate matches? or better make them equal for dedup purpose?\n    *)\n   engine_of_match : Engine_kind.engine_of_finding; [@equal fun _a _b -> true]\n+  (* metavars for the pattern match *)\n+  env : Metavariable.bindings;\n+      [@equal\n+        fun a b ->\n+          List.equal\n+            (fun (s1, m1) (s2, m2) ->\n+              (* See the comment in Metavariable.mli for location_aware_equal_mvalue,\n+                 but basically we would like to consider matches different if they\n+                 metavariables bound to the same content, but at different locations.\n+              *)\n+              s1 = s2 && Metavariable.location_aware_equal_mvalue m1 m2)\n+            a b]\n   (* location info *)\n   path : Target.path;\n   (* less: redundant with location? *)\n@@ -75,18 +87,6 @@ type t = {\n   ast_node : AST_generic.any option;\n   (* less: do we need to be lazy? *)\n   tokens : Tok.t list Lazy.t; [@equal fun _a _b -> true]\n-  (* metavars for the pattern match *)\n-  env : Metavariable.bindings;\n-      [@equal\n-        fun a b ->\n-          List.equal\n-            (fun (s1, m1) (s2, m2) ->\n-              (* See the comment in Metavariable.mli for location_aware_equal_mvalue,\n-                 but basically we would like to consider matches different if they\n-                 metavariables bound to the same content, but at different locations.\n-              *)\n-              s1 = s2 && Metavariable.location_aware_equal_mvalue m1 m2)\n-            a b]\n   (* Lazy since construction involves forcing lazy token lists. *)\n   (* We used to have `[@equal fun _a _b -> true]` here, but this causes issues with\n      multiple findings to the same sink (but different sources) being removed\n@@ -94,7 +94,9 @@ type t = {\n      We now rely on equality of taint traces, which in turn relies on equality of `Parse_info.t`.\n   *)\n   taint_trace : Taint_trace.t Lazy.t option; (* secrets stuff *)\n-  (* Indicates whether a postprocessor ran and validated this result. *)\n+  (* SCA extra info about a match (e.g., the satisfied version constraint) *)\n+  sca_match : SCA_match.t option;\n+  (* Secrets. Indicates whether a postprocessor ran and validated this result. *)\n   validation_state : Rule.validation_state;\n   (* Indicates if the rule default severity should be modified to a different\n      severity. Currently this is just used by secrets validators in order to\n@@ -109,7 +111,6 @@ type t = {\n      the override is applied on top of the default and only changes the fields\n      present in the override. *)\n   metadata_override : JSON.t option;\n-  dependency : SCA_match.kind option;\n   (* A field to be populated based on intra-formula `fix` keys.\n      This is _prior_ to AST-based autofix and interpolation, which occurs in\n      Autofix.ml.\ndiff --git a/src/core/Core_match.mli b/src/core/Core_match.mli\nindex 167c7c0d61e3..c67489b2f162 100644\n--- a/src/core/Core_match.mli\n+++ b/src/core/Core_match.mli\n@@ -3,19 +3,20 @@ type t = {\n   (* rule (or mini rule) responsible for the pattern match found *)\n   rule_id : rule_id;\n   engine_of_match : Engine_kind.engine_of_finding;\n+  env : Metavariable.bindings;\n   (* location info *)\n   path : Target.path;\n   range_loc : Tok.location * Tok.location;\n   ast_node : AST_generic.any option;\n   tokens : Tok.t list Lazy.t;\n-  env : Metavariable.bindings;\n   (* trace *)\n   taint_trace : Taint_trace.t Lazy.t option;\n-  (* for secrets *)\n+  (* for SCA *)\n+  sca_match : SCA_match.t option;\n+  (* for Secrets *)\n   validation_state : Rule.validation_state;\n   severity_override : Rule.severity option;\n   metadata_override : JSON.t option;\n-  dependency : SCA_match.kind option;\n   (* A field to be populated based on intra-formula `fix` keys.\n      This is _prior_ to AST-based autofix and interpolation, which occurs in\n      Autofix.ml.\ndiff --git a/src/core/SCA_match.ml b/src/core/SCA_match.ml\nindex fda91aa3e672..3dafe6c6ddce 100644\n--- a/src/core/SCA_match.ml\n+++ b/src/core/SCA_match.ml\n@@ -1,13 +1,18 @@\n-type t = SCA_dependency.t * SCA_pattern.t [@@deriving show, eq]\n+type t = {\n+  (* the actual dependency in the lockfile *)\n+  dep : SCA_dependency.t;\n+  (* the version constraint on a package and its ecosystem *)\n+  pat : SCA_pattern.t;\n+  kind : kind;\n+}\n \n-(* alt: use a record as both constructors have the same type *)\n-type kind =\n+and kind =\n   (* Rule had both code patterns and dependency patterns, got matches on *both*,\n    * the Pattern Match is in code, annotated with this dependency match *)\n-  | CodeAndLockfileMatch of t\n+  | CodeAndLockfileMatch\n   (* Rule had dependency patterns, they matched, the Pattern Match is in a\n    * lockfile. So the range_loc of the Dependency.t in this dependency_match\n    * should be *the same* as the range_loc in the PatternMatch.t\n    *)\n-  | LockfileOnlyMatch of t\n+  | LockfileOnlyMatch\n [@@deriving show, eq]\ndiff --git a/src/core/Trace_data.ml b/src/core/Trace_data.ml\nindex 75e24c06481d..303c0b3f1200 100644\n--- a/src/core/Trace_data.ml\n+++ b/src/core/Trace_data.ml\n@@ -12,12 +12,81 @@\n  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the file\n  * LICENSE for more details.\n  *)\n-\n (*****************************************************************************)\n (* Prelude *)\n (*****************************************************************************)\n-(* Helpers to prepare data for Opentelemetry tracing *)\n+(* NOTE: [^0] is a footnote *)\n+(* Helpers to prepare attributes for Opentelemetry. Attributes[^0] are key-value\n+   pairs that are used for sorting and filtering telemetry data, and also for\n+   attaching info that may be relevant to the telemetry data. This module helps\n+   organize these attributes, and provides some attributes that we always want\n+   to set.\n+\n+   In general, there are two places we can add \"attributes\" to Opentelemetry\n+   data. There are \"resource\" attributes, where a resource[^1] is basically a\n+   service or program (think a rest API, a database, semgrep itself) that is\n+   emitting telemetry. Resource attributes usually help categorize and describe\n+   what's emitting telemetry. Examples:\n+   * Version\n+   * Deployment environment (develop, staging, production...)\n+   * Name of the deployment (semgrep, semgrep-app...)\n+   * Runtime version (OCaml 4.15, OCaml 5.0...)\n+   * How the resource was executed (# of jobs, cli flags passed, )\n+   * Commonly defined resource attributes: https://opentelemetry.io/docs/specs/semconv/resource/\n+\n+   Some of these resourcs are handled in a special way by opentelemetry[^2], or\n+   by tools that ingest opentelemetry data, like prometheus[^3] and datadog[4].\n+   That means we have to be careful what we set as resource attributes, as it\n+   can break alarms/monitors, dashboards, or tools as a whole altogether.\n+   There's a lot of rules of what attributes are used where, so if you are\n+   setting an attribute that's handles in a special way, please review the\n+   linked docs. These attributes are set in the ocaml otel sdk by setting the\n+   `global attributes` (this is a misnomer and not according to spec, see issue\n+   here[^5]). In general these attributes should be information that is immutable\n+   once the program starts.\n+\n+   The other kind of attributes are for any kind of opentelemetry event (traces,\n+   logs, metrics), and are used to describe said event, or attach relevant info.\n+   Examples:\n+   * Line/file a log was recorded\n+   * Stacktrace of an error in a trace\n+   * Status of a trace (success, error)\n+   * Args of a function being traced\n+   * Category of metric being recorded (whether a file was scanned succesfully\n+     or not)\n+\n+   Like resource attributes, there are commonly defined resource attributes[^5].\n+   These usually don't have any special handling.\n+\n+   footnotes:\n+   [^0] https://opentelemetry.io/docs/specs/otel/common/#attribute\n+   [^1] https://opentelemetry.io/docs/specs/otel/resource/sdk/\n+   [^2] https://opentelemetry.io/docs/specs/semconv/resource/#attributes-with-special-handling\n+   [^3] https://opentelemetry.io/docs/specs/otel/compatibility/prometheus_and_openmetrics/#resource-attributes-1\n+   [^4] https://docs.datadoghq.com/opentelemetry/schema_semantics/\n+   [^5] https://opentelemetry.io/docs/specs/semconv/\n+*)\n+\n+(*****************************************************************************)\n+(* Constants *)\n+(*****************************************************************************)\n \n+(* Only add Semgrep specific attributes here, the rest should go in Tracing.ml\n+   (like ocaml runtime version, if we're in a container etc.) *)\n+module Attributes = struct\n+  (* Scan related attrs *)\n+  let semgrep_managed_scan = \"scan.semgrep_managed_scan\"\n+  let engine = \"scan.engine\"\n+  let repo_name = \"scan.repo_name\"\n+  let jobs = \"scan.jobs\"\n+  let job = \"scan.parmap_job\"\n+  let folder = \"scan.folder\"\n+  let pro_secrets_validators = \"scan.pro_secrets_validators\"\n+  let pro_historical_scanning = \"scan.pro_historical_scanning\"\n+  let pro_deep_intrafile = \"scan.pro_deep_intrafile\"\n+  let pro_deep_interfile = \"scan.pro_deep_interfile\"\n+  let pro_secrets_allowed_origins = \"scan.pro_secrets_allowed_origins\"\n+end\n (*****************************************************************************)\n (* Types *)\n (*****************************************************************************)\n@@ -40,24 +109,89 @@ type analysis_flags = {\n let allowed_origins allow_all_origins =\n   if allow_all_origins then \"all_origins\" else \"pro_rules_only\"\n \n-(* Poor man's Git repo detection. Running git repo detection again\n-   seems wasteful, but checking two env vars is pretty cheap.\n-\n-   TODO the more we port of semgrep scan and semgrep ci, the more\n-   of this information will already be in OCaml *)\n-let repo_name () =\n-  match Sys.getenv_opt \"SEMGREP_REPO_DISPLAY_NAME\" with\n-  | Some name -> name\n-  | None -> (\n-      match Sys.getenv_opt \"SEMGREP_REPO_NAME\" with\n-      | Some name -> name\n-      | None -> \"<local run>\")\n+let get_env_vars =\n+  (* just get the first env var that is set in a list of env vars *)\n+  let get_first_env_var env_vars : string option =\n+    try\n+      match env_vars |> List_.map Sys.getenv_opt |> List_.filter_some with\n+      | hd :: _ -> Some hd\n+      | [] -> None\n+      (* any Sys.* function can raise Sys_error :( *)\n+    with\n+    | Sys_error e ->\n+        (* We probably want to see this error since it'd be really weird if it\n+           happened *)\n+        (* nosemgrep *)\n+        Logs.err (fun m ->\n+            m\n+              \"System error reading an environment variable for tracing data: \\\n+               %s\"\n+              e);\n+        None\n+  in\n+  let map_env_var_to_otel_data name type_ env_vars default :\n+      string * Trace_core.user_data =\n+    let user_data_val_of_string x =\n+      let v_opt =\n+        match type_ with\n+        | `Int -> int_of_string_opt x |> Option.map (fun x -> `Int x)\n+        | `String -> Some (`String x)\n+        | `Bool -> Some (`Bool (x = \"true\" || x = \"1\" || x = \"yes\"))\n+        | `Float -> float_of_string_opt x |> Option.map (fun x -> `Float x)\n+        | `None -> None\n+      in\n+      Option.value v_opt ~default\n+    in\n+    let user_data =\n+      get_first_env_var env_vars\n+      |> Option.fold ~none:default ~some:user_data_val_of_string\n+    in\n+    (name, user_data)\n+  in\n+  List_.map (fun (name, type_, env_vars, default) ->\n+      map_env_var_to_otel_data name type_ env_vars default)\n \n (* In case we don't have a repo name, report the base folder where\n    semgrep was run. We report only the base name to avoid leaking\n    user information they may not have expected us to include. *)\n let current_working_folder () = Filename.basename (Sys.getcwd ())\n \n+(*****************************************************************************)\n+(* Defaults *)\n+(*****************************************************************************)\n+(* Format:\n+   (name, type, env_vars, default_value)\n+   where we pick the first env var that is set, and if none are set we use the\n+   default value\n+*)\n+\n+(* Resource attributes we always want to try and set from the environment *)\n+let default_resource_env_attrs =\n+  [\n+    (* coupling: semgrep/meta.py, if you change this we may want to change\n+       something about job url there, or vice versa *)\n+    (* Instance of the semgrep `service` *)\n+    ( Tracing.Attributes.instance_id,\n+      `String,\n+      [ \"SEMGREP_JOB_URL\"; \"CI_JOB_URL\" ],\n+      `None );\n+    (* coupling: semgrep/meta.py, if you change this we may want to change\n+       something about job url there, or vice versa *)\n+    ( Attributes.semgrep_managed_scan,\n+      `Bool,\n+      [ \"SEMGREP_MANAGED_SCAN\" ],\n+      `Bool false );\n+    (* Poor man's Git repo detection. Running git repo detection again\n+       seems wasteful, but checking two env vars is pretty cheap.\n+\n+       TODO the more we port of semgrep scan and semgrep ci, the more\n+       of this information will already be in OCaml *)\n+    ( Attributes.repo_name,\n+      `String,\n+      [ \"SEMGREP_REPO_DISPLAY_NAME\"; \"SEMGREP_REPO_NAME\" ],\n+      `String \"<local run>\" );\n+  ]\n+\n (*****************************************************************************)\n (* Shortcuts for Otel tracing *)\n (*****************************************************************************)\n@@ -74,21 +208,34 @@ let no_analysis_features () =\n let data_of_languages (languages : Xlang.t list) =\n   languages |> List_.map (fun l -> (Xlang.to_string l, `Bool true))\n \n-let get_top_level_data jobs version analysis_flags =\n+(* NOTE: If this IS NOT semgrep specific stick it in Tracing.ml *)\n+(* WARNING: Let's be careful what we add as a resource attribute. TL;DR; these\n+   are used in different ways by the tools that ingest otel data , and certain\n+   types of data can have different performance and cost implications for these\n+   tools. See module commentary for more info\n+*)\n+let get_resource_attrs ?(env = \"prod\") ~engine ~analysis_flags ~jobs () =\n   [\n-    (\"version\", `String version);\n-    (\"jobs\", `Int jobs);\n-    (\"folder\", `String (current_working_folder ()));\n-    (\"repo_name\", `String (repo_name ()));\n-    (\"pro_secrets_validators\", `Bool analysis_flags.secrets_validators);\n-    (\"pro_historical_scanning\", `Bool analysis_flags.historical_scan);\n-    (\"pro_deep_intrafile\", `Bool analysis_flags.deep_intra_file);\n-    (\"pro_deep_interfile\", `Bool analysis_flags.deep_inter_file);\n+    (* Version of Semgrep *)\n+    (Tracing.Attributes.version, `String Version.version);\n+    (* Whether we're running in a production, staging, or develop environment\n+       (Usually maps to SMS prod,staging,dev2) *)\n+    (Tracing.Attributes.deployment_environment_name, `String env);\n+    (Attributes.engine, `String engine);\n+    (Attributes.jobs, `Int jobs);\n+    (Attributes.folder, `String (current_working_folder ()));\n+    (Attributes.pro_secrets_validators, `Bool analysis_flags.secrets_validators);\n+    (Attributes.pro_historical_scanning, `Bool analysis_flags.historical_scan);\n+    (Attributes.pro_deep_intrafile, `Bool analysis_flags.deep_intra_file);\n+    (Attributes.pro_deep_interfile, `Bool analysis_flags.deep_inter_file);\n+    (* TODO it would be nice if we also got how the process was executed, and\n+       with what config/flags *)\n   ]\n+  @ get_env_vars default_resource_env_attrs\n   @\n   if analysis_flags.secrets_validators then\n     [\n-      ( \"pro_secrets_allowed_origins\",\n+      ( Attributes.pro_secrets_allowed_origins,\n         `String (allowed_origins analysis_flags.allow_all_origins) );\n     ]\n   else []\ndiff --git a/src/core/Trace_data.mli b/src/core/Trace_data.mli\nindex 963003ff50fe..efe40c2c82cc 100644\n--- a/src/core/Trace_data.mli\n+++ b/src/core/Trace_data.mli\n@@ -16,6 +16,21 @@ type analysis_flags = {\n }\n [@@derving show]\n \n+(* constants *)\n+module Attributes : sig\n+  val semgrep_managed_scan : string\n+  val engine : string\n+  val repo_name : string\n+  val jobs : string\n+  val job : string\n+  val folder : string\n+  val pro_secrets_validators : string\n+  val pro_historical_scanning : string\n+  val pro_deep_intrafile : string\n+  val pro_deep_interfile : string\n+  val pro_secrets_allowed_origins : string\n+end\n+\n (* Helpers *)\n \n val no_analysis_features : unit -> analysis_flags\n@@ -24,7 +39,28 @@ val no_analysis_features : unit -> analysis_flags\n val data_of_languages : Xlang.t list -> (string * Tracing.user_data) list\n (** Convenience function to turn a list of interfile languages into otel data *)\n \n-val get_top_level_data :\n-  int -> string -> analysis_flags -> (string * Tracing.user_data) list\n-(** Create the tags for the top level span. These tags make it easy to see\n-    the traces we care about *)\n+val get_resource_attrs :\n+  ?env:string ->\n+  engine:string ->\n+  analysis_flags:analysis_flags ->\n+  jobs:int ->\n+  unit ->\n+  (string * Tracing.user_data) list\n+(** [get_resource_data ~engine:\"oss\" ~env:\"prod\" ~analysis_flags () ] creates\n+    tags for the resource we report traces to. This is essentially info about\n+    the \"service\" itself, that is immutable once the service/program starts.\n+    This data is usually useful for grouping large sets of\n+    logs/traces/errors/metrics and discovering or investigating other macro\n+    trends about Semgrep. Example: Service Version, OCaml runtime version,\n+    telemetry sdk version. See module commentary for more info\n+\n+    Other data besides what's passed in as flags to this function may be\n+    gathered from the environment such as Semgrep's version number.\n+\n+    [engine] is the engine we are using, e.g. \"oss\" or \"pro\"\n+\n+    [env] is the environment we are working in (\"prod\",\"dev2\" etc.). Defaults to\n+    \"prod\"\n+\n+    [analysis_flags] see {!analysis_flags}\n+  *)\ndiff --git a/src/core_cli/Core_CLI.ml b/src/core_cli/Core_CLI.ml\nindex f9545d9fdc6d..70b2c82cae4e 100644\n--- a/src/core_cli/Core_CLI.ml\n+++ b/src/core_cli/Core_CLI.ml\n@@ -834,7 +834,8 @@ let main_exn (caps : Cap.all_caps) (argv : string array) : unit =\n           let target_source : Core_scan_config.target_source =\n             match (!target_file, !lang, roots) with\n             | Some file, None, [] -> Target_file file\n-            | None, Some lang, [ file ] when UFile.is_file file ->\n+            | None, Some lang, [ file ]\n+              when UFile.is_reg ~follow_symlinks:true file ->\n                 Targets [ Target.mk_target lang file ]\n             | _ ->\n                 (* alt: use the file targeting in targets_of_config_DEPRECATED\n@@ -858,17 +859,17 @@ let main_exn (caps : Cap.all_caps) (argv : string array) : unit =\n           match config.tracing with\n           | None -> run caps config\n           | Some tracing ->\n-              let trace_data =\n-                Trace_data.get_top_level_data config.ncores Version.version\n-                  (Trace_data.no_analysis_features ())\n+              let resource_attrs =\n+                (* Let's make sure all traces/logs/metrics etc. are tagged as\n+                   coming from the OSS invocation *)\n+                Trace_data.get_resource_attrs ?env:tracing.env ~engine:\"oss\"\n+                  ~analysis_flags:(Trace_data.no_analysis_features ())\n+                  ~jobs:config.ncores ()\n               in\n-              Tracing.configure_tracing\n-              (* Let's make sure all traces/logs/metrics etc. are tagged as coming from the pro invocation *)\n-                ~attrs:[ (\"engine\", `String \"oss\") ]\n-                ?env:tracing.env ~version:Version.version \"semgrep-core\"\n+              Tracing.configure_tracing ~attrs:resource_attrs \"semgrep-core\"\n                 tracing.endpoint;\n-              Tracing.with_tracing \"Core_command.semgrep_core_dispatch\"\n-                trace_data (fun span_id ->\n+              Tracing.with_tracing \"Core_command.semgrep_core_dispatch\" []\n+                (fun span_id ->\n                   let tracing =\n                     { tracing with top_level_span = Some span_id }\n                   in\ndiff --git a/src/core_scan/Core_scan.ml b/src/core_scan/Core_scan.ml\nindex e2b4644a47f4..6fc899f72856 100644\n--- a/src/core_scan/Core_scan.ml\n+++ b/src/core_scan/Core_scan.ml\n@@ -724,8 +724,8 @@ let rules_for_target ~analyzer ~products ~origin ~respect_rule_paths rules =\n \n let lockfile_xtarget_resolve (manifest : Manifest.t option)\n     (lockfile : Lockfile.t) : Lockfile_xtarget.t =\n-  Lockfile_xtarget.resolve Parse_lockfile.parse_manifest\n-    Parse_lockfile.parse_lockfile lockfile manifest\n+  Lockfile_xtarget.resolve Parse_lockfile.parse_manifest Parse_lockfile.parse\n+    lockfile manifest\n \n let rules_for_lockfile_kind ~lockfile_kind rules =\n   rules\n@@ -840,7 +840,7 @@ let mk_target_handler (caps : < Cap.time_limit >) (config : Core_scan_config.t)\n               caps;\n             }\n       in\n-      let matches =\n+      let matches : Core_result.matches_single_file =\n         (* !!Calling Match_rules!! Calling the matching engine!! *)\n         Match_rules.check ~match_hook ~timeout ~dependency_match_table xconf\n           rules xtarget\ndiff --git a/src/core_scan/Parmap_targets.ml b/src/core_scan/Parmap_targets.ml\nindex 9b2d58871c89..2d286afd9192 100644\n--- a/src/core_scan/Parmap_targets.ml\n+++ b/src/core_scan/Parmap_targets.ml\n@@ -86,7 +86,7 @@ let core_error_of_path_exc (internal_path : Fpath.t) (e : Exception.t) :\n let init job =\n   (* Set a global attribute to the job number so we know when we look at\n      traces/logs/metrics which job it came from! *)\n-  Tracing.add_global_attribute \"parmap.job\" (`Int job);\n+  Tracing.add_global_attribute Trace_data.Attributes.job (`Int job);\n   (* Restart tracing as it is paused before forking below in both\n      map_targets___* funcs *)\n   (* NOTE: this only restarts tracing in the child *)\ndiff --git a/src/engine/Match_SCA_mode.ml b/src/engine/Match_SCA_mode.ml\nindex d72f86bc2943..cd920b837162 100644\n--- a/src/engine/Match_SCA_mode.ml\n+++ b/src/engine/Match_SCA_mode.ml\n@@ -1,8 +1,40 @@\n+(* Matthew McQuaid\n+ *\n+ * Copyright (C) 2024 Semgrep Inc.\n+ *\n+ * This library is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU Lesser General Public License\n+ * version 2.1 as published by the Free Software Foundation, with the\n+ * special exception on linking described in file LICENSE.\n+ *\n+ * This library is distributed in the hope that it will be useful, but\n+ * WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the file\n+ * LICENSE for more details.\n+ *)\n+\n module R = Rule\n module Out = Semgrep_output_v1_t\n+module Log = Log_engine.Log\n+\n+(*****************************************************************************)\n+(* Prelude *)\n+(*****************************************************************************)\n+(* SCA matching code for both parity rules (lockfile only findings)\n+ * and reachable rules.\n+ *)\n+\n+(*****************************************************************************)\n+(* Types *)\n+(*****************************************************************************)\n \n (* TODO? could move to SCA_match.ml *)\n type dependency_match_table = (Rule_ID.t, SCA_match.t list) Hashtbl.t\n+\n+(*****************************************************************************)\n+(* Helpers *)\n+(*****************************************************************************)\n+\n type cmp = [ `EQ | `GT | `LT ]\n \n let compare_version_core (c1 : SCA_version.core) (c2 : SCA_version.core) =\n@@ -33,15 +65,22 @@ let check_constraint SCA_pattern.{ version; op } v' =\n let match_dependency_pattern (deps : SCA_dependency.t list)\n     (pat : SCA_pattern.t) : SCA_match.t list =\n   deps\n-  |> List_.filter_map @@ fun (dep : SCA_dependency.t) ->\n-     if\n-       String.equal dep.package_name pat.package_name\n-       && pat.version_constraints |> fun (SCA_pattern.SCA_And cs) ->\n-          List.for_all\n-            (fun constr -> check_constraint constr dep.package_version)\n-            cs\n-     then Some (dep, pat)\n-     else None\n+  |> List_.filter_map (fun (dep : SCA_dependency.t) ->\n+         if\n+           String.equal dep.package_name pat.package_name\n+           && pat.version_constraints |> fun (SCA_pattern.SCA_And cs) ->\n+              cs\n+              |> List.for_all (fun constr ->\n+                     let res = check_constraint constr dep.package_version in\n+                     Log.debug (fun m ->\n+                         m \"match for %s on %s = %b (package = %s)\"\n+                           (SCA_pattern.show_version_constraint constr)\n+                           (SCA_version.show dep.package_version)\n+                           res dep.package_name);\n+                     res)\n+           (* the kind can be adjusted later in annotate_pattern_match *)\n+         then Some SCA_match.{ dep; pat; kind = LockfileOnlyMatch }\n+         else None)\n \n (* Return the set of dependency/pattern pairs that matched *)\n let match_dependency_formula :\n@@ -58,7 +97,13 @@ let match_dependencies lockfile_target rule =\n let match_all_dependencies lockfile_target =\n   List_.map (fun rule -> (rule, match_dependencies lockfile_target rule))\n \n-let check_rule rule (xtarget : Lockfile_xtarget.t) dependency_formula =\n+(*****************************************************************************)\n+(* Entry points *)\n+(*****************************************************************************)\n+\n+let check_rule (rule : Rule.t) (xtarget : Lockfile_xtarget.t)\n+    (dependency_formula : Rule.sca_dependency_formula) :\n+    Core_profiling.rule_profiling Core_result.match_result =\n   let _, parse_time =\n     Common.with_time (fun () -> Lazy.force xtarget.lazy_dependencies)\n   in\n@@ -66,9 +111,11 @@ let check_rule rule (xtarget : Lockfile_xtarget.t) dependency_formula =\n     Common.with_time (fun () ->\n         match_dependency_formula xtarget dependency_formula)\n   in\n+  Log.info (fun m -> m \"found %d lockfile matches\" (List.length matches));\n   let matches =\n     matches\n-    |> List_.map (fun ((dep, pat) : SCA_match.t) ->\n+    |> List_.map (fun (sca_match : SCA_match.t) ->\n+           let dep = sca_match.dep in\n            Core_match.\n              {\n                rule_id =\n@@ -95,7 +142,7 @@ let check_rule rule (xtarget : Lockfile_xtarget.t) dependency_formula =\n                validation_state = `No_validator;\n                severity_override = None;\n                metadata_override = None;\n-               dependency = Some (SCA_match.LockfileOnlyMatch (dep, pat));\n+               sca_match = Some sca_match;\n                fix_text = None;\n                facts = [];\n              })\n@@ -107,31 +154,33 @@ let check_rule rule (xtarget : Lockfile_xtarget.t) dependency_formula =\n       rule_id = fst rule.R.id;\n     }\n \n-let annotate_pattern_match dep_matches pm =\n+let annotate_pattern_match (dep_matches : SCA_match.t list option)\n+    (pm : Core_match.t) : Core_match.t list =\n   match dep_matches with\n   | None -> [ pm ]\n   | Some dep_matches ->\n-      (* If there are two, transitive copies of a library, and no direct copies, and it's used in code, we produce TWO reachable matches *)\n+      (* If there are two, transitive copies of a library, and no direct copies,\n+       * and it's used in code, we produce TWO reachable matches *)\n       dep_matches\n-      |> List_.filter_map (fun dm ->\n+      |> List_.filter_map (fun (dm : SCA_match.t) ->\n              (* TODO: Make this not quadratic\n                 If the match is on a transitive dep and there's also a match on\n-                a direct copy of the dep, then do not include it, only use the direct one\n-                this is what the python code does\n+                a direct copy of the dep, then do not include it, only use the\n+                direct one this is what the python code does\n              *)\n              if\n                SCA_dependency.(\n-                 Out.equal_transitivity (fst dm).transitivity `Transitive)\n+                 Out.equal_transitivity dm.dep.transitivity `Transitive)\n                && dep_matches\n-                  |> List.exists (fun (dep, _) ->\n+                  |> List.exists (fun ({ dep; _ } : SCA_match.t) ->\n                          SCA_dependency.(\n                            Out.equal_transitivity dep.transitivity `Direct\n-                           && String.equal dep.package_name\n-                                (fst dm).package_name))\n+                           && String.equal dep.package_name dm.dep.package_name))\n              then None\n              else\n                Some\n                  {\n                    pm with\n-                   Core_match.dependency = Some (CodeAndLockfileMatch dm);\n+                   Core_match.sca_match =\n+                     Some { dm with kind = SCA_match.CodeAndLockfileMatch };\n                  })\ndiff --git a/src/engine/Match_SCA_mode.mli b/src/engine/Match_SCA_mode.mli\nindex 50de8298482c..88e7bedb87c6 100644\n--- a/src/engine/Match_SCA_mode.mli\n+++ b/src/engine/Match_SCA_mode.mli\n@@ -14,7 +14,7 @@\n    dependencies between matches, which would impede parallelism.\n *)\n val check_rule :\n-  Rule.rule ->\n+  Rule.t ->\n   Lockfile_xtarget.t ->\n   Rule.sca_dependency_formula ->\n   Core_profiling.rule_profiling Core_result.match_result\ndiff --git a/src/engine/Match_rules.ml b/src/engine/Match_rules.ml\nindex bacf1ff5e697..7624929d7fb1 100644\n--- a/src/engine/Match_rules.ml\n+++ b/src/engine/Match_rules.ml\n@@ -103,7 +103,9 @@ let group_rules xconf rules xtarget =\n            | _ when not relevant_rule -> Right3 r\n            | `Taint _ as mode -> Left3 { r with mode }\n            | (`Extract _ | `Search _) as mode -> Middle3 { r with mode }\n-           | `SCA _ -> failwith \"SCA rule not available in core.\"\n+           | `SCA _ ->\n+               (* alt: failwith \"SCA rule not available in core.\" *)\n+               Right3 r\n            | `Steps _ ->\n                Log.warn (fun m ->\n                    m \"Step rule not handled: %s\" (Rule.show_rule r));\ndiff --git a/src/engine/Match_search_mode.ml b/src/engine/Match_search_mode.ml\nindex ca91e3f85424..628f28dfc171 100644\n--- a/src/engine/Match_search_mode.ml\n+++ b/src/engine/Match_search_mode.ml\n@@ -428,7 +428,7 @@ let apply_focus_on_ranges (env : env) (focus_mvars_list : R.focus_mv_list list)\n                validation_state = `No_validator;\n                severity_override = None;\n                metadata_override = None;\n-               dependency = None;\n+               sca_match = None;\n                fix_text = None;\n                facts = [];\n              })\n@@ -879,7 +879,7 @@ and evaluate_formula env opt_context\n       *  - bind closer metavariable-regexp with the relevant pattern\n       *  - propagate metavariables when intersecting ranges\n       *  - distribute filter_range in intersect_range?\n-      * See https://github.com/returntocorp/semgrep/issues/2664\n+      * See https://github.com/semgrep/semgrep/issues/2664\n   *)\n   let ranges, filter_expls =\n     fold_with_expls\ndiff --git a/src/engine/Range_with_metavars.ml b/src/engine/Range_with_metavars.ml\nindex 16e2da74fb32..16473430a02d 100644\n--- a/src/engine/Range_with_metavars.ml\n+++ b/src/engine/Range_with_metavars.ml\n@@ -109,7 +109,7 @@ let inside_compatible x y =\n  *\n  * alt: we could force the user to first And the metavariable-regex\n  * with the pattern-inside, to have the right scope.\n- * See https://github.com/returntocorp/semgrep/issues/2664\n+ * See https://github.com/semgrep/semgrep/issues/2664\n  * alt: we could do the rewriting ourselves, detecting that the\n  * metavariable-regex has the wrong scope.\n  *\ndiff --git a/src/engine/Xpattern_matcher.ml b/src/engine/Xpattern_matcher.ml\nindex 627a898d9efd..980b1968915c 100644\n--- a/src/engine/Xpattern_matcher.ml\n+++ b/src/engine/Xpattern_matcher.ml\n@@ -97,7 +97,7 @@ let (matches_of_matcher :\n                               validation_state = `No_validator;\n                               severity_override = None;\n                               metadata_override = None;\n-                              dependency = None;\n+                              sca_match = None;\n                               fix_text = None;\n                               facts = [];\n                             })))\ndiff --git a/src/il/IL_helpers.ml b/src/il/IL_helpers.ml\nindex f0f151cf02a9..4555984b4b0f 100644\n--- a/src/il/IL_helpers.ml\n+++ b/src/il/IL_helpers.ml\n@@ -94,6 +94,19 @@ let is_pro_resolved_global name =\n   | None ->\n       false\n \n+(* HACK: Because we don't have a \"Class\" type, classes have themselves as types. *)\n+let is_class_name (name : name) =\n+  match (!(name.id_info.id_resolved), !(name.id_info.id_type)) with\n+  | Some resolved1, Some { t = TyN (Id (_, { id_resolved; _ })); _ } -> (\n+      match !id_resolved with\n+      | None -> false\n+      | Some resolved2 ->\n+          (* If 'name' has type 'name' then we assume it's a class. *)\n+          AST_generic.equal_resolved_name resolved1 resolved2)\n+  | _, None\n+  | _, Some _ ->\n+      false\n+\n (***********************************************)\n (* L-values *)\n (***********************************************)\ndiff --git a/src/il/IL_helpers.mli b/src/il/IL_helpers.mli\nindex 9f234150e59c..cc0c8791cc24 100644\n--- a/src/il/IL_helpers.mli\n+++ b/src/il/IL_helpers.mli\n@@ -1,6 +1,7 @@\n val is_pro_resolved_global : IL.name -> bool\n (** Test whether a name is global and has been resolved by Pro-naming. *)\n \n+val is_class_name : IL.name -> bool\n val exp_of_arg : IL.exp IL.argument -> IL.exp\n \n (** Lvalue/Rvalue helpers working on the IL *)\ndiff --git a/src/matching/Generic_vs_generic.ml b/src/matching/Generic_vs_generic.ml\nindex 337c56bc717a..b74eee243de2 100644\n--- a/src/matching/Generic_vs_generic.ml\n+++ b/src/matching/Generic_vs_generic.ml\n@@ -2093,8 +2093,8 @@ and m_type_ a b =\n    *\n    * See the following for context about why this is done here, and not with\n    * desugaring earlier in the pipeline:\n-   * - https://github.com/returntocorp/semgrep/pull/5540\n-   * - https://github.com/returntocorp/semgrep/pull/4682\n+   * - https://github.com/semgrep/semgrep/pull/5540\n+   * - https://github.com/semgrep/semgrep/pull/4682\n    *)\n   | G.TyN a1, B.TyExpr { e = N b1; _ } ->\n       m_name a1 b1\n@@ -3653,7 +3653,7 @@ and m_directive a b =\n  * `require` nodes with the `ImportFrom` but instead added the `ImportFrom`.\n  *\n  * Having two places where the same symbol was defined complicated downstream\n- * analysis. See https://github.com/returntocorp/semgrep/pull/6532 for some\n+ * analysis. See https://github.com/semgrep/semgrep/pull/6532 for some\n  * of the issues that it caused.\n  *\n  * So, in order to simplify naming and maintain the existing matching behavior,\ndiff --git a/src/matching/Match_patterns.ml b/src/matching/Match_patterns.ml\nindex a5416012fbfa..532efbafb637 100644\n--- a/src/matching/Match_patterns.ml\n+++ b/src/matching/Match_patterns.ml\n@@ -186,10 +186,9 @@ let match_rules_and_recurse\n                         validation_state = `No_validator;\n                         severity_override = None;\n                         metadata_override = None;\n-                        dependency = None;\n-                        (* NOTE: the global rule fix and fix_regexp in rule_id will be\n-                           applied _later_, in `Autofix.ml`\n-                        *)\n+                        sca_match = None;\n+                        (* NOTE: the global rule fix and fix_regexp in rule_id\n+                         * will be applied _later_, in `Autofix.ml` *)\n                         fix_text = None;\n                         facts = [];\n                       }\n@@ -448,7 +447,7 @@ let check ~hook ?(has_as_metavariable = false) ?mvar_context\n                                 validation_state = `No_validator;\n                                 severity_override = None;\n                                 metadata_override = None;\n-                                dependency = None;\n+                                sca_match = None;\n                                 fix_text = None;\n                                 facts = x.facts;\n                               }\n@@ -516,7 +515,7 @@ let check ~hook ?(has_as_metavariable = false) ?mvar_context\n                                   validation_state = `No_validator;\n                                   severity_override = None;\n                                   metadata_override = None;\n-                                  dependency = None;\n+                                  sca_match = None;\n                                   fix_text = None;\n                                   facts;\n                                 }\n@@ -571,7 +570,7 @@ let check ~hook ?(has_as_metavariable = false) ?mvar_context\n                                     validation_state = `No_validator;\n                                     severity_override = None;\n                                     metadata_override = None;\n-                                    dependency = None;\n+                                    sca_match = None;\n                                     fix_text = None;\n                                     facts = [];\n                                   }\n@@ -671,7 +670,7 @@ let check ~hook ?(has_as_metavariable = false) ?mvar_context\n                                     validation_state = `No_validator;\n                                     severity_override = None;\n                                     metadata_override = None;\n-                                    dependency = None;\n+                                    sca_match = None;\n                                     fix_text = None;\n                                     facts = [];\n                                   }\ndiff --git a/src/naming/Naming_AST.ml b/src/naming/Naming_AST.ml\nindex 476fea629362..62f367e041d5 100644\n--- a/src/naming/Naming_AST.ml\n+++ b/src/naming/Naming_AST.ml\n@@ -675,7 +675,7 @@ let resolve lang prog =\n              * Function-name resolution is useful for interprocedural analysis,\n              * feature that was requested by JS/TS users, see:\n              *\n-             *     https://github.com/returntocorp/semgrep/issues/2787.\n+             *     https://github.com/semgrep/semgrep/issues/2787.\n              *)\n             | Lang.Js\n             | Lang.Ts ->\ndiff --git a/src/osemgrep/cli_ci/Ci_subcommand.ml b/src/osemgrep/cli_ci/Ci_subcommand.ml\nindex e4e10fdc5979..b98922e5a660 100644\n--- a/src/osemgrep/cli_ci/Ci_subcommand.ml\n+++ b/src/osemgrep/cli_ci/Ci_subcommand.ml\n@@ -30,7 +30,7 @@ module Out = Semgrep_output_v1_j\n    to look at the latest errors.\n \n    As an example, here is a workflow that failed in the past:\n-   https://github.com/returntocorp/semgrep/actions/runs/6599573075/job/17928762827\n+   https://github.com/semgrep/semgrep/actions/runs/6599573075/job/17928762827\n    Looking at the job log, we can see a problem when connecting to\n    the https://semgrep.dev/api/agent/scans/14253285/complete endpoint.\n    Then in Sentry you can paste this 'url: <URL>' in the query and search\n@@ -218,6 +218,14 @@ let scan_metadata () : Out.scan_metadata =\n          Logs.debug (fun m -> m \"SMS scan id: %s\" scan_id));\n   res\n \n+(*****************************************************************************)\n+(* Project config *)\n+(*****************************************************************************)\n+(* TODO: read the .semgrepconfig.yml in the repo *)\n+let project_config () : Out.ci_config_from_repo option =\n+  (* alt: Out.{ version = \"v1\"; tags = None; } *)\n+  None\n+\n (*****************************************************************************)\n (* Scan config *)\n (*****************************************************************************)\n@@ -292,13 +300,30 @@ let scan_config_and_rules_from_deployment ~dry_run\n       m \"  Reporting start of scan for %a\"\n         Fmt.(styled `Bold string)\n         deployment_config.name);\n-  let scan_meta : Out.scan_metadata = scan_metadata () in\n+  let scan_metadata : Out.scan_metadata = scan_metadata () in\n+  let project_config : Out.ci_config_from_repo option = project_config () in\n+\n+  (* TODO: deprecated from 1.43 *)\n+  (* less: should concatenate with raw_json project_config *)\n+  let meta =\n+    (* ugly: would be good for ATDgen to generate also a json_of_xxx *)\n+    prj_meta |> Out.string_of_project_metadata |> Yojson.Basic.from_string\n+  in\n+  let request : Out.scan_request =\n+    {\n+      meta = Some meta;\n+      project_metadata = prj_meta;\n+      scan_metadata;\n+      project_config;\n+    }\n+  in\n+\n   (* TODO:\n       metadata_dict[\"is_sca_scan\"] = supply_chain\n       proj_config = ProjectConfig.load_all()\n       metadata_dict = {**metadata_dict, **proj_config.to_dict()}\n   *)\n-  match Semgrep_App.start_scan ~dry_run caps prj_meta scan_meta with\n+  match Semgrep_App.start_scan ~dry_run caps request with\n   | Error msg ->\n       Logs.err (fun m -> m \"Could not start scan %s\" msg);\n       Error.exit_code_exn (Exit_code.fatal ~__LOC__)\ndiff --git a/src/osemgrep/cli_ci/Test_ci_subcommand.ml b/src/osemgrep/cli_ci/Test_ci_subcommand.ml\nindex 3c7d2765546a..77994ea99300 100644\n--- a/src/osemgrep/cli_ci/Test_ci_subcommand.ml\n+++ b/src/osemgrep/cli_ci/Test_ci_subcommand.ml\n@@ -96,10 +96,8 @@ let test_sms_scan_id (caps : Ci_subcommand.caps) =\n                           in\n                           (* similar to Unit_ci.ml sms_scan_id assert *)\n                           match scan with\n-                          | {\n-                           scan_metadata = Some { sms_scan_id = Some str; _ };\n-                           _;\n-                          } ->\n+                          | { scan_metadata = { sms_scan_id = Some str; _ }; _ }\n+                            ->\n                               Alcotest.(check string)\n                                 \"checking sms_scan_id\" sms_scan_id str\n                           | _ -> failwith (spf \"wrong scan request: %s\" s))))))\ndiff --git a/src/osemgrep/cli_install_ci/Install_ci_CLI.ml b/src/osemgrep/cli_install_ci/Install_ci_CLI.ml\nindex 811598eae33f..eb243ce67a59 100644\n--- a/src/osemgrep/cli_install_ci/Install_ci_CLI.ml\n+++ b/src/osemgrep/cli_install_ci/Install_ci_CLI.ml\n@@ -94,7 +94,8 @@ let cmdline_term =\n     let repo =\n       match repo_arg with\n       | \".\" -> Dir (Fpath.v \".\")\n-      | _ when UFile.dir_exists (Fpath.v repo_arg) -> Dir (Fpath.v repo_arg)\n+      | _ when UFile.is_dir ~follow_symlinks:true (Fpath.v repo_arg) ->\n+          Dir (Fpath.v repo_arg)\n       | _ ->\n           let owner, repo =\n             match String.split_on_char '/' repo_arg with\ndiff --git a/src/osemgrep/cli_install_ci/Install_ci_subcommand.ml b/src/osemgrep/cli_install_ci/Install_ci_subcommand.ml\nindex 17b3a4ae6657..eae9a3182735 100644\n--- a/src/osemgrep/cli_install_ci/Install_ci_subcommand.ml\n+++ b/src/osemgrep/cli_install_ci/Install_ci_subcommand.ml\n@@ -68,7 +68,7 @@ jobs:\n         env:\n             SEMGREP_APP_TOKEN: ${{ secrets.SEMGREP_APP_TOKEN }}\n         container:\n-            image: returntocorp/semgrep\n+            image: semgrep/semgrep\n         steps:\n             - uses: actions/checkout@v3\n             - run: semgrep ci\n@@ -336,7 +336,7 @@ let git_commit (caps : < Cap.exec >) : unit =\n  *)\n let semgrep_workflow_exists (caps : < Cap.exec >) ~repo : bool =\n   let dir, cmd =\n-    if UFile.dir_exists repo then\n+    if UFile.is_dir ~follow_symlinks:true repo then\n       ( Fpath.to_dir_path repo,\n         (Cmd.Name \"gh\", [ \"workflow\"; \"view\"; \"semgrep.yml\" ]) )\n     else\n@@ -359,7 +359,7 @@ let semgrep_workflow_exists (caps : < Cap.exec >) ~repo : bool =\n    and then return the path to the cloned repo.\n *)\n let prep_repo (caps : < caps ; .. >) (repo : Fpath.t) : Fpath.t =\n-  if UFile.dir_exists repo then repo\n+  if UFile.is_dir ~follow_symlinks:true repo then repo\n   else\n     let tmp_dir =\n       CapTmp.get_temp_dir_name caps#tmp\ndiff --git a/src/osemgrep/core_runner/Core_runner.ml b/src/osemgrep/core_runner/Core_runner.ml\nindex d0884cabf950..763a40c066c5 100644\n--- a/src/osemgrep/core_runner/Core_runner.ml\n+++ b/src/osemgrep/core_runner/Core_runner.ml\n@@ -1,5 +1,6 @@\n open Common\n module Env = Semgrep_envvars\n+module Out = Semgrep_output_v1_t\n \n (*************************************************************************)\n (* Prelude *)\n@@ -282,6 +283,55 @@ let targets_for_files_and_rules (files : Fpath.t list) (rules : Rule.t list) :\n   let lang_jobs = split_jobs_by_language conf rules files in\n   lang_jobs |> List.concat_map targets_of_lang_job\n \n+(*************************************************************************)\n+(* SCA targeting *)\n+(*************************************************************************)\n+\n+(* TODO: port subproject_matchers.py\n+ * TODO(matthew): We'll ultimately need to do something different, because\n+ * right now if a lockfile is ignored by .semgrepignore, it should still\n+ * be possible to produce reachable findings that reference that lockfile,\n+ * though we won't produce any lockfile-only findings for it\n+ *)\n+let find_lockfiles (targets : Fpath.t list) : Lockfile.t list =\n+  targets\n+  |> List_.filter_map (fun (target : Fpath.t) ->\n+         let res =\n+           match Fpath.basename target with\n+           | \"package-lock.json\" ->\n+               Some (Lockfile.mk_lockfile Out.NpmPackageLockJson target)\n+           | _else_ -> None\n+         in\n+         res\n+         |> Option.iter (fun (lockfile : Lockfile.t) ->\n+                Logs.debug (fun m ->\n+                    m \"found lockfile %s\" (Lockfile.show lockfile)));\n+         res)\n+\n+(* TODO: take just Target.regular instead\n+ * TODO: port subproject_matchers.py and more\n+ * TODO: we should use the path in the lockfile and the path in the\n+ *  target to associate the most precise lockfile.\n+ * TODO: the code below is wrong; it's just a v0 to get a demo working;\n+ * it will just attach the very first lockfile we found to every single\n+ * JS target.\n+ *)\n+let add_possibly_lockfile_to_regular_target (lockfiles : Lockfile.t list)\n+    (targets : Target.t list) : Target.t list =\n+  lockfiles\n+  |> List.fold_left\n+       (fun acc (lockfile : Lockfile.t) ->\n+         acc\n+         |> List_.map (fun (target : Target.t) ->\n+                match target with\n+                | Lockfile _ -> target\n+                | Regular\n+                    ({ analyzer = Xlang.L (Lang.Js, _); lockfile = None; _ } as\n+                     regular) ->\n+                    Regular { regular with lockfile = Some lockfile }\n+                | Regular _ -> target))\n+       targets\n+\n (*************************************************************************)\n (* Input/output adapters to Core_scan input/output *)\n (*************************************************************************)\n@@ -371,9 +421,9 @@ let mk_core_run_for_osemgrep (core_scan_func : Core_scan.func) : func =\n        work or findings.\n        TODO: do this even earlier? *)\n     let valid_rules =\n-      List_.deduplicate_gen\n-        ~get_key:(fun r -> Rule_ID.to_string (fst r.Rule.id))\n-        valid_rules\n+      valid_rules\n+      |> List_.deduplicate_gen ~get_key:(fun r ->\n+             Rule_ID.to_string (fst r.Rule.id))\n     in\n     let rule_errors : Core_error.t list =\n       invalid_rules |> List_.map Core_error.error_of_invalid_rule\n@@ -393,7 +443,14 @@ let mk_core_run_for_osemgrep (core_scan_func : Core_scan.func) : func =\n     report_status_and_add_metrics_languages\n       ~respect_gitignore:targeting_conf.respect_gitignore lang_jobs valid_rules\n       targets;\n-    let targets, applicable_rules = targets_and_rules_of_lang_jobs lang_jobs in\n+    let code_targets, applicable_rules =\n+      targets_and_rules_of_lang_jobs lang_jobs\n+    in\n+    let lockfiles = find_lockfiles targets in\n+    let final_targets =\n+      add_possibly_lockfile_to_regular_target lockfiles code_targets\n+      @ (lockfiles |> List_.map (fun x -> Target.Lockfile x))\n+    in\n     Logs.debug (fun m ->\n         m \"core runner: %i applicable rules of %i valid rules, %i invalid rules\"\n           (List.length applicable_rules)\n@@ -402,7 +459,7 @@ let mk_core_run_for_osemgrep (core_scan_func : Core_scan.func) : func =\n     let config =\n       {\n         config with\n-        target_source = Targets targets;\n+        target_source = Targets final_targets;\n         rule_source = Rules applicable_rules;\n       }\n     in\ndiff --git a/src/osemgrep/language_server/Unit_LS.ml b/src/osemgrep/language_server/Unit_LS.ml\nindex 116e033b2bd5..ce27455acaeb 100644\n--- a/src/osemgrep/language_server/Unit_LS.ml\n+++ b/src/osemgrep/language_server/Unit_LS.ml\n@@ -57,15 +57,16 @@ let mock_run_results (files : string list) : Core_runner.result =\n       {\n         message = Some \"test\";\n         metavars = [];\n-        dataflow_trace = None;\n+        severity = None;\n+        metadata = None;\n         fix = None;\n         is_ignored = false;\n         engine_kind = `OSS;\n+        dataflow_trace = None;\n+        sca_match = None;\n         validation_state = Some `No_validator;\n         historical_info = None;\n         extra_extra = None;\n-        severity = None;\n-        metadata = None;\n       }\n     in\n     let (m : Out.core_match) =\ndiff --git a/src/osemgrep/networking/Semgrep_App.ml b/src/osemgrep/networking/Semgrep_App.ml\nindex 8530ddbd75d5..bb5abdd0dab4 100644\n--- a/src/osemgrep/networking/Semgrep_App.ml\n+++ b/src/osemgrep/networking/Semgrep_App.ml\n@@ -138,7 +138,12 @@ let extract_errors (data : string) : string list =\n let extract_block_override (data : string) : (app_block_override, string) result\n     =\n   match Out.ci_scan_complete_response_of_string data with\n-  | { success = _; app_block_override; app_block_reason } as response ->\n+  | {\n+      success = _;\n+      app_block_override;\n+      app_block_reason;\n+      app_blocking_match_based_ids = _TODO;\n+    } as response ->\n       Logs.debug (fun m ->\n           m \"complete response = %s\"\n             (Out.show_ci_scan_complete_response response));\n@@ -188,9 +193,8 @@ let get_deployment_from_token token =\n (* Step1 : start scan *)\n (*****************************************************************************)\n \n-(* TODO: pass project_config *)\n-let start_scan_async ~dry_run caps (prj_meta : Project_metadata.t)\n-    (scan_meta : Out.scan_metadata) : (scan_id, string) result Lwt.t =\n+let start_scan_async ~dry_run caps (request : Out.scan_request) :\n+    (scan_id, string) result Lwt.t =\n   if dry_run then (\n     Logs.app (fun m -> m \"Would have sent POST request to create scan\");\n     Lwt.return_ok \"\")\n@@ -207,21 +211,6 @@ let start_scan_async ~dry_run caps (prj_meta : Project_metadata.t)\n       ]\n     in\n     let url = Uri.with_path !Semgrep_envvars.v.semgrep_url start_scan_route in\n-    (* deprecated from 1.43 *)\n-    (* TODO: should concatenate with raw_json project_config *)\n-    let meta =\n-      (* ugly: would be good for ATDgen to generate also a json_of_xxx *)\n-      prj_meta |> Out.string_of_project_metadata |> Yojson.Basic.from_string\n-    in\n-    let request : Out.scan_request =\n-      {\n-        meta;\n-        scan_metadata = Some scan_meta;\n-        project_metadata = Some prj_meta;\n-        (* TODO *)\n-        project_config = None;\n-      }\n-    in\n     let body = Out.string_of_scan_request request in\n     let pretty_body =\n       body |> Yojson.Basic.from_string |> Yojson.Basic.pretty_to_string\n@@ -245,8 +234,8 @@ Please make sure they have been set correctly.\n         Lwt.return_error msg\n     | Error e -> Lwt.return_error (spf \"Failed to start scan: %s\" e)\n \n-let start_scan ~dry_run caps prj_meta scan_meta =\n-  Lwt_platform.run (start_scan_async ~dry_run caps prj_meta scan_meta)\n+let start_scan ~dry_run caps request =\n+  Lwt_platform.run (start_scan_async ~dry_run caps request)\n \n (*****************************************************************************)\n (* Step2 : fetch scan config (version 2) *)\ndiff --git a/src/osemgrep/networking/Semgrep_App.mli b/src/osemgrep/networking/Semgrep_App.mli\nindex 4a2f0e20a708..60f87064156d 100644\n--- a/src/osemgrep/networking/Semgrep_App.mli\n+++ b/src/osemgrep/networking/Semgrep_App.mli\n@@ -35,10 +35,9 @@ type scan_id = string\n val start_scan :\n   dry_run:bool ->\n   < Cap.network ; Auth.cap_token ; .. > ->\n-  Project_metadata.t ->\n-  Semgrep_output_v1_t.scan_metadata ->\n+  Semgrep_output_v1_t.scan_request ->\n   (scan_id, string) result\n-(** [start_scan ~dry_run ~token url prj] informs the Semgrep App that a scan\n+(** [start_scan ~dry_run caps req] informs the Semgrep App that a scan\n     is about to be started, and returns the scan id from the server. If\n     [dry_run] is [true], the empty string will be returned ([Ok \"\"]). *)\n \n@@ -119,8 +118,6 @@ val fetch_scan_config_async :\n   repository:string ->\n   < Cap.network ; Auth.cap_token ; .. > ->\n   (Semgrep_output_v1_t.scan_config, string) result Lwt.t\n-(** [fetch_scan_config_async ~token ~sca ~dry_run ~full_scan repo] returns a\n-     promise of the rules for the provided configuration. *)\n \n val report_failure_async :\n   dry_run:bool ->\n@@ -132,12 +129,8 @@ val report_failure_async :\n val start_scan_async :\n   dry_run:bool ->\n   < Cap.network ; Auth.cap_token ; .. > ->\n-  Project_metadata.t ->\n-  Semgrep_output_v1_t.scan_metadata ->\n+  Semgrep_output_v1_t.scan_request ->\n   (scan_id, string) result Lwt.t\n-(** [start_scan_async ~dry_run ~token url prj] informs the Semgrep App that a\n-    scan is about to be started, and returns the scan id from the server. If\n-    [dry_run] is [true], the empty string will be returned ([Ok \"\"]). *)\n \n val upload_findings_async :\n   dry_run:bool ->\n@@ -146,8 +139,6 @@ val upload_findings_async :\n   complete:Semgrep_output_v1_t.ci_scan_complete ->\n   < Cap.network ; Auth.cap_token ; .. > ->\n   (app_block_override, string) result Lwt.t\n-(** [upload_findings_async ~dry_run ~token ~scan_id ~results ~complete]\n-    reports the findings to Semgrep App. *)\n \n val upload_rule_to_registry_async :\n   < Cap.network ; Auth.cap_token ; .. > ->\ndiff --git a/src/osemgrep/reporting/Cli_json_output.ml b/src/osemgrep/reporting/Cli_json_output.ml\nindex d23129deaedc..0648a2194ef7 100644\n--- a/src/osemgrep/reporting/Cli_json_output.ml\n+++ b/src/osemgrep/reporting/Cli_json_output.ml\n@@ -273,6 +273,7 @@ let cli_match_of_core_match ~fixed_lines fixed_env (hrules : Rule.hrules)\n        fix;\n        is_ignored;\n        dataflow_trace;\n+       sca_match;\n      };\n   } ->\n       let rule =\n@@ -327,15 +328,13 @@ let cli_match_of_core_match ~fixed_lines fixed_env (hrules : Rule.hrules)\n             metadata;\n             fix;\n             is_ignored = Some is_ignored;\n-            (* TODO: extra fields *)\n             fingerprint =\n               Semgrep_hashing_functions.match_based_id_partial rule rule_id\n                 metavars !!path;\n-            sca_info = None;\n+            sca_info = sca_match;\n             fixed_lines;\n             dataflow_trace;\n-            (* It's optional in the CLI output, but not in the core match results!\n-             *)\n+            (* It's optional in the CLI output, but not in core match results!*)\n             engine_kind = Some engine_kind;\n             validation_state;\n             historical_info;\ndiff --git a/src/osemgrep/reporting/Gitlab_output.ml b/src/osemgrep/reporting/Gitlab_output.ml\nindex 045fac38375c..9e64e21f0bc7 100644\n--- a/src/osemgrep/reporting/Gitlab_output.ml\n+++ b/src/osemgrep/reporting/Gitlab_output.ml\n@@ -8,7 +8,7 @@ module J = JSON\n (* Output findings compatible with GitLab SAST JSON format.\n \n    - Written based on:\n-     https://github.com/returntocorp/semgrep-action/blob/678eff1a4269ed04b76631771688c8be860ec4e9/src/semgrep_agent/findings.py#L137-L165\n+     https://github.com/semgrep/semgrep-action/blob/678eff1a4269ed04b76631771688c8be860ec4e9/src/semgrep_agent/findings.py#L137-L165\n    - Docs:\n      https://docs.gitlab.com/ee/user/application_security/sast/#reports-json-format\n    - Schema:\ndiff --git a/src/osemgrep/reporting/Semgrep_hashing_functions.ml b/src/osemgrep/reporting/Semgrep_hashing_functions.ml\nindex 3afd3a915e50..2a525eaf230e 100644\n--- a/src/osemgrep/reporting/Semgrep_hashing_functions.ml\n+++ b/src/osemgrep/reporting/Semgrep_hashing_functions.ml\n@@ -197,7 +197,7 @@ let match_formula_interpolated_str (rule : Rule.t) metavars : string =\n  * quirks.\n  *\n  * The way match based ID is calculated on the python side is as follows:\n- * (see https://github.com/returntocorp/semgrep/blob/2d34ce584d16c4e954349690a5f12fae877a94d6/cli/src/semgrep/rule.py#L289-L334)\n+ * (see https://github.com/semgrep/semgrep/blob/2d34ce584d16c4e954349690a5f12fae877a94d6/cli/src/semgrep/rule.py#L289-L334)\n  * 1. Sort all top level keys (i.e pattern, patterns etc.) alphabetically\n  * 2. For each key: DFS the tree and find all pattern values (i.e. the rhs of\n  *    pattern: <THING>)\ndiff --git a/src/parsing/Parse_SCA_version.ml b/src/parsing/Parse_SCA_version.ml\nnew file mode 100644\nindex 000000000000..a7cc428d3006\n--- /dev/null\n+++ b/src/parsing/Parse_SCA_version.ml\n@@ -0,0 +1,104 @@\n+(* Matthew McQuaid, Yoann Padioleau\n+ *\n+ * Copyright (C) 2024 Semgrep Inc.\n+ *\n+ * This library is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU Lesser General Public License\n+ * version 2.1 as published by the Free Software Foundation, with the\n+ * special exception on linking described in file LICENSE.\n+ *\n+ * This library is distributed in the hope that it will be useful, but\n+ * WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the file\n+ * LICENSE for more details.\n+ *)\n+open Common\n+\n+(*****************************************************************************)\n+(* Prelude *)\n+(*****************************************************************************)\n+(* Parse a package version or version constraint\n+ *\n+ * TODO: port parts of cli/src/semdep/external/packaging/specifiers.py\n+ *)\n+\n+(*****************************************************************************)\n+(* Error management *)\n+(*****************************************************************************)\n+\n+(* alt: use Result.t *)\n+exception Error of string\n+\n+(*****************************************************************************)\n+(* Helpers *)\n+(*****************************************************************************)\n+\n+(*****************************************************************************)\n+(* Entrypoints *)\n+(*****************************************************************************)\n+\n+(* TODO: port part of specifiers.py\n+ * TODO: pass more context (e.g., a Tok.t) for better Logs.warn or error\n+ *)\n+let parse (str : string) : SCA_version.t =\n+  (* just enough to parse some toy package-lock.json and enough\n+   * to parse the version part of the version constraints in our SCA rules\n+   *)\n+  match str with\n+  (* \"1.2.3\" *)\n+  | _ when str =~ \"^\\\\([0-9]+\\\\)\\\\.\\\\([0-9]+\\\\)\\\\.\\\\([0-9]+\\\\)$\" ->\n+      let s1, s2, s3 = Common.matched3 str in\n+      SCA_version.V\n+        {\n+          major = int_of_string s1;\n+          minor = int_of_string s2;\n+          incrementals = [ int_of_string s3 ];\n+        }\n+  (* \"40.0\" *)\n+  | _ when str =~ \"^\\\\([0-9]+\\\\)\\\\.\\\\([0-9]+\\\\)$\" ->\n+      let s1, s2 = Common.matched2 str in\n+      SCA_version.V\n+        {\n+          major = int_of_string s1;\n+          minor = int_of_string s2;\n+          incrementals = [];\n+        }\n+  (* \"40\" *)\n+  | _ when str =~ \"^\\\\([0-9]+\\\\)$\" ->\n+      let s1 = Common.matched1 str in\n+      SCA_version.V { major = int_of_string s1; minor = 0; incrementals = [] }\n+  (* alt: raise (Error (spf \"wrong version format for %s\" str)) in *)\n+  | _ ->\n+      (* nosemgrep: no-logs-in-library *)\n+      Logs.warn (fun m -> m \"unrecognized version format for %s\" str);\n+      SCA_version.Other str\n+\n+(* TODO: port part of specifiers.py *)\n+let parse_constraints (s : string) : SCA_pattern.version_constraints =\n+  let error () = raise (Error (spf \"wrong constraint format for %s\" s)) in\n+  (* just enough to parse some toy package-lock.json *)\n+  (* similar to what we did for ruleid: annots in cli_test/Test_annotation.ml *)\n+  (* start from \" > 1.0.2, < 1.05 \" *)\n+  let s = String.trim s in\n+  let xs = Str.split_delim (Str.regexp \"[ \\t]*,[ \\t]*\") s in\n+  SCA_pattern.SCA_And\n+    (xs\n+    |> List_.map (fun s ->\n+           (* \"> 1.0.2\" *)\n+           let s = String.trim s in\n+           if s =~ \"^\\\\([=<>]+\\\\)[ \\t]*\\\\([^ ]+\\\\)$\" then\n+             let op, ver = Common.matched2 s in\n+             let op : SCA_pattern.sca_operator =\n+               match op with\n+               | \"=\"\n+               | \"==\" ->\n+                   Eq\n+               | \">=\" -> Gte\n+               | \"<=\" -> Lte\n+               | \">\" -> Gt\n+               | \"<\" -> Lt\n+               | _ -> error ()\n+             in\n+             let version : SCA_version.t = parse ver in\n+             SCA_pattern.{ op; version }\n+           else error ()))\ndiff --git a/src/parsing/Parse_SCA_version.mli b/src/parsing/Parse_SCA_version.mli\nnew file mode 100644\nindex 000000000000..bf8cdf408a87\n--- /dev/null\n+++ b/src/parsing/Parse_SCA_version.mli\n@@ -0,0 +1,7 @@\n+exception Error of string\n+\n+(* may raise Error *)\n+val parse : string -> SCA_version.t\n+\n+(* used in Parse_rule.ml, may also raise Error *)\n+val parse_constraints : string -> SCA_pattern.version_constraints\ndiff --git a/src/parsing/Parse_lockfile.ml b/src/parsing/Parse_lockfile.ml\nindex 01cd8be58003..c87237f15b57 100644\n--- a/src/parsing/Parse_lockfile.ml\n+++ b/src/parsing/Parse_lockfile.ml\n@@ -1,13 +1,98 @@\n-let parse_lockfile :\n-    Lockfile.kind ->\n-    Lockfile_xtarget.manifest option ->\n-    Fpath.t ->\n-    SCA_dependency.t list = function\n-  (* TODO: add parsers, guard behind semgrep-pro  *)\n+(* Matthew McQuaid, Yoann Padioleau\n+ *\n+ * Copyright (C) 2024 Semgrep Inc.\n+ *\n+ * This library is free software; you can redistribute it and/or\n+ * modify it under the terms of the GNU Lesser General Public License\n+ * version 2.1 as published by the Free Software Foundation, with the\n+ * special exception on linking described in file LICENSE.\n+ *\n+ * This library is distributed in the hope that it will be useful, but\n+ * WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the file\n+ * LICENSE for more details.\n+ *)\n+open Fpath_.Operators\n+module Log = Log_parsing.Log\n+module J = JSON\n+\n+(*****************************************************************************)\n+(* Prelude *)\n+(*****************************************************************************)\n+(* Parsing a lockfile or manifest file.\n+ *\n+ * TODO: port lots of code from cli/src/semdep/parsers/\n+ *)\n+\n+(*****************************************************************************)\n+(* Error management *)\n+(*****************************************************************************)\n+\n+exception UnsupportedFormat of string\n+exception WrongFormat of string\n+\n+(*****************************************************************************)\n+(* Helpers *)\n+(*****************************************************************************)\n+\n+(* TODO: reuse recent matthew's parser in src/sca/ in semgrep-pro *)\n+let parse_npm_package_json (file : Fpath.t) : SCA_dependency.t list =\n+  (* TODO we should use our own json parser to get the location\n+   * information.\n+   *)\n+  let loc1 = Tok.first_loc_of_file file in\n+  let tok1 = Tok.first_tok_of_file file in\n+  let loc = (loc1, loc1) in\n+  let toks = [ tok1 ] in\n+\n+  UChan.with_open_in file (fun chan ->\n+      let json = JSON.json_of_chan chan in\n+      match json with\n+      (* just enough to parse some toy package-lock.json *)\n+      | J.Object\n+          [\n+            (\"requires\", _);\n+            (\"lockfileVersion\", J.Int 1);\n+            (\"dependencies\", J.Object deps);\n+          ] ->\n+          deps\n+          |> List_.map (fun (package_name, json) ->\n+                 match json with\n+                 | J.Object\n+                     [\n+                       (\"version\", J.String ver);\n+                       (\"resolved\", J.String url);\n+                       (\"integrity\", J.String _checksum);\n+                     ] ->\n+                     SCA_dependency.\n+                       {\n+                         package_name;\n+                         package_version = Parse_SCA_version.parse ver;\n+                         package_version_string = ver;\n+                         ecosystem = `Npm;\n+                         transitivity = `Unknown;\n+                         url = Uri_.of_string_opt url;\n+                         loc;\n+                         toks;\n+                       }\n+                 | _ -> raise (WrongFormat \"package-lock.json\"))\n+      | _ -> raise (WrongFormat \"package-lock.json\"))\n+\n+(*****************************************************************************)\n+(* Entry points *)\n+(*****************************************************************************)\n+\n+(* TODO: add parsers, guard behind semgrep-pro  *)\n+let parse (kind : Lockfile.kind)\n+    (_manifest_optTODO : Lockfile_xtarget.manifest option) (file : Fpath.t) :\n+    SCA_dependency.t list =\n+  Log.debug (fun m ->\n+      m \"parsing lockfile %s with kind %s\" !!file (Lockfile.show_kind kind));\n+  match kind with\n+  | NpmPackageLockJson -> parse_npm_package_json file\n   | PipRequirementsTxt\n   | PoetryLock\n   | PipfileLock\n-  | NpmPackageLockJson\n   | YarnLock\n   | PnpmLock\n   | GemfileLock\n@@ -22,7 +107,7 @@ let parse_lockfile :\n   | MixLock\n   | UvLock\n   | ConanLock ->\n-      fun _ _ -> []\n+      raise (UnsupportedFormat (Lockfile.show_kind kind))\n \n let parse_manifest :\n     Manifest.kind -> Fpath.t -> SCA_dependency.manifest_dependency list =\n@@ -44,5 +129,6 @@ let parse_manifest :\n   | `Pipfile\n   | `PyprojectToml\n   | `ConanFilePy\n-  | `ConanFileTxt ->\n+  | `ConanFileTxt\n+  | `Csproj ->\n       fun _ -> []\ndiff --git a/src/parsing/Parse_lockfile.mli b/src/parsing/Parse_lockfile.mli\nnew file mode 100644\nindex 000000000000..427046cb6dfc\n--- /dev/null\n+++ b/src/parsing/Parse_lockfile.mli\n@@ -0,0 +1,12 @@\n+exception WrongFormat of string\n+exception UnsupportedFormat of string\n+\n+(* may raise WrongFormat or UnsupportedFormat *)\n+val parse :\n+  Lockfile.kind ->\n+  Lockfile_xtarget.manifest option ->\n+  Fpath.t ->\n+  SCA_dependency.t list\n+\n+val parse_manifest :\n+  Manifest.kind -> Fpath.t -> SCA_dependency.manifest_dependency list\ndiff --git a/src/parsing/Parse_rule.ml b/src/parsing/Parse_rule.ml\nindex 852633a25351..f47de6ae90e9 100644\n--- a/src/parsing/Parse_rule.ml\n+++ b/src/parsing/Parse_rule.ml\n@@ -825,12 +825,13 @@ let parse_dependency_pattern key env value :\n   let/ rd = parse_dict env key value in\n   let/ ecosystem = take_key rd env parse_ecosystem \"namespace\" in\n   let/ package_name = take_key rd env parse_string \"package\" in\n+  let/ version_str = take_key rd env parse_string \"version\" in\n   let/ version_constraints =\n-    (* TODO: version parser *)\n-    take_key rd env parse_string \"version\"\n-    |> Result.map (fun _ ->\n-           SCA_pattern.SCA_And\n-             [ { version = SCA_version.Other \"not implemented\"; op = Eq } ])\n+    try Ok (Parse_SCA_version.parse_constraints version_str) with\n+    | Parse_SCA_version.Error error_str ->\n+        error_at_key env.id key\n+          (spf \"bad version constraint format for %s, error = %s\" version_str\n+             error_str)\n   in\n   Ok SCA_pattern.{ ecosystem; package_name; version_constraints }\n \n@@ -1224,7 +1225,7 @@ let parse_fake_xpattern xlang str =\n (* Useful for tests *)\n (*****************************************************************************)\n \n-let parse file =\n+let parse (file : Fpath.t) : (Rule.rules, Rule_error.t) result =\n   let/ xs, _skipped = parse_file ~error_recovery:false file in\n   (* The skipped rules include Apex rules and other rules that are always\n      skippable. *)\ndiff --git a/src/parsing/Parse_rule.mli b/src/parsing/Parse_rule.mli\nindex e79b049e77ac..e931b82cba99 100644\n--- a/src/parsing/Parse_rule.mli\n+++ b/src/parsing/Parse_rule.mli\n@@ -14,7 +14,7 @@\n val parse_and_filter_invalid_rules :\n   ?rewrite_rule_ids:(Rule_ID.t -> Rule_ID.t) ->\n   Fpath.t ->\n-  (Rule_error.rules_and_invalid, Rule_error.t) Result.t\n+  (Rule_error.rules_and_invalid, Rule_error.t) result\n \n (* This is used for parsing -e/-f extended patterns in Run_semgrep.ml\n  * and now also in osemgrep Config_resolver.ml.\n@@ -22,17 +22,16 @@ val parse_and_filter_invalid_rules :\n  * Error (Rule.InvalidRegexp _) for regexp errors.\n  *)\n val parse_xpattern :\n-  Xlang.t -> string Rule.wrap -> (Xpattern.t, Rule_error.t) Result.t\n+  Xlang.t -> string Rule.wrap -> (Xpattern.t, Rule_error.t) result\n \n-val parse_fake_xpattern :\n-  Xlang.t -> string -> (Xpattern.t, Rule_error.t) Result.t\n+val parse_fake_xpattern : Xlang.t -> string -> (Xpattern.t, Rule_error.t) result\n \n (* This should be used mostly in testing code. Otherwise you should\n  * use parse_and_filter_invalid_rules.\n  * This function may raise (Rule.Err ....) or Assert_failure (when\n  * there are invalid rules).\n  *)\n-val parse : Fpath.t -> (Rule.rules, Rule_error.t) Result.t\n+val parse : Fpath.t -> (Rule.rules, Rule_error.t) result\n \n (* Internals, used by osemgrep to setup a ojsonnet import hook.\n  * The filename parameter is just used in case of missing 'rules:'\n@@ -43,4 +42,4 @@ val parse_generic_ast :\n   ?rewrite_rule_ids:(Rule_ID.t -> Rule_ID.t) ->\n   Fpath.t ->\n   AST_generic.program ->\n-  (Rule_error.rules_and_invalid, Rule_error.t) Result.t\n+  (Rule_error.rules_and_invalid, Rule_error.t) result\ndiff --git a/src/parsing/Unit_parsing.ml b/src/parsing/Unit_parsing.ml\nindex bea4c29fc6e6..a553a69e949e 100644\n--- a/src/parsing/Unit_parsing.ml\n+++ b/src/parsing/Unit_parsing.ml\n@@ -181,7 +181,14 @@ let parsing_rules_tests () =\n      in\n      tests |> Fpath_.of_strings\n      |> List_.map (fun file ->\n-            t (Fpath.basename file) (fun () -> Parse_rule.parse file |> ignore)))\n+            t (Fpath.basename file) (fun () ->\n+                let res = Parse_rule.parse file in\n+                match res with\n+                | Ok _ -> ()\n+                | Error err ->\n+                    failwith\n+                      (spf \"error %s while parsing %s\" (Rule_error.show err)\n+                         !!file))))\n \n let parsing_rules_with_atd_tests () =\n   let dir = tests_path / \"rules_v2\" in\ndiff --git a/src/parsing/dune b/src/parsing/dune\nindex df78d8f53fb2..152cc015a853 100644\n--- a/src/parsing/dune\n+++ b/src/parsing/dune\n@@ -14,6 +14,7 @@\n    lib_parsing\n    process_limits\n    spacegrep\n+   uri ; for Parse_lockfile.ml\n \n    pfff-lang_GENERIC-naming\n \ndiff --git a/src/parsing_languages/Parse_pattern2.ml b/src/parsing_languages/Parse_pattern2.ml\nindex 17b6cabf1fbb..eb3bdf9b6210 100644\n--- a/src/parsing_languages/Parse_pattern2.ml\n+++ b/src/parsing_languages/Parse_pattern2.ml\n@@ -30,7 +30,7 @@ let parse_pattern options lang str =\n   (* coupling: update the files semgrep/js/languages/<lang>/Parser.ml\n      when updating this function.\n      TODO: Share the logic of which parser to try for each language to\n-     remove this coupling. https://github.com/returntocorp/semgrep/issues/8331\n+     remove this coupling. https://github.com/semgrep/semgrep/issues/8331\n   *)\n   match lang with\n   (* use adhoc parser (neither menhir nor tree-sitter) *)\ndiff --git a/src/reporting/Core_json_output.ml b/src/reporting/Core_json_output.ml\nindex b7b0cf54c6fd..96651be2e7ec 100644\n--- a/src/reporting/Core_json_output.ml\n+++ b/src/reporting/Core_json_output.ml\n@@ -377,11 +377,12 @@ let unsafe_match_to_match\n         fix =\n           Option.map (fun edit -> edit.Textedit.replacement_text) autofix_edit;\n         is_ignored;\n-        (* TODO *)\n         engine_kind = x.engine_of_match;\n         validation_state = Some x.validation_state;\n         historical_info;\n         extra_extra = None;\n+        (* TODO: use pm.sca_match! *)\n+        sca_match = None;\n       };\n   }\n \ndiff --git a/src/reporting/Unit_core_json_output.ml b/src/reporting/Unit_core_json_output.ml\nindex 2fc49d1cb450..cddcf8275790 100644\n--- a/src/reporting/Unit_core_json_output.ml\n+++ b/src/reporting/Unit_core_json_output.ml\n@@ -41,9 +41,10 @@ let make_core_match ?(check_id = \"fake-rule-id\") ?annotated_rule_id\n         severity = None;\n         metavars = [];\n         fix = None;\n-        dataflow_trace = None;\n         engine_kind = `OSS;\n+        dataflow_trace = None;\n         is_ignored = false;\n+        sca_match = None;\n         validation_state = None;\n         historical_info = None;\n         extra_extra = None;\ndiff --git a/src/rule/Rule.ml b/src/rule/Rule.ml\nindex 49b347c2712d..7305fd83e4b5 100644\n--- a/src/rule/Rule.ml\n+++ b/src/rule/Rule.ml\n@@ -103,7 +103,7 @@ and formula_kind =\n    * The same is true for pattern-not and pattern-not-inside\n    * (see tests/rules/negation_exact.yaml)\n    * todo: try to remove this at some point, but difficult. See\n-   * https://github.com/returntocorp/semgrep/issues/1218\n+   * https://github.com/semgrep/semgrep/issues/1218\n    *)\n   | Inside of tok * formula\n   (* alt: Could do this under a `where` (call it something like `also`).\ndiff --git a/src/rule/SCA_pattern.ml b/src/rule/SCA_pattern.ml\nindex 33d23cd4fa5f..3c4c79121b55 100644\n--- a/src/rule/SCA_pattern.ml\n+++ b/src/rule/SCA_pattern.ml\n@@ -1,6 +1,7 @@\n module Out = Semgrep_output_v1_t\n \n-type sca_operator = Eq | Gte | Lte | Gt | Lt [@@deriving show, eq]\n+type sca_operator = Eq | Gte | Lte | Gt | Lt\n+[@@deriving show { with_path = false }, eq]\n \n (* Something like (>= 2.0.0) or (== 5.1.7) *)\n type version_constraint = { op : sca_operator; version : SCA_version.t }\n@@ -25,4 +26,4 @@ type t = {\n  * union.\n  *)\n and version_constraints = SCA_And of version_constraint list\n-[@@deriving show, eq]\n+[@@deriving show { with_path = false }, eq]\ndiff --git a/src/rule/SCA_version.ml b/src/rule/SCA_version.ml\nindex 782796ab070f..49ac873d3a49 100644\n--- a/src/rule/SCA_version.ml\n+++ b/src/rule/SCA_version.ml\n@@ -1,4 +1,8 @@\n-(* The \"core of a version\": a dot separated list of numbers, like 4.1.6.2.7 *)\n+(* The \"core of a version\": a dot separated list of numbers, like 4.1.6.2.7\n+ * alt: we could inline it in V of { ... } which is allowed in modern OCaml\n+ * but Match_SCA_mode.ml has also functions operating just on this type\n+ * so simpler to have a separate type with a proper name.\n+ *)\n type core = { major : int; minor : int; incrementals : int list }\n [@@deriving show, eq]\n \n@@ -7,4 +11,4 @@ type t =\n   | V of core\n   (* Versions are sometimes listed as arbitrary strings, like a github URL *)\n   | Other of string\n-[@@deriving show, eq]\n+[@@deriving show { with_path = false }, eq]\ndiff --git a/src/sca/Manifest.ml b/src/sca/Manifest.ml\nindex f6400a65f30c..d299ffb182af 100644\n--- a/src/sca/Manifest.ml\n+++ b/src/sca/Manifest.ml\n@@ -44,21 +44,25 @@ let mk_manifest (kind : kind) (path : Fpath.t) : t = { path; kind }\n \n let kind_to_ecosystem_opt : kind -> Semgrep_output_v1_t.ecosystem option =\n   function\n-  | `RequirementsIn -> Some `Pypi\n+  | `RequirementsIn\n+  | `Pipfile\n+  | `PyprojectToml ->\n+      Some `Pypi\n   | `PackageJson -> Some `Npm\n   | `Gemfile -> Some `Gem\n   | `GoMod -> Some `Gomod\n   | `CargoToml -> Some `Cargo\n-  | `PomXml -> Some `Maven\n-  | `BuildGradle -> Some `Maven\n-  | `SettingsGradle -> Some `Maven\n+  | `PomXml\n+  | `BuildGradle\n+  | `SettingsGradle ->\n+      Some `Maven\n   | `ComposerJson -> Some `Composer\n-  | `NugetManifestJson -> Some `Nuget\n+  | `NugetManifestJson\n+  | `Csproj ->\n+      Some `Nuget\n   | `PubspecYaml -> Some `Pub\n   | `PackageSwift -> Some `SwiftPM\n   | `MixExs -> Some `Mix\n-  | `Pipfile -> Some `Pypi\n-  | `PyprojectToml -> Some `Pypi\n   | `ConanFilePy\n   | `ConanFileTxt ->\n       None\ndiff --git a/src/tainting/Dataflow_tainting.ml b/src/tainting/Dataflow_tainting.ml\nindex 7bcb86870d15..f5e17445cabe 100644\n--- a/src/tainting/Dataflow_tainting.ml\n+++ b/src/tainting/Dataflow_tainting.ml\n@@ -677,87 +677,19 @@ let check_orig_if_sink env ?filter_sinks orig taints shape =\n   let effects = effects_of_tainted_sinks env taints sinks in\n   record_effects env effects\n \n-let fix_poly_taint_with_field env lval xtaint =\n-  let type_of_il_offset il_offset =\n-    match il_offset.IL.o with\n-    | Dot n -> !(n.id_info.id_type)\n-    | Index _ -> None\n-  in\n-  let add_offset_to_lval o ({ offset; _ } as lval : T.lval) =\n-    if\n-      (* If the offset we are trying to take is already in the\n-           list of offsets, don't append it! This is so we don't\n-           never-endingly loop the dataflow and make it think the\n-           Arg taint is never-endingly changing.\n-\n-           For instance, this code example would previously loop,\n-           if `x` started with an `Arg` taint:\n-           while (true) { x = x.getX(); }\n-      *)\n-      (not (List.mem o offset))\n-      && (* For perf reasons we don't allow offsets to get too long.\n-          * Otherwise in a long chain of function calls where each\n-          * function adds some offset, we could end up a very large\n-          * amount of polymorphic taint.\n-          * This actually happened with rule\n-          * semgrep.perf.rules.express-fs-filename from the Pro\n-          * benchmarks, and file\n-          * WebGoat/src/main/resources/webgoat/static/js/libs/ace.js.\n-          *\n-          * TODO: This is way less likely to happen if we had better\n-          *   type info and we used it to remove taint, e.g. if Boolean\n-          *   and integer expressions didn't propagate taint. *)\n-      List.length offset < Limits_semgrep.taint_MAX_POLY_OFFSET\n-    then { lval with offset = lval.offset @ [ o ] }\n-    else lval\n-  in\n-  (* TODO: Aren't we missing here C# and Go ? *)\n-  if\n-    env.taint_inst.lang =*= Lang.Java\n-    || Lang.is_js env.taint_inst.lang\n-    || env.taint_inst.lang =*= Lang.Python\n-  then\n-    match xtaint with\n-    | `Sanitized\n-    | `Clean\n-    | `None ->\n-        xtaint\n-    | `Tainted taints -> (\n-        match lval.rev_offset with\n-        | o :: _ -> (\n-            match (type_of_il_offset o, T.offset_of_IL o) with\n-            | Some { t = TyFun _; _ }, _ ->\n-                (* We have an l-value like `o.f` where `f` has a function type,\n-                 * so it's a method call, we return nothing here. We cannot just\n-                 * return `xtaint`, which is the taint of `o` in the environment;\n-                 * whether that taint propagates or not is determined in\n-                 * 'check_tainted_instr'/'Call'. Otherwise, if `o` had taint var\n-                 * 'o@i', the call `o.getX()` would have taints '{o@i, o@i.x}'\n-                 * when it should only have taints '{o@i.x}'. *)\n-                `None\n-            | _, Oany ->\n-                (* Cannot handle this offset. *)\n-                xtaint\n-            | __any__, ((Ofld _ | Ostr _ | Oint _) as o) ->\n-                (* Not a method call (to the best of our knowledge) or\n-                 * an unresolved Java `getX` method. *)\n-                let taints' =\n-                  taints\n-                  |> Taints.map (fun taint ->\n-                         match taint.orig with\n-                         | Var lval ->\n-                             let lval' = add_offset_to_lval o lval in\n-                             { taint with orig = Var lval' }\n-                         | Shape_var lval ->\n-                             let lval' = add_offset_to_lval o lval in\n-                             { taint with orig = Shape_var lval' }\n-                         | Src _\n-                         | Control ->\n-                             taint)\n-                in\n-                `Tainted taints')\n-        | [] -> xtaint)\n-  else xtaint\n+let fix_poly_taint_with_field lval xtaint =\n+  match xtaint with\n+  | `Sanitized\n+  | `Clean\n+  | `None ->\n+      xtaint\n+  | `Tainted taints -> (\n+      match lval.rev_offset with\n+      | o :: _ ->\n+          let o = T.offset_of_IL o in\n+          let taints = Shape.fix_poly_taint_with_offset [ o ] taints in\n+          `Tainted taints\n+      | [] -> xtaint)\n \n (*****************************************************************************)\n (* Tainted *)\n@@ -1139,7 +1071,7 @@ and check_tainted_lval_aux env (lval : IL.lval) :\n                      * where `obj.y` is tainted but `obj.x` is not tainted, we will not\n                      * produce a finding.\n                   *)\n-                  fix_poly_taint_with_field env lval sub_xtaint\n+                  fix_poly_taint_with_field lval sub_xtaint\n             in\n             (xtaint', shape)\n       in\n@@ -1450,12 +1382,8 @@ let check_function_call env fun_exp args\n           m ~tags:sigs_tag \"Call to %s : %s\"\n             (Display_IL.string_of_exp fun_exp)\n             (Signature.show fun_sig));\n-      let check_lval lval =\n-        let taints, shape, _sub, _lval_env = check_tainted_lval env lval in\n-        (taints, shape)\n-      in\n       let* call_effects =\n-        Sig_inst.instantiate_function_signature env.lval_env ~check_lval fun_sig\n+        Sig_inst.instantiate_function_signature env.lval_env fun_sig\n           ~callee:fun_exp ~args:(Some args) args_taints\n       in\n       Some\n@@ -1487,8 +1415,8 @@ let check_function_call env fun_exp args\n                    (taints_acc, shape_acc, lval_env |> Lval_env.add lval taints))\n              (Taints.empty, Bot, env.lval_env))\n   | None ->\n-      Log.debug (fun m ->\n-          m ~tags:sigs_tag \"Call to %s : NO SIGNATURE !!!!\"\n+      Log.info (fun m ->\n+          m \"No taint signature found for `%s'\"\n             (Display_IL.string_of_exp fun_exp));\n       None\n \ndiff --git a/src/tainting/Shape_and_sig.ml b/src/tainting/Shape_and_sig.ml\nindex 8e589b72bf30..658581ac3af8 100644\n--- a/src/tainting/Shape_and_sig.ml\n+++ b/src/tainting/Shape_and_sig.ml\n@@ -82,58 +82,60 @@ module rec Shape : sig\n     | Bot  (** _|_, don't know or don't care *)\n     | Obj of obj\n         (** An \"object\" or struct-like thing.\n-          *\n-          * Tuples or lists are also represented by 'Obj' shapes! We just treat\n-          * constant indexes as if they were fields, and use 'Oany' to capture the\n-          * non-constant indexes.\n+\n+           Tuples or lists are also represented by 'Obj' shapes! We just treat\n+           constant indexes as if they were fields, and use 'Oany' to capture the\n+           non-constant indexes.\n           *)\n     | Arg of Taint.arg\n         (** Represents the yet-unknown shape of a function/method parameter. It is\n-          * a polymorphic shape variable that is meant to be instantiated at call\n-          * site. Before adding 'Arg' we assumed parameters had shape 'Bot', and\n-          * 'Arg' still acts like 'Bot' in some places. *)\n+            a polymorphic shape variable that is meant to be instantiated at call\n+            site. Before adding 'Arg' we assumed parameters had shape 'Bot', and\n+            'Arg' still acts like 'Bot' in some places.\n+\n+            TODO: Generalize to 'Taint.lval', e.g. `function test(o) { return o.x }`. *)\n     | Fun of Signature.t\n         (** Function shapes. These enable Semgrep to handle HOFs. *)\n \n   and cell =\n     | Cell of Xtaint.t * shape\n         (** A cell or \"reference\" represents the \"storage\" of a value, like\n-          * a variable in C.\n-          *\n-          * A cell may be explicitly tainted ('`Tainted'), not explicitly tainted\n-          * ('`None' / \"0\"),  or explicitly clean ('`Clean' / \"C\").\n-          *\n-          * A cell that is not explicitly tainted inherits any taints from \"parent\"\n-          * refs. A cell that is explicitly clean it is clean regardless.\n-          *\n-          * For example, given a variable `x` and the following statements:\n-          *\n-          *     x.a := \"taint\";\n-          *     x.a.u := \"clean\";\n-          *\n-          * We could assign the following shape to `x`:\n-          *\n-          *     Cell(`None, Obj {\n-          *             .a -> Cell({\"taint\"}, Obj {\n-          *                     .u -> Cell(`Clean, _|_)\n-          *                     })\n-          *             })\n-          *\n-          * We have that `x` itself has no taint directly assigned to it, but `x.a` is\n-          * tainted (by the string `\"taint\"`). Other fields like `x.b` are not tainted.\n-          * When it comes to `x.a`, we have that `x.a.u` has been explicitly marked clean,\n-          * so `x.a.u` will be considered clean despite `x.a` being tainted. Any other field\n-          * of `x.a` such as `x.a.v` will inherit the same taint as `x.a`.\n-          *\n-          * INVARIANT(cell): To keep shapes minimal:\n-          *   1. If the xtaint is '`None', then the shape is not 'Bot' and we can reach\n-          *      another 'cell' whose xtaint is either '`Tainted' or '`Clean'.\n-          *   2. If the xtaint is '`Clean', then the shape is 'Bot'.\n-          *      (If we add aliasing we may need to revisit this, and instead just mark\n-          *       every reachable 'cell' as clean too.)\n-          *\n-          * TODO: We can attach \"region ids\" to refs and assign taints to regions rather than\n-          *   to refs directly, then we can have alias analysis.\n+            a variable in C.\n+\n+            A cell may be explicitly tainted ('`Tainted'), not explicitly tainted\n+            ('`None' / \"0\"),  or explicitly clean ('`Clean' / \"C\").\n+\n+            A cell that is not explicitly tainted inherits any taints from \"parent\"\n+            refs. A cell that is explicitly clean it is clean regardless.\n+\n+            For example, given a variable `x` and the following statements:\n+\n+                x.a := \"taint\";\n+                x.a.u := \"clean\";\n+\n+            We could assign the following shape to `x`:\n+\n+                Cell(`None, Obj {\n+                        .a -> Cell({\"taint\"}, Obj {\n+                                .u -> Cell(`Clean, _|_)\n+                                })\n+                        })\n+\n+            We have that `x` itself has no taint directly assigned to it, but `x.a` is\n+            tainted (by the string `\"taint\"`). Other fields like `x.b` are not tainted.\n+            When it comes to `x.a`, we have that `x.a.u` has been explicitly marked clean,\n+            so `x.a.u` will be considered clean despite `x.a` being tainted. Any other field\n+            of `x.a` such as `x.a.v` will inherit the same taint as `x.a`.\n+\n+            INVARIANT(cell): To keep shapes minimal:\n+              1. If the xtaint is '`None', then the shape is not 'Bot' and we can reach\n+                 another 'cell' whose xtaint is either '`Tainted' or '`Clean'.\n+              2. If the xtaint is '`Clean', then the shape is 'Bot'.\n+                 (If we add aliasing we may need to revisit this, and instead just mark\n+                  every reachable 'cell' as clean too.)\n+\n+            TODO: We can attach \"region ids\" to refs and assign taints to regions rather than\n+              to refs directly, then we can have alias analysis.\n           *)\n \n   and obj = cell Fields.t\ndiff --git a/src/tainting/Sig_inst.ml b/src/tainting/Sig_inst.ml\nindex aef2caa343aa..f29c489414e0 100644\n--- a/src/tainting/Sig_inst.ml\n+++ b/src/tainting/Sig_inst.ml\n@@ -495,29 +495,6 @@ let lval_of_sig_lval (fun_exp : IL.exp) fparams args_exps (sig_lval : T.lval) :\n               { arg_lval with rev_offset = rev_offset @ arg_lval.rev_offset }\n             in\n             Some (lval, obj)\n-        | RecordOrDict fields, [ Ofld o ] -> (\n-            (* JS: The argument of a function call may be a record expression such as\n-             * `{x=\"tainted\", y=\"safe\"}`, if 'sig_lval' refers to the `x` field then\n-             * we want to resolve it to `\"tainted\"`. *)\n-            match\n-              fields\n-              |> List.find_opt (function\n-                   (* The 'o' is the offset that 'sig_lval' is referring to, here\n-                    * we look for a `fld=lval` field in the record object such that\n-                    * 'fld' has the same name as 'o'. *)\n-                   | Field (fld, _) -> IL.compare_name fld o =|= 0\n-                   | Entry _\n-                   | Spread _ ->\n-                       false)\n-            with\n-            | Some (Field (_, { e = Fetch ({ base = Var obj; _ } as lval); _ }))\n-              ->\n-                (* Actual argument is of the form {..., fld=lval, ...} and the offset is 'fld',\n-                 * we return 'lval'. *)\n-                Some (lval, obj)\n-            | Some _\n-            | None ->\n-                None)\n         | __else__ -> None)\n   in\n   Some (lval, snd obj.ident)\n@@ -628,15 +605,10 @@ let taints_of_lval lval_env fparams (fun_exp : IL.exp) args_taints lval :\n         let* (Cell (xtaints, shape)) = Lval_env.find_var lval_env var in\n         Some (Xtaint.to_taints xtaints, shape)\n   in\n-  match (base_shape, offset) with\n-  | base_shape, [] -> Some (base_taints, base_shape)\n-  | Bot, _ :: _ -> None\n-  | base_shape, _ :: _ ->\n-      let* (Cell (xtaints, shape)) = Shape.find_in_shape offset base_shape in\n-      Some (Xtaint.to_taints xtaints, shape)\n+  Shape.find_in_shape_poly ~taints:base_taints offset base_shape\n \n (* What is the taint denoted by 'sig_lval' ? *)\n-let taints_of_sig_lval lval_env ~check_lval fparams fun_exp args_exps\n+let taints_of_sig_lval lval_env fparams fun_exp args_exps\n     (args_taints : (Taints.t * shape) IL.argument list) (sig_lval : T.lval) =\n   match taints_of_lval lval_env fparams fun_exp args_taints sig_lval with\n   | Some (taints, shape) -> Some (taints, shape)\n@@ -657,7 +629,11 @@ let taints_of_sig_lval lval_env ~check_lval fparams fun_exp args_exps\n           let* lval, _obj =\n             lval_of_sig_lval fun_exp fparams args_exps sig_lval\n           in\n-          let lval_taints, shape = check_lval lval in\n+          let lval_taints, shape =\n+            match Lval_env.find_lval_poly lval_env lval with\n+            | None -> (Taints.empty, Bot)\n+            | Some (taints, shape) -> (taints, shape)\n+          in\n           let lval_taints =\n             lval_taints\n             |> fix_lval_taints_if_global_or_a_field_of_this_class fun_exp\n@@ -672,8 +648,8 @@ let taints_of_sig_lval lval_env ~check_lval fparams fun_exp args_exps\n    2) Are there any effects that occur within the function due to taints being\n       input into the function body, from the calling context?\n *)\n-let rec instantiate_function_signature lval_env ~check_lval\n-    (taint_sig : Signature.t) ~callee ~(args : _ option)\n+let rec instantiate_function_signature lval_env (taint_sig : Signature.t)\n+    ~callee ~(args : _ option)\n     (args_taints : (Taints.t * shape) IL.argument list) : call_effects option =\n   let lval_to_taints lval =\n     (* This function simply produces the corresponding taints to the\n@@ -687,8 +663,7 @@ let rec instantiate_function_signature lval_env ~check_lval\n        So we will isolate this as a specific step to be applied as necessary.\n     *)\n     let opt_taints_shape =\n-      taints_of_sig_lval lval_env ~check_lval taint_sig.params callee args\n-        args_taints lval\n+      taints_of_sig_lval lval_env taint_sig.params callee args args_taints lval\n     in\n     Log.debug (fun m ->\n         m ~tags:sigs_tag \"- Instantiating %s: %s -> %s\"\n@@ -882,8 +857,8 @@ let rec instantiate_function_signature lval_env ~check_lval\n               (Effect.show_args_taints fun_args_taints)\n               (Effect.show_args_taints args_taints));\n         match\n-          instantiate_function_signature lval_env ~check_lval fun_sig\n-            ~callee:fun_exp ~args:None args_taints\n+          instantiate_function_signature lval_env fun_sig ~callee:fun_exp\n+            ~args:None args_taints\n         with\n         | Some call_effects -> call_effects\n         | None ->\ndiff --git a/src/tainting/Sig_inst.mli b/src/tainting/Sig_inst.mli\nindex 96e8fbd6ca85..3f4adaefcbf2 100644\n--- a/src/tainting/Sig_inst.mli\n+++ b/src/tainting/Sig_inst.mli\n@@ -12,10 +12,6 @@ type call_effects = call_effect list\n \n val instantiate_function_signature :\n   Taint_lval_env.t ->\n-  check_lval:(IL.lval -> Taint.Taint_set.t * Shape_and_sig.Shape.shape) ->\n-  (* TODO: 'check_lval' is just a way to avoid a recursive dependency with\n-   *   'Dataflow_tainting'. We should not need this when all field-sensitive\n-   *   taint tracking happens through shapes. *)\n   Shape_and_sig.Signature.t ->\n   callee:IL.exp ->\n   args:IL.exp IL.argument list option (** actual arguments *) ->\ndiff --git a/src/tainting/Taint.ml b/src/tainting/Taint.ml\nindex 2c1b757f4774..96cefef6bfa6 100644\n--- a/src/tainting/Taint.ml\n+++ b/src/tainting/Taint.ml\n@@ -163,9 +163,10 @@ let show_offset offset =\n   | Ostr s -> Printf.sprintf \"[%s]\" s\n   | Oany -> \"[*]\"\n \n-let show_lval { base; offset = os } =\n-  show_base base\n-  ^ if os <> [] then os |> List_.map show_offset |> String.concat \"\" else \"\"\n+let show_offset_list offset =\n+  offset |> List_.map show_offset |> String.concat \"\"\n+\n+let show_lval { base; offset } = show_base base ^ show_offset_list offset\n \n let offset_of_IL (o : IL.offset) =\n   match !hook_offset_of_IL with\ndiff --git a/src/tainting/Taint.mli b/src/tainting/Taint.mli\nindex 4b5637e1dda6..df0fcc1d5b5d 100644\n--- a/src/tainting/Taint.mli\n+++ b/src/tainting/Taint.mli\n@@ -53,6 +53,7 @@ type offset =\n \n val compare_offset : offset -> offset -> int\n val show_offset : offset -> string\n+val show_offset_list : offset list -> string\n val offset_of_IL : IL.offset -> offset\n val offset_of_rev_IL_offset : rev_offset:IL.offset list -> offset list\n \n@@ -64,7 +65,12 @@ val rev_IL_offset_of_offset : offset list -> IL.offset list option\n   * (if there is no `Oany`).\n   *)\n \n-type lval = { base : base; offset : offset list }\n+type lval = {\n+  base : base;\n+  offset : offset list;\n+      (** An offset `.a.b.c` is encoded as `[.a; .b; .c]`, note the difference\n+          wrt 'IL.offset', this offset list is **not** reversed! *)\n+}\n (** A restriction of 'IL.lval', the l-values that are in the scope of a\n  * function/method, and on which we can track taint:\n  *\ndiff --git a/src/tainting/Taint_lval_env.ml b/src/tainting/Taint_lval_env.ml\nindex d34ced7d2325..94eac2566182 100644\n--- a/src/tainting/Taint_lval_env.ml\n+++ b/src/tainting/Taint_lval_env.ml\n@@ -105,19 +105,6 @@ let union le1 le2 =\n \n let union_list ?(default = empty) les = List.fold_left union default les\n \n-(* HACK: Because we don't have a \"Class\" type, classes have themselves as types. *)\n-let is_class_name (name : IL.name) =\n-  match (!(name.id_info.id_resolved), !(name.id_info.id_type)) with\n-  | Some resolved1, Some { t = TyN (Id (_, { id_resolved; _ })); _ } -> (\n-      match !id_resolved with\n-      | None -> false\n-      | Some resolved2 ->\n-          (* If 'name' has type 'name' then we assume it's a class. *)\n-          AST_generic.equal_resolved_name resolved1 resolved2)\n-  | _, None\n-  | _, Some _ ->\n-      false\n-\n (* Reduces an l-value into the form x.a_1. ... . a_N, the resulting l-value may\n  * not represent the exact same object as the original l-value, but an\n  * overapproximation. For example, the normalized l-value of `x[i]` will be `x`,\n@@ -132,9 +119,14 @@ let normalize_lval lval =\n         Some (x, rev_offset)\n     | Var name -> (\n         match rev_offset with\n-        (* static class field, `C.x`, we normalize it to just `x` since `x` is\n-         * a unique global *)\n-        | [ { o = IL.Dot var; _ } ] when is_class_name name -> Some (var, [])\n+        (* Static class field, `C.x`, we normalize it to just `x` since `x` is\n+           a unique global.\n+\n+           TODO: C.x.y ? *)\n+        | [ { o = IL.Dot var; _ } ]\n+          when H.is_class_name name || IdFlags.is_static !(var.id_info.id_flags)\n+          ->\n+            Some (var, [])\n         | __else__ -> Some (name, rev_offset))\n     (* explicit dereference of `this` e.g. `this->x` *)\n     | Mem { e = Fetch { base = VarSpecial (This, _); rev_offset = [] }; _ }\n@@ -254,7 +246,16 @@ let find_var { tainted; _ } var = NameMap.find_opt var tainted\n let find_lval { tainted; _ } lval =\n   let* var, offsets = normalize_lval lval in\n   let* var_ref = NameMap.find_opt var tainted in\n-  Shape.find_in_cell offsets var_ref\n+  match Shape.find_in_cell offsets var_ref with\n+  | `Clean\n+  | `Not_found _ ->\n+      None\n+  | `Found cell -> Some cell\n+\n+let find_lval_poly { tainted; _ } lval =\n+  let* var, offsets = normalize_lval lval in\n+  let* var_ref = NameMap.find_opt var tainted in\n+  Shape.find_in_cell_poly offsets var_ref\n \n let find_lval_xtaint env lval =\n   match find_lval env lval with\ndiff --git a/src/tainting/Taint_lval_env.mli b/src/tainting/Taint_lval_env.mli\nindex f3ffb21f3898..6b7e21da3e74 100644\n--- a/src/tainting/Taint_lval_env.mli\n+++ b/src/tainting/Taint_lval_env.mli\n@@ -65,6 +65,9 @@ val find_var : env -> IL.name -> cell option\n val find_lval : env -> IL.lval -> cell option\n (** Find the 'cell' of an l-value. *)\n \n+val find_lval_poly : env -> IL.lval -> (Taint.taints * shape) option\n+(** Find the 'cell' of an l-value. *)\n+\n val find_lval_xtaint : env -> IL.lval -> Xtaint.t\n (** Look up an l-value on the environemnt and return whether it's tainted, clean,\n     or we hold no info about it. It does not check sub-lvalues, e.g. if we record\ndiff --git a/src/tainting/Taint_rule_inst.ml b/src/tainting/Taint_rule_inst.ml\nindex fe8206f7e8b4..c963020cef8c 100644\n--- a/src/tainting/Taint_rule_inst.ml\n+++ b/src/tainting/Taint_rule_inst.ml\n@@ -46,7 +46,7 @@ type spec_predicates = {\n        *\n        * Propagators allow to specify how taint propagates through side effects.\n        *\n-       * Note that we tried to solve this with a hack in returntocorp/semgrep#5150\n+       * Note that we tried to solve this with a hack in semgrep/semgrep#5150\n        * but it caused a bunch of FPs in semgrep-rules. The hack was essentially\n        * to assume that in `x.f(y)` taint always propagated from `y` to `x`.\n        *\ndiff --git a/src/tainting/Taint_shape.ml b/src/tainting/Taint_shape.ml\nindex 4237355e00e9..8ebe8074adff 100644\n--- a/src/tainting/Taint_shape.ml\n+++ b/src/tainting/Taint_shape.ml\n@@ -65,6 +65,82 @@ let taints_and_shape_are_relevant taints shape =\n        * by INVARIANT(cell) it contains some taint or has field marked clean. *)\n       true\n \n+(* TODO: This should fix shapes too. *)\n+let fix_poly_taint_with_offset offset taints =\n+  let type_of_offset o =\n+    match o with\n+    | T.Ofld n -> !(n.id_info.id_type)\n+    | _ -> None\n+  in\n+  let add_offset_to_lval o ({ offset; _ } as orig_lval : T.lval) =\n+    let extended_lval = { orig_lval with offset = orig_lval.offset @ [ o ] } in\n+    if\n+      (* If the offset we are trying to take is already in the\n+           list of offsets, don't append it! This is so we don't\n+           never-endingly loop the dataflow and make it think the\n+           Arg taint is never-endingly changing.\n+\n+           For instance, this code example would previously loop,\n+           if `x` started with an `Arg` taint:\n+           while (true) { x = x.getX(); }\n+      *)\n+      (not (List.mem o offset))\n+      && (* For perf reasons we don't allow offsets to get too long.\n+          * Otherwise in a long chain of function calls where each\n+          * function adds some offset, we could end up a very large\n+          * amount of polymorphic taint.\n+          * This actually happened with rule\n+          * semgrep.perf.rules.express-fs-filename from the Pro\n+          * benchmarks, and file\n+          * WebGoat/src/main/resources/webgoat/static/js/libs/ace.js.\n+          *\n+          * TODO: This is way less likely to happen if we had better\n+          *   type info and we used it to remove taint, e.g. if Boolean\n+          *   and integer expressions didn't propagate taint. *)\n+      List.length offset < Limits_semgrep.taint_MAX_POLY_OFFSET\n+    then extended_lval\n+    else (\n+      Log.warn (fun m ->\n+          m \"Taint_lval_env.fix_poly_taint_with_offset: %s is too long\"\n+            (T.show_lval extended_lval));\n+      orig_lval)\n+  in\n+  offset\n+  |> List.fold_left\n+       (fun taints o ->\n+         match (type_of_offset o, o) with\n+         | Some { t = TyFun _; _ }, _ ->\n+             (* We have an l-value like `o.f` where `f` has a function type,\n+              * so it's a method call, we return nothing here. We cannot just\n+              * return `xtaint`, which is the taint of `o` in the environment;\n+              * whether that taint propagates or not is determined in\n+              * 'check_tainted_instr'/'Call'. Otherwise, if `o` had taint var\n+              * 'o@i', the call `o.getX()` would have taints '{o@i, o@i.x}'\n+              * when it should only have taints '{o@i.x}'. *)\n+             Taints.empty\n+         | _, Oany ->\n+             (* Cannot handle this offset. *)\n+             taints\n+         | __any__, ((Ofld _ | Ostr _ | Oint _) as o) ->\n+             (* Not a method call (to the best of our knowledge) or\n+              * an unresolved Java `getX` method. *)\n+             let taints' =\n+               taints\n+               |> Taints.map (fun taint ->\n+                      match taint.orig with\n+                      | Var lval ->\n+                          let lval' = add_offset_to_lval o lval in\n+                          { taint with orig = Var lval' }\n+                      | Shape_var lval ->\n+                          let lval' = add_offset_to_lval o lval in\n+                          { taint with orig = Shape_var lval' }\n+                      | Src _\n+                      | Control ->\n+                          taint)\n+             in\n+             taints')\n+       taints\n+\n (*********************************************************)\n (* Unification (merging shapes) *)\n (*********************************************************)\n@@ -241,54 +317,90 @@ let gather_all_taints_in_shape = gather_all_taints_in_shape_acc Taints.empty\n (* Find an offset *)\n (*********************************************************)\n \n-let rec find_in_cell offset cell =\n-  let (Cell (_xtaint, shape)) = cell in\n+let rec find_in_cell_w_carry ~taints offset cell =\n+  let (Cell (xtaint, shape)) = cell in\n   match offset with\n-  | [] -> Some cell\n-  | _ :: _ -> find_in_shape offset shape\n-\n-and find_in_shape offset shape =\n+  | [] -> `Found cell\n+  | _ :: _ -> (\n+      match xtaint with\n+      | `Clean ->\n+          if shape <> Bot then\n+            Log.err (fun m ->\n+                m \"BUG: Taint_shape.find_in_cell: INVARIANT(cell).2 is broken\");\n+          `Clean\n+      | `None -> find_in_shape_w_carry ~taints offset shape\n+      | `Tainted taints -> find_in_shape_w_carry ~taints offset shape)\n+\n+and find_in_shape_w_carry ~taints offset shape =\n+  let not_found = `Not_found (taints, shape, offset) in\n   match shape with\n   (* offset <> [] *)\n-  | Bot -> None\n-  | Obj obj -> find_in_obj offset obj\n+  | Bot -> not_found\n+  | Obj obj -> find_in_obj_w_carry ~taints offset obj\n   | Arg _ ->\n       (* TODO: Here we should \"refine\" the arg shape, it should be an Obj shape. *)\n       Log.debug (fun m ->\n           m \"Could not find offset %s in polymorphic shape %s\"\n             (debug_offset offset) (show_shape shape));\n-      None\n+      not_found\n   | Fun _ ->\n       (* This is an error, we just don't want to crash here. *)\n       Log.err (fun m ->\n           m \"Could not find offset %s in function shape %s\"\n             (debug_offset offset) (show_shape shape));\n-      None\n+      not_found\n \n-and find_in_obj (offset : T.offset list) obj =\n+and find_in_obj_w_carry ~taints (offset : T.offset list) obj =\n+  let not_found = `Not_found (taints, Obj obj, offset) in\n   (* offset <> [] *)\n   match offset with\n   | [] ->\n-      Log.err (fun m -> m \"fix_xtaint_obj: Impossible happened: empty offset\");\n-      None\n+      Log.err (fun m -> m \"BUG: Taint_shape.fix_xtaint_obj: empty offset\");\n+      not_found\n   | o :: offset -> (\n       match o with\n-      | Oany (* arbitrary index [*] *) ->\n+      | Oany (* arbitrary index [*] *) -> (\n           (* consider all fields/indexes *)\n-          Fields.fold\n-            (fun _ cell acc ->\n-              match (acc, find_in_cell offset cell) with\n-              | None, None -> None\n-              | Some cell, None\n-              | None, Some cell ->\n-                  Some cell\n-              | Some cell1, Some cell2 -> Some (unify_cell cell1 cell2))\n-            obj None\n+          match\n+            Fields.fold\n+              (fun _ cell acc ->\n+                match (acc, find_in_cell_w_carry ~taints offset cell) with\n+                | None, (`Not_found _ | `Clean) -> None\n+                | Some cell, (`Not_found _ | `Clean)\n+                | None, `Found cell ->\n+                    Some cell\n+                | Some cell1, `Found cell2 -> Some (unify_cell cell1 cell2))\n+              obj None\n+          with\n+          | None -> not_found\n+          | Some cell -> `Found cell)\n       | Ofld _\n       | Oint _\n-      | Ostr _ ->\n-          let* o_cell = Fields.find_opt o obj in\n-          find_in_cell offset o_cell)\n+      | Ostr _ -> (\n+          match Fields.find_opt o obj with\n+          | None -> not_found\n+          | Some o_cell -> find_in_cell_w_carry ~taints offset o_cell))\n+\n+let find_in_cell offset cell =\n+  find_in_cell_w_carry ~taints:Taints.empty offset cell\n+\n+let option_of_find_result res =\n+  match res with\n+  | `Clean -> None\n+  | `Not_found (taints, _shape, offset) ->\n+      (* TODO: Fix _shape too. *)\n+      let taints = fix_poly_taint_with_offset offset taints in\n+      Some (taints, Bot)\n+  | `Found (Cell (xtaint, shape)) -> Some (Xtaint.to_taints xtaint, shape)\n+\n+let find_in_cell_poly offset cell =\n+  find_in_cell offset cell |> option_of_find_result\n+\n+let find_in_shape_poly ~taints offset shape =\n+  match offset with\n+  | [] -> Some (taints, shape)\n+  | _ :: _ ->\n+      find_in_shape_w_carry ~taints offset shape |> option_of_find_result\n \n (*********************************************************)\n (* Update the xtaint and shape of an offset *)\ndiff --git a/src/tainting/Taint_shape.mli b/src/tainting/Taint_shape.mli\nindex f82f0e135a16..a2586441602f 100644\n--- a/src/tainting/Taint_shape.mli\n+++ b/src/tainting/Taint_shape.mli\n@@ -6,6 +6,13 @@ val taints_and_shape_are_relevant : Taint.taints -> shape -> bool\n (** [true] iff the union of [taints] and [gather_all_taints_in_shape shape]\n  * is non-empty, or if [shape] contains a cleaned offset. *)\n \n+val fix_poly_taint_with_offset :\n+  Taint.offset list -> Taint.taints -> Taint.taints\n+(** Fix taints with an offset. It just attaches the offset to each polymorphic\n+    taint variable (see 'Taint.Var') in the set.\n+\n+    FEATURE(field-sensitivity) *)\n+\n val tuple_like_obj : (Taint.taints * shape) list -> shape\n (** Constructs a 0-indexed tuple-like 'obj' from a list of pairs, taints and shape,\n  * for each element in the tuple.  *)\n@@ -33,8 +40,41 @@ val gather_all_taints_in_cell : cell -> Taint.taints\n val gather_all_taints_in_shape : shape -> Taint.taints\n (** Gather and union all taints reachable through a shape. *)\n \n-val find_in_cell : Taint.offset list -> cell -> cell option\n-val find_in_shape : Taint.offset list -> shape -> cell option\n+val find_in_cell :\n+  Taint.offset list ->\n+  cell ->\n+  [ `Found of cell\n+  | `Clean\n+  | `Not_found of Taint.taints * shape * Taint.offset list ]\n+(** Find an offset in a cell.\n+\n+    This is a somewhat \"low-level\" version.\n+\n+    If the offset could not be found in the cell, then it returns `Not_found\n+    with the base taints and shape for the offset prefix that was found, and\n+    the offset suffix that was not. *)\n+\n+val find_in_cell_poly :\n+  Taint.offset list -> cell -> (Taint.taints * shape) option\n+(** Find an offset in a cell, BUT if the full offset cannot be found, then\n+    it returns the taints of the offset prefix that was found; and if those\n+    taints are polymorphic, then it adds to them the remaining offset.\n+\n+    For example, if `x` is tainted but `x.a` is not being tracked, it just\n+    assigns to `x.a` the same taints as `x`. If `x` had polymorphic taint\n+    (see 'Taint.Var'), then it would attach the offset `.a` to it.\n+\n+    TODO: We need to fix polymorphic shapes too instad of just returning 'Bot'.\n+\n+    FEATURE(field-sensitivity) *)\n+\n+val find_in_shape_poly :\n+  taints:Taint.taints ->\n+  Taint.offset list ->\n+  shape ->\n+  (Taint.taints * shape) option\n+(** Like 'find_in_cell_poly' but to find an offset in a shape, the 'taints'\n+    are the \"base taints\" in case the offset cannot be found. *)\n \n val update_offset_in_cell :\n   f:(Xtaint.t -> shape -> Xtaint.t * shape) ->\ndiff --git a/src/target/Target.ml b/src/target/Target.ml\nindex ff56f2eb6dac..336bb87b5255 100644\n--- a/src/target/Target.ml\n+++ b/src/target/Target.ml\n@@ -101,7 +101,7 @@ let mk_regular ?lockfile analyzer products (origin : Origin.t) : regular =\n let mk_target (xlang : Xlang.t) (file : Fpath.t) : t =\n   let all = Product.all in\n   (* TODO: should do the check in the other mk_xxx ? *)\n-  assert (UFile.is_file file);\n+  assert (UFile.is_reg ~follow_symlinks:true file);\n   Regular (mk_regular xlang all (Origin.File file))\n \n (*****************************************************************************)\ndiff --git a/src/targeting/Find_targets.ml b/src/targeting/Find_targets.ml\nindex 75ddc76fea08..6e4fea580486 100644\n--- a/src/targeting/Find_targets.ml\n+++ b/src/targeting/Find_targets.ml\n@@ -585,7 +585,7 @@ let group_scanning_roots_by_files_and_projects (conf : conf)\n   let dir_scanning_roots, file_scanning_roots =\n     List.partition\n       (fun x ->\n-        UFile.dir_exists\n+        UFile.is_dir ~follow_symlinks:true\n           (Fpath.append (Fpath.v (Sys.getcwd ())) (Scanning_root.to_fpath x)))\n       scanning_roots\n   in\ndiff --git a/stats/autofix-printing-stats/projects.txt b/stats/autofix-printing-stats/projects.txt\nindex 1c3b208b0190..2aacfe62814a 100644\n--- a/stats/autofix-printing-stats/projects.txt\n+++ b/stats/autofix-printing-stats/projects.txt\n@@ -1,1 +1,1 @@\n-https://github.com/returntocorp/semgrep-rules.git\n+https://github.com/semgrep/semgrep-rules.git\ndiff --git a/stats/parsing-stats/lang/ocaml/projects.txt b/stats/parsing-stats/lang/ocaml/projects.txt\nindex 384d531af459..4776361081b8 100644\n--- a/stats/parsing-stats/lang/ocaml/projects.txt\n+++ b/stats/parsing-stats/lang/ocaml/projects.txt\n@@ -2,8 +2,7 @@\n # one per line.\n \n # R2C'S OCaml projects\n-https://github.com/returntocorp/semgrep\n-https://github.com/returntocorp/pfff\n+https://github.com/semgrep/semgrep\n \n # Projects with more than 1000 stars on GitHub.\n # (we could add more, we're looking for large amounts of handwritten\ndiff --git a/tools/otarzan/README.md b/tools/otarzan/README.md\nindex a59e5bfc45ab..e51d06eab3a5 100644\n--- a/tools/otarzan/README.md\n+++ b/tools/otarzan/README.md\n@@ -13,7 +13,7 @@ boilerplate that a human can start from and would like to further edit\n (e.g., for AST_to_IL.ml in semgrep, for AST_to_SAST.ml in semgrep-pro).\n Otarzan is such a tool.\n \n-See https://github.com/returntocorp/semgrep/issues/6440 for more\n+See https://github.com/semgrep/semgrep/issues/6440 for more\n context on this work.\n \n alternatives:\n", "instance_id": "semgrep__semgrep-10724", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `ruamel.yaml` library due to a lack of exact version pinning, which leads to inconsistent behavior in Semgrep's YAML parsing depending on the installed patch version. It provides detailed reproduction steps, expected behavior, and links to related issues, which help in understanding the problem's context and impact. However, there are minor ambiguities: the problem statement does not explicitly define the desired solution (e.g., whether to pin to a specific version or implement a different dependency management strategy), and it lacks detailed discussion of potential edge cases or constraints (e.g., compatibility issues with other dependencies or environments). Additionally, the connection between the described issue and the extensive code changes provided (which include updates to Docker images, GitHub workflows, and more) is not immediately clear, as the changes seem to address broader updates beyond just version pinning.", "difficulty_explanation": "The difficulty of solving the core issue described\u2014pinning the `ruamel.yaml` library to a specific version\u2014is relatively low, falling into the easy category. It primarily involves a straightforward modification to the dependency specification in `cli/setup.py` (as seen in the code changes from `ruamel.yaml>=0.16.0,<0.18` to `ruamel.yaml>=0.18.5`), requiring basic understanding of Python dependency management and Semgrep's build configuration. However, the provided code changes are extensive, covering updates to GitHub workflows, Docker image references, and other unrelated modifications (e.g., renaming repository references from `returntocorp/semgrep` to `semgrep/semgrep`, updating various dependency versions, and adding new features like lazy AST parsing). These additional changes inflate the scope but do not directly pertain to the core issue of version pinning. For the specific task of addressing the `ruamel.yaml` bug, the effort is minimal, involving a single file change with no significant edge cases or architectural impact. The broader changes in the diff, while more complex, are not considered in this difficulty assessment as they appear tangential to the primary problem statement. Thus, a score of 0.25 reflects the simplicity of the core fix, acknowledging minor complexity in understanding the dependency context within Semgrep's setup.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "djangocms-versioning draft deletion causes plugins to disappear from pages\nWhen I use djangocms-alias with djangocms-versioning and I delete a draft of an alias, the alias does not get reset to the previous version on the pages using it. Instead, it gets removed entirely from all pages.\r\n\r\nSteps to reproduce:\r\n1. Create a Page and publish it\r\n2. Create a Content Alias on that page and publish it\r\n3. Create a new draft for the Content Alias\r\n4. Delete the draft\r\n\r\nI think the problem is rooted in this [line](https://github.com/django-cms/djangocms-alias/blob/master/djangocms_alias/models.py#L308) in `models.AliasContent.delete`. From the code, it looks like this is wanted behavior because it deliberately deletes all plugins using the deleted Alias Content. Hence I am not sure how to fix it (just removing it works for me).\r\n\r\nI use django-cms 4.1, and I was able to reproduces this behavior in all djangocms-alias versions I tried, including the latest master.\n", "patch": "diff --git a/djangocms_alias/admin.py b/djangocms_alias/admin.py\nindex d28e415..daac102 100644\n--- a/djangocms_alias/admin.py\n+++ b/djangocms_alias/admin.py\n@@ -1,6 +1,6 @@\n from cms.utils.permissions import get_model_permission_codename\n from cms.utils.urlutils import admin_reverse\n-from django.contrib import admin\n+from django.contrib import admin, messages\n from django.db.models.functions import Lower\n from django.template.loader import render_to_string\n from django.utils.translation import gettext_lazy as _\n@@ -168,6 +168,17 @@ def delete_model(self, request, obj):\n         if not is_versioning_enabled():\n             emit_content_delete([obj], sender=self.model)\n \n+        if obj.alias._default_manager.filter(language=obj.language).count() == 1:\n+            message = _(\n+                \"Alias content for language {} deleted. A new empty alias content will be created if needed.\"\n+            ).format(obj.language)\n+            self.message_user(request, message, level=messages.WARNING)\n+\n+        return super().delete_model(\n+            request=request,\n+            obj=obj,\n+        )\n+\n     def get_list_actions(self):\n         \"\"\"\n         Collect rendered actions from implemented methods and return as list\ndiff --git a/djangocms_alias/models.py b/djangocms_alias/models.py\nindex d63099e..f027b70 100644\n--- a/djangocms_alias/models.py\n+++ b/djangocms_alias/models.py\n@@ -305,11 +305,6 @@ def get_absolute_url(self):\n     def get_template(self):\n         return \"djangocms_alias/alias_content.html\"\n \n-    @transaction.atomic\n-    def delete(self, *args, **kwargs):\n-        super().delete(*args, **kwargs)\n-        self.alias.cms_plugins.filter(language=self.language).delete()\n-\n     @transaction.atomic\n     def populate(self, replaced_placeholder=None, replaced_plugin=None, plugins=None):\n         if not replaced_placeholder and not replaced_plugin:\n", "instance_id": "django-cms__djangocms-alias-233", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when a draft of a Content Alias is deleted in djangocms-alias with djangocms-versioning, the alias is removed from pages instead of reverting to a previous version. The steps to reproduce are provided, which helps in understanding the context and replicating the issue. Additionally, the user points to a specific line in the codebase as the potential root cause, showing some level of investigation. However, there are minor ambiguities and missing details. For instance, the expected behavior is implied (reverting to the previous version) but not explicitly stated as a requirement. Edge cases, such as what should happen if there are no previous versions or if multiple drafts exist, are not mentioned. Constraints or specific versioning behaviors in django-cms are also not detailed, which could impact the solution. Overall, the problem is valid and mostly clear, but it lacks comprehensive details on expected outcomes and edge cases.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively limited, affecting primarily two files (`admin.py` and `models.py`) with a small number of lines modified or removed. The changes involve removing a destructive deletion behavior in `models.py` and adding a warning message in `admin.py`, which suggests a straightforward fix at the surface level. However, the problem requires understanding specific technical concepts, including Django's model system, the interaction between djangocms-alias and djangocms-versioning, and the implications of deleting draft content in a versioned environment. This necessitates familiarity with Django's ORM, transaction handling (`@transaction.atomic`), and the plugin system in django-cms. Additionally, while the problem statement does not explicitly mention edge cases, the nature of versioning and alias management implies potential complexities, such as handling scenarios where no previous version exists or ensuring consistency across pages using the alias. The solution may also require deeper investigation into the intended behavior of the versioning system to avoid unintended side effects, as the user notes uncertainty about whether the deletion behavior is intentional. The changes do not appear to impact the broader system architecture significantly, but they do require careful consideration of data integrity and user experience (e.g., providing appropriate feedback via messages). Overall, this problem balances moderate complexity in understanding domain-specific logic and interactions with a relatively contained scope of code changes, warranting a difficulty score of 0.50.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Build abi3 wheels for Python 3.11+\nSome initial playing around with getting [Stable ABI](https://docs.python.org/3/c-api/stable.html) builds to work. The benefit would be that if you publish wheels with `cp3N-abi3`, then any Python versions >= 3.N can install the same wheel without your having to do anything apart from fix compatibilities with the new Python versions.\r\n\r\nLooking at the symbols indexed_gzip uses, `PyObject_GetBuffer` was only added to the stable ABI in 3.11, so that sets a lower bound on ABI3 wheels.\r\n\r\nTo get it to compile, I also needed to depend on cython/cython#5871. Leaving as draft for now, since this seems like a Bad Idea to code a dependency on in your `pyproject.toml`. `abi3audit` also complains about the use of `Py_DECREF`; I've removed them here, but we'd need to wait on https://github.com/cython/cython/issues/6063 for it to be entirely clean.\r\n\r\nI tried to modify your setup.py defensively, so that it would only attempt to build an ABI3 build if both Python and Cython were new enough. I have tested, and I'm able to install a wheel built with Python 3.11 on Python 3.12.\n", "patch": "diff --git a/.ci/download_zlib.sh b/.ci/download_zlib.sh\nindex 4f5a27d3..c003d731 100755\n--- a/.ci/download_zlib.sh\n+++ b/.ci/download_zlib.sh\n@@ -4,7 +4,7 @@\n #\n set -e\n \n-ZLIB_VERSION=1.3\n+ZLIB_VERSION=1.3.1\n \n curl -o zlib.tar.gz https://www.zlib.net/zlib-${ZLIB_VERSION}.tar.gz\n \ndiff --git a/.github/dependabot.yml b/.github/dependabot.yml\nnew file mode 100644\nindex 00000000..d13cfde6\n--- /dev/null\n+++ b/.github/dependabot.yml\n@@ -0,0 +1,14 @@\n+version: 2\n+updates:\n+  - package-ecosystem: \"github-actions\"\n+    directory: \"/\"\n+    schedule:\n+      interval: \"monthly\"\n+    groups:\n+      actions-infrastructure:\n+        patterns:\n+          - \"actions/*\"\n+      third-party:\n+        patterns:\n+          - \"docker/*\"\n+          - \"pypa/*\"\ndiff --git a/.github/workflows/main.yaml b/.github/workflows/main.yaml\nindex 8b41deb6..15fcf1b3 100644\n--- a/.github/workflows/main.yaml\n+++ b/.github/workflows/main.yaml\n@@ -14,7 +14,6 @@ defaults:\n   run:\n     shell: bash\n \n-\n jobs:\n \n   # Quick tests run on all OSes/python versions\n@@ -37,12 +36,11 @@ jobs:\n       ENV_DIR:        ./test.env\n \n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n       - name: Set up Python ${{ matrix.python-version }}\n-        uses: actions/setup-python@v2\n+        uses: actions/setup-python@v5\n         with:\n           python-version: ${{ matrix.python-version }}\n-          architecture: x64\n       - name: Create test environment\n         run:  bash ./.ci/create_test_env.sh \"$ENV_DIR\"\n       - name: Log test environment\n@@ -108,12 +106,11 @@ jobs:\n       ENV_DIR:        ./test.env\n \n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n       - name: Set up Python\n-        uses: actions/setup-python@v2\n+        uses: actions/setup-python@v5\n         with:\n           python-version: \"3.10\"\n-          architecture:   x64\n       - name: Create test environment\n         run:  bash ./.ci/create_test_env.sh \"$ENV_DIR\"\n       - name: Log test environment\n@@ -220,46 +217,11 @@ jobs:\n       ENV_DIR:        ./test.env\n \n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n       - name: Set up Python\n-        uses: actions/setup-python@v2\n+        uses: actions/setup-python@v5\n         with:\n           python-version: \"3.10\"\n-          architecture:   \"x64\"\n-      - name: Create test environment\n-        run:  bash ./.ci/create_test_env.sh \"$ENV_DIR\"\n-      - name: Log test environment\n-        run: |\n-          source ./.ci/activate_env.sh \"$ENV_DIR\"\n-          python -V\n-          pip freeze\n-      - name: Build indexed_gzip\n-        run:  bash ./.ci/build_dev_indexed_gzip.sh \"$ENV_DIR\"\n-      - name: Run tests\n-        run:  bash  ./.ci/run_tests.sh \"$ENV_DIR\"\n-\n-\n-  # Tests on 32 bit platform\n-  test-32bit:\n-    runs-on: ubuntu-latest\n-    container: i386/ubuntu:20.04\n-    strategy:\n-      matrix:\n-        python-version: [\"3.9\"]\n-        extra-args:     [\"\", \"--concat\"]\n-\n-    env:\n-      USING_OS_PYTHON: 1\n-      PYTHON_VERSION:  ${{ matrix.python-version }}\n-      EXTRA_ARGS:      ${{ matrix.extra-args }}\n-      ENV_DIR:         ./test.env\n-\n-    steps:\n-      - name: Install git\n-        run:  apt-get update -y && apt-get install -y git\n-      - uses: actions/checkout@v1\n-      - name: Install system dependencies\n-        run:  bash ./.ci/install_32bit_dependencies.sh\n       - name: Create test environment\n         run:  bash ./.ci/create_test_env.sh \"$ENV_DIR\"\n       - name: Log test environment\ndiff --git a/.github/workflows/pull_request.yaml b/.github/workflows/pull_request.yaml\nindex 892f3a1e..53d5d30a 100644\n--- a/.github/workflows/pull_request.yaml\n+++ b/.github/workflows/pull_request.yaml\n@@ -28,12 +28,11 @@ jobs:\n       ENV_DIR:        ./test.env\n \n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n       - name: Set up Python ${{ matrix.python-version }}\n-        uses: actions/setup-python@v2\n+        uses: actions/setup-python@v5\n         with:\n           python-version: ${{ matrix.python-version }}\n-          architecture: x64\n       - name: Create test environment\n         run:  bash ./.ci/create_test_env.sh \"$ENV_DIR\"\n       - name: Log test environment\n@@ -48,38 +47,3 @@ jobs:\n         run:  bash ./.ci/build_dev_indexed_gzip.sh \"$ENV_DIR\"\n       - name: Run tests\n         run:  bash  ./.ci/run_tests.sh \"$ENV_DIR\"\n-\n-\n-  # Quick tests on 32 bit platform\n-  test-32bit:\n-    runs-on: ubuntu-latest\n-    container: i386/ubuntu:20.04\n-    strategy:\n-      matrix:\n-        python-version: [\"3.9\"]\n-        extra-args:     [\"\", \"--concat\"]\n-\n-    env:\n-      USING_OS_PYTHON: 1\n-      PYTHON_VERSION:  ${{ matrix.python-version }}\n-      TEST_SUITE:      \"not slow_test\"\n-      EXTRA_ARGS:      ${{ matrix.extra-args }}\n-      ENV_DIR:         ./test.env\n-\n-    steps:\n-      - name: Install git\n-        run:  apt-get update -y && apt-get install -y git\n-      - uses: actions/checkout@v1\n-      - name: Install system dependencies\n-        run:  bash ./.ci/install_32bit_dependencies.sh\n-      - name: Create test environment\n-        run:  bash ./.ci/create_test_env.sh \"$ENV_DIR\"\n-      - name: Log test environment\n-        run: |\n-          source ./.ci/activate_env.sh \"$ENV_DIR\"\n-          python -V\n-          pip freeze\n-      - name: Build indexed_gzip\n-        run:  bash ./.ci/build_dev_indexed_gzip.sh \"$ENV_DIR\"\n-      - name: Run tests\n-        run:  bash  ./.ci/run_tests.sh \"$ENV_DIR\"\ndiff --git a/.github/workflows/release.yaml b/.github/workflows/release.yaml\nindex 8e3f0dcc..8b47e5dc 100644\n--- a/.github/workflows/release.yaml\n+++ b/.github/workflows/release.yaml\n@@ -16,9 +16,9 @@ jobs:\n     name:    Build source distribution\n     runs-on: ubuntu-latest\n     steps:\n-      - uses: actions/checkout@v2\n+      - uses: actions/checkout@v4\n \n-      - uses: actions/setup-python@v2\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n           python-version: \"3.10\"\n@@ -26,7 +26,7 @@ jobs:\n       - name: Build sdist\n         run:  python setup.py sdist\n \n-      - uses: actions/upload-artifact@v2\n+      - uses: actions/upload-artifact@v4\n         with:\n           name: sdist\n           path: ./dist/*.tar.gz\n@@ -40,8 +40,8 @@ jobs:\n       CIBW_ARCHS_MACOS: \"x86_64 arm64 universal2\"\n \n     steps:\n-      - uses: actions/checkout@v2\n-      - uses: actions/setup-python@v2\n+      - uses: actions/checkout@v4\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n           python-version: \"3.10\"\n@@ -49,7 +49,7 @@ jobs:\n       - name: Build wheels\n         run:  bash ./.ci/build_wheels.sh\n \n-      - uses: actions/upload-artifact@v2\n+      - uses: actions/upload-artifact@v4\n         with:\n           name: wheels\n           path: ./dist/*.whl\n@@ -67,8 +67,8 @@ jobs:\n       CIBW_ARCHS_WINDOWS: ${{ matrix.arch }}\n \n     steps:\n-      - uses: actions/checkout@v2\n-      - uses: actions/setup-python@v2\n+      - uses: actions/checkout@v4\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n           python-version: \"3.10\"\n@@ -76,7 +76,7 @@ jobs:\n         run:  bash ./.ci/download_zlib.sh\n       - name: Build wheels\n         run:  bash ./.ci/build_wheels.sh\n-      - uses: actions/upload-artifact@v2\n+      - uses: actions/upload-artifact@v4\n         with:\n           name: wheels\n           path: ./dist/*.whl\n@@ -95,17 +95,17 @@ jobs:\n       CIBW_ARCHS_LINUX: ${{ matrix.arch }}\n \n     steps:\n-      - uses: actions/checkout@v2\n-      - uses: actions/setup-python@v2\n+      - uses: actions/checkout@v4\n+      - uses: actions/setup-python@v5\n         name: Install Python\n         with:\n           python-version: \"3.10\"\n       - name: Set up QEMU for emulated (e.g. ARM) builds\n         if:   ${{ matrix.arch == 'aarch64' }}\n-        uses: docker/setup-qemu-action@v1\n+        uses: docker/setup-qemu-action@v3\n       - name: Build wheels\n         run:  bash ./.ci/build_wheels.sh\n-      - uses: actions/upload-artifact@v2\n+      - uses: actions/upload-artifact@v4\n         with:\n           name: wheels\n           path: ./dist/*.whl\n@@ -118,18 +118,18 @@ jobs:\n     steps:\n \n       - name: Download source archive\n-        uses: actions/download-artifact@v2\n+        uses: actions/download-artifact@v4\n         with:\n           name: sdist\n           path: dist/\n       - name: Download wheel archives\n-        uses: actions/download-artifact@v2\n+        uses: actions/download-artifact@v4\n         with:\n           name: wheels\n           path: dist/\n \n       - name: Publish archives to PyPI\n-        uses: pypa/gh-action-pypi-publish@v1.4.1\n+        uses: pypa/gh-action-pypi-publish@v1.9.0\n         with:\n           user:     __token__\n           password: ${{ secrets.PYPI_TOKEN }}\n", "instance_id": "pauldmccarthy__indexed_gzip-145", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to build ABI3 wheels for Python 3.11+ using the Stable ABI, with a specific mention of compatibility benefits and constraints (e.g., `PyObject_GetBuffer` setting a lower bound at Python 3.11). It also provides context about dependencies (e.g., Cython issues) and defensive modifications to `setup.py`. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input/output for the build process, nor does it detail specific edge cases or failure modes to handle (e.g., what happens if Python or Cython versions are insufficient). Additionally, the mention of unresolved issues (e.g., `Py_DECREF` usage and waiting on a Cython fix) introduces uncertainty about the completeness of the solution requirements. Overall, while the goal is understandable, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is significant but not architectural; the provided diffs primarily update CI/CD workflows, dependency versions (e.g., zlib from 1.3 to 1.3.1), and GitHub Actions configurations across multiple files, rather than core logic or system architecture. However, implementing ABI3 wheels requires a deep understanding of Python's C API, specifically the Stable ABI, and compatibility constraints across Python versions (3.11+), which is a specialized and complex domain. The problem also involves technical concepts like Cython dependencies, wheel building with `cibuildwheel`, and managing cross-platform builds (Windows, macOS, Linux with different architectures), which demand familiarity with build systems and Python packaging. Additionally, while edge cases are not explicitly mentioned in the problem statement, ensuring compatibility across Python versions and handling potential build failures or version mismatches introduces moderate complexity in error handling. The need to defensively modify `setup.py` and address unresolved issues (e.g., `Py_DECREF` usage flagged by `abi3audit`) further increases the challenge. Overall, this problem requires a solid grasp of Python internals and build tooling, along with careful consideration of compatibility, making it harder than a medium-difficulty task but not reaching the extreme complexity of system-level or highly domain-specific problems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Amazon SES: support extra_headers, metadata, tags for template sends\nOriginally, AWS's `ses::SendBulkEmail` API didn't allow specifying email headers. Since Anymail's Amazon SES backend also uses custom headers for `metadata` and `tags`, this meant you [couldn't use](https://anymail.dev/en/stable/esps/amazon_ses/#batch-sending-merge-and-esp-templates) any of the following message options together with a `template_id`:\r\n\r\n- `headers` (a.k.a. `extra_headers`)\r\n- `metadata` or `merge_metadata`\r\n- `tags` (except a single tag when using `AMAZON_SES_MESSAGE_TAG_NAME`)\r\n\r\nIn March, [AWS added a new `ReplacementHeaders` parameter](https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-ses-headers-sending-email/) for `ses::SendBulkEmail`, which allows per-recipient custom headers. In early May, the [new parameter was made available in boto3](https://github.com/boto/boto3/blob/1.34.98/CHANGELOG.rst#L14).\r\n\r\nAnymail should use SES's new ReplacementHeaders to support extra_headers, metadata, merge_metadata, and tags in template sends.\r\n\r\n(Related: #371)\n", "patch": "diff --git a/CHANGELOG.rst b/CHANGELOG.rst\nindex 29f5d08..f93e9e7 100644\n--- a/CHANGELOG.rst\n+++ b/CHANGELOG.rst\n@@ -48,6 +48,10 @@ Features\n   headers with template sends. (Requires boto3 >= 1.34.98.)\n   (Thanks to `@carrerasrodrigo`_ the implementation.)\n \n+* **Amazon SES:** Allow extra headers, ``metadata``, ``merge_metadata``,\n+  and ``tags`` when sending with a ``template_id``.\n+  (Requires boto3 v1.34.98 or later.)\n+\n \n v10.3\n -----\ndiff --git a/anymail/backends/amazon_ses.py b/anymail/backends/amazon_ses.py\nindex 041297a..5c62943 100644\n--- a/anymail/backends/amazon_ses.py\n+++ b/anymail/backends/amazon_ses.py\n@@ -2,6 +2,8 @@\n import email.encoders\n import email.policy\n \n+from requests.structures import CaseInsensitiveDict\n+\n from .. import __version__ as ANYMAIL_VERSION\n from ..exceptions import AnymailAPIError, AnymailImproperlyInstalled\n from ..message import AnymailRecipientStatus\n@@ -339,10 +341,14 @@ class AmazonSESV2SendBulkEmailPayload(AmazonSESBasePayload):\n \n     def init_payload(self):\n         super().init_payload()\n-        # late-bind recipients and merge_data in finalize_payload\n+        # late-bind in finalize_payload:\n         self.recipients = {\"to\": [], \"cc\": [], \"bcc\": []}\n         self.merge_data = {}\n+        self.headers = {}\n         self.merge_headers = {}\n+        self.metadata = {}\n+        self.merge_metadata = {}\n+        self.tags = []\n \n     def finalize_payload(self):\n         # Build BulkEmailEntries from recipients and merge_data.\n@@ -372,11 +378,26 @@ def finalize_payload(self):\n                 },\n             }\n \n-            if len(self.merge_headers) > 0:\n-                entry[\"ReplacementHeaders\"] = [\n-                    {\"Name\": key, \"Value\": value}\n-                    for key, value in self.merge_headers.get(to.addr_spec, {}).items()\n+            replacement_headers = []\n+            if self.headers or to.addr_spec in self.merge_headers:\n+                headers = CaseInsensitiveDict(self.headers)\n+                headers.update(self.merge_headers.get(to.addr_spec, {}))\n+                replacement_headers += [\n+                    {\"Name\": key, \"Value\": value} for key, value in headers.items()\n+                ]\n+            if self.metadata or to.addr_spec in self.merge_metadata:\n+                metadata = self.metadata.copy()\n+                metadata.update(self.merge_metadata.get(to.addr_spec, {}))\n+                if metadata:\n+                    replacement_headers.append(\n+                        {\"Name\": \"X-Metadata\", \"Value\": self.serialize_json(metadata)}\n+                    )\n+            if self.tags:\n+                replacement_headers += [\n+                    {\"Name\": \"X-Tag\", \"Value\": tag} for tag in self.tags\n                 ]\n+            if replacement_headers:\n+                entry[\"ReplacementHeaders\"] = replacement_headers\n             self.params[\"BulkEmailEntries\"].append(entry)\n \n     def parse_recipient_status(self, response):\n@@ -446,7 +467,7 @@ def set_reply_to(self, emails):\n             self.params[\"ReplyToAddresses\"] = [email.address for email in emails]\n \n     def set_extra_headers(self, headers):\n-        self.unsupported_feature(\"extra_headers with template\")\n+        self.headers = headers\n \n     def set_text_body(self, body):\n         if body:\n@@ -468,27 +489,26 @@ def set_envelope_sender(self, email):\n         self.params[\"FeedbackForwardingEmailAddress\"] = email.addr_spec\n \n     def set_metadata(self, metadata):\n-        # no custom headers with SendBulkEmail\n-        self.unsupported_feature(\"metadata with template\")\n+        self.metadata = metadata\n+\n+    def set_merge_metadata(self, merge_metadata):\n+        self.merge_metadata = merge_metadata\n \n     def set_tags(self, tags):\n-        # no custom headers with SendBulkEmail, but support\n-        # AMAZON_SES_MESSAGE_TAG_NAME if used (see tags/metadata in\n-        # AmazonSESV2SendEmailPayload for more info)\n-        if tags:\n-            if self.backend.message_tag_name is not None:\n-                if len(tags) > 1:\n-                    self.unsupported_feature(\n-                        \"multiple tags with the AMAZON_SES_MESSAGE_TAG_NAME setting\"\n-                    )\n-                self.params[\"DefaultEmailTags\"] = [\n-                    {\"Name\": self.backend.message_tag_name, \"Value\": tags[0]}\n-                ]\n-            else:\n+        self.tags = tags\n+\n+        # Also *optionally* pass a single Message Tag if the AMAZON_SES_MESSAGE_TAG_NAME\n+        # Anymail setting is set (default no). The AWS API restricts tag content in this\n+        # case. (This is useful for dashboard segmentation; use esp_extra[\"Tags\"] for\n+        # anything more complex.)\n+        if tags and self.backend.message_tag_name is not None:\n+            if len(tags) > 1:\n                 self.unsupported_feature(\n-                    \"tags with template (unless using the\"\n-                    \" AMAZON_SES_MESSAGE_TAG_NAME setting)\"\n+                    \"multiple tags with the AMAZON_SES_MESSAGE_TAG_NAME setting\"\n                 )\n+            self.params[\"DefaultEmailTags\"] = [\n+                {\"Name\": self.backend.message_tag_name, \"Value\": tags[0]}\n+            ]\n \n     def set_template_id(self, template_id):\n         # DefaultContent.Template.TemplateName\ndiff --git a/docs/esps/amazon_ses.rst b/docs/esps/amazon_ses.rst\nindex 73639a6..507682c 100644\n--- a/docs/esps/amazon_ses.rst\n+++ b/docs/esps/amazon_ses.rst\n@@ -68,6 +68,11 @@ setting to customize the Boto session.\n Limitations and quirks\n ----------------------\n \n+.. versionchanged:: 11.0\n+\n+    Anymail's :attr:`~anymail.message.AnymailMessage.merge_metadata`\n+    is now supported.\n+\n **Hard throttling**\n   Like most ESPs, Amazon SES `throttles sending`_ for new customers. But unlike\n   most ESPs, SES does not queue and slowly release throttled messages. Instead, it\n@@ -80,11 +85,6 @@ Limitations and quirks\n   :attr:`~anymail.message.AnymailMessage.tags` feature. See :ref:`amazon-ses-tags`\n   below for more information and additional options.\n \n-**No merge_metadata**\n-  Amazon SES's batch sending API does not support the custom headers Anymail uses\n-  for metadata, so Anymail's :attr:`~anymail.message.AnymailMessage.merge_metadata`\n-  feature is not available. (See :ref:`amazon-ses-tags` below for more information.)\n-\n **Open and click tracking overrides**\n   Anymail's :attr:`~anymail.message.AnymailMessage.track_opens` and\n   :attr:`~anymail.message.AnymailMessage.track_clicks` are not supported.\n@@ -126,7 +126,7 @@ Limitations and quirks\n   signal, and using it will likely prevent delivery of your email.)\n \n **Template limitations**\n-  Messages sent with templates have a number of additional limitations, such as not\n+  Messages sent with templates have some additional limitations, such as not\n   supporting attachments. See :ref:`amazon-ses-templates` below.\n \n \n@@ -195,12 +195,7 @@ characters.\n \n For more complex use cases, set the SES ``EmailTags`` parameter (or ``DefaultEmailTags``\n for template sends) directly in Anymail's :ref:`esp_extra <amazon-ses-esp-extra>`. See\n-the example below. (Because custom headers do not work with SES's SendBulkEmail call,\n-esp_extra ``DefaultEmailTags`` is the only way to attach data to SES messages also using\n-Anymail's :attr:`~anymail.message.AnymailMessage.template_id` and\n-:attr:`~anymail.message.AnymailMessage.merge_data` features, and\n-:attr:`~anymail.message.AnymailMessage.merge_metadata` cannot be supported.)\n-\n+the example below.\n \n .. _Introducing Sending Metrics:\n     https://aws.amazon.com/blogs/ses/introducing-sending-metrics/\n@@ -264,9 +259,10 @@ See Amazon's `Sending personalized email`_ guide for more information.\n When you set a message's :attr:`~anymail.message.AnymailMessage.template_id`\n to the name of one of your SES templates, Anymail will use the SES v2 `SendBulkEmail`_\n call to send template messages personalized with data\n-from Anymail's normalized :attr:`~anymail.message.AnymailMessage.merge_data`\n-and :attr:`~anymail.message.AnymailMessage.merge_global_data`\n-message attributes.\n+from Anymail's normalized :attr:`~anymail.message.AnymailMessage.merge_data`,\n+:attr:`~anymail.message.AnymailMessage.merge_global_data`,\n+:attr:`~anymail.message.AnymailMessage.merge_metadata`, and\n+:attr:`~anymail.message.AnymailMessage.merge_headers` message attributes.\n \n   .. code-block:: python\n \n@@ -284,17 +280,21 @@ message attributes.\n           'ship_date': \"May 15\",\n       }\n \n-Amazon's templated email APIs don't support several features available for regular email.\n+Amazon's templated email APIs don't support a few features available for regular email.\n When :attr:`~anymail.message.AnymailMessage.template_id` is used:\n \n-* Attachments and alternative parts (including AMPHTML) are not supported\n-* Extra headers are not supported\n+* Attachments and inline images are not supported\n+* Alternative parts (including AMPHTML) are not supported\n * Overriding the template's subject or body is not supported\n-* Anymail's :attr:`~anymail.message.AnymailMessage.metadata` is not supported\n-* Anymail's :attr:`~anymail.message.AnymailMessage.tags` are only supported\n-  with the :setting:`AMAZON_SES_MESSAGE_TAG_NAME <ANYMAIL_AMAZON_SES_MESSAGE_TAG_NAME>`\n-  setting; only a single tag is allowed, and the tag is not directly available\n-  to webhooks. (See :ref:`amazon-ses-tags` above.)\n+\n+.. versionchanged:: 11.0\n+\n+    Extra headers, :attr:`~anymail.message.AnymailMessage.metadata`,\n+    :attr:`~anymail.message.AnymailMessage.merge_metadata`, and\n+    :attr:`~anymail.message.AnymailMessage.tags` are now fully supported\n+    when using :attr:`~anymail.message.AnymailMessage.template_id`.\n+    (This requires :pypi:`boto3` v1.34.98 or later, which enables the\n+    ReplacementHeaders parameter for SendBulkEmail.)\n \n .. _Sending personalized email:\n    https://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-personalized-email-api.html\ndiff --git a/pyproject.toml b/pyproject.toml\nindex d91c329..2d2716e 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -65,7 +65,7 @@ dependencies = [\n # ESP-specific additional dependencies.\n # (For simplicity, requests is included in the base dependencies.)\n # (Do not use underscores in extra names: they get normalized to hyphens.)\n-amazon-ses = [\"boto3\"]\n+amazon-ses = [\"boto3>=1.10.17\"]\n brevo = []\n mailersend = []\n mailgun = []\n", "instance_id": "anymail__django-anymail-378", "clarity": 3, "difficulty": 0.45, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly outlines the goal of adding support for `extra_headers`, `metadata`, `merge_metadata`, and `tags` in Amazon SES template sends using the newly introduced `ReplacementHeaders` parameter in the `ses::SendBulkEmail` API. The statement provides context about the original limitation, references the AWS update and boto3 changelog, and links to related issues for additional background. Inputs, outputs, and constraints are implicitly clear through the context of the API and the provided code changes. There are no significant ambiguities, and the inclusion of references to documentation and related issues adds to the clarity. The problem statement also aligns well with the code changes, ensuring that the intent is fully understandable. Therefore, it merits a score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. \n\n1. **Scope and Depth of Code Changes:** The changes are primarily localized to a single file (`anymail/backends/amazon_ses.py`), with minor updates to documentation (`docs/esps/amazon_ses.rst`), changelog (`CHANGELOG.rst`), and dependency version in `pyproject.toml`. The core modifications involve updating the `AmazonSESV2SendBulkEmailPayload` class to handle new parameters (`headers`, `metadata`, `merge_metadata`, `tags`) and integrating them into the `ReplacementHeaders` structure for the SES API. While the changes are not architecturally significant, they require careful integration with existing logic for recipient handling and payload construction, affecting multiple methods within the class.\n\n2. **Technical Concepts Involved:** Solving this requires familiarity with Python, specifically with handling dictionaries and custom data structures (`CaseInsensitiveDict` from `requests.structures`), JSON serialization, and interacting with AWS SES via the `boto3` library. Additionally, understanding the SES API's `SendBulkEmail` call and the new `ReplacementHeaders` parameter is necessary, which involves domain-specific knowledge of email sending and AWS services. The concepts are not overly complex for an experienced developer but do require a moderate level of expertise in API integration and email protocols.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes handle scenarios like merging global and per-recipient headers/metadata, ensuring no conflicts or data loss. The implementation also retains compatibility with an optional single-tag setting (`AMAZON_SES_MESSAGE_TAG_NAME`), which shows attention to backward compatibility. Error handling logic appears to be minimally impacted, as the focus is on feature addition rather than exception management, though care must be taken to avoid introducing errors in header serialization or API parameter construction.\n\n4. **Overall Complexity:** The task requires understanding the existing codebase's structure for email payload construction and extending it to support new features. While the changes are not trivial, they do not involve deep architectural refactoring or complex algorithm design. The primary challenge lies in correctly mapping Anymail's normalized message attributes to the SES API's expected format, which is a moderately complex integration task.\n\nGiven these considerations, a difficulty score of 0.45 is appropriate, reflecting a medium-level problem that requires understanding multiple concepts and making targeted, non-trivial modifications across a few files, with some attention to compatibility and edge cases but without significant architectural impact or advanced technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Installation on CI takes very long time (25 minutes!) due to dependency resolution issues\nSee https://github.com/jupyterlab/jupyterlab-git/actions/runs/9092321766/job/24988787136?pr=1327\r\n\r\n![image](https://github.com/jupyterlab/jupyterlab-git/assets/5832902/16242865-12d0-4a21-bc24-a0591592b4b6)\r\n\r\nIt seems that possibly `notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0` is the problem as this conflicts with jupyter-server.\r\n\r\nI think it is only required for tests, which are skipped:\r\n\r\nhttps://github.com/jupyterlab/jupyterlab-git/blob/d4b0f639e56855635b3358b1b2415a39d154394a/jupyterlab_git/tests/test_hybridcontents.py#L11-L15\r\n\r\nSince on the main branch this extension is only compatible with Lab 4.x and Notebook 7.x and hence jupyter-server v2 I propose to drop `hybridcontents` test dependency and possibly remove these tests as https://github.com/viaduct-ai/hybridcontents is still only compatible with old notebook, and there is a cost for tests running for so long (especially that integration tests only kick in after build is complete).\r\n\r\nAny thoughts @Zsailer @fcollonval?\r\n\r\n<details>\r\n\r\n```\r\n+ python -m pip install '.[dev,test]'\r\nProcessing /home/runner/work/jupyterlab-git/jupyterlab-git\r\n  Installing build dependencies: started\r\n  Installing build dependencies: finished with status 'done'\r\n  Getting requirements to build wheel: started\r\n  Getting requirements to build wheel: finished with status 'done'\r\n  Installing backend dependencies: started\r\n  Installing backend dependencies: finished with status 'done'\r\n  Preparing metadata (pyproject.toml): started\r\n  Preparing metadata (pyproject.toml): finished with status 'done'\r\nRequirement already satisfied: jupyter-server<3,>=2.0.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab_git==0.50.0) (2.14.0)\r\nCollecting nbdime~=4.0.1 (from jupyterlab_git==0.50.0)\r\n  Downloading nbdime-4.0.1-py3-none-any.whl.metadata (9.5 kB)\r\nRequirement already satisfied: nbformat in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab_git==0.50.0) (5.10.4)\r\nRequirement already satisfied: packaging in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab_git==0.50.0) (24.0)\r\nRequirement already satisfied: pexpect in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab_git==0.50.0) (4.9.0)\r\nRequirement already satisfied: traitlets~=5.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab_git==0.50.0) (5.14.3)\r\nCollecting black (from jupyterlab_git==0.50.0)\r\n  Downloading black-24.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (77 kB)\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.1/77.1 kB 3.8 MB/s eta 0:00:00\r\nRequirement already satisfied: jupyterlab~=4.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab_git==0.50.0) (4.2.0)\r\nCollecting pre-commit (from jupyterlab_git==0.50.0)\r\n  Downloading pre_commit-3.5.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\nCollecting coverage (from jupyterlab_git==0.50.0)\r\n  Downloading coverage-7.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\r\nCollecting hybridcontents (from jupyterlab_git==0.50.0)\r\n  Downloading hybridcontents-0.4.0.tar.gz (15 kB)\r\n  Installing build dependencies: started\r\n  Installing build dependencies: finished with status 'done'\r\n  Getting requirements to build wheel: started\r\n  Getting requirements to build wheel: finished with status 'done'\r\n  Installing backend dependencies: started\r\n  Installing backend dependencies: finished with status 'done'\r\n  Preparing metadata (pyproject.toml): started\r\n  Preparing metadata (pyproject.toml): finished with status 'done'\r\nCollecting jupytext (from jupyterlab_git==0.50.0)\r\n  Downloading jupytext-1.16.2-py3-none-any.whl.metadata (13 kB)\r\nCollecting pytest (from jupyterlab_git==0.50.0)\r\n  Downloading pytest-8.2.0-py3-none-any.whl.metadata (7.5 kB)\r\nCollecting pytest-asyncio (from jupyterlab_git==0.50.0)\r\n  Downloading pytest_asyncio-0.23.6-py3-none-any.whl.metadata (3.9 kB)\r\nCollecting pytest-cov (from jupyterlab_git==0.50.0)\r\n  Downloading pytest_cov-5.0.0-py3-none-any.whl.metadata (27 kB)\r\nCollecting pytest-jupyter>=0.6.0 (from pytest-jupyter[server]>=0.6.0; extra == \"test\"->jupyterlab_git==0.50.0)\r\n  Downloading pytest_jupyter-0.10.1-py3-none-any.whl.metadata (9.3 kB)\r\nRequirement already satisfied: anyio>=3.1.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (4.3.0)\r\nRequirement already satisfied: argon2-cffi>=21.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (23.1.0)\r\nRequirement already satisfied: jinja2>=3.0.3 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (3.1.4)\r\nRequirement already satisfied: jupyter-client>=7.4.4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (8.6.1)\r\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (5.7.2)\r\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.10.0)\r\nRequirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.5.3)\r\nRequirement already satisfied: nbconvert>=6.4.4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (7.16.4)\r\nRequirement already satisfied: overrides>=5.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (7.7.0)\r\nRequirement already satisfied: prometheus-client>=0.9 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.20.0)\r\nRequirement already satisfied: pyzmq>=24 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (26.0.3)\r\nRequirement already satisfied: send2trash>=1.8.2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.8.3)\r\nRequirement already satisfied: terminado>=0.8.3 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.18.1)\r\nRequirement already satisfied: tornado>=6.2.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (6.4)\r\nRequirement already satisfied: websocket-client>=1.7 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.8.0)\r\nRequirement already satisfied: async-lru>=1.0.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.0.4)\r\nRequirement already satisfied: httpx>=0.25.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.27.0)\r\nRequirement already satisfied: importlib-metadata>=4.8.3 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (7.1.0)\r\nRequirement already satisfied: importlib-resources>=1.4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (6.4.0)\r\nRequirement already satisfied: ipykernel>=6.5.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (6.29.4)\r\nRequirement already satisfied: jupyter-lsp>=2.0.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.2.5)\r\nRequirement already satisfied: jupyterlab-server<3,>=2.27.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.27.1)\r\nRequirement already satisfied: notebook-shim>=0.2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.2.4)\r\nRequirement already satisfied: tomli>=1.2.2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.0.1)\r\nCollecting colorama (from nbdime~=4.0.1->jupyterlab_git==0.50.0)\r\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\r\nCollecting gitpython!=2.1.4,!=2.1.5,!=2.1.6 (from nbdime~=4.0.1->jupyterlab_git==0.50.0)\r\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\r\nCollecting jupyter-server-mathjax>=0.2.2 (from nbdime~=4.0.1->jupyterlab_git==0.50.0)\r\n  Downloading jupyter_server_mathjax-0.2.6-py3-none-any.whl.metadata (2.1 kB)\r\nRequirement already satisfied: pygments in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbdime~=4.0.1->jupyterlab_git==0.50.0) (2.18.0)\r\nRequirement already satisfied: requests in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbdime~=4.0.1->jupyterlab_git==0.50.0) (2.31.0)\r\nRequirement already satisfied: fastjsonschema>=2.15 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbformat->jupyterlab_git==0.50.0) (2.19.1)\r\nRequirement already satisfied: jsonschema>=2.6 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbformat->jupyterlab_git==0.50.0) (4.22.0)\r\nCollecting iniconfig (from pytest->jupyterlab_git==0.50.0)\r\n  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting pluggy<2.0,>=1.5 (from pytest->jupyterlab_git==0.50.0)\r\n  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\r\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from pytest->jupyterlab_git==0.50.0) (1.2.1)\r\nCollecting click>=8.0.0 (from black->jupyterlab_git==0.50.0)\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting mypy-extensions>=0.4.3 (from black->jupyterlab_git==0.50.0)\r\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\r\nCollecting pathspec>=0.9.0 (from black->jupyterlab_git==0.50.0)\r\n  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\r\nRequirement already satisfied: platformdirs>=2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from black->jupyterlab_git==0.50.0) (4.2.2)\r\nRequirement already satisfied: typing-extensions>=4.0.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from black->jupyterlab_git==0.50.0) (4.11.0)\r\nRequirement already satisfied: six>=1.9.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from hybridcontents->jupyterlab_git==0.50.0) (1.16.0)\r\nCollecting notebook>=4.0 (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading notebook-7.1.3-py3-none-any.whl.metadata (10 kB)\r\nCollecting markdown-it-py>=1.0 (from jupytext->jupyterlab_git==0.50.0)\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\nCollecting mdit-py-plugins (from jupytext->jupyterlab_git==0.50.0)\r\n  Downloading mdit_py_plugins-0.4.1-py3-none-any.whl.metadata (2.8 kB)\r\nRequirement already satisfied: pyyaml in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupytext->jupyterlab_git==0.50.0) (6.0.1)\r\nRequirement already satisfied: ptyprocess>=0.5 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from pexpect->jupyterlab_git==0.50.0) (0.7.0)\r\nCollecting cfgv>=2.0.0 (from pre-commit->jupyterlab_git==0.50.0)\r\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\r\nCollecting identify>=1.0.0 (from pre-commit->jupyterlab_git==0.50.0)\r\n  Downloading identify-2.5.36-py2.py3-none-any.whl.metadata (4.4 kB)\r\nCollecting nodeenv>=0.11.1 (from pre-commit->jupyterlab_git==0.50.0)\r\n  Downloading nodeenv-1.8.0-py2.py3-none-any.whl.metadata (21 kB)\r\nCollecting virtualenv>=20.10.0 (from pre-commit->jupyterlab_git==0.50.0)\r\n  Using cached virtualenv-20.26.2-py3-none-any.whl.metadata (4.4 kB)\r\nRequirement already satisfied: idna>=2.8 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (3.7)\r\nRequirement already satisfied: sniffio>=1.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.3.1)\r\nRequirement already satisfied: argon2-cffi-bindings in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (21.2.0)\r\nCollecting gitdb<5,>=4.0.1 (from gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime~=4.0.1->jupyterlab_git==0.50.0)\r\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\r\nRequirement already satisfied: certifi in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from httpx>=0.25.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (2024.2.2)\r\nRequirement already satisfied: httpcore==1.* in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from httpx>=0.25.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (1.0.5)\r\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.14.0)\r\nRequirement already satisfied: zipp>=0.5 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyterlab~=4.0->jupyterlab_git==0.50.0) (3.18.1)\r\nRequirement already satisfied: comm>=0.1.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.2.2)\r\nRequirement already satisfied: debugpy>=1.6.5 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (1.8.1)\r\nRequirement already satisfied: ipython>=7.23.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (8.12.3)\r\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.1.7)\r\nRequirement already satisfied: nest-asyncio in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (1.6.0)\r\nRequirement already satisfied: psutil in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (5.9.8)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.1.5)\r\nRequirement already satisfied: attrs>=22.2.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0) (23.2.0)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0) (2023.12.1)\r\nRequirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0) (1.3.10)\r\nRequirement already satisfied: referencing>=0.[28](https://github.com/jupyterlab/jupyterlab-git/actions/runs/9092321766/job/24988787136?pr=1327#step:7:29).4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0) (0.35.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0) (0.18.1)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.9.0.post0)\r\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.0.7)\r\nRequirement already satisfied: rfc3339-validator in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.1.4)\r\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.1.1)\r\nRequirement already satisfied: babel>=2.10 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.15.0)\r\nRequirement already satisfied: json5>=0.9.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.9.25)\r\nCollecting mdurl~=0.1 (from markdown-it-py>=1.0->jupytext->jupyterlab_git==0.50.0)\r\n  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\nRequirement already satisfied: beautifulsoup4 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (4.12.3)\r\nRequirement already satisfied: bleach!=5.0.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (6.1.0)\r\nRequirement already satisfied: defusedxml in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.7.1)\r\nRequirement already satisfied: jupyterlab-pygments in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.3.0)\r\nRequirement already satisfied: mistune<4,>=2.0.3 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (3.0.2)\r\nRequirement already satisfied: nbclient>=0.5.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.10.0)\r\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.5.1)\r\nRequirement already satisfied: tinycss2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.3.0)\r\nRequirement already satisfied: setuptools in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from nodeenv>=0.11.1->pre-commit->jupyterlab_git==0.50.0) (56.0.0)\r\nCollecting jupyterlab~=4.0 (from jupyterlab_git==0.50.0)\r\n  Downloading jupyterlab-4.1.8-py3-none-any.whl.metadata (16 kB)\r\nCollecting nbval (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading nbval-0.11.0-py2.py3-none-any.whl.metadata (6.7 kB)\r\nCollecting pytest-console-scripts (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading pytest_console_scripts-1.4.1-py3-none-any.whl.metadata (11 kB)\r\nCollecting pytest-timeout (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading pytest_timeout-2.3.1-py3-none-any.whl.metadata (20 kB)\r\nCollecting pytest-tornasync (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading pytest_tornasync-0.6.0.post2-py3-none-any.whl.metadata (4.8 kB)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from requests->nbdime~=4.0.1->jupyterlab_git==0.50.0) (3.3.2)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from requests->nbdime~=4.0.1->jupyterlab_git==0.50.0) (2.2.1)\r\nCollecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->jupyterlab_git==0.50.0)\r\n  Using cached distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\r\nCollecting filelock<4,>=3.12.2 (from virtualenv>=20.10.0->pre-commit->jupyterlab_git==0.50.0)\r\n  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\r\nRequirement already satisfied: pytz>=2015.7 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->jupyterlab~=4.0->jupyterlab_git==0.50.0) (2024.1)\r\nRequirement already satisfied: webencodings in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (0.5.1)\r\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime~=4.0.1->jupyterlab_git==0.50.0)\r\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\r\nRequirement already satisfied: backcall in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.2.0)\r\nRequirement already satisfied: decorator in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (5.1.1)\r\nRequirement already satisfied: jedi>=0.16 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.19.1)\r\nRequirement already satisfied: pickleshare in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.7.5)\r\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.[30](https://github.com/jupyterlab/jupyterlab-git/actions/runs/9092321766/job/24988787136?pr=1327#step:7:31) in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (3.0.43)\r\nRequirement already satisfied: stack-data in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.6.3)\r\nRequirement already satisfied: fqdn in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.5.1)\r\nRequirement already satisfied: isoduration in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (20.11.0)\r\nRequirement already satisfied: jsonpointer>1.13 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.4)\r\nRequirement already satisfied: uri-template in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.3.0)\r\nRequirement already satisfied: webcolors>=1.11 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.13)\r\nCollecting flaky (from jupyter-server[test]<3,>=2.4.0; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading flaky-3.8.1-py2.py3-none-any.whl.metadata (7.4 kB)\r\nCollecting hatch (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached hatch-1.11.0-py3-none-any.whl.metadata (5.4 kB)\r\nCollecting openapi-core~=0.18.0 (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading openapi_core-0.18.2-py3-none-any.whl.metadata (6.3 kB)\r\nCollecting openapi-spec-validator<0.8.0,>=0.6.0 (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading openapi_spec_validator-0.7.1-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting pytest (from jupyterlab_git==0.50.0)\r\n  Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\r\nCollecting requests-mock (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading requests_mock-1.12.1-py2.py3-none-any.whl.metadata (4.1 kB)\r\nCollecting ruamel-yaml (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\r\nCollecting sphinxcontrib-spelling (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading sphinxcontrib_spelling-8.0.0-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting strict-rfc3339 (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\r\n  Installing build dependencies: started\r\n  Installing build dependencies: finished with status 'done'\r\n  Getting requirements to build wheel: started\r\n  Getting requirements to build wheel: finished with status 'done'\r\n  Installing backend dependencies: started\r\n  Installing backend dependencies: finished with status 'done'\r\n  Preparing metadata (pyproject.toml): started\r\n  Preparing metadata (pyproject.toml): finished with status 'done'\r\nCollecting werkzeug (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\r\nRequirement already satisfied: cffi>=1.0.1 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.16.0)\r\nRequirement already satisfied: soupsieve>1.2 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.5)\r\nRequirement already satisfied: pycparser in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.22)\r\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.8.4)\r\nCollecting asgiref<4.0.0,>=3.6.0 (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\r\nCollecting isodate (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\r\nCollecting jsonschema-spec<0.3.0,>=0.2.3 (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading jsonschema_spec-0.2.4-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting more-itertools (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\r\nCollecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading openapi_schema_validator-0.6.2-py3-none-any.whl.metadata (5.3 kB)\r\nCollecting parse (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading parse-1.20.1-py2.py3-none-any.whl.metadata (22 kB)\r\nCollecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-spec-validator<0.8.0,>=0.6.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading jsonschema_path-0.3.2-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.6.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading lazy_object_proxy-1.10.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\r\nRequirement already satisfied: wcwidth in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.2.13)\r\nCollecting hatchling>=1.24.2 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached hatchling-1.24.2-py3-none-any.whl.metadata (3.8 kB)\r\nCollecting hyperlink>=21.0.0 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\r\nCollecting keyring>=23.5.0 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached keyring-25.2.1-py3-none-any.whl.metadata (20 kB)\r\nCollecting rich>=11.2.0 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\r\nCollecting shellingham>=1.4.0 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\r\nCollecting tomli-w>=1.0 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached tomli_w-1.0.0-py3-none-any.whl.metadata (4.9 kB)\r\nCollecting tomlkit>=0.11.1 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached tomlkit-0.12.5-py3-none-any.whl.metadata (2.7 kB)\r\nCollecting userpath~=1.7 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached userpath-1.9.2-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting uv>=0.1.35 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached uv-0.1.44-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\r\nCollecting zstandard<1 (from hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\r\nRequirement already satisfied: arrow>=0.15.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (1.3.0)\r\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading ruamel.yaml.clib-0.2.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (2.2 kB)\r\nCollecting PyEnchant>=3.1.1 (from sphinxcontrib-spelling->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading pyenchant-3.2.2-py3-none-any.whl.metadata (3.8 kB)\r\nCollecting Sphinx>=3.0.0 (from sphinxcontrib-spelling->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading sphinx-7.1.2-py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: executing>=1.2.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.0.1)\r\nRequirement already satisfied: asttokens>=2.1.0 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (2.4.1)\r\nRequirement already satisfied: pure-eval in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab~=4.0->jupyterlab_git==0.50.0) (0.2.2)\r\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0) (2.9.0.20240[31](https://github.com/jupyterlab/jupyterlab-git/actions/runs/9092321766/job/24988787136?pr=1327#step:7:32)6)\r\nCollecting trove-classifiers (from hatchling>=1.24.2->hatch->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached trove_classifiers-2024.4.10-py3-none-any.whl.metadata (2.2 kB)\r\nCollecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi-spec-validator<0.8.0,>=0.6.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading pathable-0.4.3-py3-none-any.whl.metadata (1.9 kB)\r\nCollecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0)\r\n  Downloading referencing-0.31.1-py3-none-any.whl.metadata (2.7 kB)\r\nINFO: pip is looking at multiple versions of jsonschema-spec to determine which version is compatible with other requirements. This could take a while.\r\nCollecting jsonschema-spec<0.3.0,>=0.2.3 (from openapi-core~=0.18.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading jsonschema_spec-0.2.3-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-spec-validator<0.8.0,>=0.6.0->jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading jsonschema_path-0.3.1-py3-none-any.whl.metadata (4.3 kB)\r\nINFO: pip is looking at multiple versions of jsonschema-path to determine which version is compatible with other requirements. This could take a while.\r\nCollecting openapi-core~=0.18.0 (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading openapi_core-0.18.1-py3-none-any.whl.metadata (6.3 kB)\r\nCollecting openapi-spec-validator<0.8.0,>=0.6.0 (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.[50](https://github.com/jupyterlab/jupyterlab-git/actions/runs/9092321766/job/24988787136?pr=1327#step:7:51).0)\r\n  Downloading openapi_spec_validator-0.6.0-py3-none-any.whl.metadata (5.3 kB)\r\nCollecting importlib-resources>=1.4.0 (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0)\r\n  Downloading importlib_resources-5.13.0-py3-none-any.whl.metadata (4.2 kB)\r\nCollecting openapi-core~=0.18.0 (from jupyterlab-server[test]<3,>=2.22.1; extra == \"test\"->notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Downloading openapi_core-0.18.0-py3-none-any.whl.metadata (6.3 kB)\r\nCollecting jupyterlab-server[test]<3,>=2.22.1 (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached jupyterlab_server-2.27.1-py3-none-any.whl.metadata (5.9 kB)\r\nINFO: pip is still looking at multiple versions of jsonschema-spec to determine which version is compatible with other requirements. This could take a while.\r\nCollecting jupyter-server[test]<3,>=2.4.0 (from notebook[test]>=4.0->hybridcontents->jupyterlab_git==0.50.0)\r\n  Using cached jupyter_server-2.14.0-py3-none-any.whl.metadata (8.4 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n  Downloading jupyter_server-2.13.0-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.12.5-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.12.4-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.12.3-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.12.2-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.12.1-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.12.0-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.11.2-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.11.1-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.10.1-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.10.0-py3-none-any.whl.metadata (8.4 kB)\r\n  Downloading jupyter_server-2.9.1-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.9.0-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.8.0-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.7.3-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.7.2-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.7.1-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.7.0-py3-none-any.whl.metadata (8.6 kB)\r\n  Downloading jupyter_server-2.6.0-py3-none-any.whl.metadata (8.5 kB)\r\n  Downloading jupyter_server-2.5.0-py3-none-any.whl.metadata (8.7 kB)\r\n  Downloading jupyter_server-2.4.0-py3-none-any.whl.metadata (8.7 kB)\r\nCollecting jsonschema[format-nongpl]>=4.18.0 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab_git==0.50.0)\r\n  Using cached jsonschema-4.22.0-py3-none-any.whl.metadata (8.2 kB)\r\n  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\r\n  Downloading jsonschema-4.21.0-py3-none-any.whl.metadata (8.0 kB)\r\n  Downloading jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\r\n  Downloading jsonschema-4.19.2-py3-none-any.whl.metadata (7.9 kB)\r\n  Downloading jsonschema-4.19.1-py3-none-any.whl.metadata (7.9 kB)\r\n  Downloading jsonschema-4.19.0-py3-none-any.whl.metadata (8.2 kB)\r\n  Downloading jsonschema-4.18.6-py3-none-any.whl.metadata (7.8 kB)\r\n  Downloading jsonschema-4.18.5-py3-none-any.whl.metadata (7.7 kB)\r\n  Downloading jsonschema-4.18.4-py3-none-any.whl.metadata (7.8 kB)\r\n  Downloading jsonschema-4.18.3-py3-none-any.whl.metadata (7.9 kB)\r\n  Downloading jsonschema-4.18.2-py3-none-any.whl.metadata (7.8 kB)\r\n  Downloading jsonschema-4.18.1-py3-none-any.whl.metadata (7.8 kB)\r\n  Downloading jsonschema-4.18.0-py3-none-any.whl.metadata (10 kB)\r\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->jupyterlab_git==0.50.0)\r\n  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\r\n  Downloading jsonschema_specifications-2023.11.2-py3-none-any.whl.metadata (3.0 kB)\r\n```\r\n\r\n</details>\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex e63f989d..29fab7a5 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -44,7 +44,6 @@ test = [\n     \"pytest-asyncio\",\n     \"pytest-cov\",\n     \"pytest-jupyter[server]>=0.6.0\",\n-    \"hybridcontents\",\n     \"jupytext\",\n ]\n ui-tests = [\n", "instance_id": "jupyterlab__jupyterlab-git-1329", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: long CI installation times (25 minutes) due to dependency resolution issues, specifically pointing to the `hybridcontents` dependency as a potential cause of conflict with `jupyter-server`. It provides context about compatibility issues with JupyterLab 4.x and Notebook 7.x, and proposes removing the `hybridcontents` test dependency to resolve the issue. Links to relevant GitHub actions and code snippets are included, which help in understanding the problem. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly confirm whether removing `hybridcontents` will fully resolve the dependency conflict or if other underlying issues might persist. Additionally, it lacks clarity on the impact of removing the associated tests (e.g., whether this compromises test coverage or functionality). The problem statement also does not specify alternative solutions or potential risks, which would be helpful for a comprehensive understanding. Overall, while the goal and proposed solution are clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward modification to the project's dependency configuration. The code change is minimal, consisting of removing a single dependency (`hybridcontents`) from the `pyproject.toml` file, which is a simple edit in a configuration file rather than complex logic or code implementation. The scope of the change is limited to a single line in one file, with no impact on the broader codebase architecture or interactions between modules. The technical concepts required are basic: understanding Python dependency management with `pip` and familiarity with `pyproject.toml` for package configuration, which are fundamental skills for any developer working in a Python environment. There are no significant edge cases or error handling considerations mentioned in the problem statement or evident in the code change, as the removal of a test dependency is unlikely to introduce runtime errors (though it may affect test coverage, which is not addressed as a technical challenge here). The primary challenge lies in confirming that removing this dependency does not inadvertently break test functionality or compatibility, but this is more of a validation step than a complex coding task. Overall, this is a very easy problem requiring minimal effort and expertise to implement the proposed solution.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add matrix strategy for targets\nIn some cases we would like to run a same command for multiples parameters. Currently it is possible using a \"for loop\" (in bash, or xonsh, etc) or using jinja2 \"for loop\".\r\n\r\nit would be nice to do that using a matrix strategy, similar to what github actions does (for a entire job).\r\n\r\nNOTE: matrix will be implemented just for \"task scope\".\r\n\r\nExample:\r\n\r\n```\r\nversion: 1.0.0\r\ngroups:\r\n  main:\r\n    env-file: .env\r\n    tasks:\r\n      print:\r\n        help: print the values from a matrix strategy\r\n        matrix:\r\n          mytext: \r\n            - text1\r\n            - text2\r\n            - text3\r\n        run: echo {{ matrix.mytext }}\r\n```\r\n\r\nwhen the command `makim main.print` is executed, it will run \"echo\" for each value in mytext list.\r\nif more variables is defined in the matrix, it will run the `run` section for all the combination of variables.\r\nthe values for each variable in the matrix, should be a list\r\n\n", "patch": "diff --git a/.makim.yaml b/.makim.yaml\nindex f694b93..2425236 100644\n--- a/.makim.yaml\n+++ b/.makim.yaml\n@@ -86,6 +86,7 @@ groups:\n             - task: smoke-tests.dir-relative-path\n             - task: smoke-tests.interactive-args\n             - task: smoke-tests.run-hooks\n+            - task: smoke-tests.matrix\n \n       ci:\n         help: Run all tasks used on CI\n@@ -391,6 +392,27 @@ groups:\n           makim $VERBOSE_FLAG --file $MAKIM_FILE --version\n           makim $VERBOSE_FLAG --file $MAKIM_FILE build.compile\n \n+      matrix:\n+        help: Test makim with matrix combination\n+        args:\n+          verbose-mode:\n+            help: Run the all the tests in verbose mode\n+            type: bool\n+            action: store_true\n+        env:\n+          MAKIM_FILE: tests/smoke/.makim-matrix-strategy.yaml\n+        backend: bash\n+        run: |\n+          export VERBOSE_FLAG='${{ \"--verbose\" if args.verbose_mode else \"\" }}'\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE --help\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE --version\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE build.setup\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE build.lint\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE build.lint --fix\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE test.unit\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE test.browser\n+          makim $VERBOSE_FLAG --file $MAKIM_FILE test.browser --headless\n+\n   error:\n     help: This group helps tests failure tasks\n     tasks:\ndiff --git a/src/makim/core.py b/src/makim/core.py\nindex ae4bce0..47a5202 100644\n--- a/src/makim/core.py\n+++ b/src/makim/core.py\n@@ -17,6 +17,7 @@\n import warnings\n \n from copy import deepcopy\n+from itertools import product\n from pathlib import Path\n from typing import Any, Dict, List, Optional, Union, cast\n \n@@ -363,7 +364,7 @@ def _load_dotenv(self, data_scope: dict[str, Any]) -> dict[str, str]:\n \n     def _load_scoped_data(\n         self, scope: str\n-    ) -> tuple[dict[str, str], dict[str, str]]:\n+    ) -> tuple[dict[str, str], dict[str, Any]]:\n         scope_options = ('global', 'group', 'task')\n         if scope not in scope_options:\n             raise Exception(f'The given scope `{scope}` is not valid.')\n@@ -451,6 +452,66 @@ def _load_task_args(self) -> None:\n                     else (False if is_bool else None)\n                 )\n \n+    def _log_command_execution(\n+        self, execution_context: dict[str, Any], width: int\n+    ) -> None:\n+        \"\"\"Log the command execution details.\n+\n+        execution_context should contain: args_input, current_vars,\n+        env, and optionally matrix_vars\n+        \"\"\"\n+        MakimLogs.print_info('=' * width)\n+        MakimLogs.print_info(\n+            'TARGET: ' + f'{self.group_name}.{self.task_name}'\n+        )\n+        MakimLogs.print_info('ARGS:')\n+        MakimLogs.print_info(pprint.pformat(execution_context['args_input']))\n+        MakimLogs.print_info('VARS:')\n+        MakimLogs.print_info(pprint.pformat(execution_context['current_vars']))\n+        MakimLogs.print_info('ENV:')\n+        MakimLogs.print_info(str(execution_context['env']))\n+\n+        if execution_context.get('matrix_vars'):\n+            MakimLogs.print_info('MATRIX:')\n+            MakimLogs.print_info(\n+                pprint.pformat(execution_context['matrix_vars'])\n+            )\n+\n+        MakimLogs.print_info('-' * width)\n+\n+    def _generate_matrix_combinations(\n+        self, matrix_config: dict[str, Any]\n+    ) -> list[dict[str, Any]]:\n+        \"\"\"Generate all possible combinations from matrix configuration.\n+\n+        Parameters\n+        ----------\n+        matrix_config : dict\n+            Dictionary containing matrix variables and their possible values\n+\n+        Returns\n+        -------\n+        list[dict]\n+            List of dictionaries, each containing one possible combination\n+        \"\"\"\n+        if not matrix_config:\n+            return []\n+\n+        # Convert matrix config into format suitable for product\n+        keys = list(matrix_config.keys())\n+        values = [\n+            matrix_config[k]\n+            if isinstance(matrix_config[k], list)\n+            else [matrix_config[k]]\n+            for k in keys\n+        ]\n+\n+        combinations = []\n+        for combo in product(*values):\n+            combinations.append(dict(zip(keys, combo)))\n+\n+        return combinations\n+\n     # run commands\n     def _run_hooks(self, args: dict[str, Any], hook_type: str) -> None:\n         if not self.task_data.get('hooks', {}).get(hook_type):\n@@ -518,20 +579,22 @@ def _run_hooks(self, args: dict[str, Any], hook_type: str) -> None:\n \n     def _run_command(self, args: dict[str, Any]) -> None:\n         cmd = self.task_data.get('run', '').strip()\n-        if 'vars' not in self.group_data:\n-            self.group_data['vars'] = {}\n \n-        if not isinstance(self.group_data['vars'], dict):\n+        if not isinstance(self.group_data.get('vars', {}), dict):\n             MakimLogs.raise_error(\n                 '`vars` attribute inside the group '\n                 f'{self.group_name} is not a dictionary.',\n                 MakimError.MAKIM_VARS_ATTRIBUTE_INVALID,\n             )\n \n-        env, variables = self._load_scoped_data('task')\n-        for k, v in env.items():\n-            os.environ[k] = v\n+        # Get matrix configuration if it exists\n+        matrix_combinations: list[dict[str, Any]] = []\n+        if matrix_config := self.task_data.get('matrix', {}):\n+            matrix_combinations = self._generate_matrix_combinations(\n+                matrix_config\n+            )\n \n+        env, variables = self._load_scoped_data('task')\n         self.env_scoped = deepcopy(env)\n \n         args_input = {'file': self.file}\n@@ -565,36 +628,47 @@ def _run_command(self, args: dict[str, Any]) -> None:\n                     MakimError.MAKIM_ARGUMENT_REQUIRED,\n                 )\n \n-        cmd = str(cmd)\n-        cmd = TEMPLATE.from_string(cmd).render(\n-            args=args_input, env=env, vars=variables\n-        )\n         width, _ = get_terminal_size()\n \n-        if self.verbose:\n-            MakimLogs.print_info('=' * width)\n-            MakimLogs.print_info(\n-                'TARGET: ' + f'{self.group_name}.{self.task_name}'\n+        # Run command for each matrix combination\n+        for matrix_vars in matrix_combinations or [{}]:\n+            # Update environment variables\n+            for k, v in env.items():\n+                os.environ[k] = v\n+\n+            # Create a copy of variables and update with matrix values\n+            current_vars = deepcopy(variables)\n+            if matrix_vars:\n+                current_vars['matrix'] = matrix_vars\n+\n+            # Render command with current matrix values\n+            current_cmd = TEMPLATE.from_string(cmd).render(\n+                args=args_input, env=env, vars=current_vars, matrix=matrix_vars\n             )\n-            MakimLogs.print_info('ARGS:')\n-            MakimLogs.print_info(pprint.pformat(args_input))\n-            MakimLogs.print_info('VARS:')\n-            MakimLogs.print_info(pprint.pformat(variables))\n-            MakimLogs.print_info('ENV:')\n-            MakimLogs.print_info(str(env))\n-            MakimLogs.print_info('-' * width)\n-            MakimLogs.print_info('>>> ' + cmd.replace('\\n', '\\n>>> '))\n-            MakimLogs.print_info('=' * width)\n-\n-        if not self.dry_run and cmd:\n-            self._call_shell_app(cmd)\n+\n+            if self.verbose:\n+                # Prepare execution context for logging\n+                execution_context = {\n+                    'args_input': args_input,\n+                    'current_vars': current_vars,\n+                    'env': env,\n+                    'matrix_vars': matrix_vars,\n+                }\n+                # Log command execution details\n+                self._log_command_execution(execution_context, width)\n+                MakimLogs.print_info(\n+                    '>>> ' + current_cmd.replace('\\n', '\\n>>> ')\n+                )\n+                MakimLogs.print_info('=' * width)\n+\n+            if not self.dry_run and current_cmd:\n+                self._call_shell_app(current_cmd)\n \n         # move back the environment variable to the previous values\n         os.environ.clear()\n         os.environ.update(self.env_scoped)\n \n-        # public methods\n-\n+    # public methods\n     def load(\n         self, file: str, dry_run: bool = False, verbose: bool = False\n     ) -> None:\n", "instance_id": "osl-incubator__makim-118", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of implementing a matrix strategy for running commands with multiple parameter combinations, similar to GitHub Actions. It provides a concrete example of the desired configuration format and explains the expected behavior (running a command for each combination of matrix values). However, there are minor ambiguities and missing details. For instance, it does not explicitly address how errors or failures in one matrix combination should affect the overall execution (e.g., should it stop or continue?). Additionally, edge cases such as empty matrix lists, invalid matrix configurations, or interactions with other task features (like hooks or environment variables) are not mentioned. While the intent is clear, these omissions prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes involves modifications to a core functionality in `makim/core.py`, specifically around task execution and variable handling, which requires understanding the existing rendering and execution logic (e.g., Jinja2 templating, environment variable management). The changes span a moderate amount of code, including the addition of matrix combination generation using `itertools.product` and updating the command execution loop to handle multiple combinations. This introduces complexity in managing state (e.g., environment variables, scoped data) across iterations.\n\nTechnically, the problem requires familiarity with Python's standard library (`itertools.product` for combinations), dictionary manipulation, and template rendering. It also demands an understanding of how the `makim` tool processes tasks and variables, which adds a layer of domain-specific knowledge. While the concepts themselves are not overly advanced, integrating them into the existing codebase without breaking other functionalities requires careful consideration.\n\nEdge cases and error handling add to the difficulty. Although not explicitly mentioned in the problem statement, the code changes must account for scenarios like empty matrix configurations, non-list values in the matrix, or failures during command execution for specific combinations. The provided code handles some of these implicitly (e.g., converting non-list values to lists), but robust error handling is not fully implemented, which could be a potential oversight.\n\nThe changes do not significantly impact the system's architecture, as they are localized to the task execution logic, but they do require understanding interactions between task data, environment variables, and command rendering. Overall, this problem is moderately challenging, requiring a solid grasp of the codebase and careful implementation to ensure correctness across various use cases, placing it slightly above the midpoint of the difficulty scale.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support for modularity\nIn order to simplify the benchmarking of the recently introduced [contract system in CIRCT](https://circt.llvm.org/docs/Dialects/Verif/#contracts), we should artificially introduce the concept of modularity to btor2. Let's illustrate what I mean with an example, where we will take the [reg_en example](https://github.com/dobios/btor2-opt/blob/main/tests/btor/reg_en.btor) and update it to encode the following hoare triple `{A.d=0} reg_en {A.q>0}`:\r\n```btor\r\nmodule A {\r\n    1 sort bitvec 32\r\n    2 input 1 A.d \r\n    3 sort bitvec 1\r\n    4 input 3 A.en \r\n    5 input 3 A.rst \r\n    6 input 1 B.d \r\n    7 input 3 B.en \r\n    8 input 3 B.rst \r\n    9 input 3 clk\r\n    10 state 1 A.q\r\n    11 output 10 qA \r\n    12 state 1 B.q\r\n    13 output 12 qB \r\n    14 uext 3 9 0 A.clk \r\n    15 uext 3 9 0 B.clk \r\n    16 ite 1 4 2 10\r\n    17 const 1 00000000000000000000000000000000\r\n    18 ite 1 5 17 16\r\n    19 next 1 10 18\r\n    20 ite 1 7 6 12\r\n    21 ite 1 8 17 20\r\n    22 next 1 12 21\r\n}\r\ncontract A {\r\n    1 sort bitvec 1\r\n    2 ref A 1\r\n    3 const 2 0\r\n    4 ref A 2\r\n    5 eq 1 4 2\r\n    6 prec 3\r\n    7 ref A 10\r\n    8 gt 1 7 2\r\n    9 post 5  \r\n}\r\nmodule C {\r\n    1 sort bitvec 32\r\n    2 input 1 C.i\r\n    3 inst A ; will either inline A or replace it with its contract\r\n    4 ref A 1\r\n    5 set 3 4 2 \r\n    6 sort bitvec 1\r\n    7 const 1 0\r\n    8 ref A 10\r\n    9 eq 1 7 6\r\n    10 bad 7 \r\n}\r\n```  \r\nThe new constructs introduced here are:  \r\n- `module <name> {...}` : Wraps a series of operations into a block that can be referred to by name.  \r\n- `contract <existing_name>`: Associates a contract (preconditions and post conditions) to an existing module.\r\n- `inst <existing_name>`: Create an instance of a module.\r\n- `ref` <existing_name> <line>: Create a local reference to a line in another named block.\r\n- `set <instance_id> <ref_id> <driver_id>`: Sets an instance's input to an operation result. This will require that the \"driver\" operation meet the preconditions set in the module's contract.\r\n\r\nUsing this structure, when enabled, the preconditions will be asserted in the module that instantiates the contract, and the postconditions will be assumed (turned into btor2 constraints). This will allow for the quick benchmarking of various modularization and solver parallelization methods. Btor-opt should take this as input and yield valid standard btor2: one file per module, where verifying a contract requires:\r\n- *Module*: Assume preconditions, check module, assert postconditions  \r\n- *Instance*: Assert preconditions on set inputs, assume postconditions  \r\n\n", "patch": "diff --git a/src/btoropt/__main__.py b/src/btoropt/__main__.py\nindex 6698a89..7f490f1 100644\n--- a/src/btoropt/__main__.py\n+++ b/src/btoropt/__main__.py\n@@ -21,22 +21,42 @@\n from .parser import *\n import sys\n \n+options = [\"modular\"]\n+\n def main():\n     # Retrieve flags\n     if len(sys.argv) < 3:\n-        print(\"Usage: btoropt <file.btor2> <pass_names_in_order> ...\")\n+        print(\"Usage: btoropt [optional](--modular) <file.btor2> <pass_names_in_order> ...\")\n         exit(1)\n \n+    # Check options\n+    base = 1\n+    modular = False\n+    if \"--\" in sys.argv[1].strip():\n+        option = sys.argv[1].strip().strip(\"--\")\n+        if option not in options:\n+            print(f\"Invalid option given: {option}\")\n+            exit(1)\n+        modular = True\n+        base += 1\n+        \n+\n     # Retrieve design\n     btor2str: list[str] = []\n-    with open(sys.argv[1], \"r\") as f:\n+    with open(sys.argv[base], \"r\") as f:\n         btor2str = f.readlines()\n \n     # Parse the design\n-    btor2: list[Instruction] = parse(btor2str)\n+    btor2 = None\n+    if modular:\n+        btor2 = parse_file(btor2str)\n+    else:\n+        btor2 = parse(btor2str)\n+    \n+    assert btor2 is not None\n \n     # Check that the given pass names are valid\n-    for name in sys.argv[1:]:\n+    for name in sys.argv[base:]:\n         if find_pass(all_passes, name) is None:\n             print(f\"Invalid pass given as argument: {name}\")\n             exit(1)\n@@ -46,10 +66,16 @@ def main():\n \n     # Run all passes in the pipeline\n     for p in pipeline:\n-        btor2 = p.run(btor2)\n+        if modular: \n+            btor2 = p.runOnProgram(btor2)\n+        else:\n+            btor2 = p.run(btor2)\n \n     # Show the result to the user\n-    print(serialize_p(btor2))\n+    if(modular):\n+        print(serialize_p(btor2))\n+    else:\n+        print(\"Success\")\n \n if __name__ == \"__main__\":\n     main()\ndiff --git a/src/btoropt/parser.py b/src/btoropt/parser.py\nindex ad2d27a..5e7dd70 100644\n--- a/src/btoropt/parser.py\n+++ b/src/btoropt/parser.py\n@@ -18,576 +18,734 @@\n \n from .program import *\n \n+# Retrieves an instruction with the given ID from the given standard program\n+# This is a safe wrapper around `get_inst` and enforces that the given \n+# ID must be correct.\n def find_inst(p: list[Instruction], id: int) -> Instruction:\n     inst = get_inst(p, id)\n     assert inst is not None, f\"Undeclared instruction used with id: {id}\"\n     return inst\n \n-def parse(inp: list[str]) -> list[Instruction]:\n-    # Split the string into instructions and read them 1 by 1\n+# Checks thaa a given module name has been defined\n+def check_name(name: str, modules: list[Module]) -> bool:\n+    return name in [m.name for m in modules]\n+\n+# Retrives a module by name from a list of parsed modules\n+def get_module(name: str, modules: list[Module]) -> Module:\n+    return [m for m in modules if m.name == name][0]\n+\n+# Extracts a body from an arbitrary code sequence\n+# Returns the line idx at which the scanning ended\n+def scan_body(inp: list[str], i: int) -> tuple[list[str], int]:\n+    res = []\n+    l = inp[i].split(\" \")\n+    ## Check that the declaration line ends with an '{'\n+    assert str(l[len(l)-1].strip()) == '{', f\"invalid body start: {l[len(l)-1]}\"\n+\n+    i += 1\n+    while inp[i].strip() != \"}\":\n+        # Check that there are no nested structures\n+        lid = inp[i].strip().split(\" \")[0]\n+        assert lid.isnumeric(), f\"All body lines must be instructions! Found: {lid}\"\n+        res.append(inp[i].strip())\n+        i += 1\n+    return (res, i)\n+\n+# Parses a ref instruction (only custom inst that is allowed in both modules and contracts)\n+# @param inst: the pre-split ref instruction to be parsed \n+# @param modules: the list of already parsed modules that can be referenced\n+def parse_ref(inst: list[str], modules: list[Module]) -> Ref:\n+    ## Sanity check: Must be a ref instruction\n+    assert inst[1] == \"ref\", f\"`parse_ref` can only handle ref instructions, not {inst[1]}!\"\n+    ref_mod = inst[2]\n+\n+    ## Sanity check: check that the name exists\n+    assert check_name(ref_mod, modules), f\"Named module {ref_mod} is undefined!\"\n+\n+    module = get_module(ref_mod, modules)\n+    val = find_inst(module.body, int(inst[3]))\n+    return Ref(int(inst[0]), ref_mod, val)\n+\n+# Parse a module's pre-scanned body\n+# @param body: the list of instructions contained within the body\n+# @param modules: the list of already parsed modules that can be referenced\n+def parse_module_body(body: list[str], modules: list[Module]) -> list[Instruction]:\n     p = []\n-    for line in inp:\n+    for line in body:\n         inst = line.split(\" \")\n-        # BTOR comment\n-        if inst[0] == \";\":\n+        if inst[0] == \";\": # handle comments\n             continue\n         lid = int(inst[0])\n         tag = inst[1]\n+        match tag:\n+            # Handle special instructions\n+            case \"inst\":\n+                instd_mod = inst[2]\n+                ## Sanity check: check that the name exists\n+                assert check_name(instd_mod, modules), f\"Named module {instd_mod} is undefined!\"\n+                p.append(Instance(lid, instd_mod))\n+\n+            case \"ref\":\n+                p.append(parse_ref(inst, modules))\n+\n+            case \"set\":\n+                instance = find_inst(p, int(inst[2]))\n+                ref = find_inst(p, int(inst[3]))\n+                assert ref.name == instance.name, \"`set` can only set a reference to an instance input!\"\n+                alias = find_inst(p, int(inst[4]))\n+                p.append(Set(lid, instance, ref, alias))\n+\n+            # Handle standard instructions\n+            case _:\n+                op = parse_inst(line, p)\n+                if op is not None:\n+                    p.append(op)\n \n-        # Check if tag is valid\n-        assert tag in tags, f\"Unsupported operation type: {tag} in {line}\"\n-\n-        # Create the instruction associated to the tag\n-        op = None\n+    return p\n \n+# Parse a contract's pre-scanned body\n+# @param name: the name given to the module\n+# @param body: the list of instructions contained within the body\n+# @param modules: the list of already parsed modules that can be referenced\n+def parse_contract_body(body: list[str], modules: list[Module]) -> list[Instruction]:\n+    p = []\n+    for line in body:\n+        inst = line.split(\" \")\n+        if inst[0] == \";\": # handle comments\n+            continue\n+        lid = int(inst[0])\n+        tag = inst[1]\n         match tag:\n-            case \"sort\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"sort instruction must be of the form: <lid> sort \\{bitvector|array\\} <width>. Found: \" + line\n-                assert inst[2] in sort_tags,\\\n-                    f\"sort must be of type bitvector or array! Found: {inst[2]}\"\n+            # Handle special instructions\n+            case \"prec\":\n+                # Find the op associated to this instruction\n+                cond = find_inst(p, int(inst[2]))\n+                p.append(Prec(lid, cond))\n+\n+            case \"post\":\n+                # Find the op associated to this instruction\n+                cond = find_inst(p, int(inst[2]))\n+                p.append(Post(lid, cond))\n \n-                # Construct instruction\n-                op = Sort(lid, inst[2], int(inst[3]))\n+            case \"ref\":\n+                p.append(parse_ref(inst, modules))\n \n+            # Handle standard instructions\n+            case _:\n+                op = parse_inst(line, p)\n+                if op is not None:\n+                    p.append(op)\n \n-            case \"input\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"input instruction must be of the form: <lid> input <sid> <name>. Found: \" + line\n+    return p\n \n-                # Find the sort associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                assert isinstance(sort, Sort), f\"Input sort must be a Sort. Found: \" + line\n+# Parses a single instruction\n+# @param line: the current instruction that needs to be parsed\n+# @param p: the current parsed state of the program\n+def parse_inst(line: str, p: list[Instruction]) -> Instruction:\n+    inst = line.split(\" \")\n+    # BTOR comment\n+    if inst[0] == \";\":\n+        return None\n+    lid = int(inst[0])\n+    tag = inst[1]\n \n-                # Construct instruction\n-                op = Input(lid, sort, inst[3])\n+    # Check if tag is valid\n+    assert tag in tags, f\"Unsupported operation type: {tag} in {line}\"\n \n-            case \"output\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 3,\\\n-                    \"input instruction must be of the form: <lid> output <opid>. Found: \" + line\n+    # Create the instruction associated to the tag\n+    op = None\n \n-                # Find the op associated to this instruction\n-                out = find_inst(p, int(inst[2]))\n+    match tag:\n+        case \"sort\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"sort instruction must be of the form: <lid> sort \\{bitvector|array\\} <width>. Found: \" + line\n+            assert inst[2] in sort_tags,\\\n+                f\"sort must be of type bitvector or array! Found: {inst[2]}\"\n \n-                # Construct instruction\n-                op = Output(lid, out)\n+            # Construct instruction\n+            op = Sort(lid, inst[2], int(inst[3]))\n \n-            case \"bad\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 3,\\\n-                    \"sort instruction must be of the form: <lid> bad <opid>. Found: \" + line\n \n-                # Find the op associated to this instruction\n-                cond = find_inst(p, int(inst[2]))\n+        case \"input\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"input instruction must be of the form: <lid> input <sid> <name>. Found: \" + line\n \n-                # Construct instruction\n-                op = Bad(lid, cond)\n+            # Find the sort associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            assert isinstance(sort, Sort), f\"Input sort must be a Sort. Found: \" + line\n \n-            case \"constraint\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 3,\\\n-                    \"sort instruction must be of the form: <lid> constraint <opid>. Found: \" + line\n+            # Construct instruction\n+            op = Input(lid, sort, inst[3])\n \n-                # Find the op associated to this instruction\n-                cond = find_inst(p, int(inst[2]))\n+        case \"output\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 3,\\\n+                \"input instruction must be of the form: <lid> output <opid>. Found: \" + line\n \n-                # Construct instruction\n-                op = Constraint(lid, cond)\n+            # Find the op associated to this instruction\n+            out = find_inst(p, int(inst[2]))\n \n-            case \"zero\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 3,\\\n-                    \"sort instruction must be of the form: <lid> zero <sid>. Found: \" + line\n+            # Construct instruction\n+            op = Output(lid, out)\n \n-                # Find the sort associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n+        case \"bad\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 3,\\\n+                \"sort instruction must be of the form: <lid> bad <opid>. Found: \" + line\n \n-                # Construct instruction\n-                op = Zero(lid, sort)\n+            # Find the op associated to this instruction\n+            cond = find_inst(p, int(inst[2]))\n \n-            case \"one\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 3,\\\n-                    \"sort instruction must be of the form: <lid> one <sid>. Found: \" + line\n+            # Construct instruction\n+            op = Bad(lid, cond)\n \n-                # Find the sort associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n+        case \"constraint\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 3,\\\n+                \"sort instruction must be of the form: <lid> constraint <opid>. Found: \" + line\n \n-                # Construct instruction\n-                op = One(lid, sort)\n+            # Find the op associated to this instruction\n+            cond = find_inst(p, int(inst[2]))\n \n-            case \"ones\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 3,\\\n-                    \"sort instruction must be of the form: <lid> ones <sid>. Found: \" + line\n+            # Construct instruction\n+            op = Constraint(lid, cond)\n \n-                # Find the sort associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n+        case \"zero\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 3,\\\n+                \"sort instruction must be of the form: <lid> zero <sid>. Found: \" + line\n+\n+            # Find the sort associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n \n-                # Construct instruction\n-                op = Ones(lid, sort)\n-\n-            case \"constd\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"sort instruction must be of the form: <lid> constd <sid> <value>. Found: \" + line\n+            # Construct instruction\n+            op = Zero(lid, sort)\n \n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                value = int(inst[3])\n+        case \"one\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 3,\\\n+                \"sort instruction must be of the form: <lid> one <sid>. Found: \" + line\n \n-                # Construct instruction\n-                op = Constd(lid, sort, value)\n-\n-            case \"consth\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"sort instruction must be of the form: <lid> consth <sid> <value>. Found: \" + line\n+            # Find the sort associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+\n+            # Construct instruction\n+            op = One(lid, sort)\n+\n+        case \"ones\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 3,\\\n+                \"sort instruction must be of the form: <lid> ones <sid>. Found: \" + line\n+\n+            # Find the sort associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+\n+            # Construct instruction\n+            op = Ones(lid, sort)\n+\n+        case \"constd\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"sort instruction must be of the form: <lid> constd <sid> <value>. Found: \" + line\n \n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                value = int(inst[3])\n-\n-                # Construct instruction\n-                op = Consth(lid, sort, value)\n-\n-            case \"const\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"sort instruction must be of the form: <lid> const <sid> <value>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                # Default base is 2\n-                value = int(inst[3], 2)\n-\n-                # Construct instruction\n-                op = Const(lid, sort, value)\n-\n-            case \"state\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"state instruction must be of the form: <lid> state <sid> <name>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                assert isinstance(sort, Sort), f\"State sort must be a Sort. Found: \" + line\n-                name = inst[3].strip()\n-\n-                # Construct instruction\n-                op = State(lid, sort, name)\n-\n-            case \"init\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> init <sid> <stateid> <valueid>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                state = find_inst(p, int(inst[3]))\n-                val = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Init(lid, sort, state, val)\n-\n-            case \"next\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> next <sid> <stateid> <nextid>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                state = find_inst(p, int(inst[3]))\n-                next = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Next(lid, sort, state, next)\n-\n-            case \"slice\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 6,\\\n-                    \"sort instruction must be of the form: <lid> slice <sid> <opid> <width> <lowbit>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                operand = find_inst(p, int(inst[3]))\n-                width = int(inst[4])\n-                lowbit = int(inst[5])\n-\n-                # Construct instruction\n-                op = Slice(lid, sort, operand, width, lowbit)\n-\n-            case \"ite\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 6,\\\n-                    \"sort instruction must be of the form: <lid> ite <sid> <condid> <tid> <fid>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                cond = find_inst(p, int(inst[3]))\n-                t = find_inst(p, int(inst[4]))\n-                f = find_inst(p, int(inst[5]))\n-\n-                # Construct instruction\n-                op = Ite(lid, sort, cond, t, f)\n-\n-            case \"implies\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> implies <sid> <lhsid> <rhsid>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                lhs = find_inst(p, int(inst[3]))\n-                rhs = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Implies(lid, sort, lhs, rhs)\n-\n-            case \"iff\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> iff <sid> <lhsid> <rhsid>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                lhs = find_inst(p, int(inst[3]))\n-                rhs = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Iff(lid, sort, lhs, rhs)\n-\n-            case \"add\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> add <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Add(lid, sort, op1, op2)\n-\n-            case \"sub\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> sub <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Sub(lid, sort, op1, op2)\n-\n-            case \"mul\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> mul <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Mul(lid, sort, op1, op2)\n-\n-            case \"sdiv\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> sdiv <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Sdiv(lid, sort, op1, op2)\n-\n-            case \"udiv\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> udiv <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Udiv(lid, sort, op1, op2)\n-\n-            case \"smod\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> smod <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(int(inst[4]))\n-\n-                # Construct instruction\n-                op = Smod(lid, sort, op1, op2)\n-\n-            case \"sll\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> sll <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Sll(lid, sort, op1, op2)\n-\n-            case \"srl\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> srl <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Srl(lid, sort, op1, op2)\n-\n-            case \"sra\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> sra <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Sra(lid, sort, op1, op2)\n-\n-            case \"and\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> and <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = And(lid, sort, op1, op2)\n-\n-            case \"or\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> or <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Or(lid, sort, op1, op2)\n-\n-            case \"xor\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> xor <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Xor(lid, sort, op1, op2)\n-\n-            case \"concat\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> concat <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Concat(lid, sort, op1, op2)\n-\n-            case \"not\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 4,\\\n-                    \"sort instruction must be of the form: <lid> not <sid> <cond>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                cond = find_inst(p, int(inst[3]))\n-\n-                # Construct instruction\n-                op = Not(lid, sort, cond)\n-\n-            case \"eq\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> eq <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Eq(lid, sort, op1, op2)\n-\n-            case \"neq\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> neq <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Neq(lid, sort, op1, op2)\n-\n-            case \"ugt\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> ugt <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Ugt(lid, sort, op1, op2)\n-\n-            case \"sgt\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> sgt <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Sgt(lid, sort, op1, op2)\n-\n-            case \"ugte\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> ugte <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Ugte(lid, sort, op1, op2)\n-\n-            case \"sgte\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> sgte <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Sgte(lid, sort, op1, op2)\n-\n-            case \"ult\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> ult <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Ult(lid, sort, op1, op2)\n-\n-            case \"slt\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> slt <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Slt(lid, sort, op1, op2)\n-\n-            case \"ulte\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> ulte <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Ulte(lid, sort, op1, op2)\n-\n-            case \"slte\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 5,\\\n-                    \"sort instruction must be of the form: <lid> slte <sid> <op1> <op2>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                op1 = find_inst(p, int(inst[3]))\n-                op2 = find_inst(p, int(inst[4]))\n-\n-                # Construct instruction\n-                op = Slte(lid, sort, op1, op2)\n-\n-            case \"uext\":\n-                # Sanity check: verify that instruction is well formed\n-                assert len(inst) >= 6,\\\n-                    \"sort instruction must be of the form: <lid> uext <sid> <opid> <width> <name>. Found: \" + line\n-\n-                # Find the operands associated to this instruction\n-                sort = find_inst(p, int(inst[2]))\n-                operand = find_inst(p, int(inst[3]))\n-                width = int(inst[4])\n-\n-                # Construct instruction\n-                op = Uext(lid, sort, operand, width, inst[5])\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            value = int(inst[3])\n+\n+            # Construct instruction\n+            op = Constd(lid, sort, value)\n+\n+        case \"consth\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"sort instruction must be of the form: <lid> consth <sid> <value>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            value = int(inst[3])\n+\n+            # Construct instruction\n+            op = Consth(lid, sort, value)\n+\n+        case \"const\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"sort instruction must be of the form: <lid> const <sid> <value>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            # Default base is 2\n+            value = int(inst[3], 2)\n+\n+            # Construct instruction\n+            op = Const(lid, sort, value)\n+\n+        case \"state\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"state instruction must be of the form: <lid> state <sid> <name>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            assert isinstance(sort, Sort), f\"State sort must be a Sort. Found: \" + line\n+            name = inst[3].strip()\n+\n+            # Construct instruction\n+            op = State(lid, sort, name)\n+\n+        case \"init\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> init <sid> <stateid> <valueid>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            state = find_inst(p, int(inst[3]))\n+            val = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Init(lid, sort, state, val)\n+\n+        case \"next\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> next <sid> <stateid> <nextid>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            state = find_inst(p, int(inst[3]))\n+            next = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Next(lid, sort, state, next)\n+\n+        case \"slice\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 6,\\\n+                \"sort instruction must be of the form: <lid> slice <sid> <opid> <width> <lowbit>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            operand = find_inst(p, int(inst[3]))\n+            width = int(inst[4])\n+            lowbit = int(inst[5])\n+\n+            # Construct instruction\n+            op = Slice(lid, sort, operand, width, lowbit)\n+\n+        case \"ite\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 6,\\\n+                \"sort instruction must be of the form: <lid> ite <sid> <condid> <tid> <fid>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            cond = find_inst(p, int(inst[3]))\n+            t = find_inst(p, int(inst[4]))\n+            f = find_inst(p, int(inst[5]))\n+\n+            # Construct instruction\n+            op = Ite(lid, sort, cond, t, f)\n+\n+        case \"implies\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> implies <sid> <lhsid> <rhsid>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            lhs = find_inst(p, int(inst[3]))\n+            rhs = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Implies(lid, sort, lhs, rhs)\n+\n+        case \"iff\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> iff <sid> <lhsid> <rhsid>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            lhs = find_inst(p, int(inst[3]))\n+            rhs = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Iff(lid, sort, lhs, rhs)\n+\n+        case \"add\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> add <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Add(lid, sort, op1, op2)\n+\n+        case \"sub\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> sub <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Sub(lid, sort, op1, op2)\n+\n+        case \"mul\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> mul <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Mul(lid, sort, op1, op2)\n+\n+        case \"sdiv\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> sdiv <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Sdiv(lid, sort, op1, op2)\n+\n+        case \"udiv\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> udiv <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Udiv(lid, sort, op1, op2)\n+\n+        case \"smod\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> smod <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(int(inst[4]))\n+\n+            # Construct instruction\n+            op = Smod(lid, sort, op1, op2)\n+\n+        case \"sll\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> sll <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Sll(lid, sort, op1, op2)\n+\n+        case \"srl\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> srl <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Srl(lid, sort, op1, op2)\n+\n+        case \"sra\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> sra <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Sra(lid, sort, op1, op2)\n+\n+        case \"and\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> and <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = And(lid, sort, op1, op2)\n+\n+        case \"or\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> or <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Or(lid, sort, op1, op2)\n+\n+        case \"xor\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> xor <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Xor(lid, sort, op1, op2)\n+\n+        case \"concat\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> concat <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Concat(lid, sort, op1, op2)\n+\n+        case \"not\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 4,\\\n+                \"sort instruction must be of the form: <lid> not <sid> <cond>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            cond = find_inst(p, int(inst[3]))\n+\n+            # Construct instruction\n+            op = Not(lid, sort, cond)\n+\n+        case \"eq\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> eq <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Eq(lid, sort, op1, op2)\n+\n+        case \"neq\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> neq <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Neq(lid, sort, op1, op2)\n+\n+        case \"ugt\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> ugt <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Ugt(lid, sort, op1, op2)\n+\n+        case \"sgt\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> sgt <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Sgt(lid, sort, op1, op2)\n+\n+        case \"ugte\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> ugte <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Ugte(lid, sort, op1, op2)\n+\n+        case \"sgte\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> sgte <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Sgte(lid, sort, op1, op2)\n+\n+        case \"ult\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> ult <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Ult(lid, sort, op1, op2)\n+\n+        case \"slt\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> slt <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Slt(lid, sort, op1, op2)\n+\n+        case \"ulte\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> ulte <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Ulte(lid, sort, op1, op2)\n+\n+        case \"slte\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 5,\\\n+                \"sort instruction must be of the form: <lid> slte <sid> <op1> <op2>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            op1 = find_inst(p, int(inst[3]))\n+            op2 = find_inst(p, int(inst[4]))\n+\n+            # Construct instruction\n+            op = Slte(lid, sort, op1, op2)\n+\n+        case \"uext\":\n+            # Sanity check: verify that instruction is well formed\n+            assert len(inst) >= 6,\\\n+                \"sort instruction must be of the form: <lid> uext <sid> <opid> <width> <name>. Found: \" + line\n+\n+            # Find the operands associated to this instruction\n+            sort = find_inst(p, int(inst[2]))\n+            operand = find_inst(p, int(inst[3]))\n+            width = int(inst[4])\n+\n+            # Construct instruction\n+            op = Uext(lid, sort, operand, width, inst[5])\n+\n+        case _:\n+            print(f\"Unsupported operation type: {tag} in {line}\")\n+            exit(1)\n+    return op\n+\n+# Parse an entire file that can contain contracts and modules\n+def parse_file(inp: list[str]) -> Program:\n+    m: list[Module] = []\n+    c: list[Contract] = []\n+    i = 0\n+    while i < len(inp):\n+        symbols = inp[i].strip().split(\" \")\n+        # Check whether it's a module or a contract\n+        tag = symbols[0]\n+        match tag:\n+            case \"module\":\n+                name = symbols[1]\n+                # Scan and parse the body\n+                (body, i) = scan_body(inp, i)\n+                b = parse_module_body(body, m)\n+                # Create and store the module\n+                m.append(Module(name, b))\n+                            \n+            case \"contract\":\n+                name = symbols[1]\n+                assert check_name(name, m), f\"Contract name {name} is not defined!\"\n+                (body, i) = scan_body(inp, i)\n+                body = parse_contract_body(body, m)\n+                # Create and store the module\n+                c.append(Contract(name, body))\n+\n+            case \"}\":\n+                i+=1\n+                continue\n \n             case _:\n-                print(f\"Unsupported operation type: {tag} in {line}\")\n+                print(f\"Unsupported structure: {tag} is not module | contract\")\n                 exit(1)\n+            \n+    return Program(m, c)\n \n+# Parse a standard btor2 file, does not handle custom instructions\n+def parse(inp: list[str]) -> list[Instruction]:\n+    # Split the string into instructions and read them 1 by 1\n+    p = []\n+    for line in inp:\n+        op = parse_inst(line, p)\n         if op is not None:\n             p.append(op)\n     return p\ndiff --git a/src/btoropt/passes/genericpass.py b/src/btoropt/passes/genericpass.py\nindex 56223f3..6180cdf 100644\n--- a/src/btoropt/passes/genericpass.py\n+++ b/src/btoropt/passes/genericpass.py\n@@ -18,7 +18,8 @@\n \n # Abstract class for a compiler pass\n \n-from ..program import Instruction\n+from ..program import Instruction, Program, Module\n+import multiprocessing\n \n # Base clas for compiler pass\n # @param id: the unique name of this pass\n@@ -26,5 +27,13 @@ class Pass:\n     def __init__(self, id: str):\n         self.id = id\n \n-    def run(p: list[Instruction]) -> list[Instruction]:\n+    def run(self, p: list[Instruction]) -> list[Instruction]:\n         return p\n+    \n+    # By default runs the standard pass in parallel on all modules\n+    def runOnProgram(self, p: Program) -> Program:\n+        pool = multiprocessing.Pool()\n+        f = lambda m : Module(m.name, self.run(m.body))\n+        p.modules = pool.map(f, p.modules)\n+        return p\n+\ndiff --git a/src/btoropt/program.py b/src/btoropt/program.py\nindex 0dd7224..62f58fa 100644\n--- a/src/btoropt/program.py\n+++ b/src/btoropt/program.py\n@@ -30,16 +30,24 @@\n # All legal sort types\n sort_tags = [\"bitvector\", \"bitvec\", \"array\"]\n \n+# All custom tags\n+custom_tags = [\"inst\", \"set\", \"ref\", \"prec\", \"post\"]\n+structure_tags = [\"module\", \"contract\"]\n+\n # Base class for an instruction\n # @param lid: the line identifier of the instruction\n # @param inst: the string litteral keyword for the instruction\n # @param operands: the list of operands given to this instruction,\n # these must also be instructions\n+# @param is_standard: whether or not the instruction is part of the btor2\n+#   True: instruction is part of the btor2 spec\n+#   False: instruction is a custom extension for btor-opt\n class Instruction:\n-    def __init__(self, lid: int, inst: str, operands = []):\n+    def __init__(self, lid: int, inst: str, operands = [], is_standard=True):\n         self.lid = lid\n         self.inst = inst\n         self.operands = operands\n+        self.is_standard = is_standard\n \n     def move_up(self, amount: int):\n         self.lid += amount\n@@ -69,10 +77,11 @@ def serialize(self) -> str:\n def serialize_p(p: list[Instruction]) -> str:\n     return reduce(lambda acc, s: acc + s.serialize() + \"\\n\", p, \"\")\n \n+# Extracts an instruction from a given program\n def get_inst(p: list[Instruction], lid: int) -> Instruction:\n-    for op in p:\n-        if op.lid == lid:\n-            return op\n+    ops = [op for op in p if op.lid == lid]\n+    if len(ops) > 0:\n+        return ops[0]\n     return None\n \n # Sort declaration instruction\n@@ -106,12 +115,13 @@ def eq(self, inst) -> bool:\n     def serialize(self) -> str:\n         return super().serialize() + self.name\n \n-## Unary Instructions ##\n \n class Output(Instruction):\n     def __init__(self, lid: int, out: Instruction):\n         super().__init__(lid, \"output\", [out])\n \n+## Unary Instructions ##\n+\n class Bad(Instruction):\n     def __init__(self, lid: int, cond: Instruction):\n         super().__init__(lid, \"bad\", [cond])\n@@ -330,3 +340,120 @@ class Sext(Instruction):\n     def __init__(self, lid: int, sort: Sort, op: Instruction, width: int, name: str):\n         super().__init__(lid, \"sext\", [sort, op, width, name])\n         self.width: int = width\n+\n+\n+############ NON-STANDARD: Custom extensions for btor-opt ############\n+\n+# Precondition instruction\n+# This becomes a \"x not cond; bad x\" when verifying an instance\n+#      becomes a \"constraint cond\" when verifying a module\n+class Prec(Instruction):\n+    def __init__(self, lid: int, cond: Instruction):\n+        super().__init__(lid, \"prec\", [cond], False)\n+\n+# Postcondition instruction\n+# This becomes a \"constraint cond\" when verifying an instance\n+#      becomes a \"x not cond; bad x\" when verifying a module\n+class Post(Instruction):\n+    def __init__(self, lid: int, cond: Instruction):\n+        super().__init__(lid, \"post\", [cond], False)\n+\n+# Instance Instruction\n+# Creates an instance of a named module\n+# @param name: the name of the module to instantiate\n+class Instance(Instruction):\n+    def __init__(self, lid: int, name: str):\n+        super().__init__(lid, \"inst\", [], False)\n+        self.name = name\n+\n+# Reference to an instruction in a different named region\n+# Has a weird infix notation `<mod_name>:<lid>`\n+# Not really an instruction, more of a reference to an instruction\n+class Ref(Instruction):\n+    def __init__(self, lid: int, name: str, val: Instruction):\n+        super().__init__(lid, \":\", [val], False)\n+        self.name = name\n+        self.val = val\n+\n+# Set Instruction\n+# Similarly to an alias, this sets the inputs of an instance to a specific operation\n+# @param instance: the instance this is setting inputs for\n+# @param ref: a reference to the module's input we want to set, e.g. A:2\n+# @param alias: the instruction we want to set the input to\n+class Set(Instruction):\n+    def __init__(self, lid: int, instance: Instance, ref: Ref, alias: Instruction):\n+        super().__init__(lid, \"set\", [instance, ref, alias], False)\n+\n+\n+# Structural extensions\n+class ModuleLike():\n+    def __init__(self, name: str, body: list[Instruction]) -> None:\n+        self.name = name\n+        self.body = body\n+\n+    def get_inst(self, i: int) -> Instruction:\n+        return self.body[i]\n+\n+# Module instruction \n+# Declares a named region of standard instructions\n+# Can be referred to by name and associated with a contract\n+class Module(ModuleLike):\n+    def __init__(self, name: str, body: list[Instruction]) -> None:\n+        super().__init__(name, body)\n+   \n+# Contract instruction \n+# Declares a named region of custom instructions\n+# Only preconditions and postconditions are allowed\n+# Name must be an existing module name \n+class Contract(ModuleLike):\n+    def __init__(self, name: str, body: list[Instruction]) -> None:\n+        super().__init__(name, body)\n+        self.preconditions = [i for i in body if isinstance(i, Prec)]\n+        self.postconditions = [i for i in body if isinstance(i, Post)]\n+        assert len(self.preconditions) > 0 or len(self.postconditions) > 0, \\\n+            \"Contracts must contain either a precondition or a post-condition!\"\n+        \n+\n+# Base class for a custom btor2 file (standard is simply a list of instructions)\n+class Program():\n+    def __init__(self, modules: list[Module], contracts: list[Contract]) -> None:\n+        self.modules = modules\n+        # Ignore all contracts that don't have an existing name\n+        self.contracts = contracts\n+        ## Sanity check: We should have as many modules as there are contracts\n+        assert len(modules) >= len(contracts), \\\n+            \"There should be at least as many modules as there are contracts!\"\n+        ## Sanity check: Each module should have at most one contract\n+        for m in modules:\n+            cs = [c for c in contracts if c.name == m.name]\n+            assert len(cs) <= 1, f\"Module {m.name} has more than one contract!\"\n+        ## Sanity check: Each contract should name a module exactly once\n+        for c in contracts:\n+            ms = [m for m in modules if c.name == m.name]\n+            assert len(ms) == 1, f\"Contract {c.name} references {str(len(ms))} modules instead of 1!\"\n+    \n+    # Retrieves a module by its given name\n+    # @param name: the name of the module we want to retrive\n+    def get_module(self, name: str) -> Module:\n+        res = [x for x in self.modules if x.name == name]\n+        ## Check if the given name is defined\n+        assert len(res) > 0 , f\"name: {name} is not defined!\"\n+        return res[0]\n+    \n+    # Retrieves a contract by its given name\n+    # @param name: the name of the contract we want to retrive\n+    def get_contract(self, name: str) -> Contract:\n+        res = [x for x in self.contracts if x.name == name]\n+        ## Check if the given name is defined, otherwise return None\n+        if len(res) == 0:\n+            return None\n+        return res[0]\n+    \n+    # Retrieves the contract associated to a module if it exists\n+    # If it does not exist then simply return None\n+    def get_module_contract(self, module: Module) -> Contract:\n+        c = self.get_contract(module.name)\n+        ## Check that a contract was found\n+        return c\n+\n+########################################################################\n", "instance_id": "dobios__btor2-opt-8", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal of adding modularity support to the btor2 language for benchmarking purposes within the CIRCT contract system. It provides a detailed example with a Hoare triple and introduces new constructs like `module`, `contract`, `inst`, `ref`, and `set`, along with their intended behavior (e.g., asserting preconditions and assuming postconditions). The input format and expected output (valid standard btor2 files per module) are also outlined. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem does not fully specify how the transformation to standard btor2 should handle complex interactions between modules and contracts (e.g., nested instantiations or cyclic dependencies). Additionally, edge cases such as invalid module references or contract mismatches are not addressed, which could lead to implementation uncertainties. Overall, while the core intent and structure are clear, these minor gaps in specification warrant a score of 2 (Mostly Clear).\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category (0.6-0.8), due to several factors. First, the scope of code changes is significant, involving multiple files (`__main__.py`, `parser.py`, `genericpass.py`, `program.py`) and requiring substantial additions to the codebase, such as new data structures (`Module`, `Contract`, `Program`) and parsing logic for custom instructions (`inst`, `set`, `ref`, etc.). This indicates a deep impact on the system's architecture, as the parser and pass infrastructure must now handle modular structures and contracts, which is a non-trivial extension to the original btor2 processing pipeline. Second, the number of technical concepts involved is high, including advanced parsing techniques, object-oriented design for representing program structures, and parallel processing for pass execution on modules (using `multiprocessing`). Additionally, domain-specific knowledge of formal verification and contract systems (e.g., Hoare triples, preconditions/postconditions) is necessary to correctly implement the semantics of the new constructs. Third, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest the need to handle scenarios like undefined module references, contract validation, and ensuring correct precondition/postcondition behavior during transformation, which adds complexity to error handling. Although it does not reach the \"Very Hard\" category (0.8-1.0) due to the absence of system-level or highly intricate algorithmic challenges (e.g., distributed systems or advanced optimization), the combination of architectural changes, multiple technical concepts, and implicit edge case handling makes this a challenging task requiring a deep understanding of the codebase and domain. Hence, a score of 0.75 is appropriate.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use lower-case module names everywhere\nWe currently have CamelCase class names inside CamelCase module files with the same name, and expose classes on module level by importing them in `__init__.py`, e.g.:\r\n\r\nhttps://github.com/fmi-faim/faim-ipa/blob/f4191359d3f35144190188e59dcdb90eebcefb78/src/faim_ipa/hcs/cellvoyager/__init__.py#L1-L2\r\nhttps://github.com/fmi-faim/faim-ipa/blob/f4191359d3f35144190188e59dcdb90eebcefb78/src/faim_ipa/hcs/imagexpress/__init__.py#L1-L5\r\n\r\nThis is problematic, as changing the order of imports such as:\r\n\r\n```patch\r\nfrom .ImageXpressPlateAcquisition import ImageXpressPlateAcquisition  # noqa: F401\r\nfrom .ImageXpressWellAcquisition import ImageXpressWellAcquisition  # noqa: F401\r\n+++ from .MixedAcquisition import MixedAcquisition  # noqa: F401\r\nfrom .SinglePlaneAcquisition import SinglePlaneAcquisition  # noqa: F401\r\nfrom .StackAcquisition import StackAcquisition  # noqa: F401\r\n--- from .MixedAcquisition import MixedAcquisition  # noqa: F401\r\n```\r\n\r\nleads to errors like this:\r\n\r\n```\r\nERROR tests/hcs/imagexpress/test_ImageXpress.py - TypeError: module() takes at most 2 arguments (3 given)\r\n```\r\n\r\n.. as we mix modules and classes with the same name.\r\n\r\n---\r\n\r\nWe should instead use all-lower-case names for the module files, and `CamelCase` exclusively for the class names.\r\nWhat do you think, @tibuch?\r\n\r\n\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex 85295861..8923d42d 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -91,3 +91,4 @@ mobie-example-project/\n \n # scratchpad\n scratchpad/\n+.pixi\ndiff --git a/pyproject.toml b/pyproject.toml\nindex c3631b4a..cdbada1e 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -7,17 +7,16 @@ name = \"faim-ipa\"\n dynamic = [\"version\"]\n description = 'Tools used at FMI-FAIM for Image Processing and Analysis.'\n readme = \"README.md\"\n-requires-python = \">=3.9\"\n+requires-python = \">=3.10\"\n license = \"BSD-3-Clause\"\n keywords = []\n authors = [\n-  { name = \"Tim-Oliver Buchholz\", email = \"tim-oliver.buchholz@fmi.ch\" },\n-  { name = \"Jan Eglinger\", email = \"jan.eglinger@fmi.ch\" },\n+  {name = \"Tim-Oliver Buchholz\", email = \"tim-oliver.buchholz@fmi.ch\"},\n+  {name = \"Jan Eglinger\", email = \"jan.eglinger@fmi.ch\"},\n ]\n classifiers = [\n   \"Development Status :: 4 - Beta\",\n   \"Programming Language :: Python\",\n-  \"Programming Language :: Python :: 3.9\",\n   \"Programming Language :: Python :: 3.10\",\n   \"Programming Language :: Python :: 3.11\",\n   \"Programming Language :: Python :: 3.12\",\n@@ -25,15 +24,16 @@ classifiers = [\n   \"Programming Language :: Python :: Implementation :: PyPy\",\n ]\n dependencies = [\n-  \"pydantic\",\n+  \"defusedxml\",\n   \"distributed\",\n-  \"numpy<2\",\n   \"matplotlib\",\n-  \"scikit-image\",\n-  \"pandas\",\n+  \"numpy<2\",\n   \"ome-zarr\",\n+  \"pandas\",\n+  \"pint\",\n+  \"pydantic>=2\",\n+  \"scikit-image\",\n   \"tqdm\",\n-  \"pint\"\n ]\n \n [project.urls]\n@@ -43,14 +43,46 @@ Homepage = \"https://github.com/fmi-faim/faim-ipa.git\"\n \"Source Code\" = \"https://github.com/fmi-faim/faim-ipa\"\n \"User Support\" = \"https://github.com/fmi-faim/faim-ipa/issues\"\n \n-[tool.hatch.version]\n-source = \"vcs\"\n+[tool.coverage.paths]\n+faim_ipa = [\"src/faim_ipa\", \"*/faim-ipa/src/faim_ipa\"]\n+tests = [\"tests\", \"*/faim-ipa/tests\"]\n+\n+[tool.coverage.report]\n+show_missing = true\n+exclude_lines = [\n+  \"no cov\",\n+  \"if __name__ == .__main__.:\",\n+  \"if TYPE_CHECKING:\",\n+]\n+\n+[tool.coverage.run]\n+source_pkgs = [\"faim_ipa\", \"tests\"]\n+branch = true\n+parallel = true\n+omit = [\n+  \"src/faim_ipa/__about__.py\",\n+]\n+\n+[tool.hatch.build.targets.sdist]\n+exclude = [\n+  \"/examples\",\n+  \"/resources\",\n+  \"*/__pycache__\",\n+  \"/tests\",\n+  \"/scratchpad\",\n+  \"/.github\",\n+  \".pre-commit-config.yaml\",\n+]\n+\n+[[tool.hatch.envs.all.matrix]]\n+python = [\"3.10\", \"3.11\", \"3.12\"]\n \n [tool.hatch.envs.default]\n dependencies = [\n   \"coverage[toml]>=6.5\",\n   \"pytest\",\n ]\n+\n [tool.hatch.envs.default.scripts]\n test = \"pytest {args:tests}\"\n test-cov = \"coverage run -m pytest {args:tests}\"\n@@ -73,49 +105,31 @@ cov = [\n   \"cov-report\",\n ]\n \n-[[tool.hatch.envs.all.matrix]]\n-python = [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n-\n [tool.hatch.envs.types]\n dependencies = [\n   \"mypy>=1.0.0\",\n ]\n+\n [tool.hatch.envs.types.scripts]\n check = \"mypy --install-types --non-interactive {args:src/faim_ipa tests}\"\n \n [tool.hatch.metadata]\n allow-direct-references = true\n \n-[tool.hatch.build.targets.sdist]\n-exclude = [\n-   \"/examples\",\n-  \"/resources\",\n-  \"*/__pycache__\",\n-  \"/tests\",\n-  \"/scratchpad\",\n-  \"/.github\",\n-  \".pre-commit-config.yaml\",\n-]\n+[tool.hatch.version]\n+source = \"vcs\"\n \n-[tool.coverage.run]\n-source_pkgs = [\"faim_ipa\", \"tests\"]\n-branch = true\n-parallel = true\n-omit = [\n-  \"src/faim_ipa/__about__.py\",\n+[tool.pytest.ini_options]\n+addopts = [\n+    \"--import-mode=importlib\",\n ]\n \n-[tool.coverage.paths]\n-faim_ipa = [\"src/faim_ipa\", \"*/faim-ipa/src/faim_ipa\"]\n-tests = [\"tests\", \"*/faim-ipa/tests\"]\n+[tool.ruff.lint]\n+ignore = [\"FA100\", \"S101\"]\n \n-[tool.coverage.report]\n-show_missing = true\n-exclude_lines = [\n-  \"no cov\",\n-  \"if __name__ == .__main__.:\",\n-  \"if TYPE_CHECKING:\",\n-]\n+[tool.ruff.lint.extend-per-file-ignores]\n+\"**/tests/*\" = [\"F811\", \"INP001\", \"SLF001\"]\n+\"**/examples/*\" = [\"INP001\"]\n \n [tool.setuptools_scm]\n write_to = \"src/faim_ipa/_version.py\"\ndiff --git a/src/faim_ipa/alignment/__init__.py b/src/faim_ipa/alignment/__init__.py\nindex 453dfe1c..702439fb 100644\n--- a/src/faim_ipa/alignment/__init__.py\n+++ b/src/faim_ipa/alignment/__init__.py\n@@ -1,1 +1,1 @@\n-from .alignment import GridAlignment, StageAlignment  # noqa: F401\n+from faim_ipa.alignment.alignment import GridAlignment, StageAlignment  # noqa: F401\ndiff --git a/src/faim_ipa/alignment/alignment.py b/src/faim_ipa/alignment/alignment.py\nindex b1f5b2fe..3a6f8475 100644\n--- a/src/faim_ipa/alignment/alignment.py\n+++ b/src/faim_ipa/alignment/alignment.py\n@@ -1,9 +1,10 @@\n-from abc import ABC\n+from abc import ABC, abstractmethod\n from copy import copy\n \n import numpy as np\n \n-from faim_ipa.stitching import Tile, stitching_utils\n+from faim_ipa.stitching import stitching_utils\n+from faim_ipa.stitching.tile import Tile\n \n \n class AbstractAlignment(ABC):\n@@ -15,6 +16,7 @@ def __init__(self, tiles: list[Tile]) -> None:\n         self._unaligned_tiles = stitching_utils.shift_to_origin(tiles)\n         self._aligned_tiles = self._align(self._unaligned_tiles)\n \n+    @abstractmethod\n     def _align(self, tiles: list[Tile]) -> list[Tile]:\n         raise NotImplementedError\n \n@@ -45,12 +47,12 @@ def _align(self, tiles: list[Tile]) -> list[Tile]:\n         grid_positions_x = set()\n         tile_map = {}\n         for tile in tiles:\n-            assert (\n-                tile.shape[-2:] == tile_shape[-2:]\n-            ), \"All tiles must have the same YX shape.\"\n+            if tile.shape[-2:] != tile_shape[-2:]:\n+                message = f\"All tiles must have the same YX shape. {tile.shape[-2:]} <=> {tile_shape[-2:]}\"\n+                raise ValueError(message)\n             y_pos = int(np.round(tile.position.y / tile_shape[-2]))\n             x_pos = int(np.round(tile.position.x / tile_shape[-1]))\n-            if (y_pos, x_pos) in tile_map.keys():\n+            if (y_pos, x_pos) in tile_map:\n                 tile_map[(y_pos, x_pos)].append(tile)\n             else:\n                 tile_map[(y_pos, x_pos)] = [tile]\n@@ -61,7 +63,7 @@ def _align(self, tiles: list[Tile]) -> list[Tile]:\n         grid_positions_x = sorted(grid_positions_x)\n         for y_pos in grid_positions_y:\n             for x_pos in grid_positions_x:\n-                if (y_pos, x_pos) in tile_map.keys():\n+                if (y_pos, x_pos) in tile_map:\n                     for unaligned_tile in tile_map[(y_pos, x_pos)]:\n                         new_tile = copy(unaligned_tile)\n                         new_tile.position.y = y_pos * tile_shape[-2]\ndiff --git a/src/faim_ipa/dask_utils.py b/src/faim_ipa/dask_utils.py\nindex 6f11ac4f..20236128 100644\n--- a/src/faim_ipa/dask_utils.py\n+++ b/src/faim_ipa/dask_utils.py\n@@ -1,100 +1,8 @@\n-from multiprocessing import Process, Queue\n-from time import sleep\n+from __future__ import annotations\n \n import numpy as np\n \n \n-class LocalClusterFactory:\n-    \"\"\"Creates a local dask cluster in a sub-process.\"\"\"\n-\n-    def __init__(\n-        self,\n-        n_workers: int = None,\n-        threads_per_worker: int = None,\n-        processes: bool = None,\n-        memory_limit: str = None,\n-        local_directory: str = None,\n-    ):\n-        self.n_workers = n_workers\n-        self.threads_per_worker = threads_per_worker\n-        self.processes = processes\n-        self.memory_limit = memory_limit\n-        self.local_directory = local_directory\n-        self._queue = Queue(1)\n-        self._scheduler_address = None\n-\n-        self._subprocess = Process(\n-            target=self._run_cluster,\n-            args=(\n-                self._queue,\n-                self.n_workers,\n-                self.threads_per_worker,\n-                self.processes,\n-                self.memory_limit,\n-                self.local_directory,\n-            ),\n-        )\n-        self._subprocess.start()\n-        self._scheduler_address = None\n-        self._client = None\n-\n-    def _get_scheduler_address(self):\n-        if self._scheduler_address is None:\n-            scheduler_address = self._queue.get()\n-            self._scheduler_address = scheduler_address\n-            self._queue.close()\n-            self._queue = None\n-\n-        return self._scheduler_address\n-\n-    def _shutdown(self):\n-        if self._client is not None and self._client.scheduler is not None:\n-            self._client.shutdown()\n-            self._client = None\n-\n-        if self._subprocess is not None and self._subprocess.is_alive():\n-            self._subprocess.join()\n-\n-    def __del__(self):\n-        self._shutdown()\n-\n-    @staticmethod\n-    def _run_cluster(\n-        queue: Queue,\n-        n_workers: int,\n-        threads_per_worker: int,\n-        processes: bool,\n-        memory_limit: str,\n-        local_directory: str,\n-    ):\n-        import dask\n-        import distributed\n-\n-        dask.config.set({\"distributed.workers.memory.spill\": 0.90})\n-        dask.config.set({\"distributed.workers.memory.target\": 0.85})\n-        dask.config.set({\"distributed.workers.memory.terminate\": 0.98})\n-\n-        client = distributed.Client(\n-            n_workers=n_workers,\n-            threads_per_worker=threads_per_worker,\n-            processes=processes,\n-            memory_limit=memory_limit,\n-            local_directory=local_directory,\n-        )\n-        queue.put(client.cluster.scheduler.address)\n-        while client.cluster.scheduler.status.value != \"closed\":\n-            sleep(5)\n-\n-    def get_client(self):\n-        \"\"\"Get a dask client for the local cluster.\"\"\"\n-        if self._client is None:\n-            import distributed\n-\n-            self._client = distributed.Client(self._get_scheduler_address())\n-\n-        return self._client\n-\n-\n def mean_cast_to(target_dtype):\n     \"\"\"\n     Wrap np.mean to cast the result to a given dtype.\ndiff --git a/src/faim_ipa/detection/blobs.py b/src/faim_ipa/detection/blobs.py\nindex 2b8d110f..c0e46f2a 100644\n--- a/src/faim_ipa/detection/blobs.py\n+++ b/src/faim_ipa/detection/blobs.py\n@@ -1,11 +1,11 @@\n-from typing import Optional\n+from __future__ import annotations\n \n import numpy as np\n from scipy.ndimage import gaussian_laplace\n from skimage.feature import peak_local_max\n-from skimage.morphology import h_maxima, ball\n-from skimage.util import img_as_float32\n from skimage.feature.blob import _prune_blobs\n+from skimage.morphology import ball, h_maxima\n+from skimage.util import img_as_float32\n \n from faim_ipa.detection.utils import estimate_log_rescale_factor\n \n@@ -17,7 +17,7 @@ def detect_blobs(\n     h: int,\n     scale_factors: list[int],\n     overlap: float,\n-    background_img: Optional[np.ndarray] = None,\n+    background_img: np.ndarray | None = None,\n ) -> np.ndarray:\n     \"\"\"Detect blobs of different sizes.\n \n@@ -49,10 +49,11 @@ def detect_blobs(\n     -------\n     Detected spots.\n     \"\"\"\n-    if background_img is not None:\n-        image = img_as_float32(img) - img_as_float32(background_img)\n-    else:\n-        image = img_as_float32(img)\n+    image = (\n+        img_as_float32(img) - img_as_float32(background_img)\n+        if background_img is not None\n+        else img_as_float32(img)\n+    )\n \n     rescale_factor = estimate_log_rescale_factor(\n         axial_sigma=axial_sigma, lateral_sigma=lateral_sigma\n@@ -62,7 +63,7 @@ def detect_blobs(\n         (axial_sigma * f, lateral_sigma * f, lateral_sigma * f) for f in scale_factors\n     ]\n \n-    scale_cube = np.empty(image.shape + (len(sigmas),), dtype=np.uint8)\n+    scale_cube = np.empty((*image.shape, len(sigmas)), dtype=np.uint8)\n \n     h_ = img_as_float32(np.array(h, dtype=img.dtype))\n     scale_norm = np.mean([axial_sigma, lateral_sigma, lateral_sigma])\ndiff --git a/src/faim_ipa/detection/spots.py b/src/faim_ipa/detection/spots.py\nindex 9ff97c8c..f3267ccd 100644\n--- a/src/faim_ipa/detection/spots.py\n+++ b/src/faim_ipa/detection/spots.py\n@@ -1,8 +1,8 @@\n-from typing import Optional\n+from __future__ import annotations\n \n import numpy as np\n from scipy.ndimage import gaussian_laplace\n-from skimage.morphology import h_maxima, ball\n+from skimage.morphology import ball, h_maxima\n from skimage.util import img_as_float32\n \n from faim_ipa.detection.utils import estimate_log_rescale_factor\n@@ -13,7 +13,7 @@ def detect_spots(\n     axial_sigma: float,\n     lateral_sigma: float,\n     h: int,\n-    background_img: Optional[np.ndarray] = None,\n+    background_img: np.ndarray | None = None,\n ) -> np.ndarray:\n     \"\"\"Detect diffraction limited spots.\n \n@@ -39,10 +39,11 @@ def detect_spots(\n     -------\n     Detected spots.\n     \"\"\"\n-    if background_img is not None:\n-        image = img_as_float32(img) - img_as_float32(background_img)\n-    else:\n-        image = img_as_float32(img)\n+    image = (\n+        img_as_float32(img) - img_as_float32(background_img)\n+        if background_img is not None\n+        else img_as_float32(img)\n+    )\n \n     rescale_factor = estimate_log_rescale_factor(\n         axial_sigma=axial_sigma, lateral_sigma=lateral_sigma\ndiff --git a/src/faim_ipa/detection/utils.py b/src/faim_ipa/detection/utils.py\nindex a12140b9..1349ae40 100644\n--- a/src/faim_ipa/detection/utils.py\n+++ b/src/faim_ipa/detection/utils.py\n@@ -1,8 +1,10 @@\n import numpy as np\n-from scipy.ndimage import gaussian_laplace, gaussian_filter\n+from scipy.ndimage import gaussian_filter, gaussian_laplace\n \n \n-def compute_axial_sigma(wavelength: float, NA: float, axial_spacing: float):\n+def compute_axial_sigma(\n+    wavelength: float, NA: float, axial_spacing: float  # noqa: N803\n+):\n     \"\"\"\n     Sigma which produces a Gaussian with the same full width\n     half maximum as described by Abbe's diffraction formula for axial resolution.\n@@ -25,7 +27,9 @@ def compute_axial_sigma(wavelength: float, NA: float, axial_spacing: float):\n     return 2 * wavelength / (NA**2) / (2 * np.sqrt(2 * np.log(2))) / axial_spacing\n \n \n-def compute_lateral_sigma(wavelength: float, NA: float, lateral_spacing: float):\n+def compute_lateral_sigma(\n+    wavelength: float, NA: float, lateral_spacing: float  # noqa: N803\n+):\n     \"\"\"\n     Sigma which produces a Gaussian with the same full width\n     half maximum as the theoretical resolution limit in Y/X described by E. Abbe.\ndiff --git a/src/faim_ipa/hcs/acquisition.py b/src/faim_ipa/hcs/acquisition.py\nindex ba73d8c8..eee1a2b0 100644\n--- a/src/faim_ipa/hcs/acquisition.py\n+++ b/src/faim_ipa/hcs/acquisition.py\n@@ -2,13 +2,13 @@\n from collections.abc import Iterable\n from enum import Enum\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import pandas as pd\n \n-from faim_ipa.io.ChannelMetadata import ChannelMetadata\n-from faim_ipa.stitching import Tile\n+from faim_ipa.io.metadata import ChannelMetadata\n+from faim_ipa.stitching.tile import Tile\n \n \n class TileAlignmentOptions(Enum):\n@@ -22,16 +22,16 @@ class PlateAcquisition(ABC):\n     _acquisition_dir = None\n     _wells = None\n     _alignment: TileAlignmentOptions = None\n-    _background_correction_matrices: Optional[dict[str, Union[Path, str]]]\n-    _illumination_correction_matrices: Optional[dict[str, Union[Path, str]]]\n+    _background_correction_matrices: dict[str, Path | str] | None\n+    _illumination_correction_matrices: dict[str, Path | str] | None\n     _common_well_shape: tuple[int, int, int, int, int] = None\n \n     def __init__(\n         self,\n-        acquisition_dir: Union[Path, str],\n+        acquisition_dir: Path | str,\n         alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]],\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]],\n+        background_correction_matrices: dict[str, Path | str] | None,\n+        illumination_correction_matrices: dict[str, Path | str] | None,\n     ) -> None:\n         self._acquisition_dir = acquisition_dir\n         self._alignment = alignment\n@@ -57,19 +57,18 @@ def _build_well_acquisitions(self, files: pd.DataFrame) -> list[\"WellAcquisition\n         raise NotImplementedError\n \n     def get_well_acquisitions(\n-        self, selection: Optional[list[str]] = None\n+        self, selection: list[str] | None = None\n     ) -> list[\"WellAcquisition\"]:\n         if selection is None:\n             return self._wells\n-        else:\n-            return [well for well in self._wells if well.name in selection]\n+        return [well for well in self._wells if well.name in selection]\n \n     @abstractmethod\n     def get_channel_metadata(self) -> dict[int, ChannelMetadata]:\n         \"\"\"Channel metadata.\"\"\"\n         raise NotImplementedError\n \n-    def get_well_names(self, wells: Optional[list[str]] = None) -> Iterable[str]:\n+    def get_well_names(self, wells: list[str] | None = None) -> Iterable[str]:\n         \"\"\"\n         Get the names of all wells in the acquisition.\n         \"\"\"\n@@ -88,7 +87,7 @@ def get_omero_channel_metadata(self) -> list[dict]:\n         ch_metadata = self.get_channel_metadata()\n         max_channel = max(list(ch_metadata.keys()))\n         for index in range(max_channel + 1):\n-            if index in ch_metadata.keys():\n+            if index in ch_metadata:\n                 metadata = ch_metadata[index]\n                 ome_channels.append(\n                     {\n@@ -137,10 +136,7 @@ def get_common_well_shape(self) -> tuple[int, int, int, int, int]:\n             (time, channel, z, y, x)\n         \"\"\"\n         if self._common_well_shape is None:\n-            well_shapes = []\n-            for well in self.get_well_acquisitions():\n-                well_shapes.append(well.get_shape())\n-\n+            well_shapes = [well.get_shape() for well in self.get_well_acquisitions()]\n             self._common_well_shape = tuple(np.max(well_shapes, axis=0))\n \n         return self._common_well_shape\n@@ -154,8 +150,8 @@ class WellAcquisition(ABC):\n     name: str = None\n     _files = None\n     _alignment: TileAlignmentOptions = None\n-    _background_correction_matrices: Optional[dict[str, Union[Path, str]]]\n-    _illumination_correction_matrices: Optional[dict[str, Union[Path, str]]]\n+    _background_correction_matrices: dict[str, Path | str] | None\n+    _illumination_correction_matrices: dict[str, Path | str] | None\n     _tiles = None\n     _shape: tuple[int, int] = None\n     _dtype: np.dtype = None\n@@ -164,12 +160,12 @@ def __init__(\n         self,\n         files: pd.DataFrame,\n         alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]],\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]],\n+        background_correction_matrices: dict[str, Path | str] | None,\n+        illumination_correction_matrices: dict[str, Path | str] | None,\n     ) -> None:\n-        assert (\n-            files[\"well\"].nunique() == 1\n-        ), \"WellAcquisition must contain files from a single well.\"\n+        if files[\"well\"].nunique() != 1:\n+            msg = \"WellAcquisition must contain files from a single well.\"\n+            raise ValueError(msg)\n         self.name = files[\"well\"].iloc[0]\n         self._files = files\n         self._alignment = alignment\n@@ -207,7 +203,8 @@ def _align_tiles(self, tiles: list[Tile]) -> list[Tile]:\n \n             return GridAlignment(tiles=tiles).get_tiles()\n \n-        raise ValueError(f\"Unknown alignment option: {self._alignment}\")\n+        msg = f\"Unknown alignment option: {self._alignment}\"\n+        raise ValueError(msg)\n \n     def get_tiles(self) -> list[Tile]:\n         \"\"\"List of tiles.\"\"\"\n@@ -238,7 +235,7 @@ def get_yx_spacing(self) -> tuple[float, float]:\n         raise NotImplementedError\n \n     @abstractmethod\n-    def get_z_spacing(self) -> Optional[float]:\n+    def get_z_spacing(self) -> float | None:\n         \"\"\"\n         Get the z spacing of the well acquisition.\n         \"\"\"\n@@ -307,12 +304,11 @@ def get_shape(self):\n         Compute the theoretical shape of the stitched well image.\n         \"\"\"\n         if self._shape is None:\n-            tile_extents = []\n-            for tile in self._tiles:\n-                tile_extents.append(\n-                    tile.get_position()\n-                    + np.array((1,) * (5 - len(tile.shape)) + tile.shape)\n-                )\n+            tile_extents = [\n+                tile.get_position()\n+                + np.array((1,) * (5 - len(tile.shape)) + tile.shape)\n+                for tile in self._tiles\n+            ]\n             self._shape = tuple(np.max(tile_extents, axis=0))\n \n         return self._shape\ndiff --git a/src/faim_ipa/hcs/cellvoyager/CellVoyagerWellAcquisition.py b/src/faim_ipa/hcs/cellvoyager/CellVoyagerWellAcquisition.py\ndeleted file mode 100644\nindex a71346b4..00000000\n--- a/src/faim_ipa/hcs/cellvoyager/CellVoyagerWellAcquisition.py\n+++ /dev/null\n@@ -1,155 +0,0 @@\n-from decimal import Decimal\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-import numpy as np\n-import pandas as pd\n-from tifffile import imread\n-\n-from faim_ipa.hcs.acquisition import TileAlignmentOptions, WellAcquisition\n-from faim_ipa.hcs.cellvoyager.StackedTile import StackedTile\n-from faim_ipa.stitching import Tile\n-from faim_ipa.stitching.Tile import TilePosition\n-\n-\n-class CellVoyagerWellAcquisition(WellAcquisition):\n-    \"\"\"\n-    Data structure for a CellVoyager well acquisition.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        files: pd.DataFrame,\n-        alignment: TileAlignmentOptions,\n-        metadata: pd.DataFrame,\n-        background_correction_matrices: dict[str, Union[Path, str]] = None,\n-        illumination_correction_matrices: dict[str, Union[Path, str]] = None,\n-        n_planes_in_stacked_tile: int = 1,\n-    ):\n-        self._metadata = metadata\n-        self._z_spacing = self._compute_z_spacing(files)\n-        self._dtype = self._get_dtype(files)\n-        self._n_planes_in_stacked_tile = n_planes_in_stacked_tile\n-        super().__init__(\n-            files=files,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrices,\n-            illumination_correction_matrices=illumination_correction_matrices,\n-        )\n-\n-    def _get_dtype(self, files: pd.DataFrame) -> np.dtype:\n-        return imread(files[\"path\"].iloc[0]).dtype\n-\n-    def _compute_z_spacing(self, files: pd.DataFrame) -> Optional[float]:\n-        if \"ZIndex\" in files.columns:\n-            z_steps = np.array(\n-                files.astype({\"Z\": float}).groupby(\"ZIndex\", sort=True).mean(\"Z\")[\"Z\"]\n-            )\n-\n-            precision = -Decimal(str(z_steps[0])).as_tuple().exponent\n-            z_step = np.round(np.mean(np.diff(z_steps)), decimals=precision)\n-            return z_step\n-        else:\n-            return None\n-\n-    def _assemble_tiles(self) -> list[Tile]:\n-        min_z_index = 0\n-        max_z_index = min_z_index + 1\n-        if \"ZIndex\" in self._files.columns:\n-            min_z_index = self._files[\"ZIndex\"].min()\n-            max_z_index = self._files[\"ZIndex\"].max() + 1\n-\n-        tiles = {}\n-        for i, row in self._files.iterrows():\n-            if \"ZIndex\" in row:\n-                tile_z_index = (\n-                    row[\"ZIndex\"] - min_z_index\n-                ) // self._n_planes_in_stacked_tile\n-            else:\n-                tile_z_index = min_z_index\n-            tczyx_index = (\n-                row[\"TimePoint\"],\n-                row[\"Ch\"],\n-                tile_z_index,\n-                row[\"Y\"],\n-                row[\"X\"],\n-            )\n-            if tczyx_index not in tiles:\n-                tiles[tczyx_index] = [row]\n-            else:\n-                tiles[tczyx_index].append(row)\n-\n-        stacked_tiles = []\n-        for tczyx_index, rows in tiles.items():\n-            row_dict = {}\n-            for r in rows:\n-                if \"ZIndex\" in r:\n-                    row_dict[r[\"ZIndex\"]] = r[\"path\"]\n-                else:\n-                    row_dict[min_z_index] = r[\"path\"]\n-\n-            files = []\n-            z_start = tczyx_index[2] * self._n_planes_in_stacked_tile + min_z_index\n-            for z in range(\n-                z_start, min(z_start + self._n_planes_in_stacked_tile, max_z_index)\n-            ):\n-                if z in row_dict.keys():\n-                    files.append(row_dict[z])\n-                else:\n-                    files.append(None)\n-\n-            time_point = tczyx_index[0]\n-            channel = tczyx_index[1]\n-            y, x = tczyx_index[3], tczyx_index[4]\n-\n-            ch_metadata = self._metadata[self._metadata[\"Ch\"] == channel].iloc[0]\n-            shape = (\n-                len(files),\n-                int(ch_metadata[\"VerticalPixels\"]),\n-                int(ch_metadata[\"HorizontalPixels\"]),\n-            )\n-\n-            yx_spacing = self.get_yx_spacing()\n-\n-            bgcm = None\n-            if self._background_correction_matrices is not None:\n-                bgcm = self._background_correction_matrices[str(channel)]\n-\n-            icm = None\n-            if self._illumination_correction_matrices is not None:\n-                icm = self._illumination_correction_matrices[str(channel)]\n-\n-            stacked_tiles.append(\n-                StackedTile(\n-                    paths=files,\n-                    shape=shape,\n-                    position=TilePosition(\n-                        time=time_point,\n-                        channel=int(channel),\n-                        z=z_start,\n-                        y=int(-float(y) / yx_spacing[0]),\n-                        x=int(float(x) / yx_spacing[1]),\n-                    ),\n-                    background_correction_matrix_path=bgcm,\n-                    illumination_correction_matrix_path=icm,\n-                    dtype=self._dtype,\n-                )\n-            )\n-\n-        return stacked_tiles\n-\n-    def get_axes(self) -> list[str]:\n-        if self._z_spacing is not None:\n-            return [\"c\", \"z\", \"y\", \"x\"]\n-        else:\n-            return [\"c\", \"y\", \"x\"]\n-\n-    def get_yx_spacing(self) -> tuple[float, float]:\n-        ch_metadata = self._metadata.iloc[0]\n-        return (\n-            float(ch_metadata[\"VerticalPixelDimension\"]),\n-            float(ch_metadata[\"HorizontalPixelDimension\"]),\n-        )\n-\n-    def get_z_spacing(self) -> Optional[float]:\n-        return self._z_spacing\ndiff --git a/src/faim_ipa/hcs/cellvoyager/StackAcquisition.py b/src/faim_ipa/hcs/cellvoyager/StackAcquisition.py\ndeleted file mode 100644\nindex 76ff2577..00000000\n--- a/src/faim_ipa/hcs/cellvoyager/StackAcquisition.py\n+++ /dev/null\n@@ -1,155 +0,0 @@\n-from os.path import exists, join\n-from pathlib import Path\n-import re\n-from typing import Optional, Union\n-from xml.etree import ElementTree as ET\n-\n-import pandas as pd\n-from tqdm import tqdm\n-\n-from faim_ipa.hcs.acquisition import (\n-    PlateAcquisition,\n-    TileAlignmentOptions,\n-    WellAcquisition,\n-)\n-from faim_ipa.hcs.cellvoyager.CellVoyagerWellAcquisition import (\n-    CellVoyagerWellAcquisition,\n-)\n-from faim_ipa.io.ChannelMetadata import ChannelMetadata\n-\n-BTS_NS = \"{http://www.yokogawa.co.jp/BTS/BTSSchema/1.0}\"\n-\n-\n-class StackAcquisition(PlateAcquisition):\n-    def __init__(\n-        self,\n-        acquisition_dir: Union[Path, str],\n-        alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        n_planes_in_stacked_tile: int = 1,\n-    ):\n-        self._n_planes_in_stacked_tile = n_planes_in_stacked_tile\n-        super().__init__(\n-            acquisition_dir=acquisition_dir,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrices,\n-            illumination_correction_matrices=illumination_correction_matrices,\n-        )\n-\n-    def get_channel_metadata(self) -> dict[int, ChannelMetadata]:\n-        metadata = self._parse_metadata()\n-        ch_metadata = {}\n-\n-        for i, row in metadata.iterrows():\n-            index = int(row[\"Ch\"]) - 1\n-            ch_metadata[index] = ChannelMetadata(\n-                channel_index=index,\n-                channel_name=row[\"Ch\"],\n-                display_color=row[\"Color\"],\n-                spatial_calibration_x=row[\"HorizontalPixelDimension\"],\n-                spatial_calibration_y=row[\"VerticalPixelDimension\"],\n-                spatial_calibration_units=\"um\",\n-                z_spacing=self.get_z_spacing(),\n-                wavelength=self.__parse_filter_wavelength(row[\"Acquisition\"]),\n-                exposure_time=row[\"ExposureTime\"],\n-                exposure_time_unit=\"ms\",\n-                objective=row[\"Objective\"],\n-            )\n-\n-        assert min(ch_metadata.keys()) == 0, \"Channel indices must start at 0.\"\n-\n-        return ch_metadata\n-\n-    def get_z_spacing(self) -> float:\n-        return self._wells[0].get_z_spacing()\n-\n-    def _build_well_acquisitions(self, files: pd.DataFrame) -> list[WellAcquisition]:\n-        wells = []\n-        for well in tqdm(files[\"well\"].unique()):\n-            wells.append(\n-                CellVoyagerWellAcquisition(\n-                    files=files[files[\"well\"] == well],\n-                    alignment=self._alignment,\n-                    metadata=self._parse_metadata(),\n-                    background_correction_matrices=self._background_correction_matrices,\n-                    illumination_correction_matrices=self._illumination_correction_matrices,\n-                    n_planes_in_stacked_tile=self._n_planes_in_stacked_tile,\n-                )\n-            )\n-        return wells\n-\n-    @staticmethod\n-    def __parse_filter_wavelength(value) -> int:\n-        try:\n-            return int(re.match(r\"BP(\\d+)/\", value).group(1))\n-        except AttributeError:  # no cov\n-            return 0\n-\n-    def _parse_metadata(self) -> pd.DataFrame:\n-        mrf_file = join(self._acquisition_dir, \"MeasurementDetail.mrf\")\n-        if not exists(mrf_file):\n-            raise ValueError(\n-                f\"MeasurementDetail.mrf not found in: {self._acquisition_dir}\"\n-            )\n-        mrf_tree = ET.parse(mrf_file)\n-        mrf_root = mrf_tree.getroot()\n-\n-        channels = []\n-        for channel in mrf_root.findall(BTS_NS + \"MeasurementChannel\"):\n-            row = {\n-                key.replace(BTS_NS, \"\"): value for key, value in channel.attrib.items()\n-            }\n-            channels.append(row)\n-\n-        mes_file = join(\n-            self._acquisition_dir,\n-            mrf_root.attrib[BTS_NS + \"MeasurementSettingFileName\"],\n-        )\n-        if not exists(mes_file):\n-            raise ValueError(f\"Settings file not found: {mes_file}\")\n-        mes_tree = ET.parse(mes_file)\n-        mes_root = mes_tree.getroot()\n-\n-        channel_settings = []\n-        for channel in mes_root.find(BTS_NS + \"ChannelList\").findall(\n-            BTS_NS + \"Channel\"\n-        ):\n-            row = {\n-                key.replace(BTS_NS, \"\"): value for key, value in channel.attrib.items()\n-            }\n-            channel_settings.append(row)\n-\n-        return pd.merge(\n-            pd.DataFrame(channels),\n-            pd.DataFrame(channel_settings),\n-            left_on=\"Ch\",\n-            right_on=\"Ch\",\n-        )\n-\n-    def _parse_files(self) -> pd.DataFrame:\n-        mlf_file = join(self._acquisition_dir, \"MeasurementData.mlf\")\n-        if not exists(mlf_file):\n-            raise ValueError(\n-                f\"MeasurementData.mlf not found in: {self._acquisition_dir}\"\n-            )\n-        mlf_tree = ET.parse(mlf_file)\n-        mlf_root = mlf_tree.getroot()\n-\n-        files = []\n-        for record in mlf_root.findall(BTS_NS + \"MeasurementRecord\"):\n-            row = {\n-                key.replace(BTS_NS, \"\"): value for key, value in record.attrib.items()\n-            }\n-            if row.pop(\"Type\") == \"IMG\":\n-                row |= {\n-                    \"path\": join(self._acquisition_dir, record.text),\n-                    \"well\": chr(ord(\"@\") + int(row.pop(\"Row\")))\n-                    + row.pop(\"Column\").zfill(2),\n-                }\n-                files.append(row)\n-\n-        files = pd.DataFrame(files)\n-        files[\"TimePoint\"] = files[\"TimePoint\"].astype(int)\n-        files[\"ZIndex\"] = files[\"ZIndex\"].astype(int)\n-        return files\ndiff --git a/src/faim_ipa/hcs/cellvoyager/ZAdjustedStackAcquisition.py b/src/faim_ipa/hcs/cellvoyager/ZAdjustedStackAcquisition.py\ndeleted file mode 100644\nindex d60759e8..00000000\n--- a/src/faim_ipa/hcs/cellvoyager/ZAdjustedStackAcquisition.py\n+++ /dev/null\n@@ -1,125 +0,0 @@\n-from os.path import join\n-from pathlib import Path\n-from typing import Optional, Union\n-from warnings import warn\n-\n-import numpy as np\n-from pandas.core.api import DataFrame as DataFrame\n-\n-from faim_ipa.hcs.acquisition import TileAlignmentOptions\n-from faim_ipa.hcs.cellvoyager.StackAcquisition import StackAcquisition\n-\n-\n-class ZAdjustedStackAcquisition(StackAcquisition):\n-    _trace_log_files = []\n-\n-    def __init__(\n-        self,\n-        acquisition_dir: Union[Path, str],\n-        trace_log_files: list[Union[Path, str]],\n-        alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        n_planes_in_stacked_tile: int = 1,\n-    ):\n-        self._trace_log_files = trace_log_files\n-        super().__init__(\n-            acquisition_dir,\n-            alignment,\n-            background_correction_matrices,\n-            illumination_correction_matrices,\n-            n_planes_in_stacked_tile=n_planes_in_stacked_tile,\n-        )\n-\n-    def _parse_files(self) -> DataFrame:\n-        files = super()._parse_files()\n-        z_mapping = self._create_z_mapping()\n-        # merge files left with mapping on path\n-        merged = files.merge(z_mapping, how=\"left\", left_on=[\"path\"], right_on=[\"path\"])\n-        if np.any(merged[\"z_pos\"].isna()):\n-            raise ValueError(\"At least one invalid z position.\")\n-        min_z = np.min(merged[\"z_pos\"].astype(float))\n-        z_spacing = np.mean(\n-            merged[merged[\"ZIndex\"].astype(int) == 2][\"Z\"].astype(float)\n-        ) - np.mean(merged[merged[\"ZIndex\"].astype(int) == 1][\"Z\"].astype(float))\n-        # Shift ZIndex for each field in each well according to the auto-focus value\n-        for well in merged[\"well\"].unique():\n-            for field in merged[merged[\"well\"] == well][\"FieldIndex\"].unique():\n-                for ch in merged.query(f\"well == '{well}' & FieldIndex == '{field}'\")[\n-                    \"Ch\"\n-                ].unique():\n-                    z_index_offset = int(\n-                        np.round(\n-                            (\n-                                np.min(\n-                                    merged.query(\n-                                        f\"well == '{well}' & FieldIndex == '{field}' & Ch == '{ch}'\"\n-                                    )[\"z_pos\"]\n-                                )\n-                                - min_z\n-                            )\n-                            / z_spacing\n-                        )\n-                    )\n-                    merged.loc[\n-                        (merged[\"well\"] == well)\n-                        & (merged[\"FieldIndex\"] == field)\n-                        & (merged[\"Ch\"] == ch),\n-                        \"ZIndex\",\n-                    ] += z_index_offset\n-\n-        # Start at 0\n-        merged[\"ZIndex\"] = merged[\"ZIndex\"] - merged[\"ZIndex\"].min()\n-        # update Z\n-        merged[\"Z\"] = merged[\"z_pos\"]\n-        return merged\n-\n-    def _create_z_mapping(self) -> DataFrame:\n-        z_pos = []\n-        filenames = []\n-        missing = []\n-        value = None\n-        for trace_file in self._trace_log_files:\n-            with open(trace_file) as log:\n-                for line in log:\n-                    tokens = line.split(\",\")\n-                    if (\n-                        (len(tokens) > 14)\n-                        and (tokens[7] == \"--->\")\n-                        and (tokens[8] == \"MS_MANU\")\n-                    ):\n-                        value = float(tokens[14])\n-                    elif (\n-                        (len(tokens) > 12)\n-                        and (tokens[7] == \"--->\")\n-                        and (tokens[8] == \"AF_MANU\")\n-                        and (tokens[9] == \"34\")\n-                    ):\n-                        value = float(tokens[12])\n-                    elif (\n-                        (len(tokens) > 8)\n-                        and (tokens[4] == \"Measurement\")\n-                        and (tokens[7] == \"_init_frame_save\")\n-                    ):\n-                        filename = tokens[8]\n-                        if value is None:\n-                            missing.append(join(self._acquisition_dir, filename))\n-                        else:\n-                            filenames.append(join(self._acquisition_dir, filename))\n-                            z_pos.append(value)\n-                    elif (\n-                        (len(tokens) > 7)\n-                        and (tokens[6] == \"EndPeriod\")\n-                        and (tokens[7] == \"acquire frames\")\n-                    ):\n-                        value = None\n-\n-        if len(missing) > 0:\n-            warn(\"Z position information missing for some files.\")\n-            warn(f\"First file without z position information: {missing[0]}\")\n-        return DataFrame(\n-            {\n-                \"path\": filenames,\n-                \"z_pos\": z_pos,\n-            }\n-        )\ndiff --git a/src/faim_ipa/hcs/cellvoyager/__init__.py b/src/faim_ipa/hcs/cellvoyager/__init__.py\nindex 6e4de60d..d74f9d00 100644\n--- a/src/faim_ipa/hcs/cellvoyager/__init__.py\n+++ b/src/faim_ipa/hcs/cellvoyager/__init__.py\n@@ -1,2 +1,13 @@\n-from .StackAcquisition import StackAcquisition  # noqa: F401\n-from .ZAdjustedStackAcquisition import ZAdjustedStackAcquisition  # noqa: F401\n+from faim_ipa.hcs.cellvoyager.acquisition import (\n+    CellVoyagerWellAcquisition,\n+    StackAcquisition,\n+    ZAdjustedStackAcquisition,\n+)\n+from faim_ipa.hcs.cellvoyager.tile import StackedTile\n+\n+__all__ = [\n+    \"CellVoyagerWellAcquisition\",\n+    \"StackAcquisition\",\n+    \"ZAdjustedStackAcquisition\",\n+    \"StackedTile\",\n+]\ndiff --git a/src/faim_ipa/hcs/cellvoyager/acquisition.py b/src/faim_ipa/hcs/cellvoyager/acquisition.py\nnew file mode 100644\nindex 00000000..0b32daa2\n--- /dev/null\n+++ b/src/faim_ipa/hcs/cellvoyager/acquisition.py\n@@ -0,0 +1,417 @@\n+from __future__ import annotations\n+\n+import re\n+from decimal import Decimal\n+from os.path import exists, join\n+from typing import TYPE_CHECKING\n+from warnings import warn\n+\n+import numpy as np\n+import pandas as pd\n+from defusedxml.ElementTree import parse\n+from tifffile import imread\n+from tqdm import tqdm\n+\n+from faim_ipa.hcs.acquisition import (\n+    PlateAcquisition,\n+    TileAlignmentOptions,\n+    WellAcquisition,\n+)\n+from faim_ipa.hcs.cellvoyager.tile import StackedTile\n+from faim_ipa.io.metadata import ChannelMetadata\n+from faim_ipa.stitching.tile import Tile, TilePosition\n+\n+if TYPE_CHECKING:\n+    from pathlib import Path\n+\n+BTS_NS = \"{http://www.yokogawa.co.jp/BTS/BTSSchema/1.0}\"\n+\n+\n+class CellVoyagerWellAcquisition(WellAcquisition):\n+    \"\"\"\n+    Data structure for a CellVoyager well acquisition.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        files: pd.DataFrame,\n+        alignment: TileAlignmentOptions,\n+        metadata: pd.DataFrame,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+        n_planes_in_stacked_tile: int = 1,\n+    ):\n+        self._metadata = metadata\n+        self._z_spacing = self._compute_z_spacing(files)\n+        self._dtype = self._get_dtype(files)\n+        self._n_planes_in_stacked_tile = n_planes_in_stacked_tile\n+        super().__init__(\n+            files=files,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrices,\n+            illumination_correction_matrices=illumination_correction_matrices,\n+        )\n+\n+    def _get_dtype(self, files: pd.DataFrame) -> np.dtype:\n+        return imread(files[\"path\"].iloc[0]).dtype\n+\n+    def _compute_z_spacing(self, files: pd.DataFrame) -> float | None:\n+        if \"ZIndex\" in files.columns:\n+            z_steps = np.array(\n+                files.astype({\"Z\": float}).groupby(\"ZIndex\", sort=True).mean(\"Z\")[\"Z\"]\n+            )\n+\n+            precision = -Decimal(str(z_steps[0])).as_tuple().exponent\n+            return np.round(np.mean(np.diff(z_steps)), decimals=precision)\n+        return None\n+\n+    def _assemble_tiles(self) -> list[Tile]:\n+        min_z_index = 0\n+        max_z_index = min_z_index + 1\n+        if \"ZIndex\" in self._files.columns:\n+            min_z_index = self._files[\"ZIndex\"].min()\n+            max_z_index = self._files[\"ZIndex\"].max() + 1\n+\n+        tiles = {}\n+        for _i, row in self._files.iterrows():\n+            if \"ZIndex\" in row:\n+                tile_z_index = (\n+                    row[\"ZIndex\"] - min_z_index\n+                ) // self._n_planes_in_stacked_tile\n+            else:\n+                tile_z_index = min_z_index\n+            tczyx_index = (\n+                row[\"TimePoint\"],\n+                row[\"Ch\"],\n+                tile_z_index,\n+                row[\"Y\"],\n+                row[\"X\"],\n+            )\n+            if tczyx_index not in tiles:\n+                tiles[tczyx_index] = [row]\n+            else:\n+                tiles[tczyx_index].append(row)\n+\n+        stacked_tiles = []\n+        for tczyx_index, rows in tiles.items():\n+            row_dict = {}\n+            for r in rows:\n+                if \"ZIndex\" in r:\n+                    row_dict[r[\"ZIndex\"]] = r[\"path\"]\n+                else:\n+                    row_dict[min_z_index] = r[\"path\"]\n+\n+            files = []\n+            z_start = tczyx_index[2] * self._n_planes_in_stacked_tile + min_z_index\n+            for z in range(\n+                z_start, min(z_start + self._n_planes_in_stacked_tile, max_z_index)\n+            ):\n+                if z in row_dict:\n+                    files.append(row_dict[z])\n+                else:\n+                    files.append(None)\n+\n+            time_point = tczyx_index[0]\n+            channel = tczyx_index[1]\n+            y, x = tczyx_index[3], tczyx_index[4]\n+\n+            ch_metadata = self._metadata[self._metadata[\"Ch\"] == channel].iloc[0]\n+            shape = (\n+                len(files),\n+                int(ch_metadata[\"VerticalPixels\"]),\n+                int(ch_metadata[\"HorizontalPixels\"]),\n+            )\n+\n+            yx_spacing = self.get_yx_spacing()\n+\n+            bgcm = None\n+            if self._background_correction_matrices is not None:\n+                bgcm = self._background_correction_matrices[str(channel)]\n+\n+            icm = None\n+            if self._illumination_correction_matrices is not None:\n+                icm = self._illumination_correction_matrices[str(channel)]\n+\n+            stacked_tiles.append(\n+                StackedTile(\n+                    paths=files,\n+                    shape=shape,\n+                    position=TilePosition(\n+                        time=time_point,\n+                        channel=int(channel),\n+                        z=z_start,\n+                        y=int(-float(y) / yx_spacing[0]),\n+                        x=int(float(x) / yx_spacing[1]),\n+                    ),\n+                    background_correction_matrix_path=bgcm,\n+                    illumination_correction_matrix_path=icm,\n+                    dtype=self._dtype,\n+                )\n+            )\n+\n+        return stacked_tiles\n+\n+    def get_axes(self) -> list[str]:\n+        if self._z_spacing is not None:\n+            return [\"c\", \"z\", \"y\", \"x\"]\n+        return [\"c\", \"y\", \"x\"]\n+\n+    def get_yx_spacing(self) -> tuple[float, float]:\n+        ch_metadata = self._metadata.iloc[0]\n+        return (\n+            float(ch_metadata[\"VerticalPixelDimension\"]),\n+            float(ch_metadata[\"HorizontalPixelDimension\"]),\n+        )\n+\n+    def get_z_spacing(self) -> float | None:\n+        return self._z_spacing\n+\n+\n+class StackAcquisition(PlateAcquisition):\n+    def __init__(\n+        self,\n+        acquisition_dir: Path | str,\n+        alignment: TileAlignmentOptions,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+        n_planes_in_stacked_tile: int = 1,\n+    ):\n+        self._n_planes_in_stacked_tile = n_planes_in_stacked_tile\n+        super().__init__(\n+            acquisition_dir=acquisition_dir,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrices,\n+            illumination_correction_matrices=illumination_correction_matrices,\n+        )\n+\n+    def get_channel_metadata(self) -> dict[int, ChannelMetadata]:\n+        metadata = self._parse_metadata()\n+        ch_metadata = {}\n+\n+        for _i, row in metadata.iterrows():\n+            index = int(row[\"Ch\"]) - 1\n+            ch_metadata[index] = ChannelMetadata(\n+                channel_index=index,\n+                channel_name=row[\"Ch\"],\n+                display_color=row[\"Color\"],\n+                spatial_calibration_x=row[\"HorizontalPixelDimension\"],\n+                spatial_calibration_y=row[\"VerticalPixelDimension\"],\n+                spatial_calibration_units=\"um\",\n+                z_spacing=self.get_z_spacing(),\n+                wavelength=self.__parse_filter_wavelength(row[\"Acquisition\"]),\n+                exposure_time=row[\"ExposureTime\"],\n+                exposure_time_unit=\"ms\",\n+                objective=row[\"Objective\"],\n+            )\n+\n+        assert min(ch_metadata.keys()) == 0, \"Channel indices must start at 0.\"\n+\n+        return ch_metadata\n+\n+    def get_z_spacing(self) -> float:\n+        return self._wells[0].get_z_spacing()\n+\n+    def _build_well_acquisitions(self, files: pd.DataFrame) -> list[WellAcquisition]:\n+        return [\n+            CellVoyagerWellAcquisition(\n+                files=files[files[\"well\"] == well],\n+                alignment=self._alignment,\n+                metadata=self._parse_metadata(),\n+                background_correction_matrices=self._background_correction_matrices,\n+                illumination_correction_matrices=self._illumination_correction_matrices,\n+                n_planes_in_stacked_tile=self._n_planes_in_stacked_tile,\n+            )\n+            for well in tqdm(files[\"well\"].unique())\n+        ]\n+\n+    @staticmethod\n+    def __parse_filter_wavelength(value) -> int:\n+        try:\n+            return int(re.match(r\"BP(\\d+)/\", value).group(1))\n+        except AttributeError:  # no cov\n+            return 0\n+\n+    def _parse_metadata(self) -> pd.DataFrame:\n+        mrf_file = join(self._acquisition_dir, \"MeasurementDetail.mrf\")\n+        if not exists(mrf_file):\n+            msg = f\"MeasurementDetail.mrf not found in: {self._acquisition_dir}\"\n+            raise ValueError(msg)\n+        mrf_tree = parse(mrf_file)\n+        mrf_root = mrf_tree.getroot()\n+\n+        channels = []\n+        for channel in mrf_root.findall(BTS_NS + \"MeasurementChannel\"):\n+            row = {\n+                key.replace(BTS_NS, \"\"): value for key, value in channel.attrib.items()\n+            }\n+            channels.append(row)\n+\n+        mes_file = join(\n+            self._acquisition_dir,\n+            mrf_root.attrib[BTS_NS + \"MeasurementSettingFileName\"],\n+        )\n+        if not exists(mes_file):\n+            msg = f\"Settings file not found: {mes_file}\"\n+            raise ValueError(msg)\n+        mes_tree = parse(mes_file)\n+        mes_root = mes_tree.getroot()\n+\n+        channel_settings = []\n+        for channel in mes_root.find(BTS_NS + \"ChannelList\").findall(\n+            BTS_NS + \"Channel\"\n+        ):\n+            row = {\n+                key.replace(BTS_NS, \"\"): value for key, value in channel.attrib.items()\n+            }\n+            channel_settings.append(row)\n+\n+        return pd.merge(\n+            pd.DataFrame(channels),\n+            pd.DataFrame(channel_settings),\n+            left_on=\"Ch\",\n+            right_on=\"Ch\",\n+        )\n+\n+    def _parse_files(self) -> pd.DataFrame:\n+        mlf_file = join(self._acquisition_dir, \"MeasurementData.mlf\")\n+        if not exists(mlf_file):\n+            msg = f\"MeasurementData.mlf not found in: {self._acquisition_dir}\"\n+            raise ValueError(msg)\n+        mlf_tree = parse(mlf_file)\n+        mlf_root = mlf_tree.getroot()\n+\n+        files = []\n+        for record in mlf_root.findall(BTS_NS + \"MeasurementRecord\"):\n+            row = {\n+                key.replace(BTS_NS, \"\"): value for key, value in record.attrib.items()\n+            }\n+            if row.pop(\"Type\") == \"IMG\":\n+                row |= {\n+                    \"path\": join(self._acquisition_dir, record.text),\n+                    \"well\": chr(ord(\"@\") + int(row.pop(\"Row\")))\n+                    + row.pop(\"Column\").zfill(2),\n+                }\n+                files.append(row)\n+\n+        files = pd.DataFrame(files)\n+        files[\"TimePoint\"] = files[\"TimePoint\"].astype(int)\n+        files[\"ZIndex\"] = files[\"ZIndex\"].astype(int)\n+        return files\n+\n+\n+class ZAdjustedStackAcquisition(StackAcquisition):\n+    _trace_log_files: list[str | Path]\n+\n+    def __init__(\n+        self,\n+        acquisition_dir: Path | str,\n+        trace_log_files: list[Path | str],\n+        alignment: TileAlignmentOptions,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+        n_planes_in_stacked_tile: int = 1,\n+    ):\n+        self._trace_log_files = trace_log_files\n+        super().__init__(\n+            acquisition_dir,\n+            alignment,\n+            background_correction_matrices,\n+            illumination_correction_matrices,\n+            n_planes_in_stacked_tile=n_planes_in_stacked_tile,\n+        )\n+\n+    def _parse_files(self) -> pd.DataFrame:\n+        files = super()._parse_files()\n+        z_mapping = self._create_z_mapping()\n+        # merge files left with mapping on path\n+        merged = files.merge(z_mapping, how=\"left\", left_on=[\"path\"], right_on=[\"path\"])\n+        if np.any(merged[\"z_pos\"].isna()):\n+            msg = \"At least one invalid z position.\"\n+            raise ValueError(msg)\n+        min_z = np.min(merged[\"z_pos\"].astype(float))\n+        z_spacing = np.mean(\n+            merged[merged[\"ZIndex\"].astype(int) == 2][\"Z\"].astype(float)\n+        ) - np.mean(merged[merged[\"ZIndex\"].astype(int) == 1][\"Z\"].astype(float))\n+        # Shift ZIndex for each field in each well according to the auto-focus value\n+        for well in merged[\"well\"].unique():\n+            for field in merged[merged[\"well\"] == well][\"FieldIndex\"].unique():\n+                for ch in merged.query(f\"well == '{well}' & FieldIndex == '{field}'\")[\n+                    \"Ch\"\n+                ].unique():\n+                    z_index_offset = int(\n+                        np.round(\n+                            (\n+                                np.min(\n+                                    merged.query(\n+                                        f\"well == '{well}' & FieldIndex == '{field}' & Ch == '{ch}'\"\n+                                    )[\"z_pos\"]\n+                                )\n+                                - min_z\n+                            )\n+                            / z_spacing\n+                        )\n+                    )\n+                    merged.loc[\n+                        (merged[\"well\"] == well)\n+                        & (merged[\"FieldIndex\"] == field)\n+                        & (merged[\"Ch\"] == ch),\n+                        \"ZIndex\",\n+                    ] += z_index_offset\n+\n+        # Start at 0\n+        merged[\"ZIndex\"] = merged[\"ZIndex\"] - merged[\"ZIndex\"].min()\n+        # update Z\n+        merged[\"Z\"] = merged[\"z_pos\"]\n+        return merged\n+\n+    def _create_z_mapping(self) -> pd.DataFrame:\n+        z_pos = []\n+        filenames = []\n+        missing = []\n+        value = None\n+        for trace_file in self._trace_log_files:\n+            with open(trace_file) as log:\n+                for line in log:\n+                    tokens = line.split(\",\")\n+                    if (\n+                        (len(tokens) > 14)\n+                        and (tokens[7] == \"--->\")\n+                        and (tokens[8] == \"MS_MANU\")\n+                    ):\n+                        value = float(tokens[14])\n+                    elif (\n+                        (len(tokens) > 12)\n+                        and (tokens[7] == \"--->\")\n+                        and (tokens[8] == \"AF_MANU\")\n+                        and (tokens[9] == \"34\")\n+                    ):\n+                        value = float(tokens[12])\n+                    elif (\n+                        (len(tokens) > 8)\n+                        and (tokens[4] == \"Measurement\")\n+                        and (tokens[7] == \"_init_frame_save\")\n+                    ):\n+                        filename = tokens[8]\n+                        if value is None:\n+                            missing.append(join(self._acquisition_dir, filename))\n+                        else:\n+                            filenames.append(join(self._acquisition_dir, filename))\n+                            z_pos.append(value)\n+                    elif (\n+                        (len(tokens) > 7)\n+                        and (tokens[6] == \"EndPeriod\")\n+                        and (tokens[7] == \"acquire frames\")\n+                    ):\n+                        value = None\n+\n+        if len(missing) > 0:\n+            warn(\"Z position information missing for some files.\", stacklevel=2)\n+            warn(\n+                f\"First file without z position information: {missing[0]}\", stacklevel=2\n+            )\n+        return pd.DataFrame(\n+            {\n+                \"path\": filenames,\n+                \"z_pos\": z_pos,\n+            }\n+        )\ndiff --git a/src/faim_ipa/hcs/cellvoyager/StackedTile.py b/src/faim_ipa/hcs/cellvoyager/tile.py\nsimilarity index 79%\nrename from src/faim_ipa/hcs/cellvoyager/StackedTile.py\nrename to src/faim_ipa/hcs/cellvoyager/tile.py\nindex dbff2c1f..487d31be 100644\n--- a/src/faim_ipa/hcs/cellvoyager/StackedTile.py\n+++ b/src/faim_ipa/hcs/cellvoyager/tile.py\n@@ -1,23 +1,21 @@\n from pathlib import Path\n-from typing import Optional, Union\n \n import numpy as np\n from numpy._typing import NDArray\n from tifffile import imread\n \n-from faim_ipa.stitching import Tile\n-from faim_ipa.stitching.Tile import TilePosition\n+from faim_ipa.stitching.tile import Tile, TilePosition\n \n \n class StackedTile(Tile):\n     def __init__(\n         self,\n-        paths: list[Union[Path, str]],\n+        paths: list[Path | str],\n         shape: tuple[int, int, int],\n         dtype: np.dtype,\n         position: TilePosition,\n-        background_correction_matrix_path: Optional[Union[Path, str]] = None,\n-        illumination_correction_matrix_path: Optional[Union[Path, str]] = None,\n+        background_correction_matrix_path: Path | str | None = None,\n+        illumination_correction_matrix_path: Path | str | None = None,\n     ):\n         super().__init__(\n             path=None,\ndiff --git a/src/faim_ipa/hcs/converter.py b/src/faim_ipa/hcs/converter.py\nindex 32c07665..0c43195e 100644\n--- a/src/faim_ipa/hcs/converter.py\n+++ b/src/faim_ipa/hcs/converter.py\n@@ -1,7 +1,9 @@\n+from __future__ import annotations\n+\n import os\n from os.path import join\n from pathlib import Path\n-from typing import Callable, Optional, Union\n+from typing import TYPE_CHECKING\n \n import dask.array as da\n import zarr\n@@ -19,13 +21,17 @@\n from pydantic import BaseModel\n \n from faim_ipa import dask_utils\n-from faim_ipa.hcs.acquisition import PlateAcquisition\n from faim_ipa.hcs.plate import PlateLayout, get_rows_and_columns\n from faim_ipa.stitching import stitching_utils\n \n+if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n+    from faim_ipa.hcs.acquisition import PlateAcquisition\n+\n \n class NGFFPlate(BaseModel):\n-    root_dir: Union[Path, str]\n+    root_dir: Path | str\n     name: str\n     layout: PlateLayout\n     order_name: str\n@@ -71,7 +77,7 @@ def __init__(\n         self._client = client\n \n     def create_zarr_plate(\n-        self, plate_acquisition: PlateAcquisition, wells: Optional[list[str]] = None\n+        self, plate_acquisition: PlateAcquisition, wells: list[str] | None = None\n     ) -> zarr.Group:\n         \"\"\"\n         Create empty NGFF zarr plate.\n@@ -109,19 +115,19 @@ def create_zarr_plate(\n             attrs[\"barcode\"] = self._ngff_plate.barcode\n             plate.attrs.put(attrs)\n             return plate\n-        else:\n-            store = parse_url(plate_path, mode=\"w\").store\n-            return zarr.group(store=store)\n+        store = parse_url(plate_path, mode=\"w\").store\n+        return zarr.group(store=store)\n \n     def run(\n         self,\n         plate: zarr.Group,\n         plate_acquisition: PlateAcquisition,\n-        wells: list[str] = None,\n+        wells: list[str] | None = None,\n         well_sub_group: str = \"0\",\n-        chunks: Union[tuple[int, int], tuple[int, int, int]] = (2048, 2048),\n+        chunks: tuple[int, int] | tuple[int, int, int] = (2048, 2048),\n         max_layer: int = 3,\n-        storage_options: dict = None,\n+        storage_options: dict | None = None,\n+        *,\n         build_acquisition_mask: bool = False,\n     ):\n         \"\"\"\n@@ -193,7 +199,9 @@ def _write_metadata(\n         fmt.validate_coordinate_transformations(\n             dims, len(datasets), coordinate_transformations\n         )\n-        for dataset, transform in zip(datasets, coordinate_transformations):\n+        for dataset, transform in zip(\n+            datasets, coordinate_transformations, strict=True\n+        ):\n             dataset[\"coordinateTransformations\"] = transform\n         axes = Axes(well_acquisition.get_axes(), fmt).to_list()\n         spatial_calibration_unit = Unit(\n@@ -213,7 +221,7 @@ def _write_metadata(\n         }\n         group.attrs[\"acquisition_metadata\"] = {\n             \"channels\": [\n-                ch_metadata.dict()\n+                ch_metadata.model_dump()\n                 for ch_metadata in plate_acquisition.get_channel_metadata().values()\n             ]\n         }\n@@ -322,14 +330,14 @@ def _bin_yx(self, image_da):\n                 },\n                 trim_excess=True,\n             )\n-        else:\n-            return image_da\n+        return image_da\n \n     def _stitch_well_image(\n         self,\n         chunks,\n         well_acquisition,\n         output_shape: tuple[int, int, int, int, int],\n+        *,\n         build_acquisition_mask: bool,\n     ):\n         from faim_ipa.stitching import DaskTileStitcher\n@@ -346,8 +354,9 @@ def _stitch_well_image(\n                 chunks[-2],\n                 chunks[-1],\n             )\n-        else:\n-            raise NotImplementedError(\"Tile data must be 2D or 3D.\")  # pragma: no cover\n+        else:  # pragma: no cover\n+            msg = \"Tile data must be 2D or 3D.\"\n+            raise NotImplementedError(msg)\n \n         stitcher = DaskTileStitcher(\n             tiles=well_acquisition.get_tiles(),\n@@ -355,29 +364,28 @@ def _stitch_well_image(\n             output_shape=output_shape,\n             dtype=bool if build_acquisition_mask else well_acquisition.get_dtype(),\n         )\n-        image_da = stitcher.get_stitched_dask_array(\n+        return stitcher.get_stitched_dask_array(\n             warp_func=self._warp_func,\n             fuse_func=(\n                 stitching_utils.fuse_sum if build_acquisition_mask else self._fuse_func\n             ),\n             build_acquisition_mask=build_acquisition_mask,\n         )\n-        return image_da\n \n     def _create_well_group(\n-        self, plate, well_acquisition, well_sub_group, add_to_well_images=True\n+        self, plate, well_acquisition, well_sub_group, *, add_to_well_images=True\n     ):\n         row, col = well_acquisition.get_row_col()\n         well_group = plate.require_group(row).require_group(col)\n         well_group.require_group(well_sub_group)\n         if add_to_well_images:\n             zattrs = well_group.attrs.asdict()\n-            if \"well\" in zattrs.keys() and \"images\" in zattrs[\"well\"].keys():\n+            if \"well\" in zattrs and \"images\" in zattrs[\"well\"]:\n                 existing_images = well_group.attrs.asdict()[\"well\"][\"images\"]\n             else:\n                 existing_images = []\n             write_well_metadata(\n-                well_group, existing_images + [{\"path\": well_sub_group}]\n+                well_group, [*existing_images, {\"path\": well_sub_group}]\n             )\n         return well_group\n \n@@ -388,18 +396,16 @@ def _get_storage_options(\n         chunks: tuple[int, ...],\n     ):\n         if storage_options is None:\n-            return dict(\n-                dimension_separator=\"/\",\n-                compressor=Blosc(cname=\"zstd\", clevel=3, shuffle=Blosc.SHUFFLE),\n-                chunks=ConvertToNGFFPlate._out_chunks(output_shape, chunks),\n-                write_empty_chunks=False,\n-            )\n-        else:\n-            return storage_options\n+            return {\n+                \"dimension_separator\": \"/\",\n+                \"compressor\": Blosc(cname=\"zstd\", clevel=3, shuffle=Blosc.SHUFFLE),\n+                \"chunks\": ConvertToNGFFPlate._out_chunks(output_shape, chunks),\n+                \"write_empty_chunks\": False,\n+            }\n+        return storage_options\n \n     @staticmethod\n     def _out_chunks(shape, chunks):\n         if len(shape) == len(chunks):\n             return chunks\n-        else:\n-            return (1,) * (len(shape) - len(chunks)) + chunks\n+        return (1,) * (len(shape) - len(chunks)) + chunks\ndiff --git a/src/faim_ipa/hcs/imagexpress/ImageXpressPlateAcquisition.py b/src/faim_ipa/hcs/imagexpress/ImageXpressPlateAcquisition.py\ndeleted file mode 100644\nindex 64aab1e6..00000000\n--- a/src/faim_ipa/hcs/imagexpress/ImageXpressPlateAcquisition.py\n+++ /dev/null\n@@ -1,134 +0,0 @@\n-import os\n-import re\n-from abc import abstractmethod\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-import pandas as pd\n-from tqdm import tqdm\n-\n-from faim_ipa.hcs.acquisition import (\n-    PlateAcquisition,\n-    TileAlignmentOptions,\n-    WellAcquisition,\n-)\n-from faim_ipa.hcs.imagexpress.ImageXpressWellAcquisition import (\n-    ImageXpressWellAcquisition,\n-)\n-from faim_ipa.io.ChannelMetadata import ChannelMetadata\n-from faim_ipa.io.MetaSeriesTiff import load_metaseries_tiff_metadata\n-from faim_ipa.utils import rgb_to_hex, wavelength_to_rgb\n-\n-\n-class ImageXpressPlateAcquisition(PlateAcquisition):\n-    def __init__(\n-        self,\n-        acquisition_dir: Union[Path, str],\n-        alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-    ):\n-        super().__init__(\n-            acquisition_dir=acquisition_dir,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrices,\n-            illumination_correction_matrices=illumination_correction_matrices,\n-        )\n-\n-    def _parse_files(self) -> pd.DataFrame:\n-        \"\"\"Parse all files in the acquisition directory.\n-\n-        Returns\n-        -------\n-        DataFrame\n-            Table of all files in the acquisition.\n-        \"\"\"\n-        return pd.DataFrame(\n-            ImageXpressPlateAcquisition._list_and_match_files(\n-                root_dir=self._acquisition_dir,\n-                root_re=self._get_root_re(),\n-                filename_re=self._get_filename_re(),\n-            )\n-        )\n-\n-    @staticmethod\n-    def _list_and_match_files(\n-        root_dir: Union[Path, str],\n-        root_re: re.Pattern,\n-        filename_re: re.Pattern,\n-    ) -> list[list[dict[dict, str]]]:\n-        files = []\n-        for root, _, filenames in os.walk(root_dir):\n-            m_root = root_re.fullmatch(root)\n-            if m_root:\n-                for f in filenames:\n-                    m_filename = filename_re.fullmatch(f)\n-                    if m_filename:\n-                        row = m_root.groupdict()\n-                        row |= m_filename.groupdict()\n-                        if \"channel\" not in row or row[\"channel\"] is None:\n-                            row[\"channel\"] = \"w1\"\n-                        row[\"path\"] = str(Path(root).joinpath(f))\n-                        files.append(row)\n-        return files\n-\n-    @abstractmethod\n-    def _get_root_re(self) -> re.Pattern:\n-        \"\"\"Regular expression for matching the root directory of the acquisition.\"\"\"\n-        raise NotImplementedError\n-\n-    @abstractmethod\n-    def _get_filename_re(self) -> re.Pattern:\n-        \"\"\"Regular expression for matching the filename of the acquisition.\"\"\"\n-        raise NotImplementedError\n-\n-    def _build_well_acquisitions(self, files: pd.DataFrame) -> list[WellAcquisition]:\n-        wells = []\n-        for well in tqdm(files[\"well\"].unique()):\n-            wells.append(\n-                ImageXpressWellAcquisition(\n-                    files=files[files[\"well\"] == well],\n-                    alignment=self._alignment,\n-                    z_spacing=self._get_z_spacing(),\n-                    background_correction_matrices=self._background_correction_matrices,\n-                    illumination_correction_matrices=self._illumination_correction_matrices,\n-                )\n-            )\n-\n-        return wells\n-\n-    @abstractmethod\n-    def _get_z_spacing(self) -> Optional[float]:\n-        raise NotImplementedError\n-\n-    def get_channel_metadata(self) -> dict[int, ChannelMetadata]:\n-        ch_metadata = {}\n-        _files = self._wells[0]._files\n-        for ch in _files[\"channel\"].unique():\n-            channel_files = _files[_files[\"channel\"] == ch]\n-            path = channel_files[\"path\"].iloc[0]\n-            metadata = load_metaseries_tiff_metadata(path=path)\n-            index = int(ch[1:]) - 1\n-            if \"Z Projection Method\" in metadata.keys():\n-                name = (\n-                    f\"{metadata['Z Projection Method'].replace(' ', '-')}-Projection_\"\n-                    f\"{metadata['_IllumSetting_']}\"\n-                )\n-            else:\n-                name = metadata[\"_IllumSetting_\"]\n-            ch_metadata[index] = ChannelMetadata(\n-                channel_index=index,\n-                channel_name=name,\n-                display_color=rgb_to_hex(*wavelength_to_rgb(metadata[\"wavelength\"])),\n-                spatial_calibration_x=metadata[\"spatial-calibration-x\"],\n-                spatial_calibration_y=metadata[\"spatial-calibration-y\"],\n-                spatial_calibration_units=metadata[\"spatial-calibration-units\"],\n-                z_spacing=self._get_z_spacing(),\n-                wavelength=metadata[\"wavelength\"],\n-                exposure_time=float(metadata[\"Exposure Time\"].split(\" \")[0]),\n-                exposure_time_unit=metadata[\"Exposure Time\"].split(\" \")[1],\n-                objective=metadata[\"_MagSetting_\"],\n-            )\n-\n-        assert min(ch_metadata.keys()) == 0, \"Channel indices must start at 0.\"\n-        return ch_metadata\ndiff --git a/src/faim_ipa/hcs/imagexpress/ImageXpressWellAcquisition.py b/src/faim_ipa/hcs/imagexpress/ImageXpressWellAcquisition.py\ndeleted file mode 100644\nindex e9662530..00000000\n--- a/src/faim_ipa/hcs/imagexpress/ImageXpressWellAcquisition.py\n+++ /dev/null\n@@ -1,91 +0,0 @@\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-import pandas as pd\n-\n-from faim_ipa.hcs.acquisition import TileAlignmentOptions, WellAcquisition\n-from faim_ipa.io.MetaSeriesTiff import load_metaseries_tiff_metadata\n-from faim_ipa.stitching import Tile\n-from faim_ipa.stitching.Tile import TilePosition\n-\n-\n-class ImageXpressWellAcquisition(WellAcquisition):\n-    def __init__(\n-        self,\n-        files: pd.DataFrame,\n-        alignment: TileAlignmentOptions,\n-        z_spacing: Optional[float],\n-        background_correction_matrices: dict[str, Union[Path, str]] = None,\n-        illumination_correction_matrices: dict[str, Union[Path, str]] = None,\n-    ) -> None:\n-        self._z_spacing = z_spacing\n-        super().__init__(\n-            files=files,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrices,\n-            illumination_correction_matrices=illumination_correction_matrices,\n-        )\n-\n-    def _assemble_tiles(self) -> list[Tile]:\n-        tiles = []\n-        for i, row in self._files.iterrows():\n-            file = row[\"path\"]\n-            time_point = row[\"t\"] if \"t\" in row.index and row[\"t\"] is not None else 0\n-            channel = row[\"channel\"]\n-            metadata = load_metaseries_tiff_metadata(file)\n-            if self._z_spacing is None:\n-                z = 1\n-            else:\n-                z = row[\"z\"] if row[\"z\"] is not None else 1\n-\n-            bgcm = None\n-            if self._background_correction_matrices is not None:\n-                bgcm = self._background_correction_matrices[channel]\n-\n-            icm = None\n-            if self._illumination_correction_matrices is not None:\n-                icm = self._illumination_correction_matrices[channel]\n-\n-            tiles.append(\n-                Tile(\n-                    path=file,\n-                    shape=(metadata[\"pixel-size-y\"], metadata[\"pixel-size-x\"]),\n-                    position=TilePosition(\n-                        time=time_point,\n-                        channel=int(channel[1:]),\n-                        z=z,\n-                        y=int(\n-                            metadata[\"stage-position-y\"]\n-                            / metadata[\"spatial-calibration-y\"]\n-                        ),\n-                        x=int(\n-                            metadata[\"stage-position-x\"]\n-                            / metadata[\"spatial-calibration-x\"]\n-                        ),\n-                    ),\n-                    background_correction_matrix_path=bgcm,\n-                    illumination_correction_matrix_path=icm,\n-                )\n-            )\n-        return tiles\n-\n-    def get_yx_spacing(self) -> tuple[float, float]:\n-        metadata = load_metaseries_tiff_metadata(self._files.iloc[0][\"path\"])\n-        return (metadata[\"spatial-calibration-y\"], metadata[\"spatial-calibration-x\"])\n-\n-    def get_z_spacing(self) -> Optional[float]:\n-        return self._z_spacing\n-\n-    def get_axes(self) -> list[str]:\n-        axes = [\"y\", \"x\"]\n-\n-        if \"z\" in self._files.columns:\n-            axes = [\"z\"] + axes\n-\n-        if self._files[\"channel\"].nunique() > 1:\n-            axes = [\"c\"] + axes\n-\n-        if \"t\" in self._files.columns and self._files[\"t\"].nunique() > 1:\n-            axes = [\"t\"] + axes\n-\n-        return axes\ndiff --git a/src/faim_ipa/hcs/imagexpress/MixedAcquisition.py b/src/faim_ipa/hcs/imagexpress/MixedAcquisition.py\ndeleted file mode 100644\nindex 251e1dc5..00000000\n--- a/src/faim_ipa/hcs/imagexpress/MixedAcquisition.py\n+++ /dev/null\n@@ -1,85 +0,0 @@\n-import re\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-import pandas as pd\n-from numpy._typing import NDArray\n-\n-from faim_ipa.hcs.acquisition import TileAlignmentOptions\n-from faim_ipa.hcs.imagexpress import StackAcquisition\n-\n-\n-class MixedAcquisition(StackAcquisition):\n-    \"\"\"Image stack acquisition with Projectsion acquired with a Molecular\n-    Devices ImageXpress Micro Confocal system.\n-\n-    MIP-2P-2sub-Stack --> {name} [Optional]\n-    \u2514\u2500\u2500 2023-02-21 --> {date}\n-        \u2514\u2500\u2500 1334 --> {acquisition id}\n-            \u251c\u2500\u2500 Projection-Mix_E07_s1_w1E94C24BD-45E4-450A-9919-257C714278F7.tif\n-            \u251c\u2500\u2500 Projection-Mix_E07_s1_w1_thumb4BFD4018-E675-475E-B5AB-2E959E6B6DA1.tif\n-            \u251c\u2500\u2500 ...\n-            \u251c\u2500\u2500 Projection-Mix_E08_s2_w3CCE83D85-0912-429E-9F18-716A085BB5BC.tif\n-            \u251c\u2500\u2500 Projection-Mix_E08_s2_w3_thumb4D88636E-181E-4AF6-BC53-E7A435959C8F.tif\n-            \u251c\u2500\u2500 ZStep_1\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w1E78EB128-BD0D-4D94-A6AD-3FF28BB1B105.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w1_thumb187DE64B-038A-4671-BF6B-683721723769.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w2C0A49256-E289-4C0F-ADC9-F7728ABDB141.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w2_thumb57D4B151-71BF-480E-8CC4-C23A2690B763.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w427CCB2E4-1BF4-45E7-8BC7-264B48EF9C4A.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w4_thumb555647D0-77F1-4A43-9472-AE509F95E236.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n-            \u2502\u00a0\u00a0 \u2514\u2500\u2500 Projection-Mix_E08_s2_w4_thumbD2785594-4F49-464F-9F80-1B82E30A560A.tif\n-            \u251c\u2500\u2500 ...\n-            \u2514\u2500\u2500 ZStep_9\n-                \u251c\u2500\u2500 Projection-Mix_E07_s1_w1091EB8A5-272A-466D-B8A0-7547C6BA392B.tif\n-                \u251c\u2500\u2500 ...\n-                \u2514\u2500\u2500 Projection-Mix_E08_s2_w2_thumb210C0D5D-C20E-484D-AFB2-EFE669A56B84.tif\n-\n-    Image data is stored in {name}_{well}_{field}_w{channel}{md_id}.tif.\n-    The *_thumb*.tif files, used by Molecular Devices as preview, are ignored.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        acquisition_dir: Union[Path, str],\n-        alignment: TileAlignmentOptions,\n-        background_correction_matrix: Optional[dict[str, NDArray]] = None,\n-        illumination_correction_matrix: Optional[NDArray] = None,\n-    ):\n-        super().__init__(\n-            acquisition_dir=acquisition_dir,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrix,\n-            illumination_correction_matrices=illumination_correction_matrix,\n-        )\n-\n-    def _parse_files(self) -> pd.DataFrame:\n-        files = self._filter_mips(super()._parse_files())\n-        self._z_spacing = self._compute_z_spacing(files)\n-        return files\n-\n-    def _filter_mips(self, files: pd.DataFrame) -> pd.DataFrame:\n-        \"\"\"Remove MIP files if the whole stack was acquired.\"\"\"\n-        _files = files.copy()\n-        for ch in _files[\"channel\"].unique():\n-            channel_files = _files[_files[\"channel\"] == ch]\n-            z_positions = channel_files[\"z\"].unique()\n-            has_mip = None in z_positions\n-            has_stack = len(z_positions) > 1\n-            if has_mip and has_stack:\n-                _files.drop(\n-                    _files[(_files[\"channel\"] == ch) & (_files[\"z\"].isna())].index,\n-                    inplace=True,\n-                )\n-        return _files\n-\n-    def _get_root_re(self) -> re.Pattern:\n-        return re.compile(\n-            r\".*[\\/\\\\](?P<date>\\d{4}-\\d{2}-\\d{2})[\\/\\\\](?P<acq_id>\\d+)(?:[\\/\\\\]TimePoint_(?P<t>\\d+))?(?:[\\/\\\\]ZStep_(?P<z>\\d+))?.*\"\n-        )\n-\n-    def _get_filename_re(self) -> re.Pattern:\n-        return re.compile(\n-            r\"(?P<name>.*)_(?P<well>[A-Z]+\\d{2})_?(?P<field>s\\d+)?_?(?P<channel>w[1-9]{1})?(?!_thumb)(?P<md_id>[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})(?P<ext>.tif)\"\n-        )\ndiff --git a/src/faim_ipa/hcs/imagexpress/SinglePlaneAcquisition.py b/src/faim_ipa/hcs/imagexpress/SinglePlaneAcquisition.py\ndeleted file mode 100644\nindex 16204199..00000000\n--- a/src/faim_ipa/hcs/imagexpress/SinglePlaneAcquisition.py\n+++ /dev/null\n@@ -1,63 +0,0 @@\n-import re\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-from faim_ipa.hcs.acquisition import TileAlignmentOptions\n-from faim_ipa.hcs.imagexpress import ImageXpressPlateAcquisition\n-\n-\n-class SinglePlaneAcquisition(ImageXpressPlateAcquisition):\n-    \"\"\"Parse top folder (single planes) of an acquisition of a MolecularDevices ImageXpress Micro Confocal system.\n-\n-    Storage layout on disk for 2 wells with 2 fields and 2 channels::\n-\n-        MIP-2P-2sub --> {name} [Optional]\n-        \u2514\u2500\u2500 2022-07-05 --> {date}\n-            \u2514\u2500\u2500 1075 --> {acquisition id}\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w146C9B2CD-0BB3-4B8A-9187-2805F4C90506.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w1_thumb6EFE77C6-B96D-412A-9FD1-710DBDA32821.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w2B90625C8-6EA7-4E54-8289-C539EB75263E.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w2_thumbEDDF803A-AE5E-4190-8C06-F54341AEC4A6.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w1E2913F7F-E229-4B6A-BFED-02BCF54561FA.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w1_thumb72E3641A-C91B-4501-900A-245BAC58FF46.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w241C38630-BCFD-4393-8706-58755CECE059.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w2_thumb5377A5AC-9BBF-4BAF-99A2-24896E3373A2.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w152C23B9A-EB4C-4DF6-8A7F-F4147A9E7DDE.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w1_thumb541AA634-387C-4B84-B0D8-EE4CB1C88E81.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w2FB0D7D9B-3EA0-445E-9A05-7D01154A9A5C.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w2_thumb8FA1E466-57CD-4237-B09B-CAB48154647D.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s2_w1F365E60C-BCC2-4B74-9856-BCE07C8B0FD3.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s2_w1_thumb9652366E-36A0-4B7F-8B18-DA89D7DB41BD.tif\n-                \u251c\u2500\u2500 MIP-2P-2sub_C06_s2_w20EEC6AEA-1727-41E6-806C-40FF6AF68B6C.tif\n-                \u2514\u2500\u2500 MIP-2P-2sub_C06_s2_w2_thumb710CD846-0185-4362-BBAF-C700AE0013B3.tif\n-\n-    Image data is stored in {name}_{well}_{field}_w{channel}{md_id}.tif.\n-    The *_thumb*.tif files, used by Molecular Devices as preview, are ignored.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        acquisition_dir: Union[Path, str],\n-        alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-    ):\n-        super().__init__(\n-            acquisition_dir=acquisition_dir,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrices,\n-            illumination_correction_matrices=illumination_correction_matrices,\n-        )\n-\n-    def _get_root_re(self) -> re.Pattern:\n-        return re.compile(\n-            r\".*[\\/\\\\](?P<date>\\d{4}-\\d{2}-\\d{2})[\\/\\\\](?P<acq_id>\\d+)(?:[\\/\\\\]TimePoint_(?P<t>\\d+))?\"\n-        )\n-\n-    def _get_filename_re(self) -> re.Pattern:\n-        return re.compile(\n-            r\"(?P<name>.*)_(?P<well>[A-Z]+\\d{2})_?(?P<field>s\\d+)?_?(?P<channel>w[1-9]{1})?(?!_thumb)(?P<md_id>[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})(?P<ext>.tif)\"\n-        )\n-\n-    def _get_z_spacing(self) -> Optional[float]:\n-        return None\ndiff --git a/src/faim_ipa/hcs/imagexpress/StackAcquisition.py b/src/faim_ipa/hcs/imagexpress/StackAcquisition.py\ndeleted file mode 100644\nindex e57a30f1..00000000\n--- a/src/faim_ipa/hcs/imagexpress/StackAcquisition.py\n+++ /dev/null\n@@ -1,96 +0,0 @@\n-import re\n-from decimal import Decimal\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-import numpy as np\n-import pandas as pd\n-\n-from faim_ipa.hcs.acquisition import TileAlignmentOptions\n-from faim_ipa.hcs.imagexpress import ImageXpressPlateAcquisition\n-from faim_ipa.io.MetaSeriesTiff import load_metaseries_tiff_metadata\n-\n-\n-class StackAcquisition(ImageXpressPlateAcquisition):\n-    \"\"\"Image stack acquisition with a Molecular Devices ImageXpress Micro\n-    Confocal system.\n-\n-    MIP-2P-2sub-Stack --> {name} [Optional]\n-    \u2514\u2500\u2500 2023-02-21 --> {date}\n-        \u2514\u2500\u2500 1334 --> {acquisition id}\n-            \u251c\u2500\u2500 ZStep_1\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w1E78EB128-BD0D-4D94-A6AD-3FF28BB1B105.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w1_thumb187DE64B-038A-4671-BF6B-683721723769.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w2C0A49256-E289-4C0F-ADC9-F7728ABDB141.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w2_thumb57D4B151-71BF-480E-8CC4-C23A2690B763.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w427CCB2E4-1BF4-45E7-8BC7-264B48EF9C4A.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 Projection-Mix_E07_s1_w4_thumb555647D0-77F1-4A43-9472-AE509F95E236.tif\n-            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n-            \u2502\u00a0\u00a0 \u2514\u2500\u2500 Projection-Mix_E08_s2_w4_thumbD2785594-4F49-464F-9F80-1B82E30A560A.tif\n-            \u251c\u2500\u2500 ...\n-            \u2514\u2500\u2500 ZStep_9\n-                \u251c\u2500\u2500 Projection-Mix_E07_s1_w1091EB8A5-272A-466D-B8A0-7547C6BA392B.tif\n-                \u251c\u2500\u2500 ...\n-                \u2514\u2500\u2500 Projection-Mix_E08_s2_w2_thumb210C0D5D-C20E-484D-AFB2-EFE669A56B84.tif\n-\n-    Image data is stored in {name}_{well}_{field}_w{channel}{md_id}.tif.\n-    The *_thumb*.tif files, used by Molecular Devices as preview, are ignored.\n-    \"\"\"\n-\n-    _z_spacing: float = None\n-\n-    def __init__(\n-        self,\n-        acquisition_dir: Union[Path, str],\n-        alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-        illumination_correction_matrices: Optional[dict[str, Union[Path, str]]] = None,\n-    ):\n-        super().__init__(\n-            acquisition_dir=acquisition_dir,\n-            alignment=alignment,\n-            background_correction_matrices=background_correction_matrices,\n-            illumination_correction_matrices=illumination_correction_matrices,\n-        )\n-\n-    def _parse_files(self) -> pd.DataFrame:\n-        files = super()._parse_files()\n-        self._z_spacing = self._compute_z_spacing(files)\n-        return files\n-\n-    def _get_root_re(self) -> re.Pattern:\n-        return re.compile(\n-            r\".*[\\/\\\\](?P<date>\\d{4}-\\d{2}-\\d{2})[\\/\\\\](?P<acq_id>\\d+)(?:[\\/\\\\]TimePoint_(?P<t>\\d+))?(?:[\\/\\\\]ZStep_(?P<z>\\d+))\"\n-        )\n-\n-    def _get_filename_re(self) -> re.Pattern:\n-        return re.compile(\n-            r\"(?P<name>.*)_(?P<well>[A-Z]+\\d{2})_?(?P<field>s\\d+)?_?(?P<channel>w[1-9]{1})?(?!_thumb)(?P<md_id>[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})(?P<ext>.tif)\"\n-        )\n-\n-    def _get_z_spacing(self) -> Optional[float]:\n-        return self._z_spacing\n-\n-    def _compute_z_spacing(self, files: pd.DataFrame) -> Optional[float]:\n-        assert \"z\" in files.columns, \"No z column in files DataFrame.\"\n-        channel_with_stack = np.sort(files[files[\"z\"] == \"2\"][\"channel\"].unique())[0]\n-        subset = files[files[\"channel\"] == channel_with_stack]\n-        subset = subset[subset[\"well\"] == np.sort(subset[\"well\"].unique())[0]]\n-        first_field = np.sort(subset[\"field\"].unique())[0]\n-        if first_field is not None:\n-            subset = subset[subset[\"field\"] == np.sort(subset[\"field\"].unique())[0]]\n-\n-        plane_positions = []\n-\n-        for i, row in subset.iterrows():\n-            file = row[\"path\"]\n-            if \"z\" in row.keys() and row[\"z\"] is not None:\n-                metadata = load_metaseries_tiff_metadata(file)\n-                z_position = metadata[\"stage-position-z\"]\n-                plane_positions.append(z_position)\n-\n-        plane_positions = np.array(sorted(plane_positions), dtype=np.float32)\n-\n-        precision = -Decimal(str(plane_positions[0])).as_tuple().exponent\n-        z_step = np.round(np.mean(np.diff(plane_positions)), decimals=precision)\n-        return z_step\ndiff --git a/src/faim_ipa/hcs/imagexpress/__init__.py b/src/faim_ipa/hcs/imagexpress/__init__.py\nindex e5f00530..b08a768d 100644\n--- a/src/faim_ipa/hcs/imagexpress/__init__.py\n+++ b/src/faim_ipa/hcs/imagexpress/__init__.py\n@@ -1,5 +1,15 @@\n-from .ImageXpressPlateAcquisition import ImageXpressPlateAcquisition  # noqa: F401\n-from .ImageXpressWellAcquisition import ImageXpressWellAcquisition  # noqa: F401\n-from .SinglePlaneAcquisition import SinglePlaneAcquisition  # noqa: F401\n-from .StackAcquisition import StackAcquisition  # noqa: F401\n-from .MixedAcquisition import MixedAcquisition  # noqa: F401\n+from faim_ipa.hcs.imagexpress.acquisition import (\n+    ImageXpressPlateAcquisition,\n+    ImageXpressWellAcquisition,\n+    MixedAcquisition,\n+    SinglePlaneAcquisition,\n+    StackAcquisition,\n+)\n+\n+__all__ = [\n+    \"ImageXpressPlateAcquisition\",\n+    \"ImageXpressWellAcquisition\",\n+    \"MixedAcquisition\",\n+    \"SinglePlaneAcquisition\",\n+    \"StackAcquisition\",\n+]\ndiff --git a/src/faim_ipa/hcs/imagexpress/acquisition.py b/src/faim_ipa/hcs/imagexpress/acquisition.py\nnew file mode 100644\nindex 00000000..80c818e7\n--- /dev/null\n+++ b/src/faim_ipa/hcs/imagexpress/acquisition.py\n@@ -0,0 +1,431 @@\n+import os\n+import re\n+from abc import abstractmethod\n+from decimal import Decimal\n+from pathlib import Path\n+\n+import numpy as np\n+import pandas as pd\n+from numpy._typing import NDArray\n+from tqdm import tqdm\n+\n+from faim_ipa.hcs.acquisition import (\n+    PlateAcquisition,\n+    TileAlignmentOptions,\n+    WellAcquisition,\n+)\n+from faim_ipa.io.metadata import ChannelMetadata\n+from faim_ipa.io.metaseries import load_metaseries_tiff_metadata\n+from faim_ipa.stitching.tile import Tile, TilePosition\n+from faim_ipa.utils import rgb_to_hex, wavelength_to_rgb\n+\n+\n+class ImageXpressWellAcquisition(WellAcquisition):\n+    def __init__(\n+        self,\n+        files: pd.DataFrame,\n+        alignment: TileAlignmentOptions,\n+        z_spacing: float | None,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+    ) -> None:\n+        self._z_spacing = z_spacing\n+        super().__init__(\n+            files=files,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrices,\n+            illumination_correction_matrices=illumination_correction_matrices,\n+        )\n+\n+    def _assemble_tiles(self) -> list[Tile]:\n+        tiles = []\n+        for _i, row in self._files.iterrows():\n+            file = row[\"path\"]\n+            time_point = row[\"t\"] if \"t\" in row.index and row[\"t\"] is not None else 0\n+            channel = row[\"channel\"]\n+            metadata = load_metaseries_tiff_metadata(file)\n+            z = (\n+                1\n+                if self._z_spacing is None\n+                else row[\"z\"] if row[\"z\"] is not None else 1\n+            )\n+\n+            bgcm = None\n+            if self._background_correction_matrices is not None:\n+                bgcm = self._background_correction_matrices[channel]\n+\n+            icm = None\n+            if self._illumination_correction_matrices is not None:\n+                icm = self._illumination_correction_matrices[channel]\n+\n+            tiles.append(\n+                Tile(\n+                    path=file,\n+                    shape=(metadata[\"pixel-size-y\"], metadata[\"pixel-size-x\"]),\n+                    position=TilePosition(\n+                        time=time_point,\n+                        channel=int(channel[1:]),\n+                        z=z,\n+                        y=int(\n+                            metadata[\"stage-position-y\"]\n+                            / metadata[\"spatial-calibration-y\"]\n+                        ),\n+                        x=int(\n+                            metadata[\"stage-position-x\"]\n+                            / metadata[\"spatial-calibration-x\"]\n+                        ),\n+                    ),\n+                    background_correction_matrix_path=bgcm,\n+                    illumination_correction_matrix_path=icm,\n+                )\n+            )\n+        return tiles\n+\n+    def get_yx_spacing(self) -> tuple[float, float]:\n+        metadata = load_metaseries_tiff_metadata(self._files.iloc[0][\"path\"])\n+        return (metadata[\"spatial-calibration-y\"], metadata[\"spatial-calibration-x\"])\n+\n+    def get_z_spacing(self) -> float | None:\n+        return self._z_spacing\n+\n+    def get_axes(self) -> list[str]:\n+        axes = [\"y\", \"x\"]\n+\n+        if \"z\" in self._files.columns:\n+            axes = [\"z\", *axes]\n+\n+        if self._files[\"channel\"].nunique() > 1:\n+            axes = [\"c\", *axes]\n+\n+        if \"t\" in self._files.columns and self._files[\"t\"].nunique() > 1:\n+            axes = [\"t\", *axes]\n+\n+        return axes\n+\n+\n+class ImageXpressPlateAcquisition(PlateAcquisition):\n+    def __init__(\n+        self,\n+        acquisition_dir: Path | str,\n+        alignment: TileAlignmentOptions,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+    ):\n+        super().__init__(\n+            acquisition_dir=acquisition_dir,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrices,\n+            illumination_correction_matrices=illumination_correction_matrices,\n+        )\n+\n+    def _parse_files(self) -> pd.DataFrame:\n+        \"\"\"Parse all files in the acquisition directory.\n+\n+        Returns\n+        -------\n+        DataFrame\n+            Table of all files in the acquisition.\n+        \"\"\"\n+        return pd.DataFrame(\n+            ImageXpressPlateAcquisition._list_and_match_files(\n+                root_dir=self._acquisition_dir,\n+                root_re=self._get_root_re(),\n+                filename_re=self._get_filename_re(),\n+            )\n+        )\n+\n+    @staticmethod\n+    def _list_and_match_files(\n+        root_dir: Path | str,\n+        root_re: re.Pattern,\n+        filename_re: re.Pattern,\n+    ) -> list[list[dict[dict, str]]]:\n+        files = []\n+        for root, _, filenames in os.walk(root_dir):\n+            m_root = root_re.fullmatch(root)\n+            if m_root:\n+                for f in filenames:\n+                    m_filename = filename_re.fullmatch(f)\n+                    if m_filename:\n+                        row = m_root.groupdict()\n+                        row |= m_filename.groupdict()\n+                        if \"channel\" not in row or row[\"channel\"] is None:\n+                            row[\"channel\"] = \"w1\"\n+                        row[\"path\"] = str(Path(root).joinpath(f))\n+                        files.append(row)\n+        return files\n+\n+    @abstractmethod\n+    def _get_root_re(self) -> re.Pattern:\n+        \"\"\"Regular expression for matching the root directory of the acquisition.\"\"\"\n+        raise NotImplementedError\n+\n+    @abstractmethod\n+    def _get_filename_re(self) -> re.Pattern:\n+        \"\"\"Regular expression for matching the filename of the acquisition.\"\"\"\n+        raise NotImplementedError\n+\n+    def _build_well_acquisitions(self, files: pd.DataFrame) -> list[WellAcquisition]:\n+        return [\n+            ImageXpressWellAcquisition(\n+                files=files[files[\"well\"] == well],\n+                alignment=self._alignment,\n+                z_spacing=self._get_z_spacing(),\n+                background_correction_matrices=self._background_correction_matrices,\n+                illumination_correction_matrices=self._illumination_correction_matrices,\n+            )\n+            for well in tqdm(files[\"well\"].unique())\n+        ]\n+\n+    @abstractmethod\n+    def _get_z_spacing(self) -> float | None:\n+        raise NotImplementedError\n+\n+    def get_channel_metadata(self) -> dict[int, ChannelMetadata]:\n+        ch_metadata = {}\n+        _files = self._wells[0]._files\n+        for ch in _files[\"channel\"].unique():\n+            channel_files = _files[_files[\"channel\"] == ch]\n+            path = channel_files[\"path\"].iloc[0]\n+            metadata = load_metaseries_tiff_metadata(path=path)\n+            index = int(ch[1:]) - 1\n+            if \"Z Projection Method\" in metadata:\n+                name = (\n+                    f\"{metadata['Z Projection Method'].replace(' ', '-')}-Projection_\"\n+                    f\"{metadata['_IllumSetting_']}\"\n+                )\n+            else:\n+                name = metadata[\"_IllumSetting_\"]\n+            ch_metadata[index] = ChannelMetadata(\n+                channel_index=index,\n+                channel_name=name,\n+                display_color=rgb_to_hex(*wavelength_to_rgb(metadata[\"wavelength\"])),\n+                spatial_calibration_x=metadata[\"spatial-calibration-x\"],\n+                spatial_calibration_y=metadata[\"spatial-calibration-y\"],\n+                spatial_calibration_units=metadata[\"spatial-calibration-units\"],\n+                z_spacing=self._get_z_spacing(),\n+                wavelength=metadata[\"wavelength\"],\n+                exposure_time=float(metadata[\"Exposure Time\"].split(\" \")[0]),\n+                exposure_time_unit=metadata[\"Exposure Time\"].split(\" \")[1],\n+                objective=metadata[\"_MagSetting_\"],\n+            )\n+\n+        assert min(ch_metadata.keys()) == 0, \"Channel indices must start at 0.\"\n+        return ch_metadata\n+\n+\n+class SinglePlaneAcquisition(ImageXpressPlateAcquisition):\n+    \"\"\"Parse top folder (single planes) of an acquisition of a MolecularDevices ImageXpress Micro Confocal system.\n+\n+    Storage layout on disk for 2 wells with 2 fields and 2 channels::\n+\n+        MIP-2P-2sub --> {name} [Optional]\n+        \u2514\u2500\u2500 2022-07-05 --> {date}\n+            \u2514\u2500\u2500 1075 --> {acquisition id}\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w146C9B2CD-0BB3-4B8A-9187-2805F4C90506.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w1_thumb6EFE77C6-B96D-412A-9FD1-710DBDA32821.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w2B90625C8-6EA7-4E54-8289-C539EB75263E.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s1_w2_thumbEDDF803A-AE5E-4190-8C06-F54341AEC4A6.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w1E2913F7F-E229-4B6A-BFED-02BCF54561FA.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w1_thumb72E3641A-C91B-4501-900A-245BAC58FF46.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w241C38630-BCFD-4393-8706-58755CECE059.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C05_s2_w2_thumb5377A5AC-9BBF-4BAF-99A2-24896E3373A2.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w152C23B9A-EB4C-4DF6-8A7F-F4147A9E7DDE.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w1_thumb541AA634-387C-4B84-B0D8-EE4CB1C88E81.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w2FB0D7D9B-3EA0-445E-9A05-7D01154A9A5C.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s1_w2_thumb8FA1E466-57CD-4237-B09B-CAB48154647D.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s2_w1F365E60C-BCC2-4B74-9856-BCE07C8B0FD3.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s2_w1_thumb9652366E-36A0-4B7F-8B18-DA89D7DB41BD.tif\n+                \u251c\u2500\u2500 MIP-2P-2sub_C06_s2_w20EEC6AEA-1727-41E6-806C-40FF6AF68B6C.tif\n+                \u2514\u2500\u2500 MIP-2P-2sub_C06_s2_w2_thumb710CD846-0185-4362-BBAF-C700AE0013B3.tif\n+\n+    Image data is stored in {name}_{well}_{field}_w{channel}{md_id}.tif.\n+    The *_thumb*.tif files, used by Molecular Devices as preview, are ignored.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        acquisition_dir: Path | str,\n+        alignment: TileAlignmentOptions,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+    ):\n+        super().__init__(\n+            acquisition_dir=acquisition_dir,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrices,\n+            illumination_correction_matrices=illumination_correction_matrices,\n+        )\n+\n+    def _get_root_re(self) -> re.Pattern:\n+        return re.compile(\n+            r\".*[\\/\\\\](?P<date>\\d{4}-\\d{2}-\\d{2})[\\/\\\\](?P<acq_id>\\d+)(?:[\\/\\\\]TimePoint_(?P<t>\\d+))?\"\n+        )\n+\n+    def _get_filename_re(self) -> re.Pattern:\n+        return re.compile(\n+            r\"(?P<name>.*)_(?P<well>[A-Z]+\\d{2})_?(?P<field>s\\d+)?_?(?P<channel>w[1-9]{1})?(?!_thumb)(?P<md_id>[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})(?P<ext>.tif)\"\n+        )\n+\n+    def _get_z_spacing(self) -> float | None:\n+        return None\n+\n+\n+class StackAcquisition(ImageXpressPlateAcquisition):\n+    \"\"\"Image stack acquisition with a Molecular Devices ImageXpress Micro\n+    Confocal system.\n+\n+    MIP-2P-2sub-Stack --> {name} [Optional]\n+    \u2514\u2500\u2500 2023-02-21 --> {date}\n+        \u2514\u2500\u2500 1334 --> {acquisition id}\n+            \u251c\u2500\u2500 ZStep_1\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w1E78EB128-BD0D-4D94-A6AD-3FF28BB1B105.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w1_thumb187DE64B-038A-4671-BF6B-683721723769.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w2C0A49256-E289-4C0F-ADC9-F7728ABDB141.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w2_thumb57D4B151-71BF-480E-8CC4-C23A2690B763.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w427CCB2E4-1BF4-45E7-8BC7-264B48EF9C4A.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w4_thumb555647D0-77F1-4A43-9472-AE509F95E236.tif\n+            \u2502   \u251c\u2500\u2500 ...\n+            \u2502   \u2514\u2500\u2500 Projection-Mix_E08_s2_w4_thumbD2785594-4F49-464F-9F80-1B82E30A560A.tif\n+            \u251c\u2500\u2500 ...\n+            \u2514\u2500\u2500 ZStep_9\n+                \u251c\u2500\u2500 Projection-Mix_E07_s1_w1091EB8A5-272A-466D-B8A0-7547C6BA392B.tif\n+                \u251c\u2500\u2500 ...\n+                \u2514\u2500\u2500 Projection-Mix_E08_s2_w2_thumb210C0D5D-C20E-484D-AFB2-EFE669A56B84.tif\n+\n+    Image data is stored in {name}_{well}_{field}_w{channel}{md_id}.tif.\n+    The *_thumb*.tif files, used by Molecular Devices as preview, are ignored.\n+    \"\"\"\n+\n+    _z_spacing: float = None\n+\n+    def __init__(\n+        self,\n+        acquisition_dir: Path | str,\n+        alignment: TileAlignmentOptions,\n+        background_correction_matrices: dict[str, Path | str] | None = None,\n+        illumination_correction_matrices: dict[str, Path | str] | None = None,\n+    ):\n+        super().__init__(\n+            acquisition_dir=acquisition_dir,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrices,\n+            illumination_correction_matrices=illumination_correction_matrices,\n+        )\n+\n+    def _parse_files(self) -> pd.DataFrame:\n+        files = super()._parse_files()\n+        self._z_spacing = self._compute_z_spacing(files)\n+        return files\n+\n+    def _get_root_re(self) -> re.Pattern:\n+        return re.compile(\n+            r\".*[\\/\\\\](?P<date>\\d{4}-\\d{2}-\\d{2})[\\/\\\\](?P<acq_id>\\d+)(?:[\\/\\\\]TimePoint_(?P<t>\\d+))?(?:[\\/\\\\]ZStep_(?P<z>\\d+))\"\n+        )\n+\n+    def _get_filename_re(self) -> re.Pattern:\n+        return re.compile(\n+            r\"(?P<name>.*)_(?P<well>[A-Z]+\\d{2})_?(?P<field>s\\d+)?_?(?P<channel>w[1-9]{1})?(?!_thumb)(?P<md_id>[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})(?P<ext>.tif)\"\n+        )\n+\n+    def _get_z_spacing(self) -> float | None:\n+        return self._z_spacing\n+\n+    def _compute_z_spacing(self, files: pd.DataFrame) -> float | None:\n+        assert \"z\" in files.columns, \"No z column in files DataFrame.\"\n+        channel_with_stack = np.sort(files[files[\"z\"] == \"2\"][\"channel\"].unique())[0]\n+        subset = files[files[\"channel\"] == channel_with_stack]\n+        subset = subset[subset[\"well\"] == np.sort(subset[\"well\"].unique())[0]]\n+        first_field = np.sort(subset[\"field\"].unique())[0]\n+        if first_field is not None:\n+            subset = subset[subset[\"field\"] == np.sort(subset[\"field\"].unique())[0]]\n+\n+        plane_positions = []\n+\n+        for _i, row in subset.iterrows():\n+            file = row[\"path\"]\n+            if \"z\" in row and row[\"z\"] is not None:\n+                metadata = load_metaseries_tiff_metadata(file)\n+                z_position = metadata[\"stage-position-z\"]\n+                plane_positions.append(z_position)\n+\n+        plane_positions = np.array(sorted(plane_positions), dtype=np.float32)\n+\n+        precision = -Decimal(str(plane_positions[0])).as_tuple().exponent\n+        return np.round(np.mean(np.diff(plane_positions)), decimals=precision)\n+\n+\n+class MixedAcquisition(StackAcquisition):\n+    \"\"\"Image stack acquisition with Projectsion acquired with a Molecular\n+    Devices ImageXpress Micro Confocal system.\n+\n+    MIP-2P-2sub-Stack --> {name} [Optional]\n+    \u2514\u2500\u2500 2023-02-21 --> {date}\n+        \u2514\u2500\u2500 1334 --> {acquisition id}\n+            \u251c\u2500\u2500 Projection-Mix_E07_s1_w1E94C24BD-45E4-450A-9919-257C714278F7.tif\n+            \u251c\u2500\u2500 Projection-Mix_E07_s1_w1_thumb4BFD4018-E675-475E-B5AB-2E959E6B6DA1.tif\n+            \u251c\u2500\u2500 ...\n+            \u251c\u2500\u2500 Projection-Mix_E08_s2_w3CCE83D85-0912-429E-9F18-716A085BB5BC.tif\n+            \u251c\u2500\u2500 Projection-Mix_E08_s2_w3_thumb4D88636E-181E-4AF6-BC53-E7A435959C8F.tif\n+            \u251c\u2500\u2500 ZStep_1\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w1E78EB128-BD0D-4D94-A6AD-3FF28BB1B105.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w1_thumb187DE64B-038A-4671-BF6B-683721723769.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w2C0A49256-E289-4C0F-ADC9-F7728ABDB141.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w2_thumb57D4B151-71BF-480E-8CC4-C23A2690B763.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w427CCB2E4-1BF4-45E7-8BC7-264B48EF9C4A.tif\n+            \u2502   \u251c\u2500\u2500 Projection-Mix_E07_s1_w4_thumb555647D0-77F1-4A43-9472-AE509F95E236.tif\n+            \u2502   \u251c\u2500\u2500 ...\n+            \u2502   \u2514\u2500\u2500 Projection-Mix_E08_s2_w4_thumbD2785594-4F49-464F-9F80-1B82E30A560A.tif\n+            \u251c\u2500\u2500 ...\n+            \u2514\u2500\u2500 ZStep_9\n+                \u251c\u2500\u2500 Projection-Mix_E07_s1_w1091EB8A5-272A-466D-B8A0-7547C6BA392B.tif\n+                \u251c\u2500\u2500 ...\n+                \u2514\u2500\u2500 Projection-Mix_E08_s2_w2_thumb210C0D5D-C20E-484D-AFB2-EFE669A56B84.tif\n+\n+    Image data is stored in {name}_{well}_{field}_w{channel}{md_id}.tif.\n+    The *_thumb*.tif files, used by Molecular Devices as preview, are ignored.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        acquisition_dir: Path | str,\n+        alignment: TileAlignmentOptions,\n+        background_correction_matrix: dict[str, NDArray] | None = None,\n+        illumination_correction_matrix: NDArray | None = None,\n+    ):\n+        super().__init__(\n+            acquisition_dir=acquisition_dir,\n+            alignment=alignment,\n+            background_correction_matrices=background_correction_matrix,\n+            illumination_correction_matrices=illumination_correction_matrix,\n+        )\n+\n+    def _parse_files(self) -> pd.DataFrame:\n+        files = self._filter_mips(super()._parse_files())\n+        self._z_spacing = self._compute_z_spacing(files)\n+        return files\n+\n+    def _filter_mips(self, files: pd.DataFrame) -> pd.DataFrame:\n+        \"\"\"Remove MIP files if the whole stack was acquired.\"\"\"\n+        _files = files.copy()\n+        for ch in _files[\"channel\"].unique():\n+            channel_files = _files[_files[\"channel\"] == ch]\n+            z_positions = channel_files[\"z\"].unique()\n+            has_mip = None in z_positions\n+            has_stack = len(z_positions) > 1\n+            if has_mip and has_stack:\n+                _files.drop(\n+                    _files[(_files[\"channel\"] == ch) & (_files[\"z\"].isna())].index,\n+                    inplace=True,\n+                )\n+        return _files\n+\n+    def _get_root_re(self) -> re.Pattern:\n+        return re.compile(\n+            r\".*[\\/\\\\](?P<date>\\d{4}-\\d{2}-\\d{2})[\\/\\\\](?P<acq_id>\\d+)(?:[\\/\\\\]TimePoint_(?P<t>\\d+))?(?:[\\/\\\\]ZStep_(?P<z>\\d+))?.*\"\n+        )\n+\n+    def _get_filename_re(self) -> re.Pattern:\n+        return re.compile(\n+            r\"(?P<name>.*)_(?P<well>[A-Z]+\\d{2})_?(?P<field>s\\d+)?_?(?P<channel>w[1-9]{1})?(?!_thumb)(?P<md_id>[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})(?P<ext>.tif)\"\n+        )\ndiff --git a/src/faim_ipa/hcs/plate.py b/src/faim_ipa/hcs/plate.py\nindex c579f822..66230694 100644\n--- a/src/faim_ipa/hcs/plate.py\n+++ b/src/faim_ipa/hcs/plate.py\n@@ -1,5 +1,4 @@\n from enum import IntEnum\n-from typing import Union\n \n \n class PlateLayout(IntEnum):\n@@ -11,44 +10,23 @@ class PlateLayout(IntEnum):\n     I384 = 384\n \n \n-def get_rows_and_columns(\n-    layout: Union[PlateLayout, int]\n-) -> tuple[list[str], list[str]]:\n+def get_rows_and_columns(layout: PlateLayout | int) -> tuple[list[str], list[str]]:\n     \"\"\"Return rows and columns for requested layout.\"\"\"\n     if layout == PlateLayout.I18:\n-        rows = [\"A\", \"B\", \"C\"]\n+        rows = list(\"ABC\")\n         cols = [str(i).zfill(2) for i in range(1, 7)]\n-        assert len(rows) * len(cols) == 18\n     elif layout == PlateLayout.I24:\n-        rows = [\"A\", \"B\", \"C\", \"D\"]\n+        rows = list(\"ABCD\")\n         cols = [str(i).zfill(2) for i in range(1, 7)]\n-        assert len(rows) * len(cols) == 24\n     elif layout == PlateLayout.I96:\n-        rows = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n+        rows = list(\"ABCDEFGH\")\n         cols = [str(i).zfill(2) for i in range(1, 13)]\n-        assert len(rows) * len(cols) == 96\n     elif layout == PlateLayout.I384:\n-        rows = [\n-            \"A\",\n-            \"B\",\n-            \"C\",\n-            \"D\",\n-            \"E\",\n-            \"F\",\n-            \"G\",\n-            \"H\",\n-            \"I\",\n-            \"J\",\n-            \"K\",\n-            \"L\",\n-            \"M\",\n-            \"N\",\n-            \"O\",\n-            \"P\",\n-        ]\n+        rows = list(\"ABCDEFGHIJKLMNOP\")\n         cols = [str(i).zfill(2) for i in range(1, 25)]\n-        assert len(rows) * len(cols) == 384\n     else:\n-        raise NotImplementedError(f\"{layout} layout not supported.\")\n+        msg = f\"{layout} layout not supported.\"\n+        raise NotImplementedError(msg)\n \n+    assert len(rows) * len(cols) == layout\n     return rows, cols\ndiff --git a/src/faim_ipa/UIntHistogram.py b/src/faim_ipa/histogram.py\nsimilarity index 93%\nrename from src/faim_ipa/UIntHistogram.py\nrename to src/faim_ipa/histogram.py\nindex cd673587..f2e918c4 100644\n--- a/src/faim_ipa/UIntHistogram.py\n+++ b/src/faim_ipa/histogram.py\n@@ -25,7 +25,7 @@ def _add(list_a, list_b):\n         :param list_b: list(numeric)\n         :return: list(numeric)\n         \"\"\"\n-        return list(map(lambda e: e[0] + e[1], zip(list_a, list_b)))\n+        return [e[0] + e[1] for e in zip(list_a, list_b, strict=False)]\n \n     @staticmethod\n     def _get_hist(data):\n@@ -151,13 +151,12 @@ def combine(self, histogram):\n         if self.frequencies is None:\n             self.frequencies = histogram.frequencies\n             self.offset = histogram.offset\n-        else:\n-            if histogram.frequencies is not None:\n-                self._aggregate_histograms(\n-                    offset_data=histogram.offset,\n-                    bins=histogram.n_bins(),\n-                    freq=histogram.frequencies,\n-                )\n+        elif histogram.frequencies is not None:\n+            self._aggregate_histograms(\n+                offset_data=histogram.offset,\n+                bins=histogram.n_bins(),\n+                freq=histogram.frequencies,\n+            )\n \n         return self\n \n@@ -183,9 +182,10 @@ def plot(self, width=1):\n         Plot histogram.\n         \"\"\"\n         if width > 1:\n-            heights = []\n-            for i in range(self.offset, self.offset + self.n_bins(), width):\n-                heights.append(np.sum(self.frequencies[i : i + width]))\n+            heights = [\n+                np.sum(self.frequencies[i : i + width])\n+                for i in range(self.offset, self.offset + self.n_bins(), width)\n+            ]\n         else:\n             heights = self.frequencies\n \n@@ -228,7 +228,8 @@ def quantile(self, q):\n         :param q: uint\n         :return: quantile\n         \"\"\"\n-        assert q >= 0 and q <= 1\n+        assert q >= 0\n+        assert q <= 1\n         if self.frequencies is None:\n             return 0\n         return self.offset + np.argmax(\ndiff --git a/src/faim_ipa/io/ChannelMetadata.py b/src/faim_ipa/io/metadata.py\nsimilarity index 58%\nrename from src/faim_ipa/io/ChannelMetadata.py\nrename to src/faim_ipa/io/metadata.py\nindex 49bc9768..163e472e 100644\n--- a/src/faim_ipa/io/ChannelMetadata.py\n+++ b/src/faim_ipa/io/metadata.py\n@@ -1,5 +1,3 @@\n-from typing import Optional\n-\n from pydantic import BaseModel, NonNegativeInt, PositiveFloat\n \n \n@@ -10,8 +8,8 @@ class ChannelMetadata(BaseModel):\n     spatial_calibration_x: PositiveFloat\n     spatial_calibration_y: PositiveFloat\n     spatial_calibration_units: str\n-    z_spacing: Optional[PositiveFloat] = None\n-    wavelength: Optional[NonNegativeInt] = None\n-    exposure_time: Optional[PositiveFloat] = None\n-    exposure_time_unit: Optional[str] = None\n+    z_spacing: PositiveFloat | None = None\n+    wavelength: NonNegativeInt | None = None\n+    exposure_time: PositiveFloat | None = None\n+    exposure_time_unit: str | None = None\n     objective: str\ndiff --git a/src/faim_ipa/io/MetaSeriesTiff.py b/src/faim_ipa/io/metaseries.py\nsimilarity index 100%\nrename from src/faim_ipa/io/MetaSeriesTiff.py\nrename to src/faim_ipa/io/metaseries.py\ndiff --git a/src/faim_ipa/MetaSeriesUtils_dask.py b/src/faim_ipa/metaseries_utils_dask.py\nsimilarity index 95%\nrename from src/faim_ipa/MetaSeriesUtils_dask.py\nrename to src/faim_ipa/metaseries_utils_dask.py\nindex 6dacd803..b8dded6c 100644\n--- a/src/faim_ipa/MetaSeriesUtils_dask.py\n+++ b/src/faim_ipa/metaseries_utils_dask.py\n@@ -52,7 +52,7 @@ def fuse_fw(tiles: ArrayLike, positions: ArrayLike) -> ArrayLike:\n     im_fused = np.zeros((ny_tot, nx_tot), dtype=tiles.dtype)\n     ny_tile, nx_tile = tiles.shape[-2:]\n \n-    for tile, pos in zip(tiles, positions):\n+    for tile, pos in zip(tiles, positions, strict=True):\n         im_fused[pos[0] : pos[0] + ny_tile, pos[1] : pos[1] + nx_tile] = tile\n \n     return im_fused\n@@ -70,7 +70,7 @@ def fuse_rev(tiles: ArrayLike, positions: ArrayLike) -> ArrayLike:\n     im_fused = np.zeros((ny_tot, nx_tot), dtype=tiles.dtype)\n     ny_tile, nx_tile = tiles.shape[-2:]\n \n-    for tile, pos in reversed(list(zip(tiles, positions))):\n+    for tile, pos in reversed(list(zip(tiles, positions, strict=True))):\n         im_fused[pos[0] : pos[0] + ny_tile, pos[1] : pos[1] + nx_tile] = tile\n \n     return im_fused\ndiff --git a/src/faim_ipa/mobie.py b/src/faim_ipa/mobie.py\nindex a9703a46..1de60b1b 100644\n--- a/src/faim_ipa/mobie.py\n+++ b/src/faim_ipa/mobie.py\n@@ -19,7 +19,7 @@\n from skimage.measure import regionprops_table\n from tqdm.auto import tqdm\n \n-from faim_ipa.UIntHistogram import UIntHistogram\n+from faim_ipa.histogram import UIntHistogram\n \n \n def hex_to_rgba(h) -> str:\n@@ -96,17 +96,17 @@ def add_wells_to_project(\n                 view={},  # do not create default view for source\n             )\n \n-            if key not in sources.keys():\n+            if key not in sources:\n                 sources[key] = [name]\n             else:\n                 sources[key].append(name)\n \n-            if key not in plate_hists.keys():\n+            if key not in plate_hists:\n                 plate_hists[key] = copy(hists[k])\n             else:\n                 plate_hists[key].combine(hists[k])\n \n-            if key not in plate_colors.keys():\n+            if key not in plate_colors:\n                 plate_colors[key] = hex_to_rgba(ch[\"color\"])\n \n     _add_well_regions(\n@@ -126,6 +126,7 @@ def add_wells_to_project(\n def add_labels_view(\n     plate: zarr.Group,\n     dataset_folder: str,\n+    *,\n     well_group: str = \"0\",\n     channel: int = 0,\n     label_name: str = \"default\",\n@@ -161,14 +162,14 @@ def add_labels_view(\n         spacing = datasets[0][\"coordinateTransformations\"][0][\"scale\"]\n         props = regionprops_table(\n             label_img[np.newaxis, :],\n-            properties=(\"label\", \"centroid\") + extra_properties,\n+            properties=(\"label\", \"centroid\", *extra_properties),\n             spacing=spacing,\n         )\n         if not add_empty_tables and len(props[\"label\"]) == 0:\n             continue\n \n         # write default.tsv to dataset_folder/tables/name\n-        # TODO reconcile once saving table data inside zarr is possible\n+        # TODO: reconcile once saving table data inside zarr is possible\n         table_folder = join(dataset_folder, \"tables\", name)\n         os.makedirs(table_folder, exist_ok=True)\n \n@@ -262,7 +263,6 @@ def compute_aggregate_table_values(\n     summary.columns = [\"_\".join(headers) for headers in summary.columns.to_flat_index()]\n     # add suffix to column names\n     summary.columns = [f\"{header}_{table_suffix}\" for header in summary.columns]\n-    print(summary)\n \n     # join with original wells table\n     wells_table.join(summary, on=\"region_id\").to_csv(\n@@ -295,7 +295,7 @@ def _add_channel_plate_overviews(\n         )\n         default[\"sourceTransforms\"].append(\n             get_merged_grid_source_transform(\n-                sources=[src for src in sources[ch]],\n+                sources=list(sources[ch]),\n                 merged_source_name=f\"merged_view_plate_{name}\",\n                 positions=[to_position(src[:3]) for src in sources[ch]],\n             )\n@@ -324,9 +324,9 @@ def _add_channel_plate_overviews(\n \n def _get_well_sources_per_channel(sources):\n     wells_per_channel = {}\n-    for ch in sources.keys():\n+    for ch in sources:\n         for well in sources[ch]:\n-            if well[:3] not in wells_per_channel.keys():\n+            if well[:3] not in wells_per_channel:\n                 wells_per_channel[well[:3]] = [well]\n             else:\n                 wells_per_channel[well[:3]].append(well)\ndiff --git a/src/faim_ipa/stitching/__init__.py b/src/faim_ipa/stitching/__init__.py\nindex d8f1b7b3..7a510850 100644\n--- a/src/faim_ipa/stitching/__init__.py\n+++ b/src/faim_ipa/stitching/__init__.py\n@@ -1,3 +1,7 @@\n-from .BoundingBox5D import BoundingBox5D  # noqa: F401\n-from .DaskTileStitcher import DaskTileStitcher  # noqa: F401\n-from .Tile import Tile  # noqa: F401\n+from faim_ipa.stitching.bounding_box import BoundingBox5D\n+from faim_ipa.stitching.dask import DaskTileStitcher\n+\n+__all__ = [\n+    \"BoundingBox5D\",\n+    \"DaskTileStitcher\",\n+]\ndiff --git a/src/faim_ipa/stitching/BoundingBox5D.py b/src/faim_ipa/stitching/bounding_box.py\nsimilarity index 100%\nrename from src/faim_ipa/stitching/BoundingBox5D.py\nrename to src/faim_ipa/stitching/bounding_box.py\ndiff --git a/src/faim_ipa/stitching/DaskTileStitcher.py b/src/faim_ipa/stitching/dask.py\nsimilarity index 90%\nrename from src/faim_ipa/stitching/DaskTileStitcher.py\nrename to src/faim_ipa/stitching/dask.py\nindex 270517a2..0df5c41d 100644\n--- a/src/faim_ipa/stitching/DaskTileStitcher.py\n+++ b/src/faim_ipa/stitching/dask.py\n@@ -1,13 +1,13 @@\n+from collections.abc import Callable\n from copy import copy\n from functools import partial\n-from typing import Callable, Optional, Union\n \n import numpy as np\n from dask import array as da\n from dask.array.core import normalize_chunks\n \n from faim_ipa.stitching import BoundingBox5D, stitching_utils\n-from faim_ipa.stitching.Tile import Tile\n+from faim_ipa.stitching.tile import Tile\n \n \n class DaskTileStitcher:\n@@ -18,8 +18,8 @@ class DaskTileStitcher:\n     def __init__(\n         self,\n         tiles: list[Tile],\n-        chunk_shape: Union[tuple[int, int], tuple[int, int, int]],\n-        output_shape: Optional[tuple[int, int, int, int, int]] = None,\n+        chunk_shape: tuple[int, int] | tuple[int, int, int],\n+        output_shape: tuple[int, int, int, int, int] | None = None,\n         dtype: np.dtype = np.uint16,\n     ):\n         \"\"\"\n@@ -54,7 +54,7 @@ def _build_tiles_lut(self):\n                 tile.position.channel,\n                 tile.position.z,\n             )\n-            if tcz_pos in lut.keys():\n+            if tcz_pos in lut:\n                 lut[tcz_pos].append(tile)\n             else:\n                 lut[tcz_pos] = [tile]\n@@ -82,7 +82,7 @@ def _compute_block_to_tile_map(self):\n                 shape=self.chunk_shape,\n             )\n             pos = (block_bbox.time_start, block_bbox.channel_start, block_bbox.z_start)\n-            if pos in tiles_lut.keys():\n+            if pos in tiles_lut:\n                 for tile in tiles_lut[pos]:\n                     tile_bbox = BoundingBox5D.from_pos_and_shape(\n                         position=tile.get_position(),\n@@ -97,18 +97,17 @@ def _compute_output_shape(self):\n         \"\"\"\n         Compute the shape of the stitched image.\n         \"\"\"\n-        tile_extents = []\n-        for tile in self.tiles:\n-            tile_extents.append(\n-                tile.get_position()\n-                + np.array((1,) * (5 - len(tile.shape)) + tile.shape)\n-            )\n+        tile_extents = [\n+            (tile.get_position() + np.array((1,) * (5 - len(tile.shape)) + tile.shape))\n+            for tile in self.tiles\n+        ]\n         return tuple(np.max(tile_extents, axis=0))\n \n     def get_stitched_dask_array(\n         self,\n         warp_func: Callable = stitching_utils.translate_tiles_2d,\n         fuse_func: Callable = stitching_utils.fuse_mean,\n+        *,\n         build_acquisition_mask: bool = False,\n     ) -> da.array:\n         \"\"\"\n@@ -145,6 +144,7 @@ def get_stitched_image(\n         self,\n         transform_func: Callable = stitching_utils.translate_tiles_2d,\n         fuse_func: Callable = stitching_utils.fuse_mean,\n+        *,\n         build_acquisition_mask: bool = False,\n     ) -> np.ndarray:\n         \"\"\"\ndiff --git a/src/faim_ipa/stitching/stitching_utils.py b/src/faim_ipa/stitching/stitching_utils.py\nindex edee19c7..f4d4956c 100644\n--- a/src/faim_ipa/stitching/stitching_utils.py\n+++ b/src/faim_ipa/stitching/stitching_utils.py\n@@ -4,7 +4,7 @@\n from numpy._typing import NDArray\n from scipy.ndimage import distance_transform_cdt\n \n-from faim_ipa.stitching.Tile import Tile, TilePosition\n+from faim_ipa.stitching.tile import Tile, TilePosition\n \n \n def fuse_linear(warped_tiles: NDArray, warped_distance_masks: NDArray) -> NDArray:\n@@ -101,7 +101,9 @@ def fuse_mean(warped_tiles: NDArray, warped_distance_masks: NDArray) -> NDArray:\n     return fused_image.astype(warped_tiles.dtype)\n \n \n-def fuse_sum(warped_tiles: NDArray, warped_distance_masks: NDArray) -> NDArray:\n+def fuse_sum(\n+    warped_tiles: NDArray, warped_distance_masks: NDArray  # noqa: ARG001\n+) -> NDArray:\n     \"\"\"\n     Fuse transformed tiles and compute the sum of the overlapping pixels.\n \n@@ -144,7 +146,7 @@ def fuse_overlay_fwd(warped_tiles: NDArray, warped_distance_masks: NDArray) -> N\n     warped_masks = warped_distance_masks.astype(bool)\n \n     fused_image = np.zeros_like(warped_tiles[0])\n-    for tile, mask in zip(warped_tiles, warped_masks):\n+    for tile, mask in zip(warped_tiles, warped_masks, strict=True):\n         fused_image[mask] = tile[mask]\n \n     return fused_image\n@@ -172,14 +174,14 @@ def fuse_overlay_bwd(warped_tiles: NDArray, warped_distance_masks: NDArray) -> N\n     warped_masks = warped_distance_masks.astype(bool)\n \n     fused_image = np.zeros_like(warped_tiles[0])\n-    for tile, mask in zip(reversed(warped_tiles), reversed(warped_masks)):\n+    for tile, mask in zip(reversed(warped_tiles), reversed(warped_masks), strict=True):\n         fused_image[mask] = tile[mask]\n \n     return fused_image\n \n \n def translate_tiles_2d(\n-    block_info, chunk_shape, tiles, build_acquisition_mask: bool = False\n+    block_info, chunk_shape, tiles, *, build_acquisition_mask: bool = False\n ):\n     \"\"\"\n     Translate tiles to their relative position inside the given block.\n@@ -203,7 +205,8 @@ def translate_tiles_2d(\n     )\n \n     if not all(tile.shape == tiles[0].shape for tile in tiles):\n-        raise ValueError(\"All tiles must have the same shape.\")\n+        msg = \"All tiles must have the same shape.\"\n+        raise ValueError(msg)\n     distance_mask = get_distance_mask(tiles[0].shape)\n     if distance_mask.ndim == 2:\n         distance_mask = distance_mask[np.newaxis, ...]\n@@ -212,10 +215,9 @@ def translate_tiles_2d(\n     warped_distance_masks = []\n     for tile in tiles:\n         tile_origin = np.array(tile.get_zyx_position())\n-        if build_acquisition_mask:\n-            tile_data = tile.load_data_mask()\n-        else:\n-            tile_data = tile.load_data()\n+        tile_data = (\n+            tile.load_data_mask() if build_acquisition_mask else tile.load_data()\n+        )\n         if tile_data.ndim == 2:\n             tile_data = tile_data[np.newaxis, ...]\n         warped_tile = shift_yx(chunk_zyx_origin, tile_data, tile_origin, chunk_shape)\n@@ -271,6 +273,7 @@ def assemble_chunk(\n     warp_func=None,\n     fuse_func=None,\n     dtype=None,\n+    *,\n     build_acquisition_mask: bool = False,\n ):\n     \"\"\"\n@@ -299,7 +302,10 @@ def assemble_chunk(\n \n     if len(tiles) > 0:\n         warped_tiles, warped_distance_masks = warp_func(\n-            block_info, chunk_shape[-3:], tiles, build_acquisition_mask\n+            block_info,\n+            chunk_shape[-3:],\n+            tiles,\n+            build_acquisition_mask=build_acquisition_mask,\n         )\n \n         if len(tiles) > 1:\ndiff --git a/src/faim_ipa/stitching/Tile.py b/src/faim_ipa/stitching/tile.py\nsimilarity index 85%\nrename from src/faim_ipa/stitching/Tile.py\nrename to src/faim_ipa/stitching/tile.py\nindex d07db6d1..d4524490 100644\n--- a/src/faim_ipa/stitching/Tile.py\n+++ b/src/faim_ipa/stitching/tile.py\n@@ -1,5 +1,4 @@\n from pathlib import Path\n-from typing import Optional, Union\n \n import numpy as np\n from numpy._typing import NDArray\n@@ -8,8 +7,8 @@\n \n \n class TilePosition(BaseModel):\n-    time: Optional[NonNegativeInt]\n-    channel: Optional[NonNegativeInt]\n+    time: NonNegativeInt | None\n+    channel: NonNegativeInt | None\n     z: int\n     y: int\n     x: int\n@@ -29,16 +28,16 @@ class Tile:\n     path: str\n     shape: tuple[int, ...]\n     position: TilePosition\n-    background_correction_matrix_path: Optional[Union[Path, str]] = None\n-    illumination_correction_matrix_path: Optional[Union[Path, str]] = None\n+    background_correction_matrix_path: Path | str | None = None\n+    illumination_correction_matrix_path: Path | str | None = None\n \n     def __init__(\n         self,\n-        path: Union[Path, str],\n+        path: Path | str,\n         shape: tuple[int, int],\n         position: TilePosition,\n-        background_correction_matrix_path: Optional[Union[Path, str]] = None,\n-        illumination_correction_matrix_path: Optional[Union[Path, str]] = None,\n+        background_correction_matrix_path: Path | str | None = None,\n+        illumination_correction_matrix_path: Path | str | None = None,\n     ):\n         super().__init__()\n         self.path = path\n@@ -81,9 +80,7 @@ def load_data(self) -> NDArray:\n         \"\"\"\n         data = imread(self.path)\n         data = self._apply_background_correction(data)\n-        data = self._apply_illumination_correction(data)\n-\n-        return data\n+        return self._apply_illumination_correction(data)\n \n     def load_data_mask(self) -> NDArray:\n         \"\"\"\ndiff --git a/src/faim_ipa/utils.py b/src/faim_ipa/utils.py\nindex 714b2736..0fba8f8f 100644\n--- a/src/faim_ipa/utils.py\n+++ b/src/faim_ipa/utils.py\n@@ -1,7 +1,7 @@\n import logging\n+import os.path\n import pathlib\n from datetime import datetime\n-import os.path\n \n import pydantic\n from pydantic import BaseModel\n@@ -19,40 +19,40 @@ def wavelength_to_rgb(wavelength, gamma=0.8):\n     \"\"\"\n \n     wavelength = float(wavelength)\n-    if 380 <= wavelength <= 440:\n+    if 380 <= wavelength <= 440:  # noqa: PLR2004\n         attenuation = 0.3 + 0.7 * (wavelength - 380) / (440 - 380)\n-        R = ((-(wavelength - 440) / (440 - 380)) * attenuation) ** gamma\n-        G = 0.0\n-        B = (1.0 * attenuation) ** gamma\n-    elif 440 <= wavelength <= 490:\n-        R = 0.0\n-        G = ((wavelength - 440) / (490 - 440)) ** gamma\n-        B = 1.0\n-    elif 490 <= wavelength <= 510:\n-        R = 0.0\n-        G = 1.0\n-        B = (-(wavelength - 510) / (510 - 490)) ** gamma\n-    elif 510 <= wavelength <= 580:\n-        R = ((wavelength - 510) / (580 - 510)) ** gamma\n-        G = 1.0\n-        B = 0.0\n-    elif 580 <= wavelength <= 645:\n-        R = 1.0\n-        G = (-(wavelength - 645) / (645 - 580)) ** gamma\n-        B = 0.0\n-    elif 645 <= wavelength <= 750:\n+        r = ((-(wavelength - 440) / (440 - 380)) * attenuation) ** gamma\n+        g = 0.0\n+        b = (1.0 * attenuation) ** gamma\n+    elif 440 <= wavelength <= 490:  # noqa: PLR2004\n+        r = 0.0\n+        g = ((wavelength - 440) / (490 - 440)) ** gamma\n+        b = 1.0\n+    elif 490 <= wavelength <= 510:  # noqa: PLR2004\n+        r = 0.0\n+        g = 1.0\n+        b = (-(wavelength - 510) / (510 - 490)) ** gamma\n+    elif 510 <= wavelength <= 580:  # noqa: PLR2004\n+        r = ((wavelength - 510) / (580 - 510)) ** gamma\n+        g = 1.0\n+        b = 0.0\n+    elif 580 <= wavelength <= 645:  # noqa: PLR2004\n+        r = 1.0\n+        g = (-(wavelength - 645) / (645 - 580)) ** gamma\n+        b = 0.0\n+    elif 645 <= wavelength <= 750:  # noqa: PLR2004\n         attenuation = 0.3 + 0.7 * (750 - wavelength) / (750 - 645)\n-        R = (1.0 * attenuation) ** gamma\n-        G = 0.0\n-        B = 0.0\n+        r = (1.0 * attenuation) ** gamma\n+        g = 0.0\n+        b = 0.0\n     else:\n-        R = 0.0\n-        G = 0.0\n-        B = 0.0\n-    R *= 255\n-    G *= 255\n-    B *= 255\n-    return int(R), int(G), int(B)\n+        r = 0.0\n+        g = 0.0\n+        b = 0.0\n+    r *= 255\n+    g *= 255\n+    b *= 255\n+    return int(r), int(g), int(b)\n \n \n def rgb_to_hex(r, g, b):\n@@ -70,7 +70,7 @@ def create_logger(name: str) -> logging.Logger:\n     name\n         Name of the logger instance.\n     \"\"\"\n-    logger = logging.Logger(name.capitalize())\n+    logger = logging.getLogger(name.capitalize())\n     now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n     handler = logging.FileHandler(f\"{now}-{name}.log\")\n     handler.setLevel(logging.INFO)\n@@ -148,10 +148,11 @@ def make_paths_absolute(self):\n         The paths are assumed to be relative to a git-root directory somewhere\n         in the parent directories of the class implementing `IPAConfig`.\n         \"\"\"\n-        if pydantic.__version__.startswith(\"2\"):\n-            fields = self.model_fields_set\n-        else:\n-            fields = self.__fields_set__\n+        fields = (\n+            self.model_fields_set\n+            if pydantic.__version__.startswith(\"2\")\n+            else self.__fields_set__\n+        )\n \n         for f in fields:\n             attr = getattr(self, f)\n@@ -166,10 +167,11 @@ def make_paths_relative(self):\n         somewhere in the parent directories of the class implementing\n         `IPAConfig`.\n         \"\"\"\n-        if pydantic.__version__.startswith(\"2\"):\n-            fields = self.model_fields_set\n-        else:\n-            fields = self.__fields_set__\n+        fields = (\n+            self.model_fields_set\n+            if pydantic.__version__.startswith(\"2\")\n+            else self.__fields_set__\n+        )\n \n         for f in fields:\n             attr = getattr(self, f)\ndiff --git a/src/faim_ipa/visiview/StackedTile.py b/src/faim_ipa/visiview/StackedTile.py\ndeleted file mode 100644\nindex 0f63fa2d..00000000\n--- a/src/faim_ipa/visiview/StackedTile.py\n+++ /dev/null\n@@ -1,42 +0,0 @@\n-from pathlib import Path\n-from typing import Optional, Union\n-\n-import numpy as np\n-import tifffile\n-from numpy._typing import NDArray\n-\n-from faim_ipa.stitching import Tile\n-from faim_ipa.stitching.Tile import TilePosition\n-\n-\n-class StackedTile(Tile):\n-    def __init__(\n-        self,\n-        path: Union[Path, str],\n-        shape: tuple[int, int],\n-        position: TilePosition,\n-        background_correction_matrix_path: Optional[Union[Path, str]] = None,\n-        illumination_correction_matrix_path: Optional[Union[Path, str]] = None,\n-        memmap: bool = True,\n-    ):\n-        self._memmap = memmap\n-        super().__init__(\n-            path=path,\n-            shape=shape,\n-            position=position,\n-            background_correction_matrix_path=background_correction_matrix_path,\n-            illumination_correction_matrix_path=illumination_correction_matrix_path,\n-        )\n-\n-    def load_data(self) -> NDArray:\n-        if self._memmap:\n-            data = tifffile.memmap(self.path, mode=\"r\")\n-        else:\n-            data = tifffile.imread(self.path)\n-\n-        data = self._apply_background_correction(data)\n-        data = self._apply_illumination_correction(data)\n-        return data\n-\n-    def load_data_mask(self) -> NDArray:\n-        return np.ones(self.shape, dtype=bool)\ndiff --git a/src/faim_ipa/visiview/__init__.py b/src/faim_ipa/visiview/__init__.py\nindex e69de29b..4ebb209c 100644\n--- a/src/faim_ipa/visiview/__init__.py\n+++ b/src/faim_ipa/visiview/__init__.py\n@@ -0,0 +1,11 @@\n+from faim_ipa.visiview.acquisition import (\n+    RegionAcquisitionOME,\n+    RegionAcquisitionSTK,\n+    StackedTile,\n+)\n+\n+__all__ = [\n+    \"RegionAcquisitionOME\",\n+    \"RegionAcquisitionSTK\",\n+    \"StackedTile\",\n+]\ndiff --git a/src/faim_ipa/visiview/RegionAcquisition.py b/src/faim_ipa/visiview/acquisition.py\nsimilarity index 66%\nrename from src/faim_ipa/visiview/RegionAcquisition.py\nrename to src/faim_ipa/visiview/acquisition.py\nindex 532c854f..0993c6ad 100644\n--- a/src/faim_ipa/visiview/RegionAcquisition.py\n+++ b/src/faim_ipa/visiview/acquisition.py\n@@ -1,16 +1,49 @@\n from pathlib import Path\n-from typing import Optional, Union\n \n+import numpy as np\n import pandas as pd\n+import tifffile\n from numpy._typing import NDArray\n from tifffile import TiffFile\n \n from faim_ipa.hcs.acquisition import TileAlignmentOptions, WellAcquisition\n-from faim_ipa.io.ChannelMetadata import ChannelMetadata\n-from faim_ipa.stitching import Tile\n-from faim_ipa.stitching.Tile import TilePosition\n+from faim_ipa.io.metadata import ChannelMetadata\n+from faim_ipa.stitching.tile import Tile, TilePosition\n from faim_ipa.visiview.ome_companion_utils import parse_basic_metadata\n-from faim_ipa.visiview.StackedTile import StackedTile\n+\n+\n+class StackedTile(Tile):\n+    def __init__(\n+        self,\n+        path: Path | str,\n+        shape: tuple[int, int],\n+        position: TilePosition,\n+        background_correction_matrix_path: Path | str | None = None,\n+        illumination_correction_matrix_path: Path | str | None = None,\n+        *,\n+        memmap: bool = True,\n+    ):\n+        self._memmap = memmap\n+        super().__init__(\n+            path=path,\n+            shape=shape,\n+            position=position,\n+            background_correction_matrix_path=background_correction_matrix_path,\n+            illumination_correction_matrix_path=illumination_correction_matrix_path,\n+        )\n+\n+    def load_data(self) -> NDArray:\n+        data = (\n+            tifffile.memmap(self.path, mode=\"r\")\n+            if self._memmap\n+            else tifffile.imread(self.path)\n+        )\n+\n+        data = self._apply_background_correction(data)\n+        return self._apply_illumination_correction(data)\n+\n+    def load_data_mask(self) -> NDArray:\n+        return np.ones(self.shape, dtype=bool)\n \n \n class RegionAcquisitionSTK(WellAcquisition):\n@@ -18,17 +51,20 @@ def __init__(\n         self,\n         files: pd.DataFrame,\n         alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, NDArray]],\n-        illumination_correction_matrices: Optional[dict[str, NDArray]],\n-        axes: list[str] = [\"c\", \"z\", \"y\", \"x\"],\n+        background_correction_matrices: dict[str, NDArray] | None,\n+        illumination_correction_matrices: dict[str, NDArray] | None,\n+        axes: list[str] | None = None,\n         *,\n         memmap: bool = True,\n     ):\n+        if axes is None:\n+            axes = [\"c\", \"z\", \"y\", \"x\"]\n         path = files.iloc[0][\"path\"]\n         with TiffFile(path) as tif:\n             metadata = tif.stk_metadata\n             if metadata is None:\n-                raise ValueError(f\"STK metadata is missing. Please check \" f\"{path}\")\n+                msg = f\"STK metadata is missing. Please check \" f\"{path}\"\n+                raise ValueError(msg)\n             x_spacing = metadata[\"XCalibration\"]\n             y_spacing = metadata[\"YCalibration\"]\n             self._yx_spacing = (y_spacing, x_spacing)\n@@ -46,7 +82,7 @@ def __init__(\n \n     def _assemble_tiles(self) -> list[Tile]:\n         tiles = []\n-        for i, row in self._files.iterrows():\n+        for _i, row in self._files.iterrows():\n             file = row[\"path\"]\n             time_point = row[\"time\"]\n             channel = row[\"channel\"]\n@@ -68,7 +104,7 @@ def _assemble_tiles(self) -> list[Tile]:\n \n         return tiles\n \n-    def get_z_spacing(self) -> Optional[float]:\n+    def get_z_spacing(self) -> float | None:\n         return self._z_spacing\n \n     def get_yx_spacing(self) -> tuple[float, float]:\n@@ -82,13 +118,16 @@ class RegionAcquisitionOME(WellAcquisition):\n     def __init__(\n         self,\n         files: pd.DataFrame,\n-        ome_xml: Union[Path, str],\n+        ome_xml: Path | str,\n         alignment: TileAlignmentOptions,\n-        background_correction_matrices: Optional[dict[str, NDArray]],\n-        illumination_correction_matrices: Optional[dict[str, NDArray]],\n-        axes: list[str] = [\"c\", \"z\", \"y\", \"x\"],\n+        background_correction_matrices: dict[str, NDArray] | None,\n+        illumination_correction_matrices: dict[str, NDArray] | None,\n+        axes: list[str] | None = None,\n+        *,\n         memmap: bool = True,\n     ):\n+        if axes is None:\n+            axes = [\"c\", \"z\", \"y\", \"x\"]\n         self.metadata = parse_basic_metadata(companion_file=ome_xml)\n         self.stage_positions = self.metadata[\"stage_positions\"]\n         path = files.iloc[0][\"path\"]\n@@ -106,7 +145,7 @@ def __init__(\n \n     def _assemble_tiles(self) -> list[Tile]:\n         tiles = []\n-        for i, row in self._files.iterrows():\n+        for _i, row in self._files.iterrows():\n             file = row[\"path\"]\n             time_point = row[\"time\"]\n             channel = row[\"channel\"]\n@@ -134,7 +173,7 @@ def _assemble_tiles(self) -> list[Tile]:\n \n         return tiles\n \n-    def get_z_spacing(self) -> Optional[float]:\n+    def get_z_spacing(self) -> float | None:\n         return self.metadata[\"z_spacing\"]\n \n     def get_yx_spacing(self) -> tuple[float, float]:\ndiff --git a/src/faim_ipa/visiview/ome_companion_utils.py b/src/faim_ipa/visiview/ome_companion_utils.py\nindex 46caaee2..8fd578b2 100644\n--- a/src/faim_ipa/visiview/ome_companion_utils.py\n+++ b/src/faim_ipa/visiview/ome_companion_utils.py\n@@ -1,18 +1,18 @@\n from decimal import Decimal\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n \n-import lxml\n import numpy as np\n-from lxml import etree\n+from defusedxml import ElementTree\n+from defusedxml.ElementTree import parse\n \n-from faim_ipa.io.ChannelMetadata import ChannelMetadata\n+from faim_ipa.io.metadata import ChannelMetadata\n from faim_ipa.utils import rgb_to_hex, wavelength_to_rgb\n \n SCHEMA = \"{http://www.openmicroscopy.org/Schemas/OME/2016-06}\"\n \n \n-def get_z_spacing(metadata: lxml.etree.ElementTree) -> Optional[float]:\n+def get_z_spacing(metadata: ElementTree) -> float | None:\n     \"\"\"\n     Get the Z spacing from the first image in the XML metadata.\n \n@@ -33,13 +33,12 @@ def get_z_spacing(metadata: lxml.etree.ElementTree) -> Optional[float]:\n \n     if len(z_positions) == 1:\n         return None\n-    else:\n-        z_positions = np.array(sorted(z_positions))\n-        precision = -Decimal(str(z_positions[0])).as_tuple().exponent\n-        return np.round(np.diff(z_positions).mean(), precision)\n+    z_positions = np.array(sorted(z_positions))\n+    precision = -Decimal(str(z_positions[0])).as_tuple().exponent\n+    return np.round(np.diff(z_positions).mean(), precision)\n \n \n-def get_yx_spacing(metadata: lxml.etree.ElementTree) -> tuple[float, float]:\n+def get_yx_spacing(metadata: ElementTree) -> tuple[float, float]:\n     \"\"\"\n     Get the YX spacing from the first image in the XML metadata.\n \n@@ -60,7 +59,7 @@ def get_yx_spacing(metadata: lxml.etree.ElementTree) -> tuple[float, float]:\n     )\n \n \n-def get_exposure_time(metadata: lxml.etree.ElementTree) -> tuple[float, str]:\n+def get_exposure_time(metadata: ElementTree) -> tuple[float, str]:\n     \"\"\"\n     Get the exposure time and unit from the first image in the XML metadata.\n \n@@ -79,7 +78,7 @@ def get_exposure_time(metadata: lxml.etree.ElementTree) -> tuple[float, str]:\n     return plane_0.get(\"ExposureTime\"), plane_0.get(\"ExposureTimeUnit\")\n \n \n-def get_channels(metadata: lxml.etree.ElementTree) -> dict[str, ChannelMetadata]:\n+def get_channels(metadata: ElementTree) -> dict[str, ChannelMetadata]:\n     \"\"\"\n     Get the channel metadata from the XML metadata.\n \n@@ -126,7 +125,7 @@ def get_channels(metadata: lxml.etree.ElementTree) -> dict[str, ChannelMetadata]\n \n \n def get_stage_positions(\n-    metadata: lxml.etree.ElementTree,\n+    metadata: ElementTree,\n ) -> dict[str, tuple[float, float]]:\n     \"\"\"\n     Get the stage positions for each image from the XML metadata.\n@@ -142,8 +141,8 @@ def get_stage_positions(\n     \"\"\"\n     positions = {}\n     for i, image in enumerate(metadata.iterchildren(f\"{SCHEMA}Image\")):\n-        id = image.get(\"ID\")\n-        index = int(id.split(\":\")[-1])\n+        image_id = image.get(\"ID\")\n+        index = int(image_id.split(\":\")[-1])\n         assert index == i, f\"Expected index {i} but got {index}\"\n \n         try:\n@@ -159,7 +158,7 @@ def get_stage_positions(\n     return positions\n \n \n-def parse_basic_metadata(companion_file: Union[Path, str]) -> dict[str, Any]:\n+def parse_basic_metadata(companion_file: Path | str) -> dict[str, Any]:\n     \"\"\"\n     Parse the basic metadata from the XML companion file.\n \n@@ -183,11 +182,10 @@ def parse_basic_metadata(companion_file: Union[Path, str]) -> dict[str, Any]:\n         - stage_positions\n     \"\"\"\n     with open(companion_file, \"rb\") as f:\n-        root = etree.parse(f).getroot()\n-        metadata = dict(\n-            z_spacing=get_z_spacing(root),\n-            yx_spacing=get_yx_spacing(root),\n-            channels=get_channels(root),\n-            stage_positions=get_stage_positions(root),\n-        )\n-    return metadata\n+        root = parse(f).getroot()\n+        return {\n+            \"z_spacing\": get_z_spacing(root),\n+            \"yx_spacing\": get_yx_spacing(root),\n+            \"channels\": get_channels(root),\n+            \"stage_positions\": get_stage_positions(root),\n+        }\ndiff --git a/src/faim_ipa/Zarr.py b/src/faim_ipa/zarr_utils.py\nsimilarity index 85%\nrename from src/faim_ipa/Zarr.py\nrename to src/faim_ipa/zarr_utils.py\nindex 4c799d4f..d00d1807 100644\n--- a/src/faim_ipa/Zarr.py\n+++ b/src/faim_ipa/zarr_utils.py\n@@ -15,6 +15,7 @@ def write_labels_to_group(\n     labels,\n     labels_name,\n     parent_group: Group,\n+    *,\n     storage_options: dict[str, Any],\n     max_layer: int = 0,\n     overwrite: bool = False,\n@@ -28,9 +29,9 @@ def write_labels_to_group(\n     )\n \n     axes = parent_group.attrs.asdict()[\"multiscales\"][0][\"axes\"]\n-    assert len(axes) == len(\n-        labels.shape\n-    ), f\"Group axes don't match label image dimensions: {len(axes)} <> {len(labels.shape)}.\"\n+    if len(axes) == len(labels.shape):\n+        message = f\"Group axes don't match label image dimensions: {len(axes)} <> {len(labels.shape)}.\"\n+        raise ValueError(message)\n \n     write_image(\n         image=labels,\n", "instance_id": "fmi-faim__faim-ipa-165", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to standardize module naming conventions by using lowercase for module files and CamelCase for class names to avoid import-related errors. It provides specific examples of the issue (e.g., import order causing errors) and links to relevant code in the repository, which helps in understanding the context. However, there are minor ambiguities: the statement does not explicitly define the expected output format or provide a comprehensive list of files or modules to be renamed. Additionally, it lacks detailed guidance on handling potential downstream impacts of renaming (e.g., updating dependent code or tests). While the goal is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is significant, as it involves renaming multiple module files across the codebase (e.g., moving from CamelCase to lowercase) and updating import statements in various places, as seen in the extensive diff provided. This requires understanding the structure of the project and interactions between modules, especially in a Python project where naming conventions impact import behavior. Second, it involves technical concepts like Python module naming conventions, import resolution, and potentially handling circular imports or namespace conflicts that could arise from renaming. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential issues like ensuring compatibility with existing code, updating test files, and verifying that no external dependencies or tools rely on the old naming. However, the task does not require deep architectural changes or advanced domain-specific knowledge, nor does it involve complex algorithms or performance optimization, keeping it from being classified as hard. Overall, it requires a moderate level of understanding and effort to implement correctly across multiple files, justifying a score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Separate male and female names for `hi_IN` provider\n* Faker version: `<= 29.0.0`\r\n* OS: all\r\n\r\nThe `hi_IN` locale doesn't have separate male and female names along with prefixes\r\n\r\n### Expected behavior\r\n\r\nThere should be separate male and female names for `hi_IN` locale\r\n\r\n### Actual behavior\r\n\r\nMale and female names are mixed\r\n\r\nAlso mentioned in #2073 \n", "patch": "diff --git a/faker/providers/person/hi_IN/__init__.py b/faker/providers/person/hi_IN/__init__.py\nindex d5aebad4d5..9937fc8242 100644\n--- a/faker/providers/person/hi_IN/__init__.py\n+++ b/faker/providers/person/hi_IN/__init__.py\n@@ -2,239 +2,524 @@\n \n \n class Provider(PersonProvider):\n+\n+    formats_male = (\n+        \"{{first_name_male}} {{last_name}}\",\n+        \"{{prefix_male}} {{first_name_male}} {{last_name}}\",\n+        \"{{first_name_male}} {{last_name}}{{suffix}}\",\n+        \"{{prefix}} {{first_name_male}} {{last_name}}\",\n+    )\n+    formats_female = (\n+        \"{{first_name_female}} {{last_name}}\",\n+        \"{{prefix_female}} {{first_name_female}} {{last_name}}\",\n+        \"{{first_name_female}} {{last_name}}{{suffix}}\",\n+        \"{{prefix}} {{first_name_female}} {{last_name}}\",\n+    )\n+\n     formats = (\n         \"{{first_name}} {{last_name}}\",\n-        \"{{first_name}} {{last_name}}\",\n-        \"{{last_name}}, {{first_name}}\",\n+        \"{{prefix}} {{first_name}} {{last_name}}\",\n+        \"{{first_name}} {{last_name}}{{suffix}}\",\n     )\n \n-    # First 20 names from here\n-    # https://www.babycenter.in/l25020672/top-20-indian-boys-names-of-2016-photos\n-    # Next 20 names from here\n-    # https://www.babycenter.in/l25020674/top-20-indian-girls-names-of-2016-photos\n-    first_names = (\n-        \"\u092e\u0941\u0939\u092e\u094d\u092e\u0926\",\n-        \"\u0906\u0930\u0935\",\n-        \"\u0905\u0930\u094d\u091c\u0941\u0928\",\n-        \"\u0930\u093e\u092f\u0928\",\n-        \"\u0906\u0926\u094d\u0935\u093f\u0915\",\n-        \"\u0905\u0925\u0930\u094d\u0935\",\n-        \"\u0930\u0947\u092f\u093e\u0902\u0936\",\n-        \"\u0905\u092f\u093e\u0928\",\n-        \"\u0935\u093f\u0939\u093e\u0928\",\n-        \"\u0938\u093e\u0908\",\n-        \"\u0905\u0926\u094d\u0935\u0948\u0924\",\n-        \"\u0936\u094c\u0930\u094d\u092f\",\n-        \"\u0935\u093f\u0906\u0928\",\n-        \"\u0906\u0930\u0941\u0937\",\n-        \"\u0907\u0936\u093e\u0928\",\n-        \"\u0905\u092f\u093e\u0902\u0936\",\n-        \"\u092a\u093e\u0930\u094d\u0925\",\n-        \"\u0926\u0947\u0928\u094d\u092f\u0932\",\n-        \"\u0915\u093f\u0906\u0928\",\n-        \"\u0935\u093f\u0935\u093e\u0928\",\n-        \"\u0906\u0926\u094d\u092f\u093e\",\n-        \"\u0905\u0928\u0928\u094d\u092f\u093e\",\n-        \"\u0936\u0928\u093e\u092f\u093e\",\n-        \"\u092b\u093c\u093e\u0924\u093f\u092e\u093e\",\n-        \"\u0936\u094d\u0930\u0940\",\n-        \"\u0905\u0928\u093e\u092f\u093e\",\n-        \"\u0905\u0928\u093f\u0915\u093e\",\n-        \"\u092e\u093e\u092f\u0930\u093e\",\n-        \"\u0907\u0928\u093e\u092f\u093e\",\n-        \"\u0905\u092e\u093e\u092f\u0930\u093e\",\n-        \"\u0906\u0928\u094d\u0935\u0940\",\n-        \"\u0938\u094d\u0935\u0930\u093e\",\n-        \"\u095b\u093e\u0930\u093e\",\n-        \"\u092e\u0930\u093f\u092f\u092e\",\n-        \"\u0906\u0930\u093e\u0927\u094d\u092f\u093e\",\n-        \"\u0924\u0928\u094d\u0935\u0940\",\n-        \"\u0926\u0940\u092f\u093e\",\n-        \"\u0905\u0926\u094d\u0935\u093f\u0915\u093e\",\n-        \"\u0908\u0935\u093e\",\n-        \"\u0906\u0935\u094d\u092f\u093e\",\n+    # http://www.20000-names.com/male_hindi_names.htm\n+    first_names_male = (\n         \"\u0905\u092d\u092f\",\n         \"\u0906\u0926\u093f\u0924\u094d\u092f\",\n         \"\u0905\u091c\u093f\u0924\",\n+        \"\u0906\u0915\u093e\u0919\u094d\u0915\u094d\u0937\u093e\",\n+        \"\u0905\u0915\u092c\u0930\",\n         \"\u0905\u0916\u093f\u0932\",\n         \"\u0905\u092e\u0930\",\n+        \"\u0905\u092e\u093f\u0924\",\n+        \"\u0905\u092e\u0943\u0924\",\n         \"\u0906\u0928\u0928\u094d\u0926\",\n+        \"\u0905\u0928\u0928\u094d\u200d\u0924\",\n+        \"\u0905\u0928\u093f\u0932\",\n+        \"\u0905\u0928\u093f\u0930\u0941\u0926\u094d\u0927\",\n+        \"\u0905\u0928\u093f\u0936\",\n         \"\u0905\u0902\u0915\u0941\u0930\",\n+        \"\u0905\u0928\u0941\u091c\",\n         \"\u0905\u0928\u0941\u092a\u092e\",\n+        \"\u0905\u0930\u0935\u093f\u0928\u094d\u0926\",\n+        \"\u0905\u0930\u094d\u091c\u0941\u0928\",\n+        \"\u0905\u0930\u0941\u0923\",\n+        \"\u0905\u0930\u0941\u0923\u093e\",\n+        \"\u0905\u0938\u0940\u092e\",\n         \"\u0905\u0936\u094b\u0915\",\n+        \"\u092c\u0932\",\n+        \"\u092c\u0932\u0926\u0947\u0935\",\n+        \"\u092c\u0932\u0930\u093e\u092e\",\n+        \"\u092d\u093e\u0930\u0924\",\n+        \"\u092c\u094d\u0930\u0939\u094d\u092e\u093e\",\n+        \"\u092c\u0943\u091c\u0947\u0936\",\n+        \"\u091a\u0923\u094d\u0921\",\n         \"\u091a\u0928\u094d\u0926\u0928\u093e\",\n+        \"\u091a\u0928\u094d\u0926\u094d\u0930\u0915\u093e\u0928\u094d\u0924\",\n+        \"\u0926\u093e\u092e\u094b\u0926\u0930\",\n+        \"\u0926\u0930\u094d\u0936\u0928\",\n+        \"\u0926\u092f\u093e\u0930\u093e\u092e\",\n+        \"\u0926\u0947\u0935\u0926\u093e\u0928\",\n+        \"\u0926\u0940\u092a\u0915\",\n+        \"\u0926\u0947\u0935\u0926\u093e\u0928\",\n+        \"\u0926\u0947\u0935\u0926\u093e\u0938\",\n+        \"\u0926\u0947\u0935\u0930\u093e\u091c\",\n+        \"\u0927\u0928\u091e\u094d\u091c\u092f\",\n+        \"\u0927\u0935\u0932\",\n+        \"\u0926\u093f\u0932\u0940\u092a\",\n+        \"\u0926\u093f\u0928\u0947\u0936\",\n+        \"\u0926\u0940\u092a\u0915\",\n+        \"\u0926\u093f\u0932\u0940\u092a\",\n         \"\u0917\u0923\u0947\u0936\",\n         \"\u0917\u094c\u0924\u092e\",\n+        \"\u0917\u094b\u092a\u093e\u0932\",\n+        \"\u0917\u094b\u0924\u092e\",\n         \"\u0917\u094b\u0935\u093f\u0902\u0926\u093e\",\n+        \"\u0917\u0941\u0932\u091c\u093c\u093e\u0930\",\n         \"\u0939\u0928\u0941\u092e\u093e\u0928\u094d\",\n+        \"\u0939\u0930\u0947\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0939\u0930\u093f\",\n+        \"\u0939\u0930\u0940\u0936\",\n+        \"\u0939\u0930\u094d\u0936\",\n+        \"\u0939\u0930\u094d\u0936\u0926\",\n+        \"\u0939\u0930\u094d\u0936\u0932\",\n+        \"\u0907\u0932\u093e\",\n+        \"\u0907\u0928\u094d\u0926\u094d\u0930\",\n         \"\u0907\u0928\u094d\u0926\u094d\u0930\u091c\u093f\u0924\",\n         \"\u0908\u0936\",\n         \"\u091c\u0917\u0928\u094d\u0928\u093e\u0925\",\n         \"\u091c\u0917\u0926\u0940\u0936\",\n+        \"\u091c\u0917\u091c\u0940\u0924\",\n         \"\u091c\u092f\u0926\u0947\u0935\",\n+        \"\u095b\u0938\u094d\u0935\u093f\u0928\u094d\u0926\u0947\u0930\u094d\",\n+        \"\u091c\u092f\",\n+        \"\u091c\u092f\u0928\u094d\u0924\",\n+        \"\u091c\u092f\u0947\u0928\u094d\u0926\u094d\u0930\",\n         \"\u091c\u093f\u0924\u0947\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u091c\u094c\u0939\u0930\",\n+        \"\u091c\u094d\u092f\u094b\u0924\u093f\u0937\",\n         \"\u0915\u0948\u0932\u093e\u0936\",\n         \"\u0915\u093e\u0932\u093f\u0926\u093e\u0938\",\n+        \"\u0915\u093e\u092e\",\n+        \"\u0915\u092e\u0932\",\n         \"\u0915\u092e\u094d\u092c\u094b\u091c\",\n+        \"\u0915\u092a\u093f\u0932\",\n+        \"\u0915\u0930\u094d\u0923\",\n+        \"\u0916\u093c\u093e\u0928\",\n         \"\u0915\u093f\u0930\u0923\",\n+        \"\u0915\u0936\u094b\u0930\",\n+        \"\u0915\u0943\u0937\u094d\u0923\",\n+        \"\u0915\u0941\u092e\u093e\u0930\",\n+        \"\u0915\u0941\u0923\u093e\u0932\",\n+        \"\u0932\u0915\u094d\u0937\u094d\u092e\u0923\",\n+        \"\u0932\u093e\u0932\",\n         \"\u0932\u0932\u093f\u0924\",\n+        \"\u0932\u094b\u091a\u0928\",\n+        \"\u092e\u093e\u0927\u0935\",\n+        \"\u092e\u0927\u0941\u0915\u0930\",\n+        \"\u092e\u0939\u093e\u0924\u094d\u092e\u093e\",\n+        \"\u092e\u0939\u093e\u0935\u0940\u0930\",\n+        \"\u092e\u0939\u0947\u0928\u094d\u0926\u094d\u0930\u093e\",\n         \"\u092e\u093e\u0928\u0926\u0940\u092a\",\n+        \"\u092e\u0928\u0940\u0936\",\n+        \"\u092e\u0923\u093f\",\n+        \"\u092e\u0923\u0940\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u092e\u0928\u0940\u0936\",\n+        \"\u092e\u091e\u094d\u091c\u0941\u0928\u093e\u0925\",\n         \"\u092e\u094b\u0939\u0928\",\n         \"\u092e\u0941\u0915\u0947\u0936\",\n-        \"\u0928\u0930\u0947\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0928\u0902\u0926\",\n         \"\u0928\u093e\u0930\u093e\u092f\u0923\",\n+        \"\u0928\u0930\u0947\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0928\u0935\u0940\u0928\",\n         \"\u0928\u093f\u0916\u093f\u0932\",\n+        \"\u0928\u0940\u0930\u0935\",\n+        \"\u093f\u0928\u0936\u093e\",\n+        \"\u0913\u092e\",\n+        \"\u092a\u0926\u094d\u092e\",\n+        \"\u092a\u0932\u094d\u0932\u0935\",\n+        \"\u092a\u0940\u0924\u093e\u092e\u094d\u092c\u0930\",\n         \"\u092a\u094d\u0930\u092d\u093e\u0915\u0930\",\n+        \"\u092a\u094d\u0930\u092d\u093e\u0924\",\n+        \"\u092a\u094d\u0930\u092d\u0941\",\n         \"\u092a\u094d\u0930\u092c\u094b\u0927\",\n         \"\u092a\u094d\u0930\u0926\u0940\u092a\",\n+        \"\u092a\u094d\u0930\u0915\u093e\u0936\",\n+        \"\u092a\u094d\u0930\u092e\u094b\u0926\",\n         \"\u092a\u094d\u0930\u0923\u0935\",\n+        \"\u092a\u094d\u0930\u0923\u092f\",\n+        \"\u092a\u094d\u0930\u0938\u093e\u0926\",\n+        \"\u092a\u094d\u0930\u0938\u0928\u094d\u0928\",\n+        \"\u092a\u094d\u0930\u0924\u093e\u092a\",\n         \"\u092a\u094d\u0930\u0947\u092e\",\n+        \"\u092a\u0941\u0930\u0941\u0937\u094b\u0924\u094d\u0924\u092e\",\n+        \"\u0930\u0918\u0941\",\n+        \"\u0930\u093e\u0939\u0941\u0932\",\n+        \"\u0930\u093e\u091c\",\n+        \"\u0930\u093e\u091c\u0928\",\n+        \"\u0930\u091c\u0928\u0940\u0915\u093e\u0902\u0924\",\n         \"\u0930\u093e\u091c\u0940\u0935\",\n+        \"\u0930\u093e\u091c\u0947\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0930\u093e\u091c\u0947\u0936\",\n+        \"\u0930\u093e\u091c\u0940\u0935\",\n+        \"\u0930\u093e\u0915\u0947\u0936\",\n+        \"\u0930\u093e\u092e\",\n+        \"\u0930\u093e\u092e\u091a\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0930\u093e\u092e\u0915\u0943\u0937\u094d\u0923\",\n+        \"\u0930\u091e\u094d\u091c\u093f\u0924\",\n         \"\u0930\u0924\u0928\",\n+        \"\u0930\u0924\u094d\u0928\u092e\",\n+        \"\u0930\u093e\u0935\u0923\",\n+        \"\u0930\u0935\u093f\",\n+        \"\u090b\u0937\u093f\",\n         \"\u0930\u094b\u0939\u0928\",\n-        \"\u0935\u093f\u0937\u094d\u0923\u0941\",\n-        \"\u0935\u093f\u0915\u094d\u0930\u092e\",\n-        \"\u0935\u093f\u091c\u092f\u093e\",\n+        \"\u0938\u091a\u093f\u0928\",\n+        \"\u0938\u0902\u0926\u0940\u092a\",\n+        \"\u0936\u0928\u093f\",\n+        \"\u0938\u0902\u091c\u092f\",\n+        \"\u0938\u0902\u091c\u093f\u0924\",\n+        \"\u0938\u0902\u091c\u0940\u0935\",\n+        \"\u0936\u0902\u0915\u0930\",\n+        \"\u0938\u0930\u0932\",\n+        \"\u0938\u0924\u0940\u0936\",\n+        \"\u0938\u0935\u093f\u0924\u0943\",\n+        \"\u0936\u0947\u0916\u0930\",\n+        \"\u0938\u0947\u0920\",\n+        \"\u0936\u0928\u093f\",\n+        \"\u0936\u0902\u0915\u0930\",\n+        \"\u0936\u0919\u094d\u0915\u0930\",\n+        \"\u0936\u0902\u0924\u0928\u0941\",\n+        \"\u0936\u0930\u094d\u092e\",\n+        \"\u0936\u0936\u093f\",\n+        \"\u0936\u0947\u0916\u0930\",\n+        \"\u0936\u0947\u0937\",\n+        \"\u0936\u093f\u0935\",\n+        \"\u0936\u094d\u0930\u0947\u0937\u094d\u0920\",\n+        \"\u0936\u094d\u0930\u0940\u092a\u0924\u093f\",\n+        \"\u0936\u094d\u092f\u093e\u092e\",\n+        \"\u0936\u094d\u092f\u093e\u092e\u0932\",\n+        \"\u0938\u093f\u0926\u094d\u0927\u093e\u0930\u094d\u0925\",\n+        \"\u0938\u093f\u0915\u0928\u094d\u0926\u0930\",\n+        \"\u0938\u094b\u0939\u0947\u0932\",\n+        \"\u0938\u0941\u092d\u093e\u0937\",\n+        \"\u0938\u0941\u0926\u0930\u094d\u0936\u0928\",\n+        \"\u0938\u0941\u0927\u0940\u0930\",\n+        \"\u0938\u0941\u092e\u0928\",\n+        \"\u0938\u0941\u092e\u0928\u094d\u0924\u094d\u0930\",\n+        \"\u0938\u0941\u0928\u094d\u0926\u0930\",\n+        \"\u0938\u0941\u0928\u0940\u0932\",\n+        \"\u0938\u0941\u0930\u091c\",\n+        \"\u0938\u0941\u0930\u0947\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0938\u0941\u0930\u0947\u0936\",\n+        \"\u0938\u0942\u0930\u094d\u092f\",\n+        \"\u0938\u0941\u0936\u0940\u0932\",\n+        \"\u0938\u094d\u0935\u092a\u0928\",\n+        \"\u0938\u094d\u0935\u092a\u094d\u0928\u093f\u0932\",\n+        \"\u0938\u094d\u0935\u0930\u094d\u0923\",\n+        \"\u0909\u0924\u094d\u0924\u092e\",\n+        \"\u0935\u0938\u0928\u094d\u0924\",\n+        \"\u0935\u093e\u0938\u093f\u0937\u094d\u0920\",\n+        \"\u092d\u0930\u0924\",\n         \"\u0935\u093f\u091c\u092f\",\n+        \"\u0935\u093f\u091c\u092f\u093e\",\n+        \"\u0935\u093f\u0915\u094d\u0930\u092e\",\n+        \"\u0935\u093f\u092e\u0932\",\n+        \"\u0935\u093f\u0928\u092f\",\n+        \"\u0935\u093f\u092a\u093f\u0928\",\n+        \"\u0935\u093f\u092a\u0941\u0932\",\n+        \"\u0935\u093f\u0936\u093e\u0932\",\n+        \"\u0935\u093f\u0937\u094d\u0923\u0941\",\n         \"\u0935\u093f\u0935\u0947\u0915\",\n         \"\u092f\u0936\",\n+    )\n+\n+    # http://www.20000-names.com/female_hindi_names.htm\n+    first_names_female = (\n+        \"\u0906\u092d\u093e\",\n         \"\u0905\u092d\u093f\u0932\u093e\u0937\u093e\",\n         \"\u0905\u0926\u093f\u0924\u0940\",\n         \"\u0910\u0936\u094d\u0935\u0930\u094d\u092f\u093e\",\n+        \"\u0906\u0915\u093e\u0919\u094d\u0915\u094d\u0937\u093e\",\n+        \"\u0905\u092e\u0932\u093e\",\n         \"\u0905\u092e\u093f\u0924\u093e\",\n+        \"\u0905\u092e\u0943\u0924\u093e\",\n+        \"\u0906\u0928\u0928\u094d\u0926\u093e\",\n+        \"\u0905\u0928\u093f\u0932\u093e\",\n+        \"\u0905\u0923\u093f\u092e\u093e\",\n         \"\u0905\u0902\u0915\u093f\u0924\u093e\",\n+        \"\u0905\u0928\u0941\u0937\u094d\u0915\u093e\",\n+        \"\u0905\u0928\u0941\u091c\u093e\",\n+        \"\u0905\u0930\u094d\u091a\u0928\u093e\",\n+        \"\u0905\u0930\u0941\u0902\u0927\u0924\u0940\",\n         \"\u0906\u0936\u093e\",\n         \"\u0905\u0935\u0928\u0940\",\n+        \"\u0905\u0935\u0928\u094d\u0924\u0940\",\n+        \"\u092c\u0932\",\n         \"\u092d\u0930\u0924\",\n+        \"\u091a\u0923\u094d\u0921\u093e\",\n+        \"\u091a\u0928\u094d\u0926\u0928\u093e\",\n+        \"\u091a\u0928\u094d\u0926\u094d\u0930\u0915\u093e\u0928\u094d\u0924\u093e\",\n         \"\u091a\u0947\u0924\u0928\u093e\",\n+        \"\u0926\u092e\u092f\u0902\u0924\u0940\",\n+        \"\u0926\u0930\u094d\u0936\u0928\u093e\",\n+        \"\u0926\u0940\u092a\u093e\u0932\u0940\",\n+        \"\u0926\u0940\u092a\u094d\u0924\u093f\",\n+        \"\u0926\u0947\u0935\u0940\",\n+        \"\u0926\u0940\u092a\u093e\u0932\u0940\",\n+        \"\u0926\u0940\u092a\u094d\u0924\u093f\",\n         \"\u0926\u093f\u0935\u094d\u092f\u093e\",\n+        \"\u0926\u0941\u0930\u094d\u0917\u093e\",\n         \"\u090f\u0937\u093e\",\n+        \"\u0917\u094c\u0939\u0930\",\n+        \"\u0917\u094c\u0930\u0940\",\n+        \"\u0917\u0940\u0924\u093e\",\n+        \"\u0917\u094b\u092a\u0940\u0928\u093e\u0925\",\n+        \"\u0917\u0941\u0932\u091c\u093c\u093e\u0930\",\n+        \"\u0907\u0932\u093e\",\n+        \"\u0907\u0928\u094d\u0926\u093f\u0930\u093e\",\n+        \"\u0907\u0928\u094d\u0926\u094d\u0930\u091c\u093f\u0924\",\n         \"\u0907\u0928\u094d\u0926\u0941\",\n+        \"\u095b\u0938\u094d\u0935\u093f\u0928\u094d\u0926\u0947\u0930\u094d\",\n         \"\u091c\u092f\u093e\",\n         \"\u091c\u092f\u0928\u094d\u0924\u0940\",\n         \"\u091c\u094d\u092f\u094b\u0924\u094d\u0938\u0928\u093e\",\n+        \"\u091c\u094d\u092f\u094b\u0924\u094d\u0938\u094d\u0928\u093e\",\n+        \"\u0915\u0948\u0932\u093e\u0936\",\n+        \"\u0915\u0932\u093e\",\n+        \"\u0915\u093e\u0932\u0940\",\n+        \"\u0915\u0932\u094d\u092a\u0928\u093e\",\n+        \"\u0915\u092e\u0932\u093e\",\n+        \"\u0915\u093e\u0928\u094d\u0924\u093e\",\n         \"\u0915\u093e\u0928\u094d\u0924\u0940\",\n+        \"\u0915\u0930\u093f\u0936\u094d\u092e\u093e\",\n+        \"\u0915\u093e\u0936\u0940\",\n+        \"\u0915\u094c\u0936\u0932\u094d\u092f\u093e\",\n+        \"\u093f\u0915\u0936\u094b\u0930\u0940\",\n+        \"\u0915\u094d\u0937\u093f\u0924\u093f\u091c\",\n         \"\u0915\u0941\u092e\u093e\u0930\u0940\",\n+        \"\u0915\u0941\u0902\u0924\u0940\",\n+        \"\u0932\u0915\u094d\u0937\u094d\u092e\u0940\",\n         \"\u0932\u0924\u093e\",\n+        \"\u0932\u093e\u0935\u0923\u094d\u092f\u093e\",\n+        \"\u0932\u0915\u094d\u0937\u094d\u092e\u0940\",\n+        \"\u0932\u0940\u0932\u093e\",\n+        \"\u0932\u0940\u0932\u093e\u0935\u0924\u0940\",\n+        \"\u0932\u0940\u0932\u093e\",\n         \"\u0932\u0940\u0932\u093e\",\n+        \"\u0932\u0940\u0932\u093e\u0935\u0924\u0940\",\n+        \"\u0932\u0940\u0928\u093e\",\n+        \"\u092e\u093e\u0927\u0935\u0940\",\n+        \"\u092e\u0927\u0941\",\n+        \"\u092e\u0927\u0941\u0930\",\n+        \"\u092e\u093e\u0932\u093e\",\n         \"\u092e\u093e\u0932\u0924\u0940\",\n+        \"\u092e\u0928\u0940\u0937\u093e\",\n+        \"\u092e\u091e\u094d\u091c\u0941\",\n+        \"\u092e\u091e\u094d\u091c\u0941\u0932\u093e\",\n+        \"\u092e\u091e\u094d\u091c\u0942\u0937\u093e\",\n+        \"\u092e\u093e\u092f\u093e\",\n+        \"\u092e\u0940\u0930\u093e\",\n+        \"\u092e\u094b\u0939\u0928\u093e\",\n         \"\u092e\u094b\u0939\u093f\u0928\u0940\",\n+        \"\u092e\u0941\u0915\u094d\u0924\u093e\",\n+        \"\u0928\u0947\u0939\u093e\",\n+        \"\u0928\u093f\u0916\u093f\u0932\u093e\",\n         \"\u0928\u093f\u0936\u093e\",\n+        \"\u0928\u093f\u0924\u094d\u092f\",\n+        \"\u092a\u0926\u094d\u092e\",\n+        \"\u092a\u0926\u094d\u092e\u093e\u0935\u0924\u0940\",\n+        \"\u092a\u0926\u094d\u092e\u093f\u0928\u0940\",\n+        \"\u092a\u093e\u0930\u094d\u0935\u0924\u0940\",\n+        \"\u092a\u0930\u0935\u0940\u0928\",\n+        \"\u092a\u0942\u0930\u094d\u0923\u093f\u092e\u093e\",\n+        \"\u092a\u094d\u0930\u0924\u093f\u092d\u093e\",\n+        \"\u092a\u094d\u0930\u0924\u093f\u092e\u093e\",\n+        \"\u092a\u094d\u0930\u0947\u092e\u093e\",\n+        \"\u092a\u094d\u0930\u093f\u092f\u093e\",\n         \"\u092a\u0942\u0930\u094d\u0923\u093f\u092e\u093e\",\n         \"\u092a\u0941\u0937\u094d\u092a\u093e\",\n         \"\u0930\u091a\u0928\u093e\",\n+        \"\u0930\u093e\u0927\u093e\",\n         \"\u0930\u091c\u0928\u0940\",\n+        \"\u0930\u093e\u091c\u094d\u092f\",\n+        \"\u0930\u093e\u0928\u0940\",\n         \"\u0930\u0936\u094d\u092e\u0940\",\n+        \"\u0930\u0924\u093f\",\n+        \"\u0930\u0924\u094d\u0928\",\n+        \"\u0930\u0947\u0936\u092e\u0940\",\n+        \"\u0930\u0940\u0924\u093f\u0915\u093e\",\n         \"\u0930\u093f\u092f\u093e\",\n+        \"\u0930\u094b\u0939\u0928\u093e\",\n+        \"\u0930\u0941\u0915\u094d\u092e\u093f\u0923\u0940\",\n+        \"\u0930\u0941\u092a\u093f\u0928\u094d\u0926\u094d\u0930\",\n+        \"\u0938\u0902\u091c\u0928\u093e\",\n         \"\u0938\u0930\u0932\u093e\",\n         \"\u0938\u0930\u0938\u094d\u0935\u0924\u0940\",\n+        \"\u0938\u093e\u0930\u093f\u0915\u093e\",\n+        \"\u0938\u0924\u0940\",\n         \"\u0938\u093e\u0935\u093f\u0924\u094d\u0930\u0940\",\n+        \"\u0938\u0940\u092e\u093e\",\n+        \"\u0938\u0940\u0924\u093e\",\n         \"\u0936\u0915\u094d\u0924\u093f\",\n+        \"\u0936\u0915\u0941\u0928\u094d\u0924\u0932\u093e\",\n         \"\u0936\u093e\u0928\u094d\u0924\u093e\",\n+        \"\u0936\u093e\u0928\u094d\u0924\u0940\",\n         \"\u0936\u0930\u094d\u092e\u093f\u0932\u093e\",\n+        \"\u0936\u0936\u0940\",\n+        \"\u0936\u0940\u0932\u093e\",\n+        \"\u0936\u093f\u0935\u093e\u0932\u0940\",\n+        \"\u0936\u094b\u092d\u093e\",\n         \"\u0936\u094d\u092f\u093e\u092e\u093e\",\n+        \"\u0936\u094d\u092f\u093e\u092e\u0932\u093e\",\n+        \"\u0938\u0940\u092e\u093e\",\n+        \"\u0938\u0940\u0924\u093e\",\n+        \"\u0938\u093f\u0924\u093e\u0930\u093e\",\n+        \"\u0938\u094b\u0928\u0932\",\n+        \"\u0936\u094d\u0930\u0940\",\n+        \"\u0938\u0941\u0926\u0930\u094d\u0936\u0928\u093e\",\n         \"\u0938\u0941\u0932\u092d\u093e\",\n+        \"\u0938\u0941\u092e\u0928\u093e\",\n+        \"\u0938\u0941\u092e\u0924\u0940\",\n+        \"\u0938\u0941\u0928\u0940\u0924\u093e\",\n+        \"\u0938\u0941\u0928\u0940\u0924\u0940\",\n+        \"\u0938\u0941\u0936\u0940\u0932\u093e\",\n+        \"\u0938\u094d\u0935\u0930\u094d\u0923\",\n+        \"\u0924\u093e\u0930\u093e\",\n         \"\u0924\u0943\u0937\u094d\u0923\u093e\",\n+        \"\u0909\u092e\u093e\",\n+        \"\u0909\u0937\u093e\",\n+        \"\u0935\u0938\u0928\u094d\u0924\u093e\",\n         \"\u0935\u093f\u0926\u094d\u092f\u093e\",\n-        \"\u0905\u0932\u0940\",\n-        \"\u0939\u093e\u0938\u0928\",\n-        \"\u0939\u0941\u0938\u0948\u0928\",\n-        \"\u091c\u093c\u093e\u0915\u093f\u0930\",\n-        \"\u0930\u093f\u091c\u093c\u0935\u093e\u0928\",\n-        \"\u092b\u093c\u0930\u0939\u093e\u0928\",\n-        \"\u091c\u093c\u094b\u092f\u093e\",\n+        \"\u0935\u093f\u091c\u092f\u093e\",\n+        \"\u0935\u093f\u092e\u0932\u093e\",\n     )\n \n+    first_names = first_names_male + first_names_female\n+\n+    # https://blogs.transparent.com/hindi/common-surnames-in-india/\n     last_names = (\n-        \"\u092a\u093e\u091f\u093f\u0932\",\n+        # Common Surnames in North India (Delhi, Haryana, Punjab,etc)\n         \"\u0936\u0930\u094d\u092e\u093e\",\n-        \"\u0906\u091a\u093e\u0930\u094d\u092f\",\n-        \"\u0905\u0917\u094d\u0930\u0935\u093e\u0932\",\n+        \"\u092d\u091f\",\n+        \"\u0935\u0930\u094d\u092e\u093e\",\n+        \"\u0915\u0941\u092e\u093e\u0930\",\n+        \"\u0917\u0941\u092a\u094d\u0924\u093e\",\n+        \"\u092e\u0932\u094d\u0939\u094b\u0924\u094d\u0930\u093e\",\n+        \"\u092d\u091f\u0928\u093e\u0917\u0930\",\n+        \"\u0938\u0915\u094d\u0938\u0947\u0928\u093e\",\n+        \"\u0915\u092a\u0942\u0930\",\n         \"\u0938\u093f\u0902\u0939\",\n-        \"\u0905\u0939\u0932\u0941\u0935\u093e\u0932\u093f\u092f\u093e\",\n+        \"\u092e\u0939\u0930\u093e\",\n+        \"\u091a\u094b\u092a\u0930\u093e\",\n+        \"\u0938\u0930\u0940\u0928\",\n+        \"\u092e\u093e\u0932\u093f\u0915\",\n+        \"\u0938\u0948\u0928\u0940\",\n+        \"\u091c\u0948\u0928\",\n+        \"\u0915\u094c\u0932\",\n+        \"\u0916\u0924\u094d\u0930\u0940\",\n+        \"\u0917\u094b\u092f\u0932\",\n+        \"\u0924\u093f\u0935\u093e\u0930\u0940\",\n+        \"\u092d\u0930\u0926\u094d\u0935\u093e\u091c\",\n+        \"\u091a\u094b\u092a\u0930\u093e\",\n+        \"\u092a\u094d\u0930\u0938\u093e\u0926\",\n+        \"\u0906\u091a\u093e\u0930\u094d\u092f\",\n+        \"\u0905\u0917\u0930\u0935\u093e\u0932\",\n+        \"\u0905\u0939\u0932\u0942\u0935\u093e\u0932\u093f\u092f\u093e\",\n+        \"\u091f\u0902\u0921\u0928\",\n         \"\u0906\u0939\u0942\u091c\u093e\",\n-        \"\u092a\u0941\u0937\u094d\u0915\u0930\",\n-        \"\u0936\u093f\u0930\u094b\u0933\u0947\",\n-        \"\u0917\u093e\u092f\u0915\u0935\u093e\u0921\",\n-        \"\u0917\u093e\u0935\u093f\u0924\",\n-        \"\u0936\u093f\u0930\u094b\u0933\u0947\",\n-        \"\u092c\u093e\u092a\u091f\",\n-        \"\u0905\u0930\u094b\u095c\u093e\",\n-        \"\u092c\u093e\u092c\u0942\",\n-        \"\u092c\u093e\u0926\u093e\u092e\u0940\",\n-        \"\u091c\u092e\u093e\u0928\u0924\",\n-        \"\u092c\u091c\u093e\u091c\",\n-        \"\u092c\u0915\u094d\u0937\u0940\",\n-        \"\u092c\u093e\u0932\u0915\u0943\u0937\u094d\u0923\u0928\",\n-        \"\u092c\u093e\u0932\u093e\u0938\u0941\u092c\u094d\u0930\u092e\u0923\u093f\u092f\u092e\",\n-        \"\u092c\u0938\u0941\",\n-        \"\u092d\u0902\u0921\u093e\u0930\u0940\",\n-        \"\u091a\u094c\u0927\u0930\u0940\",\n-        \"\u091a\u094c\u0939\u093e\u0928\",\n-        \"\u091b\u093e\u092c\u0930\u093e\",\n-        \"\u0926\u093e\u0926\u093e\",\n-        \"\u0921\u093e\u0928\u0940\",\n-        \"\u0921\u093e\u0930\",\n-        \"\u0926\u093e\u0930\u093e\",\n-        \"\u0926\u0924\u094d\u0924\u093e\",\n-        \"\u0926\u0935\u0947\",\n-        \"\u0926\u092f\u093e\u0932\",\n-        \"\u0927\u093e\u0932\u0940\u0935\u093e\u0932\",\n-        \"\u0926\u0940\u0915\u094d\u0937\u093f\u0924\",\n-        \"\u0926\u094b\u0937\u0940\",\n-        \"\u0926\u0941\u0906\",\n-        \"\u0926\u0942\u092c\u0947\",\n-        \"\u0922\u0940\u0902\u0917\u0930\u093e\",\n-        \"\u0935\u093e\u0932\",\n-        \"\u0938\u093e\u092f\u093e\",\n-        \"\u092c\u0928\u093e\",\n-        \"\u095c\u093e\u0932\",\n-        \"\u0917\u0930\u094d\u0917\",\n-        \"\u0917\u0923\u0947\u0936\",\n-        \"\u0917\u093e\u0902\u0917\u0941\u0932\u0940\",\n-        \"\u0917\u0941\u092a\u094d\u0924\u093e\",\n-        \"\u0939\u0947\u0917\u0921\u0947\",\n-        \"\u091c\u094b\u0936\u0940\",\n-        \"\u0915\u093e\u0932\u0947\",\n-        \"\u0915\u0943\u0937\u094d\u0923\u093e\",\n-        \"\u0915\u0943\u0937\u094d\u0923\u092e\u0942\u0930\u094d\u0924\u093f\",\n-        \"\u0915\u0943\u0937\u094d\u0923\u0928\",\n-        \"\u0915\u0941\u0932\u0915\u0930\u094d\u0923\u0940\",\n-        \"\u0915\u0941\u092e\u093e\u0930\",\n-        \"\u0915\u0941\u0923\u094d\u0921\u093e\",\n-        \"\u0928\u093e\u092e\",\n-        \"\u0930\u093e\u092e\u0932\u0932\u093e\",\n-        \"\u0932\u0924\u093e\",\n-        \"\u0932\u094b\u0926\u0940\",\n-        \"\u0932\u094b\u0915\u0928\u093e\u091f\u094d\u092f\u094b\u0902\",\n-        \"\u0935\u093f\u0915\u093e\u0935\u093f\",\n-        \"\u0932\u093e\u0932\",\n-        \"\u0932\u093e\u0932\u093e\",\n-        \"\u0935\u092b\u093e\u0926\u093e\u0930\",\n-        \"\u0932\u0942\u0925\u0930\u093e\",\n-        \"\u092e\u0926\u0928\",\n-        \"\u092e\u0917\u0930\",\n-        \"\u092d\u093e\u0930\u0924\",\n-        \"\u092e\u0939\u093e\u0935\u0940\u0930\",\n-        \"\u092e\u0939\u093e\u0926\u0947\u0935\",\n-        \"\u092e\u0939\u093e\u091c\u0928\",\n-        \"\u092e\u0939\u093e\u0930\u093e\u091c\",\n+        \"\u0905\u0930\u094b\u0930\u093e\",\n+        # Common Surnames in East India: (Bengal, Orrisa, etc.)\n+        \"\u091a\u091f\u0930\u094d\u091c\u0940\",\n+        \"\u091a\u0924\u0941\u0930\u094d\u0935\u0947\u0926\u0940\",\n+        \"\u0938\u0947\u0928\",\n+        \"\u092c\u094b\u0938\",\n+        \"\u0938\u0947\u0928\u0917\u0941\u092a\u094d\u0924\u093e\",\n+        \"\u0926\u093e\u0938\",\n+        \"\u0926\u093e\u0938\u0917\u0941\u092a\u094d\u0924\u093e\",\n+        \"\u092e\u0941\u0959\u0930\u094d\u091c\u0940\",\n+        \"\u0926\u0941\u0924\u094d\u0924\u093e\",\n+        \"\u092c\u0928\u0930\u094d\u091c\u0940\",\n+        \"\u091a\u0915\u094d\u0930\u0935\u0930\u094d\u0924\u0940\",\n+        \"\u092d\u091f\u094d\u091f\u093e\u091a\u093e\u0930\u094d\u092f\",\n+        \"\u0918\u094b\u0937\",\n+        \"\u092e\u093f\u0924\u094d\u0930\u093e\",\n+        \"\u0917\u0941\u0939\u093e\",\n+        \"\u0938\u0930\u0915\u093e\u0930\",\n+        \"\u0938\u093e\u0939\u093e\",\n+        \"\u0930\u0949\u092f\",\n+        \"\u091a\u094b\u0927\u0930\u0940\",\n+        \"\u0930\u0949\u092f \u091a\u094c\u0927\u0930\u0940\",\n         \"\u092e\u091c\u0942\u092e\u0926\u093e\u0930\",\n-        \"\u092e\u0932\u094d\u0932\u093f\u0915\",\n-        \"\u0938\u0947\u0928\u093e\u0927\u0940\u0936\",\n-        \"\u092e\u093e\u0928\u0947\",\n-        \"\u092e\u0902\u0917\u0932\",\n-        \"\u092e\u0902\u0917\u0924\",\n-        \"\u0930\u093e\u092e\u0936\u0930\u094d\u092e\u093e\",\n-        \"\u092e\u0923\u093f\",\n-        \"\u092e\u093e\u0928\",\n-        \"\u0936\u094d\u0930\u0940\u0935\u093f\u092e\u0932\",\n-        \"\u0915\u0941\u092e\u093e\u0930\",\n         \"\u092e\u0902\u0921\u0932\",\n-        \"\u0905\u0932\u0940\",\n-        \"\u0939\u093e\u0938\u0928\",\n-        \"\u0939\u0941\u0938\u0948\u0928\",\n+        \"\u092e\u0948\u0924\u0940\",\n+        \"\u0915\u0932\u093f\u0924\u093e\",\n+        \"\u0939\u091c\u093e\u0930\u093f\u0915\u093e\",\n+        \"\u0928\u093e\u0925\",\n+        \"\u092c\u0941\u0930\u0941\u093e\u0939\",\n+        \"\u0925\u093e\u092a\u093e\",\n+        \"\u0917\u0941\u0930\u0941\u0902\u0917\",\n+        \"\u0930\u093e\u092f\",\n+        \"\u092a\u094d\u0930\u0927\u093e\u0928\",\n+        \"\u0924\u092e\u093e\u0902\u0917\",\n+        \"\u091b\u0947\u0924\u094d\u0930\u0940\",\n+        # Common Surnames in South India (Karnataka, Tamil Nadu, Kerala, etc.)\n+        \"\u0928\u093e\u092f\u0930\",\n+        \"\u092e\u0947\u0928\u0928\",\n+        \"\u092a\u093f\u0932\u094d\u0932\u0908\",\n+        \"\u0935\u0947\u0902\u0915\u091f\u090f\u0938\u0928\",\n+        \"\u092c\u0932\u093e\u0938\u0941\u092c\u094d\u0930\u092e\u093e\u0928\u093f\u092f\u092e\",\n+        \"\u0930\u093e\u0935\",\n+        \"\u091c\u092f\u0930\u093e\u092e\u0928\",\n+        \"\u0938\u0941\u092c\u094d\u0930\u092e\u0923\u094d\u092f\u092e\",\n+        \"\u0930\u0902\u0917\u0928\",\n+        \"\u0930\u0902\u0917\u0930\u093e\u091c\u0928\",\n+        \"\u0928\u093e\u0930\u093e\u092f\u0923\",\n+        \"\u0930\u0947\u0921\u094d\u0921\u0940\",\n+        # Common Surnames in Central India (Bihar/ Uttar Pradesh, Madhya Pradesh, etc)\n+        \"\u0938\u093f\u0902\u0939\",\n+        \"\u0926\u094d\u0935\u093f\u0935\u0947\u0926\u0940\",\n+        \"\u092e\u093f\u0936\u094d\u0930\u093e\",\n+        \"\u0924\u094d\u0930\u093f\u0935\u0947\u0926\u0940\",\n+        \"\u091d\u093e\",\n+        \"\u0936\u0941\u0915\u094d\u0932\u093e\",\n+        \"\u092f\u093e\u0926\u0935\",\n+        \"\u0938\u093f\u0928\u094d\u0939\u093e\",\n+        \"\u092a\u093e\u0923\u094d\u0921\u0947\u092f\",\n+        \"\u091d\u093e\u0926\u0935\",\n+        \"\u091c\u0947\u091f\u0932\u0940\",\n+        \"\u091a\u094c\u0939\u093e\u0928\",\n+        \"\u091c\u094b\u0936\u0940\",\n+        \"\u092e\u093f\u0938\u094d\u0924\u094d\u0930\u0940\",\n         \"\u0916\u093e\u0928\",\n-        \"\u0905\u092c\u094d\u092c\u093e\u0938\u0940\",\n-        \"\u0928\u0942\u0930\u093e\u0928\u0940\",\n+        \"\u0936\u094d\u0930\u0940\u0935\u093e\u0938\u094d\u0924\u0935\",\n+        # Common Surnames in West India (Maharashtra, Gujarat, Goa etc)\n+        \"\u0936\u093e\u0939\",\n+        \"\u0926\u0947\u0936\u092a\u093e\u0902\u0921\u0947\",\n+        \"\u0917\u093e\u0935\u0921\u0947\",\n+        \"\u0915\u0926\u092e\",\n+        \"\u0924\u093e\u092e\u094d\u092c\u0947\",\n+        \"\u092e\u0947\u0939\u0924\u093e\",\n+        \"\u092a\u091f\u0947\u0932\",\n+        \"\u092a\u093e\u091f\u093f\u0932\",\n+        \"\u092a\u0935\u093e\u0930\",\n+        \"\u091a\u0935\u0928\",\n+        \"\u0921\u0940\u2019\u0938\u094b\u0909\u091c\u093c\u093e\",\n+        \"\u0932\u094b\u092c\u094b\",\n+        \"\u0930\u094b\u0926\u094d\u0930\u093f\u0917\u0941\u090f\u0938\",\n+        \"\u0921\u0940\u2019\u0915\u094b\u0938\u094d\u091f\u093e\",\n     )\n+\n+    prefixes_male = (\"\u0936\u094d\u0930\u0940\", \"\u0936\u094d\u0930\u0940\u092e\u093e\u0928\")\n+\n+    prefixes_female = (\"\u0936\u094d\u0930\u0940\", \"\u0936\u094d\u0930\u0940\u092e\u0924\u0940\")\n+\n+    prefixes = (\n+        \"\u092e\u093e\u0928\u0928\u0940\u092f\",\n+        \"\u0906\u0926\u0930\u0938\u0942\u091a\u0915\",\n+        \"\u0938\u092e\u094d\u092e\u093e\u0928\u0938\u0942\u091a\u0915\",\n+        \"\u0938\u0902\u092e\u093e\u0928\u093f\u0924\",\n+        \"\u0906\u0926\u0930\u0935\u093e\u091a\u0915\",\n+        \"\u0938\u092e\u094d\u092e\u093e\u0928\u093e\u0924\u094d\u092e\u0915\",\n+    )\n+\n+    suffixes = (\"\u091c\u0940\",)\n", "instance_id": "joke2k__faker-2109", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: it aims to separate male and female names for the `hi_IN` locale in the Faker library, addressing an issue where names are currently mixed. The expected behavior (separate male and female names) and actual behavior (mixed names) are explicitly stated, which provides a clear goal. However, there are minor ambiguities and missing details. For instance, the problem does not specify how the separation should be implemented (e.g., should there be separate methods or data structures for male and female names?), nor does it mention any constraints or edge cases (e.g., handling of unisex names or cultural nuances). Additionally, there are no examples provided to illustrate the desired output format. Despite these minor gaps, the intent and scope are understandable, especially when paired with the provided code changes, which clarify the implementation approach. Thus, a score of 2 (Mostly Clear) is appropriate.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are confined to a single file (`faker/providers/person/hi_IN/__init__.py`) and involve straightforward modifications to data structures. The diff shows the addition of separate lists for male and female first names, along with corresponding format templates for generating names. There is no impact on the broader codebase architecture or interactions with other modules. The amount of code change is moderate, primarily involving the addition of new lists and minor updates to existing format strings, but it does not require complex logic or refactoring.\n\n2. **Number of Technical Concepts:** The problem requires basic knowledge of Python and familiarity with how the Faker library structures its providers. No advanced language features, algorithms, or design patterns are needed. The task is essentially data organization\u2014splitting a mixed list of names into gender-specific lists and updating format templates accordingly. There is no need for domain-specific knowledge beyond a basic understanding of naming conventions in the `hi_IN` locale, which is already provided in the data sources referenced in the code comments.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention any specific edge cases or error conditions, such as handling unisex names or ensuring cultural accuracy. The code changes also do not introduce new error handling logic. While there might be implicit edge cases (e.g., ensuring the lists are balanced or handling incorrect data), these are not complex and do not significantly increase the difficulty.\n\n4. **Overall Complexity:** The task is a simple feature addition that involves minimal logic beyond updating static data and format strings. It does not require deep understanding of the Faker library's internals, as the changes follow the existing pattern of other locale providers. The primary effort lies in curating the name lists, which is more about data collection than technical complexity.\n\nGiven these factors, a difficulty score of 0.25 reflects an \"Easy\" problem that requires understanding some code logic (the structure of the Faker provider) and making simple modifications (adding and organizing data). It is not as trivial as fixing a typo (0.0-0.2), but it is far from requiring complex logic or architectural changes (0.4 and above).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Contains comparison doesn't work as expected for PostgreSQL ArrayField\nDefining a model with ArrayField supports contains filtering but should work differently from the string contains comparison. The comparison should support both fields\r\n```python\r\ndef test_mock_set_contains():\r\n    from django.contrib.postgres.fields import ArrayField\r\n    from django.db.models import IntegerField, Model\r\n    from django_mock_queries.query import MockSet\r\n\r\n    class TestModel(Model):\r\n        x = ArrayField(IntegerField())\r\n\r\n        class Meta:\r\n            app_label = \"prompt\"\r\n\r\n    set = MockSet(model=TestModel)\r\n    assert len(set.filter(x__contains=[1])) == 0\r\n    set.create(x=[1, 2, 3])\r\n    assert len(set.filter(x__contains=[1])) == 1\r\n```\n", "patch": "diff --git a/django_mock_queries/comparisons.py b/django_mock_queries/comparisons.py\nindex dbccfc3..852bbfd 100644\n--- a/django_mock_queries/comparisons.py\n+++ b/django_mock_queries/comparisons.py\n@@ -10,6 +10,9 @@ def iexact_comparison(first, second):\n \n \n def contains_comparison(first, second):\n+    if isinstance(first, (list, tuple)):\n+        return set(second).issubset(first)\n+\n     return second in first\n \n \n", "instance_id": "stphivos__django-mock-queries-172", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `contains` comparison for PostgreSQL `ArrayField` in a Django mock query setup does not behave as expected and needs to be adjusted to handle array-specific logic differently from string contains comparison. The goal (fixing the comparison behavior for arrays) and the provided test code give a reasonable understanding of the expected outcome. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the `contains` comparison should work for arrays (e.g., whether it should check for subset or exact match) beyond the test case. Additionally, edge cases (e.g., empty arrays, nested arrays, or non-list inputs) are not mentioned, which could lead to incomplete solutions if not considered. Overall, while the intent is clear, the lack of comprehensive constraints and edge case specifications prevents it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a minimal change in a single file (`comparisons.py`), specifically in the `contains_comparison` function. The modification involves adding a conditional check for lists or tuples and using `set().issubset()` to handle array containment. This is a localized change with no apparent impact on the broader codebase architecture or interactions between modules. The amount of code change is very small (just a few lines).\n\n2. **Technical Concepts Involved**: Solving this requires basic knowledge of Python (e.g., type checking with `isinstance`, set operations with `issubset`), as well as a rudimentary understanding of Django's `ArrayField` behavior in the context of mock queries. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic Django testing utilities are needed. The concepts are straightforward for anyone with intermediate Python experience.\n\n3. **Edge Cases and Error Handling**: The problem statement and test case do not explicitly mention edge cases like empty arrays, non-list inputs, or invalid data types. However, the code change itself does not address these either, and the test case only covers a simple positive scenario. While a robust solution might consider additional error handling, the current scope of the change does not demand it, keeping the difficulty low. Identifying and handling potential edge cases would slightly increase complexity but is not required based on the provided context.\n\n4. **Overall Complexity**: The problem requires understanding a specific bug in mock query filtering for `ArrayField` and implementing a simple logic change to fix it. There are no performance considerations, architectural impacts, or complex debugging involved. The solution is a straightforward adjustment to an existing function.\n\nGiven these points, a difficulty score of 0.30 reflects an easy problem that requires minimal code modification and basic understanding of Python and Django mock queries. It is slightly above the \"very easy\" range due to the need to understand the context of Django's `ArrayField` and mock query behavior, but it remains a simple fix overall.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "read_sql_query should support tuples as `params` dict values\n**Describe the bug**\r\nThe type for [`pandas.read_sql_query`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html) is specified by `pandas-stubs` as a dictionary mapping from `str` to `str | bytes | date | timedelta | datetime64 | timedelta64 | int | float | complex`. But for some database drivers like `psycopg2` or `mysql-connector`, it's also possible to pass in tuples e.g. as the right-hand side of `IN` conditions (`WHERE ... IN (1, 2, 3)`).\r\n\r\n**To Reproduce**\r\nA minimal example using Postgres (it looks similar for MySQL except the parameter is written as `:ids` in the query):\r\n\r\n1. Launch a Postgres Docker container: `docker run -p 9876:5432 -e POSTGRES_HOST_AUTH_METHOD=trust postgres`\r\n2. Install dependencies and run this script to see that tuple substitutions for `IN` conditions work fine (I included [PEP 723](https://peps.python.org/pep-0723/) script metadata so you can just do `uv run` or `pipx run` on it to avoid needing to install dependencies manually):\r\n\r\n```python\r\n# To make this executable directly w/ pipx run or uv run:\r\n# /// script\r\n# dependencies = [\r\n#   \"psycopg2==2.9.9\",\r\n#   \"pandas==2.2.2\"\r\n# ]\r\n# ///\r\n\r\nimport pandas as pd\r\nimport psycopg2\r\n\r\nconn = psycopg2.connect(\r\n  host=\"localhost\", dbname=\"postgres\", user=\"postgres\", port=9876\r\n)\r\ncur = conn.cursor()\r\ncur.execute(\"CREATE TABLE test (id serial PRIMARY KEY)\")\r\ncur.execute(\"INSERT INTO test(id) VALUES(1)\")\r\ncur.execute(\"INSERT INTO test(id) VALUES(2)\")\r\ncur.execute(\"INSERT INTO test(id) VALUES(3)\")\r\n\r\ndf = pd.read_sql_query(\r\n  \"SELECT * FROM test WHERE id IN %(ids)s\",\r\n  conn,\r\n  params={\"ids\": (1, 2)},\r\n)\r\nprint(df)\r\n\r\n```\r\nOutput is as expected:\r\n```\r\n   id\r\n0   1\r\n1   2\r\n```\r\n\r\n3. Run Mypy on it and get this error message:\r\n```\r\nbug.py:24: error: Dict entry 0 has incompatible type \"str\": \"list[int]\"; expected \"str\": \"str | bytes | date | timedelta | datetime64 | timedelta64 | int | float | complex\"  [dict-item]\r\n```\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: Linux\r\n - OS Version: Ubuntu 22.04\r\n - python version: 3.10\r\n - version of type checker: Mypy 1.11.2\r\n - version of installed `pandas-stubs`: 2.2.2.240909\r\n\r\n\r\n**Additional context**\r\nSomething like this doesn't work for `sqlite3` connections, because it really does only accept \"primitive\" values for substitutions. \r\n\r\nBut IMO it doesn't make sense to restrict the set of allowed types to the universally supported subset, because some other weird database could decide to not even support `int` substitutions or something like that and then even the current set wouldn't be valid.\r\n\n", "patch": "diff --git a/pandas-stubs/io/sql.pyi b/pandas-stubs/io/sql.pyi\nindex 7157b70e..d53f9b05 100644\n--- a/pandas-stubs/io/sql.pyi\n+++ b/pandas-stubs/io/sql.pyi\n@@ -66,7 +66,14 @@ def read_sql_query(\n     con: _SQLConnection,\n     index_col: str | list[str] | None = ...,\n     coerce_float: bool = ...,\n-    params: list[Scalar] | tuple[Scalar, ...] | Mapping[str, Scalar] | None = ...,\n+    params: (\n+        list[Scalar]\n+        | tuple[Scalar, ...]\n+        | tuple[tuple[Scalar, ...], ...]\n+        | Mapping[str, Scalar]\n+        | Mapping[str, tuple[Scalar, ...]]\n+        | None\n+    ) = ...,\n     parse_dates: list[str] | dict[str, str] | dict[str, dict[str, Any]] | None = ...,\n     *,\n     chunksize: int,\n@@ -79,7 +86,14 @@ def read_sql_query(\n     con: _SQLConnection,\n     index_col: str | list[str] | None = ...,\n     coerce_float: bool = ...,\n-    params: list[Scalar] | tuple[Scalar, ...] | Mapping[str, Scalar] | None = ...,\n+    params: (\n+        list[Scalar]\n+        | tuple[Scalar, ...]\n+        | tuple[tuple[Scalar, ...], ...]\n+        | Mapping[str, Scalar]\n+        | Mapping[str, tuple[Scalar, ...]]\n+        | None\n+    ) = ...,\n     parse_dates: list[str] | dict[str, str] | dict[str, dict[str, Any]] | None = ...,\n     chunksize: None = ...,\n     dtype: DtypeArg | None = ...,\n@@ -91,7 +105,14 @@ def read_sql(\n     con: _SQLConnection,\n     index_col: str | list[str] | None = ...,\n     coerce_float: bool = ...,\n-    params: list[Scalar] | tuple[Scalar, ...] | Mapping[str, Scalar] | None = ...,\n+    params: (\n+        list[Scalar]\n+        | tuple[Scalar, ...]\n+        | tuple[tuple[Scalar, ...], ...]\n+        | Mapping[str, Scalar]\n+        | Mapping[str, tuple[Scalar, ...]]\n+        | None\n+    ) = ...,\n     parse_dates: list[str] | dict[str, str] | dict[str, dict[str, Any]] | None = ...,\n     columns: list[str] = ...,\n     *,\n@@ -105,7 +126,14 @@ def read_sql(\n     con: _SQLConnection,\n     index_col: str | list[str] | None = ...,\n     coerce_float: bool = ...,\n-    params: list[Scalar] | tuple[Scalar, ...] | Mapping[str, Scalar] | None = ...,\n+    params: (\n+        list[Scalar]\n+        | tuple[Scalar, ...]\n+        | tuple[tuple[Scalar, ...], ...]\n+        | Mapping[str, Scalar]\n+        | Mapping[str, tuple[Scalar, ...]]\n+        | None\n+    ) = ...,\n     parse_dates: list[str] | dict[str, str] | dict[str, dict[str, Any]] | None = ...,\n     columns: list[str] = ...,\n     chunksize: None = ...,\n", "instance_id": "pandas-dev__pandas-stubs-997", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug in the type definition of `pandas.read_sql_query` regarding the `params` argument, specifically the lack of support for tuples as dictionary values for certain database drivers like `psycopg2` and `mysql-connector`. The goal is explicit: update the type hints in `pandas-stubs` to accommodate tuples for `IN` conditions. The statement includes a reproducible example with detailed steps, code, and expected output, as well as the exact error message from Mypy. Constraints and context are provided, such as the limitation with `sqlite3` and the rationale for not restricting types to a universal subset. There are no significant ambiguities, and the problem's scope is well-defined with relevant additional context about the environment (OS, Python version, type checker version, etc.).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue is isolated to updating type hints in a single file (`pandas-stubs/io/sql.pyi`) for the `params` parameter of a few related functions (`read_sql_query` and `read_sql`). The code changes are straightforward, involving the addition of tuple types (`tuple[Scalar, ...]` and `tuple[tuple[Scalar, ...], ...]`) to the existing type union for `params`. This requires basic familiarity with Python type annotations and stub files, but no deep understanding of the broader `pandas` codebase or its runtime behavior is necessary. The scope of changes is minimal, with no impact on the system's architecture or interactions between modules. There are no complex edge cases or error handling requirements to address beyond the type system, as the runtime behavior is already correct (as demonstrated by the example). The primary technical concept involved is understanding Python's type system and how Mypy interprets type hints, which is not particularly advanced for a senior engineer. Overall, this is a simple bug fix in the type definitions with a clear path to resolution.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Index.rename() should return Self\n**Describe the bug**\r\n`Index.rename()` should return `Self`\r\n\r\n**To Reproduce**\r\n```python\r\nimport pandas as pd\r\n\r\nind = pd.Index([1,2,3], name=\"foo\")\r\nind2 = ind.rename(\"goo\")\r\nreveal_type(ind2)\r\n```\r\nshows that `ind2` is unknown\r\n\r\n\r\n**Please complete the following information:**\r\n - OS: Windows 10\r\n - OS Version ??\r\n - python version  3.9\r\n - version of type checker  pyright 1.1.362\r\n - version of installed `pandas-stubs` :  2.2.2.240514  \r\n\r\n\r\n\r\n\n", "patch": "diff --git a/pandas-stubs/core/indexes/base.pyi b/pandas-stubs/core/indexes/base.pyi\nindex 388b3d33..b8dc9444 100644\n--- a/pandas-stubs/core/indexes/base.pyi\n+++ b/pandas-stubs/core/indexes/base.pyi\n@@ -292,7 +292,7 @@ class Index(IndexOpsMixin[S1]):\n     @names.setter\n     def names(self, names: list[_str]): ...\n     def set_names(self, names, *, level=..., inplace: bool = ...): ...\n-    def rename(self, name, inplace: bool = ...): ...\n+    def rename(self, name, inplace: bool = ...) -> Self: ...\n     @property\n     def nlevels(self) -> int: ...\n     def sortlevel(self, level=..., ascending: bool = ..., sort_remaining=...): ...\n", "instance_id": "pandas-dev__pandas-stubs-936", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent: it identifies a bug in the type hinting of the `Index.rename()` method in the `pandas-stubs` library, where the return type should be `Self` but is currently unknown as per the type checker. The reproduction steps are provided with a clear code snippet, and relevant environment details (OS, Python version, type checker version, and library version) are included. However, there are minor ambiguities, such as the lack of explicit mention of the expected behavior beyond \"should return Self\" (e.g., does this apply to all cases of `rename()`, including when `inplace=True`?). Additionally, edge cases or specific constraints related to the type hinting change are not discussed. Overall, the problem is valid and mostly clear, but minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to a type hint in a single line of a `.pyi` stub file for the `pandas-stubs` library. The code change is minimal, requiring only the addition of a return type annotation (`-> Self`) to the `rename()` method signature. The scope is limited to a single file (`base.pyi`) and does not impact the broader codebase or architecture. No deep understanding of the `pandas` library internals, complex algorithms, or domain-specific knowledge is required beyond basic familiarity with Python type hinting and the concept of `Self` in type annotations (introduced in Python 3.11 or via `typing` module in earlier versions). There are no edge cases or error handling considerations mentioned in the problem statement, and the change does not introduce any risk of breaking existing functionality since it is purely a type hint update. Given the simplicity and isolated nature of the fix, I assign a difficulty score of 0.1, placing it in the \"Very Easy\" category (0.0-0.2).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "pd.concat with names=None\n**Describe the bug**\r\n`pd.concat` should accept `names=None` as it is the default value.\r\n\r\n**To Reproduce**\r\n1. Provide a minimal runnable `pandas` example that is not properly checked by the stubs.\r\n\r\n```\r\ndef maybe_concat(\r\n    dd: dict[str, pd.DataFrame], names: list[str] | None = None\r\n) -> pd.DataFrame:\r\n    \"\"\"Concatenate a dict of dataframes if available.\"\"\"\r\n    return pd.concat(dd, names=names) if dd else pd.DataFrame()\r\n```\r\n\r\n2. Indicate which type checker you are using (`mypy` or  `pyright`).\r\n**mypy**\r\n3. Show the error message received from that type checker while checking your example.\r\n```\r\nerror: Argument \"names\" to \"concat\" has incompatible type \"list[str] | None\"; expected \"list[str]\"  [arg-type]\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: [e.g. Windows, Linux, MacOS] **Linux**\r\n - OS Version [e.g. 22] **Fedora 7**\r\n - python version **3.11.8**\r\n - version of type checker **1.8.0**\r\n - version of installed `pandas-stubs` **2.1.4**\r\n\npd.concat with names=None\n**Describe the bug**\r\n`pd.concat` should accept `names=None` as it is the default value.\r\n\r\n**To Reproduce**\r\n1. Provide a minimal runnable `pandas` example that is not properly checked by the stubs.\r\n\r\n```\r\ndef maybe_concat(\r\n    dd: dict[str, pd.DataFrame], names: list[str] | None = None\r\n) -> pd.DataFrame:\r\n    \"\"\"Concatenate a dict of dataframes if available.\"\"\"\r\n    return pd.concat(dd, names=names) if dd else pd.DataFrame()\r\n```\r\n\r\n2. Indicate which type checker you are using (`mypy` or  `pyright`).\r\n**mypy**\r\n3. Show the error message received from that type checker while checking your example.\r\n```\r\nerror: Argument \"names\" to \"concat\" has incompatible type \"list[str] | None\"; expected \"list[str]\"  [arg-type]\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: [e.g. Windows, Linux, MacOS] **Linux**\r\n - OS Version [e.g. 22] **Fedora 7**\r\n - python version **3.11.8**\r\n - version of type checker **1.8.0**\r\n - version of installed `pandas-stubs` **2.1.4**\r\n\n", "patch": "diff --git a/pandas-stubs/core/reshape/concat.pyi b/pandas-stubs/core/reshape/concat.pyi\nindex bdd11b7e..61e547ac 100644\n--- a/pandas-stubs/core/reshape/concat.pyi\n+++ b/pandas-stubs/core/reshape/concat.pyi\n@@ -32,7 +32,7 @@ def concat(\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n@@ -46,7 +46,7 @@ def concat(  # type: ignore[overload-overlap] # pyright: ignore[reportOverlappin\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n@@ -60,7 +60,7 @@ def concat(  # type: ignore[overload-overlap] # pyright: ignore[reportOverlappin\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n@@ -74,7 +74,7 @@ def concat(\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n@@ -88,7 +88,7 @@ def concat(  # type: ignore[overload-overlap]\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n@@ -102,7 +102,7 @@ def concat(  # type: ignore[overload-overlap]\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n@@ -119,7 +119,7 @@ def concat(\n     ignore_index: bool = ...,\n     keys: Iterable[HashableT2] = ...,\n     levels: Sequence[list[HashableT3] | tuple[HashableT3, ...]] = ...,\n-    names: list[HashableT4] = ...,\n+    names: list[HashableT4] | None = ...,\n     verify_integrity: bool = ...,\n     sort: bool = ...,\n     copy: bool = ...,\n", "instance_id": "pandas-dev__pandas-stubs-894", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: `pd.concat` in the `pandas-stubs` library does not accept `names=None` as a valid argument type, despite it being the default value in the actual `pandas` implementation. The statement provides a minimal reproducible example, specifies the type checker used (`mypy`), and includes the exact error message, which helps in understanding the issue. However, it lacks some minor details, such as explicit mention of expected behavior beyond accepting `names=None` (e.g., what should happen when `names=None` is passed), and does not discuss potential edge cases or constraints related to this parameter. Additionally, the problem statement is duplicated in the provided text, which introduces unnecessary redundancy but does not significantly detract from clarity. Overall, the description is valid and clear but misses some finer points that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward type annotation fix in the `pandas-stubs` library. The code changes are minimal and localized to a single file (`concat.pyi`), where the type of the `names` parameter is updated from `list[HashableT4]` to `list[HashableT4] | None` across multiple overloads of the `concat` function. This requires basic knowledge of Python type hints and stub files, with no complex logic, algorithms, or deep understanding of the broader codebase or `pandas` internals. The scope of the change is small, with no impact on the system's architecture or interactions between modules. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as this is purely a type annotation adjustment for compatibility with a type checker. The task is essentially a simple bug fix in the type definition, making it very easy for anyone with basic familiarity with Python type systems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
