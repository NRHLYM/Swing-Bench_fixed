{"problem_statement": "Unable to use patched version of grpcio with google-cloud-pubsub\nThere are a number of bugs (functionally and security wise) in grpcio < 1.63. However, >1.63 now requires >5 protobuf, but this library requires protobuf <5 making it no longer possible to install this library with a patched working version of grpcio.\r\n\r\n```\r\nBecause no versions of google-cloud-pubsub match >2.21.3\r\n and google-cloud-pubsub (2.21.3) depends on protobuf (>=3.19.5,<3.20.0 || >3.20.0,<3.20.1 || >3.20.1,<4.21.0 || >4.21.0,<4.21.1 || >4.21.1,<4.21.2 || >4.21.2,<4.21.3 || >4.21.3,<4.21.4 || >4.21.4,<4.21.5 || >4.21.5,<5.0.0dev), google-cloud-pubsub (>=2.21.3) requires protobuf (>=3.19.5,<3.20.0 || >3.20.0,<3.20.1 || >3.20.1,<4.21.0 || >4.21.0,<4.21.1 || >4.21.1,<4.21.2 || >4.21.2,<4.21.3 || >4.21.3,<4.21.4 || >4.21.4,<4.21.5 || >4.21.5,<5.0.0dev).\r\nBecause grpcio (1.64.1) depends on protobuf (>=5.26.1,<6.0dev)\r\n and no versions of grpcio match >1.64.1,<2.0.0, grpcio (>=1.64.1,<2.0.0) requires protobuf (>=5.26.1,<6.0dev).\r\nThus, google-cloud-pubsub (>=2.21.3) is incompatible with grpcio (>=1.64.1,<2.0.0).\r\n```\r\n\r\nCan support for a newer version of protobuf be included in this library?\n", "patch": "diff --git a/setup.py b/setup.py\nindex a6af31207..dbb66cf7c 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -42,7 +42,7 @@\n     \"google-api-core[grpc] >= 1.34.0, <3.0.0dev,!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,!=2.10.*\",\n     \"proto-plus >= 1.22.0, <2.0.0dev\",\n     \"proto-plus >= 1.22.2, <2.0.0dev; python_version>='3.11'\",\n-    \"protobuf>=3.19.5,<5.0.0dev,!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5\",\n+    \"protobuf>=3.20.2,<6.0.0dev,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5\",\n     \"grpc-google-iam-v1 >= 0.12.4, < 1.0.0dev\",\n     \"grpcio-status >= 1.33.2\",\n ]\n", "instance_id": "googleapis__python-pubsub-1193", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: there is a version incompatibility between `grpcio` and `protobuf` when used with `google-cloud-pubsub`. The goal is to enable support for a newer version of `protobuf` to resolve this conflict. The statement includes detailed dependency constraints and error messages, which help in understanding the root cause. However, it lacks explicit mention of potential side effects or risks of updating the `protobuf` version (e.g., compatibility with other dependencies or breaking changes in the library). Additionally, there are no examples or test cases provided to validate the solution, and edge cases or specific scenarios to consider are not mentioned. Thus, while the core issue is clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to the dependency constraints in a single file (`setup.py`). The code change is minimal, requiring only an update to the version range for `protobuf` to allow compatibility with newer versions required by `grpcio`. The scope is limited to a single line in the setup configuration, with no impact on the broader codebase architecture or logic. The technical concepts involved are basic\u2014understanding Python package dependencies and version specifiers, which are fundamental for any developer working with Python libraries. There are no complex algorithms, design patterns, or domain-specific knowledge required. While there might be implicit edge cases (e.g., ensuring other dependencies or downstream users are not broken by the updated `protobuf` version), the problem statement and code change do not explicitly address or require handling such cases. Overall, this is a very easy task, fitting within the 0.0-0.2 range, and I assign a score of 0.15 to reflect the minimal effort and expertise needed, with a slight bump above 0.0 due to the need for basic awareness of dependency management.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\u2728[Feature] Delayed Initialization for `TRTModule` Classes\n# Context\r\nFor models requiring fallback to Torch due to converter capabilities, custom operators, or other needs, each of the `TRTEngine` objects is compiled, initialized, inserted into the Torch `nn.Module`, and runtime-ready during compile time. This takes up an unnecessary amount of memory on the GPU at compile time.\r\n\r\n# Proposal\r\nUse the GPU as a build space for `TRTEngine` objects, but do not deserialize or initialize the engines until the first forward pass, similar to what is done here:\r\nhttps://github.com/pytorch/TensorRT/blob/ad74a735056667726692c49a175a790647ef889e/py/torch_tensorrt/fx/trt_module.py#L25-L27\r\n\r\n## API Details\r\nThe `TRTModule` objects will take a parameter, `construct_live=True`, which can be specified to `False` if it is desired to initialize the engines at the first `forward` pass, thereby avoiding unnecessary usage of GPU space during compilation. After building the engine at compile time, the serialized object is moved to host memory until runtime, at which point it is initialized. `check_initialized()` is called at every `forward` pass, only having a measurable effect on the first pass of inference at which point the engines are moved from host to device memory for usage.\n", "patch": "diff --git a/py/torch_tensorrt/_compile.py b/py/torch_tensorrt/_compile.py\nindex 89a965fee7..8a168e9001 100644\n--- a/py/torch_tensorrt/_compile.py\n+++ b/py/torch_tensorrt/_compile.py\n@@ -27,7 +27,7 @@\n     from torch.export import ExportedProgram\n     from torch_tensorrt.dynamo._compiler import compile as dynamo_compile\n     from torch_tensorrt.dynamo._compiler import (\n-        convert_module_to_trt_engine as dynamo_convert_module_to_trt_engine,\n+        convert_exported_program_to_serialized_trt_engine as dynamo_convert_exported_program_to_serialized_trt_engine,\n     )\n     from torch_tensorrt.dynamo._tracer import trace as dynamo_trace\n \n@@ -351,7 +351,7 @@ def convert_method_to_trt_engine(\n         torchtrt_inputs = prepare_inputs(inputs)\n         exp_program = torch_tensorrt.dynamo.trace(module, torchtrt_inputs, **kwargs)\n \n-        return dynamo_convert_module_to_trt_engine(\n+        return dynamo_convert_exported_program_to_serialized_trt_engine(\n             exp_program,\n             inputs=tuple(inputs),\n             enabled_precisions=enabled_precisions_set,\ndiff --git a/py/torch_tensorrt/dynamo/__init__.py b/py/torch_tensorrt/dynamo/__init__.py\nindex 83597db0b6..79bd113ab8 100644\n--- a/py/torch_tensorrt/dynamo/__init__.py\n+++ b/py/torch_tensorrt/dynamo/__init__.py\n@@ -7,7 +7,7 @@\n logger = logging.getLogger(__name__)\n \n if version.parse(sanitized_torch_version()) >= version.parse(\"2.1.dev\"):\n-    from ._compiler import compile, convert_module_to_trt_engine\n+    from ._compiler import compile, convert_exported_program_to_serialized_trt_engine\n     from ._exporter import export\n     from ._refit import refit_module_weights\n     from ._settings import CompilationSettings\ndiff --git a/py/torch_tensorrt/dynamo/_compiler.py b/py/torch_tensorrt/dynamo/_compiler.py\nindex 4dcf5a22e4..0362a010df 100644\n--- a/py/torch_tensorrt/dynamo/_compiler.py\n+++ b/py/torch_tensorrt/dynamo/_compiler.py\n@@ -79,6 +79,7 @@ def compile(\n     dryrun: bool = _defaults.DRYRUN,\n     hardware_compatible: bool = _defaults.HARDWARE_COMPATIBLE,\n     timing_cache_path: str = _defaults.TIMING_CACHE_PATH,\n+    lazy_engine_init: bool = _defaults.LAZY_ENGINE_INIT,\n     **kwargs: Any,\n ) -> torch.fx.GraphModule:\n     \"\"\"Compile an ExportedProgram module for NVIDIA GPUs using TensorRT\n@@ -141,6 +142,7 @@ def compile(\n         dryrun (bool): Toggle for \"Dryrun\" mode, running everything except conversion to TRT and logging outputs\n         hardware_compatible (bool): Build the TensorRT engines compatible with GPU architectures other than that of the GPU on which the engine was built (currently works for NVIDIA Ampere and newer)\n         timing_cache_path (str): Path to the timing cache if it exists (or) where it will be saved after compilation\n+        lazy_engine_init (bool): Defer setting up engines until the compilation of all engines is complete. Can allow larger models with multiple graph breaks to compile but can lead to oversubscription of GPU memory at runtime.\n         **kwargs: Any,\n     Returns:\n         torch.fx.GraphModule: Compiled FX Module, when run it will execute via TensorRT\n@@ -236,6 +238,7 @@ def compile(\n         \"dryrun\": dryrun,\n         \"hardware_compatible\": hardware_compatible,\n         \"timing_cache_path\": timing_cache_path,\n+        \"lazy_engine_init\": lazy_engine_init,\n     }\n \n     settings = CompilationSettings(**compilation_options)\n@@ -454,6 +457,8 @@ def contains_metadata(gm: torch.fx.GraphModule) -> bool:\n     # Replace all FX Modules with TRT Modules\n     for name, trt_module in trt_modules.items():\n         setattr(partitioned_module, name, trt_module)\n+        if settings.lazy_engine_init:\n+            getattr(partitioned_module, name).setup_engine()\n \n     # Reset settings object to user specification after fallback to global partitioning mode\n     if fast_partitioner_failed:\n@@ -464,7 +469,7 @@ def contains_metadata(gm: torch.fx.GraphModule) -> bool:\n     return partitioned_module\n \n \n-def convert_module_to_trt_engine(\n+def convert_exported_program_to_serialized_trt_engine(\n     exported_program: ExportedProgram,\n     inputs: Sequence[Any],\n     *,\n@@ -647,10 +652,5 @@ def convert_module_to_trt_engine(\n             exc_info=True,\n         )\n \n-    import io\n-\n-    with io.BytesIO() as engine_bytes:\n-        engine_bytes.write(interpreter_result.engine)\n-        engine_bytearray: bytes = engine_bytes.getvalue()\n-\n-    return engine_bytearray\n+    serialized_engine: bytes = interpreter_result.serialized_engine\n+    return serialized_engine\ndiff --git a/py/torch_tensorrt/dynamo/_defaults.py b/py/torch_tensorrt/dynamo/_defaults.py\nindex dbf0265496..2696e26936 100644\n--- a/py/torch_tensorrt/dynamo/_defaults.py\n+++ b/py/torch_tensorrt/dynamo/_defaults.py\n@@ -32,6 +32,7 @@\n HARDWARE_COMPATIBLE = False\n SUPPORTED_KERNEL_PRECISIONS = {dtype.f32, dtype.f16, dtype.bf16, dtype.i8, dtype.f8}\n TIMING_CACHE_PATH = os.path.join(tempfile.gettempdir(), \"timing_cache.bin\")\n+LAZY_ENGINE_INIT = False\n \n \n def default_device() -> Device:\ndiff --git a/py/torch_tensorrt/dynamo/_settings.py b/py/torch_tensorrt/dynamo/_settings.py\nindex 13c786b858..4a9792d3dc 100644\n--- a/py/torch_tensorrt/dynamo/_settings.py\n+++ b/py/torch_tensorrt/dynamo/_settings.py\n@@ -16,6 +16,7 @@\n     ENABLED_PRECISIONS,\n     ENGINE_CAPABILITY,\n     HARDWARE_COMPATIBLE,\n+    LAZY_ENGINE_INIT,\n     MAKE_REFITABLE,\n     MAX_AUX_STREAMS,\n     MIN_BLOCK_SIZE,\n@@ -104,3 +105,4 @@ class CompilationSettings:\n     dryrun: Union[bool, str] = DRYRUN\n     hardware_compatible: bool = HARDWARE_COMPATIBLE\n     timing_cache_path: str = TIMING_CACHE_PATH\n+    lazy_engine_init: bool = LAZY_ENGINE_INIT\ndiff --git a/py/torch_tensorrt/dynamo/conversion/_TRTInterpreter.py b/py/torch_tensorrt/dynamo/conversion/_TRTInterpreter.py\nindex 09fcccf5d8..703a650c99 100644\n--- a/py/torch_tensorrt/dynamo/conversion/_TRTInterpreter.py\n+++ b/py/torch_tensorrt/dynamo/conversion/_TRTInterpreter.py\n@@ -1,3 +1,4 @@\n+import io\n import logging\n import os\n import warnings\n@@ -5,7 +6,6 @@\n from typing import Any, Callable, Dict, List, NamedTuple, Optional, Sequence, Set, Tuple\n \n import numpy as np\n-import tensorrt as trt\n import torch\n import torch.fx\n from torch.fx.node import _get_qualified_name\n@@ -29,6 +29,7 @@\n from torch_tensorrt.fx.observer import Observer\n from torch_tensorrt.logging import TRT_LOGGER\n \n+import tensorrt as trt\n from packaging import version\n \n _LOGGER: logging.Logger = logging.getLogger(__name__)\n@@ -43,7 +44,7 @@ class UnsupportedOperatorException(RuntimeError):\n \n \n class TRTInterpreterResult(NamedTuple):\n-    engine: Any\n+    serialized_engine: bytes\n     input_names: Sequence[str]\n     output_names: Sequence[str]\n \n@@ -358,9 +359,11 @@ def run(\n             builder_config, self.compilation_settings.timing_cache_path\n         )\n \n-        return TRTInterpreterResult(\n-            serialized_engine, self._input_names, self._output_names\n-        )\n+        with io.BytesIO() as engine_bytes:\n+            engine_bytes.write(serialized_engine)\n+            engine_str = engine_bytes.getvalue()\n+\n+        return TRTInterpreterResult(engine_str, self._input_names, self._output_names)\n \n     def run_node(self, n: torch.fx.Node) -> torch.fx.Node:\n         self._cur_node_name = get_node_name(n)\ndiff --git a/py/torch_tensorrt/dynamo/conversion/_conversion.py b/py/torch_tensorrt/dynamo/conversion/_conversion.py\nindex ea3034cb8c..8f22a6c993 100644\n--- a/py/torch_tensorrt/dynamo/conversion/_conversion.py\n+++ b/py/torch_tensorrt/dynamo/conversion/_conversion.py\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import io\n import logging\n from typing import List, Sequence\n \n@@ -102,33 +101,30 @@ def convert_module(\n         settings: Compilation settings\n         name: TRT engine name\n     Returns:\n-        _PythonTorchTensorRTModule or TorchTensorRTModule\n+        PythonTorchTensorRTModule or TorchTensorRTModule\n     \"\"\"\n     interpreter_result = interpret_module_to_result(module, inputs, settings)\n \n-    if settings.use_python_runtime or not ENABLED_FEATURES.torch_tensorrt_runtime:\n-        if not settings.use_python_runtime:\n-            logger.info(\n-                \"Since Torch-TensorRT runtime is not available, using Python Runtime, some features may not be available\"\n-            )\n-        return PythonTorchTensorRTModule(\n-            engine=interpreter_result.engine,\n-            input_names=list(interpreter_result.input_names),\n-            output_names=list(interpreter_result.output_names),\n-            settings=settings,\n-        )\n+    rt_cls = PythonTorchTensorRTModule\n+\n+    if ENABLED_FEATURES.torch_tensorrt_runtime and not settings.use_python_runtime:\n \n-    else:\n         from torch_tensorrt.dynamo.runtime import TorchTensorRTModule\n \n-        with io.BytesIO() as engine_bytes:\n-            engine_bytes.write(interpreter_result.engine)\n-            engine_str = engine_bytes.getvalue()\n+        rt_cls = TorchTensorRTModule\n+\n+    elif (\n+        not ENABLED_FEATURES.torch_tensorrt_runtime and not settings.use_python_runtime\n+    ):\n \n-        return TorchTensorRTModule(\n-            serialized_engine=engine_str,\n-            name=name,\n-            input_binding_names=list(interpreter_result.input_names),\n-            output_binding_names=list(interpreter_result.output_names),\n-            settings=settings,\n+        logger.info(\n+            \"Since Torch-TensorRT runtime is not available, using Python Runtime, some features may not be available\"\n         )\n+\n+    return rt_cls(\n+        serialized_engine=interpreter_result.serialized_engine,\n+        input_binding_names=list(interpreter_result.input_names),\n+        output_binding_names=list(interpreter_result.output_names),\n+        name=name,\n+        settings=settings,\n+    )\ndiff --git a/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py b/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py\nindex 6c94b112a7..659f18af52 100644\n--- a/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py\n+++ b/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py\n@@ -4,8 +4,8 @@\n from contextlib import nullcontext\n from typing import Any, Dict, List, Optional, Sequence, Tuple\n \n-import tensorrt as trt\n import torch\n+import torch_tensorrt\n from torch.nn import Module\n from torch_tensorrt._Device import Device\n from torch_tensorrt._enums import dtype\n@@ -18,7 +18,7 @@\n from torch_tensorrt.dynamo.utils import DYNAMIC_DIM\n from torch_tensorrt.logging import TRT_LOGGER\n \n-import torch_tensorrt\n+import tensorrt as trt\n \n logger = logging.getLogger(__name__)\n \n@@ -32,17 +32,45 @@ class PythonTorchTensorRTModule(Module):  # type: ignore[misc]\n \n     def __init__(\n         self,\n-        engine: bytes,\n-        input_names: Optional[List[str]] = None,\n-        output_names: Optional[List[str]] = None,\n+        serialized_engine: Optional[bytes] = None,\n+        input_binding_names: Optional[List[str]] = None,\n+        output_binding_names: Optional[List[str]] = None,\n+        *,\n+        name: str = \"\",\n         settings: CompilationSettings = CompilationSettings(),\n     ):\n+        \"\"\"Takes a name, target device, serialized TensorRT engine, and binding names / order and constructs\n+        a PyTorch ``torch.nn.Module`` around it. Uses TensorRT Python APIs to run the engine\n+\n+        Arguments:\n+            serialized_engine (bytes): Serialized TensorRT engine in the form of a bytearray\n+            input_binding_names (List[str]): List of input TensorRT engine binding names in the order they would be passed to the TRT modules\n+            output_binding_names (List[str]): List of output TensorRT engine binding names in the order they should be returned\n+\n+        Keyword Arguments:\n+            name (str): Name for module\n+            settings (torch_tensorrt.dynamo.CompilationSettings): Settings used to compile engine, assumes engine was built with default compilation settings if object not passed\n+\n+        Example:\n+\n+            .. code-block:: py\n+\n+                trt_module = PythonTorchTensorRTModule(\n+                    engine_str,\n+                    input_binding_names=[\"x\"],\n+                    output_binding_names=[\"output\"],\n+                    name=\"my_module\",\n+                    settings=CompilationSettings(device=torch.cuda.current_device)\n+                )\n+\n+        \"\"\"\n         super(PythonTorchTensorRTModule, self).__init__()\n         self._register_state_dict_hook(PythonTorchTensorRTModule._on_state_dict)\n \n         # Run multi-gpu device check to validate engine instantiation\n         multi_gpu_device_check()\n \n+        self.name = name\n         self.input_buffers: List[torch.Tensor] = []\n         self.output_buffers: List[torch.Tensor] = []\n         self.cudagraph: Optional[torch.cuda.CUDAGraph] = None\n@@ -55,9 +83,13 @@ def __init__(\n         # Unused currently - to be used by Dynamic Shape support implementation\n         self.memory_pool = None\n \n-        self.engine = engine\n-        self.input_names = input_names if input_names is not None else []\n-        self.output_names = output_names if output_names is not None else []\n+        self.serialized_engine = serialized_engine\n+        self.input_names = (\n+            input_binding_names if input_binding_names is not None else []\n+        )\n+        self.output_names = (\n+            output_binding_names if output_binding_names is not None else []\n+        )\n         self.initialized = False\n         self.target_device_id = (\n             settings.device.gpu_id\n@@ -69,12 +101,15 @@ def __init__(\n         )\n         self.profiling_enabled = settings.debug if settings.debug is not None else False\n         self.settings = settings\n-        self._initialize()\n+        self.engine = None\n+\n+        if self.serialized_engine is not None and not self.settings.lazy_engine_init:\n+            self.setup_engine()\n \n-    def _initialize(self) -> None:\n+    def setup_engine(self) -> None:\n         self.initialized = True\n         runtime = trt.Runtime(TRT_LOGGER)\n-        self.engine = runtime.deserialize_cuda_engine(self.engine)\n+        self.engine = runtime.deserialize_cuda_engine(self.serialized_engine)\n         self.context = self.engine.create_execution_context()\n \n         assert self.engine.num_io_tensors == (\n@@ -114,8 +149,7 @@ def _check_initialized(self) -> None:\n             raise RuntimeError(\"PythonTorchTensorRTModule is not initialized.\")\n \n     def _on_state_dict(self, state_dict: Dict[str, Any], prefix: str, _: Any) -> None:\n-        self._check_initialized()\n-        state_dict[prefix + \"engine\"] = bytearray(self.engine.serialize())\n+        state_dict[prefix + \"engine\"] = self.serialized_engine\n         state_dict[prefix + \"input_names\"] = self.input_names\n         state_dict[prefix + \"output_names\"] = self.output_names\n \n@@ -129,17 +163,13 @@ def _load_from_state_dict(\n         unexpected_keys: Any,\n         error_msgs: Any,\n     ) -> None:\n-        engine_bytes = state_dict[prefix + \"engine\"]\n+        self.serialized_engine = state_dict[prefix + \"engine\"]\n+        self.input_names = state_dict[prefix + \"input_names\"]\n+        self.output_names = state_dict[prefix + \"output_names\"]\n \n         # Run multi-gpu device check to validate engine instantiation\n         multi_gpu_device_check()\n-\n-        runtime = trt.Runtime(TRT_LOGGER)\n-        self.engine = runtime.deserialize_cuda_engine(engine_bytes)\n-\n-        self.input_names = state_dict[prefix + \"input_names\"]\n-        self.output_names = state_dict[prefix + \"output_names\"]\n-        self._initialize()\n+        self.setup_engine()\n \n     def __getstate__(self) -> Dict[str, Any]:\n         state = self.__dict__.copy()\ndiff --git a/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py b/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py\nindex 601147279a..0ab0dd49ca 100644\n--- a/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py\n+++ b/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py\n@@ -53,13 +53,14 @@ class TorchTensorRTModule(torch.nn.Module):  # type: ignore[misc]\n     def __init__(\n         self,\n         serialized_engine: Optional[bytes] = None,\n-        name: str = \"\",\n         input_binding_names: Optional[List[str]] = None,\n         output_binding_names: Optional[List[str]] = None,\n-        settings: CompilationSettings = CompilationSettings(),\n+        *,\n+        name: str = \"\",\n+        settings: CompilationSettings = CompilationSettings(),  # Assumes engine was built with default compilation settings if object not passed\n     ):\n         \"\"\"Takes a name, target device, serialized TensorRT engine, and binding names / order and constructs\n-        a PyTorch ``torch.nn.Module`` around it.\n+        a PyTorch ``torch.nn.Module`` around it. Uses the Torch-TensorRT runtime extension to run the engines\n \n         If binding names are not provided, it is assumed that the engine binding names follow the following convention:\n \n@@ -67,12 +68,13 @@ def __init__(\n                 - ex. [x.0, x.1, x.2] -> [y.0]\n \n         Arguments:\n-            name (str): Name for module\n-            serialized_engine (bytearray): Serialized TensorRT engine in the form of a bytearray\n+            serialized_engine (bytes): Serialized TensorRT engine in the form of a bytearray\n             input_binding_names (List[str]): List of input TensorRT engine binding names in the order they would be passed to the TRT modules\n             output_binding_names (List[str]): List of output TensorRT engine binding names in the order they should be returned\n-            target_device (torch_tensorrt.Device): Device to instantiate TensorRT engine on. Must be a compatible device i.e. same GPU model / compute capability as was used to build the engine\n-            hardware_compatible (bool): If the engine has be built with the hardware compatibility feature enabled\n+\n+        Keyword Arguments:\n+            name (str): Name for module\n+            settings (torch_tensorrt.dynamo.CompilationSettings): Settings used to compile engine, assumes engine was built with default compilation settings if object not passed\n \n         Example:\n \n@@ -84,9 +86,10 @@ def __init__(\n \n                 trt_module = TorchTensorRTModule(\n                     engine_str,\n-                    name=\"my_module\",\n                     input_binding_names=[\"x\"],\n                     output_binding_names=[\"output\"],\n+                    name=\"my_module\",\n+                    settings=CompilationSettings(device=torch.cuda.current_device)\n                 )\n \n         \"\"\"\n@@ -102,26 +105,43 @@ def __init__(\n             output_binding_names if output_binding_names is not None else []\n         )\n         self.name = name\n-        target_device = (\n-            settings.device if settings.device is not None else Device._current_device()\n-        )\n         self.hardware_compatible = settings.hardware_compatible\n         self.settings = copy.deepcopy(settings)\n-        if serialized_engine is not None:\n-            self.engine = torch.classes.tensorrt.Engine(\n-                [\n-                    torch.ops.tensorrt.ABI_VERSION(),\n-                    self.name + \"_engine\" if self.name != \"\" else \"tensorrt_engine\",\n-                    target_device._to_serialized_rt_device(),\n-                    serialized_engine,\n-                    TorchTensorRTModule._pack_binding_names(self.input_binding_names),\n-                    TorchTensorRTModule._pack_binding_names(self.output_binding_names),\n-                    str(int(self.hardware_compatible)),\n-                    self.encode_metadata(settings),\n-                ]\n-            )\n-        else:\n-            self.engine = None\n+        self.serialized_engine = serialized_engine\n+        self.engine = None\n+\n+        if serialized_engine and not self.settings.lazy_engine_init:\n+            self.setup_engine()\n+\n+    def setup_engine(self) -> None:\n+        \"\"\"\n+        Setup engine for a module which has deferred engine setup.\n+\n+        Will setup the TensorRT engine for this module in the case that setup has been\n+        deferred. In the case that the engine has already been setup, will return without\n+        changing anything. Assumes that serialized engine and settings have already been passed\n+        to the module.\n+        \"\"\"\n+        if self.engine is not None:\n+            return\n+\n+        target_device = (\n+            self.settings.device\n+            if self.settings.device is not None\n+            else Device._current_device()\n+        )\n+        self.engine = torch.classes.tensorrt.Engine(\n+            [\n+                torch.ops.tensorrt.ABI_VERSION(),\n+                self.name + \"_engine\" if self.name != \"\" else \"tensorrt_engine\",\n+                target_device._to_serialized_rt_device(),\n+                self.serialized_engine,\n+                TorchTensorRTModule._pack_binding_names(self.input_binding_names),\n+                TorchTensorRTModule._pack_binding_names(self.output_binding_names),\n+                str(int(self.hardware_compatible)),\n+                self.encode_metadata(self.settings),\n+            ]\n+        )\n \n     def encode_metadata(self, settings: Any) -> str:\n         settings = copy.deepcopy(settings)\n@@ -140,9 +160,12 @@ def decode_metadata(encoded_settings: bytes) -> Any:\n         return settings\n \n     def get_extra_state(self) -> SerializedTorchTensorRTModuleFmt:\n+        if self.engine is None and self.serialized_engine is not None:\n+            self.setup_engine()\n+\n         return (\n             self.name,\n-            self.engine.__getstate__() if self.engine is not None else None,\n+            self.engine.__getstate__() if self.engine else None,\n             self.input_binding_names,\n             self.output_binding_names,\n         )\n@@ -152,13 +175,13 @@ def set_extra_state(self, state: SerializedTorchTensorRTModuleFmt) -> None:\n         if state[1] is not None:\n             serialized_engine_info: SerializedTensorRTEngineFmt = state[1]\n \n-            serialized_engine = base64.b64decode(serialized_engine_info[3])\n+            self.serialized_engine = base64.b64decode(serialized_engine_info[3])\n             self.engine = torch.classes.tensorrt.Engine(\n                 [\n                     serialized_engine_info[ABI_TARGET_IDX],\n                     serialized_engine_info[NAME_IDX],\n                     serialized_engine_info[DEVICE_IDX],\n-                    serialized_engine,\n+                    self.serialized_engine,\n                     serialized_engine_info[INPUT_BINDING_NAMES_IDX],\n                     serialized_engine_info[OUTPUT_BINDING_NAMES_IDX],\n                     serialized_engine_info[HW_COMPATIBLE_IDX],\n@@ -185,7 +208,7 @@ def forward(self, *inputs: Any) -> torch.Tensor | Tuple[torch.Tensor, ...]:\n             torch.Tensor or Tuple(torch.Tensor): Result of the engine computation\n         \"\"\"\n         if self.engine is None:\n-            raise RuntimeError(\"Engine has not been initialized yet.\")\n+            raise RuntimeError(\"Engine has not been setup yet.\")\n \n         assert len(inputs) == len(\n             self.input_binding_names\n", "instance_id": "pytorch__TensorRT-2997", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement for the \"Delayed Initialization for TRTModule Classes\" feature is mostly clear, providing a solid context about the memory issue during compile time for GPU usage with TensorRT engines. It outlines the goal of deferring engine initialization to the first forward pass to save GPU memory, and it specifies the API change with the `construct_live` parameter (though named differently as `lazy_engine_init` in the code). The proposal also references existing code for similar behavior, which helps in understanding the intended approach. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not explicitly discuss potential risks or trade-offs of lazy initialization, such as runtime oversubscription of GPU memory (mentioned in the code comments but not in the proposal). Additionally, edge cases or specific constraints (e.g., compatibility with different model sizes or GPU architectures) are not addressed in the statement, which could impact implementation decisions. Overall, while the goal and high-level approach are clear, these minor gaps in detail result in a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of implementing the delayed initialization feature for `TRTModule` classes is rated at 0.75, placing it in the \"Hard\" category (0.6-0.8). This assessment is based on several factors. First, the scope of code changes is significant, spanning multiple files and modules within the TensorRT integration for PyTorch, including compilation settings, runtime modules, and engine serialization logic. The changes involve renaming functions (e.g., `convert_module_to_trt_engine` to `convert_exported_program_to_serialized_trt_engine`), adding new parameters (`lazy_engine_init`), and modifying initialization logic across both Python and native runtime implementations (`PythonTorchTensorRTModule` and `TorchTensorRTModule`). This requires a deep understanding of the codebase architecture and interactions between compilation and runtime phases. Second, the technical concepts involved are complex, including TensorRT engine serialization/deserialization, GPU memory management, PyTorch module integration, and runtime initialization logic. Familiarity with TensorRT APIs and PyTorch's FX and Dynamo frameworks is necessary, along with handling state serialization for model persistence. Third, while edge cases are not extensively detailed in the problem statement, the code changes and comments (e.g., potential GPU memory oversubscription at runtime) suggest non-trivial considerations for large models or multi-GPU setups, as well as ensuring correctness during the first forward pass. Finally, the impact of these changes is substantial, as they alter the memory usage behavior of a core component in a performance-critical system, requiring careful testing and validation. While not reaching the \"Very Hard\" category (0.8-1.0) due to the absence of extremely intricate domain-specific challenges or system-level redesign, this task demands advanced knowledge and experience, justifying a score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Improve `expr` method signatures\n# Problem\r\n\r\nWhile I was working on https://github.com/vega/altair/pull/3544, I tried out re-writing some expression strings to use `alt.expr`.\r\nThis idea stems from what I saw as a poor UX - that has a relatively simple solution with multiple benefits.\r\n\r\n## Related\r\n- *\"expressions are hard to debug\"* in https://github.com/vega/altair/discussions/3476\r\n- https://github.com/vega/altair/issues/3365\r\n\r\n## Minimal Repro\r\nBased on https://github.com/vega/altair/commit/a681ec53cb57524f1fd347200df94c88c1d151ef\r\n\r\n<details><summary>Code block</summary>\r\n<p>\r\n\r\n```py\r\nimport altair as alt\r\n\r\nlabel = alt.datum.label\r\nwindow_lead_label = alt.datum.window_lead_label\r\n\r\nsource = {\r\n    \"values\": [\r\n        {\"label\": \"Begin\", \"amount\": 4000},\r\n        {\"label\": \"Jan\", \"amount\": 1707},\r\n        {\"label\": \"Feb\", \"amount\": -1425},\r\n        {\"label\": \"Mar\", \"amount\": -1030},\r\n        {\"label\": \"Apr\", \"amount\": 1812},\r\n        {\"label\": \"May\", \"amount\": -1067},\r\n        {\"label\": \"Jun\", \"amount\": -1481},\r\n        {\"label\": \"Jul\", \"amount\": 1228},\r\n        {\"label\": \"Aug\", \"amount\": 1176},\r\n        {\"label\": \"Sep\", \"amount\": 1146},\r\n        {\"label\": \"Oct\", \"amount\": 1205},\r\n        {\"label\": \"Nov\", \"amount\": -1388},\r\n        {\"label\": \"Dec\", \"amount\": 1492},\r\n        {\"label\": \"End\", \"amount\": 0},\r\n    ]\r\n}\r\n\r\nchart = (\r\n    alt.Chart(source)\r\n    .mark_bar(size=45)\r\n    .encode(alt.X(\"label:O\", sort=None))\r\n    .transform_window(window_lead_label=\"lead(label)\")\r\n    .transform_calculate(calc_lead=alt.expr.if_(window_lead_label == None, label))\r\n)\r\n>>> alt.expr.if_(window_lead_label == None, label)\r\n>>> chart\r\n```\r\n</p>\r\n</details> \r\n\r\n## Visual Feedback\r\n\r\n<details><summary>Screenshot</summary>\r\n<p>\r\n\r\n![image](https://github.com/user-attachments/assets/efaa7546-622d-4145-b789-67c4bf57000b)\r\n\r\n</p>\r\n</details> \r\n\r\n- Error is only present **after** validation\r\n- The error itself is not useful if the spec contains *more than one* `expr`\r\n- Also not clear *which* argument(s) are missing\r\n\r\n# Solution\r\nReplacing the `alt.expr` classmethod(s) `*args` parameter with named [positional-only parameter(s)](https://peps.python.org/pep-0570/).\r\n\r\n## Benefits\r\n- Raising a `TypeError` at the time of `expr` definition\r\n\t- Rather than silently passing until validated within a `ChartType`\r\n\t- Importantly, this is within `python` and **not** `javascript`\r\n- Providing a meaningful traceback, identifying both the `expr` and the missing argument\r\n- Opens the door for defining default argument values\r\n\t-  E.g. in [waterfall_chart](https://altair-viz.github.io/gallery/waterfall_chart.html), there are repeat uses of `\"\"` for the `elseValue`\r\n\t- If this is a common case, we could use that as a default and it would be clear by the signature alone\r\n- The current docstrings - which refer to parameters by name - would be easier to understand\r\n\t- Right now, you'd need to refer to [Vega Expressions](https://vega.github.io/vega/docs/expressions/) to determine the order they must appear in\r\n- The change **would not** add any new requirements to currently valid specs\r\n\t- Therefore, suitable as a [**MINOR**](https://altair-viz.github.io/about/versioning.html#version-numbers) version feature\r\n\r\n## Example (`alt.expr.if_`)\r\nAlthough there are a **large number** of signatures to be updated, the process for each should be as simple as refering to the docstring or [Vega Expressions](https://vega.github.io/vega/docs/expressions/)\r\n\r\n![image](https://github.com/user-attachments/assets/f1f75cea-d0db-4cce-9dbe-c16c0e8a731c)\r\n\r\n## Drawbacks\r\nThe only downside I can see - hypothetically - is if there is a utility in having invalid expressions at `expr` definition time?\r\nMaybe there is some use-case here I'm unaware of, but it seems unlikely to me it could outweigh the traceback improvement\n", "patch": "diff --git a/altair/expr/__init__.py b/altair/expr/__init__.py\nindex 1f93ac2b7..38d87f4c5 100644\n--- a/altair/expr/__init__.py\n+++ b/altair/expr/__init__.py\n@@ -1,8 +1,12 @@\n+# The contents of this file are automatically written by\n+# tools/generate_schema_wrapper.py. Do not modify directly.\n+\n \"\"\"Tools for creating transform & filter expressions with a python syntax.\"\"\"\n \n from __future__ import annotations\n \n import sys\n+from typing import TYPE_CHECKING, Any\n \n from altair.expr.core import ConstExpression, FunctionExpression\n from altair.vegalite.v5.schema.core import ExprRef as _ExprRef\n@@ -12,58 +16,65 @@\n else:\n     from typing_extensions import override\n \n+if TYPE_CHECKING:\n+    from altair.expr.core import Expression, IntoExpression\n+\n+\n+class _ExprMeta(type):\n+    \"\"\"\n+    Metaclass for :class:`expr`.\n \n-class _ConstExpressionType(type):\n-    \"\"\"Metaclass providing read-only class properties for :class:`expr`.\"\"\"\n+    Currently providing read-only class properties, representing JavaScript constants.\n+    \"\"\"\n \n     @property\n-    def NaN(cls) -> ConstExpression:\n+    def NaN(cls) -> Expression:\n         \"\"\"Not a number (same as JavaScript literal NaN).\"\"\"\n         return ConstExpression(\"NaN\")\n \n     @property\n-    def LN10(cls) -> ConstExpression:\n+    def LN10(cls) -> Expression:\n         \"\"\"The natural log of 10 (alias to Math.LN10).\"\"\"\n         return ConstExpression(\"LN10\")\n \n     @property\n-    def E(cls) -> ConstExpression:\n+    def E(cls) -> Expression:\n         \"\"\"The transcendental number e (alias to Math.E).\"\"\"\n         return ConstExpression(\"E\")\n \n     @property\n-    def LOG10E(cls) -> ConstExpression:\n+    def LOG10E(cls) -> Expression:\n         \"\"\"The base 10 logarithm e (alias to Math.LOG10E).\"\"\"\n         return ConstExpression(\"LOG10E\")\n \n     @property\n-    def LOG2E(cls) -> ConstExpression:\n+    def LOG2E(cls) -> Expression:\n         \"\"\"The base 2 logarithm of e (alias to Math.LOG2E).\"\"\"\n         return ConstExpression(\"LOG2E\")\n \n     @property\n-    def SQRT1_2(cls) -> ConstExpression:\n+    def SQRT1_2(cls) -> Expression:\n         \"\"\"The square root of 0.5 (alias to Math.SQRT1_2).\"\"\"\n         return ConstExpression(\"SQRT1_2\")\n \n     @property\n-    def LN2(cls) -> ConstExpression:\n+    def LN2(cls) -> Expression:\n         \"\"\"The natural log of 2 (alias to Math.LN2).\"\"\"\n         return ConstExpression(\"LN2\")\n \n     @property\n-    def SQRT2(cls) -> ConstExpression:\n+    def SQRT2(cls) -> Expression:\n         \"\"\"The square root of 2 (alias to Math.SQRT1_2).\"\"\"\n         return ConstExpression(\"SQRT2\")\n \n     @property\n-    def PI(cls) -> ConstExpression:\n+    def PI(cls) -> Expression:\n         \"\"\"The transcendental number pi (alias to Math.PI).\"\"\"\n         return ConstExpression(\"PI\")\n \n \n-class expr(_ExprRef, metaclass=_ConstExpressionType):\n-    r\"\"\"\n+class expr(_ExprRef, metaclass=_ExprMeta):\n+    \"\"\"\n     Utility providing *constants* and *classmethods* to construct expressions.\n \n     `Expressions`_ can be used to write basic formulas that enable custom interactions.\n@@ -110,1321 +121,1803 @@ class expr(_ExprRef, metaclass=_ConstExpressionType):\n \n     @override\n     def __new__(cls: type[_ExprRef], expr: str) -> _ExprRef:  # type: ignore[misc]\n-        # NOTE: `mypy<=1.10.1` is not consistent with typing spec\n-        # https://github.com/python/mypy/issues/1020\n-        # https://docs.python.org/3/reference/datamodel.html#object.__new__\n-        # https://typing.readthedocs.io/en/latest/spec/constructors.html#new-method\n         return _ExprRef(expr=expr)\n \n     @classmethod\n-    def if_(cls, *args) -> FunctionExpression:\n-        \"\"\"\n-        If *test* is truthy, returns *thenValue*. Otherwise, returns *elseValue*.\n-\n-        The *if* function is equivalent to the ternary operator `a ? b : c`.\n-        \"\"\"\n-        return FunctionExpression(\"if\", args)\n+    def isArray(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if ``value`` is an array, false otherwise.\"\"\"\n+        return FunctionExpression(\"isArray\", (value,))\n \n     @classmethod\n-    def isArray(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if *value* is an array, false otherwise.\"\"\"\n-        return FunctionExpression(\"isArray\", args)\n+    def isBoolean(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if ``value`` is a boolean (``true`` or ``false``), false otherwise.\"\"\"\n+        return FunctionExpression(\"isBoolean\", (value,))\n \n     @classmethod\n-    def isBoolean(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if *value* is a boolean (`true` or `false`), false otherwise.\"\"\"\n-        return FunctionExpression(\"isBoolean\", args)\n-\n-    @classmethod\n-    def isDate(cls, *args) -> FunctionExpression:\n+    def isDate(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns true if *value* is a Date object, false otherwise.\n+        Returns true if ``value`` is a Date object, false otherwise.\n \n-        This method will return false for timestamp numbers or date-formatted strings; it recognizes Date objects only.\n+        This method will return false for timestamp numbers or date-formatted strings; it recognizes\n+        Date objects only.\n         \"\"\"\n-        return FunctionExpression(\"isDate\", args)\n+        return FunctionExpression(\"isDate\", (value,))\n \n     @classmethod\n-    def isDefined(cls, *args) -> FunctionExpression:\n+    def isDefined(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns true if *value* is a defined value, false if *value* equals `undefined`.\n+        Returns true if ``value`` is a defined value, false if ``value`` equals ``undefined``.\n \n-        This method will return true for `null` and `NaN` values.\n+        This method will return true for ``null`` and ``NaN`` values.\n         \"\"\"\n-        return FunctionExpression(\"isDefined\", args)\n+        return FunctionExpression(\"isDefined\", (value,))\n \n     @classmethod\n-    def isNumber(cls, *args) -> FunctionExpression:\n+    def isNumber(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns true if *value* is a number, false otherwise.\n+        Returns true if ``value`` is a number, false otherwise.\n \n-        `NaN` and `Infinity` are considered numbers.\n+        ``NaN`` and ``Infinity`` are considered numbers.\n         \"\"\"\n-        return FunctionExpression(\"isNumber\", args)\n+        return FunctionExpression(\"isNumber\", (value,))\n+\n+    @classmethod\n+    def isObject(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if ``value`` is an object (including arrays and Dates), false otherwise.\"\"\"\n+        return FunctionExpression(\"isObject\", (value,))\n \n     @classmethod\n-    def isObject(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if *value* is an object (including arrays and Dates), false otherwise.\"\"\"\n-        return FunctionExpression(\"isObject\", args)\n+    def isRegExp(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if ``value`` is a RegExp (regular expression) object, false otherwise.\"\"\"\n+        return FunctionExpression(\"isRegExp\", (value,))\n \n     @classmethod\n-    def isRegExp(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if *value* is a RegExp (regular expression) object, false otherwise.\"\"\"\n-        return FunctionExpression(\"isRegExp\", args)\n+    def isString(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if ``value`` is a string, false otherwise.\"\"\"\n+        return FunctionExpression(\"isString\", (value,))\n \n     @classmethod\n-    def isString(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if *value* is a string, false otherwise.\"\"\"\n-        return FunctionExpression(\"isString\", args)\n+    def isValid(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if ``value`` is not ``null``, ``undefined``, or ``NaN``, false otherwise.\"\"\"\n+        return FunctionExpression(\"isValid\", (value,))\n \n     @classmethod\n-    def isValid(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if *value* is not `null`, `undefined`, or `NaN`, false otherwise.\"\"\"\n-        return FunctionExpression(\"isValid\", args)\n+    def toBoolean(cls, value: IntoExpression, /) -> Expression:\n+        \"\"\"\n+        Coerces the input ``value`` to a string.\n+\n+        Null values and empty strings are mapped to ``null``.\n+        \"\"\"\n+        return FunctionExpression(\"toBoolean\", (value,))\n \n     @classmethod\n-    def toBoolean(cls, *args) -> FunctionExpression:\n+    def toDate(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Coerces the input *value* to a string.\n+        Coerces the input ``value`` to a Date instance.\n \n-        Null values and empty strings are mapped to `null`.\n+        Null values and empty strings are mapped to ``null``. If an optional *parser* function is\n+        provided, it is used to perform date parsing, otherwise ``Date.parse`` is used. Be aware\n+        that ``Date.parse`` has different implementations across browsers!\n         \"\"\"\n-        return FunctionExpression(\"toBoolean\", args)\n+        return FunctionExpression(\"toDate\", (value,))\n \n     @classmethod\n-    def toDate(cls, *args) -> FunctionExpression:\n+    def toNumber(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Coerces the input *value* to a Date instance.\n+        Coerces the input ``value`` to a number.\n \n-        Null values and empty strings are mapped to `null`.\n-        If an optional *parser* function is provided, it is used to perform date parsing, otherwise `Date.parse` is used.\n-        Be aware that `Date.parse` has different implementations across browsers!\n+        Null values and empty strings are mapped to ``null``.\n         \"\"\"\n-        return FunctionExpression(\"toDate\", args)\n+        return FunctionExpression(\"toNumber\", (value,))\n \n     @classmethod\n-    def toNumber(cls, *args) -> FunctionExpression:\n+    def toString(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Coerces the input *value* to a number.\n+        Coerces the input ``value`` to a string.\n \n-        Null values and empty strings are mapped to `null`.\n+        Null values and empty strings are mapped to ``null``.\n         \"\"\"\n-        return FunctionExpression(\"toNumber\", args)\n+        return FunctionExpression(\"toString\", (value,))\n \n     @classmethod\n-    def toString(cls, *args) -> FunctionExpression:\n+    def if_(\n+        cls,\n+        test: IntoExpression,\n+        thenValue: IntoExpression,\n+        elseValue: IntoExpression,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Coerces the input *value* to a string.\n+        If ``test`` is truthy, returns ``thenValue``.\n \n-        Null values and empty strings are mapped to `null`.\n+        Otherwise, returns ``elseValue``. The *if* function is equivalent to the ternary operator\n+        ``a ? b : c``.\n         \"\"\"\n-        return FunctionExpression(\"toString\", args)\n+        return FunctionExpression(\"if\", (test, thenValue, elseValue))\n \n     @classmethod\n-    def isNaN(cls, *args) -> FunctionExpression:\n+    def isNaN(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns true if *value* is not a number.\n+        Returns true if ``value`` is not a number.\n+\n+        Same as JavaScript's `Number.isNaN`_.\n \n-        Same as JavaScript's `isNaN`.\n+        .. _Number.isNaN:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/isNan\n         \"\"\"\n-        return FunctionExpression(\"isNaN\", args)\n+        return FunctionExpression(\"isNaN\", (value,))\n \n     @classmethod\n-    def isFinite(cls, *args) -> FunctionExpression:\n+    def isFinite(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns true if *value* is a finite number.\n+        Returns true if ``value`` is a finite number.\n+\n+        Same as JavaScript's `Number.isFinite`_.\n \n-        Same as JavaScript's `isFinite`.\n+        .. _Number.isFinite:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/isFinite\n         \"\"\"\n-        return FunctionExpression(\"isFinite\", args)\n+        return FunctionExpression(\"isFinite\", (value,))\n \n     @classmethod\n-    def abs(cls, *args) -> FunctionExpression:\n+    def abs(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the absolute value of *value*.\n+        Returns the absolute value of ``value``.\n \n-        Same as JavaScript's `Math.abs`.\n+        Same as JavaScript's `Math.abs`_.\n+\n+        .. _Math.abs:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/abs\n         \"\"\"\n-        return FunctionExpression(\"abs\", args)\n+        return FunctionExpression(\"abs\", (value,))\n \n     @classmethod\n-    def acos(cls, *args) -> FunctionExpression:\n+    def acos(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Trigonometric arccosine.\n \n-        Same as JavaScript's `Math.acos`.\n+        Same as JavaScript's `Math.acos`_.\n+\n+        .. _Math.acos:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/acos\n         \"\"\"\n-        return FunctionExpression(\"acos\", args)\n+        return FunctionExpression(\"acos\", (value,))\n \n     @classmethod\n-    def asin(cls, *args) -> FunctionExpression:\n+    def asin(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Trigonometric arcsine.\n \n-        Same as JavaScript's `Math.asin`.\n+        Same as JavaScript's `Math.asin`_.\n+\n+        .. _Math.asin:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/asin\n         \"\"\"\n-        return FunctionExpression(\"asin\", args)\n+        return FunctionExpression(\"asin\", (value,))\n \n     @classmethod\n-    def atan(cls, *args) -> FunctionExpression:\n+    def atan(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Trigonometric arctangent.\n \n-        Same as JavaScript's `Math.atan`.\n+        Same as JavaScript's `Math.atan`_.\n+\n+        .. _Math.atan:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/atan\n         \"\"\"\n-        return FunctionExpression(\"atan\", args)\n+        return FunctionExpression(\"atan\", (value,))\n \n     @classmethod\n-    def atan2(cls, *args) -> FunctionExpression:\n+    def atan2(cls, dy: IntoExpression, dx: IntoExpression, /) -> Expression:\n         \"\"\"\n         Returns the arctangent of *dy / dx*.\n \n-        Same as JavaScript's `Math.atan2`.\n+        Same as JavaScript's `Math.atan2`_.\n+\n+        .. _Math.atan2:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/atan2\n         \"\"\"\n-        return FunctionExpression(\"atan2\", args)\n+        return FunctionExpression(\"atan2\", (dy, dx))\n \n     @classmethod\n-    def ceil(cls, *args) -> FunctionExpression:\n+    def ceil(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Rounds *value* to the nearest integer of equal or greater value.\n+        Rounds ``value`` to the nearest integer of equal or greater value.\n+\n+        Same as JavaScript's `Math.ceil`_.\n \n-        Same as JavaScript's `Math.ceil`.\n+        .. _Math.ceil:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/ceil\n         \"\"\"\n-        return FunctionExpression(\"ceil\", args)\n+        return FunctionExpression(\"ceil\", (value,))\n \n     @classmethod\n-    def clamp(cls, *args) -> FunctionExpression:\n-        \"\"\"Restricts *value* to be between the specified *min* and *max*.\"\"\"\n-        return FunctionExpression(\"clamp\", args)\n+    def clamp(\n+        cls, value: IntoExpression, min: IntoExpression, max: IntoExpression, /\n+    ) -> Expression:\n+        \"\"\"Restricts ``value`` to be between the specified ``min`` and ``max``.\"\"\"\n+        return FunctionExpression(\"clamp\", (value, min, max))\n \n     @classmethod\n-    def cos(cls, *args) -> FunctionExpression:\n+    def cos(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Trigonometric cosine.\n \n-        Same as JavaScript's `Math.cos`.\n+        Same as JavaScript's `Math.cos`_.\n+\n+        .. _Math.cos:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/cos\n         \"\"\"\n-        return FunctionExpression(\"cos\", args)\n+        return FunctionExpression(\"cos\", (value,))\n \n     @classmethod\n-    def exp(cls, *args) -> FunctionExpression:\n+    def exp(cls, exponent: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the value of *e* raised to the provided *exponent*.\n+        Returns the value of *e* raised to the provided ``exponent``.\n+\n+        Same as JavaScript's `Math.exp`_.\n \n-        Same as JavaScript's `Math.exp`.\n+        .. _Math.exp:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/exp\n         \"\"\"\n-        return FunctionExpression(\"exp\", args)\n+        return FunctionExpression(\"exp\", (exponent,))\n \n     @classmethod\n-    def floor(cls, *args) -> FunctionExpression:\n+    def floor(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Rounds *value* to the nearest integer of equal or lower value.\n+        Rounds ``value`` to the nearest integer of equal or lower value.\n+\n+        Same as JavaScript's `Math.floor`_.\n \n-        Same as JavaScript's `Math.floor`.\n+        .. _Math.floor:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/floor\n         \"\"\"\n-        return FunctionExpression(\"floor\", args)\n+        return FunctionExpression(\"floor\", (value,))\n \n     @classmethod\n-    def hypot(cls, *args) -> FunctionExpression:\n+    def hypot(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Returns the square root of the sum of squares of its arguments.\n \n-        Same as JavaScript's `Math.hypot`.\n+        Same as JavaScript's `Math.hypot`_.\n+\n+        .. _Math.hypot:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/hypot\n         \"\"\"\n-        return FunctionExpression(\"hypot\", args)\n+        return FunctionExpression(\"hypot\", (value,))\n \n     @classmethod\n-    def log(cls, *args) -> FunctionExpression:\n+    def log(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the natural logarithm of *value*.\n+        Returns the natural logarithm of ``value``.\n+\n+        Same as JavaScript's `Math.log`_.\n \n-        Same as JavaScript's `Math.log`.\n+        .. _Math.log:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/log\n         \"\"\"\n-        return FunctionExpression(\"log\", args)\n+        return FunctionExpression(\"log\", (value,))\n \n     @classmethod\n-    def max(cls, *args) -> FunctionExpression:\n+    def max(\n+        cls, value1: IntoExpression, value2: IntoExpression, *args: Any\n+    ) -> Expression:\n         \"\"\"\n         Returns the maximum argument value.\n \n-        Same as JavaScript's `Math.max`.\n+        Same as JavaScript's `Math.max`_.\n+\n+        .. _Math.max:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/max\n         \"\"\"\n-        return FunctionExpression(\"max\", args)\n+        return FunctionExpression(\"max\", (value1, value2, *args))\n \n     @classmethod\n-    def min(cls, *args) -> FunctionExpression:\n+    def min(\n+        cls, value1: IntoExpression, value2: IntoExpression, *args: Any\n+    ) -> Expression:\n         \"\"\"\n         Returns the minimum argument value.\n \n-        Same as JavaScript's `Math.min`.\n+        Same as JavaScript's `Math.min`_.\n+\n+        .. _Math.min:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/min\n         \"\"\"\n-        return FunctionExpression(\"min\", args)\n+        return FunctionExpression(\"min\", (value1, value2, *args))\n \n     @classmethod\n-    def pow(cls, *args) -> FunctionExpression:\n+    def pow(cls, value: IntoExpression, exponent: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns *value* raised to the given *exponent*.\n+        Returns ``value`` raised to the given ``exponent``.\n+\n+        Same as JavaScript's `Math.pow`_.\n \n-        Same as JavaScript's `Math.pow`.\n+        .. _Math.pow:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/pow\n         \"\"\"\n-        return FunctionExpression(\"pow\", args)\n+        return FunctionExpression(\"pow\", (value, exponent))\n \n     @classmethod\n-    def random(cls, *args) -> FunctionExpression:\n+    def random(cls) -> Expression:\n         \"\"\"\n-        Returns a pseudo-random number in the range `[0, 1]`.\n+        Returns a pseudo-random number in the range [0,1).\n+\n+        Same as JavaScript's `Math.random`_.\n \n-        Same as JavaScript's `Math.random`.\n+        .. _Math.random:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random\n         \"\"\"\n-        return FunctionExpression(\"random\", args)\n+        return FunctionExpression(\"random\", ())\n \n     @classmethod\n-    def round(cls, *args) -> FunctionExpression:\n+    def round(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Rounds *value* to the nearest integer.\n+        Rounds ``value`` to the nearest integer.\n \n-        Same as JavaScript's `Math.round`.\n+        Same as JavaScript's `Math.round`_.\n+\n+        .. _Math.round:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/round\n         \"\"\"\n-        return FunctionExpression(\"round\", args)\n+        return FunctionExpression(\"round\", (value,))\n \n     @classmethod\n-    def sin(cls, *args) -> FunctionExpression:\n+    def sin(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Trigonometric sine.\n \n-        Same as JavaScript's `Math.sin`.\n+        Same as JavaScript's `Math.sin`_.\n+\n+        .. _Math.sin:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/sin\n         \"\"\"\n-        return FunctionExpression(\"sin\", args)\n+        return FunctionExpression(\"sin\", (value,))\n \n     @classmethod\n-    def sqrt(cls, *args) -> FunctionExpression:\n+    def sqrt(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Square root function.\n \n-        Same as JavaScript's `Math.sqrt`.\n+        Same as JavaScript's `Math.sqrt`_.\n+\n+        .. _Math.sqrt:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/sqrt\n         \"\"\"\n-        return FunctionExpression(\"sqrt\", args)\n+        return FunctionExpression(\"sqrt\", (value,))\n \n     @classmethod\n-    def tan(cls, *args) -> FunctionExpression:\n+    def tan(cls, value: IntoExpression, /) -> Expression:\n         \"\"\"\n         Trigonometric tangent.\n \n-        Same as JavaScript's `Math.tan`.\n+        Same as JavaScript's `Math.tan`_.\n+\n+        .. _Math.tan:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/tan\n         \"\"\"\n-        return FunctionExpression(\"tan\", args)\n+        return FunctionExpression(\"tan\", (value,))\n \n     @classmethod\n-    def sampleNormal(cls, *args) -> FunctionExpression:\n+    def sampleNormal(\n+        cls, mean: IntoExpression = None, stdev: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a sample from a univariate `normal (Gaussian) probability distribution <https://en.wikipedia.org/wiki/Normal_distribution>`__ with specified *mean* and standard deviation *stdev*.\n+        Returns a sample from a univariate `normal (Gaussian) probability distribution`_ with specified ``mean`` and standard deviation ``stdev``.\n+\n+        If unspecified, the mean defaults to ``0`` and the standard deviation defaults to ``1``.\n \n-        If unspecified, the mean defaults to `0` and the standard deviation defaults to `1`.\n+        .. _normal (Gaussian) probability distribution:\n+            https://en.wikipedia.org/wiki/Normal_distribution\n         \"\"\"\n-        return FunctionExpression(\"sampleNormal\", args)\n+        return FunctionExpression(\"sampleNormal\", (mean, stdev))\n \n     @classmethod\n-    def cumulativeNormal(cls, *args) -> FunctionExpression:\n+    def cumulativeNormal(\n+        cls,\n+        value: IntoExpression,\n+        mean: IntoExpression = None,\n+        stdev: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the value of the `cumulative distribution function <https://en.wikipedia.org/wiki/Cumulative_distribution_function>`__ at the given input domain *value* for a normal distribution with specified *mean* and standard deviation *stdev*.\n+        Returns the value of the `cumulative distribution function`_ at the given input domain ``value`` for a normal distribution with specified ``mean`` and standard deviation ``stdev``.\n+\n+        If unspecified, the mean defaults to ``0`` and the standard deviation defaults to ``1``.\n \n-        If unspecified, the mean defaults to `0` and the standard deviation defaults to `1`.\n+        .. _cumulative distribution function:\n+            https://en.wikipedia.org/wiki/Cumulative_distribution_function\n         \"\"\"\n-        return FunctionExpression(\"cumulativeNormal\", args)\n+        return FunctionExpression(\"cumulativeNormal\", (value, mean, stdev))\n \n     @classmethod\n-    def densityNormal(cls, *args) -> FunctionExpression:\n+    def densityNormal(\n+        cls,\n+        value: IntoExpression,\n+        mean: IntoExpression = None,\n+        stdev: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the value of the `probability density function <https://en.wikipedia.org/wiki/Probability_density_function>`__ at the given input domain *value*, for a normal distribution with specified *mean* and standard deviation *stdev*.\n+        Returns the value of the `probability density function`_ at the given input domain ``value``, for a normal distribution with specified ``mean`` and standard deviation ``stdev``.\n \n-        If unspecified, the mean defaults to `0` and the standard deviation defaults to `1`.\n+        If unspecified, the mean defaults to ``0`` and the standard deviation defaults to ``1``.\n+\n+        .. _probability density function:\n+            https://en.wikipedia.org/wiki/Probability_density_function\n         \"\"\"\n-        return FunctionExpression(\"densityNormal\", args)\n+        return FunctionExpression(\"densityNormal\", (value, mean, stdev))\n \n     @classmethod\n-    def quantileNormal(cls, *args) -> FunctionExpression:\n+    def quantileNormal(\n+        cls,\n+        probability: IntoExpression,\n+        mean: IntoExpression = None,\n+        stdev: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the quantile value (the inverse of the `cumulative distribution function <https://en.wikipedia.org/wiki/Cumulative_distribution_function)>`__ for the given input *probability*, for a normal distribution with specified *mean* and standard deviation *stdev*.\n+        Returns the quantile value (the inverse of the `cumulative distribution function`_) for the given input ``probability``, for a normal distribution with specified ``mean`` and standard deviation ``stdev``.\n+\n+        If unspecified, the mean defaults to ``0`` and the standard deviation defaults to ``1``.\n \n-        If unspecified, the mean defaults to `0` and the standard deviation defaults to `1`.\n+        .. _cumulative distribution function:\n+            https://en.wikipedia.org/wiki/Cumulative_distribution_function\n         \"\"\"\n-        return FunctionExpression(\"quantileNormal\", args)\n+        return FunctionExpression(\"quantileNormal\", (probability, mean, stdev))\n \n     @classmethod\n-    def sampleLogNormal(cls, *args) -> FunctionExpression:\n+    def sampleLogNormal(\n+        cls, mean: IntoExpression = None, stdev: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a sample from a univariate `log-normal probability distribution <https://en.wikipedia.org/wiki/Log-normal_distribution>`__ with specified log *mean* and log standard deviation *stdev*.\n+        Returns a sample from a univariate `log-normal probability distribution`_ with specified log ``mean`` and log standard deviation ``stdev``.\n+\n+        If unspecified, the log mean defaults to ``0`` and the log standard deviation defaults to\n+        ``1``.\n \n-        If unspecified, the log mean defaults to `0` and the log standard deviation defaults to `1`.\n+        .. _log-normal probability distribution:\n+            https://en.wikipedia.org/wiki/Log-normal_distribution\n         \"\"\"\n-        return FunctionExpression(\"sampleLogNormal\", args)\n+        return FunctionExpression(\"sampleLogNormal\", (mean, stdev))\n \n     @classmethod\n-    def cumulativeLogNormal(cls, *args) -> FunctionExpression:\n+    def cumulativeLogNormal(\n+        cls,\n+        value: IntoExpression,\n+        mean: IntoExpression = None,\n+        stdev: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the value of the `cumulative distribution function <https://en.wikipedia.org/wiki/Cumulative_distribution_function>`__ at the given input domain *value* for a log-normal distribution with specified log *mean* and log standard deviation *stdev*.\n+        Returns the value of the `cumulative distribution function`_ at the given input domain ``value`` for a log-normal distribution with specified log ``mean`` and log standard deviation ``stdev``.\n \n-        If unspecified, the log mean defaults to `0` and the log standard deviation defaults to `1`.\n+        If unspecified, the log mean defaults to ``0`` and the log standard deviation defaults to\n+        ``1``.\n+\n+        .. _cumulative distribution function:\n+            https://en.wikipedia.org/wiki/Cumulative_distribution_function\n         \"\"\"\n-        return FunctionExpression(\"cumulativeLogNormal\", args)\n+        return FunctionExpression(\"cumulativeLogNormal\", (value, mean, stdev))\n \n     @classmethod\n-    def densityLogNormal(cls, *args) -> FunctionExpression:\n+    def densityLogNormal(\n+        cls,\n+        value: IntoExpression,\n+        mean: IntoExpression = None,\n+        stdev: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the value of the `probability density function <https://en.wikipedia.org/wiki/Probability_density_function>`__ at the given input domain *value*, for a log-normal distribution with specified log *mean* and log standard deviation *stdev*.\n+        Returns the value of the `probability density function`_ at the given input domain ``value``, for a log-normal distribution with specified log ``mean`` and log standard deviation ``stdev``.\n+\n+        If unspecified, the log mean defaults to ``0`` and the log standard deviation defaults to\n+        ``1``.\n \n-        If unspecified, the log mean defaults to `0` and the log standard deviation defaults to `1`.\n+        .. _probability density function:\n+            https://en.wikipedia.org/wiki/Probability_density_function\n         \"\"\"\n-        return FunctionExpression(\"densityLogNormal\", args)\n+        return FunctionExpression(\"densityLogNormal\", (value, mean, stdev))\n \n     @classmethod\n-    def quantileLogNormal(cls, *args) -> FunctionExpression:\n+    def quantileLogNormal(\n+        cls,\n+        probability: IntoExpression,\n+        mean: IntoExpression = None,\n+        stdev: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the quantile value (the inverse of the `cumulative distribution function <https://en.wikipedia.org/wiki/Cumulative_distribution_function)>`__ for the given input *probability*, for a log-normal distribution with specified log *mean* and log standard deviation *stdev*.\n+        Returns the quantile value (the inverse of the `cumulative distribution function`_) for the given input ``probability``, for a log-normal distribution with specified log ``mean`` and log standard deviation ``stdev``.\n+\n+        If unspecified, the log mean defaults to ``0`` and the log standard deviation defaults to\n+        ``1``.\n \n-        If unspecified, the log mean defaults to `0` and the log standard deviation defaults to `1`.\n+        .. _cumulative distribution function:\n+            https://en.wikipedia.org/wiki/Cumulative_distribution_function\n         \"\"\"\n-        return FunctionExpression(\"quantileLogNormal\", args)\n+        return FunctionExpression(\"quantileLogNormal\", (probability, mean, stdev))\n \n     @classmethod\n-    def sampleUniform(cls, *args) -> FunctionExpression:\n+    def sampleUniform(\n+        cls, min: IntoExpression = None, max: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a sample from a univariate `continuous uniform probability distribution <https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)>`__ over the interval `[min, max]`.\n+        Returns a sample from a univariate `continuous uniform probability distribution`_) over the interval [``min``, ``max``).\n \n-        If unspecified, *min* defaults to `0` and *max* defaults to `1`.\n-        If only one argument is provided, it is interpreted as the *max* value.\n+        If unspecified, ``min`` defaults to ``0`` and ``max`` defaults to ``1``. If only one\n+        argument is provided, it is interpreted as the ``max`` value.\n+\n+        .. _continuous uniform probability distribution:\n+            https://en.wikipedia.org/wiki/Uniform_distribution_(continuous\n         \"\"\"\n-        return FunctionExpression(\"sampleUniform\", args)\n+        return FunctionExpression(\"sampleUniform\", (min, max))\n \n     @classmethod\n-    def cumulativeUniform(cls, *args) -> FunctionExpression:\n+    def cumulativeUniform(\n+        cls,\n+        value: IntoExpression,\n+        min: IntoExpression = None,\n+        max: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the value of the `cumulative distribution function <https://en.wikipedia.org/wiki/Cumulative_distribution_function>`__ at the given input domain *value* for a uniform distribution over the interval `[min, max]`.\n+        Returns the value of the `cumulative distribution function`_ at the given input domain ``value`` for a uniform distribution over the interval [``min``, ``max``).\n+\n+        If unspecified, ``min`` defaults to ``0`` and ``max`` defaults to ``1``. If only one\n+        argument is provided, it is interpreted as the ``max`` value.\n \n-        If unspecified, *min* defaults to `0` and *max* defaults to `1`.\n-        If only one argument is provided, it is interpreted as the *max* value.\n+        .. _cumulative distribution function:\n+            https://en.wikipedia.org/wiki/Cumulative_distribution_function\n         \"\"\"\n-        return FunctionExpression(\"cumulativeUniform\", args)\n+        return FunctionExpression(\"cumulativeUniform\", (value, min, max))\n \n     @classmethod\n-    def densityUniform(cls, *args) -> FunctionExpression:\n+    def densityUniform(\n+        cls,\n+        value: IntoExpression,\n+        min: IntoExpression = None,\n+        max: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the value of the `probability density function <https://en.wikipedia.org/wiki/Probability_density_function>`__ at the given input domain *value*,  for a uniform distribution over the interval `[min, max]`.\n+        Returns the value of the `probability density function`_ at the given input domain ``value``,  for a uniform distribution over the interval [``min``, ``max``).\n \n-        If unspecified, *min* defaults to `0` and *max* defaults to `1`.\n-        If only one argument is provided, it is interpreted as the *max* value.\n+        If unspecified, ``min`` defaults to ``0`` and ``max`` defaults to ``1``. If only one\n+        argument is provided, it is interpreted as the ``max`` value.\n+\n+        .. _probability density function:\n+            https://en.wikipedia.org/wiki/Probability_density_function\n         \"\"\"\n-        return FunctionExpression(\"densityUniform\", args)\n+        return FunctionExpression(\"densityUniform\", (value, min, max))\n \n     @classmethod\n-    def quantileUniform(cls, *args) -> FunctionExpression:\n+    def quantileUniform(\n+        cls,\n+        probability: IntoExpression,\n+        min: IntoExpression = None,\n+        max: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the quantile value (the inverse of the `cumulative distribution function <https://en.wikipedia.org/wiki/Cumulative_distribution_function)>`__ for the given input *probability*, for a uniform distribution over the interval `[min, max]`.\n+        Returns the quantile value (the inverse of the `cumulative distribution function`_) for the given input ``probability``,  for a uniform distribution over the interval [``min``, ``max``).\n+\n+        If unspecified, ``min`` defaults to ``0`` and ``max`` defaults to ``1``. If only one\n+        argument is provided, it is interpreted as the ``max`` value.\n \n-        If unspecified, *min* defaults to `0` and *max* defaults to `1`.\n-        If only one argument is provided, it is interpreted as the *max* value.\n+        .. _cumulative distribution function:\n+            https://en.wikipedia.org/wiki/Cumulative_distribution_function\n         \"\"\"\n-        return FunctionExpression(\"quantileUniform\", args)\n+        return FunctionExpression(\"quantileUniform\", (probability, min, max))\n \n     @classmethod\n-    def now(cls, *args) -> FunctionExpression:\n+    def now(cls) -> Expression:\n         \"\"\"Returns the timestamp for the current time.\"\"\"\n-        return FunctionExpression(\"now\", args)\n+        return FunctionExpression(\"now\", ())\n \n     @classmethod\n-    def datetime(cls, *args) -> FunctionExpression:\n+    def datetime(\n+        cls,\n+        year: IntoExpression,\n+        month: IntoExpression,\n+        day: IntoExpression = None,\n+        hour: IntoExpression = None,\n+        min: IntoExpression = None,\n+        sec: IntoExpression = None,\n+        millisec: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns a new `Date` instance.\n+        Returns a new ``Date`` instance.\n \n-        The *month* is 0-based, such that `1` represents February.\n+        The ``month`` is 0-based, such that ``1`` represents February.\n         \"\"\"\n-        return FunctionExpression(\"datetime\", args)\n+        return FunctionExpression(\n+            \"datetime\", (year, month, day, hour, min, sec, millisec)\n+        )\n \n     @classmethod\n-    def date(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the day of the month for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"date\", args)\n+    def date(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the day of the month for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"date\", (datetime,))\n \n     @classmethod\n-    def day(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the day of the week for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"day\", args)\n+    def day(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the day of the week for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"day\", (datetime,))\n \n     @classmethod\n-    def dayofyear(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the one-based day of the year for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"dayofyear\", args)\n+    def dayofyear(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the one-based day of the year for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"dayofyear\", (datetime,))\n \n     @classmethod\n-    def year(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the year for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"year\", args)\n+    def year(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the year for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"year\", (datetime,))\n \n     @classmethod\n-    def quarter(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the quarter of the year (0-3) for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"quarter\", args)\n+    def quarter(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the quarter of the year (0-3) for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"quarter\", (datetime,))\n \n     @classmethod\n-    def month(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the (zero-based) month for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"month\", args)\n+    def month(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the (zero-based) month for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"month\", (datetime,))\n \n     @classmethod\n-    def week(cls, *args) -> FunctionExpression:\n+    def week(cls, date: IntoExpression, /) -> Expression:\n         \"\"\"\n         Returns the week number of the year for the given *datetime*, in local time.\n \n-        This function assumes Sunday-based weeks.\n-        Days before the first Sunday of the year are considered to be in week 0,\n-        the first Sunday of the year is the start of week 1,\n-        the second Sunday week 2, etc.\n+        This function assumes Sunday-based weeks. Days before the first Sunday of the year are\n+        considered to be in week 0, the first Sunday of the year is the start of week 1, the second\n+        Sunday week 2, *etc.*.\n         \"\"\"\n-        return FunctionExpression(\"week\", args)\n+        return FunctionExpression(\"week\", (date,))\n \n     @classmethod\n-    def hours(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the hours component for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"hours\", args)\n+    def hours(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the hours component for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"hours\", (datetime,))\n \n     @classmethod\n-    def minutes(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the minutes component for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"minutes\", args)\n+    def minutes(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the minutes component for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"minutes\", (datetime,))\n \n     @classmethod\n-    def seconds(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the seconds component for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"seconds\", args)\n+    def seconds(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the seconds component for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"seconds\", (datetime,))\n \n     @classmethod\n-    def milliseconds(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the milliseconds component for the given *datetime* value, in local time.\"\"\"\n-        return FunctionExpression(\"milliseconds\", args)\n+    def milliseconds(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the milliseconds component for the given ``datetime`` value, in local time.\"\"\"\n+        return FunctionExpression(\"milliseconds\", (datetime,))\n \n     @classmethod\n-    def time(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the epoch-based timestamp for the given *datetime* value.\"\"\"\n-        return FunctionExpression(\"time\", args)\n+    def time(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the epoch-based timestamp for the given ``datetime`` value.\"\"\"\n+        return FunctionExpression(\"time\", (datetime,))\n \n     @classmethod\n-    def timezoneoffset(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the timezone offset from the local timezone to UTC for the given *datetime* value.\"\"\"\n-        return FunctionExpression(\"timezoneoffset\", args)\n+    def timezoneoffset(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the timezone offset from the local timezone to UTC for the given ``datetime`` value.\"\"\"\n+        return FunctionExpression(\"timezoneoffset\", (datetime,))\n \n     @classmethod\n-    def timeOffset(cls, *args) -> FunctionExpression:\n+    def timeOffset(\n+        cls, unit: IntoExpression, date: IntoExpression, step: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a new `Date` instance that offsets the given *date* by the specified time `unit <https://vega.github.io/vega/docs/api/time/#time-units>`__ in the local timezone.\n+        Returns a new ``Date`` instance that offsets the given ``date`` by the specified time `*unit*`_ in the local timezone.\n+\n+        The optional ``step`` argument indicates the number of time unit steps to offset by (default\n+        1).\n \n-        The optional *step* argument indicates the number of time unit steps to offset by (default 1).\n+        .. _*unit*:\n+            https://vega.github.io/vega/docs/api/time/#time-units\n         \"\"\"\n-        return FunctionExpression(\"timeOffset\", args)\n+        return FunctionExpression(\"timeOffset\", (unit, date, step))\n \n     @classmethod\n-    def timeSequence(cls, *args) -> FunctionExpression:\n+    def timeSequence(\n+        cls,\n+        unit: IntoExpression,\n+        start: IntoExpression,\n+        stop: IntoExpression,\n+        step: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns an array of `Date` instances from *start* (inclusive) to *stop* (exclusive), with each entry separated by the given time `unit <https://vega.github.io/vega/docs/api/time/#time-units>`__ in the local timezone.\n+        Returns an array of ``Date`` instances from ``start`` (inclusive) to ``stop`` (exclusive), with each entry separated by the given time `*unit*`_ in the local timezone.\n \n-        The optional *step* argument indicates the number of time unit steps to take between each sequence entry (default 1).\n+        The optional ``step`` argument indicates the number of time unit steps to take between each\n+        sequence entry (default 1).\n+\n+        .. _*unit*:\n+            https://vega.github.io/vega/docs/api/time/#time-units\n         \"\"\"\n-        return FunctionExpression(\"timeSequence\", args)\n+        return FunctionExpression(\"timeSequence\", (unit, start, stop, step))\n \n     @classmethod\n-    def utc(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns a timestamp for the given UTC date. The *month* is 0-based, such that `1` represents February.\"\"\"\n-        return FunctionExpression(\"utc\", args)\n+    def utc(\n+        cls,\n+        year: IntoExpression,\n+        month: IntoExpression,\n+        day: IntoExpression = None,\n+        hour: IntoExpression = None,\n+        min: IntoExpression = None,\n+        sec: IntoExpression = None,\n+        millisec: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n+        \"\"\"\n+        Returns a timestamp for the given UTC date.\n+\n+        The ``month`` is 0-based, such that ``1`` represents February.\n+        \"\"\"\n+        return FunctionExpression(\"utc\", (year, month, day, hour, min, sec, millisec))\n \n     @classmethod\n-    def utcdate(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the day of the month for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcdate\", args)\n+    def utcdate(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the day of the month for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcdate\", (datetime,))\n \n     @classmethod\n-    def utcday(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the day of the week for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcday\", args)\n+    def utcday(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the day of the week for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcday\", (datetime,))\n \n     @classmethod\n-    def utcdayofyear(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the one-based day of the year for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcdayofyear\", args)\n+    def utcdayofyear(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the one-based day of the year for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcdayofyear\", (datetime,))\n \n     @classmethod\n-    def utcyear(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the year for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcyear\", args)\n+    def utcyear(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the year for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcyear\", (datetime,))\n \n     @classmethod\n-    def utcquarter(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the quarter of the year (0-3) for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcquarter\", args)\n+    def utcquarter(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the quarter of the year (0-3) for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcquarter\", (datetime,))\n \n     @classmethod\n-    def utcmonth(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the (zero-based) month for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcmonth\", args)\n+    def utcmonth(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the (zero-based) month for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcmonth\", (datetime,))\n \n     @classmethod\n-    def utcweek(cls, *args) -> FunctionExpression:\n+    def utcweek(cls, date: IntoExpression, /) -> Expression:\n         \"\"\"\n         Returns the week number of the year for the given *datetime*, in UTC time.\n \n-        This function assumes Sunday-based weeks.\n-        Days before the first Sunday of the year are considered to be in week 0,\n-        the first Sunday of the year is the start of week 1,\n-        the second Sunday week 2, etc.\n+        This function assumes Sunday-based weeks. Days before the first Sunday of the year are\n+        considered to be in week 0, the first Sunday of the year is the start of week 1, the second\n+        Sunday week 2, *etc.*.\n         \"\"\"\n-        return FunctionExpression(\"utcweek\", args)\n+        return FunctionExpression(\"utcweek\", (date,))\n \n     @classmethod\n-    def utchours(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the hours component for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utchours\", args)\n+    def utchours(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the hours component for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utchours\", (datetime,))\n \n     @classmethod\n-    def utcminutes(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the minutes component for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcminutes\", args)\n+    def utcminutes(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the minutes component for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcminutes\", (datetime,))\n \n     @classmethod\n-    def utcseconds(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the seconds component for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcseconds\", args)\n+    def utcseconds(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the seconds component for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcseconds\", (datetime,))\n \n     @classmethod\n-    def utcmilliseconds(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the milliseconds component for the given *datetime* value, in UTC time.\"\"\"\n-        return FunctionExpression(\"utcmilliseconds\", args)\n+    def utcmilliseconds(cls, datetime: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the milliseconds component for the given ``datetime`` value, in UTC time.\"\"\"\n+        return FunctionExpression(\"utcmilliseconds\", (datetime,))\n \n     @classmethod\n-    def utcOffset(cls, *args) -> FunctionExpression:\n+    def utcOffset(\n+        cls, unit: IntoExpression, date: IntoExpression, step: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a new `Date` instance that offsets the given *date* by the specified time `unit <https://vega.github.io/vega/docs/api/time/#time-units>`__ in UTC time.\n+        Returns a new ``Date`` instance that offsets the given ``date`` by the specified time `*unit*`_ in UTC time.\n+\n+        The optional ``step`` argument indicates the number of time unit steps to offset by (default\n+        1).\n \n-        The optional *step* argument indicates the number of time unit steps to offset by (default 1).\n+        .. _*unit*:\n+            https://vega.github.io/vega/docs/api/time/#time-units\n         \"\"\"\n-        return FunctionExpression(\"utcOffset\", args)\n+        return FunctionExpression(\"utcOffset\", (unit, date, step))\n \n     @classmethod\n-    def utcSequence(cls, *args) -> FunctionExpression:\n+    def utcSequence(\n+        cls,\n+        unit: IntoExpression,\n+        start: IntoExpression,\n+        stop: IntoExpression,\n+        step: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns an array of `Date` instances from *start* (inclusive) to *stop* (exclusive), with each entry separated by the given time `unit <https://vega.github.io/vega/docs/api/time/#time-units>`__ in UTC time.\n+        Returns an array of ``Date`` instances from ``start`` (inclusive) to ``stop`` (exclusive), with each entry separated by the given time `*unit*`_ in UTC time.\n+\n+        The optional ``step`` argument indicates the number of time unit steps to take between each\n+        sequence entry (default 1).\n \n-        The optional *step* argument indicates the number of time unit steps to take between each sequence entry (default 1).\n+        .. _*unit*:\n+            https://vega.github.io/vega/docs/api/time/#time-units\n         \"\"\"\n-        return FunctionExpression(\"utcSequence\", args)\n+        return FunctionExpression(\"utcSequence\", (unit, start, stop, step))\n \n     @classmethod\n-    def extent(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns a new `[min, max]` array with the minimum and maximum values of the input array, ignoring `null`, `undefined`, and `NaN` values.\"\"\"\n-        return FunctionExpression(\"extent\", args)\n+    def extent(cls, array: IntoExpression, /) -> Expression:\n+        \"\"\"Returns a new *[min, max]* array with the minimum and maximum values of the input array, ignoring ``null``, ``undefined``, and ``NaN`` values.\"\"\"\n+        return FunctionExpression(\"extent\", (array,))\n \n     @classmethod\n-    def clampRange(cls, *args) -> FunctionExpression:\n+    def clampRange(\n+        cls, range: IntoExpression, min: IntoExpression, max: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Clamps a two-element *range* array in a span-preserving manner.\n+        Clamps a two-element ``range`` array in a span-preserving manner.\n \n-        If the span of the input *range* is less than `(max - min)` and an endpoint exceeds either the *min* or *max* value,\n-        the range is translated such that the span is preserved and one endpoint touches the boundary of the `[min, max]` range.\n-        If the span exceeds `(max - min)`, the range `[min, max]` is returned.\n+        If the span of the input ``range`` is less than *(max - min)* and an endpoint exceeds either\n+        the ``min`` or ``max`` value, the range is translated such that the span is preserved and\n+        one endpoint touches the boundary of the *[min, max]* range. If the span exceeds *(max -\n+        min)*, the range *[min, max]* is returned.\n         \"\"\"\n-        return FunctionExpression(\"clampRange\", args)\n+        return FunctionExpression(\"clampRange\", (range, min, max))\n \n     @classmethod\n-    def indexof(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the first index of *value* in the input *array*, or the first index of *substring* in the input *string*.\"\"\"\n-        return FunctionExpression(\"indexof\", args)\n+    def indexof(cls, array: IntoExpression, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the first index of ``value`` in the input ``array``.\"\"\"\n+        return FunctionExpression(\"indexof\", (array, value))\n \n     @classmethod\n-    def inrange(cls, *args) -> FunctionExpression:\n-        \"\"\"Tests whether *value* lies within (or is equal to either) the first and last values of the *range* array.\"\"\"\n-        return FunctionExpression(\"inrange\", args)\n+    def inrange(cls, value: IntoExpression, range: IntoExpression, /) -> Expression:\n+        \"\"\"Tests whether ``value`` lies within (or is equal to either) the first and last values of the ``range`` array.\"\"\"\n+        return FunctionExpression(\"inrange\", (value, range))\n \n     @classmethod\n-    def join(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns a new string by concatenating all of the elements of the input *array*, separated by commas or a specified *separator* string.\"\"\"\n-        return FunctionExpression(\"join\", args)\n+    def join(\n+        cls, array: IntoExpression, separator: IntoExpression = None, /\n+    ) -> Expression:\n+        \"\"\"Returns a new string by concatenating all of the elements of the input ``array``, separated by commas or a specified ``separator`` string.\"\"\"\n+        return FunctionExpression(\"join\", (array, separator))\n \n     @classmethod\n-    def lastindexof(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the last index of *value* in the input *array*, or the last index of *substring* in the input *string*.\"\"\"\n-        return FunctionExpression(\"lastindexof\", args)\n+    def lastindexof(cls, array: IntoExpression, value: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the last index of ``value`` in the input ``array``.\"\"\"\n+        return FunctionExpression(\"lastindexof\", (array, value))\n \n     @classmethod\n-    def length(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the length of the input *array*, or the length of the input *string*.\"\"\"\n-        return FunctionExpression(\"length\", args)\n+    def length(cls, array: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the length of the input ``array``.\"\"\"\n+        return FunctionExpression(\"length\", (array,))\n \n     @classmethod\n-    def lerp(cls, *args) -> FunctionExpression:\n+    def lerp(cls, array: IntoExpression, fraction: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the linearly interpolated value between the first and last entries in the *array* for the provided interpolation *fraction* (typically between 0 and 1).\n+        Returns the linearly interpolated value between the first and last entries in the ``array`` for the provided interpolation ``fraction`` (typically between 0 and 1).\n \n-        For example, `lerp([0, 50], 0.5)` returns 25.\n+        For example, ``alt.expr.lerp([0, 50], 0.5)`` returns 25.\n         \"\"\"\n-        return FunctionExpression(\"lerp\", args)\n+        return FunctionExpression(\"lerp\", (array, fraction))\n \n     @classmethod\n-    def peek(cls, *args) -> FunctionExpression:\n+    def peek(cls, array: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the last element in the input *array*.\n+        Returns the last element in the input ``array``.\n \n-        Similar to the built-in `Array.pop` method, except that it does not remove the last element.\n-        This method is a convenient shorthand for `array[array.length - 1]`.\n+        Similar to the built-in ``Array.pop`` method, except that it does not remove the last\n+        element. This method is a convenient shorthand for ``array[array.length - 1]``.\n         \"\"\"\n-        return FunctionExpression(\"peek\", args)\n+        return FunctionExpression(\"peek\", (array,))\n \n     @classmethod\n-    def pluck(cls, *args) -> FunctionExpression:\n+    def pluck(cls, array: IntoExpression, field: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Retrieves the value for the specified *field* from a given *array* of objects.\n+        Retrieves the value for the specified ``field`` from a given ``array`` of objects.\n \n-        The input *field* string may include nested properties (e.g., `foo.bar.bz`).\n+        The input ``field`` string may include nested properties (e.g., ``foo.bar.bz``).\n         \"\"\"\n-        return FunctionExpression(\"pluck\", args)\n+        return FunctionExpression(\"pluck\", (array, field))\n \n     @classmethod\n-    def reverse(cls, *args) -> FunctionExpression:\n+    def reverse(cls, array: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns a new array with elements in a reverse order of the input *array*.\n+        Returns a new array with elements in a reverse order of the input ``array``.\n \n         The first array element becomes the last, and the last array element becomes the first.\n         \"\"\"\n-        return FunctionExpression(\"reverse\", args)\n+        return FunctionExpression(\"reverse\", (array,))\n \n     @classmethod\n-    def sequence(cls, *args) -> FunctionExpression:\n-        r\"\"\"\n+    def sequence(cls, *args: Any) -> Expression:\n+        \"\"\"\n         Returns an array containing an arithmetic sequence of numbers.\n \n-        If *step* is omitted, it defaults to 1.\n-        If *start* is omitted, it defaults to 0.\n-\n-        The *stop* value is exclusive; it is not included in the result.\n-        If *step* is positive, the last element is the largest `start + i * step` less than *stop*;\n-        if *step* is negative, the last element is the smallest `start + i * step` greater than *stop*.\n-\n-        If the returned array would contain an infinite number of values, an empty range is returned.\n-        The arguments are not required to be integers.\n+        If ``step`` is omitted, it defaults to 1. If ``start`` is omitted, it defaults to 0. The\n+        ``stop`` value is exclusive; it is not included in the result. If ``step`` is positive, the\n+        last element is the largest *start + i * step* less than ``stop``; if ``step`` is negative,\n+        the last element is the smallest *start + i * step* greater than ``stop``. If the returned\n+        array would contain an infinite number of values, an empty range is returned. The arguments\n+        are not required to be integers.\n         \"\"\"\n         return FunctionExpression(\"sequence\", args)\n \n     @classmethod\n-    def slice(cls, *args) -> FunctionExpression:\n+    def slice(\n+        cls, array: IntoExpression, start: IntoExpression, end: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a section of *array* between the *start* and *end* indices.\n+        Returns a section of ``array`` between the ``start`` and ``end`` indices.\n \n-        If the *end* argument is negative, it is treated as an offset from the end of the array `length(array) + end`.\n+        If the ``end`` argument is negative, it is treated as an offset from the end of the array\n+        (*alt.expr.length(array) + end*).\n         \"\"\"\n-        return FunctionExpression(\"slice\", args)\n+        return FunctionExpression(\"slice\", (array, start, end))\n \n     @classmethod\n-    def span(cls, *args) -> FunctionExpression:\n-        \"\"\"\n-        Returns the span of *array*: the difference between the last and first elements, or `array[array.length-1] - array[0]`.\n-\n-        Or if input is a string: a section of *string* between the *start* and *end* indices.\n-        If the *end* argument is negative, it is treated as an offset from the end of the string `length(string) + end`.\n-        \"\"\"\n-        return FunctionExpression(\"span\", args)\n+    def span(cls, array: IntoExpression, /) -> Expression:\n+        \"\"\"Returns the span of ``array``: the difference between the last and first elements, or *array[array.length-1] - array[0]*.\"\"\"\n+        return FunctionExpression(\"span\", (array,))\n \n     @classmethod\n-    def lower(cls, *args) -> FunctionExpression:\n-        \"\"\"Transforms *string* to lower-case letters.\"\"\"\n-        return FunctionExpression(\"lower\", args)\n+    def lower(cls, string: IntoExpression, /) -> Expression:\n+        \"\"\"Transforms ``string`` to lower-case letters.\"\"\"\n+        return FunctionExpression(\"lower\", (string,))\n \n     @classmethod\n-    def pad(cls, *args) -> FunctionExpression:\n+    def pad(\n+        cls,\n+        string: IntoExpression,\n+        length: IntoExpression,\n+        character: IntoExpression = None,\n+        align: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Pads a *string* value with repeated instances of a *character* up to a specified *length*.\n+        Pads a ``string`` value with repeated instances of a ``character`` up to a specified ``length``.\n \n-        If *character* is not specified, a space (' ') is used.\n-        By default, padding is added to the end of a string.\n-        An optional *align* parameter specifies if padding should be added to the `'left'` (beginning), `'center'`, or `'right'` (end) of the input string.\n+        If ``character`` is not specified, a space (' ') is used. By default, padding is added to\n+        the end of a string. An optional ``align`` parameter specifies if padding should be added to\n+        the ``'left'`` (beginning), ``'center'``, or ``'right'`` (end) of the input string.\n         \"\"\"\n-        return FunctionExpression(\"pad\", args)\n+        return FunctionExpression(\"pad\", (string, length, character, align))\n \n     @classmethod\n-    def parseFloat(cls, *args) -> FunctionExpression:\n+    def parseFloat(cls, string: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Parses the input *string* to a floating-point value.\n+        Parses the input ``string`` to a floating-point value.\n \n-        Same as JavaScript's `parseFloat`.\n+        Same as JavaScript's ``parseFloat``.\n         \"\"\"\n-        return FunctionExpression(\"parseFloat\", args)\n+        return FunctionExpression(\"parseFloat\", (string,))\n \n     @classmethod\n-    def parseInt(cls, *args) -> FunctionExpression:\n+    def parseInt(cls, string: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Parses the input *string* to an integer value.\n+        Parses the input ``string`` to an integer value.\n \n-        Same as JavaScript's `parseInt`.\n+        Same as JavaScript's ``parseInt``.\n         \"\"\"\n-        return FunctionExpression(\"parseInt\", args)\n+        return FunctionExpression(\"parseInt\", (string,))\n \n     @classmethod\n-    def replace(cls, *args) -> FunctionExpression:\n+    def replace(\n+        cls,\n+        string: IntoExpression,\n+        pattern: IntoExpression,\n+        replacement: IntoExpression,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns a new string with some or all matches of *pattern* replaced by a *replacement* string.\n+        Returns a new string with some or all matches of ``pattern`` replaced by a ``replacement`` string.\n \n-        The *pattern* can be a string or a regular expression.\n-        If *pattern* is a string, only the first instance will be replaced.\n-        Same as `JavaScript's String.replace <https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replace>`__.\n-        \"\"\"\n-        return FunctionExpression(\"replace\", args)\n+        The ``pattern`` can be a string or a regular expression. If ``pattern`` is a string, only\n+        the first instance will be replaced. Same as `JavaScript's String.replace`_.\n \n-    @classmethod\n-    def split(cls, *args) -> FunctionExpression:\n+        .. _JavaScript's String.replace:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replace\n         \"\"\"\n-        Returns an array of tokens created by splitting the input *string* according to a provided *separator* pattern.\n-\n-        The result can optionally be constrained to return at most *limit* tokens.\n-        \"\"\"\n-        return FunctionExpression(\"split\", args)\n+        return FunctionExpression(\"replace\", (string, pattern, replacement))\n \n     @classmethod\n-    def substring(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns a section of *string* between the *start* and *end* indices.\"\"\"\n-        return FunctionExpression(\"substring\", args)\n+    def substring(\n+        cls,\n+        string: IntoExpression,\n+        start: IntoExpression,\n+        end: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n+        \"\"\"Returns a section of ``string`` between the ``start`` and ``end`` indices.\"\"\"\n+        return FunctionExpression(\"substring\", (string, start, end))\n \n     @classmethod\n-    def trim(cls, *args) -> FunctionExpression:\n+    def trim(cls, string: IntoExpression, /) -> Expression:\n         \"\"\"Returns a trimmed string with preceding and trailing whitespace removed.\"\"\"\n-        return FunctionExpression(\"trim\", args)\n+        return FunctionExpression(\"trim\", (string,))\n \n     @classmethod\n-    def truncate(cls, *args) -> FunctionExpression:\n-        r\"\"\"\n-        Truncates an input *string* to a target *length*.\n+    def truncate(\n+        cls,\n+        string: IntoExpression,\n+        length: IntoExpression,\n+        align: IntoExpression = None,\n+        ellipsis: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n+        \"\"\"\n+        Truncates an input ``string`` to a target ``length``.\n \n-        The optional *align* argument indicates what part of the string should be truncated: `'left'` (the beginning), `'center'`, or `'right'` (the end).\n-        By default, the `'right'` end of the string is truncated.\n-        The optional *ellipsis* argument indicates the string to use to indicate truncated content;\n-        by default the ellipsis character `...` (`\\\\u2026`) is used.\n+        The optional ``align`` argument indicates what part of the string should be truncated:\n+        ``'left'`` (the beginning), ``'center'``, or ``'right'`` (the end). By default, the\n+        ``'right'`` end of the string is truncated. The optional ``ellipsis`` argument indicates the\n+        string to use to indicate truncated content; by default the ellipsis character ``\u2026``\n+        (``\\u2026``) is used.\n         \"\"\"\n-        return FunctionExpression(\"truncate\", args)\n+        return FunctionExpression(\"truncate\", (string, length, align, ellipsis))\n \n     @classmethod\n-    def upper(cls, *args) -> FunctionExpression:\n-        \"\"\"Transforms *string* to upper-case letters.\"\"\"\n-        return FunctionExpression(\"upper\", args)\n+    def upper(cls, string: IntoExpression, /) -> Expression:\n+        \"\"\"Transforms ``string`` to upper-case letters.\"\"\"\n+        return FunctionExpression(\"upper\", (string,))\n \n     @classmethod\n-    def merge(cls, *args) -> FunctionExpression:\n+    def merge(\n+        cls, object1: IntoExpression, object2: IntoExpression = None, *args: Any\n+    ) -> Expression:\n         \"\"\"\n-        Merges the input objects *object1*, *object2*, etc into a new output object.\n-\n-        Inputs are visited in sequential order, such that key values from later arguments can overwrite those from earlier arguments.\n+        Merges the input objects ``object1``, ``object2``, etc into a new output object.\n \n-        Example: `merge({a:1, b:2}, {a:3}) -> {a:3, b:2}`.\n+        Inputs are visited in sequential order, such that key values from later arguments can\n+        overwrite those from earlier arguments. Example: ``alt.expr.merge({a:1, b:2}, {a:3}) ->\n+        {a:3, b:2}``.\n         \"\"\"\n-        return FunctionExpression(\"merge\", args)\n+        return FunctionExpression(\"merge\", (object1, object2, *args))\n \n     @classmethod\n-    def dayFormat(cls, *args) -> FunctionExpression:\n+    def dayFormat(cls, day: IntoExpression, /) -> Expression:\n         \"\"\"\n         Formats a (0-6) *weekday* number as a full week day name, according to the current locale.\n \n-        For example: `dayFormat(0) -> \"Sunday\"`.\n+        For example: ``alt.expr.dayFormat(0) -> \"Sunday\"``.\n         \"\"\"\n-        return FunctionExpression(\"dayFormat\", args)\n+        return FunctionExpression(\"dayFormat\", (day,))\n \n     @classmethod\n-    def dayAbbrevFormat(cls, *args) -> FunctionExpression:\n+    def dayAbbrevFormat(cls, day: IntoExpression, /) -> Expression:\n         \"\"\"\n         Formats a (0-6) *weekday* number as an abbreviated week day name, according to the current locale.\n \n-        For example: `dayAbbrevFormat(0) -> \"Sun\"`.\n+        For example: ``alt.expr.dayAbbrevFormat(0) -> \"Sun\"``.\n         \"\"\"\n-        return FunctionExpression(\"dayAbbrevFormat\", args)\n+        return FunctionExpression(\"dayAbbrevFormat\", (day,))\n \n     @classmethod\n-    def format(cls, *args) -> FunctionExpression:\n+    def format(cls, value: IntoExpression, specifier: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Formats a numeric *value* as a string.\n+        Formats a numeric ``value`` as a string.\n \n-        The *specifier* must be a valid `d3-format specifier <https://d3js.org/d3-format/>`__ (e.g., `format(value, ',.2f')`.\n+        The ``specifier`` must be a valid `d3-format specifier`_ (e.g., ``alt.expr.format(value,\n+        ',.2f')``. Null values are formatted as ``\"null\"``.\n+\n+        .. _d3-format specifier:\n+            https://github.com/d3/d3-format/\n         \"\"\"\n-        return FunctionExpression(\"format\", args)\n+        return FunctionExpression(\"format\", (value, specifier))\n \n     @classmethod\n-    def monthFormat(cls, *args) -> FunctionExpression:\n+    def monthFormat(cls, month: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Formats a (zero-based) *month* number as a full month name, according to the current locale.\n+        Formats a (zero-based) ``month`` number as a full month name, according to the current locale.\n \n-        For example: `monthFormat(0) -> \"January\"`.\n+        For example: ``alt.expr.monthFormat(0) -> \"January\"``.\n         \"\"\"\n-        return FunctionExpression(\"monthFormat\", args)\n+        return FunctionExpression(\"monthFormat\", (month,))\n \n     @classmethod\n-    def monthAbbrevFormat(cls, *args) -> FunctionExpression:\n+    def monthAbbrevFormat(cls, month: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Formats a (zero-based) *month* number as an abbreviated month name, according to the current locale.\n+        Formats a (zero-based) ``month`` number as an abbreviated month name, according to the current locale.\n \n-        For example: `monthAbbrevFormat(0) -> \"Jan\"`.\n+        For example: ``alt.expr.monthAbbrevFormat(0) -> \"Jan\"``.\n         \"\"\"\n-        return FunctionExpression(\"monthAbbrevFormat\", args)\n+        return FunctionExpression(\"monthAbbrevFormat\", (month,))\n \n     @classmethod\n-    def timeUnitSpecifier(cls, *args) -> FunctionExpression:\n+    def timeUnitSpecifier(\n+        cls, units: IntoExpression, specifiers: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Returns a time format specifier string for the given time `unit <https://vega.github.io/vega/docs/api/time/#time-units>`__.\n-\n-        The optional *specifiers* object provides a set of specifier sub-strings for customizing the format;\n-        for more, see the `timeUnitSpecifier API documentation <https://vega.github.io/vega/docs/api/time/#timeUnitSpecifier>`__.\n+        Returns a time format specifier string for the given time `*units*`_.\n \n-        The resulting specifier string can then be used as input to the `timeFormat <https://vega.github.io/vega/docs/expressions/#timeFormat>`__ or\n-        `utcFormat <https://vega.github.io/vega/docs/expressions/#utcFormat>`__ functions, or as the *format* parameter of an axis or legend.\n+        The optional ``specifiers`` object provides a set of specifier sub-strings for customizing\n+        the format; for more, see the `timeUnitSpecifier API documentation`_. The resulting\n+        specifier string can then be used as input to the `timeFormat`_ or `utcFormat`_ functions,\n+        or as the *format* parameter of an axis or legend. For example: ``alt.expr.timeFormat(date,\n+        alt.expr.timeUnitSpecifier('year'))`` or ``alt.expr.timeFormat(date,\n+        alt.expr.timeUnitSpecifier(['hours', 'minutes']))``.\n \n-        For example: `timeFormat(date, timeUnitSpecifier('year'))` or `timeFormat(date, timeUnitSpecifier(['hours', 'minutes']))`.\n+        .. _*units*:\n+            https://vega.github.io/vega/docs/api/time/#time-units\n+        .. _timeUnitSpecifier API documentation:\n+            https://vega.github.io/vega/docs/api/time/#timeUnitSpecifier\n+        .. _timeFormat:\n+            https://vega.github.io/vega/docs/expressions/#timeFormat\n+        .. _utcFormat:\n+            https://vega.github.io/vega/docs/expressions/#utcFormat\n         \"\"\"\n-        return FunctionExpression(\"timeUnitSpecifier\", args)\n+        return FunctionExpression(\"timeUnitSpecifier\", (units, specifiers))\n \n     @classmethod\n-    def timeFormat(cls, *args) -> FunctionExpression:\n+    def timeFormat(\n+        cls, value: IntoExpression, specifier: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Formats a datetime *value* (either a `Date` object or timestamp) as a string, according to the local time.\n+        Formats a datetime ``value`` (either a ``Date`` object or timestamp) as a string, according to the local time.\n+\n+        The ``specifier`` must be a valid `d3-time-format specifier`_ or `TimeMultiFormat object`_.\n+        For example: ``alt.expr.timeFormat(timestamp, '%A')``. Null values are formatted as\n+        ``\"null\"``.\n \n-        The *specifier* must be a valid `d3-time-format specifier <https://d3js.org/d3-time-format/>`__.\n-        For example: `timeFormat(timestamp, '%A')`.\n+        .. _d3-time-format specifier:\n+            https://github.com/d3/d3-time-format/\n+        .. _TimeMultiFormat object:\n+            https://vega.github.io/vega/docs/types/#TimeMultiFormat\n         \"\"\"\n-        return FunctionExpression(\"timeFormat\", args)\n+        return FunctionExpression(\"timeFormat\", (value, specifier))\n \n     @classmethod\n-    def timeParse(cls, *args) -> FunctionExpression:\n+    def timeParse(\n+        cls, string: IntoExpression, specifier: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Parses a *string* value to a Date object, according to the local time.\n+        Parses a ``string`` value to a Date object, according to the local time.\n \n-        The *specifier* must be a valid `d3-time-format specifier <https://d3js.org/d3-time-format/>`__.\n-        For example: `timeParse('June 30, 2015', '%B %d, %Y')`.\n+        The ``specifier`` must be a valid `d3-time-format specifier`_. For example:\n+        ``alt.expr.timeParse('June 30, 2015', '%B %d, %Y')``.\n+\n+        .. _d3-time-format specifier:\n+            https://github.com/d3/d3-time-format/\n         \"\"\"\n-        return FunctionExpression(\"timeParse\", args)\n+        return FunctionExpression(\"timeParse\", (string, specifier))\n \n     @classmethod\n-    def utcFormat(cls, *args) -> FunctionExpression:\n+    def utcFormat(\n+        cls, value: IntoExpression, specifier: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Formats a datetime *value* (either a `Date` object or timestamp) as a string, according to `UTC <https://en.wikipedia.org/wiki/Coordinated_Universal_Time>`__ time.\n+        Formats a datetime ``value`` (either a ``Date`` object or timestamp) as a string, according to `UTC`_ time.\n+\n+        The ``specifier`` must be a valid `d3-time-format specifier`_ or `TimeMultiFormat object`_.\n+        For example: ``alt.expr.utcFormat(timestamp, '%A')``. Null values are formatted as\n+        ``\"null\"``.\n \n-        The *specifier* must be a valid `d3-time-format specifier <https://d3js.org/d3-time-format/>`__.\n-        For example: `utcFormat(timestamp, '%A')`.\n+        .. _UTC:\n+            https://en.wikipedia.org/wiki/Coordinated_Universal_Time\n+        .. _d3-time-format specifier:\n+            https://github.com/d3/d3-time-format/\n+        .. _TimeMultiFormat object:\n+            https://vega.github.io/vega/docs/types/#TimeMultiFormat\n         \"\"\"\n-        return FunctionExpression(\"utcFormat\", args)\n+        return FunctionExpression(\"utcFormat\", (value, specifier))\n \n     @classmethod\n-    def utcParse(cls, *args) -> FunctionExpression:\n+    def utcParse(\n+        cls, value: IntoExpression, specifier: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Parses a *string* value to a Date object, according to `UTC <https://en.wikipedia.org/wiki/Coordinated_Universal_Time>`__ time.\n+        Parses a *string* value to a Date object, according to `UTC`_ time.\n+\n+        The ``specifier`` must be a valid `d3-time-format specifier`_. For example:\n+        ``alt.expr.utcParse('June 30, 2015', '%B %d, %Y')``.\n \n-        The *specifier* must be a valid `d3-time-format specifier <https://d3js.org/d3-time-format/>`__.\n-        For example: `utcParse('June 30, 2015', '%B %d, %Y')`.\n+        .. _UTC:\n+            https://en.wikipedia.org/wiki/Coordinated_Universal_Time\n+        .. _d3-time-format specifier:\n+            https://github.com/d3/d3-time-format/\n         \"\"\"\n-        return FunctionExpression(\"utcParse\", args)\n+        return FunctionExpression(\"utcParse\", (value, specifier))\n \n     @classmethod\n-    def regexp(cls, *args) -> FunctionExpression:\n+    def regexp(\n+        cls, pattern: IntoExpression, flags: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        Creates a regular expression instance from an input *pattern* string and optional *flags*.\n+        Creates a regular expression instance from an input ``pattern`` string and optional ``flags``.\n \n-        Same as `JavaScript's `RegExp` <https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp>`__.\n+        Same as `JavaScript's RegExp`_.\n+\n+        .. _JavaScript's RegExp:\n+            https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp\n         \"\"\"\n-        return FunctionExpression(\"regexp\", args)\n+        return FunctionExpression(\"regexp\", (pattern, flags))\n \n     @classmethod\n-    def test(cls, *args) -> FunctionExpression:\n+    def test(\n+        cls, regexp: IntoExpression, string: IntoExpression = None, /\n+    ) -> Expression:\n         r\"\"\"\n-        Evaluates a regular expression *regexp* against the input *string*, returning `true` if the string matches the pattern, `false` otherwise.\n+        Evaluates a regular expression ``regexp`` against the input ``string``, returning ``true`` if the string matches the pattern, ``false`` otherwise.\n \n-        For example: `test(\\d{3}, \"32-21-9483\") -> true`.\n+        For example: ``alt.expr.test(/\\\\d{3}/, \"32-21-9483\") -> true``.\n         \"\"\"\n-        return FunctionExpression(\"test\", args)\n+        return FunctionExpression(\"test\", (regexp, string))\n \n     @classmethod\n-    def rgb(cls, *args) -> FunctionExpression:\n+    def rgb(cls, *args: Any) -> Expression:\n         \"\"\"\n-        Constructs a new `RGB <https://en.wikipedia.org/wiki/RGB_color_model>`__ color.\n+        Constructs a new `RGB`_ color.\n+\n+        If ``r``, ``g`` and ``b`` are specified, these represent the channel values of the returned\n+        color; an ``opacity`` may also be specified. If a CSS Color Module Level 3 *specifier*\n+        string is specified, it is parsed and then converted to the RGB color space. Uses\n+        `d3-color's rgb function`_.\n \n-        If *r*, *g* and *b* are specified, these represent the channel values of the returned color; an *opacity* may also be specified.\n-        If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the RGB color space. Uses `d3-color's rgb function <https://d3js.org/d3-color#color_rgb>`__.\n+        .. _RGB:\n+            https://en.wikipedia.org/wiki/RGB_color_model\n+        .. _d3-color's rgb function:\n+            https://github.com/d3/d3-color#rgb\n         \"\"\"\n         return FunctionExpression(\"rgb\", args)\n \n     @classmethod\n-    def hsl(cls, *args) -> FunctionExpression:\n+    def hsl(cls, *args: Any) -> Expression:\n         \"\"\"\n-        Constructs a new `HSL <https://en.wikipedia.org/wiki/HSL_and_HSV>`__ color.\n+        Constructs a new `HSL`_ color.\n+\n+        If ``h``, ``s`` and ``l`` are specified, these represent the channel values of the returned\n+        color; an ``opacity`` may also be specified. If a CSS Color Module Level 3 *specifier*\n+        string is specified, it is parsed and then converted to the HSL color space. Uses\n+        `d3-color's hsl function`_.\n \n-        If *h*, *s* and *l* are specified, these represent the channel values of the returned color; an *opacity* may also be specified.\n-        If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the HSL color space.\n-        Uses `d3-color's hsl function <https://d3js.org/d3-color#hsl>`__.\n+        .. _HSL:\n+            https://en.wikipedia.org/wiki/HSL_and_HSV\n+        .. _d3-color's hsl function:\n+            https://github.com/d3/d3-color#hsl\n         \"\"\"\n         return FunctionExpression(\"hsl\", args)\n \n     @classmethod\n-    def lab(cls, *args) -> FunctionExpression:\n+    def lab(cls, *args: Any) -> Expression:\n         \"\"\"\n-        Constructs a new `CIE LAB <https://en.wikipedia.org/wiki/Lab_color_space#CIELAB>`__ color.\n+        Constructs a new `CIE LAB`_ color.\n \n-        If *l*, *a* and *b* are specified, these represent the channel values of the returned color; an *opacity* may also be specified.\n-        If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the LAB color space.\n-        Uses `d3-color's lab function <https://d3js.org/d3-color#lab>`__.\n+        If ``l``, ``a`` and ``b`` are specified, these represent the channel values of the returned\n+        color; an ``opacity`` may also be specified. If a CSS Color Module Level 3 *specifier*\n+        string is specified, it is parsed and then converted to the LAB color space. Uses\n+        `d3-color's lab function`_.\n+\n+        .. _CIE LAB:\n+            https://en.wikipedia.org/wiki/Lab_color_space#CIELAB\n+        .. _d3-color's lab function:\n+            https://github.com/d3/d3-color#lab\n         \"\"\"\n         return FunctionExpression(\"lab\", args)\n \n     @classmethod\n-    def hcl(cls, *args) -> FunctionExpression:\n+    def hcl(cls, *args: Any) -> Expression:\n         \"\"\"\n-        Constructs a new `HCL <https://en.wikipedia.org/wiki/Lab_color_space#CIELAB>`__ (hue, chroma, luminance) color.\n+        Constructs a new `HCL`_ (hue, chroma, luminance) color.\n+\n+        If ``h``, ``c`` and ``l`` are specified, these represent the channel values of the returned\n+        color; an ``opacity`` may also be specified. If a CSS Color Module Level 3 *specifier*\n+        string is specified, it is parsed and then converted to the HCL color space. Uses\n+        `d3-color's hcl function`_.\n \n-        If *h*, *c* and *l* are specified, these represent the channel values of the returned color; an *opacity* may also be specified.\n-        If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the HCL color space.\n-        Uses `d3-color's hcl function <https://d3js.org/d3-color#hcl>`__.\n+        .. _HCL:\n+            https://en.wikipedia.org/wiki/Lab_color_space#CIELAB\n+        .. _d3-color's hcl function:\n+            https://github.com/d3/d3-color#hcl\n         \"\"\"\n         return FunctionExpression(\"hcl\", args)\n \n     @classmethod\n-    def luminance(cls, *args) -> FunctionExpression:\n+    def luminance(cls, specifier: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the luminance for the given color *specifier* (compatible with `d3-color's rgb function <https://d3js.org/d3-color#rgb)>`__.\n+        Returns the luminance for the given color ``specifier`` (compatible with `d3-color's rgb function`_).\n+\n+        The luminance is calculated according to the `W3C Web Content Accessibility Guidelines`_.\n \n-        The luminance is calculated according to the `W3C Web Content Accessibility Guidelines <https://www.w3.org/TR/2008/REC-WCAG20-20081211/#relativeluminancedef>`__.\n+        .. _d3-color's rgb function:\n+            https://github.com/d3/d3-color#rgb\n+        .. _W3C Web Content Accessibility Guidelines:\n+            https://www.w3.org/TR/2008/REC-WCAG20-20081211/#relativeluminancedef\n         \"\"\"\n-        return FunctionExpression(\"luminance\", args)\n+        return FunctionExpression(\"luminance\", (specifier,))\n \n     @classmethod\n-    def contrast(cls, *args) -> FunctionExpression:\n+    def contrast(\n+        cls, specifier1: IntoExpression, specifier2: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n         Returns the contrast ratio between the input color specifiers as a float between 1 and 21.\n \n-        The contrast is calculated according to the `W3C Web Content Accessibility Guidelines <https://www.w3.org/TR/2008/REC-WCAG20-20081211/#contrast-ratiodef>`__.\n+        The contrast is calculated according to the `W3C Web Content Accessibility Guidelines`_.\n+\n+        .. _W3C Web Content Accessibility Guidelines:\n+            https://www.w3.org/TR/2008/REC-WCAG20-20081211/#contrast-ratiodef\n         \"\"\"\n-        return FunctionExpression(\"contrast\", args)\n+        return FunctionExpression(\"contrast\", (specifier1, specifier2))\n \n     @classmethod\n-    def item(cls, *args) -> FunctionExpression:\n+    def item(cls) -> Expression:\n         \"\"\"Returns the current scenegraph item that is the target of the event.\"\"\"\n-        return FunctionExpression(\"item\", args)\n+        return FunctionExpression(\"item\", ())\n \n     @classmethod\n-    def group(cls, *args) -> FunctionExpression:\n+    def group(cls, name: IntoExpression = None, /) -> Expression:\n         \"\"\"\n         Returns the scenegraph group mark item in which the current event has occurred.\n \n-        If no arguments are provided, the immediate parent group is returned.\n-        If a group name is provided, the matching ancestor group item is returned.\n+        If no arguments are provided, the immediate parent group is returned. If a group name is\n+        provided, the matching ancestor group item is returned.\n         \"\"\"\n-        return FunctionExpression(\"group\", args)\n+        return FunctionExpression(\"group\", (name,))\n \n     @classmethod\n-    def xy(cls, *args) -> FunctionExpression:\n+    def xy(cls, item: IntoExpression = None, /) -> Expression:\n         \"\"\"\n         Returns the x- and y-coordinates for the current event as a two-element array.\n \n-        If no arguments are provided, the top-level coordinate space of the view is used.\n-        If a scenegraph *item* (or string group name) is provided, the coordinate space of the group item is used.\n+        If no arguments are provided, the top-level coordinate space of the view is used. If a\n+        scenegraph ``item`` (or string group name) is provided, the coordinate space of the group\n+        item is used.\n         \"\"\"\n-        return FunctionExpression(\"xy\", args)\n+        return FunctionExpression(\"xy\", (item,))\n \n     @classmethod\n-    def x(cls, *args) -> FunctionExpression:\n+    def x(cls, item: IntoExpression = None, /) -> Expression:\n         \"\"\"\n         Returns the x coordinate for the current event.\n \n-        If no arguments are provided, the top-level coordinate space of the view is used.\n-        If a scenegraph *item* (or string group name) is provided, the coordinate space of the group item is used.\n+        If no arguments are provided, the top-level coordinate space of the view is used. If a\n+        scenegraph ``item`` (or string group name) is provided, the coordinate space of the group\n+        item is used.\n         \"\"\"\n-        return FunctionExpression(\"x\", args)\n+        return FunctionExpression(\"x\", (item,))\n \n     @classmethod\n-    def y(cls, *args) -> FunctionExpression:\n+    def y(cls, item: IntoExpression = None, /) -> Expression:\n         \"\"\"\n         Returns the y coordinate for the current event.\n \n-        If no arguments are provided, the top-level coordinate space of the view is used.\n-        If a scenegraph *item* (or string group name) is provided, the coordinate space of the group item is used.\n+        If no arguments are provided, the top-level coordinate space of the view is used. If a\n+        scenegraph ``item`` (or string group name) is provided, the coordinate space of the group\n+        item is used.\n         \"\"\"\n-        return FunctionExpression(\"y\", args)\n+        return FunctionExpression(\"y\", (item,))\n \n     @classmethod\n-    def pinchDistance(cls, *args) -> FunctionExpression:\n+    def pinchDistance(cls, event: IntoExpression, /) -> Expression:\n         \"\"\"Returns the pixel distance between the first two touch points of a multi-touch event.\"\"\"\n-        return FunctionExpression(\"pinchDistance\", args)\n+        return FunctionExpression(\"pinchDistance\", (event,))\n \n     @classmethod\n-    def pinchAngle(cls, *args) -> FunctionExpression:\n+    def pinchAngle(cls, event: IntoExpression, /) -> Expression:\n         \"\"\"Returns the angle of the line connecting the first two touch points of a multi-touch event.\"\"\"\n-        return FunctionExpression(\"pinchAngle\", args)\n+        return FunctionExpression(\"pinchAngle\", (event,))\n \n     @classmethod\n-    def inScope(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns true if the given scenegraph *item* is a descendant of the group mark in which the event handler was defined, false otherwise.\"\"\"\n-        return FunctionExpression(\"inScope\", args)\n+    def inScope(cls, item: IntoExpression, /) -> Expression:\n+        \"\"\"Returns true if the given scenegraph ``item`` is a descendant of the group mark in which the event handler was defined, false otherwise.\"\"\"\n+        return FunctionExpression(\"inScope\", (item,))\n \n     @classmethod\n-    def data(cls, *args) -> FunctionExpression:\n+    def data(cls, name: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Returns the array of data objects for the Vega data set with the given *name*.\n+        Returns the array of data objects for the Vega data set with the given ``name``.\n \n         If the data set is not found, returns an empty array.\n         \"\"\"\n-        return FunctionExpression(\"data\", args)\n+        return FunctionExpression(\"data\", (name,))\n \n     @classmethod\n-    def indata(cls, *args) -> FunctionExpression:\n+    def indata(\n+        cls, name: IntoExpression, field: IntoExpression, value: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Tests if the data set with a given *name* contains a datum with a *field* value that matches the input *value*.\n+        Tests if the data set with a given ``name`` contains a datum with a ``field`` value that matches the input ``value``.\n \n-        For example: `indata('table', 'category', value)`.\n+        For example: ``alt.expr.indata('table', 'category', value)``.\n         \"\"\"\n-        return FunctionExpression(\"indata\", args)\n+        return FunctionExpression(\"indata\", (name, field, value))\n \n     @classmethod\n-    def scale(cls, *args) -> FunctionExpression:\n+    def scale(\n+        cls,\n+        name: IntoExpression,\n+        value: IntoExpression,\n+        group: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Applies the named scale transform (or projection) to the specified *value*.\n+        Applies the named scale transform (or projection) to the specified ``value``.\n \n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the scale or projection.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the scale or projection.\n         \"\"\"\n-        return FunctionExpression(\"scale\", args)\n+        return FunctionExpression(\"scale\", (name, value, group))\n \n     @classmethod\n-    def invert(cls, *args) -> FunctionExpression:\n+    def invert(\n+        cls,\n+        name: IntoExpression,\n+        value: IntoExpression,\n+        group: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Inverts the named scale transform (or projection) for the specified *value*.\n+        Inverts the named scale transform (or projection) for the specified ``value``.\n \n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the scale or projection.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the scale or projection.\n         \"\"\"\n-        return FunctionExpression(\"invert\", args)\n+        return FunctionExpression(\"invert\", (name, value, group))\n \n     @classmethod\n-    def copy(cls, *args) -> FunctionExpression:  # type: ignore[override]\n+    def copy(cls, name: IntoExpression, group: IntoExpression = None, /) -> Expression:  # type: ignore[override]\n         \"\"\"\n-        Returns a copy (a new cloned instance) of the named scale transform of projection, or `undefined` if no scale or projection is found.\n+        Returns a copy (a new cloned instance) of the named scale transform of projection, or ``undefined`` if no scale or projection is found.\n \n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the scale or projection.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the scale or projection.\n         \"\"\"\n-        # error: Signature of \"copy\" incompatible with supertype \"SchemaBase\"  [override]\n-        # note:      def copy(self, deep: bool | Iterable[Any] = ..., ignore: list[str] | None = ...) -> expr\n-        # NOTE: Not relevant as `expr() -> ExprRef`\n-        # this method is only accesible via `expr.copy()`\n-        return FunctionExpression(\"copy\", args)\n+        return FunctionExpression(\"copy\", (name, group))\n \n     @classmethod\n-    def domain(cls, *args) -> FunctionExpression:\n+    def domain(\n+        cls, name: IntoExpression, group: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n         Returns the scale domain array for the named scale transform, or an empty array if the scale is not found.\n \n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the scale.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the scale.\n         \"\"\"\n-        return FunctionExpression(\"domain\", args)\n+        return FunctionExpression(\"domain\", (name, group))\n \n     @classmethod\n-    def range(cls, *args) -> FunctionExpression:\n+    def range(cls, name: IntoExpression, group: IntoExpression = None, /) -> Expression:\n         \"\"\"\n         Returns the scale range array for the named scale transform, or an empty array if the scale is not found.\n \n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the scale.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the scale.\n         \"\"\"\n-        return FunctionExpression(\"range\", args)\n+        return FunctionExpression(\"range\", (name, group))\n \n     @classmethod\n-    def bandwidth(cls, *args) -> FunctionExpression:\n+    def bandwidth(\n+        cls, name: IntoExpression, group: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n         Returns the current band width for the named band scale transform, or zero if the scale is not found or is not a band scale.\n \n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the scale.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the scale.\n         \"\"\"\n-        return FunctionExpression(\"bandwidth\", args)\n+        return FunctionExpression(\"bandwidth\", (name, group))\n \n     @classmethod\n-    def bandspace(cls, *args) -> FunctionExpression:\n+    def bandspace(\n+        cls,\n+        count: IntoExpression,\n+        paddingInner: IntoExpression = None,\n+        paddingOuter: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the number of steps needed within a band scale, based on the *count* of domain elements and the inner and outer padding values.\n+        Returns the number of steps needed within a band scale, based on the ``count`` of domain elements and the inner and outer padding values.\n \n-        While normally calculated within the scale itself, this function can be helpful for determining the size of a chart's layout.\n+        While normally calculated within the scale itself, this function can be helpful for\n+        determining the size of a chart's layout.\n         \"\"\"\n-        return FunctionExpression(\"bandspace\", args)\n+        return FunctionExpression(\"bandspace\", (count, paddingInner, paddingOuter))\n \n     @classmethod\n-    def gradient(cls, *args) -> FunctionExpression:\n+    def gradient(\n+        cls,\n+        scale: IntoExpression,\n+        p0: IntoExpression,\n+        p1: IntoExpression,\n+        count: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns a linear color gradient for the *scale* (whose range must be a `continuous color scheme <https://vega.github.io/vega/docs/schemes>`__ and starting and ending points *p0* and *p1*, each an `[x, y]` array.\n+        Returns a linear color gradient for the ``scale`` (whose range must be a `continuous color scheme`_) and starting and ending points ``p0`` and ``p1``, each an *[x, y]* array.\n \n-        The points *p0* and *p1* should be expressed in normalized coordinates in the domain `[0, 1]`, relative to the bounds of the item being colored.\n+        The points ``p0`` and ``p1`` should be expressed in normalized coordinates in the domain [0,\n+        1], relative to the bounds of the item being colored. If unspecified, ``p0`` defaults to\n+        ``[0, 0]`` and ``p1`` defaults to ``[1, 0]``, for a horizontal gradient that spans the full\n+        bounds of an item. The optional ``count`` argument indicates a desired target number of\n+        sample points to take from the color scale.\n \n-        If unspecified, *p0* defaults to `[0, 0]` and *p1* defaults to `[1, 0]`, for a horizontal gradient that spans the full bounds of an item.\n-        The optional *count* argument indicates a desired target number of sample points to take from the color scale.\n+        .. _continuous color scheme:\n+            https://vega.github.io/vega/docs/schemes\n         \"\"\"\n-        return FunctionExpression(\"gradient\", args)\n+        return FunctionExpression(\"gradient\", (scale, p0, p1, count))\n \n     @classmethod\n-    def panLinear(cls, *args) -> FunctionExpression:\n+    def panLinear(cls, domain: IntoExpression, delta: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Given a linear scale *domain* array with numeric or datetime values, returns a new two-element domain array that is the result of panning the domain by a fractional *delta*.\n+        Given a linear scale ``domain`` array with numeric or datetime values, returns a new two-element domain array that is the result of panning the domain by a fractional ``delta``.\n \n-        The *delta* value represents fractional units of the scale range; for example, `0.5` indicates panning the scale domain to the right by half the scale range.\n+        The ``delta`` value represents fractional units of the scale range; for example, ``0.5``\n+        indicates panning the scale domain to the right by half the scale range.\n         \"\"\"\n-        return FunctionExpression(\"panLinear\", args)\n+        return FunctionExpression(\"panLinear\", (domain, delta))\n \n     @classmethod\n-    def panLog(cls, *args) -> FunctionExpression:\n+    def panLog(cls, domain: IntoExpression, delta: IntoExpression, /) -> Expression:\n         \"\"\"\n-        Given a log scale *domain* array with numeric or datetime values, returns a new two-element domain array that is the result of panning the domain by a fractional *delta*.\n+        Given a log scale ``domain`` array with numeric or datetime values, returns a new two-element domain array that is the result of panning the domain by a fractional ``delta``.\n \n-        The *delta* value represents fractional units of the scale range; for example, `0.5` indicates panning the scale domain to the right by half the scale range.\n+        The ``delta`` value represents fractional units of the scale range; for example, ``0.5``\n+        indicates panning the scale domain to the right by half the scale range.\n         \"\"\"\n-        return FunctionExpression(\"panLog\", args)\n+        return FunctionExpression(\"panLog\", (domain, delta))\n \n     @classmethod\n-    def panPow(cls, *args) -> FunctionExpression:\n+    def panPow(\n+        cls, domain: IntoExpression, delta: IntoExpression, exponent: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Given a power scale *domain* array with numeric or datetime values and the given *exponent*, returns a new two-element domain array that is the result of panning the domain by a fractional *delta*.\n+        Given a power scale ``domain`` array with numeric or datetime values and the given ``exponent``, returns a new two-element domain array that is the result of panning the domain by a fractional ``delta``.\n \n-        The *delta* value represents fractional units of the scale range; for example, `0.5` indicates panning the scale domain to the right by half the scale range.\n+        The ``delta`` value represents fractional units of the scale range; for example, ``0.5``\n+        indicates panning the scale domain to the right by half the scale range.\n         \"\"\"\n-        return FunctionExpression(\"panPow\", args)\n+        return FunctionExpression(\"panPow\", (domain, delta, exponent))\n \n     @classmethod\n-    def panSymlog(cls, *args) -> FunctionExpression:\n+    def panSymlog(\n+        cls, domain: IntoExpression, delta: IntoExpression, constant: IntoExpression, /\n+    ) -> Expression:\n         \"\"\"\n-        Given a symmetric log scale *domain* array with numeric or datetime values parameterized by the given *constant*, returns a new two-element domain array that is the result of panning the domain by a fractional *delta*.\n+        Given a symmetric log scale ``domain`` array with numeric or datetime values parameterized by the given ``constant``, returns a new two-element domain array that is the result of panning the domain by a fractional ``delta``.\n \n-        The *delta* value represents fractional units of the scale range; for example, `0.5` indicates panning the scale domain to the right by half the scale range.\n+        The ``delta`` value represents fractional units of the scale range; for example, ``0.5``\n+        indicates panning the scale domain to the right by half the scale range.\n         \"\"\"\n-        return FunctionExpression(\"panSymlog\", args)\n+        return FunctionExpression(\"panSymlog\", (domain, delta, constant))\n \n     @classmethod\n-    def zoomLinear(cls, *args) -> FunctionExpression:\n+    def zoomLinear(\n+        cls,\n+        domain: IntoExpression,\n+        anchor: IntoExpression,\n+        scaleFactor: IntoExpression,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Given a linear scale *domain* array with numeric or datetime values, returns a new two-element domain array that is the result of zooming the domain by a *scaleFactor*, centered at the provided fractional *anchor*.\n+        Given a linear scale ``domain`` array with numeric or datetime values, returns a new two-element domain array that is the result of zooming the domain by a ``scaleFactor``, centered at the provided fractional ``anchor``.\n \n-        The *anchor* value represents the zoom position in terms of fractional units of the scale range; for example, `0.5` indicates a zoom centered on the mid-point of the scale range.\n+        The ``anchor`` value represents the zoom position in terms of fractional units of the scale\n+        range; for example, ``0.5`` indicates a zoom centered on the mid-point of the scale range.\n         \"\"\"\n-        return FunctionExpression(\"zoomLinear\", args)\n+        return FunctionExpression(\"zoomLinear\", (domain, anchor, scaleFactor))\n \n     @classmethod\n-    def zoomLog(cls, *args) -> FunctionExpression:\n+    def zoomLog(\n+        cls,\n+        domain: IntoExpression,\n+        anchor: IntoExpression,\n+        scaleFactor: IntoExpression,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Given a log scale *domain* array with numeric or datetime values, returns a new two-element domain array that is the result of zooming the domain by a *scaleFactor*, centered at the provided fractional *anchor*.\n+        Given a log scale ``domain`` array with numeric or datetime values, returns a new two-element domain array that is the result of zooming the domain by a ``scaleFactor``, centered at the provided fractional ``anchor``.\n \n-        The *anchor* value represents the zoom position in terms of fractional units of the scale range; for example, `0.5` indicates a zoom centered on the mid-point of the scale range.\n+        The ``anchor`` value represents the zoom position in terms of fractional units of the scale\n+        range; for example, ``0.5`` indicates a zoom centered on the mid-point of the scale range.\n         \"\"\"\n-        return FunctionExpression(\"zoomLog\", args)\n+        return FunctionExpression(\"zoomLog\", (domain, anchor, scaleFactor))\n \n     @classmethod\n-    def zoomPow(cls, *args) -> FunctionExpression:\n+    def zoomPow(\n+        cls,\n+        domain: IntoExpression,\n+        anchor: IntoExpression,\n+        scaleFactor: IntoExpression,\n+        exponent: IntoExpression,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Given a power scale *domain* array with numeric or datetime values and the given *exponent*, returns a new two-element domain array that is the result of zooming the domain by a *scaleFactor*, centered at the provided fractional *anchor*.\n+        Given a power scale ``domain`` array with numeric or datetime values and the given ``exponent``, returns a new two-element domain array that is the result of zooming the domain by a ``scaleFactor``, centered at the provided fractional ``anchor``.\n \n-        The *anchor* value represents the zoom position in terms of fractional units of the scale range; for example, `0.5` indicates a zoom centered on the mid-point of the scale range.\n+        The ``anchor`` value represents the zoom position in terms of fractional units of the scale\n+        range; for example, ``0.5`` indicates a zoom centered on the mid-point of the scale range.\n         \"\"\"\n-        return FunctionExpression(\"zoomPow\", args)\n+        return FunctionExpression(\"zoomPow\", (domain, anchor, scaleFactor, exponent))\n \n     @classmethod\n-    def zoomSymlog(cls, *args) -> FunctionExpression:\n+    def zoomSymlog(\n+        cls,\n+        domain: IntoExpression,\n+        anchor: IntoExpression,\n+        scaleFactor: IntoExpression,\n+        constant: IntoExpression,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Given a symmetric log scale *domain* array with numeric or datetime values parameterized by the given *constant*, returns a new two-element domain array that is the result of zooming the domain by a *scaleFactor*, centered at the provided fractional *anchor*.\n+        Given a symmetric log scale ``domain`` array with numeric or datetime values parameterized by the given ``constant``, returns a new two-element domain array that is the result of zooming the domain by a ``scaleFactor``, centered at the provided fractional ``anchor``.\n \n-        The *anchor* value represents the zoom position in terms of fractional units of the scale range; for example, `0.5` indicates a zoom centered on the mid-point of the scale range.\n+        The ``anchor`` value represents the zoom position in terms of fractional units of the scale\n+        range; for example, ``0.5`` indicates a zoom centered on the mid-point of the scale range.\n         \"\"\"\n-        return FunctionExpression(\"zoomSymlog\", args)\n+        return FunctionExpression(\"zoomSymlog\", (domain, anchor, scaleFactor, constant))\n \n     @classmethod\n-    def geoArea(cls, *args) -> FunctionExpression:\n+    def geoArea(\n+        cls,\n+        projection: IntoExpression,\n+        feature: IntoExpression,\n+        group: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the projected planar area (typically in square pixels) of a GeoJSON *feature* according to the named *projection*.\n+        Returns the projected planar area (typically in square pixels) of a GeoJSON ``feature`` according to the named ``projection``.\n+\n+        If the ``projection`` argument is ``null``, computes the spherical area in steradians using\n+        unprojected longitude, latitude coordinates. The optional ``group`` argument takes a\n+        scenegraph group mark item to indicate the specific scope in which to look up the\n+        projection. Uses d3-geo's `geoArea`_ and `path.area`_ methods.\n \n-        If the *projection* argument is `null`, computes the spherical area in steradians using unprojected longitude, latitude coordinates.\n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the projection.\n-        Uses d3-geo's `geoArea <https://d3js.org/d3-geo#geoArea>`__ and `path.area <https://d3js.org/d3-geo#path_area>`__ methods.\n+        .. _geoArea:\n+            https://github.com/d3/d3-geo#geoArea\n+        .. _path.area:\n+            https://github.com/d3/d3-geo#path_area\n         \"\"\"\n-        return FunctionExpression(\"geoArea\", args)\n+        return FunctionExpression(\"geoArea\", (projection, feature, group))\n \n     @classmethod\n-    def geoBounds(cls, *args) -> FunctionExpression:\n+    def geoBounds(\n+        cls,\n+        projection: IntoExpression,\n+        feature: IntoExpression,\n+        group: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the projected planar bounding box (typically in pixels) for the specified GeoJSON *feature*, according to the named *projection*.\n+        Returns the projected planar bounding box (typically in pixels) for the specified GeoJSON ``feature``, according to the named ``projection``.\n \n-        The bounding box is represented by a two-dimensional array: `[[x0, y0], [x1, y1]]`,\n-        where *x0* is the minimum x-coordinate, *y0* is the minimum y-coordinate,\n-        *x1* is the maximum x-coordinate, and *y1* is the maximum y-coordinate.\n+        The bounding box is represented by a two-dimensional array: [[*x\u2080*, *y\u2080*], [*x\u2081*, *y\u2081*]],\n+        where *x\u2080* is the minimum x-coordinate, *y\u2080* is the minimum y-coordinate, *x\u2081* is the\n+        maximum x-coordinate, and *y\u2081* is the maximum y-coordinate. If the ``projection`` argument\n+        is ``null``, computes the spherical bounding box using unprojected longitude, latitude\n+        coordinates. The optional ``group`` argument takes a scenegraph group mark item to indicate\n+        the specific scope in which to look up the projection. Uses d3-geo's `geoBounds`_ and\n+        `path.bounds`_ methods.\n \n-        If the *projection* argument is `null`, computes the spherical bounding box using unprojected longitude, latitude coordinates.\n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the projection.\n-        Uses d3-geo's `geoBounds <https://d3js.org/d3-geo#geoBounds>`__ and `path.bounds <https://d3js.org/d3-geo#path_bounds>`__ methods.\n+        .. _geoBounds:\n+            https://github.com/d3/d3-geo#geoBounds\n+        .. _path.bounds:\n+            https://github.com/d3/d3-geo#path_bounds\n         \"\"\"\n-        return FunctionExpression(\"geoBounds\", args)\n+        return FunctionExpression(\"geoBounds\", (projection, feature, group))\n \n     @classmethod\n-    def geoCentroid(cls, *args) -> FunctionExpression:\n+    def geoCentroid(\n+        cls,\n+        projection: IntoExpression,\n+        feature: IntoExpression,\n+        group: IntoExpression = None,\n+        /,\n+    ) -> Expression:\n         \"\"\"\n-        Returns the projected planar centroid (typically in pixels) for the specified GeoJSON *feature*, according to the named *projection*.\n+        Returns the projected planar centroid (typically in pixels) for the specified GeoJSON ``feature``, according to the named ``projection``.\n+\n+        If the ``projection`` argument is ``null``, computes the spherical centroid using\n+        unprojected longitude, latitude coordinates. The optional ``group`` argument takes a\n+        scenegraph group mark item to indicate the specific scope in which to look up the\n+        projection. Uses d3-geo's `geoCentroid`_ and `path.centroid`_ methods.\n \n-        If the *projection* argument is `null`, computes the spherical centroid using unprojected longitude, latitude coordinates.\n-        The optional *group* argument takes a scenegraph group mark item to indicate the specific scope in which to look up the projection.\n-        Uses d3-geo's `geoCentroid <https://d3js.org/d3-geo#geoCentroid>`__ and `path.centroid <https://d3js.org/d3-geo#path_centroid>`__ methods.\n+        .. _geoCentroid:\n+            https://github.com/d3/d3-geo#geoCentroid\n+        .. _path.centroid:\n+            https://github.com/d3/d3-geo#path_centroid\n         \"\"\"\n-        return FunctionExpression(\"geoCentroid\", args)\n+        return FunctionExpression(\"geoCentroid\", (projection, feature, group))\n \n     @classmethod\n-    def treePath(cls, *args) -> FunctionExpression:\n+    def geoScale(\n+        cls, projection: IntoExpression, group: IntoExpression = None, /\n+    ) -> Expression:\n         \"\"\"\n-        For the hierarchy data set with the given *name*, returns the shortest path through from the *source* node id to the *target* node id.\n+        Returns the scale value for the named ``projection``.\n \n-        The path starts at the *source* node, ascends to the least common ancestor of the *source* node and the *target* node, and then descends to the *target* node.\n+        The optional ``group`` argument takes a scenegraph group mark item to indicate the specific\n+        scope in which to look up the projection.\n         \"\"\"\n-        return FunctionExpression(\"treePath\", args)\n+        return FunctionExpression(\"geoScale\", (projection, group))\n \n     @classmethod\n-    def treeAncestors(cls, *args) -> FunctionExpression:\n-        \"\"\"For the hierarchy data set with the given *name*, returns the array of ancestors nodes, starting with the input *node*, then followed by each parent up to the root.\"\"\"\n-        return FunctionExpression(\"treeAncestors\", args)\n+    def treePath(\n+        cls, name: IntoExpression, source: IntoExpression, target: IntoExpression, /\n+    ) -> Expression:\n+        \"\"\"\n+        For the hierarchy data set with the given ``name``, returns the shortest path through from the ``source`` node id to the ``target`` node id.\n+\n+        The path starts at the ``source`` node, ascends to the least common ancestor of the\n+        ``source`` node and the ``target`` node, and then descends to the ``target`` node.\n+        \"\"\"\n+        return FunctionExpression(\"treePath\", (name, source, target))\n \n     @classmethod\n-    def containerSize(cls, *args) -> FunctionExpression:\n+    def treeAncestors(cls, name: IntoExpression, node: IntoExpression, /) -> Expression:\n+        \"\"\"For the hierarchy data set with the given ``name``, returns the array of ancestors nodes, starting with the input ``node``, then followed by each parent up to the root.\"\"\"\n+        return FunctionExpression(\"treeAncestors\", (name, node))\n+\n+    @classmethod\n+    def containerSize(cls) -> Expression:\n         \"\"\"\n-        Returns the current CSS box size (`[el.clientWidth, el.clientHeight]`) of the parent DOM element that contains the Vega view.\n+        Returns the current CSS box size (``[el.clientWidth, el.clientHeight]``) of the parent DOM element that contains the Vega view.\n \n-        If there is no container element, returns `[undefined, undefined]`.\n+        If there is no container element, returns ``[undefined, undefined]``.\n         \"\"\"\n-        return FunctionExpression(\"containerSize\", args)\n+        return FunctionExpression(\"containerSize\", ())\n \n     @classmethod\n-    def screen(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the `window.screen <https://developer.mozilla.org/en-US/docs/Web/API/Window/screen>`__ object, or `{}` if Vega is not running in a browser environment.\"\"\"\n-        return FunctionExpression(\"screen\", args)\n+    def screen(cls) -> Expression:\n+        \"\"\"\n+        Returns the `window.screen`_ object, or ``{}`` if Vega is not running in a browser environment.\n+\n+        .. _window.screen:\n+            https://developer.mozilla.org/en-US/docs/Web/API/Window/screen\n+        \"\"\"\n+        return FunctionExpression(\"screen\", ())\n \n     @classmethod\n-    def windowSize(cls, *args) -> FunctionExpression:\n-        \"\"\"Returns the current window size (`[window.innerWidth, window.innerHeight]`) or `[undefined, undefined]` if Vega is not running in a browser environment.\"\"\"\n-        return FunctionExpression(\"windowSize\", args)\n+    def windowSize(cls) -> Expression:\n+        \"\"\"Returns the current window size (``[window.innerWidth, window.innerHeight]``) or ``[undefined, undefined]`` if Vega is not running in a browser environment.\"\"\"\n+        return FunctionExpression(\"windowSize\", ())\n \n     @classmethod\n-    def warn(cls, *args) -> FunctionExpression:\n+    def warn(\n+        cls, value1: IntoExpression, value2: IntoExpression = None, *args: Any\n+    ) -> Expression:\n         \"\"\"\n         Logs a warning message and returns the last argument.\n \n-        For the message to appear in the console, the visualization view must have the appropriate logging level set.\n+        For the message to appear in the console, the visualization view must have the appropriate\n+        logging level set.\n         \"\"\"\n-        return FunctionExpression(\"warn\", args)\n+        return FunctionExpression(\"warn\", (value1, value2, *args))\n \n     @classmethod\n-    def info(cls, *args) -> FunctionExpression:\n+    def info(\n+        cls, value1: IntoExpression, value2: IntoExpression = None, *args: Any\n+    ) -> Expression:\n         \"\"\"\n         Logs an informative message and returns the last argument.\n \n-        For the message to appear in the console, the visualization view must have the appropriate logging level set.\n+        For the message to appear in the console, the visualization view must have the appropriate\n+        logging level set.\n         \"\"\"\n-        return FunctionExpression(\"info\", args)\n+        return FunctionExpression(\"info\", (value1, value2, *args))\n \n     @classmethod\n-    def debug(cls, *args) -> FunctionExpression:\n+    def debug(\n+        cls, value1: IntoExpression, value2: IntoExpression = None, *args: Any\n+    ) -> Expression:\n         \"\"\"\n         Logs a debugging message and returns the last argument.\n \n-        For the message to appear in the console, the visualization view must have the appropriate logging level set.\n+        For the message to appear in the console, the visualization view must have the appropriate\n+        logging level set.\n         \"\"\"\n-        return FunctionExpression(\"debug\", args)\n+        return FunctionExpression(\"debug\", (value1, value2, *args))\n \n \n _ExprType = expr\ndiff --git a/tools/__init__.py b/tools/__init__.py\nindex 052b8e9c0..46fc97553 100644\n--- a/tools/__init__.py\n+++ b/tools/__init__.py\n@@ -1,8 +1,15 @@\n-from tools import generate_api_docs, generate_schema_wrapper, schemapi, update_init_file\n+from tools import (\n+    generate_api_docs,\n+    generate_schema_wrapper,\n+    markup,\n+    schemapi,\n+    update_init_file,\n+)\n \n __all__ = [\n     \"generate_api_docs\",\n     \"generate_schema_wrapper\",\n+    \"markup\",\n     \"schemapi\",\n     \"update_init_file\",\n ]\ndiff --git a/tools/generate_schema_wrapper.py b/tools/generate_schema_wrapper.py\nindex 85860e8f3..58a395fb3 100644\n--- a/tools/generate_schema_wrapper.py\n+++ b/tools/generate_schema_wrapper.py\n@@ -20,14 +20,8 @@\n sys.path.insert(0, str(Path.cwd()))\n \n \n-from tools.schemapi import (  # noqa: F401\n-    CodeSnippet,\n-    SchemaInfo,\n-    arg_invalid_kwds,\n-    arg_kwds,\n-    arg_required_kwds,\n-    codegen,\n-)\n+from tools.markup import rst_syntax_for_class\n+from tools.schemapi import CodeSnippet, SchemaInfo, arg_kwds, arg_required_kwds, codegen\n from tools.schemapi.utils import (\n     SchemaProperties,\n     TypeAliasTracer,\n@@ -37,16 +31,17 @@\n     import_typing_extensions,\n     indent_docstring,\n     resolve_references,\n-    rst_syntax_for_class,\n     ruff_format_py,\n     ruff_write_lint_format_str,\n     spell_literal,\n )\n+from tools.vega_expr import write_expr_module\n \n if TYPE_CHECKING:\n     from tools.schemapi.codegen import ArgInfo, AttrGetter\n     from vl_convert import VegaThemes\n \n+\n SCHEMA_VERSION: Final = \"v5.20.1\"\n \n \n@@ -60,8 +55,14 @@\n \"\"\"\n \n SCHEMA_URL_TEMPLATE: Final = \"https://vega.github.io/schema/{library}/{version}.json\"\n+VL_PACKAGE_TEMPLATE = (\n+    \"https://raw.githubusercontent.com/vega/vega-lite/refs/tags/{version}/package.json\"\n+)\n SCHEMA_FILE = \"vega-lite-schema.json\"\n THEMES_FILE = \"vega-themes.json\"\n+EXPR_FILE: Path = (\n+    Path(__file__).parent / \"..\" / \"altair\" / \"expr\" / \"__init__.py\"\n+).resolve()\n \n CHANNEL_MYPY_IGNORE_STATEMENTS: Final = \"\"\"\\\n # These errors need to be ignored as they come from the overload methods\n@@ -1207,6 +1208,11 @@ def main() -> None:\n     args = parser.parse_args()\n     copy_schemapi_util()\n     vegalite_main(args.skip_download)\n+    write_expr_module(\n+        vlc.get_vega_version(),\n+        output=EXPR_FILE,\n+        header=HEADER_COMMENT,\n+    )\n \n     # The modules below are imported after the generation of the new schema files\n     # as these modules import Altair. This allows them to use the new changes\ndiff --git a/tools/markup.py b/tools/markup.py\nnew file mode 100644\nindex 000000000..b17e7ad24\n--- /dev/null\n+++ b/tools/markup.py\n@@ -0,0 +1,150 @@\n+\"\"\"Tools for working with formats like ``.md``, ``.rst``.\"\"\"\n+\n+from __future__ import annotations\n+\n+import re\n+from html import unescape\n+from pathlib import Path\n+from typing import TYPE_CHECKING, Any, Iterable, Literal\n+from urllib import request\n+\n+import mistune.util\n+from mistune import InlineParser as _InlineParser\n+from mistune import Markdown as _Markdown\n+from mistune.renderers.rst import RSTRenderer as _RSTRenderer\n+\n+if TYPE_CHECKING:\n+    import sys\n+\n+    if sys.version_info >= (3, 11):\n+        from typing import TypeAlias\n+    else:\n+        from typing_extensions import TypeAlias\n+    from re import Pattern\n+\n+    from mistune import BaseRenderer, BlockParser, BlockState, InlineState\n+\n+    Url: TypeAlias = str\n+\n+Token: TypeAlias = \"dict[str, Any]\"\n+\n+_RE_LINK: Pattern[str] = re.compile(r\"(?<=\\[)([^\\]]+)(?=\\]\\([^\\)]+\\))\", re.MULTILINE)\n+_RE_SPECIAL: Pattern[str] = re.compile(r\"[*_]{2,3}|`\", re.MULTILINE)\n+_RE_LIQUID_INCLUDE: Pattern[str] = re.compile(r\"( \\{% include.+%\\})\")\n+\n+\n+class RSTRenderer(_RSTRenderer):\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+    def inline_html(self, token: Token, state: BlockState) -> str:\n+        html = token[\"raw\"]\n+        return rf\"\\ :raw-html:`{html}`\\ \"\n+\n+\n+class RSTParse(_Markdown):\n+    \"\"\"\n+    Minor extension to support partial `ast`_ conversion.\n+\n+    Only need to convert the docstring tokens to `.rst`.\n+\n+    .. _ast:\n+        https://mistune.lepture.com/en/latest/guide.html#abstract-syntax-tree\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        renderer: BaseRenderer | Literal[\"ast\"] | None,\n+        block: BlockParser | None = None,\n+        inline: _InlineParser | None = None,\n+        plugins=None,\n+    ) -> None:\n+        if renderer == \"ast\":\n+            renderer = None\n+        super().__init__(renderer, block, inline, plugins)\n+\n+    def __call__(self, s: str) -> str:\n+        s = super().__call__(s)  # pyright: ignore[reportAssignmentType]\n+        return unescape(s).replace(r\"\\ ,\", \",\").replace(r\"\\ \", \" \")\n+\n+    def render_tokens(self, tokens: Iterable[Token], /) -> str:\n+        \"\"\"\n+        Render ast tokens originating from another parser.\n+\n+        Parameters\n+        ----------\n+        tokens\n+            All tokens will be rendered into a single `.rst` string\n+        \"\"\"\n+        if self.renderer is None:\n+            msg = \"Unable to render tokens without a renderer.\"\n+            raise TypeError(msg)\n+        state = self.block.state_cls()\n+        s = self.renderer(self._iter_render(tokens, state), state)\n+        return mistune.util.unescape(s)\n+\n+\n+class RSTParseVegaLite(RSTParse):\n+    def __init__(\n+        self,\n+        renderer: RSTRenderer | None = None,\n+        block: BlockParser | None = None,\n+        inline: _InlineParser | None = None,\n+        plugins=None,\n+    ) -> None:\n+        super().__init__(renderer or RSTRenderer(), block, inline, plugins)\n+\n+    def __call__(self, s: str) -> str:\n+        # remove formatting from links\n+        description = \"\".join(\n+            _RE_SPECIAL.sub(\"\", d) if i % 2 else d\n+            for i, d in enumerate(_RE_LINK.split(s))\n+        )\n+\n+        description = super().__call__(description)\n+        # Some entries in the Vega-Lite schema miss the second occurence of '__'\n+        description = description.replace(\"__Default value: \", \"__Default value:__ \")\n+        # Links to the vega-lite documentation cannot be relative but instead need to\n+        # contain the full URL.\n+        description = description.replace(\n+            \"types#datetime\", \"https://vega.github.io/vega-lite/docs/datetime.html\"\n+        )\n+        # Fixing ambiguous unicode, RUF001 produces RUF002 in docs\n+        description = description.replace(\"\u2019\", \"'\")  # noqa: RUF001 [RIGHT SINGLE QUOTATION MARK]\n+        description = description.replace(\"\u2013\", \"-\")  # noqa: RUF001 [EN DASH]\n+        description = description.replace(\"\u00a0\", \" \")  # noqa: RUF001 [NO-BREAK SPACE]\n+        return description.strip()\n+\n+\n+class InlineParser(_InlineParser):\n+    def __init__(self, hard_wrap: bool = False) -> None:\n+        super().__init__(hard_wrap)\n+\n+    def process_text(self, text: str, state: InlineState) -> None:\n+        \"\"\"\n+        Removes `liquid`_ templating markup.\n+\n+        .. _liquid:\n+        https://shopify.github.io/liquid/\n+        \"\"\"\n+        state.append_token({\"type\": \"text\", \"raw\": _RE_LIQUID_INCLUDE.sub(r\"\", text)})\n+\n+\n+def read_ast_tokens(source: Url | Path, /) -> list[Token]:\n+    \"\"\"\n+    Read from ``source``, drop ``BlockState``.\n+\n+    Factored out to provide accurate typing.\n+    \"\"\"\n+    markdown = _Markdown(renderer=None, inline=InlineParser())\n+    if isinstance(source, Path):\n+        tokens = markdown.read(source)\n+    else:\n+        with request.urlopen(source) as response:\n+            s = response.read().decode(\"utf-8\")\n+        tokens = markdown.parse(s, markdown.block.state_cls())\n+    return tokens[0]\n+\n+\n+def rst_syntax_for_class(class_name: str) -> str:\n+    return f\":class:`{class_name}`\"\ndiff --git a/tools/schemapi/__init__.py b/tools/schemapi/__init__.py\nindex 55c5f4148..b3ea70704 100644\n--- a/tools/schemapi/__init__.py\n+++ b/tools/schemapi/__init__.py\n@@ -9,6 +9,7 @@\n )\n from tools.schemapi.schemapi import SchemaBase, Undefined\n from tools.schemapi.utils import OneOrSeq, SchemaInfo\n+from tools.vega_expr import write_expr_module\n \n __all__ = [\n     \"CodeSnippet\",\n@@ -21,4 +22,5 @@\n     \"arg_required_kwds\",\n     \"codegen\",\n     \"utils\",\n+    \"write_expr_module\",\n ]\ndiff --git a/tools/schemapi/utils.py b/tools/schemapi/utils.py\nindex 5c4a84f9c..6bc7b1f4b 100644\n--- a/tools/schemapi/utils.py\n+++ b/tools/schemapi/utils.py\n@@ -8,7 +8,6 @@\n import sys\n import textwrap\n import urllib.parse\n-from html import unescape\n from itertools import chain\n from keyword import iskeyword\n from operator import itemgetter\n@@ -27,9 +26,7 @@\n     overload,\n )\n \n-import mistune\n-from mistune.renderers.rst import RSTRenderer as _RSTRenderer\n-\n+from tools.markup import RSTParseVegaLite, rst_syntax_for_class\n from tools.schemapi.schemapi import _resolve_references as resolve_references\n \n if TYPE_CHECKING:\n@@ -37,7 +34,6 @@\n     from pathlib import Path\n     from re import Pattern\n \n-    from mistune import BlockState\n \n if sys.version_info >= (3, 12):\n     from typing import TypeAliasType\n@@ -76,8 +72,7 @@\n }\n \n _VALID_IDENT: Pattern[str] = re.compile(r\"^[^\\d\\W]\\w*\\Z\", re.ASCII)\n-_RE_LINK: Pattern[str] = re.compile(r\"(?<=\\[)([^\\]]+)(?=\\]\\([^\\)]+\\))\", re.MULTILINE)\n-_RE_SPECIAL: Pattern[str] = re.compile(r\"[*_]{2,3}|`\", re.MULTILINE)\n+\n _RE_LIST_MISSING_ASTERISK: Pattern[str] = re.compile(r\"^-(?=[ `\\\"a-z])\", re.MULTILINE)\n _RE_LIST_MISSING_WHITESPACE: Pattern[str] = re.compile(r\"^\\*(?=[`\\\"a-z])\", re.MULTILINE)\n \n@@ -1083,30 +1078,6 @@ def import_typing_extensions(\n \"\"\"\n \n \n-class RSTRenderer(_RSTRenderer):\n-    def __init__(self) -> None:\n-        super().__init__()\n-\n-    def inline_html(self, token: dict[str, Any], state: BlockState) -> str:\n-        html = token[\"raw\"]\n-        return rf\"\\ :raw-html:`{html}`\\ \"\n-\n-\n-class RSTParse(mistune.Markdown):\n-    def __init__(\n-        self,\n-        renderer: mistune.BaseRenderer,\n-        block: mistune.BlockParser | None = None,\n-        inline: mistune.InlineParser | None = None,\n-        plugins=None,\n-    ) -> None:\n-        super().__init__(renderer, block, inline, plugins)\n-\n-    def __call__(self, s: str) -> str:\n-        s = super().__call__(s)  # pyright: ignore[reportAssignmentType]\n-        return unescape(s).replace(r\"\\ ,\", \",\").replace(r\"\\ \", \" \")\n-\n-\n def indent_docstring(  # noqa: C901\n     lines: Iterable[str], indent_level: int, width: int = 100, lstrip=True\n ) -> str:\n@@ -1192,31 +1163,10 @@ def fix_docstring_issues(docstring: str) -> str:\n     )\n \n \n-def rst_syntax_for_class(class_name: str) -> str:\n-    return f\":class:`{class_name}`\"\n-\n-\n-rst_parse: RSTParse = RSTParse(RSTRenderer())\n+rst_parse: RSTParseVegaLite = RSTParseVegaLite()\n \n \n # TODO: Investigate `mistune.Markdown.(before|after)_render_hooks`.\n def process_description(description: str) -> str:\n     \"\"\"Parse a JSON encoded markdown description into an `RST` string.\"\"\"\n-    # remove formatting from links\n-    description = \"\".join(\n-        _RE_SPECIAL.sub(\"\", d) if i % 2 else d\n-        for i, d in enumerate(_RE_LINK.split(description))\n-    )\n-    description = rst_parse(description)\n-    # Some entries in the Vega-Lite schema miss the second occurence of '__'\n-    description = description.replace(\"__Default value: \", \"__Default value:__ \")\n-    # Links to the vega-lite documentation cannot be relative but instead need to\n-    # contain the full URL.\n-    description = description.replace(\n-        \"types#datetime\", \"https://vega.github.io/vega-lite/docs/datetime.html\"\n-    )\n-    # Fixing ambiguous unicode, RUF001 produces RUF002 in docs\n-    description = description.replace(\"\u2019\", \"'\")  # noqa: RUF001 [RIGHT SINGLE QUOTATION MARK]\n-    description = description.replace(\"\u2013\", \"-\")  # noqa: RUF001 [EN DASH]\n-    description = description.replace(\"\u00a0\", \" \")  # noqa: RUF001 [NO-BREAK SPACE]\n-    return description.strip()\n+    return rst_parse(description)\ndiff --git a/tools/vega_expr.py b/tools/vega_expr.py\nnew file mode 100644\nindex 000000000..ce87cb2fb\n--- /dev/null\n+++ b/tools/vega_expr.py\n@@ -0,0 +1,980 @@\n+\"\"\"\n+Parsing `Vega Expressions`_ docs to write the ``alt.expr`` module.\n+\n+.. _Vega Expressions:\n+   https://vega.github.io/vega/docs/expressions/\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import dataclasses\n+import enum\n+import keyword\n+import re\n+from collections import deque\n+from inspect import getmembers\n+from itertools import chain\n+from textwrap import TextWrapper as _TextWrapper\n+from textwrap import indent\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    ClassVar,\n+    Iterable,\n+    Iterator,\n+    Literal,\n+    Mapping,\n+    Sequence,\n+    overload,\n+)\n+\n+from tools.markup import RSTParse, Token, read_ast_tokens\n+from tools.markup import RSTRenderer as _RSTRenderer\n+from tools.schemapi.schemapi import SchemaBase as _SchemaBase\n+from tools.schemapi.utils import (\n+    ruff_write_lint_format_str as _ruff_write_lint_format_str,\n+)\n+\n+if TYPE_CHECKING:\n+    import sys\n+    from pathlib import Path\n+    from re import Match, Pattern\n+\n+    from mistune import BlockState\n+\n+    if sys.version_info >= (3, 11):\n+        from typing import LiteralString, Self\n+    else:\n+        from typing_extensions import LiteralString, Self\n+    from _typeshed import SupportsKeysAndGetItem\n+\n+    from tools.markup import Url\n+\n+__all__ = [\"parse_expressions\", \"write_expr_module\"]\n+\n+\n+# NOTE: Urls/fragments\n+VEGA_DOCS_URL: LiteralString = \"https://vega.github.io/vega/docs/\"\n+EXPRESSIONS_DOCS_URL: LiteralString = f\"{VEGA_DOCS_URL}expressions/\"\n+EXPRESSIONS_URL_TEMPLATE = \"https://raw.githubusercontent.com/vega/vega/refs/tags/{version}/docs/docs/expressions.md\"\n+\n+\n+# NOTE: Regex patterns\n+FUNCTION_DEF_LINE: Pattern[str] = re.compile(\n+    r\"<a name=\\\"(?P<name>.+)\\\" href=\\\"#(.+)\\\">\"\n+)\n+SENTENCE_BREAK: Pattern[str] = re.compile(r\"(?<!\\.)\\. \")\n+\n+# NOTE: `mistune` token keys/values\n+TYPE: Literal[r\"type\"] = r\"type\"\n+RAW: Literal[\"raw\"] = \"raw\"\n+SOFTBREAK: Literal[\"softbreak\"] = \"softbreak\"\n+TEXT: Literal[\"text\"] = \"text\"\n+CHILDREN: Literal[\"children\"] = \"children\"\n+\n+# NOTE: Punctuation/markers\n+ELLIPSIS: Literal[\"...\"] = \"...\"\n+OPEN_PAREN: Literal[\"(\"] = \"(\"\n+CLOSE_PAREN: Literal[\")\"] = \")\"\n+OPEN_BRACKET: Literal[\"[\"] = \"[\"\n+CLOSE_BRACKET: Literal[\"]\"] = \"]\"\n+INLINE_OVERLOAD: Literal[\" |\"] = \" |\"\n+\n+METHOD_INDENT: LiteralString = 8 * \" \"\n+SECTION_BREAK: Literal[\"\\n\\n\"] = \"\\n\\n\"\n+\n+# NOTE: `altair` types (for annotations)\n+RETURN_WRAPPER: LiteralString = \"FunctionExpression\"\n+RETURN_ANNOTATION: LiteralString = \"Expression\"\n+\"\"\"\n+The annotation is intentionally *less* specific than the real type.\n+\n+``Expression`` is shorter, while preserving all the user-facing functionality\n+\"\"\"\n+\n+CONST_WRAPPER: LiteralString = \"ConstExpression\"\n+CLS_META: LiteralString = \"_ExprMeta\"\n+INPUT_ANNOTATION: LiteralString = \"IntoExpression\"\n+\n+# NOTE: `python`/`mypy` related literals\n+NONE: Literal[r\"None\"] = r\"None\"\n+STAR_ARGS: Literal[\"*args\"] = \"*args\"\n+DECORATOR: LiteralString = r\"@classmethod\"\n+IGNORE_OVERRIDE: LiteralString = r\"# type: ignore[override]\"\n+IGNORE_MISC: LiteralString = r\"# type: ignore[misc]\"\n+\n+MODULE_PRE = '''\\\n+{header}\n+\"\"\"Tools for creating transform & filter expressions with a python syntax.\"\"\"\n+\n+from __future__ import annotations\n+\n+import sys\n+from typing import Any, TYPE_CHECKING\n+\n+from altair.expr.core import {const}, {func}\n+from altair.vegalite.v5.schema.core import ExprRef as _ExprRef\n+\n+if sys.version_info >= (3, 12):\n+    from typing import override\n+else:\n+    from typing_extensions import override\n+\n+if TYPE_CHECKING:\n+    from altair.expr.core import {return_ann}, {input_ann}\n+\n+\n+class {metaclass}(type):\n+    \"\"\"\n+    Metaclass for :class:`expr`.\n+\n+    Currently providing read-only class properties, representing JavaScript constants.\n+    \"\"\"\n+\n+    @property\n+    def NaN(cls) -> {return_ann}:\n+        \"\"\"Not a number (same as JavaScript literal NaN).\"\"\"\n+        return {const}(\"NaN\")\n+\n+    @property\n+    def LN10(cls) -> {return_ann}:\n+        \"\"\"The natural log of 10 (alias to Math.LN10).\"\"\"\n+        return {const}(\"LN10\")\n+\n+    @property\n+    def E(cls) -> {return_ann}:\n+        \"\"\"The transcendental number e (alias to Math.E).\"\"\"\n+        return {const}(\"E\")\n+\n+    @property\n+    def LOG10E(cls) -> {return_ann}:\n+        \"\"\"The base 10 logarithm e (alias to Math.LOG10E).\"\"\"\n+        return {const}(\"LOG10E\")\n+\n+    @property\n+    def LOG2E(cls) -> {return_ann}:\n+        \"\"\"The base 2 logarithm of e (alias to Math.LOG2E).\"\"\"\n+        return {const}(\"LOG2E\")\n+\n+    @property\n+    def SQRT1_2(cls) -> {return_ann}:\n+        \"\"\"The square root of 0.5 (alias to Math.SQRT1_2).\"\"\"\n+        return {const}(\"SQRT1_2\")\n+\n+    @property\n+    def LN2(cls) -> {return_ann}:\n+        \"\"\"The natural log of 2 (alias to Math.LN2).\"\"\"\n+        return {const}(\"LN2\")\n+\n+    @property\n+    def SQRT2(cls) -> {return_ann}:\n+        \"\"\"The square root of 2 (alias to Math.SQRT1_2).\"\"\"\n+        return {const}(\"SQRT2\")\n+\n+    @property\n+    def PI(cls) -> {return_ann}:\n+        \"\"\"The transcendental number pi (alias to Math.PI).\"\"\"\n+        return {const}(\"PI\")\n+'''\n+\n+MODULE_POST = \"\"\"\\\n+_ExprType = expr\n+# NOTE: Compatibility alias for previous type of `alt.expr`.\n+# `_ExprType` was not referenced in any internal imports/tests.\n+\"\"\"\n+\n+CLS_DOC = \"\"\"\n+    Utility providing *constants* and *classmethods* to construct expressions.\n+\n+    `Expressions`_ can be used to write basic formulas that enable custom interactions.\n+\n+    Alternatively, an `inline expression`_ may be defined via :class:`expr()`.\n+\n+    Parameters\n+    ----------\n+    expr: str\n+        A `vega expression`_ string.\n+\n+    Returns\n+    -------\n+    ``ExprRef``\n+\n+    .. _Expressions:\n+        https://altair-viz.github.io/user_guide/interactions.html#expressions\n+    .. _inline expression:\n+       https://altair-viz.github.io/user_guide/interactions.html#inline-expressions\n+    .. _vega expression:\n+       https://vega.github.io/vega/docs/expressions/\n+\n+    Examples\n+    --------\n+    >>> import altair as alt\n+\n+    >>> bind_range = alt.binding_range(min=100, max=300, name=\"Slider value:  \")\n+    >>> param_width = alt.param(bind=bind_range, name=\"param_width\")\n+    >>> param_color = alt.param(\n+    ...     expr=alt.expr.if_(param_width < 200, \"red\", \"black\"),\n+    ...     name=\"param_color\",\n+    ... )\n+    >>> y = alt.Y(\"yval\").axis(titleColor=param_color)\n+\n+    >>> y\n+    Y({\n+      axis: {'titleColor': Parameter('param_color', VariableParameter({\n+        expr: if((param_width < 200),'red','black'),\n+        name: 'param_color'\n+      }))},\n+      shorthand: 'yval'\n+    })\n+    \"\"\"\n+\n+CLS_TEMPLATE = '''\\\n+class expr({base}, metaclass={metaclass}):\n+    \"\"\"{doc}\"\"\"\n+\n+    @override\n+    def __new__(cls: type[{base}], expr: str) -> {base}:  {type_ignore}\n+        return {base}(expr=expr)\n+'''\n+\n+METHOD_SIGNATURE = (\n+    \"\"\"def {title}(cls{sep}{param_list}{marker}) -> {return_ann}:{type_ignore}\"\"\"\n+)\n+\n+METHOD_TEMPLATE = '''\\\n+    {decorator}\n+    {signature}\n+        \"\"\"\n+        {doc}\n+        \"\"\"\n+        return {return_wrapper}({name}, {body_params})\n+'''\n+\n+\n+def _override_predicate(obj: Any, /) -> bool:\n+    return callable(obj) and not (name := obj.__name__).startswith(\"_\")  # noqa: F841\n+\n+\n+_SCHEMA_BASE_MEMBERS: frozenset[str] = frozenset(\n+    nm for nm, _ in getmembers(_SchemaBase, _override_predicate)\n+)\n+\n+\n+class RSTRenderer(_RSTRenderer):\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+    def link(self, token: Token, state: BlockState) -> str:\n+        \"\"\"Store link url, for appending at the end of doc.\"\"\"\n+        attrs = token[\"attrs\"]\n+        url = expand_urls(attrs[\"url\"])\n+        text = self.render_children(token, state)\n+        text = text.replace(\"`\", \"\")\n+        inline = f\"`{text}`_\"\n+        state.env[\"ref_links\"][text] = {\"url\": url}\n+        return inline\n+\n+    def _with_links(self, s: str, links: dict[str, Any] | Any, /) -> str:\n+        it = chain.from_iterable(\n+            (f\".. _{ref_name}:\", f\"    {attrs['url']}\")\n+            for ref_name, attrs in links.items()\n+        )\n+        return \"\\n\".join(chain([s], it))\n+\n+    def __call__(self, tokens: Iterable[Token], state: BlockState) -> str:\n+        result = super().__call__(tokens, state)\n+        if links := state.env.get(\"ref_links\", {}):\n+            return self._with_links(result, links)\n+        else:\n+            return result\n+\n+\n+parser: RSTParse = RSTParse(RSTRenderer())\n+text_wrap = _TextWrapper(\n+    width=100,\n+    break_long_words=False,\n+    break_on_hyphens=False,\n+    initial_indent=METHOD_INDENT,\n+    subsequent_indent=METHOD_INDENT,\n+)\n+\n+\n+class ReplaceMany:\n+    \"\"\"\n+    Perform many ``1:1`` replacements on a given text.\n+\n+    Structured wrapper around a `dict`_ and `re.sub`_.\n+\n+    Parameters\n+    ----------\n+    mapping\n+        Optional initial mapping.\n+    fmt_match\n+        **Combined** format string/regex pattern.\n+        Receives the keys of the final ``self._mapping`` as a positional argument.\n+\n+        .. note::\n+            Special characters must be escaped **first**, if present.\n+\n+    fmt_replace\n+        Format string applied to a succesful match, after substition.\n+        Receives ``self._mapping[key]`` as a positional argument.\n+\n+    .. _dict:\n+        https://docs.python.org/3/library/stdtypes.html#mapping-types-dict\n+    .. _re.sub:\n+        https://docs.python.org/3/library/re.html#re.sub\n+\n+    Examples\n+    --------\n+    Providing a mapping during construction:\n+\n+        string = \"The dog chased the cat, chasing the mouse. Poor mouse\"\n+        animal_replacer = ReplaceMany({\"dog\": \"cat\"})\n+        >>> animal_replacer(string)\n+        'The cat chased the cat, chasing the mouse. Poor mouse'\n+\n+    Updating with new replacements:\n+\n+        animal_replacer.update({\"cat\": \"mouse\", \"mouse\": \"dog\"}, duck=\"rabbit\")\n+        >>> animal_replacer(string, refresh=True)\n+        'The cat chased the mouse, chasing the dog. Poor dog'\n+\n+    Further calls will continue using the most recent update:\n+\n+        >>> animal_replacer(\"duck\")\n+        'rabbit'\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        mapping: Mapping[str, str] | None = None,\n+        /,\n+        fmt_match: str = \"(?P<key>{0})\",\n+        fmt_replace: str = \"{0}\",\n+    ) -> None:\n+        self._mapping: dict[str, str] = dict(mapping) if mapping else {}\n+        self._fmt_match: str = fmt_match\n+        self._fmt_replace: str = fmt_replace\n+        self.pattern: Pattern[str]\n+        self.repl: Callable[[Match[str]], str]\n+        self._is_prepared: bool = False\n+\n+    def update(\n+        self,\n+        m: SupportsKeysAndGetItem[str, str] | Iterable[tuple[str, str]],\n+        /,\n+        **kwds: str,\n+    ) -> None:\n+        \"\"\"Update replacements mapping.\"\"\"\n+        self._mapping.update(m, **kwds)\n+\n+    def clear(self) -> None:\n+        \"\"\"Reset replacements mapping.\"\"\"\n+        self._mapping.clear()\n+\n+    def refresh(self) -> None:\n+        \"\"\"\n+        Compile replacement pattern and generate substitution function.\n+\n+        Notes\n+        -----\n+        Should be called **after** all (old, new) pairs have been collected.\n+        \"\"\"\n+        self.pattern = self._compile()\n+        self.repl = self._replacer()\n+        self._is_prepared = True\n+\n+    def __call__(self, s: str, count: int = 0, /, refresh: bool = False) -> str:\n+        \"\"\"\n+        Replace the leftmost non-overlapping occurrences of ``self.pattern`` in ``s`` using ``self.repl``.\n+\n+        Wraps `re.sub`_\n+\n+        .. _re.sub:\n+            https://docs.python.org/3/library/re.html#re.sub\n+        \"\"\"\n+        if not self._is_prepared or refresh:\n+            self.refresh()\n+        return self.pattern.sub(self.repl, s, count)\n+\n+    def _compile(self) -> Pattern[str]:\n+        if not self._mapping:\n+            name = self._mapping.__qualname__  # type: ignore[attr-defined]\n+            msg = (\n+                f\"Requires {name!r} to be populated, but got:\\n\"\n+                f\"{name}={self._mapping!r}\"\n+            )\n+            raise TypeError(msg)\n+        return re.compile(rf\"{self._fmt_match.format('|'.join(self._mapping))}\")\n+\n+    def _replacer(self) -> Callable[[Match[str]], str]:\n+        def repl(m: Match[str], /) -> str:\n+            return self._fmt_replace.format(self._mapping[m[\"key\"]])\n+\n+        return repl\n+\n+    def __getitem__(self, key: str) -> str:\n+        return self._mapping[key]\n+\n+    def __setitem__(self, key: str, value: str) -> None:\n+        self._mapping[key] = value\n+\n+    def __repr__(self) -> str:\n+        return f\"{type(self).__name__}(\\n    {self._mapping!r}\\n)\"\n+\n+\n+class Special(enum.Enum):\n+    \"\"\"\n+    Special-case identifiers.\n+\n+    Representing ``VegaExprDef`` states that may be otherwise ambiguous.\n+    \"\"\"\n+\n+    NO_PARAMETERS = enum.auto()\n+\n+\n+class VegaExprDef:\n+    \"\"\"\n+    ``SchemaInfo``-like, but operates on `expressions.md`_.\n+\n+    .. _expressions.md:\n+        https://raw.githubusercontent.com/vega/vega/main/docs/docs/expressions.md\n+    \"\"\"\n+\n+    remap_title: ClassVar[ReplaceMany] = ReplaceMany(\n+        fmt_match=r\"(?P<key>{0})\\(\", fmt_replace=\"{0}(\"\n+    )\n+\n+    def __init__(self, name: str, children: Sequence[Token], /) -> None:\n+        self.name: str = name\n+        self._children: Sequence[Token] = children\n+        self.parameters: list[VegaExprParam] = []\n+        self.doc: str = \"\"\n+        self.signature: str = \"\"\n+        self._special: set[Special] = set()\n+\n+    def with_doc(self) -> Self:\n+        \"\"\"\n+        Parses docstring content in full.\n+\n+        Accessible via ``self.doc``\n+        \"\"\"\n+        s: str = parser.render_tokens(self._doc_tokens())\n+        s = italics_to_backticks(s, self.parameter_names(variadic=False))\n+        s = type(self).remap_title(s)\n+        self.doc = format_doc(s)\n+        return self\n+\n+    def with_parameters(self) -> Self:\n+        \"\"\"\n+        Parses signature content into an intermediate representation.\n+\n+        Accessible via  ``self.parameters``.\n+        \"\"\"\n+        split: Iterator[str] = self._split_signature_tokens(exclude_name=True)\n+        self.parameters = list(VegaExprParam.from_texts(split))\n+        if not self.parameters:\n+            self._special.add(Special.NO_PARAMETERS)\n+        return self\n+\n+    def with_signature(self) -> Self:\n+        \"\"\"\n+        Parses ``self.parameters`` into a full signature definition line.\n+\n+        Accessible via  ``self.signature``\n+        \"\"\"\n+        param_list = (\n+            VegaExprParam.star_args()\n+            if self.is_overloaded()\n+            else \", \".join(p.render() for p in self.parameters)\n+        )\n+        self.signature = METHOD_SIGNATURE.format(\n+            title=self.title,\n+            sep=\"\" if self.is_no_parameters() else \",\",\n+            param_list=param_list,\n+            marker=\"\" if (self.is_variadic() or self.is_no_parameters()) else \", /\",\n+            return_ann=RETURN_ANNOTATION,\n+            type_ignore=(\n+                f\"  {IGNORE_OVERRIDE}\" if self.is_incompatible_override() else \"\"\n+            ),\n+        )\n+        return self\n+\n+    def parameter_names(self, *, variadic: bool = True) -> Iterator[str]:\n+        \"\"\"Pass ``variadic=False`` to omit names like``*args``.\"\"\"\n+        if self.parameters:\n+            it: Iterator[str] = (\n+                (p.name for p in self.parameters)\n+                if variadic\n+                else (p.name for p in self.parameters if not p.variadic)\n+            )\n+            yield from it\n+        elif self.is_no_parameters():\n+            yield from ()\n+        else:\n+            msg = (\n+                f\"Cannot provide `parameter_names` until they have been initialized via:\\n\"\n+                f\"{type(self).__name__}.with_parameters()\\n\\n\"\n+                f\"{self!r}\"\n+            )\n+            raise TypeError(msg)\n+\n+    def render(self) -> str:\n+        \"\"\"Return fully parsed method definition.\"\"\"\n+        if self.is_overloaded():\n+            body_params = STAR_ARGS[1:]\n+        else:\n+            body_params = (\n+                f\"({self.parameters[0].name},)\"\n+                if len(self.parameters) == 1\n+                else f\"({','.join(self.parameter_names())})\"\n+            )\n+        return METHOD_TEMPLATE.format(\n+            decorator=DECORATOR,\n+            signature=self.signature,\n+            doc=self.doc,\n+            return_wrapper=RETURN_WRAPPER,\n+            name=f\"{self.name!r}\",\n+            body_params=body_params,\n+        )\n+\n+    @property\n+    def title(self) -> str:\n+        \"\"\"\n+        Use for the method definition, but not when calling internally.\n+\n+        Updates ``remap_title`` class variable for documentation example substitutions.\n+        \"\"\"\n+        title = f\"{self.name}_\" if self.is_keyword() else self.name\n+        type(self).remap_title.update({self.name: f\"alt.expr.{title}\"})\n+        return title\n+\n+    def _signature_tokens(self) -> Iterator[Token]:\n+        \"\"\"\n+        Target for signature appears between 2 softbreak tokens.\n+\n+        - Proceeds to the first token **after** a softbreak\n+        - Yield **only** text tokens\n+        - Skips all inline html tags\n+        - Stops at 2nd softbreak\n+        \"\"\"\n+        it: Iterator[Token] = iter(self)\n+        current = next(it)\n+        while current[TYPE] != SOFTBREAK:\n+            current = next(it)\n+        next(it)\n+        for target in it:\n+            if target[TYPE] == TEXT:\n+                yield target\n+            elif target[TYPE] == SOFTBREAK:\n+                break\n+            else:\n+                continue\n+\n+    def _split_signature_tokens(self, *, exclude_name: bool = False) -> Iterator[str]:\n+        \"\"\"\n+        Normalize the text content of the signature.\n+\n+        Examples\n+        --------\n+        The following definition:\n+\n+            <a name=\"sequence\" href=\"#sequence\">#</a>\n+            <b>sequence</b>([<i>start</i>, ]<i>stop</i>[, <i>step</i>])<br/>\n+            Returns an array containing an arithmetic sequence of numbers.\n+            ...\n+\n+        Will yield:\n+\n+            ['sequence', '(', '[', 'start', ']', 'stop', '[', 'step', ']', ')']\n+\n+        When called with ``exclude_name=True``:\n+\n+            ['(', '[', 'start', ']', 'stop', '[', 'step', ']', ')']\n+        \"\"\"\n+        EXCLUDE_INNER: set[str] = {self.name} if exclude_name else set()\n+        EXCLUDE: set[str] = {\", \"} | EXCLUDE_INNER\n+        for token in self._signature_tokens():\n+            raw: str = token[RAW]\n+            if raw == OPEN_PAREN:\n+                yield raw\n+            elif raw.startswith(OPEN_PAREN):\n+                yield raw[0]\n+                for s in raw[1:].split(\",\"):\n+                    if (clean := s.strip(\" -\")) not in EXCLUDE_INNER:\n+                        yield from VegaExprDef._split_markers(clean)\n+            elif (clean := raw.strip(\", -\")) not in EXCLUDE:\n+                yield from VegaExprDef._split_markers(clean)\n+\n+    @staticmethod\n+    def _split_markers(s: str, /) -> Iterator[str]:\n+        \"\"\"\n+        When ``s`` ends with one of these markers:\n+\n+            \")\", \"]\", \"...\", \" |\"\n+\n+        - Split ``s`` into rest, match\n+            - using the length of the match to index\n+        - Append match to ``end``\n+        - Recurse\n+        \"\"\"  # noqa: D400\n+        if s.isalnum():\n+            yield s\n+        else:\n+            end: list[str] = []\n+            if s.endswith((CLOSE_PAREN, CLOSE_BRACKET)):\n+                end.append(s[-1])\n+                s = s[:-1]\n+            elif s.endswith(ELLIPSIS):\n+                end.append(s[-3:])\n+                s = s[:-3]\n+            elif s.endswith(INLINE_OVERLOAD):\n+                end.append(s[-2:])\n+                s = s[:-2]\n+            if len(s) == 1:\n+                yield s\n+            elif len(s) > 1:\n+                yield from VegaExprDef._split_markers(s)\n+            yield from end\n+\n+    def _doc_tokens(self) -> Sequence[Token]:\n+        \"\"\"Return the slice of `self.children` that contains docstring content.\"\"\"\n+        for idx, item in enumerate(self):\n+            if item[TYPE] == SOFTBREAK and self[idx + 1][TYPE] == TEXT:\n+                return self[idx + 1 :]\n+            else:\n+                continue\n+        msg = (\n+            f\"Expected to find a text node marking the start of docstring content.\\n\"\n+            f\"Failed for:\\n\\n{self!r}\"\n+        )\n+        raise NotImplementedError(msg)\n+\n+    def is_callable(self) -> bool:\n+        \"\"\"\n+        Rough filter for excluding `constants`_.\n+\n+        - Most of the parsing is to handle varying signatures.\n+        - Constants can just be referenced by name, so can skip those\n+\n+        Notes\n+        -----\n+        - Overwriting the <a name> with the rendered text\n+        - required for `clamprange` -> `clampRange`\n+\n+        .. _constants:\n+            https://vega.github.io/vega/docs/expressions/#constants\n+        \"\"\"\n+        if self.is_overloaded_string_array() or self.is_bound_variable_name():\n+            return False\n+        it: Iterator[Token] = iter(self)\n+        current: str = next(it, {}).get(RAW, \"\")\n+        name: str = self.name.casefold()\n+        while current.casefold() != name:\n+            if (el := next(it, None)) is not None:\n+                current = el.get(RAW, \"\")\n+            else:\n+                return False\n+        if current != self.name:\n+            self.name = current\n+        next(it)\n+        return next(it).get(RAW, \"\").startswith(OPEN_PAREN)\n+\n+    def is_bound_variable_name(self) -> bool:\n+        \"\"\"\n+        ``Vega`` `bound variables`_.\n+\n+        These do not provide signatures:\n+\n+            {\"datum\", \"event\", \"signal\"}\n+\n+        .. _bound variables:\n+            https://vega.github.io/vega/docs/expressions/#bound-variables\n+        \"\"\"\n+        RESERVED_NAMES: set[str] = {\"datum\", \"event\", \"signal\"}\n+        return self.name in RESERVED_NAMES\n+\n+    def is_overloaded(self) -> bool:\n+        \"\"\"\n+        Covers the `color functions`_.\n+\n+        These look like:\n+\n+            lab(l, a, b[, opacity]) | lab(specifier)\n+\n+        Looping of parameters is for signatures like `sequence`_:\n+\n+            sequence([start, ]stop[, step])\n+\n+        The optional first parameter, followed by a required one would need an\n+        ``@overload`` in ``python``.\n+\n+        .. _color functions:\n+            https://vega.github.io/vega/docs/expressions/#color-functions\n+        .. _sequence:\n+            https://vega.github.io/vega/docs/expressions/#sequence\n+        \"\"\"\n+        for idx, item in enumerate(self):\n+            if item[TYPE] == TEXT and item.get(RAW, \"\").endswith(INLINE_OVERLOAD):\n+                return self[idx + 1][TYPE] == SOFTBREAK\n+            else:\n+                continue\n+        for idx, p in enumerate(self.parameters):\n+            if not p.required:\n+                others = self.parameters[idx + 1 :]\n+                if not others:\n+                    return False\n+                else:\n+                    return any(sp.required for sp in others)\n+\n+        return False\n+\n+    def is_overloaded_string_array(self) -> bool:\n+        \"\"\"\n+        HACK: There are string/array functions that overlap.\n+\n+        - the `.md` handles this by prefixing the `<a name=...` for the string version\n+        - This is very different to the handled overload kinds\n+        - Both definitions have full documentation and appear under different sections\n+            - Unlike color functions, sequence\n+            - These are inline\n+        \"\"\"\n+        return self.name.startswith(\"string_\")\n+\n+    def is_keyword(self) -> bool:\n+        return keyword.iskeyword(self.name)\n+\n+    def is_incompatible_override(self) -> bool:\n+        \"\"\"\n+        ``self.title`` shadows an unrelated ``SchemaBase`` method.\n+\n+        Requires an ignore comment for a type checker.\n+        \"\"\"\n+        return self.title in _SCHEMA_BASE_MEMBERS\n+\n+    def is_variadic(self) -> bool:\n+        \"\"\"Position-only parameter separator `\"/\"` not allowed after `\"*\"` parameter.\"\"\"\n+        return self.is_overloaded() or any(p.variadic for p in self.parameters)\n+\n+    def is_no_parameters(self) -> bool:\n+        \"\"\"\n+        Signature has been parsed for parameters, but none were present.\n+\n+        For example the definition for `now`_ would **only** return ``True``\n+        after calling ``self.with_parameters()``.\n+\n+        .. _now:\n+            https://vega.github.io/vega/docs/expressions/#now\n+        \"\"\"\n+        return bool(self._special) and Special.NO_PARAMETERS in self._special\n+\n+    def __iter__(self) -> Iterator[Token]:\n+        yield from self._children\n+\n+    @overload\n+    def __getitem__(self, index: int) -> Token: ...\n+    @overload\n+    def __getitem__(self, index: slice) -> Sequence[Token]: ...\n+    def __getitem__(self, index: int | slice) -> Token | Sequence[Token]:\n+        return self._children.__getitem__(index)\n+\n+    def __repr__(self) -> str:\n+        return (\n+            f\"{type(self).__name__}(\\n    \"\n+            f\"name={self.name!r},\\n    \"\n+            f\"parameters={self.parameters!r},\\n    \"\n+            f\"doc={self.doc!r}\\n\"\n+            \")\"\n+        )\n+\n+    @classmethod\n+    def from_tokens(cls, tokens: Iterable[Token], /) -> Iterator[Self]:\n+        \"\"\"\n+        Lazy, filtered partial parser.\n+\n+        Applies a series of filters before rendering everything but the docs.\n+\n+        Parameters\n+        ----------\n+        tokens\n+            `ast tokens`_ produced by ``mistune``\n+\n+        .. _ast tokens:\n+            https://mistune.lepture.com/en/latest/guide.html#abstract-syntax-tree\n+        \"\"\"\n+        for tok in tokens:\n+            if (\n+                (children := tok.get(CHILDREN)) is not None\n+                and (child := next(iter(children)).get(RAW)) is not None\n+                and (match := FUNCTION_DEF_LINE.match(child))\n+                and (node := cls(match[\"name\"], children)).is_callable()\n+            ):\n+                yield node.with_parameters().with_signature()\n+\n+\n+@dataclasses.dataclass\n+class VegaExprParam:\n+    name: str\n+    required: bool\n+    variadic: bool = False\n+\n+    @staticmethod\n+    def star_args() -> LiteralString:\n+        return f\"{STAR_ARGS}: Any\"\n+\n+    def render(self) -> str:\n+        \"\"\"Return as an annotated parameter, with a default if needed.\"\"\"\n+        if self.required:\n+            return f\"{self.name}: {INPUT_ANNOTATION}\"\n+        elif not self.variadic:\n+            return f\"{self.name}: {INPUT_ANNOTATION} = {NONE}\"\n+        else:\n+            return self.star_args()\n+\n+    @classmethod\n+    def from_texts(cls, raw_texts: Iterable[str], /) -> Iterator[Self]:\n+        \"\"\"Yields an ordered parameter list.\"\"\"\n+        is_required: bool = True\n+        for s in raw_texts:\n+            if s not in {OPEN_PAREN, CLOSE_PAREN}:\n+                if s == OPEN_BRACKET:\n+                    is_required = False\n+                    continue\n+                elif s == CLOSE_BRACKET:\n+                    is_required = True\n+                    continue\n+                elif s.isalnum():\n+                    yield cls(s, required=is_required)\n+                elif s == ELLIPSIS:\n+                    yield cls(STAR_ARGS, required=False, variadic=True)\n+                else:\n+                    continue\n+\n+\n+def expand_urls(url: str, /) -> str:\n+    if url.startswith(\"#\"):\n+        url = f\"{EXPRESSIONS_DOCS_URL}{url}\"\n+    else:\n+        url = url.replace(r\"../\", VEGA_DOCS_URL)\n+    return url\n+\n+\n+def format_doc(doc: str, /) -> str:\n+    \"\"\"\n+    Format rendered docstring content.\n+\n+    Primarily used to prevent wrapping on `summary line`_ and references.\n+\n+    Notes\n+    -----\n+    - Source is very different to `vega-lite`\n+    - There are no real sections, so these are created here\n+    - Single line docs are unchanged\n+    - Multi-line have everything following the first line wrappped.\n+        - With a double break inserted for a summary line\n+    - Reference-like links section (if present) are also ommitted from wrapping\n+\n+    .. _summary line:\n+        https://numpydoc.readthedocs.io/en/latest/format.html#short-summary\n+    \"\"\"\n+    sentences: deque[str] = deque(SENTENCE_BREAK.split(doc))\n+    if len(sentences) > 1:\n+        references: str = \"\"\n+        summary = f\"{sentences.popleft()}.\\n\"\n+        last_line = sentences.pop().strip()\n+        sentences = deque(f\"{s}. \" for s in sentences)\n+        if SECTION_BREAK in last_line:\n+            last_line, references = last_line.split(SECTION_BREAK, maxsplit=1)\n+        sentences.append(last_line)\n+        sentences = deque(text_wrap.wrap(\"\".join(sentences)))\n+        sentences.appendleft(summary)\n+        if references:\n+            sentences.extend((\"\", indent(references, METHOD_INDENT)))\n+        return \"\\n\".join(sentences)\n+    elif SECTION_BREAK in doc:\n+        # NOTE: 2 cases have a single line with a reference\n+        summary, references = doc.split(SECTION_BREAK, maxsplit=1)\n+        return \"\\n\".join((summary, \"\", indent(references, METHOD_INDENT)))\n+    else:\n+        return sentences.pop().strip()\n+\n+\n+def italics_to_backticks(s: str, names: Iterable[str], /) -> str:\n+    \"\"\"\n+    Perform a targeted replacement, considering links.\n+\n+    Parameters\n+    ----------\n+    s\n+        String containing rendered `.rst`.\n+    names\n+        Group of names the replacement applies to.\n+\n+    Notes\n+    -----\n+    - Avoids adding backticks to parameter names that are also used in a link.\n+    - All cases of these are for `unit|units`.\n+\n+    Examples\n+    --------\n+    >>> italics_to_backticks(\n+    ...     \"some text and *name* and more text but also *other* text\",\n+    ...     (\"name\", \"other\"),\n+    ... )\n+    \"some text and ``name`` and more text but also ``other`` text\"\n+    \"\"\"\n+    pattern = rf\"(?P<not_link_start>[^`_])\\*(?P<name>{'|'.join(names)})\\*(?P<not_link_end>[^`])\"\n+    return re.sub(pattern, r\"\\g<not_link_start>``\\g<name>``\\g<not_link_end>\", s)\n+\n+\n+def parse_expressions(source: Url | Path, /) -> Iterator[VegaExprDef]:\n+    \"\"\"\n+    Download remote or read local `.md` resource and eagerly parse signatures of relevant definitions.\n+\n+    Yields with docs to ensure each can use all remapped names, regardless of the order they appear.\n+    \"\"\"\n+    tokens = read_ast_tokens(source)\n+    expr_defs = tuple(VegaExprDef.from_tokens(tokens))\n+    VegaExprDef.remap_title.refresh()\n+    for expr_def in expr_defs:\n+        yield expr_def.with_doc()\n+\n+\n+def write_expr_module(version: str, output: Path, *, header: str) -> None:\n+    \"\"\"\n+    Parse an ``expressions.md`` into a ``.py`` module.\n+\n+    Parameters\n+    ----------\n+    version\n+        Vega release version, e.g. ``\"v5.30.0\"``.\n+    output\n+        Target path to write to.\n+    \"\"\"\n+    version = version if version.startswith(\"v\") else f\"v{version}\"\n+    url = EXPRESSIONS_URL_TEMPLATE.format(version=version)\n+    content = (\n+        MODULE_PRE.format(\n+            header=header,\n+            metaclass=CLS_META,\n+            const=CONST_WRAPPER,\n+            return_ann=RETURN_ANNOTATION,\n+            input_ann=INPUT_ANNOTATION,\n+            func=RETURN_WRAPPER,\n+        ),\n+        CLS_TEMPLATE.format(\n+            base=\"_ExprRef\",\n+            metaclass=CLS_META,\n+            doc=CLS_DOC,\n+            type_ignore=IGNORE_MISC,\n+        ),\n+    )\n+    contents = chain(\n+        content,\n+        (expr_def.render() for expr_def in parse_expressions(url)),\n+        [MODULE_POST],\n+    )\n+    print(f\"Generating\\n {url!s}\\n  ->{output!s}\")\n+    _ruff_write_lint_format_str(output, contents)\n", "instance_id": "vega__altair-3600", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement is mostly clear in its intent to improve the user experience of the `alt.expr` method signatures in the Altair library by replacing `*args` with named positional-only parameters. The goal is well-defined: to enhance debugging and usability by raising errors earlier (at definition time) and providing meaningful tracebacks. The problem description includes relevant context, such as links to related issues and discussions, a minimal reproducible example, and a screenshot to illustrate the current error feedback. Benefits and potential drawbacks are also outlined, which adds to the clarity of the proposed solution. Additionally, the example of `alt.expr.if_` provides a concrete illustration of the intended change.\n\nHowever, there are minor ambiguities and missing details that prevent a perfect score. The problem statement mentions that a \"large number\" of signatures need to be updated but does not specify exactly how many or provide a complete list of affected methods, which could impact the scope of work. While the process for updating signatures is described as \"simple\" (referring to docstrings or Vega Expressions), it lacks explicit guidance on handling potential inconsistencies or special cases in the Vega Expressions documentation. Furthermore, edge cases or specific error handling requirements are not mentioned in the problem statement, leaving some uncertainty about whether all use cases are covered by the proposed change. Overall, the statement is valid and clear but misses some minor details that would make it comprehensive.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.45, placing this problem in the medium range. Here's the reasoning based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes primarily affect a single file (`altair/expr/__init__.py`), which limits the scope of impact on the broader codebase. However, the changes are extensive within this file, as they involve updating a large number of method signatures (over 100 methods as seen in the diff). Each method's signature is modified from using `*args` to named positional-only parameters, which requires careful attention to detail to ensure correctness. Additionally, there are minor updates to related tools (`generate_schema_wrapper.py`, `vega_expr.py`, etc.) to support the automated generation of these changes. While the changes do not significantly impact the system's architecture, they do require understanding the interaction between the `expr` class and the underlying Vega expression system.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several Python-specific concepts, including positional-only parameters (introduced in PEP 570), type annotations, and classmethod decorators. It also necessitates understanding the Altair library's integration with Vega expressions and how expressions are validated and executed (in Python vs. JavaScript contexts). Knowledge of automated code generation tools (as seen in `tools/vega_expr.py`) is beneficial, though not strictly necessary for manual updates. The concepts are not overly complex for an experienced developer, but they do require a moderate level of expertise in Python and familiarity with the library's design.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement highlights the benefit of raising `TypeError` at definition time rather than during validation, which improves error handling. However, it does not explicitly address specific edge cases, such as how to handle invalid or malformed expressions beyond missing arguments, or whether there are backward compatibility concerns for existing users who rely on the current `*args` behavior. The code changes themselves do not introduce complex error handling logic beyond ensuring named parameters are provided, but the sheer number of methods to update increases the risk of oversight or inconsistency if not automated properly.\n\n4. **Overall Complexity**: The task is not trivial due to the volume of changes and the need for precision in updating method signatures to match Vega documentation. However, it does not require deep architectural changes or advanced algorithmic work. The primary challenge lies in ensuring all signatures are updated correctly and consistently, which can be mitigated by the provided automation scripts. For a senior engineer with experience in Python and familiarity with Altair or Vega, this task is manageable but requires careful execution to avoid errors across numerous method definitions.\n\nIn summary, the problem falls into the medium difficulty range (0.4-0.6) because it involves a significant number of repetitive but straightforward changes, moderate technical concepts, and limited but present concerns about edge cases. A score of 0.45 reflects the balance between the volume of work and the relatively contained scope of impact.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`fill_null` only fills Int32 columns when filling with an untyped integer literal\n### Checks\n\n- [X] I have checked that this issue has not already been reported.\n- [X] I have confirmed this bug exists on the [latest version](https://pypi.org/project/polars/) of Polars.\n\n### Reproducible example\n\n```python\r\nimport polars as pl\r\n\r\ndf = pl.DataFrame({\r\n    'a': [1, None],\r\n    'b': [None, 2],\r\n})\r\n\r\n# Returns a DataFrame that still contains nulls:\r\nprint(df.fill_null(pl.lit(0)))\r\n\r\n# Works and fills with zero:\r\nprint(df.fill_null(pl.lit(0, dtype=pl.Int64)))\r\n```\r\n\n\n### Log output\n\n```shell\nshape: (2, 2)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a    \u2506 b    \u2502\r\n\u2502 ---  \u2506 ---  \u2502\r\n\u2502 i64  \u2506 i64  \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 1    \u2506 null \u2502\r\n\u2502 null \u2506 2    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nshape: (2, 2)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a   \u2506 b   \u2502\r\n\u2502 --- \u2506 --- \u2502\r\n\u2502 i64 \u2506 i64 \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 1   \u2506 0   \u2502\r\n\u2502 0   \u2506 2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\n### Issue description\n\n-\n\n### Expected behavior\n\nIf this is unsupported then an error could be thrown, but it seems reasonable that this should work and fill nulls with zeros.\n\n### Installed versions\n\n<details>\r\n\r\n```\r\n--------Version info---------\r\nPolars:              1.12.0\r\nIndex type:          UInt32\r\nPlatform:            Linux-6.11.5-200.fc40.x86_64-x86_64-with-glibc2.39\r\nPython:              3.12.7 (main, Oct  1 2024, 00:00:00) [GCC 14.2.1 20240912 (Red Hat 14.2.1-3)]\r\nLTS CPU:             False\r\n\r\n----Optional dependencies----\r\nadbc_driver_manager  <not installed>\r\naltair               <not installed>\r\ncloudpickle          <not installed>\r\nconnectorx           <not installed>\r\ndeltalake            <not installed>\r\nfastexcel            <not installed>\r\nfsspec               2024.6.1\r\ngevent               <not installed>\r\ngreat_tables         <not installed>\r\nmatplotlib           3.8.2\r\nnest_asyncio         1.5.8\r\nnumpy                1.26.3\r\nopenpyxl             <not installed>\r\npandas               2.2.0\r\npyarrow              17.0.0\r\npydantic             <not installed>\r\npyiceberg            <not installed>\r\nsqlalchemy           <not installed>\r\ntorch                2.4.1+cu121\r\nxlsx2csv             <not installed>\r\nxlsxwriter           <not installed>\r\n```\r\n\r\n</details>\r\n\n", "patch": "diff --git a/py-polars/polars/lazyframe/frame.py b/py-polars/polars/lazyframe/frame.py\nindex a72146414b59..9b438ae8dbfa 100644\n--- a/py-polars/polars/lazyframe/frame.py\n+++ b/py-polars/polars/lazyframe/frame.py\n@@ -5667,7 +5667,7 @@ def fill_null(\n         \u2502 4   \u2506 13.0 \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \"\"\"\n-        dtypes: Sequence[PolarsDataType]\n+        dtypes: Sequence[PolarsDataType] | None\n \n         if value is not None:\n \n@@ -5675,7 +5675,7 @@ def infer_dtype(value: Any) -> PolarsDataType:\n                 return next(iter(self.select(value).collect_schema().values()))\n \n             if isinstance(value, pl.Expr):\n-                dtypes = [infer_dtype(value)]\n+                dtypes = None\n             elif isinstance(value, bool):\n                 dtypes = [Boolean]\n             elif matches_supertype and isinstance(value, (int, float)):\n@@ -5707,9 +5707,12 @@ def infer_dtype(value: Any) -> PolarsDataType:\n                 dtypes = [String, Categorical]\n             else:\n                 # fallback; anything not explicitly handled above\n-                dtypes = [infer_dtype(F.lit(value))]\n+                dtypes = None\n \n-            return self.with_columns(F.col(dtypes).fill_null(value, strategy, limit))\n+            if dtypes:\n+                return self.with_columns(\n+                    F.col(dtypes).fill_null(value, strategy, limit)\n+                )\n \n         return self.select(F.all().fill_null(value, strategy, limit))\n \n", "instance_id": "pola-rs__polars-19656", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear, as it identifies a specific issue with the `fill_null` method in the Polars library: it only fills null values in Int32 columns when using an untyped integer literal, but not for other integer types like Int64 unless explicitly specified. The goal is implied (to fix this behavior so that it works consistently across integer types or throws an error if unsupported), and a reproducible example is provided, which helps in understanding the issue. The expected behavior is also mentioned, though not in great detail. However, there are minor ambiguities: the problem description lacks explicit mention of edge cases (e.g., behavior with other data types or mixed-type DataFrames), and the constraints or desired error handling are not fully specified. Additionally, the issue description section is empty, which could have provided more context. Overall, it is clear enough to understand the core issue but misses some details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is limited to a single file (`frame.py`) and specifically to the `fill_null` method, with a small number of lines modified (around 10 lines in the diff). The changes do not impact the broader architecture of the Polars library and are localized to adjusting type inference logic. Second, the technical concepts involved are relatively straightforward: understanding Polars' expression system (`pl.Expr`), type inference, and basic conditional logic in Python. No advanced algorithms, design patterns, or domain-specific knowledge are required beyond familiarity with the Polars API. Third, the problem does not explicitly mention complex edge cases or performance considerations, though the developer must consider whether the change might affect other data types or introduce inconsistencies (e.g., fallback behavior for unhandled types). The code change itself involves modifying how data types are inferred and applied in the `fill_null` operation, which requires some understanding of the library's internals but is not overly complex. Overall, this is a moderately easy bug fix that requires understanding a specific part of the codebase and making targeted modifications, without significant architectural impact or advanced technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "PyPy3.10 test failures due to datetime mocking: `TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types, got MagicMock`\n**Description of issue or feature request**:\r\n\r\nThe test suite currently fails 3 tests on PyPy3.10 7.3.17. This is due to a known limitation of pure Python `datetime` module from CPython. However, CPython normally covers it with a C extension, hiding the problem, but PyPy suffers from it.\r\n\r\n\r\n**Current behavior**:\r\n\r\n```pytb\r\n$ tox -e pypy310\r\npypy310: commands[0]> python3 --version\r\nPython 3.10.14 (39dc8d3c85a7, Aug 29 2024, 13:06:01)\r\n[PyPy 7.3.17 with GCC 13.3.1 20240614]\r\npypy310: commands[1]> python3 -m coverage run aggregate_tests.py\r\n.................................................................................................................................................EEE.....................................\r\n======================================================================\r\nERROR: test_expired_metadata (test_updater_top_level_update.TestRefresh)\r\nVerifies that expired local timestamp/snapshot can be used for\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1379, in patched\r\n    return func(*newargs, **newkeywargs)\r\n  File \"/tmp/python-tuf/tests/test_updater_top_level_update.py\", line 752, in test_expired_metadata\r\n    now = datetime.datetime.now(timezone.utc)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1114, in __call__\r\n    return self._mock_call(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1118, in _mock_call\r\n    return self._execute_mock_call(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1188, in _execute_mock_call\r\n    return self._mock_wraps(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1720, in now\r\n    return cls.fromtimestamp(t, tz)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1709, in fromtimestamp\r\n    return cls._fromtimestamp(t, tz is not None, tz)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1698, in _fromtimestamp\r\n    result = tz.fromutc(result)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 2362, in fromutc\r\n    if isinstance(dt, datetime):\r\nTypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types, got MagicMock\r\n\r\n======================================================================\r\nERROR: test_expired_timestamp_snapshot_rollback (test_updater_top_level_update.TestRefresh)\r\nVerifies that rollback protection is done even if local timestamp has expired.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1379, in patched\r\n    return func(*newargs, **newkeywargs)\r\n  File \"/tmp/python-tuf/tests/test_updater_top_level_update.py\", line 358, in test_expired_timestamp_snapshot_rollback\r\n    now = datetime.datetime.now(timezone.utc)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1114, in __call__\r\n    return self._mock_call(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1118, in _mock_call\r\n    return self._execute_mock_call(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1188, in _execute_mock_call\r\n    return self._mock_wraps(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1720, in now\r\n    return cls.fromtimestamp(t, tz)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1709, in fromtimestamp\r\n    return cls._fromtimestamp(t, tz is not None, tz)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1698, in _fromtimestamp\r\n    result = tz.fromutc(result)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 2362, in fromutc\r\n    if isinstance(dt, datetime):\r\nTypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types, got MagicMock\r\n\r\n======================================================================\r\nERROR: test_expired_timestamp_version_rollback (test_updater_top_level_update.TestRefresh)\r\nVerifies that local timestamp is used in rollback checks even if it is expired.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1379, in patched\r\n    return func(*newargs, **newkeywargs)\r\n  File \"/tmp/python-tuf/tests/test_updater_top_level_update.py\", line 322, in test_expired_timestamp_version_rollback\r\n    now = datetime.datetime.now(timezone.utc)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1114, in __call__\r\n    return self._mock_call(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1118, in _mock_call\r\n    return self._execute_mock_call(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/unittest/mock.py\", line 1188, in _execute_mock_call\r\n    return self._mock_wraps(*args, **kwargs)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1720, in now\r\n    return cls.fromtimestamp(t, tz)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1709, in fromtimestamp\r\n    return cls._fromtimestamp(t, tz is not None, tz)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 1698, in _fromtimestamp\r\n    result = tz.fromutc(result)\r\n  File \"/usr/lib/pypy3.10/datetime.py\", line 2362, in fromutc\r\n    if isinstance(dt, datetime):\r\nTypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types, got MagicMock\r\n\r\n----------------------------------------------------------------------\r\nRan 185 tests in 6.709s\r\n\r\nFAILED (errors=3)\r\npypy310: exit 1 (7.69 seconds) /tmp/python-tuf/tests> python3 -m coverage run aggregate_tests.py pid=490151\r\n  pypy310: FAIL code 1 (7.78=setup[0.07]+cmd[0.02,7.69] seconds)\r\n  evaluation failed :( (7.84 seconds)\r\n```\r\n\r\n**Expected behavior**:\r\n\r\nTests passing :-).\r\n\r\nOne possible solution would be to use a more complete time mocking solution, e.g. [freezegun](https://pypi.org/project/freezegun/). I've been able to quickly confirm that it solves the problem, using a quick patch:\r\n\r\n```diff\r\n from tests.repository_simulator import RepositorySimulator\r\n from tuf.api.exceptions import (\r\n@@ -306,8 +308,7 @@ def test_new_timestamp_unsigned(self) -> None:\r\n \r\n         self._assert_files_exist([Root.type])\r\n \r\n-    @patch.object(datetime, \"datetime\", wraps=datetime.datetime)\r\n-    def test_expired_timestamp_version_rollback(self, mock_time: Mock) -> None:\r\n+    def test_expired_timestamp_version_rollback(self) -> None:\r\n         \"\"\"Verifies that local timestamp is used in rollback checks even if it is expired.\r\n \r\n         The timestamp updates and rollback checks are performed\r\n@@ -331,14 +332,12 @@ def test_expired_timestamp_version_rollback(self, mock_time: Mock) -> None:\r\n \r\n         self.sim.timestamp.version = 1\r\n \r\n-        mock_time.now.return_value = datetime.datetime.now(\r\n-            timezone.utc\r\n-        ) + datetime.timedelta(days=18)\r\n-        patcher = patch(\"datetime.datetime\", mock_time)\r\n-        # Check that a rollback protection is performed even if\r\n-        # local timestamp has expired\r\n-        with patcher, self.assertRaises(BadVersionNumberError):\r\n-            self._run_refresh()\r\n+        with freezegun.freeze_time(datetime.datetime.now( timezone.utc) +\r\n+                                   datetime.timedelta(days=18)):\r\n+            # Check that a rollback protection is performed even if\r\n+            # local timestamp has expired\r\n+            with self.assertRaises(BadVersionNumberError):\r\n+                self._run_refresh()\r\n \r\n         self._assert_version_equals(Timestamp.type, 2)\r\n \r\n```\r\n\r\nIf you agree with this approach, I can make a proper pull request.\n", "patch": "diff --git a/requirements/lint.txt b/requirements/lint.txt\nindex 55640d1f78..b69387daa9 100644\n--- a/requirements/lint.txt\n+++ b/requirements/lint.txt\n@@ -8,3 +8,6 @@\n # are pinned to prevent unexpected linting failures when tools update)\n ruff==0.6.7\n mypy==1.11.2\n+\n+# Required for type stubs\n+freezegun==1.5.1\n", "instance_id": "theupdateframework__python-tuf-2716", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: test failures on PyPy3.10 due to a limitation in the pure Python `datetime` module when mocking is involved, leading to a `TypeError` with `isinstance()` checks. The goal (fixing failing tests) and the context (PyPy vs. CPython behavior) are well-articulated, and a potential solution using `freezegun` is suggested with a code snippet. However, there are minor ambiguities: the problem statement does not explicitly define the full scope of the required changes (e.g., whether all instances of `datetime` mocking need to be replaced or just specific test cases), and it lacks details on potential edge cases or compatibility concerns with `freezegun` across different Python versions or environments. Additionally, while the provided traceback and diff are helpful, there are no explicit constraints or requirements for the solution beyond \"tests passing.\" Overall, the statement is valid and clear but misses some minor details that could affect implementation.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The proposed solution involves replacing the current `datetime` mocking approach (using `unittest.mock`) with `freezegun`, as shown in the provided diff. The code changes appear to be localized to specific test files (e.g., `test_updater_top_level_update.py`) and involve minimal modifications\u2014primarily swapping out mocking logic. The addition of `freezegun` to `requirements/lint.txt` indicates a dependency update, but this is straightforward. There is no indication of widespread changes across the codebase or impact on the system's architecture, making the scope relatively small.\n\n2. **Technical Concepts Involved**: Solving this requires understanding Python's `unittest.mock` library, the behavior of the `datetime` module in PyPy vs. CPython, and the usage of `freezegun` for time mocking. These concepts are moderately complex for a beginner but straightforward for an experienced developer familiar with Python testing and mocking libraries. No advanced algorithms, design patterns, or domain-specific knowledge beyond Python testing practices are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond the PyPy-specific failure, and the proposed solution with `freezegun` appears to address the core issue without introducing new error handling requirements. However, there is a minor implicit concern about ensuring `freezegun` compatibility across different environments or Python versions, though this is not complex to address.\n\n4. **Overall Complexity**: The task requires understanding the root cause of the PyPy failure (a known limitation in `datetime` mocking) and applying a well-documented library (`freezegun`) as a fix. This is a relatively simple bug fix that does not demand deep architectural changes or extensive debugging beyond the provided traceback and solution snippet.\n\nA score of 0.35 reflects an \"Easy\" problem that involves some logic understanding and minor dependency management but remains well within the capabilities of a developer with intermediate Python experience. It is slightly above the lower end of the range due to the need to understand PyPy-specific behavior and mocking nuances, but it does not approach medium difficulty as the solution path is clear and localized.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add switch to phase basic income in\nShould default to false. Currently we phase it in at 100% rate:\r\n* https://github.com/PolicyEngine/policyengine-us/blob/master/policyengine_us/variables/contrib/ubi_center/basic_income/basic_income_phase_in.py\r\n* https://policyengine.org/us/policy?reform=2&focus=gov.contrib.ubi_center.basic_income.phase_in.rate\r\n\r\n[Example: ](https://policyengine.org/us/household?focus=householdOutput.earnings&reform=65723&baseline=2&timePeriod=2024&region=us&household=44009)\r\n<img width=\"865\" alt=\"image\" src=\"https://github.com/user-attachments/assets/19256a99-191b-488d-b927-a63080eb7eb2\">\r\n\r\nAlso add a test to ensure under default conditions someone with no earnings gets the full basic income.\n", "patch": "diff --git a/changelog_entry.yaml b/changelog_entry.yaml\nindex e69de29bb2d..c8ff5abc75f 100644\n--- a/changelog_entry.yaml\n+++ b/changelog_entry.yaml\n@@ -0,0 +1,4 @@\n+- bump: patch\n+  changes:\n+    fixed:\n+    - Add switch to basic income phase-in.\ndiff --git a/policyengine_us/parameters/gov/contrib/ubi_center/basic_income/phase_in/in_effect.yaml b/policyengine_us/parameters/gov/contrib/ubi_center/basic_income/phase_in/in_effect.yaml\nnew file mode 100644\nindex 00000000000..78d95b4677f\n--- /dev/null\n+++ b/policyengine_us/parameters/gov/contrib/ubi_center/basic_income/phase_in/in_effect.yaml\n@@ -0,0 +1,7 @@\n+description: Basic income is phased in if this is true.\n+values:\n+  0000-01-01: false\n+metadata:\n+  unit: bool\n+  period: year\n+  label: Basic income phase-in in effect\ndiff --git a/policyengine_us/variables/contrib/ubi_center/basic_income/basic_income_before_phase_out.py b/policyengine_us/variables/contrib/ubi_center/basic_income/basic_income_before_phase_out.py\nindex 2ae0b5e354b..bf3b5b421d3 100644\n--- a/policyengine_us/variables/contrib/ubi_center/basic_income/basic_income_before_phase_out.py\n+++ b/policyengine_us/variables/contrib/ubi_center/basic_income/basic_income_before_phase_out.py\n@@ -9,33 +9,39 @@ class basic_income_before_phase_out(Variable):\n     definition_period = YEAR\n \n     def formula(tax_unit, period, parameters):\n-        p = parameters(period).gov.contrib.ubi_center.basic_income.amount\n+        p = parameters(period).gov.contrib.ubi_center.basic_income\n         # Start with flat person-level amount.\n-        total_flat_amount = p.person.flat * tax_unit(\"tax_unit_size\", period)\n+        total_flat_amount = p.amount.person.flat * tax_unit(\n+            \"tax_unit_size\", period\n+        )\n         # Add per-age person-level amount.\n         person = tax_unit.members\n         age = person(\"age\", period)\n-        amount_by_age = p.person.by_age.calc(age)\n+        amount_by_age = p.amount.person.by_age.calc(age)\n         total_amount_by_age = tax_unit.sum(amount_by_age)\n         # If available, apply a marriage bonus\n         married = tax_unit.family(\"is_married\", period)\n-        marriage_bonus_rate = p.person.marriage_bonus * total_amount_by_age\n+        marriage_bonus_rate = (\n+            p.amount.person.marriage_bonus * total_amount_by_age\n+        )\n         post_marriage_bonus_amount = total_amount_by_age + marriage_bonus_rate\n         applicable_amount_by_age = where(\n             married, post_marriage_bonus_amount, total_amount_by_age\n         )\n         # Now compute FPG amount.\n         fpg = tax_unit(\"tax_unit_fpg\", period)\n-        fpg_amount = p.tax_unit.fpg_percent * fpg\n+        fpg_amount = p.amount.tax_unit.fpg_percent * fpg\n \n         # Disability amount\n         disabled = person(\"is_ssi_disabled\", period)\n-        disabled_amount = tax_unit.sum(disabled * p.person.disability)\n+        disabled_amount = tax_unit.sum(disabled * p.amount.person.disability)\n         base_amount = (\n             total_flat_amount\n             + applicable_amount_by_age\n             + fpg_amount\n             + disabled_amount\n         )\n-        phase_in_amount = tax_unit(\"basic_income_phase_in\", period)\n-        return min_(base_amount, phase_in_amount)\n+        if p.phase_in.in_effect:\n+            phase_in_amount = tax_unit(\"basic_income_phase_in\", period)\n+            return min_(base_amount, phase_in_amount)\n+        return base_amount\n", "instance_id": "PolicyEngine__policyengine-us-5080", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add a switch for phasing in basic income, defaulting to false, and ensuring that under default conditions, someone with no earnings receives the full basic income. It provides relevant links to the codebase and an example via an external link with a screenshot. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what \"phasing in at 100% rate\" means in technical terms or how the switch should interact with existing logic beyond the default state. Additionally, while a test is requested, the exact conditions or expected output for the test (beyond \"full basic income\") are not specified. Edge cases or potential constraints related to the switch are also not mentioned, which could lead to assumptions during implementation. Overall, the statement is valid and mostly clear but lacks some precision and detail.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting a single variable calculation file (`basic_income_before_phase_out.py`) and adding a new parameter file (`in_effect.yaml`). The modifications involve adding a conditional check for the phase-in switch and updating parameter references. There is no indication of widespread impact on the system's architecture or interactions with multiple modules beyond the immediate context. The amount of code change is small, with a few lines added or modified.\n\n2. **Number of Technical Concepts**: The solution requires basic familiarity with Python, conditional logic, and parameter handling within the PolicyEngine framework. No advanced language features, complex algorithms, or design patterns are needed. The domain-specific knowledge relates to understanding basic income calculations, which appears straightforward based on the provided code (e.g., summing amounts based on flat rates, age, marriage status, etc.). The concepts involved are not particularly complex for a developer with moderate experience.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases or error conditions related to the switch. The code changes introduce a simple boolean check (`if p.phase_in.in_effect`), and no additional error handling logic is added or required based on the diff. The request for a test implies some validation of default behavior, but the complexity of handling edge cases appears minimal at this stage.\n\n4. **Overall Complexity**: The task involves understanding a small part of the codebase and making a straightforward modification to toggle behavior based on a parameter. The addition of a test (though not shown in the diff) would likely be simple, focusing on a specific scenario (no earnings, full basic income). There are no performance considerations, architectural changes, or deep dependencies to navigate.\n\nA score of 0.35 reflects a task that is slightly more involved than a trivial change (e.g., changing a constant) due to the need to understand the parameter structure and basic income logic, but it remains easy overall as it does not require deep expertise or complex modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "33 fixing get object no such key\n## Change Summary\r\n - In `MockMinioBucket.get_object()`, when object cannot be found, raises `S3Error` with error code `NoSuchKey` instead of `404`.\r\n\r\n## Related issue number\r\n - Closes #33 \r\n\r\n## Checklist\r\n- [x] code is ready\r\n- [x] add tests\r\n- [x] all tests passing\r\n- [x] test coverage did not drop\r\n- [x] PR is ready for review\r\n\r\n## Additional Context\r\n\r\nMight be a good idea to reference this part of code? \r\n\r\nhttps://github.com/minio/minio-py/blob/fd3571aa9b9402f050192ad4c6c087d2806ca9eb/minio/api.py#L378\r\n\r\nIt contains error code mapping to a more dev-friendly string. There are many other places where 3-digits error code can be replaced by string, but it depends on your decision. Cheers :)\n", "patch": "diff --git a/setup.py b/setup.py\nindex 3e38d18..1a166d0 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -18,10 +18,10 @@\n     author=\"Oussama Jarrousse\",\n     author_email=\"oussama@jarrousse.org\",\n     description=\"A pytest plugin for mocking Minio S3 interactions\",\n-    long_description=open(\"README.md\").read(),\n+    long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n     keywords=\"pytest minio mock\",\n     extras_require={\"dev\": [\"pre-commit\", \"tox\"]},\n-    version=\"0.4.14\",\n+    version=\"0.4.15\",\n     long_description_content_type=\"text/markdown\",\n     classifiers=[\n         \"Framework :: Pytest\",\n", "instance_id": "oussjarrousse__pytest-minio-mock-35", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes a specific issue in the `MockMinioBucket.get_object()` method where an incorrect error (404) is raised instead of the expected `S3Error` with the error code `NoSuchKey`. The goal of the change is evident, and it ties to a specific issue (#33). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the expected behavior or provide examples of input/output scenarios. Additionally, while a reference to the `minio-py` repository is provided for context on error code mapping, it is not clear whether this is a suggestion for broader changes or just additional information. Edge cases or specific constraints are not mentioned, which could lead to uncertainty during implementation. Overall, the statement is valid and mostly clear but lacks comprehensive details and examples to make it fully unambiguous.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue involves a straightforward bug fix in the error handling logic of a specific method (`MockMinioBucket.get_object()`), likely requiring a small, localized change in a single file or function to replace a 404 error with an `S3Error` having the `NoSuchKey` code. The provided code changes in the diff only show a version bump and a minor update to the `README.md` file encoding in `setup.py`, which suggests that the core logic change is not included in the provided diff but is expected to be minimal. The scope of the change appears limited, with no indication of architectural impact or complex interactions across the codebase. The technical concepts involved are basic\u2014understanding error handling and potentially familiarity with S3 error codes or the Minio library, which are not particularly complex for a developer with moderate experience. Edge cases or additional error handling requirements are not explicitly mentioned in the problem statement, further reducing the complexity. Overall, this task requires understanding some code logic and making a simple modification, aligning with a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`pd.Timedelta` should also accept `float` values\nThe `value` parameter of `Timedelta` currently has the following type-hint:\r\nhttps://github.com/pandas-dev/pandas-stubs/blob/f71224c6211ff436424be614ba47f494f64c4618/pandas-stubs/_libs/tslibs/timedeltas.pyi#L105\r\n\r\nI think it should also accept float, so that type checkers won't complain on following code snippet:\r\n```python\r\nimport pandas as pd\r\npd.Timedelta(3.5, 'hours')\r\n```\r\n\r\n```\r\nArgument of type \"float\" cannot be assigned to parameter \"value\" of type \"str | int | Timedelta | timedelta | timedelta64\" in function \"__new__\"\r\n  Type \"float\" is not assignable to type \"str | int | Timedelta | timedelta | timedelta64\"\r\n    \"float\" is not assignable to \"str\"\r\n    \"float\" is not assignable to \"int\"\r\n    \"float\" is not assignable to \"Timedelta\"\r\n    \"float\" is not assignable to \"timedelta\"\r\n    \"float\" is not assignable to \"timedelta64\"\r\nPylance[reportArgumentType](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportArgumentType)\r\n```\r\n\n`pd.Timedelta` should also accept `float` values\nThe `value` parameter of `Timedelta` currently has the following type-hint:\r\nhttps://github.com/pandas-dev/pandas-stubs/blob/f71224c6211ff436424be614ba47f494f64c4618/pandas-stubs/_libs/tslibs/timedeltas.pyi#L105\r\n\r\nI think it should also accept float, so that type checkers won't complain on following code snippet:\r\n```python\r\nimport pandas as pd\r\npd.Timedelta(3.5, 'hours')\r\n```\r\n\r\n```\r\nArgument of type \"float\" cannot be assigned to parameter \"value\" of type \"str | int | Timedelta | timedelta | timedelta64\" in function \"__new__\"\r\n  Type \"float\" is not assignable to type \"str | int | Timedelta | timedelta | timedelta64\"\r\n    \"float\" is not assignable to \"str\"\r\n    \"float\" is not assignable to \"int\"\r\n    \"float\" is not assignable to \"Timedelta\"\r\n    \"float\" is not assignable to \"timedelta\"\r\n    \"float\" is not assignable to \"timedelta64\"\r\nPylance[reportArgumentType](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportArgumentType)\r\n```\r\n\n", "patch": "diff --git a/pandas-stubs/_libs/tslibs/timedeltas.pyi b/pandas-stubs/_libs/tslibs/timedeltas.pyi\nindex b0f9bdf0..081f11e9 100644\n--- a/pandas-stubs/_libs/tslibs/timedeltas.pyi\n+++ b/pandas-stubs/_libs/tslibs/timedeltas.pyi\n@@ -102,7 +102,7 @@ class Timedelta(timedelta):\n     value: int\n     def __new__(\n         cls,\n-        value: str | int | Timedelta | timedelta | np.timedelta64 = ...,\n+        value: str | float | Timedelta | timedelta | np.timedelta64 = ...,\n         unit: TimeDeltaUnitChoices = ...,\n         *,\n         days: float | np.integer | np.floating = ...,\n", "instance_id": "pandas-dev__pandas-stubs-1026", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent: it identifies a specific issue with the type hint for the `value` parameter of `pd.Timedelta` in the pandas-stubs repository, which currently does not accept `float` values, leading to type checker errors. The goal is explicitly stated (to update the type hint to include `float`), and a relevant code snippet and error message are provided as examples. However, there are minor ambiguities and missing details. For instance, the problem does not discuss whether there are any specific constraints or considerations for accepting `float` values (e.g., precision issues or compatibility with the underlying pandas implementation). Additionally, it does not mention potential edge cases or whether this change aligns with the actual runtime behavior of `pd.Timedelta` in the pandas library. These omissions prevent it from being fully comprehensive, but the core issue and desired outcome are clear enough to proceed with the task.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The required code change is minimal and straightforward: it involves updating a single type hint in a stub file (`pandas-stubs/_libs/tslibs/timedeltas.pyi`) by replacing `int` with `float` in the union type for the `value` parameter. This modification is isolated to a single line in a single file and does not impact the broader codebase or architecture, as it pertains only to type hints for static type checking and not to runtime behavior. The scope of understanding required is limited to basic knowledge of Python type hints and familiarity with union types, which are fundamental concepts. No complex algorithms, design patterns, or domain-specific knowledge are needed, and there are no interactions with other parts of the codebase to consider. Additionally, the problem statement and code changes do not indicate any specific edge cases or error handling requirements beyond the type hint update. While a contributor might need to verify that this change aligns with the actual pandas implementation (to ensure the type hint reflects runtime behavior), this is a minor consideration and does not significantly increase the difficulty. Overall, this is a very easy task suitable for beginners or those with basic familiarity with Python type annotations.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[FEATURE] \u6dfb\u52a0\u914d\u7f6e\u9879\uff0c\u4ee5\u4fbf\u4f7f\u7528\u4e91\u670d\u52a1\u5668\u6216\u53cd\u5411\u4ee3\u7406\u65f6\u751f\u6210\u6b63\u786e\u7684\u516c\u7f51 API \u5730\u5740\n**\u89e3\u51b3\u7684\u95ee\u9898 / Problem Solved**  \r\n\u5728\u70b9\u51fb\u68c0\u7d22\u5230\u7684\u6587\u6863, \u8fdb\u884c\u4e0b\u8f7d\u65f6, \u76ee\u524d\u7684\u903b\u8f91\u662f\u8df3\u8f6c\u5230api\u670d\u52a1\u53bb.\r\n![image](https://github.com/user-attachments/assets/d4b7f3ef-6cdc-414c-92b4-a9f8ff282d5a)\r\n\r\n\u4f46\u662f\u5728\u4ecewebui\u670d\u52a1\u8df3\u8f6c\u5230api\u670d\u52a1\u65f6\u5b58\u5728\u4e24\u4e2a\u95ee\u9898, \u4f1a\u5bfc\u81f4\u8bbf\u95ee\u5931\u8d25:\r\n1. \u8df3\u8f6c\u94fe\u63a5\u76f4\u63a5\u53d6\u7684api\u670d\u52a1\u7684host\u5730\u5740, \u5982\u679capi.host\u914d\u7684\u662f0.0.0.0, \u90a3\u4e48\u5728api_address()\u65b9\u6cd5\u91cc\u9762\u4f1a\u88ab\u8f6c\u6210127.0.0.1; \u5982\u679c\u5728\u4e91\u670d\u52a1\u5668\u4e0a\u914d\u6210\u516c\u7f51IP, \u5219\u4f1abind\u5931\u8d25(\u56e0\u4e3a\u516c\u7f51IP\u5e76\u4e0d\u5728\u4e91\u670d\u52a1\u5668\u7684\u7f51\u5361\u4e0a). \u8fd9\u4e24\u79cd\u60c5\u51b5\u5bfc\u81f4\u6ca1\u529e\u6cd5\u8ba9webui\u80fd\u591f\u6b63\u786e\u8df3\u8f6c\u5230api\u670d\u52a1\u7684\u516c\u7f51ip\u4e0a.\r\n2. port\u540e\u9762\u591a\u4e86\u4e00\u4e2a\"/\", \u4e5f\u4f1a\u5bfc\u81f4\u8bbf\u95ee\u5931\u8d25.\r\n\r\n(\u5982\u679c\u662f\u672c\u673a\u542f\u52a8\u670d\u52a1, \u7136\u540e\u5728\u672c\u5730\u5f00\u6d4f\u89c8\u5668, \u662f\u4e0d\u4f1a\u53d1\u73b0\u8fd9\u4e2a\u95ee\u9898\u7684. \u4f46\u662f\u5728\u4e91\u670d\u52a1\u5668\u4e0a\u542f\u52a8\u670d\u52a1, \u8981\u4ece\u672c\u5730\u8bbf\u95ee, \u5c31\u6709\u95ee\u9898\u4e86)\r\n\r\n![image](https://github.com/user-attachments/assets/de88f386-bb49-4aec-8333-ce41c99079b7)\r\n\r\n\u7ecf\u68c0\u67e5\u6e90\u7801, \u4e3b\u8981\u662f\u8fd9\u91cc\u7684\u95ee\u9898.\r\n![image](https://github.com/user-attachments/assets/d1d334ed-2ac0-4fb9-a33e-20bed3d811c7)\r\n\r\n**\u5b9e\u73b0\u5efa\u8bae / Implementation Suggestions**  \r\n\u5728api\u670d\u52a1\u7684host\u914d\u7f6e\u4e4b\u5916\u989d\u5916\u589e\u52a0\u4e00\u4e2apublic_host\u4e4b\u7c7b\u7684\u914d\u7f6e, \u5982\u679c\u8981\u4ecewebui\u670d\u52a1\u8df3\u8f6c\u5230api\u670d\u52a1, \u9700\u8981\u4f7f\u7528\u8fd9\u4e2aIP\u5730\u5740, \u800c\u4e0d\u662f\u76f4\u63a5\u53d6host\u5730\u5740(\u5f88\u591a\u65f6\u5019\u90fd\u662f0.0.0.0).\r\n\r\n![image](https://github.com/user-attachments/assets/44ed9463-2711-4ebd-b8dd-529b04605a26)\nFix bug of ChromaDB Collection has no attribute 'as_retriever'\nFix issue #4525 \r\nUpdated ChromaDB service refering to langchain document:\r\nhttps://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/#retriever-options\r\n* Passing a Chroma Client into Langchain instead of using Chroma Client directly\r\n* Use Chroma as a retriever\r\n\r\nInstall langchain_chroma with:\r\n`pip install langchain-chroma`\n", "patch": "diff --git a/README.md b/README.md\nindex 411637a66e..70ea5fc9ef 100644\n--- a/README.md\n+++ b/README.md\n@@ -303,9 +303,9 @@ chatchat start -a\n ### Docker \u90e8\u7f72\n \n ```shell\n-docker pull chatimage/chatchat:0.3.1.1-2024-0714\n+docker pull chatimage/chatchat:0.3.1.2\n \n-docker pull ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.1-2024-0714 # \u56fd\u5185\u955c\u50cf\n+docker pull ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.2 # \u56fd\u5185\u955c\u50cf\n ```\n \n > [!important]\ndiff --git a/docker/docker-compose.yaml b/docker/docker-compose.yaml\nindex 8c8cbbaf73..c7cdeac1d0 100644\n--- a/docker/docker-compose.yaml\n+++ b/docker/docker-compose.yaml\n@@ -25,8 +25,8 @@ services:\n     # environment:\n     #   - XINFERENCE_MODEL_SRC=modelscope\n   chatchat:\n-    image: chatimage/chatchat:0.3.1.1-2024-0714\n-    # image: ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.1-2024-0714\n+    image: chatimage/chatchat:0.3.1.2\n+    # image: ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.2\n     restart: always\n     # ports: # \u4e0d\u4f7f\u7528 host network \u65f6\u53ef\u6253\u5f00.\n     #   - \"7861:7861\"\ndiff --git a/docs/install/README_docker.md b/docs/install/README_docker.md\nindex af4b0354be..c627e84fc8 100644\n--- a/docs/install/README_docker.md\n+++ b/docs/install/README_docker.md\n@@ -59,7 +59,7 @@ docker-compose up -d\n ```text\n WARN[0000] /root/docker-compose.yaml: `version` is obsolete \n NAME                IMAGE                           COMMAND                  SERVICE      CREATED         STATUS         PORTS\n-root-chatchat-1     chatimage/chatchat:0.3.1.1-2024-0714   \"chatchat -a\"            chatchat     3 minutes ago   Up 3 minutes   \n+root-chatchat-1     chatimage/chatchat:0.3.1.2   \"chatchat -a\"            chatchat     3 minutes ago   Up 3 minutes   \n root-xinference-1   xprobe/xinference:v0.12.1       \"/opt/nvidia/nvidia_\u2026\"   xinference   3 minutes ago   Up 3 minutes\n ```\n ```shell\n@@ -95,7 +95,7 @@ mkdir -p ~/chatchat\n ```yaml\n   (\u4e0a\u6587 ...)\n   chatchat:\n-    image: chatimage/chatchat:0.3.1.1-2024-0714\n+    image: chatimage/chatchat:0.3.1.2\n     (\u7701\u7565 ...)\n     # \u5c06\u672c\u5730\u8def\u5f84(~/chatchat/data)\u6302\u8f7d\u5230\u5bb9\u5668\u9ed8\u8ba4\u6570\u636e\u8def\u5f84(/usr/local/lib/python3.11/site-packages/chatchat/data)\u4e2d\n     # volumes:\n@@ -106,7 +106,7 @@ mkdir -p ~/chatchat\n ```yaml\n   (\u4e0a\u6587 ...)\n   chatchat:\n-    image: chatimage/chatchat:0.3.1.1-2024-0714\n+    image: chatimage/chatchat:0.3.1.2\n     (\u7701\u7565 ...)\n     # \u5c06\u672c\u5730\u8def\u5f84(~/chatchat/data)\u6302\u8f7d\u5230\u5bb9\u5668\u9ed8\u8ba4\u6570\u636e\u8def\u5f84(/usr/local/lib/python3.11/site-packages/chatchat/data)\u4e2d\n     volumes:\n@@ -163,7 +163,7 @@ WARN[0000] /root/docker-compose.yaml: `version` is obsolete\n [root@VM-2-15-centos ~]# docker-compose ps\n WARN[0000] /root/docker-compose.yaml: `version` is obsolete \n NAME                IMAGE                           COMMAND                  SERVICE      CREATED          STATUS          PORTS\n-root-chatchat-1     chatimage/chatchat:0.3.1.1-2024-0714   \"chatchat -a\"            chatchat     33 seconds ago   Up 32 seconds   \n+root-chatchat-1     chatimage/chatchat:0.3.1.2   \"chatchat -a\"            chatchat     33 seconds ago   Up 32 seconds   \n root-xinference-1   xprobe/xinference:v0.12.1       \"/opt/nvidia/nvidia_\u2026\"   xinference   45 minutes ago   Up 45 minutes   \n [root@VM-2-15-centos ~]# ss -anptl | grep -E '(8501|7861|9997)'\n LISTEN 0      128          0.0.0.0:9997       0.0.0.0:*    users:((\"pt_main_thread\",pid=1489804,fd=21))\ndiff --git a/libs/chatchat-server/README.md b/libs/chatchat-server/README.md\nindex 98ec04f8ca..a3895c8bc2 100644\n--- a/libs/chatchat-server/README.md\n+++ b/libs/chatchat-server/README.md\n@@ -43,9 +43,9 @@ pip install langchain-chatchat\n 3. Docker\n \n ```shell\n-docker pull chatimage/chatchat:0.3.1.1-2024-0714\n+docker pull chatimage/chatchat:0.3.1.2-2024-0720\n \n-docker pull ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.1-2024-0714 # \u56fd\u5185\u955c\u50cf\n+docker pull ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.2-2024-0720 # \u56fd\u5185\u955c\u50cf\n ```\n \n > [!important]\n@@ -82,6 +82,34 @@ chatchat start -a\n \u66f4\u591a\u547d\u4ee4\u53ef\u4ee5\u901a\u8fc7 `chatchat --help` \u67e5\u770b\u3002\n \n ### \u66f4\u65b0\u65e5\u5fd7\uff1a\n+\n+#### 0.3.1.2 (2024-07-20)\n+- \u65b0\u529f\u80fd\uff1a\n+    - Model Platform \u652f\u6301\u914d\u7f6e\u4ee3\u7406 by @liunux4odoo (#4492)\n+    - \u7ed9\u5b9a\u4e00\u4e2a\u9ed8\u8ba4\u53ef\u7528\u7684 searx \u670d\u52a1\u5668 by @liunux4odoo (#4504)\n+    - \u66f4\u65b0 docker \u955c\u50cf by @yuehua-s @imClumsyPanda (#4511)\n+    - \u65b0\u589eURL\u5185\u5bb9\u9605\u8bfb\u5668\uff1a\u901a\u8fc7jina-ai/reader\u9879\u76ee\uff0c\u5c06url\u5185\u5bb9\u5904\u7406\u4e3allm\u6613\u4e8e\u7406\u89e3\u7684\u6587\u672c\u5f62\u5f0f by @ganwumeng @imClumsyPanda (#4547)\n+    - \u4f18\u5316qwen\u6a21\u578b\u4e0b\u5bf9tools\u7684json\u4fee\u590d\u6210\u529f\u7387 by @ganwumeng (#4554)\n+    - \u5141\u8bb8\u7528\u6237\u5728 basic_settings.API_SERVER \u4e2d\u914d\u7f6e public_host,public_port\uff0c\u4ee5\u4fbf\u4f7f\u7528\u4e91\u670d\u52a1\u5668\u6216\u53cd\u5411\u4ee3\u7406\u65f6\u751f\u6210\u6b63\u786e\u7684\u516c\u7f51 API\n+      \u5730\u5740 by @liunux4odoo (#4567)\n+    - \u6dfb\u52a0\u6a21\u578b\u548c\u670d\u52a1\u81ea\u52a8\u5316\u811a\u672c by @glide-the (#4573)\n+    - \u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5 by @glide-the (#4573)\n+- \u4fee\u590d\uff1a\n+    - WEBUI \u4e2d\u8bbe\u7f6e System message \u65e0\u6548 by @liunux4odoo (#4491)\n+    - \u79fb\u9664\u65e0\u6548\u7684 vqa_processor & aqa_processor \u5de5\u5177 by @liunux4odoo (#4498)\n+    - KeyError of 'template' \u9519\u8bef by @liunux4odoo (#4501)\n+    - \u6267\u884c chatchat init \u65f6 nltk_data \u76ee\u5f55\u8bbe\u7f6e\u9519\u8bef by @liunux4odoo (#4523)\n+    - \u6267\u884c chatchat init \u65f6 \u51fa\u73b0 xinference-client \u8fde\u63a5\u9519\u8bef by @imClumsyPanda (#4573)\n+    - xinference \u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u4f7f\u7528\u7f13\u5b58\uff0c\u63d0\u9ad8 UI \u54cd\u5e94\u901f\u5ea6 by @liunux4odoo (#4510)\n+    - chatchat.log \u4e2d\u91cd\u590d\u8bb0\u5f55 by @liunux4odoo (#4517)\n+    - \u4f18\u5316\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u9012\u548c\u524d\u7aef\u663e\u793a by @liunux4odoo (#4531)\n+    - \u4fee\u6b63 openai.chat.completions.create \u53c2\u6570\u6784\u9020\u65b9\u5f0f\uff0c\u63d0\u9ad8\u517c\u5bb9\u6027  by @liunux4odoo (#4540)\n+    - Milvus retriever NotImplementedError by @kwunhang (#4536)\n+    - Fix bug of ChromaDB Collection as retriever by @kwunhang (#4541)\n+    - langchain \u7248\u672c\u5347\u7ea7\u540e\uff0cDocumentWithVsId \u51fa\u73b0 id \u91cd\u590d\u95ee\u9898 by @liunux4odoo (#4548)\n+    - \u91cd\u5efa\u77e5\u8bc6\u5e93\u65f6\u53ea\u5904\u7406\u4e86\u4e00\u4e2a\u77e5\u8bc6\u5e93 by @liunux4odoo (#4549)\n+    - chat api error because openapi set max_tokens to 0 by default by @liunux4odoo (#4564)\n+\n #### 0.3.1.1 (2024-07-15)\n - \u4fee\u590d\uff1a\n   - WEBUI \u4e2d\u8bbe\u7f6e system message \u65e0\u6548([#4491](https://github.com/chatchat-space/Langchain-Chatchat/pull/4491))\ndiff --git a/libs/chatchat-server/README_en.md b/libs/chatchat-server/README_en.md\nindex 0001fdabb9..9a1475e61b 100644\n--- a/libs/chatchat-server/README_en.md\n+++ b/libs/chatchat-server/README_en.md\n@@ -37,9 +37,9 @@ In addition to installing through Pypi, you can also choose to use [source code\n \n 3. Docker\n ```shell\n-docker pull chatimage/chatchat:0.3.1.1-2024-0714\n+docker pull chatimage/chatchat:0.3.1.2\n \n-docker pull ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.1-2024-0714 # \u56fd\u5185\u955c\u50cf\n+docker pull ccr.ccs.tencentyun.com/chatchat/chatchat:0.3.1.2 # \u56fd\u5185\u955c\u50cf\n ```\n > [!important]\n > Strong recommendation: Use docker compose for deployment, refer to [README.docker](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/docs/install/README_docker.md) for details \n@@ -66,6 +66,7 @@ chatchat start -a\n ```\n If there are no errors, the browser page will automatically pop up.\n ### Update log:\n+ \n #### 0.3.1.1 (2024-07-15)\n - Fix:\n   - Invalid system message setting in WEBUI ([# 4491](https://github.com/chatchat-space/Langchain-Chatchat/pull/4491 ))\ndiff --git a/libs/chatchat-server/chatchat/__init__.py b/libs/chatchat-server/chatchat/__init__.py\nindex 260c070a89..7ec2aea660 100644\n--- a/libs/chatchat-server/chatchat/__init__.py\n+++ b/libs/chatchat-server/chatchat/__init__.py\n@@ -1,1 +1,1 @@\n-__version__ = \"0.3.1\"\n+__version__ = \"0.3.1.2\"\ndiff --git a/libs/chatchat-server/chatchat/cli.py b/libs/chatchat-server/chatchat/cli.py\nindex c32149d3f1..be63055148 100644\n--- a/libs/chatchat-server/chatchat/cli.py\n+++ b/libs/chatchat-server/chatchat/cli.py\n@@ -46,15 +46,16 @@ def init(\n     Settings.set_auto_reload(False)\n     bs = Settings.basic_settings\n     kb_names = [x.strip() for x in kb_names.split(\",\")]\n-    logger.info(f\"\u5f00\u59cb\u521d\u59cb\u5316\u9879\u76ee\u6570\u636e\u76ee\u5f55\uff1a{Settings.CHATCHAT_ROOT}\")\n+    logger.success(f\"\u5f00\u59cb\u521d\u59cb\u5316\u9879\u76ee\u6570\u636e\u76ee\u5f55\uff1a{Settings.CHATCHAT_ROOT}\")\n     Settings.basic_settings.make_dirs()\n-    logger.info(\"\u521b\u5efa\u6240\u6709\u6570\u636e\u76ee\u5f55\uff1a\u6210\u529f\u3002\")\n+    logger.success(\"\u521b\u5efa\u6240\u6709\u6570\u636e\u76ee\u5f55\uff1a\u6210\u529f\u3002\")\n     if(bs.PACKAGE_ROOT / \"data/knowledge_base/samples\" != Path(bs.KB_ROOT_PATH) / \"samples\"):\n         shutil.copytree(bs.PACKAGE_ROOT / \"data/knowledge_base/samples\", Path(bs.KB_ROOT_PATH) / \"samples\", dirs_exist_ok=True)\n-    logger.info(\"\u590d\u5236 samples \u77e5\u8bc6\u5e93\u6587\u4ef6\uff1a\u6210\u529f\u3002\")\n+    logger.success(\"\u590d\u5236 samples \u77e5\u8bc6\u5e93\u6587\u4ef6\uff1a\u6210\u529f\u3002\")\n     nltk.data.path.append(str(bs.PACKAGE_ROOT / \"data/nltk_data\"))\n+    logger.success(\"\u8bbe\u7f6e nltk_data \u8def\u5f84\uff1a\u6210\u529f\u3002\")\n     create_tables()\n-    logger.info(\"\u521d\u59cb\u5316\u77e5\u8bc6\u5e93\u6570\u636e\u5e93\uff1a\u6210\u529f\u3002\")\n+    logger.success(\"\u521d\u59cb\u5316\u77e5\u8bc6\u5e93\u6570\u636e\u5e93\uff1a\u6210\u529f\u3002\")\n \n     if xf_endpoint:\n         Settings.model_settings.MODEL_PLATFORMS[0].api_base_url = xf_endpoint\n@@ -66,8 +67,8 @@ def init(\n     Settings.createl_all_templates()\n     Settings.set_auto_reload(True)\n \n-    logger.info(\"\u751f\u6210\u9ed8\u8ba4\u914d\u7f6e\u6587\u4ef6\uff1a\u6210\u529f\u3002\")\n-    logger.warning(\"<red>\u8bf7\u5148\u68c0\u67e5 model_settings.yaml \u91cc\u6a21\u578b\u5e73\u53f0\u3001LLM\u6a21\u578b\u548cEmbed\u6a21\u578b\u4fe1\u606f\u6b63\u786e</red>\")\n+    logger.success(\"\u751f\u6210\u9ed8\u8ba4\u914d\u7f6e\u6587\u4ef6\uff1a\u6210\u529f\u3002\")\n+    logger.success(\"\u8bf7\u5148\u68c0\u67e5\u786e\u8ba4 model_settings.yaml \u91cc\u6a21\u578b\u5e73\u53f0\u3001LLM\u6a21\u578b\u548cEmbed\u6a21\u578b\u4fe1\u606f\u5df2\u7ecf\u6b63\u786e\")\n \n     if recreate_kb:\n         folder2db(kb_names=kb_names,\n@@ -76,7 +77,7 @@ def init(\n                   embed_model=get_default_embedding())\n         logger.success(\"<green>\u6240\u6709\u521d\u59cb\u5316\u5df2\u5b8c\u6210\uff0c\u6267\u884c chatchat start -a \u542f\u52a8\u670d\u52a1\u3002</green>\")\n     else:\n-        logger.success(\"<green>\u6267\u884c chatchat kb -r \u521d\u59cb\u5316\u77e5\u8bc6\u5e93\uff0c\u7136\u540e chatchat start -a \u542f\u52a8\u670d\u52a1\u3002</green>\")\n+        logger.success(\"\u6267\u884c chatchat kb -r \u521d\u59cb\u5316\u77e5\u8bc6\u5e93\uff0c\u7136\u540e chatchat start -a \u542f\u52a8\u670d\u52a1\u3002\")\n \n \n main.add_command(startup_main, \"start\")\ndiff --git a/libs/chatchat-server/chatchat/init_database.py b/libs/chatchat-server/chatchat/init_database.py\nindex bdd94817d3..506df79e85 100644\n--- a/libs/chatchat-server/chatchat/init_database.py\n+++ b/libs/chatchat-server/chatchat/init_database.py\n@@ -58,7 +58,7 @@ def worker(args: dict):\n         end_time = datetime.now()\n         print(f\"\u603b\u8ba1\u7528\u65f6\\t\uff1a{end_time-start_time}\\n\")\n     except Exception as e:\n-        logger.error(e, exc_info=True)\n+        logger.exception(e)\n \n \n @click.command(help=\"\u77e5\u8bc6\u5e93\u76f8\u5173\u529f\u80fd\")\ndiff --git a/libs/chatchat-server/chatchat/server/agent/agent_factory/qwen_agent.py b/libs/chatchat-server/chatchat/server/agent/agent_factory/qwen_agent.py\nindex 4141a9bfad..c957f3cd3c 100644\n--- a/libs/chatchat-server/chatchat/server/agent/agent_factory/qwen_agent.py\n+++ b/libs/chatchat-server/chatchat/server/agent/agent_factory/qwen_agent.py\n@@ -92,6 +92,12 @@ def format_messages(self, **kwargs) -> str:\n         formatted = self.template.format(**kwargs)\n         return [HumanMessage(content=formatted)]\n \n+def validate_json(json_data: str):\n+    try:\n+        json.loads(json_data)\n+        return True\n+    except ValueError:\n+        return False\n \n class QwenChatAgentOutputParserCustom(StructuredChatOutputParser):\n     \"\"\"Output parser with retries for the structured chat agent with custom qwen prompt.\"\"\"\n@@ -109,9 +115,26 @@ def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n                 # ollama\u90e8\u7f72\u7684qwen\uff0c\u8fd4\u56de\u7684json\u952e\u503c\u53ef\u80fd\u4e3a\u5355\u5f15\u53f7\uff0c\u53ef\u80fd\u7f3a\u5c11\u6700\u540e\u7684\u5f15\u53f7\u548c\u62ec\u53f7\n                 if not json_string.endswith('\"}'):\n                     print(\"\u5c1d\u8bd5\u4fee\u590d\u683c\u5f0f\u4e0d\u6b63\u786e\u7684json\u8f93\u51fa:\" + json_string)\n-                    json_string = (json_string + '\"}').replace(\"'\", '\"')\n-                    print(\"\u4fee\u590d\u540e\u7684json:\" + json_string)\n+                    fixed_json_string = (json_string + '\"}').replace(\"'\", '\"')\n+\n+                    fixed = True\n+                    if not validate_json(fixed_json_string):\n+                        # ollama\u90e8\u7f72\u7684qwen\uff0c\u8fd4\u56de\u7684json\u53ef\u80fd\u6709\u6ce8\u91ca\uff0c\u9700\u8981\u53bb\u6389\u6ce8\u91ca\n+                        fixed_json_string = (re.sub(r'//.*', '', (json_string + '\"}').replace(\"'\", '\"'))\n+                                             .strip()\n+                                             .replace('\\n', ''))\n+                        if not validate_json(fixed_json_string):\n+                            fixed = False\n+                            print(\"\u5c1d\u8bd5\u4fee\u590djson\u683c\u5f0f\u5931\u8d25\uff1a\" + json_string)\n+                    if fixed:\n+                        json_string = fixed_json_string\n+                        print(\"\u4fee\u590d\u540e\u7684json\u8f93\u51fa:\" + json_string)\n+\n                     json_input = json.loads(json_string)\n+            # \u6709\u6982\u7387key\u4e3acommand\u800c\u975equery\uff0c\u9700\u4fee\u6539\n+            if \"command\" in json_input:\n+                json_input[\"query\"] = json_input.pop(\"command\")\n+\n             return AgentAction(tool=s[0].strip(), tool_input=json_input, log=text)\n         elif s := re.findall(r\"\\nFinal\\sAnswer:\\s*(.+)\", text, flags=re.DOTALL):\n             s = s[-1]\ndiff --git a/libs/chatchat-server/chatchat/server/agent/tools_factory/__init__.py b/libs/chatchat-server/chatchat/server/agent/tools_factory/__init__.py\nindex 05153eac20..64e87ea4f0 100644\n--- a/libs/chatchat-server/chatchat/server/agent/tools_factory/__init__.py\n+++ b/libs/chatchat-server/chatchat/server/agent/tools_factory/__init__.py\n@@ -12,3 +12,4 @@\n from .amap_weather import amap_weather\n from .wikipedia_search import wikipedia_search\n from .text2promql import text2promql\n+from .url_reader import url_reader\ndiff --git a/libs/chatchat-server/chatchat/server/agent/tools_factory/url_reader.py b/libs/chatchat-server/chatchat/server/agent/tools_factory/url_reader.py\nnew file mode 100644\nindex 0000000000..b690271508\n--- /dev/null\n+++ b/libs/chatchat-server/chatchat/server/agent/tools_factory/url_reader.py\n@@ -0,0 +1,43 @@\n+\"\"\"\n+\u901a\u8fc7jina-ai/reader\u9879\u76ee\uff0c\u5c06url\u5185\u5bb9\u5904\u7406\u4e3allm\u6613\u4e8e\u7406\u89e3\u7684\u6587\u672c\u5f62\u5f0f\n+\"\"\"\n+import requests\n+\n+import re\n+\n+from chatchat.server.pydantic_v1 import Field\n+from chatchat.server.utils import get_tool_config\n+\n+from chatchat.server.agent.tools_factory.tools_registry import format_context\n+\n+from .tools_registry import BaseToolOutput, regist_tool\n+\n+\n+@regist_tool(title=\"URL\u5185\u5bb9\u9605\u8bfb\")\n+def url_reader(\n+        url: str = Field(\n+            description=\"The URL to be processed, so that its web content can be made more clear to read. Then provide a detailed description of the content in about 500 words. As structured as possible. ONLY THE LINK SHOULD BE PASSED IN.\"),\n+):\n+    \"\"\"Use this tool to get the clear content of a URL.\"\"\"\n+\n+    tool_config = get_tool_config(\"url_reader\")\n+    timeout = tool_config.get(\"timeout\")\n+\n+    # \u63d0\u53d6url\u6587\u672c\u4e2d\u7684\u7f51\u9875\u94fe\u63a5\u90e8\u5206\u3002url\u6587\u672c\u53ef\u80fd\u662f\u4e00\u53e5\u8bdd\n+    url_pattern = r'http[s]?://[a-zA-Z0-9./?&=_%#-]+'\n+    match = re.search(url_pattern, url)\n+    url = match.group(0) if match else None\n+\n+    if url is None:\n+        return BaseToolOutput({\"error\": \"No URL\"})\n+\n+    reader_url = \"https://r.jina.ai/{url}\".format(url=url)\n+\n+    response = requests.get(reader_url, timeout=timeout)\n+\n+    if response.status_code == 200:\n+        return BaseToolOutput(\n+            {\"result\": response.text, \"docs\": [{\"page_content\": response.text, \"metadata\": {'source': url, 'id': ''}}]},\n+            format=format_context)\n+    else:\n+        return BaseToolOutput({\"error\": \"Timeout\"})\ndiff --git a/libs/chatchat-server/chatchat/server/api_server/chat_routes.py b/libs/chatchat-server/chatchat/server/api_server/chat_routes.py\nindex 30dc79a2fd..4eb8fe76d4 100644\n--- a/libs/chatchat-server/chatchat/server/api_server/chat_routes.py\n+++ b/libs/chatchat-server/chatchat/server/api_server/chat_routes.py\n@@ -42,7 +42,6 @@\n chat_router.post(\"/file_chat\", summary=\"\u6587\u4ef6\u5bf9\u8bdd\")(file_chat)\n \n \n-\n @chat_router.post(\"/chat/completions\", summary=\"\u517c\u5bb9 openai \u7684\u7edf\u4e00 chat \u63a5\u53e3\")\n async def chat_completions(\n     request: Request,\n@@ -60,11 +59,11 @@ async def chat_completions(\n     \u4ee5\u540e\u8fd8\u8981\u8003\u8651\u5176\u5b83\u7684\u7ec4\u5408\uff08\u5982\u6587\u4ef6\u5bf9\u8bdd\uff09\n     \u8fd4\u56de\u4e0e openai \u517c\u5bb9\u7684 Dict\n     \"\"\"\n-    import rich\n-    rich.print(body)\n+    # import rich\n+    # rich.print(body)\n \n     # \u5f53\u8c03\u7528\u672c\u63a5\u53e3\u4e14 body \u4e2d\u6ca1\u6709\u4f20\u5165 \"max_tokens\" \u53c2\u6570\u65f6, \u9ed8\u8ba4\u4f7f\u7528\u914d\u7f6e\u4e2d\u5b9a\u4e49\u7684\u503c\n-    if body.max_tokens == None:\n+    if body.max_tokens in [None, 0]:\n         body.max_tokens = Settings.model_settings.MAX_TOKENS\n \n     client = get_OpenAIClient(model_name=body.model, is_async=True)\n@@ -106,15 +105,19 @@ async def chat_completions(\n                 }\n             ]\n         if tool_input := extra.get(\"tool_input\"):\n-            message_id = (\n-                add_message_to_db(\n-                    chat_type=\"tool_call\",\n-                    query=body.messages[-1][\"content\"],\n-                    conversation_id=conversation_id,\n+            try:\n+                message_id = (\n+                    add_message_to_db(\n+                        chat_type=\"tool_call\",\n+                        query=body.messages[-1][\"content\"],\n+                        conversation_id=conversation_id,\n+                    )\n+                    if conversation_id\n+                    else None\n                 )\n-                if conversation_id\n-                else None\n-            )\n+            except Exception as e:\n+                logger.warning(f\"failed to add message to db: {e}\")\n+                message_id = None\n \n             tool_result = await tool.ainvoke(tool_input)\n             prompt_template = PromptTemplate.from_template(\n@@ -153,15 +156,19 @@ def temp_gen():\n \n     # agent chat with tool calls\n     if body.tools:\n-        message_id = (\n-            add_message_to_db(\n-                chat_type=\"agent_chat\",\n-                query=body.messages[-1][\"content\"],\n-                conversation_id=conversation_id,\n+        try:\n+            message_id = (\n+                add_message_to_db(\n+                    chat_type=\"agent_chat\",\n+                    query=body.messages[-1][\"content\"],\n+                    conversation_id=conversation_id,\n+                )\n+                if conversation_id\n+                else None\n             )\n-            if conversation_id\n-            else None\n-        )\n+        except Exception as e:\n+            logger.warning(f\"failed to add message to db: {e}\")\n+            message_id = None\n \n         chat_model_config = {}  # TODO: \u524d\u7aef\u652f\u6301\u914d\u7f6e\u6a21\u578b\n         tool_names = [x[\"function\"][\"name\"] for x in body.tools]\n@@ -191,7 +198,7 @@ def temp_gen():\n                 else None\n             )\n         except Exception as e:\n-            logger.error(f\"failed to add message to db\")\n+            logger.warning(f\"failed to add message to db: {e}\")\n             message_id = None\n \n         extra_json = {\ndiff --git a/libs/chatchat-server/chatchat/server/api_server/kb_routes.py b/libs/chatchat-server/chatchat/server/api_server/kb_routes.py\nindex f720e76cd2..bf1d5ee553 100644\n--- a/libs/chatchat-server/chatchat/server/api_server/kb_routes.py\n+++ b/libs/chatchat-server/chatchat/server/api_server/kb_routes.py\n@@ -44,6 +44,9 @@ async def kb_chat_endpoint(\n     # import rich\n     # rich.print(body)\n \n+    if body.max_tokens in [None, 0]:\n+        body.max_tokens = Settings.model_settings.MAX_TOKENS\n+\n     extra = body.model_extra\n     ret = await kb_chat(\n         query=body.messages[-1][\"content\"],\ndiff --git a/libs/chatchat-server/chatchat/server/api_server/openai_routes.py b/libs/chatchat-server/chatchat/server/api_server/openai_routes.py\nindex 3f7b316933..a73e197b31 100644\n--- a/libs/chatchat-server/chatchat/server/api_server/openai_routes.py\n+++ b/libs/chatchat-server/chatchat/server/api_server/openai_routes.py\n@@ -2,7 +2,6 @@\n \n import asyncio\n import base64\n-import logging\n import os\n import shutil\n from contextlib import asynccontextmanager\n@@ -10,11 +9,10 @@\n from pathlib import Path\n from typing import AsyncGenerator, Dict, Iterable, Tuple\n \n-from fastapi import APIRouter, Request\n+from fastapi import APIRouter, Request, HTTPException\n from fastapi.responses import FileResponse\n from openai import AsyncClient\n-from openai.types.file_object import FileObject\n-from sse_starlette.sse import EventSourceResponse\n+from sse_starlette.sse import EventSourceResponse, ServerSentEvent\n \n from chatchat.settings import Settings\n from chatchat.server.utils import get_config_platforms, get_model_info, get_OpenAIClient\n@@ -60,7 +58,7 @@ async def get_model_client(model_name: str) -> AsyncGenerator[AsyncClient]:\n         await semaphore.acquire()\n         yield get_OpenAIClient(platform_name=selected_platform, is_async=True)\n     except Exception:\n-        logger.error(f\"failed when request to {key}\", exc_info=True)\n+        logger.exception(f\"failed when request to {key}\")\n     finally:\n         semaphore.release()\n \n@@ -73,34 +71,43 @@ async def openai_request(\n     \"\"\"\n \n     async def generator():\n-        for x in header:\n-            if isinstance(x, str):\n-                x = OpenAIChatOutput(content=x, object=\"chat.completion.chunk\")\n-            elif isinstance(x, dict):\n-                x = OpenAIChatOutput.model_validate(x)\n-            else:\n-                raise RuntimeError(f\"unsupported value: {header}\")\n-            for k, v in extra_json.items():\n-                setattr(x, k, v)\n-            yield x.model_dump_json()\n-\n-        async for chunk in await method(**params):\n-            for k, v in extra_json.items():\n-                setattr(chunk, k, v)\n-            yield chunk.model_dump_json()\n-\n-        for x in tail:\n-            if isinstance(x, str):\n-                x = OpenAIChatOutput(content=x, object=\"chat.completion.chunk\")\n-            elif isinstance(x, dict):\n-                x = OpenAIChatOutput.model_validate(x)\n-            else:\n-                raise RuntimeError(f\"unsupported value: {tail}\")\n-            for k, v in extra_json.items():\n-                setattr(x, k, v)\n-            yield x.model_dump_json()\n+        try:\n+            for x in header:\n+                if isinstance(x, str):\n+                    x = OpenAIChatOutput(content=x, object=\"chat.completion.chunk\")\n+                elif isinstance(x, dict):\n+                    x = OpenAIChatOutput.model_validate(x)\n+                else:\n+                    raise RuntimeError(f\"unsupported value: {header}\")\n+                for k, v in extra_json.items():\n+                    setattr(x, k, v)\n+                yield x.model_dump_json()\n+\n+            async for chunk in await method(**params):\n+                for k, v in extra_json.items():\n+                    setattr(chunk, k, v)\n+                yield chunk.model_dump_json()\n+\n+            for x in tail:\n+                if isinstance(x, str):\n+                    x = OpenAIChatOutput(content=x, object=\"chat.completion.chunk\")\n+                elif isinstance(x, dict):\n+                    x = OpenAIChatOutput.model_validate(x)\n+                else:\n+                    raise RuntimeError(f\"unsupported value: {tail}\")\n+                for k, v in extra_json.items():\n+                    setattr(x, k, v)\n+                yield x.model_dump_json()\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n+        except Exception as e:\n+            logger.error(f\"openai request error: {e}\")\n+            yield {\"data\": json.dumps({\"error\": str(e)})}\n \n     params = body.model_dump(exclude_unset=True)\n+    if params.get(\"max_tokens\") == 0:\n+        params[\"max_tokens\"] = Settings.model_settings.MAX_TOKENS\n \n     if hasattr(body, \"stream\") and body.stream:\n         return EventSourceResponse(generator())\n@@ -123,7 +130,7 @@ async def task(name: str, config: Dict):\n             models = await client.models.list()\n             return [{**x.model_dump(), \"platform_name\": name} for x in models.data]\n         except Exception:\n-            logger.error(f\"failed request to platform: {name}\", exc_info=True)\n+            logger.exception(f\"failed request to platform: {name}\")\n             return []\n \n     result = []\n@@ -139,10 +146,8 @@ async def task(name: str, config: Dict):\n \n @openai_router.post(\"/chat/completions\")\n async def create_chat_completions(\n-    request: Request,\n     body: OpenAIChatInput,\n ):\n-    logger.debug(body)\n     async with get_model_client(body.model) as client:\n         result = await openai_request(client.chat.completions.create, body)\n         return result\ndiff --git a/libs/chatchat-server/chatchat/server/api_server/server_routes.py b/libs/chatchat-server/chatchat/server/api_server/server_routes.py\nindex f77cfb273d..22f8df34dc 100644\n--- a/libs/chatchat-server/chatchat/server/api_server/server_routes.py\n+++ b/libs/chatchat-server/chatchat/server/api_server/server_routes.py\n@@ -2,15 +2,14 @@\n \n from fastapi import APIRouter, Body\n \n+from chatchat.server.types.server.response.base import BaseResponse\n from chatchat.settings import Settings\n from chatchat.server.utils import get_prompt_template, get_server_configs\n \n server_router = APIRouter(prefix=\"/server\", tags=[\"Server State\"])\n \n-\n available_template_types = list(Settings.prompt_settings.model_fields.keys())\n \n-\n # \u670d\u52a1\u5668\u76f8\u5173\u63a5\u53e3\n server_router.post(\n     \"/configs\",\n@@ -18,11 +17,14 @@\n )(get_server_configs)\n \n \n-@server_router.post(\"/get_prompt_template\", summary=\"\u83b7\u53d6\u670d\u52a1\u533a\u914d\u7f6e\u7684 prompt \u6a21\u677f\")\n+@server_router.post(\"/get_prompt_template\", summary=\"\u83b7\u53d6\u670d\u52a1\u533a\u914d\u7f6e\u7684 prompt \u6a21\u677f\", response_model=BaseResponse)\n def get_server_prompt_template(\n-    type: str = Body(\n-        \"llm_model\", description=\"\u6a21\u677f\u7c7b\u578b\uff0c\u53ef\u9009\u503c\uff1a{available_template_types}\"\n-    ),\n-    name: str = Body(\"default\", description=\"\u6a21\u677f\u540d\u79f0\"),\n-) -> str:\n-    return get_prompt_template(type=type, name=name)\n+        type: str = Body(\n+            \"llm_model\", description=\"\u6a21\u677f\u7c7b\u578b\uff0c\u53ef\u9009\u503c\uff1a{available_template_types}\"\n+        ),\n+        name: str = Body(\"default\", description=\"\u6a21\u677f\u540d\u79f0\"),\n+):\n+    prompt_template = get_prompt_template(type=type, name=name)\n+    if prompt_template is None:\n+        return BaseResponse.error(\"Prompt template not found\")\n+    return BaseResponse.success(prompt_template)\ndiff --git a/libs/chatchat-server/chatchat/server/api_server/tool_routes.py b/libs/chatchat-server/chatchat/server/api_server/tool_routes.py\nindex 933b08a93a..92f7458457 100644\n--- a/libs/chatchat-server/chatchat/server/api_server/tool_routes.py\n+++ b/libs/chatchat-server/chatchat/server/api_server/tool_routes.py\n@@ -41,7 +41,7 @@ async def call_tool(\n             return {\"data\": result}\n         except Exception:\n             msg = f\"failed to call tool '{name}'\"\n-            logger.error(msg, exc_info=True)\n+            logger.exception(msg)\n             return {\"code\": 500, \"msg\": msg}\n     else:\n         return {\"code\": 500, \"msg\": f\"no tool named '{name}'\"}\ndiff --git a/libs/chatchat-server/chatchat/server/chat/chat.py b/libs/chatchat-server/chatchat/server/chat/chat.py\nindex b43710427b..f1df170dee 100644\n--- a/libs/chatchat-server/chatchat/server/chat/chat.py\n+++ b/libs/chatchat-server/chatchat/server/chat/chat.py\n@@ -1,15 +1,12 @@\n import asyncio\n import json\n-import time\n import uuid\n from typing import AsyncIterable, List\n \n from fastapi import Body\n from langchain.chains import LLMChain\n-from langchain.prompts import PromptTemplate\n from langchain.prompts.chat import ChatPromptTemplate\n from langchain_core.messages import AIMessage, HumanMessage, convert_to_messages\n-from langchain_core.output_parsers import StrOutputParser\n from sse_starlette.sse import EventSourceResponse\n \n from chatchat.settings import Settings\n@@ -30,9 +27,13 @@\n     get_tool,\n     wrap_done,\n     get_default_llm,\n+    build_logger,\n )\n \n \n+logger = build_logger()\n+\n+\n def create_models_from_config(configs, callbacks, stream, max_tokens):\n     configs = configs or Settings.model_settings.LLM_MODEL_CONFIG\n     models = {}\n@@ -122,119 +123,127 @@ async def chat(\n     \"\"\"Agent \u5bf9\u8bdd\"\"\"\n \n     async def chat_iterator() -> AsyncIterable[OpenAIChatOutput]:\n-        callback = AgentExecutorAsyncIteratorCallbackHandler()\n-        callbacks = [callback]\n+        try:\n+            callback = AgentExecutorAsyncIteratorCallbackHandler()\n+            callbacks = [callback]\n \n-        # Enable langchain-chatchat to support langfuse\n-        import os\n+            # Enable langchain-chatchat to support langfuse\n+            import os\n \n-        langfuse_secret_key = os.environ.get(\"LANGFUSE_SECRET_KEY\")\n-        langfuse_public_key = os.environ.get(\"LANGFUSE_PUBLIC_KEY\")\n-        langfuse_host = os.environ.get(\"LANGFUSE_HOST\")\n-        if langfuse_secret_key and langfuse_public_key and langfuse_host:\n-            from langfuse import Langfuse\n-            from langfuse.callback import CallbackHandler\n+            langfuse_secret_key = os.environ.get(\"LANGFUSE_SECRET_KEY\")\n+            langfuse_public_key = os.environ.get(\"LANGFUSE_PUBLIC_KEY\")\n+            langfuse_host = os.environ.get(\"LANGFUSE_HOST\")\n+            if langfuse_secret_key and langfuse_public_key and langfuse_host:\n+                from langfuse import Langfuse\n+                from langfuse.callback import CallbackHandler\n \n-            langfuse_handler = CallbackHandler()\n-            callbacks.append(langfuse_handler)\n+                langfuse_handler = CallbackHandler()\n+                callbacks.append(langfuse_handler)\n \n-        models, prompts = create_models_from_config(\n-            callbacks=callbacks, configs=chat_model_config, stream=stream, max_tokens=max_tokens\n-        )\n-        all_tools = get_tool().values()\n-        tools = [tool for tool in all_tools if tool.name in tool_config]\n-        tools = [t.copy(update={\"callbacks\": callbacks}) for t in tools]\n-        full_chain = create_models_chains(\n-            prompts=prompts,\n-            models=models,\n-            conversation_id=conversation_id,\n-            tools=tools,\n-            callbacks=callbacks,\n-            history=history,\n-            history_len=history_len,\n-            metadata=metadata,\n-        )\n+            models, prompts = create_models_from_config(\n+                callbacks=callbacks, configs=chat_model_config, stream=stream, max_tokens=max_tokens\n+            )\n+            all_tools = get_tool().values()\n+            tools = [tool for tool in all_tools if tool.name in tool_config]\n+            tools = [t.copy(update={\"callbacks\": callbacks}) for t in tools]\n+            full_chain = create_models_chains(\n+                prompts=prompts,\n+                models=models,\n+                conversation_id=conversation_id,\n+                tools=tools,\n+                callbacks=callbacks,\n+                history=history,\n+                history_len=history_len,\n+                metadata=metadata,\n+            )\n \n-        _history = [History.from_data(h) for h in history]\n-        chat_history = [h.to_msg_tuple() for h in _history]\n+            _history = [History.from_data(h) for h in history]\n+            chat_history = [h.to_msg_tuple() for h in _history]\n \n-        history_message = convert_to_messages(chat_history)\n+            history_message = convert_to_messages(chat_history)\n \n-        task = asyncio.create_task(\n-            wrap_done(\n-                full_chain.ainvoke(\n-                    {\n-                        \"input\": query,\n-                        \"chat_history\": history_message,\n-                    }\n-                ),\n-                callback.done,\n+            task = asyncio.create_task(\n+                wrap_done(\n+                    full_chain.ainvoke(\n+                        {\n+                            \"input\": query,\n+                            \"chat_history\": history_message,\n+                        }\n+                    ),\n+                    callback.done,\n+                )\n             )\n-        )\n \n-        last_tool = {}\n-        async for chunk in callback.aiter():\n-            data = json.loads(chunk)\n-            data[\"tool_calls\"] = []\n-            data[\"message_type\"] = MsgType.TEXT\n+            last_tool = {}\n+            async for chunk in callback.aiter():\n+                data = json.loads(chunk)\n+                data[\"tool_calls\"] = []\n+                data[\"message_type\"] = MsgType.TEXT\n \n-            if data[\"status\"] == AgentStatus.tool_start:\n-                last_tool = {\n-                    \"index\": 0,\n-                    \"id\": data[\"run_id\"],\n-                    \"type\": \"function\",\n-                    \"function\": {\n-                        \"name\": data[\"tool\"],\n-                        \"arguments\": data[\"tool_input\"],\n-                    },\n-                    \"tool_output\": None,\n-                    \"is_error\": False,\n-                }\n-                data[\"tool_calls\"].append(last_tool)\n-            if data[\"status\"] in [AgentStatus.tool_end]:\n-                last_tool.update(\n-                    tool_output=data[\"tool_output\"],\n-                    is_error=data.get(\"is_error\", False),\n-                )\n-                data[\"tool_calls\"] = [last_tool]\n-                last_tool = {}\n-                try:\n-                    tool_output = json.loads(data[\"tool_output\"])\n-                    if message_type := tool_output.get(\"message_type\"):\n-                        data[\"message_type\"] = message_type\n-                except:\n-                    ...\n-            elif data[\"status\"] == AgentStatus.agent_finish:\n-                try:\n-                    tool_output = json.loads(data[\"text\"])\n-                    if message_type := tool_output.get(\"message_type\"):\n-                        data[\"message_type\"] = message_type\n-                except:\n-                    ...\n+                if data[\"status\"] == AgentStatus.tool_start:\n+                    last_tool = {\n+                        \"index\": 0,\n+                        \"id\": data[\"run_id\"],\n+                        \"type\": \"function\",\n+                        \"function\": {\n+                            \"name\": data[\"tool\"],\n+                            \"arguments\": data[\"tool_input\"],\n+                        },\n+                        \"tool_output\": None,\n+                        \"is_error\": False,\n+                    }\n+                    data[\"tool_calls\"].append(last_tool)\n+                if data[\"status\"] in [AgentStatus.tool_end]:\n+                    last_tool.update(\n+                        tool_output=data[\"tool_output\"],\n+                        is_error=data.get(\"is_error\", False),\n+                    )\n+                    data[\"tool_calls\"] = [last_tool]\n+                    last_tool = {}\n+                    try:\n+                        tool_output = json.loads(data[\"tool_output\"])\n+                        if message_type := tool_output.get(\"message_type\"):\n+                            data[\"message_type\"] = message_type\n+                    except:\n+                        ...\n+                elif data[\"status\"] == AgentStatus.agent_finish:\n+                    try:\n+                        tool_output = json.loads(data[\"text\"])\n+                        if message_type := tool_output.get(\"message_type\"):\n+                            data[\"message_type\"] = message_type\n+                    except:\n+                        ...\n \n-            ret = OpenAIChatOutput(\n-                id=f\"chat{uuid.uuid4()}\",\n-                object=\"chat.completion.chunk\",\n-                content=data.get(\"text\", \"\"),\n-                role=\"assistant\",\n-                tool_calls=data[\"tool_calls\"],\n-                model=models[\"llm_model\"].model_name,\n-                status=data[\"status\"],\n-                message_type=data[\"message_type\"],\n-                message_id=message_id,\n-            )\n-            yield ret.model_dump_json()\n-        # yield OpenAIChatOutput( # return blank text lastly\n-        #         id=f\"chat{uuid.uuid4()}\",\n-        #         object=\"chat.completion.chunk\",\n-        #         content=\"\",\n-        #         role=\"assistant\",\n-        #         model=models[\"llm_model\"].model_name,\n-        #         status = data[\"status\"],\n-        #         message_type = data[\"message_type\"],\n-        #         message_id=message_id,\n-        # )\n-        await task\n+                ret = OpenAIChatOutput(\n+                    id=f\"chat{uuid.uuid4()}\",\n+                    object=\"chat.completion.chunk\",\n+                    content=data.get(\"text\", \"\"),\n+                    role=\"assistant\",\n+                    tool_calls=data[\"tool_calls\"],\n+                    model=models[\"llm_model\"].model_name,\n+                    status=data[\"status\"],\n+                    message_type=data[\"message_type\"],\n+                    message_id=message_id,\n+                )\n+                yield ret.model_dump_json()\n+            # yield OpenAIChatOutput( # return blank text lastly\n+            #         id=f\"chat{uuid.uuid4()}\",\n+            #         object=\"chat.completion.chunk\",\n+            #         content=\"\",\n+            #         role=\"assistant\",\n+            #         model=models[\"llm_model\"].model_name,\n+            #         status = data[\"status\"],\n+            #         message_type = data[\"message_type\"],\n+            #         message_id=message_id,\n+            # )\n+            await task\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n+        except Exception as e:\n+            logger.error(f\"error in chat: {e}\")\n+            yield {\"data\": json.dumps({\"error\": str(e)})}\n+            return\n \n     if stream:\n         return EventSourceResponse(chat_iterator())\ndiff --git a/libs/chatchat-server/chatchat/server/chat/completion.py b/libs/chatchat-server/chatchat/server/chat/completion.py\nindex ae4023b116..7342c800d2 100644\n--- a/libs/chatchat-server/chatchat/server/chat/completion.py\n+++ b/libs/chatchat-server/chatchat/server/chat/completion.py\n@@ -7,7 +7,10 @@\n from langchain.prompts import PromptTemplate\n from sse_starlette.sse import EventSourceResponse\n \n-from chatchat.server.utils import get_OpenAI, get_prompt_template, wrap_done\n+from chatchat.server.utils import get_OpenAI, get_prompt_template, wrap_done, build_logger\n+\n+\n+logger = build_logger()\n \n \n async def completion(\n@@ -31,40 +34,44 @@ async def completion_iterator(\n         prompt_name: str = prompt_name,\n         echo: bool = echo,\n     ) -> AsyncIterable[str]:\n-        nonlocal max_tokens\n-        callback = AsyncIteratorCallbackHandler()\n-        if isinstance(max_tokens, int) and max_tokens <= 0:\n-            max_tokens = None\n+        try:\n+            nonlocal max_tokens\n+            callback = AsyncIteratorCallbackHandler()\n+            if isinstance(max_tokens, int) and max_tokens <= 0:\n+                max_tokens = None\n \n-        model = get_OpenAI(\n-            model_name=model_name,\n-            temperature=temperature,\n-            max_tokens=max_tokens,\n-            callbacks=[callback],\n-            echo=echo,\n-            local_wrap=True,\n-        )\n+            model = get_OpenAI(\n+                model_name=model_name,\n+                temperature=temperature,\n+                max_tokens=max_tokens,\n+                callbacks=[callback],\n+                echo=echo,\n+                local_wrap=True,\n+            )\n \n-        prompt_template = get_prompt_template(\"llm_model\", prompt_name)\n-        prompt = PromptTemplate.from_template(prompt_template, template_format=\"jinja2\")\n-        chain = LLMChain(prompt=prompt, llm=model)\n+            prompt_template = get_prompt_template(\"llm_model\", prompt_name)\n+            prompt = PromptTemplate.from_template(prompt_template, template_format=\"jinja2\")\n+            chain = LLMChain(prompt=prompt, llm=model)\n \n-        # Begin a task that runs in the background.\n-        task = asyncio.create_task(\n-            wrap_done(chain.acall({\"input\": query}), callback.done),\n-        )\n+            # Begin a task that runs in the background.\n+            task = asyncio.create_task(\n+                wrap_done(chain.acall({\"input\": query}), callback.done),\n+            )\n \n-        if stream:\n-            async for token in callback.aiter():\n-                # Use server-sent-events to stream the response\n-                yield token\n-        else:\n-            answer = \"\"\n-            async for token in callback.aiter():\n-                answer += token\n-            yield answer\n+            if stream:\n+                async for token in callback.aiter():\n+                    # Use server-sent-events to stream the response\n+                    yield token\n+            else:\n+                answer = \"\"\n+                async for token in callback.aiter():\n+                    answer += token\n+                yield answer\n \n-        await task\n+            await task\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n \n     return EventSourceResponse(\n         completion_iterator(\ndiff --git a/libs/chatchat-server/chatchat/server/chat/file_chat.py b/libs/chatchat-server/chatchat/server/chat/file_chat.py\nindex 73eae7fee7..c164b999e6 100644\n--- a/libs/chatchat-server/chatchat/server/chat/file_chat.py\n+++ b/libs/chatchat-server/chatchat/server/chat/file_chat.py\n@@ -152,81 +152,85 @@ async def file_chat(\n     history = [History.from_data(h) for h in history]\n \n     async def knowledge_base_chat_iterator() -> AsyncIterable[str]:\n-        nonlocal max_tokens\n-        callback = AsyncIteratorCallbackHandler()\n-        if isinstance(max_tokens, int) and max_tokens <= 0:\n-            max_tokens = None\n-\n-        callbacks = [callback]\n-        # Enable langchain-chatchat to support langfuse\n-        import os\n-\n-        langfuse_secret_key = os.environ.get(\"LANGFUSE_SECRET_KEY\")\n-        langfuse_public_key = os.environ.get(\"LANGFUSE_PUBLIC_KEY\")\n-        langfuse_host = os.environ.get(\"LANGFUSE_HOST\")\n-        if langfuse_secret_key and langfuse_public_key and langfuse_host:\n-            from langfuse import Langfuse\n-            from langfuse.callback import CallbackHandler\n-\n-            langfuse_handler = CallbackHandler()\n-            callbacks.append(langfuse_handler)\n-\n-        model = get_ChatOpenAI(\n-            model_name=model_name,\n-            temperature=temperature,\n-            max_tokens=max_tokens,\n-            callbacks=callbacks,\n-        )\n-        embed_func = get_Embeddings()\n-        embeddings = await embed_func.aembed_query(query)\n-        with memo_faiss_pool.acquire(knowledge_id) as vs:\n-            docs = vs.similarity_search_with_score_by_vector(\n-                embeddings, k=top_k, score_threshold=score_threshold\n+        try:\n+            nonlocal max_tokens\n+            callback = AsyncIteratorCallbackHandler()\n+            if isinstance(max_tokens, int) and max_tokens <= 0:\n+                max_tokens = None\n+\n+            callbacks = [callback]\n+            # Enable langchain-chatchat to support langfuse\n+            import os\n+\n+            langfuse_secret_key = os.environ.get(\"LANGFUSE_SECRET_KEY\")\n+            langfuse_public_key = os.environ.get(\"LANGFUSE_PUBLIC_KEY\")\n+            langfuse_host = os.environ.get(\"LANGFUSE_HOST\")\n+            if langfuse_secret_key and langfuse_public_key and langfuse_host:\n+                from langfuse import Langfuse\n+                from langfuse.callback import CallbackHandler\n+\n+                langfuse_handler = CallbackHandler()\n+                callbacks.append(langfuse_handler)\n+\n+            model = get_ChatOpenAI(\n+                model_name=model_name,\n+                temperature=temperature,\n+                max_tokens=max_tokens,\n+                callbacks=callbacks,\n+            )\n+            embed_func = get_Embeddings()\n+            embeddings = await embed_func.aembed_query(query)\n+            with memo_faiss_pool.acquire(knowledge_id) as vs:\n+                docs = vs.similarity_search_with_score_by_vector(\n+                    embeddings, k=top_k, score_threshold=score_threshold\n+                )\n+                docs = [x[0] for x in docs]\n+\n+            context = \"\\n\".join([doc.page_content for doc in docs])\n+            if len(docs) == 0:  # \u5982\u679c\u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\uff0c\u4f7f\u7528Empty\u6a21\u677f\n+                prompt_template = get_prompt_template(\"rag\", \"empty\")\n+            else:\n+                prompt_template = get_prompt_template(\"rag\", \"default\")\n+            input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n+            chat_prompt = ChatPromptTemplate.from_messages(\n+                [i.to_msg_template() for i in history] + [input_msg]\n             )\n-            docs = [x[0] for x in docs]\n-\n-        context = \"\\n\".join([doc.page_content for doc in docs])\n-        if len(docs) == 0:  # \u5982\u679c\u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\uff0c\u4f7f\u7528Empty\u6a21\u677f\n-            prompt_template = get_prompt_template(\"rag\", \"empty\")\n-        else:\n-            prompt_template = get_prompt_template(\"rag\", \"default\")\n-        input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n-        chat_prompt = ChatPromptTemplate.from_messages(\n-            [i.to_msg_template() for i in history] + [input_msg]\n-        )\n-\n-        chain = LLMChain(prompt=chat_prompt, llm=model)\n-\n-        # Begin a task that runs in the background.\n-        task = asyncio.create_task(\n-            wrap_done(\n-                chain.acall({\"context\": context, \"question\": query}), callback.done\n-            ),\n-        )\n \n-        source_documents = []\n-        for inum, doc in enumerate(docs):\n-            filename = doc.metadata.get(\"source\")\n-            text = f\"\"\"\u51fa\u5904 [{inum + 1}] [{filename}] \\n\\n{doc.page_content}\\n\\n\"\"\"\n-            source_documents.append(text)\n+            chain = LLMChain(prompt=chat_prompt, llm=model)\n \n-        if len(source_documents) == 0:  # \u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\n-            source_documents.append(\n-                f\"\"\"<span style='color:red'>\u672a\u627e\u5230\u76f8\u5173\u6587\u6863,\u8be5\u56de\u7b54\u4e3a\u5927\u6a21\u578b\u81ea\u8eab\u80fd\u529b\u89e3\u7b54\uff01</span>\"\"\"\n+            # Begin a task that runs in the background.\n+            task = asyncio.create_task(\n+                wrap_done(\n+                    chain.acall({\"context\": context, \"question\": query}), callback.done\n+                ),\n             )\n \n-        if stream:\n-            async for token in callback.aiter():\n-                # Use server-sent-events to stream the response\n-                yield json.dumps({\"answer\": token}, ensure_ascii=False)\n-            yield json.dumps({\"docs\": source_documents}, ensure_ascii=False)\n-        else:\n-            answer = \"\"\n-            async for token in callback.aiter():\n-                answer += token\n-            yield json.dumps(\n-                {\"answer\": answer, \"docs\": source_documents}, ensure_ascii=False\n-            )\n-        await task\n+            source_documents = []\n+            for inum, doc in enumerate(docs):\n+                filename = doc.metadata.get(\"source\")\n+                text = f\"\"\"\u51fa\u5904 [{inum + 1}] [{filename}] \\n\\n{doc.page_content}\\n\\n\"\"\"\n+                source_documents.append(text)\n+\n+            if len(source_documents) == 0:  # \u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\n+                source_documents.append(\n+                    f\"\"\"<span style='color:red'>\u672a\u627e\u5230\u76f8\u5173\u6587\u6863,\u8be5\u56de\u7b54\u4e3a\u5927\u6a21\u578b\u81ea\u8eab\u80fd\u529b\u89e3\u7b54\uff01</span>\"\"\"\n+                )\n+\n+            if stream:\n+                async for token in callback.aiter():\n+                    # Use server-sent-events to stream the response\n+                    yield json.dumps({\"answer\": token}, ensure_ascii=False)\n+                yield json.dumps({\"docs\": source_documents}, ensure_ascii=False)\n+            else:\n+                answer = \"\"\n+                async for token in callback.aiter():\n+                    answer += token\n+                yield json.dumps(\n+                    {\"answer\": answer, \"docs\": source_documents}, ensure_ascii=False\n+                )\n+            await task\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n \n     return EventSourceResponse(knowledge_base_chat_iterator())\ndiff --git a/libs/chatchat-server/chatchat/server/chat/kb_chat.py b/libs/chatchat-server/chatchat/server/chat/kb_chat.py\nindex dd90811e0f..3aa1ce4e54 100644\n--- a/libs/chatchat-server/chatchat/server/chat/kb_chat.py\n+++ b/libs/chatchat-server/chatchat/server/chat/kb_chat.py\n@@ -1,7 +1,6 @@\n from __future__ import annotations\n \n import asyncio, json\n-from urllib.parse import urlencode\n import uuid\n from typing import AsyncIterable, List, Optional, Literal\n \n@@ -20,10 +19,14 @@\n from chatchat.server.knowledge_base.kb_doc_api import search_docs, search_temp_docs\n from chatchat.server.knowledge_base.utils import format_reference\n from chatchat.server.utils import (wrap_done, get_ChatOpenAI, get_default_llm,\n-                                   BaseResponse, get_prompt_template,\n+                                   BaseResponse, get_prompt_template, build_logger,\n+                                   check_embed_model, api_address\n                                 )\n \n \n+logger = build_logger()\n+\n+\n async def kb_chat(query: str = Body(..., description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\u4f60\u597d\"]),\n                 mode: Literal[\"local_kb\", \"temp_kb\", \"search_engine\"] = Body(\"local_kb\", description=\"\u77e5\u8bc6\u6765\u6e90\"),\n                 kb_name: str = Body(\"\", description=\"mode=local_kb\u65f6\u4e3a\u77e5\u8bc6\u5e93\u540d\u79f0\uff1btemp_kb\u65f6\u4e3a\u4e34\u65f6\u77e5\u8bc6\u5e93ID\uff0csearch_engine\u65f6\u4e3a\u641c\u7d22\u5f15\u64ce\u540d\u79f0\", examples=[\"samples\"]),\n@@ -63,143 +66,161 @@ async def kb_chat(query: str = Body(..., description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\n             return BaseResponse(code=404, msg=f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 {kb_name}\")\n     \n     async def knowledge_base_chat_iterator() -> AsyncIterable[str]:\n-        nonlocal history, prompt_name\n-\n-        history = [History.from_data(h) for h in history]\n-\n-        if mode == \"local_kb\":\n-            docs = await run_in_threadpool(search_docs,\n-                                            query=query,\n-                                            knowledge_base_name=kb_name,\n-                                            top_k=top_k,\n-                                            score_threshold=score_threshold,\n-                                            file_name=\"\",\n-                                            metadata={})\n-            source_documents = format_reference(kb_name, docs, request.base_url)\n-        elif mode == \"temp_kb\":\n-            docs = await run_in_threadpool(search_temp_docs,\n-                                            kb_name,\n-                                            query=query,\n-                                            top_k=top_k,\n-                                            score_threshold=score_threshold)\n-            source_documents = format_reference(kb_name, docs, request.base_url)\n-        elif mode == \"search_engine\":\n-            result = await run_in_threadpool(search_engine, query, top_k, kb_name)\n-            docs = [x.dict() for x in result.get(\"docs\", [])]\n-            source_documents = [f\"\"\"\u51fa\u5904 [{i + 1}] [{d['metadata']['filename']}]({d['metadata']['source']}) \\n\\n{d['page_content']}\\n\\n\"\"\" for i,d in enumerate(docs)]\n-        else:\n-            docs = []\n-            source_documents = []\n-        # import rich\n-        # rich.print(dict(\n-        #     mode=mode,\n-        #     query=query,\n-        #     knowledge_base_name=kb_name,\n-        #     top_k=top_k,\n-        #     score_threshold=score_threshold,\n-        # ))\n-        # rich.print(docs)\n-        if return_direct:\n-            yield OpenAIChatOutput(\n-                id=f\"chat{uuid.uuid4()}\",\n-                model=None,\n-                object=\"chat.completion\",\n-                content=\"\",\n-                role=\"assistant\",\n-                finish_reason=\"stop\",\n-                docs=source_documents,\n-            ) .model_dump_json()\n-            return\n-\n-        callback = AsyncIteratorCallbackHandler()\n-        callbacks = [callback]\n-\n-        # Enable langchain-chatchat to support langfuse\n-        import os\n-        langfuse_secret_key = os.environ.get('LANGFUSE_SECRET_KEY')\n-        langfuse_public_key = os.environ.get('LANGFUSE_PUBLIC_KEY')\n-        langfuse_host = os.environ.get('LANGFUSE_HOST')\n-        if langfuse_secret_key and langfuse_public_key and langfuse_host :\n-            from langfuse import Langfuse\n-            from langfuse.callback import CallbackHandler\n-            langfuse_handler = CallbackHandler()\n-            callbacks.append(langfuse_handler)\n-\n-        llm = get_ChatOpenAI(\n-            model_name=model,\n-            temperature=temperature,\n-            max_tokens=max_tokens,\n-            callbacks=callbacks,\n-        )\n-        # TODO\uff1a \u89c6\u60c5\u51b5\u4f7f\u7528 API\n-        # # \u52a0\u5165reranker\n-        # if Settings.kb_settings.USE_RERANKER:\n-        #     reranker_model_path = get_model_path(Settings.kb_settings.RERANKER_MODEL)\n-        #     reranker_model = LangchainReranker(top_n=top_k,\n-        #                                     device=embedding_device(),\n-        #                                     max_length=Settings.kb_settings.RERANKER_MAX_LENGTH,\n-        #                                     model_name_or_path=reranker_model_path\n-        #                                     )\n-        #     print(\"-------------before rerank-----------------\")\n-        #     print(docs)\n-        #     docs = reranker_model.compress_documents(documents=docs,\n-        #                                              query=query)\n-        #     print(\"------------after rerank------------------\")\n-        #     print(docs)\n-        context = \"\\n\\n\".join([doc[\"page_content\"] for doc in docs])\n-\n-        if len(docs) == 0:  # \u5982\u679c\u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\uff0c\u4f7f\u7528empty\u6a21\u677f\n-            prompt_name = \"empty\"\n-        prompt_template = get_prompt_template(\"rag\", prompt_name)\n-        input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n-        chat_prompt = ChatPromptTemplate.from_messages(\n-            [i.to_msg_template() for i in history] + [input_msg])\n-\n-        chain = chat_prompt | llm\n-\n-        # Begin a task that runs in the background.\n-        task = asyncio.create_task(wrap_done(\n-            chain.ainvoke({\"context\": context, \"question\": query}),\n-            callback.done),\n-        )\n-\n-        if len(source_documents) == 0:  # \u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\n-            source_documents.append(f\"<span style='color:red'>\u672a\u627e\u5230\u76f8\u5173\u6587\u6863,\u8be5\u56de\u7b54\u4e3a\u5927\u6a21\u578b\u81ea\u8eab\u80fd\u529b\u89e3\u7b54\uff01</span>\")\n-\n-        if stream:\n-            # yield documents first\n-            ret = OpenAIChatOutput(\n-                id=f\"chat{uuid.uuid4()}\",\n-                object=\"chat.completion.chunk\",\n-                content=\"\",\n-                role=\"assistant\",\n-                model=model,\n-                docs=source_documents,\n+        try:\n+            nonlocal history, prompt_name, max_tokens\n+\n+            history = [History.from_data(h) for h in history]\n+\n+            if mode == \"local_kb\":\n+                kb = KBServiceFactory.get_service_by_name(kb_name)\n+                ok, msg = kb.check_embed_model()\n+                if not ok:\n+                    raise ValueError(msg)\n+                docs = await run_in_threadpool(search_docs,\n+                                                query=query,\n+                                                knowledge_base_name=kb_name,\n+                                                top_k=top_k,\n+                                                score_threshold=score_threshold,\n+                                                file_name=\"\",\n+                                                metadata={})\n+                source_documents = format_reference(kb_name, docs, api_address(is_public=True))\n+            elif mode == \"temp_kb\":\n+                ok, msg = check_embed_model()\n+                if not ok:\n+                    raise ValueError(msg)\n+                docs = await run_in_threadpool(search_temp_docs,\n+                                                kb_name,\n+                                                query=query,\n+                                                top_k=top_k,\n+                                                score_threshold=score_threshold)\n+                source_documents = format_reference(kb_name, docs, api_address(is_public=True))\n+            elif mode == \"search_engine\":\n+                result = await run_in_threadpool(search_engine, query, top_k, kb_name)\n+                docs = [x.dict() for x in result.get(\"docs\", [])]\n+                source_documents = [f\"\"\"\u51fa\u5904 [{i + 1}] [{d['metadata']['filename']}]({d['metadata']['source']}) \\n\\n{d['page_content']}\\n\\n\"\"\" for i,d in enumerate(docs)]\n+            else:\n+                docs = []\n+                source_documents = []\n+            # import rich\n+            # rich.print(dict(\n+            #     mode=mode,\n+            #     query=query,\n+            #     knowledge_base_name=kb_name,\n+            #     top_k=top_k,\n+            #     score_threshold=score_threshold,\n+            # ))\n+            # rich.print(docs)\n+            if return_direct:\n+                yield OpenAIChatOutput(\n+                    id=f\"chat{uuid.uuid4()}\",\n+                    model=None,\n+                    object=\"chat.completion\",\n+                    content=\"\",\n+                    role=\"assistant\",\n+                    finish_reason=\"stop\",\n+                    docs=source_documents,\n+                ) .model_dump_json()\n+                return\n+\n+            callback = AsyncIteratorCallbackHandler()\n+            callbacks = [callback]\n+\n+            # Enable langchain-chatchat to support langfuse\n+            import os\n+            langfuse_secret_key = os.environ.get('LANGFUSE_SECRET_KEY')\n+            langfuse_public_key = os.environ.get('LANGFUSE_PUBLIC_KEY')\n+            langfuse_host = os.environ.get('LANGFUSE_HOST')\n+            if langfuse_secret_key and langfuse_public_key and langfuse_host :\n+                from langfuse import Langfuse\n+                from langfuse.callback import CallbackHandler\n+                langfuse_handler = CallbackHandler()\n+                callbacks.append(langfuse_handler)\n+\n+            if max_tokens in [None, 0]:\n+                max_tokens = Settings.model_settings.MAX_TOKENS\n+\n+            llm = get_ChatOpenAI(\n+                model_name=model,\n+                temperature=temperature,\n+                max_tokens=max_tokens,\n+                callbacks=callbacks,\n+            )\n+            # TODO\uff1a \u89c6\u60c5\u51b5\u4f7f\u7528 API\n+            # # \u52a0\u5165reranker\n+            # if Settings.kb_settings.USE_RERANKER:\n+            #     reranker_model_path = get_model_path(Settings.kb_settings.RERANKER_MODEL)\n+            #     reranker_model = LangchainReranker(top_n=top_k,\n+            #                                     device=embedding_device(),\n+            #                                     max_length=Settings.kb_settings.RERANKER_MAX_LENGTH,\n+            #                                     model_name_or_path=reranker_model_path\n+            #                                     )\n+            #     print(\"-------------before rerank-----------------\")\n+            #     print(docs)\n+            #     docs = reranker_model.compress_documents(documents=docs,\n+            #                                              query=query)\n+            #     print(\"------------after rerank------------------\")\n+            #     print(docs)\n+            context = \"\\n\\n\".join([doc[\"page_content\"] for doc in docs])\n+\n+            if len(docs) == 0:  # \u5982\u679c\u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\uff0c\u4f7f\u7528empty\u6a21\u677f\n+                prompt_name = \"empty\"\n+            prompt_template = get_prompt_template(\"rag\", prompt_name)\n+            input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n+            chat_prompt = ChatPromptTemplate.from_messages(\n+                [i.to_msg_template() for i in history] + [input_msg])\n+\n+            chain = chat_prompt | llm\n+\n+            # Begin a task that runs in the background.\n+            task = asyncio.create_task(wrap_done(\n+                chain.ainvoke({\"context\": context, \"question\": query}),\n+                callback.done),\n             )\n-            yield ret.model_dump_json()\n \n-            async for token in callback.aiter():\n+            if len(source_documents) == 0:  # \u6ca1\u6709\u627e\u5230\u76f8\u5173\u6587\u6863\n+                source_documents.append(f\"<span style='color:red'>\u672a\u627e\u5230\u76f8\u5173\u6587\u6863,\u8be5\u56de\u7b54\u4e3a\u5927\u6a21\u578b\u81ea\u8eab\u80fd\u529b\u89e3\u7b54\uff01</span>\")\n+\n+            if stream:\n+                # yield documents first\n                 ret = OpenAIChatOutput(\n                     id=f\"chat{uuid.uuid4()}\",\n                     object=\"chat.completion.chunk\",\n-                    content=token,\n+                    content=\"\",\n                     role=\"assistant\",\n                     model=model,\n+                    docs=source_documents,\n                 )\n                 yield ret.model_dump_json()\n-        else:\n-            answer = \"\"\n-            async for token in callback.aiter():\n-                answer += token\n-            ret = OpenAIChatOutput(\n-                id=f\"chat{uuid.uuid4()}\",\n-                object=\"chat.completion\",\n-                content=answer,\n-                role=\"assistant\",\n-                model=model,\n-            )\n-            yield ret.model_dump_json()\n-        await task\n+\n+                async for token in callback.aiter():\n+                    ret = OpenAIChatOutput(\n+                        id=f\"chat{uuid.uuid4()}\",\n+                        object=\"chat.completion.chunk\",\n+                        content=token,\n+                        role=\"assistant\",\n+                        model=model,\n+                    )\n+                    yield ret.model_dump_json()\n+            else:\n+                answer = \"\"\n+                async for token in callback.aiter():\n+                    answer += token\n+                ret = OpenAIChatOutput(\n+                    id=f\"chat{uuid.uuid4()}\",\n+                    object=\"chat.completion\",\n+                    content=answer,\n+                    role=\"assistant\",\n+                    model=model,\n+                )\n+                yield ret.model_dump_json()\n+            await task\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n+        except Exception as e:\n+            logger.error(f\"error in knowledge chat: {e}\")\n+            yield {\"data\": json.dumps({\"error\": str(e)})}\n+            return\n \n     if stream:\n         return EventSourceResponse(knowledge_base_chat_iterator())\ndiff --git a/libs/chatchat-server/chatchat/server/constant/__init__.py b/libs/chatchat-server/chatchat/server/constant/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/chatchat-server/chatchat/server/constant/response_code.py b/libs/chatchat-server/chatchat/server/constant/response_code.py\nnew file mode 100644\nindex 0000000000..8dafd53655\n--- /dev/null\n+++ b/libs/chatchat-server/chatchat/server/constant/response_code.py\n@@ -0,0 +1,5 @@\n+class ResponseCode:\n+    SUCCESS = 200\n+    BAD_REQUEST = 400\n+    NOT_FOUND = 404\n+    INTERNAL_SERVER_ERROR = 500\ndiff --git a/libs/chatchat-server/chatchat/server/file_rag/retrievers/__init__.py b/libs/chatchat-server/chatchat/server/file_rag/retrievers/__init__.py\nindex ef78fac775..7ef2f45278 100644\n--- a/libs/chatchat-server/chatchat/server/file_rag/retrievers/__init__.py\n+++ b/libs/chatchat-server/chatchat/server/file_rag/retrievers/__init__.py\n@@ -1,3 +1,4 @@\n from chatchat.server.file_rag.retrievers.base import BaseRetrieverService\n from chatchat.server.file_rag.retrievers.ensemble import EnsembleRetrieverService\n from chatchat.server.file_rag.retrievers.vectorstore import VectorstoreRetrieverService\n+from chatchat.server.file_rag.retrievers.milvus_vectorstore import MilvusVectorstoreRetrieverService\n\\ No newline at end of file\ndiff --git a/libs/chatchat-server/chatchat/server/file_rag/retrievers/base.py b/libs/chatchat-server/chatchat/server/file_rag/retrievers/base.py\nindex 6cda595825..e1491578dd 100644\n--- a/libs/chatchat-server/chatchat/server/file_rag/retrievers/base.py\n+++ b/libs/chatchat-server/chatchat/server/file_rag/retrievers/base.py\n@@ -1,3 +1,6 @@\n+from __future__ import annotations\n+\n+\n from abc import ABCMeta, abstractmethod\n \n from langchain.vectorstores import VectorStore\n@@ -15,7 +18,7 @@ def do_init(self, **kwargs):\n     def from_vectorstore(\n         vectorstore: VectorStore,\n         top_k: int,\n-        score_threshold: int or float,\n+        score_threshold: int | float,\n     ):\n         pass\n \ndiff --git a/libs/chatchat-server/chatchat/server/file_rag/retrievers/ensemble.py b/libs/chatchat-server/chatchat/server/file_rag/retrievers/ensemble.py\nindex 31a6aaea78..36b234f5bf 100644\n--- a/libs/chatchat-server/chatchat/server/file_rag/retrievers/ensemble.py\n+++ b/libs/chatchat-server/chatchat/server/file_rag/retrievers/ensemble.py\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n from langchain.retrievers import EnsembleRetriever\n from langchain.vectorstores import VectorStore\n from langchain_community.retrievers import BM25Retriever\n@@ -20,7 +22,7 @@ def do_init(\n     def from_vectorstore(\n         vectorstore: VectorStore,\n         top_k: int,\n-        score_threshold: int or float,\n+        score_threshold: int | float,\n     ):\n         faiss_retriever = vectorstore.as_retriever(\n             search_type=\"similarity_score_threshold\",\ndiff --git a/libs/chatchat-server/chatchat/server/file_rag/retrievers/milvus_vectorstore.py b/libs/chatchat-server/chatchat/server/file_rag/retrievers/milvus_vectorstore.py\nnew file mode 100644\nindex 0000000000..d0de13baba\n--- /dev/null\n+++ b/libs/chatchat-server/chatchat/server/file_rag/retrievers/milvus_vectorstore.py\n@@ -0,0 +1,124 @@\n+import warnings\n+\n+from langchain.vectorstores import VectorStore\n+from langchain_core.retrievers import BaseRetriever\n+from langchain_core.vectorstores import VectorStoreRetriever\n+\n+from chatchat.server.file_rag.retrievers.base import BaseRetrieverService\n+\n+from langchain.docstore.document import Document\n+from langchain_core.callbacks.manager import (\n+        AsyncCallbackManagerForRetrieverRun,\n+        CallbackManagerForRetrieverRun\n+)\n+\n+from typing import List\n+\n+class MilvusRetriever(VectorStoreRetriever):\n+    def _get_relevant_documents(\n+        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n+    ) -> List[Document]:\n+        if self.search_type == \"similarity\":\n+            docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n+        elif self.search_type == \"similarity_score_threshold\":\n+            docs_and_similarities = self.vectorstore.similarity_search_with_score(query)\n+            \n+            score_threshold = self.search_kwargs.get(\"score_threshold\", None)\n+            k = self.search_kwargs.get(\"k\", 4)\n+            \n+            if any(\n+                similarity < 0.0 or similarity > 1.0\n+                for _, similarity in docs_and_similarities\n+            ):\n+                warnings.warn(\n+                    \"Relevance scores must be between\"\n+                    f\" 0 and 1, got {docs_and_similarities}\"\n+                )\n+            if score_threshold is not None:\n+                docs_and_similarities = [\n+                doc\n+                for doc, similarity in docs_and_similarities\n+                if similarity >= score_threshold\n+            ]\n+            if len(docs_and_similarities) == 0:\n+                warnings.warn(\n+                    \"No relevant docs were retrieved using the relevance score\"\n+                    f\" threshold {score_threshold}\"\n+                )\n+            return docs_and_similarities\n+        elif self.search_type == \"mmr\":\n+            docs = self.vectorstore.max_marginal_relevance_search(\n+                query, **self.search_kwargs\n+            )\n+        else:\n+            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\n+        return docs\n+\n+    async def _aget_relevant_documents(\n+        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n+    ) -> List[Document]:\n+        if self.search_type == \"similarity\":\n+            docs = await self.vectorstore.asimilarity_search(\n+                query, **self.search_kwargs\n+            )\n+        elif self.search_type == \"similarity_score_threshold\":\n+            docs_and_similarities = (\n+                await self.vectorstore.asimilarity_search_with_score(query)\n+            )\n+            \n+            score_threshold = self.search_kwargs.get(\"score_threshold\", None)\n+            k = self.search_kwargs.get(\"k\", 4)\n+            \n+            if any(\n+                similarity < 0.0 or similarity > 1.0\n+                for _, similarity in docs_and_similarities\n+            ):\n+                warnings.warn(\n+                    \"Relevance scores must be between\"\n+                    f\" 0 and 1, got {docs_and_similarities}\"\n+                )\n+            if score_threshold is not None:\n+                docs_and_similarities = [\n+                (doc, similarity)\n+                for doc, similarity in docs_and_similarities\n+                if similarity >= score_threshold\n+            ]\n+            if len(docs_and_similarities) == 0:\n+                warnings.warn(\n+                    \"No relevant docs were retrieved using the relevance score\"\n+                    f\" threshold {score_threshold}\"\n+                )\n+            return docs_and_similarities\n+        elif self.search_type == \"mmr\":\n+            docs = await self.vectorstore.amax_marginal_relevance_search(\n+                query, **self.search_kwargs\n+            )\n+        else:\n+            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\n+        return docs \n+\n+class MilvusVectorstoreRetrieverService(BaseRetrieverService):\n+    def do_init(\n+        self,\n+        retriever: BaseRetriever = None,\n+        top_k: int = 5,\n+    ):\n+        self.vs = None\n+        self.top_k = top_k\n+        self.retriever = retriever\n+\n+    @staticmethod\n+    def from_vectorstore(\n+        vectorstore: VectorStore,\n+        top_k: int,\n+        score_threshold: int or float,\n+    ):\n+        retriever = MilvusRetriever(vectorstore=vectorstore, \n+                                    search_type=\"similarity_score_threshold\",\n+                                    search_kwargs={\"score_threshold\": score_threshold, \"k\": top_k}\n+                                    )\n+        \n+        return MilvusVectorstoreRetrieverService(retriever=retriever)\n+\n+    def get_relevant_documents(self, query: str):\n+        return self.retriever.get_relevant_documents(query)[: self.top_k]\ndiff --git a/libs/chatchat-server/chatchat/server/file_rag/retrievers/vectorstore.py b/libs/chatchat-server/chatchat/server/file_rag/retrievers/vectorstore.py\nindex ba65353c3d..018e3e506f 100644\n--- a/libs/chatchat-server/chatchat/server/file_rag/retrievers/vectorstore.py\n+++ b/libs/chatchat-server/chatchat/server/file_rag/retrievers/vectorstore.py\n@@ -1,3 +1,6 @@\n+from __future__ import annotations\n+\n+\n from langchain.vectorstores import VectorStore\n from langchain_core.retrievers import BaseRetriever\n \n@@ -18,7 +21,7 @@ def do_init(\n     def from_vectorstore(\n         vectorstore: VectorStore,\n         top_k: int,\n-        score_threshold: int or float,\n+        score_threshold: int | float,\n     ):\n         retriever = vectorstore.as_retriever(\n             search_type=\"similarity_score_threshold\",\ndiff --git a/libs/chatchat-server/chatchat/server/file_rag/utils.py b/libs/chatchat-server/chatchat/server/file_rag/utils.py\nindex cd767451a4..2f35245c45 100644\n--- a/libs/chatchat-server/chatchat/server/file_rag/utils.py\n+++ b/libs/chatchat-server/chatchat/server/file_rag/utils.py\n@@ -2,9 +2,11 @@\n     BaseRetrieverService,\n     EnsembleRetrieverService,\n     VectorstoreRetrieverService,\n+    MilvusVectorstoreRetrieverService,\n )\n \n Retrivals = {\n+    \"milvusvectorstore\": MilvusVectorstoreRetrieverService,\n     \"vectorstore\": VectorstoreRetrieverService,\n     \"ensemble\": EnsembleRetrieverService,\n }\ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/kb_cache/faiss_cache.py b/libs/chatchat-server/chatchat/server/knowledge_base/kb_cache/faiss_cache.py\nindex 0ce85f2074..5c9bc0d07a 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/kb_cache/faiss_cache.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/kb_cache/faiss_cache.py\n@@ -137,7 +137,7 @@ def load_vector_store(\n         except Exception as e:\n             if locked:  # we don't know exception raised before or after atomic.release\n                 self.atomic.release()\n-            logger.error(e, exc_info=True)\n+            logger.exception(e)\n             raise RuntimeError(f\"\u5411\u91cf\u5e93 {kb_name} \u52a0\u8f7d\u5931\u8d25\u3002\")\n         return self.get((kb_name, vector_name))\n \ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/kb_doc_api.py b/libs/chatchat-server/chatchat/server/knowledge_base/kb_doc_api.py\nindex aa250180bd..fd5ea6bb05 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/kb_doc_api.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/kb_doc_api.py\n@@ -1,3 +1,4 @@\n+import asyncio\n import json\n import os\n import urllib\n@@ -32,11 +33,13 @@\n )\n from chatchat.utils import build_logger\n \n-\n logger = build_logger()\n \n \n-def search_temp_docs(knowledge_id: str, query: str, top_k:int, score_threshold: float) -> List[Dict]:\n+def search_temp_docs(knowledge_id: str = Body(..., description=\"\u77e5\u8bc6\u5e93 ID\", examples=[\"example_id\"]),\n+                     query: str = Body(\"\", description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\u4f60\u597d\"]),\n+                     top_k: int = Body(..., description=\"\u8fd4\u56de\u7684\u6587\u6863\u6570\u91cf\", examples=[5]),\n+                     score_threshold: float = Body(..., description=\"\u5206\u6570\u9608\u503c\", examples=[0.8])) -> List[Dict]:\n     '''\u4ece\u4e34\u65f6 FAISS \u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u6587\u6863\uff0c\u7528\u4e8e\u6587\u4ef6\u5bf9\u8bdd'''\n     with memo_faiss_pool.acquire(knowledge_id) as vs:\n         docs = vs.similarity_search_with_score(\n@@ -47,21 +50,21 @@ def search_temp_docs(knowledge_id: str, query: str, top_k:int, score_threshold:\n \n \n def search_docs(\n-    query: str = Body(\"\", description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\u4f60\u597d\"]),\n-    knowledge_base_name: str = Body(\n-        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n-    ),\n-    top_k: int = Body(Settings.kb_settings.VECTOR_SEARCH_TOP_K, description=\"\u5339\u914d\u5411\u91cf\u6570\"),\n-    score_threshold: float = Body(\n-        Settings.kb_settings.SCORE_THRESHOLD,\n-        description=\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-1\u4e4b\u95f4\uff0c\"\n-        \"SCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\"\n-        \"\u53d6\u52302\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\",\n-        ge=0.0,\n-        le=2.0,\n-    ),\n-    file_name: str = Body(\"\", description=\"\u6587\u4ef6\u540d\u79f0\uff0c\u652f\u6301 sql \u901a\u914d\u7b26\"),\n-    metadata: dict = Body({}, description=\"\u6839\u636e metadata \u8fdb\u884c\u8fc7\u6ee4\uff0c\u4ec5\u652f\u6301\u4e00\u7ea7\u952e\"),\n+        query: str = Body(\"\", description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\u4f60\u597d\"]),\n+        knowledge_base_name: str = Body(\n+            ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+        ),\n+        top_k: int = Body(Settings.kb_settings.VECTOR_SEARCH_TOP_K, description=\"\u5339\u914d\u5411\u91cf\u6570\"),\n+        score_threshold: float = Body(\n+            Settings.kb_settings.SCORE_THRESHOLD,\n+            description=\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-1\u4e4b\u95f4\uff0c\"\n+                        \"SCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\"\n+                        \"\u53d6\u52302\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\",\n+            ge=0.0,\n+            le=2.0,\n+        ),\n+        file_name: str = Body(\"\", description=\"\u6587\u4ef6\u540d\u79f0\uff0c\u652f\u6301 sql \u901a\u914d\u7b26\"),\n+        metadata: dict = Body({}, description=\"\u6839\u636e metadata \u8fdb\u884c\u8fc7\u6ee4\uff0c\u4ec5\u652f\u6301\u4e00\u7ea7\u952e\"),\n ) -> List[Dict]:\n     kb = KBServiceFactory.get_service_by_name(knowledge_base_name)\n     data = []\n@@ -69,7 +72,7 @@ def search_docs(\n         if query:\n             docs = kb.search_docs(query, top_k, score_threshold)\n             # data = [DocumentWithVSId(**x[0].dict(), score=x[1], id=x[0].metadata.get(\"id\")) for x in docs]\n-            data = [DocumentWithVSId(**x.dict(), id=x.metadata.get(\"id\")) for x in docs]\n+            data = [DocumentWithVSId(**{\"id\": x.metadata.get(\"id\"), **x.dict()}) for x in docs]\n         elif file_name or metadata:\n             data = kb.list_docs(file_name=file_name, metadata=metadata)\n             for d in data:\n@@ -94,7 +97,7 @@ def list_files(knowledge_base_name: str) -> ListResponse:\n \n \n def _save_files_in_thread(\n-    files: List[UploadFile], knowledge_base_name: str, override: bool\n+        files: List[UploadFile], knowledge_base_name: str, override: bool\n ):\n     \"\"\"\n     \u901a\u8fc7\u591a\u7ebf\u7a0b\u5c06\u4e0a\u4f20\u7684\u6587\u4ef6\u4fdd\u5b58\u5230\u5bf9\u5e94\u77e5\u8bc6\u5e93\u76ee\u5f55\u5185\u3002\n@@ -114,9 +117,9 @@ def save_file(file: UploadFile, knowledge_base_name: str, override: bool) -> dic\n \n             file_content = file.file.read()  # \u8bfb\u53d6\u4e0a\u4f20\u6587\u4ef6\u7684\u5185\u5bb9\n             if (\n-                os.path.isfile(file_path)\n-                and not override\n-                and os.path.getsize(file_path) == len(file_content)\n+                    os.path.isfile(file_path)\n+                    and not override\n+                    and os.path.getsize(file_path) == len(file_content)\n             ):\n                 file_status = f\"\u6587\u4ef6 {filename} \u5df2\u5b58\u5728\u3002\"\n                 logger.warn(file_status)\n@@ -154,17 +157,17 @@ def save_file(file: UploadFile, knowledge_base_name: str, override: bool) -> dic\n \n \n def upload_docs(\n-    files: List[UploadFile] = File(..., description=\"\u4e0a\u4f20\u6587\u4ef6\uff0c\u652f\u6301\u591a\u6587\u4ef6\"),\n-    knowledge_base_name: str = Form(\n-        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n-    ),\n-    override: bool = Form(False, description=\"\u8986\u76d6\u5df2\u6709\u6587\u4ef6\"),\n-    to_vector_store: bool = Form(True, description=\"\u4e0a\u4f20\u6587\u4ef6\u540e\u662f\u5426\u8fdb\u884c\u5411\u91cf\u5316\"),\n-    chunk_size: int = Form(Settings.kb_settings.CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n-    chunk_overlap: int = Form(Settings.kb_settings.OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n-    zh_title_enhance: bool = Form(Settings.kb_settings.ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n-    docs: str = Form(\"\", description=\"\u81ea\u5b9a\u4e49\u7684docs\uff0c\u9700\u8981\u8f6c\u4e3ajson\u5b57\u7b26\u4e32\"),\n-    not_refresh_vs_cache: bool = Form(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n+        files: List[UploadFile] = File(..., description=\"\u4e0a\u4f20\u6587\u4ef6\uff0c\u652f\u6301\u591a\u6587\u4ef6\"),\n+        knowledge_base_name: str = Form(\n+            ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+        ),\n+        override: bool = Form(False, description=\"\u8986\u76d6\u5df2\u6709\u6587\u4ef6\"),\n+        to_vector_store: bool = Form(True, description=\"\u4e0a\u4f20\u6587\u4ef6\u540e\u662f\u5426\u8fdb\u884c\u5411\u91cf\u5316\"),\n+        chunk_size: int = Form(Settings.kb_settings.CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n+        chunk_overlap: int = Form(Settings.kb_settings.OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n+        zh_title_enhance: bool = Form(Settings.kb_settings.ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n+        docs: str = Form(\"\", description=\"\u81ea\u5b9a\u4e49\u7684docs\uff0c\u9700\u8981\u8f6c\u4e3ajson\u5b57\u7b26\u4e32\"),\n+        not_refresh_vs_cache: bool = Form(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n ) -> BaseResponse:\n     \"\"\"\n     API\u63a5\u53e3\uff1a\u4e0a\u4f20\u6587\u4ef6\uff0c\u5e76/\u6216\u5411\u91cf\u5316\n@@ -182,7 +185,7 @@ def upload_docs(\n \n     # \u5148\u5c06\u4e0a\u4f20\u7684\u6587\u4ef6\u4fdd\u5b58\u5230\u78c1\u76d8\n     for result in _save_files_in_thread(\n-        files, knowledge_base_name=knowledge_base_name, override=override\n+            files, knowledge_base_name=knowledge_base_name, override=override\n     ):\n         filename = result[\"data\"][\"file_name\"]\n         if result[\"code\"] != 200:\n@@ -213,10 +216,10 @@ def upload_docs(\n \n \n def delete_docs(\n-    knowledge_base_name: str = Body(..., examples=[\"samples\"]),\n-    file_names: List[str] = Body(..., examples=[[\"file_name.md\", \"test.txt\"]]),\n-    delete_content: bool = Body(False),\n-    not_refresh_vs_cache: bool = Body(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n+        knowledge_base_name: str = Body(..., examples=[\"samples\"]),\n+        file_names: List[str] = Body(..., examples=[[\"file_name.md\", \"test.txt\"]]),\n+        delete_content: bool = Body(False),\n+        not_refresh_vs_cache: bool = Body(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n ) -> BaseResponse:\n     if not validate_kb_name(knowledge_base_name):\n         return BaseResponse(code=403, msg=\"Don't attack me\")\n@@ -250,10 +253,10 @@ def delete_docs(\n \n \n def update_info(\n-    knowledge_base_name: str = Body(\n-        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n-    ),\n-    kb_info: str = Body(..., description=\"\u77e5\u8bc6\u5e93\u4ecb\u7ecd\", examples=[\"\u8fd9\u662f\u4e00\u4e2a\u77e5\u8bc6\u5e93\"]),\n+        knowledge_base_name: str = Body(\n+            ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+        ),\n+        kb_info: str = Body(..., description=\"\u77e5\u8bc6\u5e93\u4ecb\u7ecd\", examples=[\"\u8fd9\u662f\u4e00\u4e2a\u77e5\u8bc6\u5e93\"]),\n ):\n     if not validate_kb_name(knowledge_base_name):\n         return BaseResponse(code=403, msg=\"Don't attack me\")\n@@ -267,18 +270,18 @@ def update_info(\n \n \n def update_docs(\n-    knowledge_base_name: str = Body(\n-        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n-    ),\n-    file_names: List[str] = Body(\n-        ..., description=\"\u6587\u4ef6\u540d\u79f0\uff0c\u652f\u6301\u591a\u6587\u4ef6\", examples=[[\"file_name1\", \"text.txt\"]]\n-    ),\n-    chunk_size: int = Body(Settings.kb_settings.CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n-    chunk_overlap: int = Body(Settings.kb_settings.OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n-    zh_title_enhance: bool = Body(Settings.kb_settings.ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n-    override_custom_docs: bool = Body(False, description=\"\u662f\u5426\u8986\u76d6\u4e4b\u524d\u81ea\u5b9a\u4e49\u7684docs\"),\n-    docs: str = Body(\"\", description=\"\u81ea\u5b9a\u4e49\u7684docs\uff0c\u9700\u8981\u8f6c\u4e3ajson\u5b57\u7b26\u4e32\"),\n-    not_refresh_vs_cache: bool = Body(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n+        knowledge_base_name: str = Body(\n+            ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+        ),\n+        file_names: List[str] = Body(\n+            ..., description=\"\u6587\u4ef6\u540d\u79f0\uff0c\u652f\u6301\u591a\u6587\u4ef6\", examples=[[\"file_name1\", \"text.txt\"]]\n+        ),\n+        chunk_size: int = Body(Settings.kb_settings.CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n+        chunk_overlap: int = Body(Settings.kb_settings.OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n+        zh_title_enhance: bool = Body(Settings.kb_settings.ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n+        override_custom_docs: bool = Body(False, description=\"\u662f\u5426\u8986\u76d6\u4e4b\u524d\u81ea\u5b9a\u4e49\u7684docs\"),\n+        docs: str = Body(\"\", description=\"\u81ea\u5b9a\u4e49\u7684docs\uff0c\u9700\u8981\u8f6c\u4e3ajson\u5b57\u7b26\u4e32\"),\n+        not_refresh_vs_cache: bool = Body(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n ) -> BaseResponse:\n     \"\"\"\n     \u66f4\u65b0\u77e5\u8bc6\u5e93\u6587\u6863\n@@ -315,10 +318,10 @@ def update_docs(\n     # \u4ece\u6587\u4ef6\u751f\u6210docs\uff0c\u5e76\u8fdb\u884c\u5411\u91cf\u5316\u3002\n     # \u8fd9\u91cc\u5229\u7528\u4e86KnowledgeFile\u7684\u7f13\u5b58\u529f\u80fd\uff0c\u5728\u591a\u7ebf\u7a0b\u4e2d\u52a0\u8f7dDocument\uff0c\u7136\u540e\u4f20\u7ed9KnowledgeFile\n     for status, result in files2docs_in_thread(\n-        kb_files,\n-        chunk_size=chunk_size,\n-        chunk_overlap=chunk_overlap,\n-        zh_title_enhance=zh_title_enhance,\n+            kb_files,\n+            chunk_size=chunk_size,\n+            chunk_overlap=chunk_overlap,\n+            zh_title_enhance=zh_title_enhance,\n     ):\n         if status:\n             kb_name, file_name, new_docs = result\n@@ -353,11 +356,11 @@ def update_docs(\n \n \n def download_doc(\n-    knowledge_base_name: str = Query(\n-        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n-    ),\n-    file_name: str = Query(..., description=\"\u6587\u4ef6\u540d\u79f0\", examples=[\"test.txt\"]),\n-    preview: bool = Query(False, description=\"\u662f\uff1a\u6d4f\u89c8\u5668\u5185\u9884\u89c8\uff1b\u5426\uff1a\u4e0b\u8f7d\"),\n+        knowledge_base_name: str = Query(\n+            ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+        ),\n+        file_name: str = Query(..., description=\"\u6587\u4ef6\u540d\u79f0\", examples=[\"test.txt\"]),\n+        preview: bool = Query(False, description=\"\u662f\uff1a\u6d4f\u89c8\u5668\u5185\u9884\u89c8\uff1b\u5426\uff1a\u4e0b\u8f7d\"),\n ):\n     \"\"\"\n     \u4e0b\u8f7d\u77e5\u8bc6\u5e93\u6587\u6863\n@@ -395,14 +398,14 @@ def download_doc(\n \n \n def recreate_vector_store(\n-    knowledge_base_name: str = Body(..., examples=[\"samples\"]),\n-    allow_empty_kb: bool = Body(True),\n-    vs_type: str = Body(Settings.kb_settings.DEFAULT_VS_TYPE),\n-    embed_model: str = Body(get_default_embedding()),\n-    chunk_size: int = Body(Settings.kb_settings.CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n-    chunk_overlap: int = Body(Settings.kb_settings.OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n-    zh_title_enhance: bool = Body(Settings.kb_settings.ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n-    not_refresh_vs_cache: bool = Body(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n+        knowledge_base_name: str = Body(..., examples=[\"samples\"]),\n+        allow_empty_kb: bool = Body(True),\n+        vs_type: str = Body(Settings.kb_settings.DEFAULT_VS_TYPE),\n+        embed_model: str = Body(get_default_embedding()),\n+        chunk_size: int = Body(Settings.kb_settings.CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n+        chunk_overlap: int = Body(Settings.kb_settings.OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n+        zh_title_enhance: bool = Body(Settings.kb_settings.ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n+        not_refresh_vs_cache: bool = Body(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\n ):\n     \"\"\"\n     recreate vector store from the content.\n@@ -412,57 +415,59 @@ def recreate_vector_store(\n     \"\"\"\n \n     def output():\n-        kb = KBServiceFactory.get_service(knowledge_base_name, vs_type, embed_model)\n-        if not kb.exists() and not allow_empty_kb:\n-            yield {\"code\": 404, \"msg\": f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 \u2018{knowledge_base_name}\u2019\"}\n-        else:\n-            error_msg = (\n-                f\"could not recreate vector store because failed to access embed model.\"\n-            )\n-            if not kb.check_embed_model(error_msg):\n-                yield {\"code\": 404, \"msg\": error_msg}\n+        try:\n+            kb = KBServiceFactory.get_service(knowledge_base_name, vs_type, embed_model)\n+            if not kb.exists() and not allow_empty_kb:\n+                yield {\"code\": 404, \"msg\": f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 \u2018{knowledge_base_name}\u2019\"}\n             else:\n-                if kb.exists():\n-                    kb.clear_vs()\n-                kb.create_kb()\n-                files = list_files_from_folder(knowledge_base_name)\n-                kb_files = [(file, knowledge_base_name) for file in files]\n-                i = 0\n-                for status, result in files2docs_in_thread(\n-                    kb_files,\n-                    chunk_size=chunk_size,\n-                    chunk_overlap=chunk_overlap,\n-                    zh_title_enhance=zh_title_enhance,\n-                ):\n-                    if status:\n-                        kb_name, file_name, docs = result\n-                        kb_file = KnowledgeFile(\n-                            filename=file_name, knowledge_base_name=kb_name\n-                        )\n-                        kb_file.splited_docs = docs\n-                        yield json.dumps(\n-                            {\n-                                \"code\": 200,\n-                                \"msg\": f\"({i + 1} / {len(files)}): {file_name}\",\n-                                \"total\": len(files),\n-                                \"finished\": i + 1,\n-                                \"doc\": file_name,\n-                            },\n-                            ensure_ascii=False,\n-                        )\n-                        kb.add_doc(kb_file, not_refresh_vs_cache=True)\n-                    else:\n-                        kb_name, file_name, error = result\n-                        msg = f\"\u6dfb\u52a0\u6587\u4ef6\u2018{file_name}\u2019\u5230\u77e5\u8bc6\u5e93\u2018{knowledge_base_name}\u2019\u65f6\u51fa\u9519\uff1a{error}\u3002\u5df2\u8df3\u8fc7\u3002\"\n-                        logger.error(msg)\n-                        yield json.dumps(\n-                            {\n-                                \"code\": 500,\n-                                \"msg\": msg,\n-                            }\n-                        )\n-                    i += 1\n-                if not not_refresh_vs_cache:\n-                    kb.save_vector_store()\n+                ok, msg = kb.check_embed_model()\n+                if not ok:\n+                    yield {\"code\": 404, \"msg\": msg}\n+                else:\n+                    if kb.exists():\n+                        kb.clear_vs()\n+                    kb.create_kb()\n+                    files = list_files_from_folder(knowledge_base_name)\n+                    kb_files = [(file, knowledge_base_name) for file in files]\n+                    i = 0\n+                    for status, result in files2docs_in_thread(\n+                            kb_files,\n+                            chunk_size=chunk_size,\n+                            chunk_overlap=chunk_overlap,\n+                            zh_title_enhance=zh_title_enhance,\n+                    ):\n+                        if status:\n+                            kb_name, file_name, docs = result\n+                            kb_file = KnowledgeFile(\n+                                filename=file_name, knowledge_base_name=kb_name\n+                            )\n+                            kb_file.splited_docs = docs\n+                            yield json.dumps(\n+                                {\n+                                    \"code\": 200,\n+                                    \"msg\": f\"({i + 1} / {len(files)}): {file_name}\",\n+                                    \"total\": len(files),\n+                                    \"finished\": i + 1,\n+                                    \"doc\": file_name,\n+                                },\n+                                ensure_ascii=False,\n+                            )\n+                            kb.add_doc(kb_file, not_refresh_vs_cache=True)\n+                        else:\n+                            kb_name, file_name, error = result\n+                            msg = f\"\u6dfb\u52a0\u6587\u4ef6\u2018{file_name}\u2019\u5230\u77e5\u8bc6\u5e93\u2018{knowledge_base_name}\u2019\u65f6\u51fa\u9519\uff1a{error}\u3002\u5df2\u8df3\u8fc7\u3002\"\n+                            logger.error(msg)\n+                            yield json.dumps(\n+                                {\n+                                    \"code\": 500,\n+                                    \"msg\": msg,\n+                                }\n+                            )\n+                        i += 1\n+                    if not not_refresh_vs_cache:\n+                        kb.save_vector_store()\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n \n     return EventSourceResponse(output())\ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/base.py b/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/base.py\nindex fe2c23f7f0..7d5c4c4791 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/base.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/base.py\n@@ -80,12 +80,8 @@ def save_vector_store(self):\n         \"\"\"\n         pass\n \n-    def check_embed_model(self, error_msg: str) -> bool:\n-        if not _check_embed_model(self.embed_model):\n-            logger.error(error_msg, exc_info=True)\n-            return False\n-        else:\n-            return True\n+    def check_embed_model(self) -> Tuple[bool, str]:\n+        return _check_embed_model(self.embed_model)\n \n     def create_kb(self):\n         \"\"\"\n@@ -123,9 +119,7 @@ def add_doc(self, kb_file: KnowledgeFile, docs: List[Document] = [], **kwargs):\n         \u5411\u77e5\u8bc6\u5e93\u6dfb\u52a0\u6587\u4ef6\n         \u5982\u679c\u6307\u5b9a\u4e86docs\uff0c\u5219\u4e0d\u518d\u5c06\u6587\u672c\u5411\u91cf\u5316\uff0c\u5e76\u5c06\u6570\u636e\u5e93\u5bf9\u5e94\u6761\u76ee\u6807\u4e3acustom_docs=True\n         \"\"\"\n-        if not self.check_embed_model(\n-            f\"could not add docs because failed to access embed model.\"\n-        ):\n+        if not self.check_embed_model()[0]:\n             return False\n \n         if docs:\n@@ -186,9 +180,7 @@ def update_doc(self, kb_file: KnowledgeFile, docs: List[Document] = [], **kwargs\n         \u4f7f\u7528content\u4e2d\u7684\u6587\u4ef6\u66f4\u65b0\u5411\u91cf\u5e93\n         \u5982\u679c\u6307\u5b9a\u4e86docs\uff0c\u5219\u4f7f\u7528\u81ea\u5b9a\u4e49docs\uff0c\u5e76\u5c06\u6570\u636e\u5e93\u5bf9\u5e94\u6761\u76ee\u6807\u4e3acustom_docs=True\n         \"\"\"\n-        if not self.check_embed_model(\n-            f\"could not update docs because failed to access embed model.\"\n-        ):\n+        if not self.check_embed_model()[0]:\n             return False\n \n         if os.path.exists(kb_file.filepath):\n@@ -212,10 +204,9 @@ def search_docs(\n         top_k: int = Settings.kb_settings.VECTOR_SEARCH_TOP_K,\n         score_threshold: float = Settings.kb_settings.SCORE_THRESHOLD,\n     ) -> List[Document]:\n-        if not self.check_embed_model(\n-            f\"could not search docs because failed to access embed model.\"\n-        ):\n+        if not self.check_embed_model()[0]:\n             return []\n+\n         docs = self.do_search(query, top_k, score_threshold)\n         return docs\n \n@@ -230,9 +221,7 @@ def update_doc_by_ids(self, docs: Dict[str, Document]) -> bool:\n         \u4f20\u5165\u53c2\u6570\u4e3a\uff1a {doc_id: Document, ...}\n         \u5982\u679c\u5bf9\u5e94 doc_id \u7684\u503c\u4e3a None\uff0c\u6216\u5176 page_content \u4e3a\u7a7a\uff0c\u5219\u5220\u9664\u8be5\u6587\u6863\n         \"\"\"\n-        if not self.check_embed_model(\n-            f\"could not update docs because failed to access embed model.\"\n-        ):\n+        if not self.check_embed_model()[0]:\n             return False\n \n         self.del_doc_by_ids(list(docs.keys()))\n@@ -260,7 +249,7 @@ def list_docs(\n             doc_info = self.get_doc_by_ids([x[\"id\"]])[0]\n             if doc_info is not None:\n                 # \u5904\u7406\u975e\u7a7a\u7684\u60c5\u51b5\n-                doc_with_id = DocumentWithVSId(**doc_info.dict(), id=x[\"id\"])\n+                doc_with_id = DocumentWithVSId(**{**doc_info.dict(), \"id\":x[\"id\"]})\n                 docs.append(doc_with_id)\n             else:\n                 # \u5904\u7406\u7a7a\u7684\u60c5\u51b5\ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/chromadb_kb_service.py b/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/chromadb_kb_service.py\nindex ed0cf42b1c..bdcf1cf7d3 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/chromadb_kb_service.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/chromadb_kb_service.py\n@@ -4,6 +4,7 @@\n import chromadb\n from chromadb.api.types import GetResult, QueryResult\n from langchain.docstore.document import Document\n+from langchain_chroma import Chroma\n \n from chatchat.settings import Settings\n from chatchat.server.file_rag.utils import get_Retriever\n@@ -49,9 +50,9 @@ def _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n class ChromaKBService(KBService):\n     vs_path: str\n     kb_path: str\n+    chroma: Chroma\n \n     client = None\n-    collection = None\n \n     def vs_type(self) -> str:\n         return SupportedVSType.CHROMADB\n@@ -61,16 +62,23 @@ def get_vs_path(self) -> str:\n \n     def get_kb_path(self) -> str:\n         return get_kb_path(self.kb_name)\n+    \n+    def _load_chroma(self):\n+        self.chroma = Chroma(\n+            client=self.client,\n+            collection_name=self.kb_name,\n+            embedding_function=get_Embeddings(self.embed_model),\n+        )\n \n     def do_init(self) -> None:\n         self.kb_path = self.get_kb_path()\n         self.vs_path = self.get_vs_path()\n         self.client = chromadb.PersistentClient(path=self.vs_path)\n-        self.collection = self.client.get_or_create_collection(self.kb_name)\n+        collection = self.client.get_or_create_collection(self.kb_name)\n+        self._load_chroma()\n \n     def do_create_kb(self) -> None:\n-        # In ChromaDB, creating a KB is equivalent to creating a collection\n-        self.collection = self.client.get_or_create_collection(self.kb_name)\n+        pass\n \n     def do_drop_kb(self):\n         # Dropping a KB is equivalent to deleting a collection in ChromaDB\n@@ -84,7 +92,7 @@ def do_search(\n         self, query: str, top_k: int, score_threshold: float = Settings.kb_settings.SCORE_THRESHOLD\n     ) -> List[Tuple[Document, float]]:\n         retriever = get_Retriever(\"vectorstore\").from_vectorstore(\n-            self.collection,\n+            self.chroma,\n             top_k=top_k,\n             score_threshold=score_threshold,\n         )\n@@ -99,18 +107,18 @@ def do_add_doc(self, docs: List[Document], **kwargs) -> List[Dict]:\n         embeddings = embed_func.embed_documents(texts=texts)\n         ids = [str(uuid.uuid1()) for _ in range(len(texts))]\n         for _id, text, embedding, metadata in zip(ids, texts, embeddings, metadatas):\n-            self.collection.add(\n+            self.chroma._collection.add(\n                 ids=_id, embeddings=embedding, metadatas=metadata, documents=text\n             )\n             doc_infos.append({\"id\": _id, \"metadata\": metadata})\n         return doc_infos\n \n     def get_doc_by_ids(self, ids: List[str]) -> List[Document]:\n-        get_result: GetResult = self.collection.get(ids=ids)\n+        get_result: GetResult = self.chroma._collection.get(ids=ids)\n         return _get_result_to_documents(get_result)\n \n     def del_doc_by_ids(self, ids: List[str]) -> bool:\n-        self.collection.delete(ids=ids)\n+        self.chroma._collection.delete(ids=ids)\n         return True\n \n     def do_clear_vs(self):\n@@ -118,4 +126,4 @@ def do_clear_vs(self):\n         self.do_drop_kb()\n \n     def do_delete_doc(self, kb_file: KnowledgeFile, **kwargs):\n-        return self.collection.delete(where={\"source\": kb_file.filepath})\n+        return self.chroma._collection.delete(where={\"source\": kb_file.filepath})\ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/milvus_kb_service.py b/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/milvus_kb_service.py\nindex e6bd9c4b06..ef06c242e7 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/milvus_kb_service.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/kb_service/milvus_kb_service.py\n@@ -80,7 +80,7 @@ def do_search(self, query: str, top_k: int, score_threshold: float):\n         # embed_func = get_Embeddings(self.embed_model)\n         # embeddings = embed_func.embed_query(query)\n         # docs = self.milvus.similarity_search_with_score_by_vector(embeddings, top_k)\n-        retriever = get_Retriever(\"vectorstore\").from_vectorstore(\n+        retriever = get_Retriever(\"milvusvectorstore\").from_vectorstore(\n             self.milvus,\n             top_k=top_k,\n             score_threshold=score_threshold,\ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/kb_summary_api.py b/libs/chatchat-server/chatchat/server/knowledge_base/kb_summary_api.py\nindex a49c58d0e2..22c6a1d3c5 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/kb_summary_api.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/kb_summary_api.py\n@@ -1,3 +1,4 @@\n+import asyncio\n import json\n from typing import List, Optional\n \n@@ -16,7 +17,6 @@\n \n logger = build_logger()\n \n-\n def recreate_summary_vector_store(\n     knowledge_base_name: str = Body(..., examples=[\"samples\"]),\n     allow_empty_kb: bool = Body(True),\n@@ -41,71 +41,77 @@ def recreate_summary_vector_store(\n     :param embed_model:\n     :return:\n     \"\"\"\n+    if max_tokens in [None, 0]:\n+        max_tokens = Settings.model_settings.MAX_TOKENS\n \n     def output():\n-        kb = KBServiceFactory.get_service(knowledge_base_name, vs_type, embed_model)\n-        if not kb.exists() and not allow_empty_kb:\n-            yield {\"code\": 404, \"msg\": f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 \u2018{knowledge_base_name}\u2019\"}\n-        else:\n-            error_msg = f\"could not recreate summary vector store because failed to access embed model.\"\n-            if not kb.check_embed_model(error_msg):\n-                yield {\"code\": 404, \"msg\": error_msg}\n+        try:\n+            kb = KBServiceFactory.get_service(knowledge_base_name, vs_type, embed_model)\n+            if not kb.exists() and not allow_empty_kb:\n+                yield {\"code\": 404, \"msg\": f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 \u2018{knowledge_base_name}\u2019\"}\n             else:\n-                # \u91cd\u65b0\u521b\u5efa\u77e5\u8bc6\u5e93\n-                kb_summary = KBSummaryService(knowledge_base_name, embed_model)\n-                kb_summary.drop_kb_summary()\n-                kb_summary.create_kb_summary()\n+                ok, msg = kb.check_embed_model()\n+                if not ok:\n+                    yield {\"code\": 404, \"msg\": msg}\n+                else:\n+                    # \u91cd\u65b0\u521b\u5efa\u77e5\u8bc6\u5e93\n+                    kb_summary = KBSummaryService(knowledge_base_name, embed_model)\n+                    kb_summary.drop_kb_summary()\n+                    kb_summary.create_kb_summary()\n \n-                llm = get_ChatOpenAI(\n-                    model_name=model_name,\n-                    temperature=temperature,\n-                    max_tokens=max_tokens,\n-                    local_wrap=True,\n-                )\n-                reduce_llm = get_ChatOpenAI(\n-                    model_name=model_name,\n-                    temperature=temperature,\n-                    max_tokens=max_tokens,\n-                    local_wrap=True,\n-                )\n-                # \u6587\u672c\u6458\u8981\u9002\u914d\u5668\n-                summary = SummaryAdapter.form_summary(\n-                    llm=llm, reduce_llm=reduce_llm, overlap_size=Settings.kb_settings.OVERLAP_SIZE\n-                )\n-                files = list_files_from_folder(knowledge_base_name)\n-\n-                i = 0\n-                for i, file_name in enumerate(files):\n-                    doc_infos = kb.list_docs(file_name=file_name)\n-                    docs = summary.summarize(\n-                        file_description=file_description, docs=doc_infos\n+                    llm = get_ChatOpenAI(\n+                        model_name=model_name,\n+                        temperature=temperature,\n+                        max_tokens=max_tokens,\n+                        local_wrap=True,\n                     )\n-\n-                    status_kb_summary = kb_summary.add_kb_summary(\n-                        summary_combine_docs=docs\n+                    reduce_llm = get_ChatOpenAI(\n+                        model_name=model_name,\n+                        temperature=temperature,\n+                        max_tokens=max_tokens,\n+                        local_wrap=True,\n                     )\n-                    if status_kb_summary:\n-                        logger.info(f\"({i + 1} / {len(files)}): {file_name} \u603b\u7ed3\u5b8c\u6210\")\n-                        yield json.dumps(\n-                            {\n-                                \"code\": 200,\n-                                \"msg\": f\"({i + 1} / {len(files)}): {file_name}\",\n-                                \"total\": len(files),\n-                                \"finished\": i + 1,\n-                                \"doc\": file_name,\n-                            },\n-                            ensure_ascii=False,\n+                    # \u6587\u672c\u6458\u8981\u9002\u914d\u5668\n+                    summary = SummaryAdapter.form_summary(\n+                        llm=llm, reduce_llm=reduce_llm, overlap_size=Settings.kb_settings.OVERLAP_SIZE\n+                    )\n+                    files = list_files_from_folder(knowledge_base_name)\n+\n+                    i = 0\n+                    for i, file_name in enumerate(files):\n+                        doc_infos = kb.list_docs(file_name=file_name)\n+                        docs = summary.summarize(\n+                            file_description=file_description, docs=doc_infos\n                         )\n-                    else:\n-                        msg = f\"\u77e5\u8bc6\u5e93'{knowledge_base_name}'\u603b\u7ed3\u6587\u4ef6\u2018{file_name}\u2019\u65f6\u51fa\u9519\u3002\u5df2\u8df3\u8fc7\u3002\"\n-                        logger.error(msg)\n-                        yield json.dumps(\n-                            {\n-                                \"code\": 500,\n-                                \"msg\": msg,\n-                            }\n+\n+                        status_kb_summary = kb_summary.add_kb_summary(\n+                            summary_combine_docs=docs\n                         )\n-                    i += 1\n+                        if status_kb_summary:\n+                            logger.info(f\"({i + 1} / {len(files)}): {file_name} \u603b\u7ed3\u5b8c\u6210\")\n+                            yield json.dumps(\n+                                {\n+                                    \"code\": 200,\n+                                    \"msg\": f\"({i + 1} / {len(files)}): {file_name}\",\n+                                    \"total\": len(files),\n+                                    \"finished\": i + 1,\n+                                    \"doc\": file_name,\n+                                },\n+                                ensure_ascii=False,\n+                            )\n+                        else:\n+                            msg = f\"\u77e5\u8bc6\u5e93'{knowledge_base_name}'\u603b\u7ed3\u6587\u4ef6\u2018{file_name}\u2019\u65f6\u51fa\u9519\u3002\u5df2\u8df3\u8fc7\u3002\"\n+                            logger.error(msg)\n+                            yield json.dumps(\n+                                {\n+                                    \"code\": 500,\n+                                    \"msg\": msg,\n+                                }\n+                            )\n+                        i += 1\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n \n     return EventSourceResponse(output())\n \n@@ -138,55 +144,59 @@ def summary_file_to_vector_store(\n     \"\"\"\n \n     def output():\n-        kb = KBServiceFactory.get_service(knowledge_base_name, vs_type, embed_model)\n-        if not kb.exists() and not allow_empty_kb:\n-            yield {\"code\": 404, \"msg\": f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 \u2018{knowledge_base_name}\u2019\"}\n-        else:\n-            # \u91cd\u65b0\u521b\u5efa\u77e5\u8bc6\u5e93\n-            kb_summary = KBSummaryService(knowledge_base_name, embed_model)\n-            kb_summary.create_kb_summary()\n-\n-            llm = get_ChatOpenAI(\n-                model_name=model_name,\n-                temperature=temperature,\n-                max_tokens=max_tokens,\n-                local_wrap=True,\n-            )\n-            reduce_llm = get_ChatOpenAI(\n-                model_name=model_name,\n-                temperature=temperature,\n-                max_tokens=max_tokens,\n-                local_wrap=True,\n-            )\n-            # \u6587\u672c\u6458\u8981\u9002\u914d\u5668\n-            summary = SummaryAdapter.form_summary(\n-                llm=llm, reduce_llm=reduce_llm, overlap_size=Settings.kb_settings.OVERLAP_SIZE\n-            )\n-\n-            doc_infos = kb.list_docs(file_name=file_name)\n-            docs = summary.summarize(file_description=file_description, docs=doc_infos)\n+        try:\n+            kb = KBServiceFactory.get_service(knowledge_base_name, vs_type, embed_model)\n+            if not kb.exists() and not allow_empty_kb:\n+                yield {\"code\": 404, \"msg\": f\"\u672a\u627e\u5230\u77e5\u8bc6\u5e93 \u2018{knowledge_base_name}\u2019\"}\n+            else:\n+                # \u91cd\u65b0\u521b\u5efa\u77e5\u8bc6\u5e93\n+                kb_summary = KBSummaryService(knowledge_base_name, embed_model)\n+                kb_summary.create_kb_summary()\n \n-            status_kb_summary = kb_summary.add_kb_summary(summary_combine_docs=docs)\n-            if status_kb_summary:\n-                logger.info(f\" {file_name} \u603b\u7ed3\u5b8c\u6210\")\n-                yield json.dumps(\n-                    {\n-                        \"code\": 200,\n-                        \"msg\": f\"{file_name} \u603b\u7ed3\u5b8c\u6210\",\n-                        \"doc\": file_name,\n-                    },\n-                    ensure_ascii=False,\n+                llm = get_ChatOpenAI(\n+                    model_name=model_name,\n+                    temperature=temperature,\n+                    max_tokens=max_tokens,\n+                    local_wrap=True,\n                 )\n-            else:\n-                msg = f\"\u77e5\u8bc6\u5e93'{knowledge_base_name}'\u603b\u7ed3\u6587\u4ef6\u2018{file_name}\u2019\u65f6\u51fa\u9519\u3002\u5df2\u8df3\u8fc7\u3002\"\n-                logger.error(msg)\n-                yield json.dumps(\n-                    {\n-                        \"code\": 500,\n-                        \"msg\": msg,\n-                    }\n+                reduce_llm = get_ChatOpenAI(\n+                    model_name=model_name,\n+                    temperature=temperature,\n+                    max_tokens=max_tokens,\n+                    local_wrap=True,\n+                )\n+                # \u6587\u672c\u6458\u8981\u9002\u914d\u5668\n+                summary = SummaryAdapter.form_summary(\n+                    llm=llm, reduce_llm=reduce_llm, overlap_size=Settings.kb_settings.OVERLAP_SIZE\n                 )\n \n+                doc_infos = kb.list_docs(file_name=file_name)\n+                docs = summary.summarize(file_description=file_description, docs=doc_infos)\n+\n+                status_kb_summary = kb_summary.add_kb_summary(summary_combine_docs=docs)\n+                if status_kb_summary:\n+                    logger.info(f\" {file_name} \u603b\u7ed3\u5b8c\u6210\")\n+                    yield json.dumps(\n+                        {\n+                            \"code\": 200,\n+                            \"msg\": f\"{file_name} \u603b\u7ed3\u5b8c\u6210\",\n+                            \"doc\": file_name,\n+                        },\n+                        ensure_ascii=False,\n+                    )\n+                else:\n+                    msg = f\"\u77e5\u8bc6\u5e93'{knowledge_base_name}'\u603b\u7ed3\u6587\u4ef6\u2018{file_name}\u2019\u65f6\u51fa\u9519\u3002\u5df2\u8df3\u8fc7\u3002\"\n+                    logger.error(msg)\n+                    yield json.dumps(\n+                        {\n+                            \"code\": 500,\n+                            \"msg\": msg,\n+                        }\n+                    )\n+        except asyncio.exceptions.CancelledError:\n+            logger.warning(\"streaming progress has been interrupted by user.\")\n+            return\n+\n     return EventSourceResponse(output())\n \n \n@@ -240,7 +250,7 @@ def summary_doc_ids_to_vector_store(\n         doc_infos = kb.get_doc_by_ids(ids=doc_ids)\n         # doc_infos\u8f6c\u6362\u6210DocumentWithVSId\u5305\u88c5\u7684\u5bf9\u8c61\n         doc_info_with_ids = [\n-            DocumentWithVSId(**doc.dict(), id=with_id)\n+            DocumentWithVSId(**{**doc.dict(), \"id\":with_id})\n             for with_id, doc in zip(doc_ids, doc_infos)\n         ]\n \ndiff --git a/libs/chatchat-server/chatchat/server/knowledge_base/utils.py b/libs/chatchat-server/chatchat/server/knowledge_base/utils.py\nindex f10e9cdf6b..f7b27276b1 100644\n--- a/libs/chatchat-server/chatchat/server/knowledge_base/utils.py\n+++ b/libs/chatchat-server/chatchat/server/knowledge_base/utils.py\n@@ -475,6 +475,7 @@ def format_reference(kb_name: str, docs: List[Dict], api_base_url: str=\"\") -> Li\n                 \"file_name\": filename,\n             }\n         )\n+        api_base_url = api_base_url.strip(\" /\")\n         url = (\n             f\"{api_base_url}/knowledge_base/download_doc?\" + parameters\n         )\ndiff --git a/libs/chatchat-server/chatchat/server/localai_embeddings.py b/libs/chatchat-server/chatchat/server/localai_embeddings.py\nindex e27041c211..559a4a4fd6 100644\n--- a/libs/chatchat-server/chatchat/server/localai_embeddings.py\n+++ b/libs/chatchat-server/chatchat/server/localai_embeddings.py\n@@ -156,7 +156,7 @@ class LocalAIEmbeddings(BaseModel, Embeddings):\n     disallowed_special: Union[Literal[\"all\"], Set[str], Sequence[str]] = \"all\"\n     chunk_size: int = 1000\n     \"\"\"Maximum number of texts to embed in each batch\"\"\"\n-    max_retries: int = 6\n+    max_retries: int = 3\n     \"\"\"Maximum number of retries to make when generating.\"\"\"\n     request_timeout: Union[float, Tuple[float, float], Any, None] = Field(\n         default=None, alias=\"timeout\"\ndiff --git a/libs/chatchat-server/chatchat/server/types/__init__.py b/libs/chatchat-server/chatchat/server/types/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/chatchat-server/chatchat/server/types/server/__init__.py b/libs/chatchat-server/chatchat/server/types/server/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/chatchat-server/chatchat/server/types/server/response/__init__.py b/libs/chatchat-server/chatchat/server/types/server/response/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/chatchat-server/chatchat/server/types/server/response/base.py b/libs/chatchat-server/chatchat/server/types/server/response/base.py\nnew file mode 100644\nindex 0000000000..126a8debb7\n--- /dev/null\n+++ b/libs/chatchat-server/chatchat/server/types/server/response/base.py\n@@ -0,0 +1,32 @@\n+from typing import Any, List, Optional, Union\n+\n+from pydantic import BaseModel, Field\n+\n+from chatchat.server.constant.response_code import ResponseCode\n+\n+\n+def default_data():\n+    return \"\"\n+\n+\n+class BaseResponse(BaseModel):\n+    code: int = Field(200, description=\"API status code\")\n+    msg: str = Field(\"success\", description=\"API status message\")\n+    data: Optional[Union[Any, None]] = Field(default_factory=default_data, description=\"API data\")\n+\n+    class Config:\n+        json_schema_extra = {\n+            \"example\": {\n+                \"code\": 200,\n+                \"msg\": \"success\",\n+                \"data\": None\n+            }\n+        }\n+\n+    @classmethod\n+    def success(cls, data: Optional[Any] = \"\", message: str = \"success\"):\n+        return BaseResponse(code=ResponseCode.SUCCESS, msg=message, data=data)\n+\n+    @classmethod\n+    def error(cls, data: Optional[Any] = \"\", message: str = \"error\", code=ResponseCode.INTERNAL_SERVER_ERROR):\n+        return BaseResponse(code=code, msg=message, data=data)\ndiff --git a/libs/chatchat-server/chatchat/server/utils.py b/libs/chatchat-server/chatchat/server/utils.py\nindex fc762ab0ff..d58d4a4c0e 100644\n--- a/libs/chatchat-server/chatchat/server/utils.py\n+++ b/libs/chatchat-server/chatchat/server/utils.py\n@@ -27,6 +27,7 @@\n from langchain_core.embeddings import Embeddings\n from langchain_openai.chat_models import ChatOpenAI\n from langchain_openai.llms import OpenAI\n+from memoization import cached, CachingAlgorithmFlag\n \n from chatchat.settings import Settings, XF_MODELS_TYPES\n from chatchat.server.pydantic_v2 import BaseModel, Field\n@@ -62,11 +63,58 @@ def get_config_platforms() -> Dict[str, Dict]:\n     return {m[\"platform_name\"]: m for m in platforms}\n \n \n+@cached(max_size=10, ttl=60, algorithm=CachingAlgorithmFlag.LRU)\n+def detect_xf_models(xf_url: str) -> Dict[str, List[str]]:\n+    '''\n+    use cache for xinference model detecting to avoid:\n+    - too many requests in short intervals\n+    - multiple requests to one platform for every model\n+    the cache will be invalidated after one minute\n+    '''\n+    xf_model_type_maps = {\n+        \"llm_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                        if \"LLM\" == v[\"model_type\"]\n+                                        and \"vision\" not in v[\"model_ability\"]],\n+        \"embed_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                        if \"embedding\" == v[\"model_type\"]],\n+        \"text2image_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                                if \"image\" == v[\"model_type\"]],\n+        \"image2image_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                                if \"image\" == v[\"model_type\"]],\n+        \"image2text_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                                if \"LLM\" == v[\"model_type\"]\n+                                                and \"vision\" in v[\"model_ability\"]],\n+        \"rerank_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                            if \"rerank\" == v[\"model_type\"]],\n+        \"speech2text_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                                if v.get(list(XF_MODELS_TYPES[\"speech2text\"].keys())[0])\n+                                                in XF_MODELS_TYPES[\"speech2text\"].values()],\n+        \"text2speech_models\": lambda xf_models: [k for k, v in xf_models.items()\n+                                                if v.get(list(XF_MODELS_TYPES[\"text2speech\"].keys())[0])\n+                                                in XF_MODELS_TYPES[\"text2speech\"].values()],\n+    }\n+    models = {}\n+    try:\n+        from xinference_client import RESTfulClient as Client\n+        xf_client = Client(xf_url)\n+        xf_models = xf_client.list_models()\n+        for m_type, filter in xf_model_type_maps.items():\n+            models[m_type] = filter(xf_models)\n+    except ImportError:\n+        logger.warning('auto_detect_model needs xinference-client installed. '\n+                        'Please try \"pip install xinference-client\". ')\n+    except requests.exceptions.ConnectionError:\n+        logger.warning(f\"cannot connect to xinference host: {xf_url}, please check your configuration.\")\n+    except Exception as e:\n+        logger.warning(f\"error when connect to xinference server({xf_url}): {e}\")\n+    return models\n+\n+\n def get_config_models(\n         model_name: str = None,\n-        model_type: Literal[\n+        model_type: Optional[Literal[\n             \"llm\", \"embed\", \"text2image\", \"image2image\", \"image2text\", \"rerank\", \"speech2text\", \"text2speech\"\n-        ] = None,\n+        ]] = None,\n         platform_name: str = None,\n ) -> Dict[str, Dict]:\n     \"\"\"\n@@ -96,29 +144,6 @@ def get_config_models(\n     else:\n         model_types = [f\"{model_type}_models\"]\n \n-    xf_model_type_maps = {\n-        \"llm_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                         if \"LLM\" == v[\"model_type\"]\n-                                         and \"vision\" not in v[\"model_ability\"]],\n-        \"embed_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                           if \"embedding\" == v[\"model_type\"]],\n-        \"text2image_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                                if \"image\" == v[\"model_type\"]],\n-        \"image2image_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                                 if \"image\" == v[\"model_type\"]],\n-        \"image2text_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                                if \"LLM\" == v[\"model_type\"]\n-                                                and \"vision\" in v[\"model_ability\"]],\n-        \"rerank_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                            if \"rerank\" == v[\"model_type\"]],\n-        \"speech2text_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                                 if v.get(list(XF_MODELS_TYPES[\"speech2text\"].keys())[0])\n-                                                 in XF_MODELS_TYPES[\"speech2text\"].values()],\n-        \"text2speech_models\": lambda xf_models: [k for k, v in xf_models.items()\n-                                                 if v.get(list(XF_MODELS_TYPES[\"text2speech\"].keys())[0])\n-                                                 in XF_MODELS_TYPES[\"text2speech\"].values()],\n-    }\n-\n     for m in list(get_config_platforms().values()):\n         if platform_name is not None and platform_name != m.get(\"platform_name\"):\n             continue\n@@ -127,22 +152,12 @@ def get_config_models(\n             if not m.get(\"platform_type\") == \"xinference\":  # TODO\uff1a\u5f53\u524d\u4ec5\u652f\u6301 xf \u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\n                 logger.warning(f\"auto_detect_model not supported for {m.get('platform_type')} yet\")\n                 continue\n+            xf_url = get_base_url(m.get(\"api_base_url\"))\n+            xf_models = detect_xf_models(xf_url)\n             for m_type in model_types:\n                 # if m.get(m_type) != \"auto\":\n                 #     continue\n-                try:\n-                    from xinference_client import RESTfulClient as Client\n-                    xf_url = get_base_url(m.get(\"api_base_url\"))\n-                    xf_client = Client(xf_url)\n-                    xf_models = xf_client.list_models()\n-                    m[m_type] = xf_model_type_maps[m_type](xf_models)\n-                except ImportError:\n-                    logger.warning('auto_detect_model needs xinference-client installed. '\n-                                   'Please try \"pip install xinference-client\". ')\n-                except requests.exceptions.ConnectionError:\n-                    logger.warning(f\"cannot connect to xinference host: {xf_url}\")\n-                except Exception as e:\n-                    logger.warning(f\"error when connect to xinference server({xf_url}): {e}\")\n+                m[m_type] = xf_models.get(m_type, [])\n \n         for m_type in model_types:\n             models = m.get(m_type, [])\n@@ -240,9 +255,7 @@ def get_ChatOpenAI(\n             )\n         model = ChatOpenAI(**params)\n     except Exception as e:\n-        logger.error(\n-            f\"failed to create ChatOpenAI for model: {model_name}.\", exc_info=True\n-        )\n+        logger.exception(f\"failed to create ChatOpenAI for model: {model_name}.\")\n         model = None\n     return model\n \n@@ -284,7 +297,7 @@ def get_OpenAI(\n             )\n         model = OpenAI(**params)\n     except Exception as e:\n-        logger.error(f\"failed to create OpenAI for model: {model_name}.\", exc_info=True)\n+        logger.exception(f\"failed to create OpenAI for model: {model_name}.\")\n         model = None\n     return model\n \n@@ -325,21 +338,22 @@ def get_Embeddings(\n         else:\n             return LocalAIEmbeddings(**params)\n     except Exception as e:\n-        logger.error(\n-            f\"failed to create Embeddings for model: {embed_model}.\", exc_info=True\n-        )\n+        logger.exception(f\"failed to create Embeddings for model: {embed_model}.\")\n \n \n-def check_embed_model(embed_model: str = get_default_embedding()) -> bool:\n+def check_embed_model(embed_model: str = None) -> Tuple[bool, str]:\n+    '''\n+    check weather embed_model accessable, use default embed model if None\n+    '''\n+    embed_model = embed_model or get_default_embedding()\n     embeddings = get_Embeddings(embed_model=embed_model)\n     try:\n         embeddings.embed_query(\"this is a test\")\n-        return True\n+        return True, \"\"\n     except Exception as e:\n-        logger.error(\n-            f\"failed to access embed model '{embed_model}': {e}\", exc_info=True\n-        )\n-        return False\n+        msg = f\"failed to access embed model '{embed_model}': {e}\"\n+        logger.error(msg)\n+        return False, msg\n \n \n def get_OpenAIClient(\n@@ -603,13 +617,22 @@ async def redoc_html(request: Request) -> HTMLResponse:\n #         return path_str  # THUDM/chatglm06b\n \n \n-def api_address() -> str:\n+def api_address(is_public: bool = False) -> str:\n+    '''\n+    \u5141\u8bb8\u7528\u6237\u5728 basic_settings.API_SERVER \u4e2d\u914d\u7f6e public_host, public_port\n+    \u4ee5\u4fbf\u4f7f\u7528\u4e91\u670d\u52a1\u5668\u6216\u53cd\u5411\u4ee3\u7406\u65f6\u751f\u6210\u6b63\u786e\u7684\u516c\u7f51 API \u5730\u5740\uff08\u5982\u77e5\u8bc6\u5e93\u6587\u6863\u4e0b\u8f7d\u94fe\u63a5\uff09\n+    '''\n     from chatchat.settings import Settings\n \n-    host = Settings.basic_settings.API_SERVER[\"host\"]\n-    if host == \"0.0.0.0\":\n-        host = \"127.0.0.1\"\n-    port = Settings.basic_settings.API_SERVER[\"port\"]\n+    server = Settings.basic_settings.API_SERVER\n+    if is_public:\n+        host = server.get(\"public_host\", \"127.0.0.1\")\n+        port = server.get(\"public_port\", \"7861\")\n+    else:\n+        host = server.get(\"host\", \"127.0.0.1\")\n+        port = server.get(\"port\", \"7861\")\n+        if host == \"0.0.0.0\":\n+            host = \"127.0.0.1\"\n     return f\"http://{host}:{port}\"\n \n \n@@ -707,7 +730,7 @@ def run_in_thread_pool(\n             try:\n                 yield obj.result()\n             except Exception as e:\n-                logger.error(f\"error in sub thread: {e}\", exc_info=True)\n+                logger.exception(f\"error in sub thread: {e}\")\n \n \n def run_in_process_pool(\n@@ -732,7 +755,7 @@ def run_in_process_pool(\n             try:\n                 yield obj.result()\n             except Exception as e:\n-                logger.error(f\"error in sub process: {e}\", exc_info=True)\n+                logger.exception(f\"error in sub process: {e}\")\n \n \n def get_httpx_client(\ndiff --git a/libs/chatchat-server/chatchat/settings.py b/libs/chatchat-server/chatchat/settings.py\nindex fa30bafaea..6bd7575ab6 100644\n--- a/libs/chatchat-server/chatchat/settings.py\n+++ b/libs/chatchat-server/chatchat/settings.py\n@@ -23,7 +23,8 @@\n class BasicSettings(BaseFileSettings):\n     \"\"\"\n     \u670d\u52a1\u5668\u57fa\u672c\u914d\u7f6e\u4fe1\u606f\n-    \u9664 log_verbose/HTTPX_DEFAULT_TIMEOUT \u4fee\u6539\u540e\u5373\u65f6\u751f\u6548\uff0c\u5176\u5b83\u914d\u7f6e\u9879\u4fee\u6539\u540e\u90fd\u9700\u8981\u91cd\u542f\u670d\u52a1\u5668\u624d\u80fd\u751f\u6548\n+    \u9664 log_verbose/HTTPX_DEFAULT_TIMEOUT \u4fee\u6539\u540e\u5373\u65f6\u751f\u6548\n+    \u5176\u5b83\u914d\u7f6e\u9879\u4fee\u6539\u540e\u90fd\u9700\u8981\u91cd\u542f\u670d\u52a1\u5668\u624d\u80fd\u751f\u6548\uff0c\u670d\u52a1\u8fd0\u884c\u671f\u95f4\u8bf7\u52ff\u4fee\u6539\n     \"\"\"\n \n     model_config = SettingsConfigDict(yaml_file=CHATCHAT_ROOT / \"basic_settings.yaml\")\n@@ -34,9 +35,6 @@ class BasicSettings(BaseFileSettings):\n     log_verbose: bool = False\n     \"\"\"\u662f\u5426\u5f00\u542f\u65e5\u5fd7\u8be6\u7ec6\u4fe1\u606f\"\"\"\n \n-    LOG_FORMAT: str = \"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\"\n-    \"\"\"\u65e5\u5fd7\u683c\u5f0f\"\"\"\n-\n     HTTPX_DEFAULT_TIMEOUT: float = 300\n     \"\"\"httpx \u8bf7\u6c42\u9ed8\u8ba4\u8d85\u65f6\u65f6\u95f4\uff08\u79d2\uff09\u3002\u5982\u679c\u52a0\u8f7d\u6a21\u578b\u6216\u5bf9\u8bdd\u8f83\u6162\uff0c\u51fa\u73b0\u8d85\u65f6\u9519\u8bef\uff0c\u53ef\u4ee5\u9002\u5f53\u52a0\u5927\u8be5\u503c\u3002\"\"\"\n \n@@ -101,14 +99,14 @@ def BASE_TEMP_DIR(self) -> Path:\n     OPEN_CROSS_DOMAIN: bool = False\n     \"\"\"API \u662f\u5426\u5f00\u542f\u8de8\u57df\"\"\"\n \n-    DEFAULT_BIND_HOST: str = \"127.0.0.1\" if sys.platform != \"win32\" else \"127.0.0.1\"\n+    DEFAULT_BIND_HOST: str = \"0.0.0.0\" if sys.platform != \"win32\" else \"127.0.0.1\"\n     \"\"\"\n     \u5404\u670d\u52a1\u5668\u9ed8\u8ba4\u7ed1\u5b9ahost\u3002\u5982\u6539\u4e3a\"0.0.0.0\"\u9700\u8981\u4fee\u6539\u4e0b\u65b9\u6240\u6709XX_SERVER\u7684host\n     Windows \u4e0b WEBUI \u81ea\u52a8\u5f39\u51fa\u6d4f\u89c8\u5668\u65f6\uff0c\u5982\u679c\u5730\u5740\u4e3a \"0.0.0.0\" \u662f\u65e0\u6cd5\u8bbf\u95ee\u7684\uff0c\u9700\u8981\u624b\u52a8\u4fee\u6539\u5730\u5740\u680f\n     \"\"\"\n \n-    API_SERVER: dict = {\"host\": DEFAULT_BIND_HOST, \"port\": 7861}\n-    \"\"\"API \u670d\u52a1\u5668\u5730\u5740\"\"\"\n+    API_SERVER: dict = {\"host\": DEFAULT_BIND_HOST, \"port\": 7861, \"public_host\": \"127.0.0.1\", \"public_port\": 7861}\n+    \"\"\"API \u670d\u52a1\u5668\u5730\u5740\u3002\u5176\u4e2d public_host \u7528\u4e8e\u751f\u6210\u4e91\u670d\u52a1\u516c\u7f51\u8bbf\u95ee\u94fe\u63a5\uff08\u5982\u77e5\u8bc6\u5e93\u6587\u6863\u94fe\u63a5\uff09\"\"\"\n \n     WEBUI_SERVER: dict = {\"host\": DEFAULT_BIND_HOST, \"port\": 8501}\n     \"\"\"WEBUI \u670d\u52a1\u5668\u5730\u5740\"\"\"\n@@ -275,18 +273,10 @@ class PlatformConfig(MyBaseModel):\n     auto_detect_model: bool = False\n     \"\"\"\u662f\u5426\u81ea\u52a8\u83b7\u53d6\u5e73\u53f0\u53ef\u7528\u6a21\u578b\u5217\u8868\u3002\u8bbe\u4e3a True \u65f6\u4e0b\u65b9\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u53ef\u81ea\u52a8\u68c0\u6d4b\"\"\"\n \n-    llm_models: t.Union[t.Literal[\"auto\"], t.List[str]] = [\n-        \"glm4-chat\",\n-        \"qwen1.5-chat\",\n-        \"qwen2-instruct\",\n-        \"gpt-3.5-turbo\",\n-        \"gpt-4o\",\n-    ]\n+    llm_models: t.Union[t.Literal[\"auto\"], t.List[str]] = []\n     \"\"\"\u8be5\u5e73\u53f0\u652f\u6301\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5217\u8868\uff0cauto_detect_model \u8bbe\u4e3a True \u65f6\u81ea\u52a8\u68c0\u6d4b\"\"\"\n \n-    embed_models: t.Union[t.Literal[\"auto\"], t.List[str]] = [\n-        \"bge-large-zh-v1.5\",\n-    ]\n+    embed_models: t.Union[t.Literal[\"auto\"], t.List[str]] = []\n     \"\"\"\u8be5\u5e73\u53f0\u652f\u6301\u7684\u5d4c\u5165\u6a21\u578b\u5217\u8868\uff0cauto_detect_model \u8bbe\u4e3a True \u65f6\u81ea\u52a8\u68c0\u6d4b\"\"\"\n \n     text2image_models: t.Union[t.Literal[\"auto\"], t.List[str]] = []\n@@ -385,7 +375,7 @@ class ApiModelSettings(BaseFileSettings):\n \n     MODEL_PLATFORMS: t.List[PlatformConfig] = [\n             PlatformConfig(**{\n-                \"platform_name\": \"xinference-auto\",\n+                \"platform_name\": \"xinference\",\n                 \"platform_type\": \"xinference\",\n                 \"api_base_url\": \"http://127.0.0.1:9997/v1\",\n                 \"api_key\": \"EMPTY\",\n@@ -399,26 +389,6 @@ class ApiModelSettings(BaseFileSettings):\n                 \"speech2text_models\": [],\n                 \"text2speech_models\": [],\n             }),\n-            PlatformConfig(**{\n-                \"platform_name\": \"xinference\",\n-                \"platform_type\": \"xinference\",\n-                \"api_base_url\": \"http://127.0.0.1:9997/v1\",\n-                \"api_key\": \"EMPTY\",\n-                \"api_concurrencies\": 5,\n-                \"llm_models\": [\n-                    \"glm4-chat\",\n-                    \"qwen1.5-chat\",\n-                    \"qwen2-instruct\",\n-                ],\n-                \"embed_models\": [\n-                    \"bge-large-zh-v1.5\",\n-                ],\n-                \"text2image_models\": [],\n-                \"image2text_models\": [],\n-                \"rerank_models\": [],\n-                \"speech2text_models\": [],\n-                \"text2speech_models\": [],\n-            }),\n             PlatformConfig(**{\n                 \"platform_name\": \"ollama\",\n                 \"platform_type\": \"ollama\",\n@@ -630,6 +600,14 @@ class ToolSettings(BaseFileSettings):\n     3\u3001\u5f53\u524d\u4ec5\u652f\u6301 \u5355prometheus \u67e5\u8be2, \u540e\u7eed\u8003\u8651\u652f\u6301 \u591aprometheus \u67e5\u8be2.\n     '''\n \n+    url_reader: dict = {\n+        \"use\": False,\n+        \"timeout\": \"10000\",\n+    }\n+    '''URL\u5185\u5bb9\u9605\u8bfb\uff08https://r.jina.ai/\uff09\u5de5\u5177\u914d\u7f6e\u9879\n+    \u8bf7\u786e\u4fdd\u90e8\u7f72\u7684\u7f51\u7edc\u73af\u5883\u826f\u597d\uff0c\u4ee5\u514d\u9020\u6210\u8d85\u65f6\u7b49\u95ee\u9898'''\n+\n+\n \n class PromptSettings(BaseFileSettings):\n     \"\"\"Prompt \u6a21\u677f.\u9664 Agent \u6a21\u677f\u4f7f\u7528 f-string \u5916\uff0c\u5176\u5b83\u5747\u4f7f\u7528 jinja2 \u683c\u5f0f\"\"\"\ndiff --git a/libs/chatchat-server/chatchat/utils.py b/libs/chatchat-server/chatchat/utils.py\nindex d349487291..7821c5d66a 100644\n--- a/libs/chatchat-server/chatchat/utils.py\n+++ b/libs/chatchat-server/chatchat/utils.py\n@@ -10,6 +10,16 @@\n from chatchat.settings import Settings\n \n \n+def _filter_logs(record: dict) -> bool:\n+    # hide debug logs if Settings.basic_settings.log_verbose=False \n+    if record[\"level\"].no <= 10 and not Settings.basic_settings.log_verbose:\n+        return False\n+    # hide traceback logs if Settings.basic_settings.log_verbose=False \n+    if record[\"level\"].no == 40 and not Settings.basic_settings.log_verbose:\n+        record[\"exception\"] = None\n+    return True\n+\n+\n # \u9ed8\u8ba4\u6bcf\u8c03\u7528\u4e00\u6b21 build_logger \u5c31\u4f1a\u6dfb\u52a0\u4e00\u6b21 hanlder\uff0c\u5bfc\u81f4 chatchat.log \u91cc\u91cd\u590d\u8f93\u51fa\n @cached(max_size=100, algorithm=CachingAlgorithmFlag.LRU)\n def build_logger(log_file: str = \"chatchat\"):\n@@ -20,27 +30,19 @@ def build_logger(log_file: str = \"chatchat\"):\n     logger.info(\"<green>some message</green>\")\n \n     user can set basic_settings.log_verbose=True to output debug logs\n+    use logger.exception to log errors with exceptions\n     \"\"\"\n+    loguru.logger._core.handlers[0]._filter = _filter_logs\n     logger = loguru.logger.opt(colors=True)\n     logger.opt = partial(loguru.logger.opt, colors=True)\n+    # logger.error = partial(logger.exception)\n \n     if log_file:\n         if not log_file.endswith(\".log\"):\n             log_file = f\"{log_file}.log\"\n         if not os.path.isabs(log_file):\n             log_file = str((Settings.basic_settings.LOG_PATH / log_file).resolve())\n-        logger.add(log_file, colorize=False)\n-\n-    logger.error = logger.opt(exception=True).error\n-\n-    _debug = logger.debug\n-\n-    def debug(*args, **kwds):\n-        if (Settings.basic_settings.log_verbose\n-                and _debug is not debug):\n-            _debug(*args, **kwds)\n-\n-    logger.debug = debug\n+        logger.add(log_file, colorize=False, filter=_filter_logs)\n \n     return logger\n \ndiff --git a/libs/chatchat-server/chatchat/webui_pages/dialogue/dialogue.py b/libs/chatchat-server/chatchat/webui_pages/dialogue/dialogue.py\nindex 2f22a0957a..75da7c3b0b 100644\n--- a/libs/chatchat-server/chatchat/webui_pages/dialogue/dialogue.py\n+++ b/libs/chatchat-server/chatchat/webui_pages/dialogue/dialogue.py\n@@ -19,8 +19,8 @@\n from chatchat.settings import Settings\n from chatchat.server.callback_handler.agent_callback_handler import AgentStatus\n from chatchat.server.knowledge_base.model.kb_document_model import DocumentWithVSId\n+from chatchat.server.knowledge_base.utils import format_reference\n from chatchat.server.utils import MsgType, get_config_models, get_config_platforms, get_default_llm\n-from chatchat.webui_pages.dialogue.utils import process_files\n from chatchat.webui_pages.utils import *\n \n \n@@ -206,6 +206,8 @@ def rename_conversation():\n             use_agent = st.checkbox(\n                 \"\u542f\u7528Agent\", help=\"\u8bf7\u786e\u4fdd\u9009\u62e9\u7684\u6a21\u578b\u5177\u5907Agent\u80fd\u529b\", key=\"use_agent\"\n             )\n+            output_agent = st.checkbox(\"\u663e\u793a Agent \u8fc7\u7a0b\", key=\"output_agent\")\n+\n             # \u9009\u62e9\u5de5\u5177\n             tools = list_tools(api)\n             tool_names = [\"None\"] + list(tools)\n@@ -422,119 +424,114 @@ def on_conv_change():\n             upload_image=upload_image,\n         )\n         stream = not is_vision_chat\n+        params = dict(\n+            messages=messages,\n+            model=llm_model,\n+            stream=stream, # TODO\uff1axinference qwen-vl-chat \u6d41\u5f0f\u8f93\u51fa\u4f1a\u51fa\u9519\uff0c\u540e\u7eed\u770b\u66f4\u65b0\n+            extra_body=extra_body,\n+        )\n+        if tools:\n+            params[\"tools\"] = tools\n+        if tool_choice:\n+            params[\"tool_choice\"] = tool_choice\n+        if Settings.model_settings.MAX_TOKENS:\n+            params[\"max_tokens\"] = Settings.model_settings.MAX_TOKENS\n+\n         if stream:\n-            for d in client.chat.completions.create(\n-                messages=messages,\n-                model=llm_model,\n-                stream=stream, # TODO\uff1axinference qwen-vl-chat \u6d41\u5f0f\u8f93\u51fa\u4f1a\u51fa\u9519\uff0c\u540e\u7eed\u770b\u66f4\u65b0\n-                tools=tools or openai.NOT_GIVEN,\n-                tool_choice=tool_choice,\n-                extra_body=extra_body,\n-                max_tokens=Settings.model_settings.MAX_TOKENS,\n-            ):\n-                # import rich\n-                # rich.print(d)\n-                message_id = d.message_id\n-                metadata = {\n-                    \"message_id\": message_id,\n-                }\n-\n-                # clear initial message\n-                if not started:\n-                    chat_box.update_msg(\"\", streaming=False)\n-                    started = True\n-\n-                if d.status == AgentStatus.error:\n-                    st.error(d.choices[0].delta.content)\n-                elif d.status == AgentStatus.llm_start:\n-                    chat_box.insert_msg(\"\u6b63\u5728\u89e3\u8bfb\u5de5\u5177\u8f93\u51fa\u7ed3\u679c...\")\n-                    text = d.choices[0].delta.content or \"\"\n-                elif d.status == AgentStatus.llm_new_token:\n-                    text += d.choices[0].delta.content or \"\"\n-                    chat_box.update_msg(\n-                        text.replace(\"\\n\", \"\\n\\n\"), streaming=True, metadata=metadata\n-                    )\n-                elif d.status == AgentStatus.llm_end:\n-                    text += d.choices[0].delta.content or \"\"\n-                    chat_box.update_msg(\n-                        text.replace(\"\\n\", \"\\n\\n\"), streaming=False, metadata=metadata\n-                    )\n-                # tool \u7684\u8f93\u51fa\u4e0e llm \u8f93\u51fa\u91cd\u590d\u4e86\n-                # elif d.status == AgentStatus.tool_start:\n-                #     formatted_data = {\n-                #         \"Function\": d.choices[0].delta.tool_calls[0].function.name,\n-                #         \"function_input\": d.choices[0].delta.tool_calls[0].function.arguments,\n-                #     }\n-                #     formatted_json = json.dumps(formatted_data, indent=2, ensure_ascii=False)\n-                #     text = \"\"\"\\n```{}\\n```\\n\"\"\".format(formatted_json)\n-                #     chat_box.insert_msg( # TODO: insert text directly not shown\n-                #         Markdown(text, title=\"Function call\", in_expander=True, expanded=True, state=\"running\"))\n-                # elif d.status == AgentStatus.tool_end:\n-                #     tool_output = d.choices[0].delta.tool_calls[0].tool_output\n-                #     if d.message_type == MsgType.IMAGE:\n-                #         for url in json.loads(tool_output).get(\"images\", []):\n-                #             url = f\"{api.base_url}/media/{url}\"\n-                #             chat_box.insert_msg(Image(url))\n-                #         chat_box.update_msg(expanded=False, state=\"complete\")\n-                #     else:\n-                #         text += \"\"\"\\n```\\nObservation:\\n{}\\n```\\n\"\"\".format(tool_output)\n-                #         chat_box.update_msg(text, streaming=False, expanded=False, state=\"complete\")\n-                elif d.status == AgentStatus.agent_finish:\n-                    text = d.choices[0].delta.content or \"\"\n-                    chat_box.update_msg(text.replace(\"\\n\", \"\\n\\n\"))\n-                elif d.status is None:  # not agent chat\n-                    if getattr(d, \"is_ref\", False):\n-                        context = str(d.tool_output)\n-                        if isinstance(d.tool_output, dict):\n-                            docs = d.tool_output.get(\"docs\", [])\n-                            source_documents = []\n-                            for inum, doc in enumerate(docs):\n-                                doc = DocumentWithVSId.parse_obj(doc)\n-                                filename = doc.metadata.get(\"source\")\n-                                parameters = urlencode(\n-                                    {\n-                                        \"knowledge_base_name\": d.tool_output.get(\n-                                            \"knowledge_base\"\n-                                        ),\n-                                        \"file_name\": filename,\n-                                    }\n-                                )\n-                                url = (\n-                                    f\"{api.base_url}/knowledge_base/download_doc?\" + parameters\n-                                )\n-                                ref = f\"\"\"\u51fa\u5904 [{inum + 1}] [{filename}]({url}) \\n\\n{doc.page_content}\\n\\n\"\"\"\n-                                source_documents.append(ref)\n-                            context = \"\\n\".join(source_documents)\n-\n-                        chat_box.insert_msg(\n-                            Markdown(\n-                                context,\n-                                in_expander=True,\n-                                state=\"complete\",\n-                                title=\"\u53c2\u8003\u8d44\u6599\",\n-                            )\n-                        )\n-                        chat_box.insert_msg(\"\")\n-                    elif getattr(d, \"tool_call\", None) == \"text2images\":  # TODO\uff1a\u7279\u5b9a\u5de5\u5177\u7279\u522b\u5904\u7406\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u5904\u7406\u65b9\u5f0f\n-                        for img in d.tool_output.get(\"images\", []):\n-                            chat_box.insert_msg(Image(f\"{api.base_url}/media/{img}\"), pos=-2)\n-                    else:\n+            try:\n+                for d in client.chat.completions.create(**params):\n+                    # import rich\n+                    # rich.print(d)\n+                    message_id = d.message_id\n+                    metadata = {\n+                        \"message_id\": message_id,\n+                    }\n+\n+                    # clear initial message\n+                    if not started:\n+                        chat_box.update_msg(\"\", streaming=False)\n+                        started = True\n+\n+                    if d.status == AgentStatus.error:\n+                        st.error(d.choices[0].delta.content)\n+                    elif d.status == AgentStatus.llm_start:\n+                        if not output_agent:\n+                            continue\n+                        chat_box.insert_msg(\"\u6b63\u5728\u89e3\u8bfb\u5de5\u5177\u8f93\u51fa\u7ed3\u679c...\")\n+                        text = d.choices[0].delta.content or \"\"\n+                    elif d.status == AgentStatus.llm_new_token:\n+                        if not output_agent:\n+                            continue\n                         text += d.choices[0].delta.content or \"\"\n                         chat_box.update_msg(\n                             text.replace(\"\\n\", \"\\n\\n\"), streaming=True, metadata=metadata\n                         )\n-            chat_box.update_msg(text, streaming=False, metadata=metadata)\n+                    elif d.status == AgentStatus.llm_end:\n+                        if not output_agent:\n+                            continue\n+                        text += d.choices[0].delta.content or \"\"\n+                        chat_box.update_msg(\n+                            text.replace(\"\\n\", \"\\n\\n\"), streaming=False, metadata=metadata\n+                        )\n+                    # tool \u7684\u8f93\u51fa\u4e0e llm \u8f93\u51fa\u91cd\u590d\u4e86\n+                    # elif d.status == AgentStatus.tool_start:\n+                    #     formatted_data = {\n+                    #         \"Function\": d.choices[0].delta.tool_calls[0].function.name,\n+                    #         \"function_input\": d.choices[0].delta.tool_calls[0].function.arguments,\n+                    #     }\n+                    #     formatted_json = json.dumps(formatted_data, indent=2, ensure_ascii=False)\n+                    #     text = \"\"\"\\n```{}\\n```\\n\"\"\".format(formatted_json)\n+                    #     chat_box.insert_msg( # TODO: insert text directly not shown\n+                    #         Markdown(text, title=\"Function call\", in_expander=True, expanded=True, state=\"running\"))\n+                    # elif d.status == AgentStatus.tool_end:\n+                    #     tool_output = d.choices[0].delta.tool_calls[0].tool_output\n+                    #     if d.message_type == MsgType.IMAGE:\n+                    #         for url in json.loads(tool_output).get(\"images\", []):\n+                    #             url = f\"{api.base_url}/media/{url}\"\n+                    #             chat_box.insert_msg(Image(url))\n+                    #         chat_box.update_msg(expanded=False, state=\"complete\")\n+                    #     else:\n+                    #         text += \"\"\"\\n```\\nObservation:\\n{}\\n```\\n\"\"\".format(tool_output)\n+                    #         chat_box.update_msg(text, streaming=False, expanded=False, state=\"complete\")\n+                    elif d.status == AgentStatus.agent_finish:\n+                        text = d.choices[0].delta.content or \"\"\n+                        chat_box.update_msg(text.replace(\"\\n\", \"\\n\\n\"))\n+                    elif d.status is None:  # not agent chat\n+                        if getattr(d, \"is_ref\", False):\n+                            context = str(d.tool_output)\n+                            if isinstance(d.tool_output, dict):\n+                                docs = d.tool_output.get(\"docs\", [])\n+                                source_documents = format_reference(kb_name=d.tool_output.get(\"knowledge_base\"),\n+                                                                    docs=docs,\n+                                                                    api_base_url=api_address(is_public=True))\n+                                context = \"\\n\".join(source_documents)\n+\n+                            chat_box.insert_msg(\n+                                Markdown(\n+                                    context,\n+                                    in_expander=True,\n+                                    state=\"complete\",\n+                                    title=\"\u53c2\u8003\u8d44\u6599\",\n+                                )\n+                            )\n+                            chat_box.insert_msg(\"\")\n+                        elif getattr(d, \"tool_call\", None) == \"text2images\":  # TODO\uff1a\u7279\u5b9a\u5de5\u5177\u7279\u522b\u5904\u7406\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u5904\u7406\u65b9\u5f0f\n+                            for img in d.tool_output.get(\"images\", []):\n+                                chat_box.insert_msg(Image(f\"{api.base_url}/media/{img}\"), pos=-2)\n+                        else:\n+                            text += d.choices[0].delta.content or \"\"\n+                            chat_box.update_msg(\n+                                text.replace(\"\\n\", \"\\n\\n\"), streaming=True, metadata=metadata\n+                            )\n+                    chat_box.update_msg(text, streaming=False, metadata=metadata)\n+            except Exception as e:\n+                st.error(e.body)\n         else:\n-            d =client.chat.completions.create(\n-                messages=messages,\n-                model=llm_model,\n-                stream=stream, # TODO\uff1axinference qwen-vl-chat \u6d41\u5f0f\u8f93\u51fa\u4f1a\u51fa\u9519\uff0c\u540e\u7eed\u770b\u66f4\u65b0\n-                tools=tools or openai.NOT_GIVEN,\n-                tool_choice=tool_choice,\n-                extra_body=extra_body,\n-                max_tokens=Settings.model_settings.MAX_TOKENS,\n-            )\n-            chat_box.update_msg(d.choices[0].message.content or \"\", streaming=False)\n+            try:\n+                d =client.chat.completions.create(**params)\n+                chat_box.update_msg(d.choices[0].message.content or \"\", streaming=False)\n+            except Exception as e:\n+                st.error(e.body)\n \n         # if os.path.exists(\"tmp/image.jpg\"):\n         #     with open(\"tmp/image.jpg\", \"rb\") as image_file:\ndiff --git a/libs/chatchat-server/chatchat/webui_pages/kb_chat.py b/libs/chatchat-server/chatchat/webui_pages/kb_chat.py\nindex 2c54447e7a..bcadb82778 100644\n--- a/libs/chatchat-server/chatchat/webui_pages/kb_chat.py\n+++ b/libs/chatchat-server/chatchat/webui_pages/kb_chat.py\n@@ -10,7 +10,7 @@\n \n from chatchat.settings import Settings\n from chatchat.server.knowledge_base.utils import LOADER_DICT\n-from chatchat.server.utils import get_config_models, get_config_platforms, get_default_llm\n+from chatchat.server.utils import get_config_models, get_config_platforms, get_default_llm, api_address\n from chatchat.webui_pages.dialogue.dialogue import (save_session, restore_session, rerun,\n                                                     get_messages_history, upload_temp_docs,\n                                                     add_conv, del_conv, clear_conv)\n@@ -194,8 +194,9 @@ def on_conv_change():\n             return_direct=return_direct,\n         )\n     \n+        api_url = api_address(is_public=True)\n         if dialogue_mode == \"\u77e5\u8bc6\u5e93\u95ee\u7b54\":\n-            client = openai.Client(base_url=f\"{api_address()}/knowledge_base/local_kb/{selected_kb}\", api_key=\"NONE\")\n+            client = openai.Client(base_url=f\"{api_url}/knowledge_base/local_kb/{selected_kb}\", api_key=\"NONE\")\n             chat_box.ai_say([\n                 Markdown(\"...\", in_expander=True, title=\"\u77e5\u8bc6\u5e93\u5339\u914d\u7ed3\u679c\", state=\"running\", expanded=return_direct),\n                 f\"\u6b63\u5728\u67e5\u8be2\u77e5\u8bc6\u5e93 `{selected_kb}` ...\",\n@@ -205,13 +206,13 @@ def on_conv_change():\n                 st.error(\"\u8bf7\u5148\u4e0a\u4f20\u6587\u4ef6\u518d\u8fdb\u884c\u5bf9\u8bdd\")\n                 st.stop()\n             knowledge_id=st.session_state.get(\"file_chat_id\")\n-            client = openai.Client(base_url=f\"{api_address()}/knowledge_base/temp_kb/{knowledge_id}\", api_key=\"NONE\")\n+            client = openai.Client(base_url=f\"{api_url}/knowledge_base/temp_kb/{knowledge_id}\", api_key=\"NONE\")\n             chat_box.ai_say([\n                 Markdown(\"...\", in_expander=True, title=\"\u77e5\u8bc6\u5e93\u5339\u914d\u7ed3\u679c\", state=\"running\", expanded=return_direct),\n                 f\"\u6b63\u5728\u67e5\u8be2\u6587\u4ef6 `{st.session_state.get('file_chat_id')}` ...\",\n             ])\n         else:\n-            client = openai.Client(base_url=f\"{api_address()}/knowledge_base/search_engine/{search_engine}\", api_key=\"NONE\")\n+            client = openai.Client(base_url=f\"{api_url}/knowledge_base/search_engine/{search_engine}\", api_key=\"NONE\")\n             chat_box.ai_say([\n                 Markdown(\"...\", in_expander=True, title=\"\u77e5\u8bc6\u5e93\u5339\u914d\u7ed3\u679c\", state=\"running\", expanded=return_direct),\n                 f\"\u6b63\u5728\u6267\u884c `{search_engine}` \u641c\u7d22...\",\n@@ -220,16 +221,19 @@ def on_conv_change():\n         text = \"\"\n         first = True\n \n-        for d in client.chat.completions.create(messages=messages, model=llm_model, stream=True, extra_body=extra_body):\n-            if first:\n-                chat_box.update_msg(\"\\n\\n\".join(d.docs), element_index=0, streaming=False, state=\"complete\")\n-                chat_box.update_msg(\"\", streaming=False)\n-                first = False\n-                continue\n-            text += d.choices[0].delta.content or \"\"\n-            chat_box.update_msg(text.replace(\"\\n\", \"\\n\\n\"), streaming=True)\n-        chat_box.update_msg(text, streaming=False)\n-        # TODO: \u641c\u7d22\u672a\u914d\u7f6eAPI KEY\u65f6\u4ea7\u751f\u62a5\u9519\n+        try:\n+            for d in client.chat.completions.create(messages=messages, model=llm_model, stream=True, extra_body=extra_body):\n+                if first:\n+                    chat_box.update_msg(\"\\n\\n\".join(d.docs), element_index=0, streaming=False, state=\"complete\")\n+                    chat_box.update_msg(\"\", streaming=False)\n+                    first = False\n+                    continue\n+                text += d.choices[0].delta.content or \"\"\n+                chat_box.update_msg(text.replace(\"\\n\", \"\\n\\n\"), streaming=True)\n+            chat_box.update_msg(text, streaming=False)\n+            # TODO: \u641c\u7d22\u672a\u914d\u7f6eAPI KEY\u65f6\u4ea7\u751f\u62a5\u9519\n+        except Exception as e:\n+            st.error(e.body)\n \n     now = datetime.now()\n     with tabs[1]:\ndiff --git a/libs/chatchat-server/pyproject.toml b/libs/chatchat-server/pyproject.toml\nindex 09d712dbd7..9974bed5d0 100644\n--- a/libs/chatchat-server/pyproject.toml\n+++ b/libs/chatchat-server/pyproject.toml\n@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"langchain-chatchat\"\n-version = \"0.3.1.1\"\n+version = \"0.3.1.2\"\n description = \"Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM, Qwen and Llama) RAG and Agent app with langchain\"\n authors = [\"chatchat\"]\n readme = \"README.md\"\ndiff --git a/libs/python-sdk/open_chatcaht/__init__.py b/libs/python-sdk/open_chatcaht/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/_constants.py b/libs/python-sdk/open_chatcaht/_constants.py\nnew file mode 100644\nindex 0000000000..53121b7663\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/_constants.py\n@@ -0,0 +1,38 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+from typing import Literal\n+\n+import httpx\n+\n+RAW_RESPONSE_HEADER = \"X-Stainless-Raw-Response\"\n+OVERRIDE_CAST_TO_HEADER = \"____stainless_override_cast_to\"\n+\n+# default timeout is 10 minutes\n+DEFAULT_TIMEOUT = httpx.Timeout(timeout=600.0, connect=5.0)\n+DEFAULT_MAX_RETRIES = 2\n+DEFAULT_CONNECTION_LIMITS = httpx.Limits(max_connections=1000, max_keepalive_connections=100)\n+\n+INITIAL_RETRY_DELAY = 0.5\n+MAX_RETRY_DELAY = 8.0\n+\n+EMBEDDING_MODEL: str = \"bge-large-zh-v1.5\"\n+HTTPX_TIMEOUT: float = 10.0\n+API_BASE_URI: str = 'http://127.0.0.1:7861/'\n+\n+# \u77e5\u8bc6\u5e93\u76f8\u5173\n+\"\"\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u957f\u5ea6(\u4e0d\u9002\u7528MarkdownHeaderTextSplitter)\"\"\"\n+CHUNK_SIZE: int = 250\n+\"\"\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6(\u4e0d\u9002\u7528MarkdownHeaderTextSplitter)\"\"\"\n+OVERLAP_SIZE: int = 50\n+\"\"\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\uff0c\u4ee5\u53ca\u6807\u9898\u589e\u5f3a\u7684\u76f8\u5173\u914d\u7f6e\"\"\"\n+ZH_TITLE_ENHANCE: bool = False\n+\"\"\"\u77e5\u8bc6\u5e93\u5339\u914d\u5411\u91cf\u6570\u91cf\"\"\"\n+VECTOR_SEARCH_TOP_K: int = 3  # TODO: \u4e0e tool \u914d\u7f6e\u9879\u91cd\u590d\n+\"\"\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-2\u4e4b\u95f4\uff0cSCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\u53d6\u52302\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\"\"\"\n+SCORE_THRESHOLD: float = 0.4\n+\"\"\"\u9ed8\u8ba4\u5411\u91cf\u5e93/\u5168\u6587\u68c0\u7d22\u5f15\u64ce\u7c7b\u578b\"\"\"\n+VS_TYPE: Literal[\"faiss\", \"milvus\", \"zilliz\", \"pg\", \"es\", \"relyt\", \"chromadb\"] = \"faiss\"\n+# llm\n+TEMPERATURE: float = 0.7\n+LLM_MODEL = \"chatglm-6b\"\n+MAX_TOKENS = 2048\n+\ndiff --git a/libs/python-sdk/open_chatcaht/_version.py b/libs/python-sdk/open_chatcaht/_version.py\nnew file mode 100644\nindex 0000000000..fe34e93d76\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/_version.py\n@@ -0,0 +1,3 @@\n+\n+__title__ = \"open_chatcaht\"\n+__version__ = \"1.35.13\"\ndiff --git a/libs/python-sdk/open_chatcaht/api/__init__.py b/libs/python-sdk/open_chatcaht/api/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/api/chat/__init__.py b/libs/python-sdk/open_chatcaht/api/chat/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/api/chat/chat_client.py b/libs/python-sdk/open_chatcaht/api/chat/chat_client.py\nnew file mode 100644\nindex 0000000000..4dc8aee9db\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/api/chat/chat_client.py\n@@ -0,0 +1,87 @@\n+from typing import Optional, List, Literal, Union\n+\n+from pydantic import Field\n+\n+from open_chatcaht._constants import MAX_TOKENS, LLM_MODEL, TEMPERATURE, SCORE_THRESHOLD, VECTOR_SEARCH_TOP_K\n+from open_chatcaht.api_client import ApiClient\n+from open_chatcaht.types.chat.chat_feedback_param import ChatFeedbackParam\n+from open_chatcaht.types.chat.chat_message import ChatMessage\n+from open_chatcaht.types.chat.file_chat_param import FileChatParam\n+from open_chatcaht.types.chat.kb_chat_param import KbChatParam\n+\n+API_URI_CHAT_FEEDBACK = \"/chat/feedback\"\n+API_URI_FILE_CHAT = \"/chat/file_chat\"\n+API_URI_KB_CHAT = \"/chat/kb_chat\"\n+\n+\n+class ChatClient(ApiClient):\n+\n+    def chat_feedback(self,\n+                      message_id: str,\n+                      score: int = 100,\n+                      reason: str = \"\"):\n+        data = ChatFeedbackParam(\n+            message_id=message_id,\n+            score=score,\n+            reason=reason,\n+        ).dict()\n+        resp = self._post(API_URI_CHAT_FEEDBACK, json=data)\n+        return self._get_response_value(resp, as_json=True)\n+\n+    def kb_chat(self,\n+                query: str,\n+                mode: Literal[\"local_kb\", \"temp_kb\", \"search_engine\"] = \"local_kb\",\n+                kb_name: str = \"\",\n+                top_k: int = VECTOR_SEARCH_TOP_K,\n+                score_threshold: float = SCORE_THRESHOLD,\n+                history: List[Union[ChatMessage, dict]] = [],\n+                stream: bool = True,\n+                model: str = LLM_MODEL,\n+                temperature: float = TEMPERATURE,\n+                max_tokens: Optional[int] = MAX_TOKENS,\n+                prompt_name: str = \"default\",\n+                return_direct: bool = False,\n+                ):\n+        kb_chat_param = KbChatParam(\n+            query=query,\n+            mode=mode,\n+            kb_name=kb_name,\n+            top_k=top_k,\n+            score_threshold=score_threshold,\n+            history=history,\n+            stream=stream,\n+            model=model,\n+            temperature=temperature,\n+            max_tokens=max_tokens,\n+            prompt_name=prompt_name,\n+            return_direct=return_direct,\n+        ).dict()\n+        response = self._post(API_URI_KB_CHAT, json=kb_chat_param, stream=True)\n+        return self._httpx_stream2generator(response, as_json=True)\n+\n+    def file_chat(self,\n+                  query: str,\n+                  knowledge_id: str,\n+                  top_k: int = VECTOR_SEARCH_TOP_K,\n+                  score_threshold: float = SCORE_THRESHOLD,\n+                  history: List[Union[dict, ChatMessage]] = [],\n+                  stream: bool = True,\n+                  model_name: str = LLM_MODEL,\n+                  temperature: float = 0.01,\n+                  max_tokens: Optional[int] = MAX_TOKENS,\n+                  prompt_name: str = \"default\",\n+                  ):\n+        file_chat_param = FileChatParam(\n+            query=query,\n+            knowledge_id=knowledge_id,\n+            top_k=top_k,\n+            score_threshold=score_threshold,\n+            history=history,\n+            stream=stream,\n+            model_name=model_name,\n+            temperature=temperature,\n+            max_tokens=max_tokens,\n+            prompt_name=prompt_name,\n+        ).dict()\n+        response = self._post(API_URI_FILE_CHAT, json=file_chat_param, stream=True)\n+        return self._httpx_stream2generator(response, as_json=True)\ndiff --git a/libs/python-sdk/open_chatcaht/api/knowledge_base/__init__.py b/libs/python-sdk/open_chatcaht/api/knowledge_base/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/api/knowledge_base/knowledge_base_client.py b/libs/python-sdk/open_chatcaht/api/knowledge_base/knowledge_base_client.py\nnew file mode 100644\nindex 0000000000..074413ad75\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/api/knowledge_base/knowledge_base_client.py\n@@ -0,0 +1,321 @@\n+from pydantic import Field\n+\n+from open_chatcaht._constants import EMBEDDING_MODEL, VS_TYPE, VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, CHUNK_SIZE, \\\n+    OVERLAP_SIZE, ZH_TITLE_ENHANCE, LLM_MODEL\n+from open_chatcaht.api_client import ApiClient, post\n+from open_chatcaht.types.knowledge_base.create_knowledge_base_param import CreateKnowledgeBaseParam\n+import json\n+import os\n+from io import BytesIO\n+from pathlib import Path\n+from typing import *\n+\n+from open_chatcaht.types.knowledge_base.delete_knowledge_base_param import DeleteKnowledgeBaseParam\n+from open_chatcaht.types.knowledge_base.doc.delete_kb_docs_param import DeleteKbDocsParam\n+from open_chatcaht.types.knowledge_base.doc.download_kb_doc_param import DownloadKbDocParam\n+from open_chatcaht.types.knowledge_base.doc.search_kb_docs_param import SearchKbDocsParam\n+from open_chatcaht.types.knowledge_base.doc.search_temp_docs_param import SearchTempDocsParam\n+from open_chatcaht.types.knowledge_base.doc.upload_kb_docs_param import UploadKbDocsParam\n+from open_chatcaht.types.knowledge_base.doc.upload_temp_docs_param import UploadTempDocsParam\n+from open_chatcaht.types.knowledge_base.recreate_vector_store_param import RecreateVectorStoreParam\n+from open_chatcaht.types.knowledge_base.summary.recreate_summary_vector_store_param import \\\n+    RecreateSummaryVectorStoreParam\n+from open_chatcaht.types.knowledge_base.summary.summary_doc_ids_to_vector_store_param import \\\n+    SummaryDocIdsToVectorStoreParam\n+from open_chatcaht.types.knowledge_base.summary.summary_file_to_vector_store_param import SummaryFileToVectorStoreParam\n+from open_chatcaht.types.knowledge_base.update_kb_info_param import UpdateKbInfoParam\n+from open_chatcaht.types.response.base import BaseResponse\n+from open_chatcaht.utils import convert_file\n+\n+API_URI_CREATE_KB = \"/knowledge_base/create_knowledge_base\"\n+API_URI_DELETE_KB = \"/knowledge_base/delete_knowledge_base\"\n+API_URI_KB_UPDATE_INFO = \"/knowledge_base/update_info\"\n+API_URI_LIST_KB = \"/knowledge_base/list_knowledge_bases\"\n+\n+API_URI_URI_LIST_KB_FILE = \"/knowledge_base/list_files\"\n+API_URI_SEARCH_KB_DOCS = \"/knowledge_base/search_docs\"\n+\n+API_URI_KB_UPLOAD_DOCS = \"/knowledge_base/upload_docs\"\n+API_URI_KB_DOWNLOAD_DOC = \"/knowledge_base/download_doc\"\n+API_URI_DELETE_KB_DOCS = \"/knowledge_base/delete_docs\"\n+API_URI_KB_RECREATE_VECTOR_STORE = \"/knowledge_base/recreate_vector_store\"\n+API_URI_KB_SEARCH_TEMP_DOCS = \"/knowledge_base/search_temp_docs\"\n+API_URI_KB_UPLOAD_TEMP_DOCS = \"/knowledge_base/upload_temp_docs\"\n+\n+API_URI_KB_SUMMARY_FILE_TO_VECTOR_STORE = \"/knowledge_base/kb_summary_api/summary_file_to_vector_store\"\n+API_URI_KB_SUMMARY_DOC_IDS_TO_VECTOR_STORE = \"/knowledge_base/kb_summary_api/summary_doc_ids_to_vector_store\"\n+API_URI_KB_SUMMARY_RECREATE_VECTOR_STORE = \"/knowledge_base/kb_summary_api/recreate_summary_vector_store\"\n+\n+\n+class KbClient(ApiClient):\n+\n+    @post(url=API_URI_CREATE_KB\n+        , body_model=CreateKnowledgeBaseParam)\n+    def create_kb(\n+            self,\n+            knowledge_base_name: str,\n+            kb_info: str = \"\",\n+            vector_store_type: str = VS_TYPE,\n+            embed_model: str = EMBEDDING_MODEL,\n+    ) -> BaseResponse:\n+        ...\n+\n+    # def create_knowledge_base(\n+    #         self,\n+    #         knowledge_base_name: str,\n+    #         kb_info: str = \"\",\n+    #         vector_store_type: str = VS_TYPE,\n+    #         embed_model: str = EMBEDDING_MODEL,\n+    # ):\n+    #     data = CreateKnowledgeBaseParam(\n+    #         knowledge_base_name=knowledge_base_name,\n+    #         kb_info=kb_info,\n+    #         vector_store_type=vector_store_type,\n+    #         embed_model=embed_model,\n+    #     ).dict()\n+    #     response = self.post(API_URI_CREATE_KB, json=data)\n+    #     return self._get_response_value(response, as_json=True)\n+\n+    def delete_kb(\n+            self,\n+            knowledge_base_name: str,\n+    ):\n+        response = self._post(API_URI_DELETE_KB, json=knowledge_base_name)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def list_kb(self):\n+        response = self._get(API_URI_LIST_KB)\n+        return self._get_response_value(response, as_json=True, value_func=lambda r: r.get(\"data\", []))\n+\n+    def list_kb_docs_file(\n+            self,\n+            knowledge_base_name: str,\n+    ):\n+        params = DeleteKnowledgeBaseParam(knowledge_base_name=knowledge_base_name).dict()\n+        response = self._get(API_URI_URI_LIST_KB_FILE, params=params)\n+        return self._get_response_value(response, as_json=True, value_func=lambda r: r.get(\"data\", []))\n+\n+    def search_kb_docs(\n+            self,\n+            knowledge_base_name: str,\n+            query: str = \"\",\n+            top_k: int = VECTOR_SEARCH_TOP_K,\n+            score_threshold: float = SCORE_THRESHOLD,\n+            file_name: str = \"\",\n+            metadata: dict = {},\n+    ) -> List:\n+        data = SearchKbDocsParam(\n+            query=query,\n+            knowledge_base_name=knowledge_base_name,\n+            top_k=top_k,\n+            score_threshold=score_threshold,\n+            file_name=file_name,\n+            metadata=metadata,\n+        ).dict()\n+        response = self._post(API_URI_SEARCH_KB_DOCS, json=data)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def upload_kb_docs(\n+            self,\n+            files: List[Union[str, Path, bytes]],\n+            knowledge_base_name: str,\n+            override: bool = False,\n+            to_vector_store: bool = True,\n+            chunk_size=CHUNK_SIZE,\n+            chunk_overlap=OVERLAP_SIZE,\n+            zh_title_enhance=ZH_TITLE_ENHANCE,\n+            docs: Dict = {},\n+            not_refresh_vs_cache: bool = False,\n+    ):\n+        files = [convert_file(file) for file in files]\n+        data = UploadKbDocsParam(\n+            knowledge_base_name=knowledge_base_name,\n+            override=override,\n+            to_vector_store=to_vector_store,\n+            chunk_size=chunk_size,\n+            chunk_overlap=chunk_overlap,\n+            zh_title_enhance=zh_title_enhance,\n+            docs=json.dumps(docs, ensure_ascii=False),\n+            not_refresh_vs_cache=not_refresh_vs_cache,\n+        ).dict()\n+        response = self._post(API_URI_KB_UPLOAD_DOCS, data=data,\n+                              files=[(\"files\", (filename, file)) for filename, file in files])\n+        return self._get_response_value(response, as_json=True)\n+\n+    def delete_kb_docs(\n+            self,\n+            knowledge_base_name: str,\n+            file_names: List[str],\n+            delete_content: bool = False,\n+            not_refresh_vs_cache: bool = False,\n+    ):\n+        data = DeleteKbDocsParam(\n+            knowledge_base_name=knowledge_base_name,\n+            file_names=file_names,\n+            delete_content=delete_content,\n+            not_refresh_vs_cache=not_refresh_vs_cache,\n+        ).dict()\n+        response = self._post(API_URI_DELETE_KB_DOCS, json=data)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def update_kb_info(self, knowledge_base_name, kb_info):\n+        data = UpdateKbInfoParam(\n+            knowledge_base_name=knowledge_base_name,\n+            kb_info=kb_info,\n+        ).dict()\n+        response = self._post(API_URI_KB_UPDATE_INFO, json=data)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def recreate_vector_store(\n+            self,\n+            knowledge_base_name: str,\n+            allow_empty_kb: bool = True,\n+            vs_type: str = VS_TYPE,\n+            embed_model: str = EMBEDDING_MODEL,\n+            chunk_size=CHUNK_SIZE,\n+            chunk_overlap=OVERLAP_SIZE,\n+            zh_title_enhance=ZH_TITLE_ENHANCE,\n+    ):\n+        data = RecreateVectorStoreParam(\n+            knowledge_base_name=knowledge_base_name,\n+            allow_empty_kb=allow_empty_kb,\n+            vs_type=vs_type,\n+            embed_model=embed_model,\n+            chunk_size=chunk_size,\n+            chunk_overlap=chunk_overlap,\n+            zh_title_enhance=zh_title_enhance,\n+        ).dict()\n+        response = self._post(API_URI_KB_RECREATE_VECTOR_STORE, json=data, stream=True, timeout=None)\n+        return self._httpx_stream2generator(response, as_json=True)\n+\n+    # def recreate_summary_vector_store(self,\n+    #                                   knowledge_base_name: str,\n+    #                                   allow_empty_kb: bool = True,\n+    #                                   vs_type: str = VS_TYPE,\n+    #                                   embed_model: str = EMBEDDING_MODEL,\n+    #                                   file_description: str = \"\",\n+    #                                   model_name: str = None,\n+    #                                   temperature: float = 0.01,\n+    #                                   max_tokens: Optional[int] = None):\n+    #     data = RecreateSummaryVectorStoreParam(\n+    #         knowledge_base_name=knowledge_base_name,\n+    #         allow_empty_kb=allow_empty_kb,\n+    #         vs_type=vs_type,\n+    #         embed_model=embed_model,\n+    #         file_description=file_description,\n+    #         model_name=model_name,\n+    #         temperature=temperature,\n+    #         max_tokens=max_tokens).dict()\n+    #     response = self._post(API_URI_KB_SUMMARY_RECREATE_VECTOR_STORE, json=data)\n+    #     return self._get_response_value(response, as_json=True)\n+    #\n+    # def summary_doc_ids_to_vector_store(self,\n+    #                                     knowledge_base_name: str,\n+    #                                     doc_ids: List = [],\n+    #                                     vs_type: str = VS_TYPE,\n+    #                                     embed_model: str = EMBEDDING_MODEL,\n+    #                                     file_description: str = \"\",\n+    #                                     model_name: str = None,\n+    #                                     temperature: float = 0.01,\n+    #                                     max_tokens: Optional[int] = None,\n+    #                                     ):\n+    #     data = SummaryDocIdsToVectorStoreParam(\n+    #         knowledge_base_name=knowledge_base_name,\n+    #         doc_ids=doc_ids,\n+    #         vs_type=vs_type,\n+    #         embed_model=embed_model,\n+    #         file_description=file_description,\n+    #         model_name=model_name,\n+    #         temperature=temperature,\n+    #         max_tokens=max_tokens,\n+    #     ).dict()\n+    #     response = self._post(API_URI_KB_SUMMARY_DOC_IDS_TO_VECTOR_STORE, json=data)\n+    #     return self._get_response_value(response, as_json=True)\n+    #\n+    # def summary_file_to_vector_store(self, knowledge_base_name: str,\n+    #                                  file_name: str,\n+    #                                  allow_empty_kb: bool = True,\n+    #                                  vs_type: str = VS_TYPE,\n+    #                                  embed_model: str = EMBEDDING_MODEL,\n+    #                                  file_description: str = \"\",\n+    #                                  model_name: str = LLM_MODEL,\n+    #                                  temperature: float = 0.01,\n+    #                                  max_tokens: Optional[int] = 1000):\n+    #     data = SummaryFileToVectorStoreParam(\n+    #         knowledge_base_name=knowledge_base_name,\n+    #         file_name=file_name,\n+    #         allow_empty_kb=allow_empty_kb,\n+    #         vs_type=vs_type,\n+    #         embed_model=embed_model,\n+    #         file_description=file_description,\n+    #         model_name=model_name,\n+    #         temperature=temperature,\n+    #         max_tokens=max_tokens,\n+    #     ).dict()\n+    #     response = self._post(API_URI_KB_SUMMARY_FILE_TO_VECTOR_STORE, json=data,stream=True)\n+    #     return self._httpx_stream2generator(response, as_json=True)\n+\n+    def upload_temp_docs(self,\n+                         files: List[Union[str, Path, bytes]],\n+                         knowledge_id: str = None,\n+                         chunk_size: int = CHUNK_SIZE,\n+                         chunk_overlap: int = OVERLAP_SIZE,\n+                         zh_title_enhance: bool = ZH_TITLE_ENHANCE,\n+                         ):\n+        data = UploadTempDocsParam(\n+            prev_id=knowledge_id,\n+            chunk_size=chunk_size,\n+            chunk_overlap=chunk_overlap,\n+            zh_title_enhance=zh_title_enhance\n+        ).dict()\n+        _files = [convert_file(file) for file in files]\n+        response = self._post(\n+            \"/knowledge_base/upload_temp_docs\",\n+            data=data,\n+            files=[(\"files\", (filename, file)) for filename, file in _files],\n+        )\n+        return self._get_response_value(response, as_json=True)\n+        # _files = [convert_file(file) for file in files]\n+        # response = self._post(API_URI_KB_UPLOAD_TEMP_DOCS, data=data,\n+        #                       files=[(\"files\", (filename, file)) for filename, file in _files])\n+        # return self._get_response_value(response, as_json=True)\n+\n+    def search_temp_kb_docs(\n+            self,\n+            knowledge_id: str,\n+            query: str,\n+            top_k: int = VECTOR_SEARCH_TOP_K,\n+            score_threshold: float = SCORE_THRESHOLD,\n+    ) -> List:\n+        data = SearchTempDocsParam(\n+            knowledge_id=knowledge_id,\n+            query=query,\n+            top_k=top_k,\n+            score_threshold=score_threshold,\n+        ).dict()\n+        response = self._post(API_URI_KB_SEARCH_TEMP_DOCS, json=data)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def download_kb_doc_file(self, knowledge_base_name: str, file_name: str, file_path: Optional[str] = None):\n+        params = DownloadKbDocParam(\n+            knowledge_base_name=knowledge_base_name,\n+            file_name=file_name,\n+            preview=False\n+        ).dict()\n+        response = self._get(API_URI_KB_DOWNLOAD_DOC, params=params)\n+        file_content = self._get_response_value(response, as_json=False, value_func=lambda r: r.content)\n+        if file_path is None:\n+            file_path = file_name\n+        with open(file_path, 'wb') as file:\n+            file.write(file_content)\n+        return file_path\n+\n+    def kb_doc_file_content(self, knowledge_base_name: str, file_name: str):\n+        params = DownloadKbDocParam(\n+            knowledge_base_name=knowledge_base_name,\n+            file_name=file_name,\n+            preview=True\n+        ).dict()\n+        response = self._get(API_URI_KB_DOWNLOAD_DOC, params=params)\n+        file_content = self._get_response_value(response, as_json=False, value_func=lambda r: r.content)\n+        return file_content.decode('utf-8')\ndiff --git a/libs/python-sdk/open_chatcaht/api/server/__init__.py b/libs/python-sdk/open_chatcaht/api/server/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/api/server/server_client.py b/libs/python-sdk/open_chatcaht/api/server/server_client.py\nnew file mode 100644\nindex 0000000000..f07cf13f9e\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/api/server/server_client.py\n@@ -0,0 +1,23 @@\n+from open_chatcaht.api_client import ApiClient\n+\n+API_URI_GET_SERVER_CONFIGS = \"/server/configs\"\n+API_URI_GET_PROMPT_TEMPLATE = \"/server/get_prompt_template\"\n+\n+\n+class ServerClient(ApiClient):\n+    # \u670d\u52a1\u5668\u4fe1\u606f\n+    def get_server_configs(self) -> dict:\n+        response = self._post(API_URI_GET_SERVER_CONFIGS)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def get_prompt_template(\n+            self,\n+            _type: str = \"knowledge_base_chat\",\n+            name: str = \"default\",\n+    ) -> str:\n+        data = {\n+            \"type\": _type,  # \u6a21\u677f\u7c7b\u578b\n+            \"name\": name  # \u6a21\u677f\u540d\u79f0\n+        }\n+        response = self._post(API_URI_GET_PROMPT_TEMPLATE, json=data)\n+        return self._get_response_value(response, value_func=lambda r: r.text)\ndiff --git a/libs/python-sdk/open_chatcaht/api/standard_openai/__init__.py b/libs/python-sdk/open_chatcaht/api/standard_openai/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/api/standard_openai/standard_openai_client.py b/libs/python-sdk/open_chatcaht/api/standard_openai/standard_openai_client.py\nnew file mode 100644\nindex 0000000000..6f80f4243c\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/api/standard_openai/standard_openai_client.py\n@@ -0,0 +1,116 @@\n+from typing import Dict, List\n+\n+from open_chatcaht.api_client import ApiClient, get\n+from open_chatcaht.types.standard_openai.audio_speech_input import OpenAIAudioSpeechInput\n+from open_chatcaht.types.standard_openai.audio_transcriptions_input import OpenAIAudioTranscriptionsInput\n+from open_chatcaht.types.standard_openai.audio_translations_input import OpenAIAudioTranslationsInput\n+from open_chatcaht.types.standard_openai.chat_input import OpenAIChatInput\n+from open_chatcaht.types.standard_openai.embeddings_Input import OpenAIEmbeddingsInput\n+from open_chatcaht.types.standard_openai.image_edits_input import OpenAIImageEditsInput\n+from open_chatcaht.types.standard_openai.image_generations_input import OpenAIImageGenerationsInput\n+from open_chatcaht.types.standard_openai.image_variations_input import OpenAIImageVariationsInput\n+\n+API_UTI_STANDARD_OPENAI_LIST_MODELS = \"/v1/models\"\n+API_UTI_STANDARD_OPENAI_CHAT_COMPLETIONS = \"/v1/chat/completions\"\n+API_UTI_STANDARD_OPENAI_COMPLETIONS = \"/v1/chat/completions\"\n+API_UTI_STANDARD_OPENAI_EMBEDDINGS = \"/v1/embeddings\"\n+\n+API_UTI_STANDARD_OPENAI_IMAGE_GENERATIONS = \"/v1//images/generations\"\n+API_UTI_STANDARD_OPENAI_IMAGE_VARIATIONS = \"/v1//images/variations\"\n+API_UTI_STANDARD_OPENAI_IMAGE_EDIT = \"/v1//images/edit\"\n+\n+API_UTI_STANDARD_OPENAI_AUDIO_TRANSLATIONS = \"/v1//audio/translations\"\n+API_UTI_STANDARD_OPENAI_AUDIO_TRANSCRIPTIONS = \"/v1//audio/transcriptions\"\n+API_UTI_STANDARD_OPENAI_AUDIO_SPEECH = \"/v1/audio/speech\"\n+\n+API_UTI_STANDARD_OPENAI_FILES = \"/v1/files\"\n+API_UTI_STANDARD_OPENAI_LIST_FILES = \"/v1/list_files\"\n+API_UTI_STANDARD_OPENAI_RETRIEVE_FILE = \"/v1//files/{file_id}\"\n+API_UTI_STANDARD_OPENAI_RETRIEVE_FILE_CONTENT = \"/v1//files/{file_id}/content\"\n+API_UTI_STANDARD_OPENAI_DELETE_FILE = \"/v1//files/{file_id}\"\n+\n+\n+class StandardOpenaiClient(ApiClient):\n+\n+    def list_models(self) -> dict:\n+        response = self._get(API_UTI_STANDARD_OPENAI_LIST_MODELS)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def chat_completions(self, chat_input: OpenAIChatInput) -> dict:\n+        response = self._post(API_UTI_STANDARD_OPENAI_CHAT_COMPLETIONS, json=chat_input.dict(), stream=True)\n+        return self._httpx_stream2generator(response, as_json=True)\n+\n+    def completions(self, chat_input: OpenAIChatInput) -> dict:\n+        response = self._post(API_UTI_STANDARD_OPENAI_COMPLETIONS, json=chat_input.dict(), stream=True)\n+        return self._httpx_stream2generator(response, as_json=True)\n+\n+    def embeddings(self, embeddings_input: OpenAIEmbeddingsInput):\n+        response = self._post(API_UTI_STANDARD_OPENAI_EMBEDDINGS, json=embeddings_input.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    def image_generations(\n+            self,\n+            data: OpenAIImageGenerationsInput,\n+    ):\n+        response = self._post(API_UTI_STANDARD_OPENAI_IMAGE_GENERATIONS, json=data.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    def image_variations(\n+            self,\n+            data: OpenAIImageVariationsInput,\n+    ):\n+        response = self._post(API_UTI_STANDARD_OPENAI_IMAGE_VARIATIONS, json=data.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    def image_edit(\n+            self,\n+            data: OpenAIImageEditsInput,\n+    ):\n+        response = self._post(API_UTI_STANDARD_OPENAI_IMAGE_EDIT, json=data.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    def audio_translations(\n+            self,\n+            data: OpenAIAudioTranslationsInput,\n+    ):\n+        response = self._post(API_UTI_STANDARD_OPENAI_AUDIO_TRANSLATIONS, json=data.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    def audio_transcriptions(\n+            self,\n+            data: OpenAIAudioTranscriptionsInput,\n+    ):\n+        response = self._post(API_UTI_STANDARD_OPENAI_AUDIO_TRANSCRIPTIONS, json=data.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    def audio_speech(\n+            self,\n+            data: OpenAIAudioSpeechInput,\n+    ):\n+        response = self._post(API_UTI_STANDARD_OPENAI_AUDIO_SPEECH, json=data.dict())\n+        return self._get_response_value(response, as_json=True)\n+\n+    # todo \u5f85\u5b8c\u6210\n+    async def files(\n+            self,\n+            file: str,\n+            purpose: str = \"assistants\",\n+    ) -> Dict:\n+        response = self._post(API_UTI_STANDARD_OPENAI_FILES)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def list_files(self, purpose: str) -> Dict[str, List[Dict]]:\n+        response = self._get(API_UTI_STANDARD_OPENAI_LIST_FILES)\n+        return self._get_response_value(response, as_json=True)\n+\n+    def retrieve_file(self, file_id: str) -> Dict:\n+        response = self._get(API_UTI_STANDARD_OPENAI_RETRIEVE_FILE.format(file_id=file_id))\n+        return self._get_response_value(response, as_json=True)\n+\n+    def retrieve_file_content(self, file_id: str) -> Dict:\n+        response = self._get(API_UTI_STANDARD_OPENAI_RETRIEVE_FILE_CONTENT.format(file_id=file_id))\n+        return self._get_response_value(response, as_json=True)\n+\n+    def delete_file(self, file_id: str) -> Dict:\n+        response = self._delete(API_UTI_STANDARD_OPENAI_DELETE_FILE.format(file_id=file_id))\n+        return self._get_response_value(response, as_json=True)\ndiff --git a/libs/python-sdk/open_chatcaht/api/tools/__init__.py b/libs/python-sdk/open_chatcaht/api/tools/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/api/tools/tool_client.py b/libs/python-sdk/open_chatcaht/api/tools/tool_client.py\nnew file mode 100644\nindex 0000000000..8ee0c55854\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/api/tools/tool_client.py\n@@ -0,0 +1,26 @@\n+from open_chatcaht.api_client import ApiClient\n+from open_chatcaht.types.tools.call_tool_param import CallToolParam\n+\n+API_URI_TOOL_CALL = \"/tools/call\"\n+API_URI_TOOL_LIST = \"/tools\"\n+\n+\n+class ToolClient(ApiClient):\n+    def list(self) -> dict:\n+        \"\"\"\n+        \u5217\u51fa\u6240\u6709\u5de5\u5177\n+        \"\"\"\n+        resp = self._get(API_URI_TOOL_LIST)\n+        return self._get_response_value(resp, as_json=True, value_func=lambda r: r.get(\"data\", {}))\n+\n+    def call(\n+            self,\n+            name: str,\n+            tool_input: dict = {},\n+    ):\n+        \"\"\"\n+        \u8c03\u7528\u5de5\u5177\n+        \"\"\"\n+        data = CallToolParam(name=name, tool_input=tool_input).dict()\n+        resp = self._post(API_URI_TOOL_CALL, json=data)\n+        return self._get_response_value(resp, as_json=True, value_func=lambda r: r.get(\"data\"))\ndiff --git a/libs/python-sdk/open_chatcaht/api_client.py b/libs/python-sdk/open_chatcaht/api_client.py\nnew file mode 100644\nindex 0000000000..895736bb26\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/api_client.py\n@@ -0,0 +1,322 @@\n+import contextlib\n+import inspect\n+import json\n+import logging\n+import os\n+from typing import *\n+\n+from open_chatcaht._constants import API_BASE_URI\n+from open_chatcaht.utils import set_httpx_config, get_httpx_client, get_variable, get_function_default_params, \\\n+    merge_dicts\n+from functools import wraps\n+from typing import Type, get_type_hints\n+\n+import httpx\n+import requests\n+from pydantic import BaseModel\n+\n+set_httpx_config()\n+\n+CHATCHAT_API_BASE = get_variable(os.getenv('CHATCHAT_API_BASE'), 'http://127.0.0.1:8000')\n+CHATCHAT_CLIENT_TIME_OUT = get_variable(os.getenv('CHATCHAT_CLIENT_TIME_OUT'), 60)\n+CHATCHAT_CLIENT_DEFAULT_RETRY_COUNT = get_variable(os.getenv('CHATCHAT_CLIENT_DEFAULT_RETRY'), 3)\n+CHATCHAT_CLIENT_DEFAULT_RETRY_INTERVAL = get_variable(os.getenv('CHATCHAT_CLIENT_DEFAULT_RETRY_INTERVAL'), 60)\n+\n+\n+class ApiClient:\n+    \"\"\"\n+    api.py\u8c03\u7528\u7684\u5c01\u88c5\uff08\u540c\u6b65\u6a21\u5f0f\uff09,\u7b80\u5316api\u8c03\u7528\u65b9\u5f0f\n+    \"\"\"\n+\n+    def __init__(\n+            self,\n+            base_url: str = API_BASE_URI,\n+            timeout: float = 60,\n+            use_async: bool = False,\n+            use_proxy: bool = False,\n+            proxies=None,\n+            log_level: int = logging.INFO,\n+            retry: int = 3,\n+            retry_interval: int = 1,\n+    ):\n+        if proxies is None:\n+            proxies = {}\n+        self.base_url = get_variable(base_url, CHATCHAT_API_BASE)\n+        self.timeout = get_variable(timeout, CHATCHAT_CLIENT_TIME_OUT)\n+        self._use_async = use_async\n+        self.use_proxy = use_proxy\n+        self.default_retry_count = get_variable(retry, CHATCHAT_CLIENT_DEFAULT_RETRY_COUNT)\n+        self.default_retry_interval = get_variable(retry_interval, CHATCHAT_CLIENT_DEFAULT_RETRY_INTERVAL)\n+        self.proxies = proxies\n+        self._client = None\n+        self.logger = logging.getLogger(__name__)\n+        self.logger.setLevel(log_level)\n+\n+    @property\n+    def client(self):\n+        if self._client is None or self._client.is_closed:\n+            self._client = get_httpx_client(\n+                base_url=self.base_url, use_async=self._use_async, timeout=self.timeout\n+            )\n+        return self._client\n+\n+    def _get(\n+            self,\n+            url: str,\n+            params: Union[Dict, List[Tuple], bytes] = None,\n+            retry: int = 3,\n+            stream: bool = False,\n+            **kwargs: Any,\n+    ) -> Union[httpx.Response, Iterator[httpx.Response], None]:\n+        while retry > 0:\n+            try:\n+                if stream:\n+                    return self.client.stream(\"GET\", url, params=params, **kwargs)\n+                else:\n+                    return self.client.get(url, params=params, **kwargs)\n+            except Exception as e:\n+                msg = f\"error when get {url}: {e}\"\n+                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+                retry -= 1\n+\n+    def _post(\n+            self,\n+            url: str,\n+            data: Dict = None,\n+            json: Dict = None,\n+            retry: int = 3,\n+            stream: bool = False,\n+            **kwargs: Any,\n+    ) -> Union[httpx.Response, Iterator[httpx.Response], None]:\n+        while retry > 0:\n+            try:\n+                # print(kwargs)\n+                if stream:\n+\n+                    return self.client.stream(\n+                        \"POST\", url, data=data, json=json, **kwargs\n+                    )\n+                else:\n+                    self.logger.debug(f\"post {url} with data: {data}\")\n+                    return self.client.post(url, data=data, json=json, **kwargs)\n+            except Exception as e:\n+                msg = f\"error when post {url}: {e}\"\n+                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+                retry -= 1\n+\n+    def _delete(\n+            self,\n+            url: str,\n+            data: Dict = None,\n+            json: Dict = None,\n+            retry: int = 3,\n+            stream: bool = False,\n+            **kwargs: Any,\n+    ) -> Union[httpx.Response, Iterator[httpx.Response], None]:\n+        while retry > 0:\n+            try:\n+                if stream:\n+                    return self.client.stream(\n+                        \"DELETE\", url, data=data, json=json, **kwargs\n+                    )\n+                else:\n+                    return self.client.delete(url, data=data, json=json, **kwargs)\n+            except Exception as e:\n+                msg = f\"error when delete {url}: {e}\"\n+                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+                retry -= 1\n+\n+    def _httpx_stream2generator(\n+            self,\n+            response: contextlib._GeneratorContextManager,\n+            as_json: bool = False,\n+    ):\n+        \"\"\"\n+        \u5c06httpx.stream\u8fd4\u56de\u7684GeneratorContextManager\u8f6c\u5316\u4e3a\u666e\u901a\u751f\u6210\u5668\n+        \"\"\"\n+\n+        async def ret_async(response, as_json):\n+            try:\n+                async with response as r:\n+                    chunk_cache = \"\"\n+                    async for chunk in r.aiter_text(None):\n+                        if not chunk:  # fastchat api yield empty bytes on start and end\n+                            continue\n+                        if as_json:\n+                            try:\n+                                if chunk.startswith(\"data: \"):\n+                                    data = json.loads(chunk_cache + chunk[6:-2])\n+                                elif chunk.startswith(\":\"):  # skip sse comment line\n+                                    continue\n+                                else:\n+                                    data = json.loads(chunk_cache + chunk)\n+\n+                                chunk_cache = \"\"\n+                                yield data\n+                            except Exception as e:\n+                                msg = f\"\u63a5\u53e3\u8fd4\u56dejson\u9519\u8bef\uff1a \u2018{chunk}\u2019\u3002\u9519\u8bef\u4fe1\u606f\u662f\uff1a{e}\u3002\"\n+                                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+\n+                                if chunk.startswith(\"data: \"):\n+                                    chunk_cache += chunk[6:-2]\n+                                elif chunk.startswith(\":\"):  # skip sse comment line\n+                                    continue\n+                                else:\n+                                    chunk_cache += chunk\n+                                continue\n+                        else:\n+                            # print(chunk, end=\"\", flush=True)\n+                            yield chunk\n+            except httpx.ConnectError as e:\n+                msg = f\"\u65e0\u6cd5\u8fde\u63a5API\u670d\u52a1\u5668\uff0c\u8bf7\u786e\u8ba4 \u2018api.py\u2019 \u5df2\u6b63\u5e38\u542f\u52a8\u3002({e})\"\n+                self.logger.error(msg)\n+                yield {\"code\": 500, \"msg\": msg}\n+            except httpx.ReadTimeout as e:\n+                msg = f\"API\u901a\u4fe1\u8d85\u65f6\uff0c\u8bf7\u786e\u8ba4\u5df2\u542f\u52a8FastChat\u4e0eAPI\u670d\u52a1\uff08\u8be6\u89c1Wiki '5. \u542f\u52a8 API \u670d\u52a1\u6216 Web UI'\uff09\u3002\uff08{e}\uff09\"\n+                self.logger.error(msg)\n+                yield {\"code\": 500, \"msg\": msg}\n+            except Exception as e:\n+                msg = f\"API\u901a\u4fe1\u9047\u5230\u9519\u8bef\uff1a{e}\"\n+                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+                yield {\"code\": 500, \"msg\": msg}\n+\n+        def ret_sync(response, as_json):\n+            try:\n+                with response as r:\n+                    chunk_cache = \"\"\n+                    for chunk in r.iter_text(None):\n+                        if not chunk:  # fastchat api yield empty bytes on start and end\n+                            continue\n+                        if as_json:\n+                            try:\n+                                if chunk.startswith(\"data: \"):\n+                                    data = json.loads(chunk_cache + chunk[6:-2])\n+                                elif chunk.startswith(\":\"):  # skip sse comment line\n+                                    continue\n+                                else:\n+                                    data = json.loads(chunk_cache + chunk)\n+\n+                                chunk_cache = \"\"\n+                                yield data\n+                            except Exception as e:\n+                                msg = f\"\u63a5\u53e3\u8fd4\u56dejson\u9519\u8bef\uff1a \u2018{chunk}\u2019\u3002\u9519\u8bef\u4fe1\u606f\u662f\uff1a{e}\u3002\"\n+                                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+\n+                                if chunk.startswith(\"data: \"):\n+                                    chunk_cache += chunk[6:-2]\n+                                elif chunk.startswith(\":\"):  # skip sse comment line\n+                                    continue\n+                                else:\n+                                    chunk_cache += chunk\n+                                continue\n+                        else:\n+                            # print(chunk, end=\"\", flush=True)\n+                            yield chunk\n+            except httpx.ConnectError as e:\n+                msg = f\"\u65e0\u6cd5\u8fde\u63a5API\u670d\u52a1\u5668\uff0c\u8bf7\u786e\u8ba4 \u2018api.py\u2019 \u5df2\u6b63\u5e38\u542f\u52a8\u3002({e})\"\n+                self.logger.error(msg)\n+                yield {\"code\": 500, \"msg\": msg}\n+            except httpx.ReadTimeout as e:\n+                msg = f\"API\u901a\u4fe1\u8d85\u65f6\uff0c\u8bf7\u786e\u8ba4\u5df2\u542f\u52a8FastChat\u4e0eAPI\u670d\u52a1\uff08\u8be6\u89c1Wiki '5. \u542f\u52a8 API \u670d\u52a1\u6216 Web UI'\uff09\u3002\uff08{e}\uff09\"\n+                self.logger.error(msg)\n+                yield {\"code\": 500, \"msg\": msg}\n+            except Exception as e:\n+                msg = f\"API\u901a\u4fe1\u9047\u5230\u9519\u8bef\uff1a{e}\"\n+                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+                yield {\"code\": 500, \"msg\": msg}\n+\n+        if self._use_async:\n+            return ret_async(response, as_json)\n+        else:\n+            return ret_sync(response, as_json)\n+\n+    def _get_response_value(\n+            self,\n+            response: httpx.Response,\n+            as_json: bool = False,\n+            value_func: Callable = None,\n+    ):\n+        \"\"\"\n+        \u8f6c\u6362\u540c\u6b65\u6216\u5f02\u6b65\u8bf7\u6c42\u8fd4\u56de\u7684\u54cd\u5e94\n+        `as_json`: \u8fd4\u56dejson\n+        `value_func`: \u7528\u6237\u53ef\u4ee5\u81ea\u5b9a\u4e49\u8fd4\u56de\u503c\uff0c\u8be5\u51fd\u6570\u63a5\u53d7response\u6216json\n+        \"\"\"\n+\n+        def to_json(r):\n+            try:\n+                return r.json()\n+            except Exception as e:\n+                msg = \"API\u672a\u80fd\u8fd4\u56de\u6b63\u786e\u7684JSON\u3002\" + str(e)\n+                self.logger.error(f\"{e.__class__.__name__}: {msg}\")\n+                return {\"code\": 500, \"msg\": msg, \"data\": None}\n+\n+        if value_func is None:\n+            value_func = lambda r: r\n+\n+        async def ret_async(response):\n+            if as_json:\n+                return value_func(to_json(await response))\n+            else:\n+                return value_func(await response)\n+\n+        if self._use_async:\n+            return ret_async(response)\n+        else:\n+            if as_json:\n+                return value_func(to_json(response))\n+            else:\n+                return value_func(response)\n+\n+\n+def get_request_method(api_client_obj: ApiClient, method):\n+    if method is httpx.post:\n+        return getattr(api_client_obj, \"_post\")\n+    elif method is httpx.get:\n+        return getattr(api_client_obj, \"_get\")\n+    # elif method is httpx.put:\n+    #     return api_client_obj.put\n+    elif method is httpx.delete:\n+        return getattr(api_client_obj, \"_delete\")\n+\n+\n+def http_request(method):\n+    def decorator(url, base_url='', headers=None, body_model: Type[BaseModel] = None, **options):\n+        headers = headers or {}\n+\n+        def wrapper(func):\n+            @wraps(func)\n+            def inner(*args, **kwargs):\n+                try:\n+                    default_param: dict = get_function_default_params(func)\n+\n+                    api_client_obj: ApiClient = args[0] if len(args) > 0 and isinstance(args[0], ApiClient) else None\n+                    return_type = get_type_hints(func).get('return')\n+                    full_url = base_url + url\n+                    param = merge_dicts(kwargs, default_param)\n+                    if body_model is not None:\n+                        param = body_model(**kwargs).dict()\n+                    # Send the HTTP request\n+                    response = None\n+                    if api_client_obj is not None:\n+                        _method = get_request_method(api_client_obj, method)\n+                        response = _method(full_url, headers=headers, json=param)\n+                    else:\n+                        response = method(full_url, headers=headers, json=param)\n+                        response.raise_for_status()\n+                    return response.json()\n+                except requests.exceptions.HTTPError as http_err:\n+                    print(f\"HTTP error occurred: {http_err}\")\n+                except Exception as err:\n+                    print(f\"An error occurred: {err}\")\n+\n+            return inner\n+\n+        return wrapper\n+\n+    return decorator\n+\n+\n+post = http_request(httpx.post)\n+get = http_request(httpx.get)\n+delete = http_request(httpx.delete)\n+put = http_request(httpx.put)\ndiff --git a/libs/python-sdk/open_chatcaht/chatchat_api.py b/libs/python-sdk/open_chatcaht/chatchat_api.py\nnew file mode 100644\nindex 0000000000..48bfd2f5d9\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/chatchat_api.py\n@@ -0,0 +1,43 @@\n+import logging\n+\n+from open_chatcaht._constants import API_BASE_URI\n+from open_chatcaht.api.chat.chat_client import ChatClient\n+from open_chatcaht.api.knowledge_base.knowledge_base_client import KbClient\n+from open_chatcaht.api.server.server_client import ServerClient\n+from open_chatcaht.api.standard_openai.standard_openai_client import StandardOpenaiClient\n+from open_chatcaht.api.tools.tool_client import ToolClient\n+\n+\n+class ChatChat:\n+    knowledge: KbClient = None\n+    tool: ToolClient = None\n+    server: ServerClient = None\n+    chat: ChatClient = None\n+    openai_adapter: StandardOpenaiClient = None\n+\n+    def __init__(self,\n+                 base_url: str = API_BASE_URI,\n+                 timeout: float = 60,\n+                 use_async: bool = False,\n+                 use_proxy: bool = False,\n+                 proxies=None,\n+                 log_level: int = logging.INFO,\n+                 retry: int = 3,\n+                 retry_interval: int = 1, ):\n+        param = {\n+            'log_level': log_level,\n+            'retry': retry,\n+            'retry_interval': retry_interval,\n+            'base_url': base_url,\n+            'timeout': timeout,\n+            'use_async': use_async,\n+            'use_proxy': use_proxy,\n+            'proxies': proxies\n+        }\n+\n+        self.knowledge = KbClient(**param)\n+        self.tool = ToolClient(**param)\n+        self.server = ServerClient(**param)\n+        self.chat = ChatClient(**param)\n+        self.openai_adapter = StandardOpenaiClient(**param)\n+\ndiff --git a/libs/python-sdk/open_chatcaht/exceptions.py b/libs/python-sdk/open_chatcaht/exceptions.py\nnew file mode 100644\nindex 0000000000..0bae734652\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/exceptions.py\n@@ -0,0 +1,121 @@\n+from __future__ import annotations\n+\n+from open_chatcaht.utils import is_dict\n+from typing import Any, Optional, cast\n+from typing_extensions import Literal\n+\n+import httpx\n+\n+__all__ = [\n+    \"BadRequestError\",\n+    \"AuthenticationError\",\n+    \"PermissionDeniedError\",\n+    \"NotFoundError\",\n+    \"ConflictError\",\n+    \"UnprocessableEntityError\",\n+    \"RateLimitError\",\n+    \"InternalServerError\",\n+]\n+\n+\n+class ChatChatError(Exception):\n+    pass\n+\n+\n+class APIError(ChatChatError):\n+    message: str\n+    request: httpx.Request\n+\n+    body: object | None\n+    \"\"\"\n+        API\u54cd\u5e94\u4f53\u3002\n+        \u5982\u679cAPI\u54cd\u5e94\u4e86\u4e00\u4e2a\u6709\u6548\u7684JSON\u7ed3\u6784\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5c5e\u6027\u5c06\u662f\n+        \u89e3\u7801\u7ed3\u679c\u3002\n+        \u5982\u679c\u5b83\u4e0d\u662f\u4e00\u4e2a\u6709\u6548\u7684JSON\u7ed3\u6784\uff0c\u90a3\u4e48\u8fd9\u5c06\u662f\u539f\u59cb\u54cd\u5e94\u3002\n+        \u5982\u679c\u6ca1\u6709\u4e0e\u6b64\u9519\u8bef\u76f8\u5173\u7684\u54cd\u5e94\uff0c\u90a3\u4e48\u5b83\u5c06\u662f' None '\u3002\n+    \"\"\"\n+\n+    code: Optional[str] = None\n+    param: Optional[str] = None\n+    type: Optional[str]\n+\n+    def __init__(self, message: str, request: httpx.Request, *, body: object | None) -> None:\n+        super().__init__(message)\n+        self.request = request\n+        self.message = message\n+        self.body = body\n+\n+        if is_dict(body):\n+            self.code = cast(str, body.get(\"code\"))\n+            self.param = cast(str, body.get(\"param\"))\n+            self.type = cast(str, body.get(\"type\"))\n+        else:\n+            self.code = None\n+            self.param = None\n+            self.type = None\n+\n+\n+class APIResponseValidationError(APIError):\n+    response: httpx.Response\n+    status_code: int\n+\n+    def __init__(self, response: httpx.Response, body: object | None, *, message: str | None = None) -> None:\n+        super().__init__(message or \"API\u8fd4\u56de\u7684\u6570\u636e\u5bf9\u9884\u671f\u7684\u6a21\u5f0f\u65e0\u6548\u3002\", response.request, body=body)\n+        self.response = response\n+        self.status_code = response.status_code\n+\n+\n+class APIStatusError(APIError):\n+    \"\"\"\u5f53API\u54cd\u5e94\u7684\u72b6\u6001\u7801\u4e3a4xx\u62165xx\u65f6\u5f15\u53d1\u3002\"\"\"\n+\n+    response: httpx.Response\n+    status_code: int\n+    request_id: str | None\n+\n+    def __init__(self, message: str, *, response: httpx.Response, body: object | None) -> None:\n+        super().__init__(message, response.request, body=body)\n+        self.response = response\n+        self.status_code = response.status_code\n+        self.request_id = response.headers.get(\"x-request-id\")\n+\n+\n+class APIConnectionError(APIError):\n+    def __init__(self, *, message: str = \"\u8fde\u63a5\u9519\u8bef\", request: httpx.Request) -> None:\n+        super().__init__(message, request, body=None)\n+\n+\n+class APITimeoutError(APIConnectionError):\n+    def __init__(self, request: httpx.Request) -> None:\n+        super().__init__(message=\"\u8bf7\u6c42\u8d85\u65f6\", request=request)\n+\n+\n+class BadRequestError(APIStatusError):\n+    status_code: Literal[400] = 400\n+\n+\n+class AuthenticationError(APIStatusError):\n+    status_code: Literal[401] = 401\n+\n+\n+class PermissionDeniedError(APIStatusError):\n+    status_code: Literal[403] = 403\n+\n+\n+class NotFoundError(APIStatusError):\n+    status_code: Literal[404] = 404\n+\n+\n+class ConflictError(APIStatusError):\n+    status_code: Literal[409] = 409\n+\n+\n+class UnprocessableEntityError(APIStatusError):\n+    status_code: Literal[422] = 422\n+\n+\n+class RateLimitError(APIStatusError):\n+    status_code: Literal[429] = 429\n+\n+\n+class InternalServerError(APIStatusError):\n+    pass\ndiff --git a/libs/python-sdk/open_chatcaht/extra/__init__.py b/libs/python-sdk/open_chatcaht/extra/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/extra/langchain/__init__.py b/libs/python-sdk/open_chatcaht/extra/langchain/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/extra/langchain/chatchat_kb_retrieve.py b/libs/python-sdk/open_chatcaht/extra/langchain/chatchat_kb_retrieve.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/extra/llmaindex/__init__.py b/libs/python-sdk/open_chatcaht/extra/llmaindex/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/extra/llmaindex/chatchat_kb_retrieve.py b/libs/python-sdk/open_chatcaht/extra/llmaindex/chatchat_kb_retrieve.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/chat/__init__.py b/libs/python-sdk/open_chatcaht/types/chat/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/chat/chat_completions.py b/libs/python-sdk/open_chatcaht/types/chat/chat_completions.py\nnew file mode 100644\nindex 0000000000..3871f65000\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/chat/chat_completions.py\n@@ -0,0 +1,5 @@\n+from chatchat.server.api.api_schemas import OpenAIChatInput\n+\n+\n+class ChatCompletions(OpenAIChatInput):\n+    ...\ndiff --git a/libs/python-sdk/open_chatcaht/types/chat/chat_feedback_param.py b/libs/python-sdk/open_chatcaht/types/chat/chat_feedback_param.py\nnew file mode 100644\nindex 0000000000..252475c8c8\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/chat/chat_feedback_param.py\n@@ -0,0 +1,7 @@\n+from pydantic import Field, BaseModel\n+\n+\n+class ChatFeedbackParam(BaseModel):\n+    message_id: str = Field(\"\", max_length=32, description=\"\u804a\u5929\u8bb0\u5f55id\"),\n+    score: int = Field(0, max=100, description=\"\u7528\u6237\u8bc4\u5206\uff0c\u6ee1\u5206100\uff0c\u8d8a\u5927\u8868\u793a\u8bc4\u4ef7\u8d8a\u9ad8\"),\n+    reason: str = Field(\"\", description=\"\u7528\u6237\u8bc4\u5206\u7406\u7531\uff0c\u6bd4\u5982\u4e0d\u7b26\u5408\u4e8b\u5b9e\u7b49\"),\ndiff --git a/libs/python-sdk/open_chatcaht/types/chat/chat_message.py b/libs/python-sdk/open_chatcaht/types/chat/chat_message.py\nnew file mode 100644\nindex 0000000000..af996d31d3\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/chat/chat_message.py\n@@ -0,0 +1,6 @@\n+from pydantic import BaseModel, Field\n+\n+\n+class ChatMessage(BaseModel):\n+    role: str = Field(...)\n+    content: str = Field(...)\ndiff --git a/libs/python-sdk/open_chatcaht/types/chat/file_chat_param.py b/libs/python-sdk/open_chatcaht/types/chat/file_chat_param.py\nnew file mode 100644\nindex 0000000000..5ed7aada96\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/chat/file_chat_param.py\n@@ -0,0 +1,40 @@\n+from typing import Optional, List\n+\n+from pydantic import BaseModel, Field\n+\n+from open_chatcaht._constants import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD\n+from open_chatcaht.types.chat.chat_message import ChatMessage\n+\n+\n+class FileChatParam(BaseModel):\n+    \"\"\"\u6587\u4ef6\u5bf9\u8bdd\u7c7b\"\"\"\n+\n+    query: str = Field(..., description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\u4f60\u597d\"]),\n+    knowledge_id: str = Field(..., description=\"\u4e34\u65f6\u77e5\u8bc6\u5e93ID\"),\n+    top_k: int = Field(VECTOR_SEARCH_TOP_K, description=\"\u5339\u914d\u5411\u91cf\u6570\"),\n+    score_threshold: float = Field(\n+        SCORE_THRESHOLD,\n+        description=\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-1\u4e4b\u95f4\uff0cSCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\u53d6\u52301\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\",\n+        ge=0,\n+        le=2,\n+    ),\n+    history: List[ChatMessage] = Field(\n+        [],\n+        description=\"\u5386\u53f2\u5bf9\u8bdd\",\n+        examples=[\n+            [\n+                {\"role\": \"user\", \"content\": \"\u6211\u4eec\u6765\u73a9\u6210\u8bed\u63a5\u9f99\uff0c\u6211\u5148\u6765\uff0c\u751f\u9f99\u6d3b\u864e\"},\n+                {\"role\": \"assistant\", \"content\": \"\u864e\u5934\u864e\u8111\"},\n+            ]\n+        ],\n+    ),\n+    stream: bool = Field(False, description=\"\u6d41\u5f0f\u8f93\u51fa\"),\n+    model_name: str = Field(None, description=\"LLM \u6a21\u578b\u540d\u79f0\u3002\"),\n+    temperature: float = Field(0.01, description=\"LLM \u91c7\u6837\u6e29\u5ea6\", ge=0.0, le=1.0),\n+    max_tokens: Optional[int] = Field(\n+        None, description=\"\u9650\u5236LLM\u751f\u6210Token\u6570\u91cf\uff0c\u9ed8\u8ba4None\u4ee3\u8868\u6a21\u578b\u6700\u5927\u503c\"\n+    ),\n+    prompt_name: str = Field(\n+        \"default\",\n+        description=\"\u4f7f\u7528\u7684prompt\u6a21\u677f\u540d\u79f0(\u5728 prompt_settings.yaml \u4e2d\u914d\u7f6e)\",\n+    ),\ndiff --git a/libs/python-sdk/open_chatcaht/types/chat/kb_chat_param.py b/libs/python-sdk/open_chatcaht/types/chat/kb_chat_param.py\nnew file mode 100644\nindex 0000000000..6bedee8955\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/chat/kb_chat_param.py\n@@ -0,0 +1,42 @@\n+from typing import Optional, List, Literal\n+\n+from pydantic import BaseModel, Field\n+\n+from open_chatcaht._constants import MAX_TOKENS, TEMPERATURE, SCORE_THRESHOLD, VECTOR_SEARCH_TOP_K, LLM_MODEL\n+from open_chatcaht.types.chat.chat_message import ChatMessage\n+\n+\n+class KbChatParam(BaseModel):\n+    query: str = Field(..., description=\"\u7528\u6237\u8f93\u5165\", examples=[\"\u4f60\u597d\"]),\n+    mode: Literal[\"local_kb\", \"temp_kb\", \"search_engine\"] = Field(\"local_kb\", description=\"\u77e5\u8bc6\u6765\u6e90\"),\n+    kb_name: str = Field(\"\",\n+                         description=\"mode=local_kb\u65f6\u4e3a\u77e5\u8bc6\u5e93\u540d\u79f0\uff1btemp_kb\u65f6\u4e3a\u4e34\u65f6\u77e5\u8bc6\u5e93ID\uff0csearch_engine\u65f6\u4e3a\u641c\u7d22\u5f15\u64ce\u540d\u79f0\",\n+                         examples=[\"samples\"]),\n+    top_k: int = Field(VECTOR_SEARCH_TOP_K, description=\"\u5339\u914d\u5411\u91cf\u6570\"),\n+    score_threshold: float = Field(\n+        SCORE_THRESHOLD,\n+        description=\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-1\u4e4b\u95f4\uff0cSCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\u53d6\u52301\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\",\n+        ge=0,\n+        le=2,\n+    ),\n+    history: List[ChatMessage] = Field(\n+        [],\n+        description=\"\u5386\u53f2\u5bf9\u8bdd\",\n+        examples=[[\n+            {\"role\": \"user\",\n+             \"content\": \"\u6211\u4eec\u6765\u73a9\u6210\u8bed\u63a5\u9f99\uff0c\u6211\u5148\u6765\uff0c\u751f\u9f99\u6d3b\u864e\"},\n+            {\"role\": \"assistant\",\n+             \"content\": \"\u864e\u5934\u864e\u8111\"}]]\n+    ),\n+    stream: bool = Field(True, description=\"\u6d41\u5f0f\u8f93\u51fa\"),\n+    model: str = Field(LLM_MODEL, description=\"LLM \u6a21\u578b\u540d\u79f0\u3002\"),\n+    temperature: float = Field(TEMPERATURE, description=\"LLM \u91c7\u6837\u6e29\u5ea6\", ge=0.0, le=2.0),\n+    max_tokens: Optional[int] = Field(\n+        MAX_TOKENS,\n+        description=\"\u9650\u5236LLM\u751f\u6210Token\u6570\u91cf\uff0c\u9ed8\u8ba4None\u4ee3\u8868\u6a21\u578b\u6700\u5927\u503c\"\n+    ),\n+    prompt_name: str = Field(\n+        \"default\",\n+        description=\"\u4f7f\u7528\u7684prompt\u6a21\u677f\u540d\u79f0(\u5728prompt_settings.yaml\u4e2d\u914d\u7f6e)\"\n+    ),\n+    return_direct: bool = Field(False, description=\"\u76f4\u63a5\u8fd4\u56de\u68c0\u7d22\u7ed3\u679c\uff0c\u4e0d\u9001\u5165 LLM\"),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/__init__.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/create_knowledge_base_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/create_knowledge_base_param.py\nnew file mode 100644\nindex 0000000000..b451a41c75\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/create_knowledge_base_param.py\n@@ -0,0 +1,11 @@\n+from typing import Optional\n+\n+from pydantic import Field, BaseModel\n+\n+\n+class CreateKnowledgeBaseParam(BaseModel):\n+    knowledge_base_name: str = Field(default=None, description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\")\n+    vector_store_type: str = Field(default=None, description=\"\u5411\u91cf\u5b58\u50a8\u7c7b\u578b\")\n+    kb_info: Optional[str] = Field(default=None, description=\"\u77e5\u8bc6\u5e93\u4fe1\u606f\")\n+    vs_type: Optional[str] = Field(default=None, description=\"\u5411\u91cf\u5e93\u7c7b\u578b\")\n+    embed_model: Optional[str] = Field(default=None, description=\"\u5411\u91cf\u5316\u6a21\u578b\")\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/delete_knowledge_base_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/delete_knowledge_base_param.py\nnew file mode 100644\nindex 0000000000..ea7abdee01\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/delete_knowledge_base_param.py\n@@ -0,0 +1,5 @@\n+from pydantic import Field, BaseModel\n+\n+\n+class DeleteKnowledgeBaseParam(BaseModel):\n+    knowledge_base_name: str = Field(..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\")\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/__init__.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/delete_kb_docs_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/delete_kb_docs_param.py\nnew file mode 100644\nindex 0000000000..8c510f0d8b\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/delete_kb_docs_param.py\n@@ -0,0 +1,10 @@\n+from typing import List\n+\n+from pydantic import BaseModel, Field\n+\n+\n+class DeleteKbDocsParam(BaseModel):\n+    knowledge_base_name: str = Field(..., examples=[\"samples\"]),\n+    file_names: List[str] = Field(..., examples=[[\"file_name.md\", \"test.txt\"]]),\n+    delete_content: bool = Field(False),\n+    not_refresh_vs_cache: bool = Field(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/download_kb_doc_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/download_kb_doc_param.py\nnew file mode 100644\nindex 0000000000..e980153334\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/download_kb_doc_param.py\n@@ -0,0 +1,11 @@\n+from typing import List\n+\n+from pydantic import BaseModel, Field\n+\n+\n+class DownloadKbDocParam(BaseModel):\n+    knowledge_base_name: str = Field(\n+        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+    ),\n+    file_name: str = Field(..., description=\"\u6587\u4ef6\u540d\u79f0\", examples=[\"test.txt\"]),\n+    preview: bool = Field(False, description=\"\u662f\uff1a\u6d4f\u89c8\u5668\u5185\u9884\u89c8\uff1b\u5426\uff1a\u4e0b\u8f7d\"),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/list_kb_docs_file_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/list_kb_docs_file_param.py\nnew file mode 100644\nindex 0000000000..3a75138f70\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/list_kb_docs_file_param.py\n@@ -0,0 +1,5 @@\n+from pydantic import BaseModel, Field\n+\n+\n+class ListKbDocsFileParam(BaseModel):\n+    knowledge_base_name: str = Field(description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\")\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/search_kb_docs_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/search_kb_docs_param.py\nnew file mode 100644\nindex 0000000000..22d78b5941\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/search_kb_docs_param.py\n@@ -0,0 +1,17 @@\n+from pydantic import BaseModel, Field\n+\n+from open_chatcaht._constants import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD\n+\n+\n+class SearchKbDocsParam(BaseModel):\n+    query: str = Field(description=\"\u68c0\u7d22\u5185\u5bb9\")\n+    knowledge_base_name: str = Field(description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\")\n+    top_k: int = Field(default=VECTOR_SEARCH_TOP_K, description=\"\u5339\u914d\u5411\u91cf\u6570\")\n+    score_threshold: float = Field(default=SCORE_THRESHOLD,\n+                                   ge=0.0,\n+                                   le=1.0,\n+                                   description=\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-1\u4e4b\u95f4\uff0c\"\n+                                               \"SCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\"\n+                                               \"\u53d6\u52301\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\")\n+    file_name: str = Field(\"\", description=\"\u6587\u4ef6\u540d\u79f0\uff0c\u652f\u6301 sql \u901a\u914d\u7b26\"),\n+    metadata: dict = Field({}, description=\"\u6839\u636e metadata \u8fdb\u884c\u8fc7\u6ee4\uff0c\u4ec5\u652f\u6301\u4e00\u7ea7\u952e\"),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/search_temp_docs_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/search_temp_docs_param.py\nnew file mode 100644\nindex 0000000000..a3ead33d07\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/search_temp_docs_param.py\n@@ -0,0 +1,15 @@\n+from pydantic import BaseModel, Field\n+\n+from open_chatcaht._constants import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD\n+\n+\n+class SearchTempDocsParam(BaseModel):\n+    knowledge_id: str\n+    query: str\n+    top_k: int = Field(default=VECTOR_SEARCH_TOP_K, description=\"\u5339\u914d\u5411\u91cf\u6570\")\n+    score_threshold: float = Field(default=SCORE_THRESHOLD,\n+                                   ge=0.0,\n+                                   le=1.0,\n+                                   description=\"\u77e5\u8bc6\u5e93\u5339\u914d\u76f8\u5173\u5ea6\u9608\u503c\uff0c\u53d6\u503c\u8303\u56f4\u57280-1\u4e4b\u95f4\uff0c\"\n+                                               \"SCORE\u8d8a\u5c0f\uff0c\u76f8\u5173\u5ea6\u8d8a\u9ad8\uff0c\"\n+                                               \"\u53d6\u52301\u76f8\u5f53\u4e8e\u4e0d\u7b5b\u9009\uff0c\u5efa\u8bae\u8bbe\u7f6e\u57280.5\u5de6\u53f3\")\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/upload_kb_docs_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/upload_kb_docs_param.py\nnew file mode 100644\nindex 0000000000..3f376f0c3a\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/upload_kb_docs_param.py\n@@ -0,0 +1,15 @@\n+from pydantic import BaseModel, Field\n+from open_chatcaht._constants import CHUNK_SIZE, OVERLAP_SIZE, ZH_TITLE_ENHANCE\n+\n+\n+class UploadKbDocsParam(BaseModel):\n+    knowledge_base_name: str = Field(\n+        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+    ),\n+    override: bool = Field(False, description=\"\u8986\u76d6\u5df2\u6709\u6587\u4ef6\"),\n+    to_vector_store: bool = Field(True, description=\"\u4e0a\u4f20\u6587\u4ef6\u540e\u662f\u5426\u8fdb\u884c\u5411\u91cf\u5316\"),\n+    chunk_size: int = Field(CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n+    chunk_overlap: int = Field(OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n+    zh_title_enhance: bool = Field(ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n+    docs: str = Field(\"\", description=\"\u81ea\u5b9a\u4e49\u7684docs\uff0c\u9700\u8981\u8f6c\u4e3ajson\u5b57\u7b26\u4e32\"),\n+    not_refresh_vs_cache: bool = Field(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\"),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/upload_temp_docs_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/upload_temp_docs_param.py\nnew file mode 100644\nindex 0000000000..5a7c266ac5\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/doc/upload_temp_docs_param.py\n@@ -0,0 +1,11 @@\n+from typing import Union, List\n+\n+from pydantic import BaseModel, Field\n+from open_chatcaht._constants import CHUNK_SIZE, OVERLAP_SIZE, ZH_TITLE_ENHANCE\n+\n+\n+class UploadTempDocsParam(BaseModel):\n+    prev_id: str = Field(None, description=\"\u524d\u77e5\u8bc6\u5e93ID\"),\n+    chunk_size: int = Field(CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n+    chunk_overlap: int = Field(OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n+    zh_title_enhance: bool = Field(ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n\\ No newline at end of file\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/knowledge_base.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/knowledge_base.py\nnew file mode 100644\nindex 0000000000..7f73056641\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/knowledge_base.py\n@@ -0,0 +1,14 @@\n+from typing import Optional\n+\n+from pydantic import BaseModel, Field\n+from datetime import datetime\n+\n+\n+class KnowledgeBaseInfo(BaseModel):\n+    id: int = Field(default=None,  description=\"\u77e5\u8bc6\u5e93id\")\n+    kb_name: str = Field(default=None,  description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\")\n+    kb_info: Optional[str] = Field(default=None,  description=\"\u77e5\u8bc6\u5e93\u4fe1\u606f\")\n+    vs_type: Optional[str] = Field(default=None,  description=\"\u5411\u91cf\u5e93\u7c7b\u578b\")\n+    embed_model: Optional[str] = Field(default=None,  description=\"\u5411\u91cf\u5316\u6a21\u578b\")\n+    file_count: Optional[int] = Field(default=None,  description=\"\u6587\u4ef6\u6570\u91cf\")\n+    create_time: Optional[datetime] = Field(default=None,  description=\"\u521b\u5efa\u65f6\u95f4\")\n\\ No newline at end of file\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/recreate_vector_store_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/recreate_vector_store_param.py\nnew file mode 100644\nindex 0000000000..4f8822bcb2\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/recreate_vector_store_param.py\n@@ -0,0 +1,15 @@\n+\n+from pydantic import BaseModel, Field\n+\n+from open_chatcaht._constants import VS_TYPE, EMBEDDING_MODEL, CHUNK_SIZE, OVERLAP_SIZE, ZH_TITLE_ENHANCE\n+\n+\n+class RecreateVectorStoreParam(BaseModel):\n+    knowledge_base_name: str = Field(..., examples=[\"samples\"], description='\u77e5\u8bc6\u5e93\u540d\u79f0'),\n+    allow_empty_kb: bool = Field(True),\n+    vs_type: str = Field(VS_TYPE, description='\u5411\u91cf\u5e93\u7c7b\u578b'),\n+    embed_model: str = Field(EMBEDDING_MODEL, description=\"\u5411\u91cf\u6a21\u578b\"),\n+    chunk_size: int = Field(CHUNK_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u5355\u6bb5\u6587\u672c\u6700\u5927\u957f\u5ea6\"),\n+    chunk_overlap: int = Field(OVERLAP_SIZE, description=\"\u77e5\u8bc6\u5e93\u4e2d\u76f8\u90bb\u6587\u672c\u91cd\u5408\u957f\u5ea6\"),\n+    zh_title_enhance: bool = Field(ZH_TITLE_ENHANCE, description=\"\u662f\u5426\u5f00\u542f\u4e2d\u6587\u6807\u9898\u52a0\u5f3a\"),\n+    not_refresh_vs_cache: bool = Field(False, description=\"\u6682\u4e0d\u4fdd\u5b58\u5411\u91cf\u5e93\uff08\u7528\u4e8eFAISS\uff09\")\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/__init__.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/recreate_summary_vector_store_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/recreate_summary_vector_store_param.py\nnew file mode 100644\nindex 0000000000..7286c1893f\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/recreate_summary_vector_store_param.py\n@@ -0,0 +1,18 @@\n+from typing import Optional\n+\n+from pydantic import Field, BaseModel\n+\n+from open_chatcaht._constants import VS_TYPE, EMBEDDING_MODEL\n+\n+\n+class RecreateSummaryVectorStoreParam(BaseModel):\n+    knowledge_base_name: str = Field(..., examples=[\"samples\"]),\n+    allow_empty_kb: bool = Field(True),\n+    vs_type: str = Field(VS_TYPE),\n+    embed_model: str = Field(EMBEDDING_MODEL),\n+    file_description: str = Field(\"\"),\n+    model_name: str = Field(None, description=\"LLM \u6a21\u578b\u540d\u79f0\u3002\"),\n+    temperature: float = Field(0.01, description=\"LLM \u91c7\u6837\u6e29\u5ea6\", ge=0.0, le=1.0),\n+    max_tokens: Optional[int] = Field(\n+        None, description=\"\u9650\u5236LLM\u751f\u6210Token\u6570\u91cf\uff0c\u9ed8\u8ba4None\u4ee3\u8868\u6a21\u578b\u6700\u5927\u503c\"\n+    ),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/summary_doc_ids_to_vector_store_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/summary_doc_ids_to_vector_store_param.py\nnew file mode 100644\nindex 0000000000..af3307f4e7\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/summary_doc_ids_to_vector_store_param.py\n@@ -0,0 +1,18 @@\n+from typing import Optional, List\n+\n+from pydantic import Field, BaseModel\n+\n+from open_chatcaht._constants import VS_TYPE, EMBEDDING_MODEL\n+\n+\n+class SummaryDocIdsToVectorStoreParam(BaseModel):\n+    knowledge_base_name: str = Field(..., examples=[\"samples\"]),\n+    doc_ids: List = Field([], examples=[[\"uuid\"]]),\n+    vs_type: str = Field(VS_TYPE),\n+    embed_model: str = Field(EMBEDDING_MODEL),\n+    file_description: str = Field(\"\"),\n+    model_name: str = Field(None, description=\"LLM \u6a21\u578b\u540d\u79f0\u3002\"),\n+    temperature: float = Field(0.01, description=\"LLM \u91c7\u6837\u6e29\u5ea6\", ge=0.0, le=1.0),\n+    max_tokens: Optional[int] = Field(\n+        None, description=\"\u9650\u5236LLM\u751f\u6210Token\u6570\u91cf\uff0c\u9ed8\u8ba4None\u4ee3\u8868\u6a21\u578b\u6700\u5927\u503c\"\n+    ),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/summary_file_to_vector_store_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/summary_file_to_vector_store_param.py\nnew file mode 100644\nindex 0000000000..5e78014195\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/summary/summary_file_to_vector_store_param.py\n@@ -0,0 +1,19 @@\n+from typing import Optional\n+\n+from pydantic import Field, BaseModel\n+\n+from open_chatcaht._constants import VS_TYPE, EMBEDDING_MODEL\n+\n+\n+class SummaryFileToVectorStoreParam(BaseModel):\n+    knowledge_base_name: str = Field(..., examples=[\"samples\"]),\n+    file_name: str = Field(..., examples=[\"test.pdf\"]),\n+    allow_empty_kb: bool = Field(True),\n+    vs_type: str = Field(VS_TYPE),\n+    embed_model: str = Field(EMBEDDING_MODEL),\n+    file_description: str = Field(\"\"),\n+    model_name: str = Field(None, description=\"LLM \u6a21\u578b\u540d\u79f0\u3002\"),\n+    temperature: float = Field(0.01, description=\"LLM \u91c7\u6837\u6e29\u5ea6\", ge=0.0, le=1.0),\n+    max_tokens: Optional[int] = Field(\n+        None, description=\"\u9650\u5236LLM\u751f\u6210Token\u6570\u91cf\uff0c\u9ed8\u8ba4None\u4ee3\u8868\u6a21\u578b\u6700\u5927\u503c\"\n+    ),\ndiff --git a/libs/python-sdk/open_chatcaht/types/knowledge_base/update_kb_info_param.py b/libs/python-sdk/open_chatcaht/types/knowledge_base/update_kb_info_param.py\nnew file mode 100644\nindex 0000000000..89f8fb49b8\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/knowledge_base/update_kb_info_param.py\n@@ -0,0 +1,8 @@\n+from pydantic import BaseModel, Field\n+\n+\n+class UpdateKbInfoParam(BaseModel):\n+    knowledge_base_name: str = Field(\n+        ..., description=\"\u77e5\u8bc6\u5e93\u540d\u79f0\", examples=[\"samples\"]\n+    ),\n+    kb_info: str = Field(..., description=\"\u77e5\u8bc6\u5e93\u4ecb\u7ecd\", examples=[\"\u8fd9\u662f\u4e00\u4e2a\u77e5\u8bc6\u5e93\"]),\ndiff --git a/libs/python-sdk/open_chatcaht/types/response/__init__.py b/libs/python-sdk/open_chatcaht/types/response/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/response/base.py b/libs/python-sdk/open_chatcaht/types/response/base.py\nnew file mode 100644\nindex 0000000000..2c6dd31e32\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/response/base.py\n@@ -0,0 +1,30 @@\n+from typing import Any, List\n+\n+from pydantic import BaseModel, Field\n+\n+\n+class BaseResponse(BaseModel):\n+    code: int = Field(200, description=\"API status code\")\n+    msg: str = Field(\"success\", description=\"API status message\")\n+    data: Any = Field(None, description=\"API data\")\n+\n+    class Config:\n+        json_schema_extra = {\n+            \"example\": {\n+                \"code\": 200,\n+                \"msg\": \"success\",\n+            }\n+        }\n+\n+\n+class ListResponse(BaseResponse):\n+    data: List[Any] = Field(..., description=\"List of data\")\n+\n+    class Config:\n+        json_schema_extra = {\n+            \"example\": {\n+                \"code\": 200,\n+                \"msg\": \"success\",\n+                \"data\": [\"doc1.docx\", \"doc2.pdf\", \"doc3.txt\"],\n+            }\n+        }\ndiff --git a/libs/python-sdk/open_chatcaht/types/server/__init__.py b/libs/python-sdk/open_chatcaht/types/server/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/__init__.py b/libs/python-sdk/open_chatcaht/types/standard_openai/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/audio_speech_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/audio_speech_input.py\nnew file mode 100644\nindex 0000000000..141abe1480\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/audio_speech_input.py\n@@ -0,0 +1,13 @@\n+from typing import Optional, Literal\n+\n+from open_chatcaht.types.standard_openai.base import OpenAIBaseInput\n+\n+\n+class OpenAIAudioSpeechInput(OpenAIBaseInput):\n+    input: str\n+    model: str\n+    voice: str\n+    response_format: Optional[\n+        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"pcm\", \"wav\"]\n+    ] = None\n+    speed: Optional[float] = None\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/audio_transcriptions_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/audio_transcriptions_input.py\nnew file mode 100644\nindex 0000000000..4d3442b995\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/audio_transcriptions_input.py\n@@ -0,0 +1,8 @@\n+from typing import Optional, List, Literal\n+\n+from open_chatcaht.types.standard_openai.audio_translations_input import OpenAIAudioTranslationsInput\n+\n+\n+class OpenAIAudioTranscriptionsInput(OpenAIAudioTranslationsInput):\n+    language: Optional[str] = None\n+    timestamp_granularities: Optional[List[Literal[\"word\", \"segment\"]]] = None\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/audio_translations_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/audio_translations_input.py\nnew file mode 100644\nindex 0000000000..60a74f4077\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/audio_translations_input.py\n@@ -0,0 +1,14 @@\n+from typing import Union, Optional, Any\n+\n+from pydantic import AnyUrl\n+\n+from open_chatcaht._constants import TEMPERATURE\n+from open_chatcaht.types.standard_openai.base import OpenAIBaseInput\n+\n+\n+class OpenAIAudioTranslationsInput(OpenAIBaseInput):\n+    file: Union[Any, AnyUrl]\n+    model: str\n+    prompt: Optional[str] = None\n+    response_format: Optional[str] = None\n+    temperature: float = TEMPERATURE\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/base.py b/libs/python-sdk/open_chatcaht/types/standard_openai/base.py\nnew file mode 100644\nindex 0000000000..1996b102ff\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/base.py\n@@ -0,0 +1,16 @@\n+from typing import Optional, Dict\n+\n+from pydantic import BaseModel, Field\n+\n+\n+class OpenAIBaseInput(BaseModel):\n+    user: Optional[str] = None\n+    # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n+    # The extra values given here take precedence over values defined on the client or passed to this method.\n+    extra_headers: Optional[Dict] = None\n+    extra_query: Optional[Dict] = None\n+    extra_json: Optional[Dict] = Field(None, alias=\"extra_body\")\n+    timeout: Optional[float] = None\n+\n+    class Config:\n+        extra = \"allow\"\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/chat_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/chat_input.py\nnew file mode 100644\nindex 0000000000..b472624db5\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/chat_input.py\n@@ -0,0 +1,32 @@\n+from typing import List, Optional, Dict, Union\n+\n+from open_chatcaht._constants import LLM_MODEL, TEMPERATURE\n+from open_chatcaht.types.standard_openai.base import OpenAIBaseInput\n+from openai.types.chat import (\n+    ChatCompletionMessageParam,\n+    ChatCompletionToolChoiceOptionParam,\n+    ChatCompletionToolParam,\n+    completion_create_params,\n+)\n+\n+\n+class OpenAIChatInput(OpenAIBaseInput):\n+    messages: List[Union[Dict, ChatCompletionMessageParam]] = []\n+    model: str = LLM_MODEL\n+    frequency_penalty: Optional[float] = None\n+    function_call: Optional[completion_create_params.FunctionCall] = []\n+    functions: List[completion_create_params.Function] = None\n+    logit_bias: Optional[Dict[str, int]] = None\n+    logprobs: Optional[bool] = None\n+    max_tokens: Optional[int] = None\n+    n: Optional[int] = None\n+    presence_penalty: Optional[float] = None\n+    response_format: completion_create_params.ResponseFormat = None\n+    seed: Optional[int] = None\n+    stop: Union[Optional[str], List[str]] = None\n+    stream: Optional[bool] = True\n+    temperature: Optional[float] = TEMPERATURE\n+    tool_choice: Optional[Union[ChatCompletionToolChoiceOptionParam, str]] = None\n+    tools: List[Union[ChatCompletionToolParam, str]] = None\n+    top_logprobs: Optional[int] = None\n+    top_p: Optional[float] = None\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/embeddings_Input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/embeddings_Input.py\nnew file mode 100644\nindex 0000000000..e563ce2bb7\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/embeddings_Input.py\n@@ -0,0 +1,10 @@\n+from typing import Union, List, Optional, Literal\n+\n+from open_chatcaht.types.standard_openai.base import OpenAIBaseInput\n+\n+\n+class OpenAIEmbeddingsInput(OpenAIBaseInput):\n+    input: Union[str, List[str]]\n+    model: str\n+    dimensions: Optional[int] = None\n+    encoding_format: Optional[Literal[\"float\", \"base64\"]] = None\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/image_base_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/image_base_input.py\nnew file mode 100644\nindex 0000000000..e3ff9b78b2\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/image_base_input.py\n@@ -0,0 +1,12 @@\n+from typing import Optional, Literal\n+\n+from open_chatcaht.types.standard_openai.base import OpenAIBaseInput\n+\n+\n+class OpenAIImageBaseInput(OpenAIBaseInput):\n+    model: str\n+    n: int = 1\n+    response_format: Optional[Literal[\"url\", \"b64_json\"]] = None\n+    size: Optional[\n+        Literal[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"]\n+    ] = \"256x256\"\n\\ No newline at end of file\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/image_edits_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/image_edits_input.py\nnew file mode 100644\nindex 0000000000..d148eff0dc\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/image_edits_input.py\n@@ -0,0 +1,10 @@\n+from typing import Any, Union\n+\n+from pydantic import AnyUrl\n+\n+from open_chatcaht.types.standard_openai.image_variations_input import OpenAIImageVariationsInput\n+\n+\n+class OpenAIImageEditsInput(OpenAIImageVariationsInput):\n+    prompt: str\n+    mask: Union[Any, AnyUrl]\n\\ No newline at end of file\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/image_generations_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/image_generations_input.py\nnew file mode 100644\nindex 0000000000..73ebd3483e\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/image_generations_input.py\n@@ -0,0 +1,9 @@\n+from typing import Literal, Optional\n+\n+from open_chatcaht.types.standard_openai.image_base_input import OpenAIImageBaseInput\n+\n+\n+class OpenAIImageGenerationsInput(OpenAIImageBaseInput):\n+    prompt: str\n+    quality: Literal[\"standard\", \"hd\"] = None\n+    style: Optional[Literal[\"vivid\", \"natural\"]] = None\n\\ No newline at end of file\ndiff --git a/libs/python-sdk/open_chatcaht/types/standard_openai/image_variations_input.py b/libs/python-sdk/open_chatcaht/types/standard_openai/image_variations_input.py\nnew file mode 100644\nindex 0000000000..494763145b\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/standard_openai/image_variations_input.py\n@@ -0,0 +1,9 @@\n+from typing import Union, Any\n+\n+from pydantic import AnyUrl\n+\n+from open_chatcaht.types.standard_openai.image_base_input import OpenAIImageBaseInput\n+\n+\n+class OpenAIImageVariationsInput(OpenAIImageBaseInput):\n+    image: Union[Any, AnyUrl]\ndiff --git a/libs/python-sdk/open_chatcaht/types/tools/__init__.py b/libs/python-sdk/open_chatcaht/types/tools/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/open_chatcaht/types/tools/call_tool_param.py b/libs/python-sdk/open_chatcaht/types/tools/call_tool_param.py\nnew file mode 100644\nindex 0000000000..0aff467e46\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/types/tools/call_tool_param.py\n@@ -0,0 +1,6 @@\n+from pydantic import Field, BaseModel\n+\n+\n+class CallToolParam(BaseModel):\n+    name: str = Field(..., description=\"\u5de5\u5177\u540d\u79f0\")\n+    tool_input: dict = Field({}, description=\"\u77e5\u8bc6\u5e93\u4fe1\u606f\"),\ndiff --git a/libs/python-sdk/open_chatcaht/utils.py b/libs/python-sdk/open_chatcaht/utils.py\nnew file mode 100644\nindex 0000000000..458ee2286d\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/utils.py\n@@ -0,0 +1,254 @@\n+import base64\n+import inspect\n+import os\n+from io import BytesIO\n+from pathlib import Path\n+from typing import Union, List, Dict\n+\n+import httpx\n+from pydantic import BaseModel\n+from typing_extensions import TypeGuard\n+\n+from open_chatcaht._constants import HTTPX_TIMEOUT\n+\n+\n+def get_httpx_client(\n+        use_async: bool = False,\n+        proxies: Union[str, Dict] = None,\n+        timeout: float = HTTPX_TIMEOUT,\n+        unused_proxies: List[str] = [],\n+        **kwargs,\n+) -> Union[httpx.Client, httpx.AsyncClient]:\n+    \"\"\"\n+    helper to get httpx client with default proxies that bypass local addesses.\n+    \"\"\"\n+    default_proxies = {\n+        # do not use proxy for locahost\n+        \"all://127.0.0.1\": None,\n+        \"all://localhost\": None,\n+    }\n+    # do not use proxy for user deployed fastchat servers\n+    for x in unused_proxies:\n+        host = \":\".join(x.split(\":\")[:2])\n+        default_proxies.update({host: None})\n+\n+    # get proxies from system envionrent\n+    # proxy not str empty string, None, False, 0, [] or {}\n+    default_proxies.update(\n+        {\n+            \"http://\": (\n+                os.environ.get(\"http_proxy\")\n+                if os.environ.get(\"http_proxy\")\n+                   and len(os.environ.get(\"http_proxy\").strip())\n+                else None\n+            ),\n+            \"https://\": (\n+                os.environ.get(\"https_proxy\")\n+                if os.environ.get(\"https_proxy\")\n+                   and len(os.environ.get(\"https_proxy\").strip())\n+                else None\n+            ),\n+            \"all://\": (\n+                os.environ.get(\"all_proxy\")\n+                if os.environ.get(\"all_proxy\")\n+                   and len(os.environ.get(\"all_proxy\").strip())\n+                else None\n+            ),\n+        }\n+    )\n+    for host in os.environ.get(\"no_proxy\", \"\").split(\",\"):\n+        if host := host.strip():\n+            # default_proxies.update({host: None}) # Origin code\n+            default_proxies.update(\n+                {\"all://\" + host: None}\n+            )  # PR 1838 fix, if not add 'all://', httpx will raise error\n+\n+    # merge default proxies with user provided proxies\n+    if isinstance(proxies, str):\n+        proxies = {\"all://\": proxies}\n+\n+    if isinstance(proxies, dict):\n+        default_proxies.update(proxies)\n+\n+    # construct Client\n+    kwargs.update(timeout=timeout, proxies=default_proxies)\n+\n+    if use_async:\n+        return httpx.AsyncClient(**kwargs)\n+    else:\n+        return httpx.Client(**kwargs)\n+\n+\n+def set_httpx_config(\n+        timeout: float = HTTPX_TIMEOUT,\n+        proxy: Union[str, Dict] = None,\n+        unused_proxies: List[str] = [],\n+):\n+    \"\"\"\n+    \u8bbe\u7f6ehttpx\u9ed8\u8ba4timeout\u3002httpx\u9ed8\u8ba4timeout\u662f5\u79d2\uff0c\u5728\u8bf7\u6c42LLM\u56de\u7b54\u65f6\u4e0d\u591f\u7528\u3002\n+    \u5c06\u672c\u9879\u76ee\u76f8\u5173\u670d\u52a1\u52a0\u5165\u65e0\u4ee3\u7406\u5217\u8868\uff0c\u907f\u514dfastchat\u7684\u670d\u52a1\u5668\u8bf7\u6c42\u9519\u8bef\u3002(windows\u4e0b\u65e0\u6548)\n+    \u5bf9\u4e8echatgpt\u7b49\u5728\u7ebfAPI\uff0c\u5982\u8981\u4f7f\u7528\u4ee3\u7406\u9700\u8981\u624b\u52a8\u914d\u7f6e\u3002\u641c\u7d22\u5f15\u64ce\u7684\u4ee3\u7406\u5982\u4f55\u5904\u7f6e\u8fd8\u9700\u8003\u8651\u3002\n+    \"\"\"\n+\n+    import os\n+\n+    import httpx\n+\n+    httpx._config.DEFAULT_TIMEOUT_CONFIG.connect = timeout\n+    httpx._config.DEFAULT_TIMEOUT_CONFIG.read = timeout\n+    httpx._config.DEFAULT_TIMEOUT_CONFIG.write = timeout\n+\n+    # \u5728\u8fdb\u7a0b\u8303\u56f4\u5185\u8bbe\u7f6e\u7cfb\u7edf\u7ea7\u4ee3\u7406\n+    proxies = {}\n+    if isinstance(proxy, str):\n+        for n in [\"http\", \"https\", \"all\"]:\n+            proxies[n + \"_proxy\"] = proxy\n+    elif isinstance(proxy, dict):\n+        for n in [\"http\", \"https\", \"all\"]:\n+            if p := proxy.get(n):\n+                proxies[n + \"_proxy\"] = p\n+            elif p := proxy.get(n + \"_proxy\"):\n+                proxies[n + \"_proxy\"] = p\n+\n+    for k, v in proxies.items():\n+        os.environ[k] = v\n+\n+    # set host to bypass proxy\n+    no_proxy = [\n+        x.strip() for x in os.environ.get(\"no_proxy\", \"\").split(\",\") if x.strip()\n+    ]\n+    no_proxy += [\n+        # do not use proxy for locahost\n+        \"http://127.0.0.1\",\n+        \"http://localhost\",\n+    ]\n+    # do not use proxy for user deployed fastchat servers\n+    for x in unused_proxies:\n+        host = \":\".join(x.split(\":\")[:2])\n+        if host not in no_proxy:\n+            no_proxy.append(host)\n+    os.environ[\"NO_PROXY\"] = \",\".join(no_proxy)\n+\n+    def _get_proxies():\n+        return proxies\n+\n+    import urllib.request\n+\n+    urllib.request.getproxies = _get_proxies\n+\n+\n+def get_img_base64(file_path: str) -> str:\n+    \"\"\"\n+    get_img_base64 used in streamlit.\n+    \"\"\"\n+    image = file_path\n+    # \u8bfb\u53d6\u56fe\u7247\n+    with open(image, \"rb\") as f:\n+        buffer = BytesIO(f.read())\n+        base_str = base64.b64encode(buffer.getvalue()).decode()\n+    return f\"data:image/png;base64,{base_str}\"\n+\n+\n+def check_success_msg(data: Union[str, dict, list], key: str = \"msg\") -> str:\n+    \"\"\"\n+    return error message if error occured when requests API\n+    \"\"\"\n+    if (\n+            isinstance(data, dict)\n+            and key in data\n+            and \"code\" in data\n+            and data[\"code\"] == 200\n+    ):\n+        return data[key]\n+    return \"\"\n+\n+\n+def check_error_msg(data: Union[str, dict, list], key: str = \"errorMsg\") -> str:\n+    \"\"\"\n+    return error message if error occured when requests API\n+    \"\"\"\n+    if isinstance(data, dict):\n+        if key in data:\n+            return data[key]\n+        if \"code\" in data and data[\"code\"] != 200:\n+            return data[\"msg\"]\n+    return \"\"\n+\n+\n+def get_variable(*args):\n+    for var in args:\n+        if var:\n+            return var\n+    return None\n+\n+\n+def is_dict(obj: object) -> TypeGuard[dict[object, object]]:\n+    return isinstance(obj, dict)\n+\n+\n+def model_to_dict(model: BaseModel) -> dict[str, object]:\n+    return model.dict()\n+\n+\n+def get_function_default_params(func) -> dict:\n+    \"\"\"\n+    \u83b7\u53d6\u51fd\u6570\u7684\u53c2\u6570\u53ca\u5176\u9ed8\u8ba4\u503c\u3002\n+\n+    \u53c2\u6570:\n+        func (function): \u8981\u5206\u6790\u7684\u51fd\u6570\u3002\n+\n+    \u8fd4\u56de:\n+        dict: \u4e00\u4e2a\u5305\u542b\u53c2\u6570\u540d\u79f0\u53ca\u5176\u9ed8\u8ba4\u503c\u7684\u5b57\u5178\u3002\n+    \"\"\"\n+    signature = inspect.signature(func)\n+    params = signature.parameters\n+    params_dict = {}\n+\n+    for param_name, param in params.items():\n+        if param.default is inspect.Parameter.empty:\n+            params_dict[param_name] = None\n+        else:\n+            params_dict[param_name] = param.default\n+\n+    return params_dict\n+\n+\n+def merge_dicts(dict1, dict2) -> dict:\n+    \"\"\"\n+    \u5408\u5e76\u4e24\u4e2a\u5b57\u5178\uff0c\u4f18\u5148\u4f7f\u7528\u7b2c\u4e00\u4e2a\u5b57\u5178\u4e2d\u7684\u975e\u7a7a\u503c\u3002\n+\n+    \u53c2\u6570:\n+        dict1 (dict): \u7b2c\u4e00\u4e2a\u5b57\u5178\u3002\n+        dict2 (dict): \u7b2c\u4e8c\u4e2a\u5b57\u5178\u3002\n+\n+    \u8fd4\u56de:\n+        dict: \u5408\u5e76\u540e\u7684\u5b57\u5178\u3002\n+    \"\"\"\n+    merged_dict = {}\n+\n+    # \u904d\u5386\u4e24\u4e2a\u5b57\u5178\u7684\u952e\u96c6\u5408\n+    all_keys = set(dict1.keys()).union(set(dict2.keys()))\n+\n+    for key in all_keys:\n+        value1 = dict1.get(key)\n+        value2 = dict2.get(key)\n+\n+        # \u5982\u679c\u7b2c\u4e00\u4e2a\u5b57\u5178\u4e2d\u7684\u503c\u4e0d\u4e3a\u7a7a\uff0c\u4f7f\u7528\u7b2c\u4e00\u4e2a\u5b57\u5178\u7684\u503c\n+        if value1:\n+            merged_dict[key] = value1\n+        else:\n+            # \u5426\u5219\u4f7f\u7528\u7b2c\u4e8c\u4e2a\u5b57\u5178\u4e2d\u7684\u503c\n+            merged_dict[key] = value2\n+\n+    return merged_dict\n+\n+\n+def convert_file(file, filename=None):\n+    if isinstance(file, bytes):  # raw bytes\n+        file = BytesIO(file)\n+    elif hasattr(file, \"read\"):  # a file io like object\n+        filename = filename or file.name\n+    else:  # a local path\n+        file = Path(file).absolute().open(\"rb\")\n+        filename = filename or os.path.split(file.name)[-1]\n+    return filename, file\n\\ No newline at end of file\ndiff --git a/libs/python-sdk/open_chatcaht/version.py b/libs/python-sdk/open_chatcaht/version.py\nnew file mode 100644\nindex 0000000000..01a08ab5a9\n--- /dev/null\n+++ b/libs/python-sdk/open_chatcaht/version.py\n@@ -0,0 +1,3 @@\n+from ._version import __version__\n+\n+VERSION: str = __version__\ndiff --git a/libs/python-sdk/poetry.toml b/libs/python-sdk/poetry.toml\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/libs/python-sdk/pyproject.toml b/libs/python-sdk/pyproject.toml\nnew file mode 100644\nindex 0000000000..54f1428c41\n--- /dev/null\n+++ b/libs/python-sdk/pyproject.toml\n@@ -0,0 +1,16 @@\n+[tool.poetry]\n+name = \"open_langchain_chatchat\"\n+version = \"0.3.0.20240708\"\n+description = \"Langchain-Chatchat sdk\"\n+authors = [\"chatchat\"]\n+packages = [\n+    {include = \"open_chatcaht\"}\n+]\n+\n+\n+\n+[tool.poetry.dependencies]\n+python = \">=3.8.1,<3.12,!=3.9.7\"\n+pydantic = \"^2.8.2\"\n+openai = \"^1.35.13\"\n+httpx = \"^0.27.0\"\ndiff --git a/tools/autodl_start_script/start_chatchat.sh b/tools/autodl_start_script/start_chatchat.sh\nindex 634800b6c8..6e934de337 100644\n--- a/tools/autodl_start_script/start_chatchat.sh\n+++ b/tools/autodl_start_script/start_chatchat.sh\n@@ -1,6 +1,6 @@\n #!/bin/bash\n \n-conda run -n chatchat  --no-capture-output chatchat -a > >(tee chatchat-output.log) 2>&1 &\n+conda run -n chatchat  --no-capture-output export CHATCHAT_ROOT=/root/chatchat-data && chatchat start -a > >(tee chatchat-output.log) 2>&1 &\n PID=$!\n echo \"Started chatchat with PID $PID\"\n \ndiff --git a/tools/autodl_start_script/start_models_emb.sh b/tools/autodl_start_script/start_models_emb.sh\nindex a9854bd388..cd722f28c7 100755\n--- a/tools/autodl_start_script/start_models_emb.sh\n+++ b/tools/autodl_start_script/start_models_emb.sh\n@@ -13,4 +13,4 @@ curl 'http://127.0.0.1:9997/v1/models' \\\n   -H 'sec-ch-ua: \"Chromium\";v=\"124\", \"Google Chrome\";v=\"124\", \"Not-A.Brand\";v=\"99\"' \\\n   -H 'sec-ch-ua-mobile: ?0' \\\n   -H 'sec-ch-ua-platform: \"Linux\"' \\\n-  --data-raw '{\"model_uid\":\"bge-large-zh-v1.5\",\"model_name\":\"autodl-tmp-bge-large-zh\",\"model_type\":\"embedding\",\"replica\":1,\"n_gpu\":\"auto\",\"worker_ip\":null,\"gpu_idx\":null}'\n\\ No newline at end of file\n+  --data-raw '{\"model_uid\":\"bge-large-zh-v1.5\",\"model_name\":\"autodl-tmp-bge-large-zh\",\"model_type\":\"embedding\",\"replica\":1,\"n_gpu\":null,\"worker_ip\":null,\"gpu_idx\":null}'\n\\ No newline at end of file\n", "instance_id": "chatchat-space__Langchain-Chatchat-4573", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue with generating correct public API addresses when using cloud servers or reverse proxies. It identifies two specific problems: the incorrect handling of the API host address (e.g., converting 0.0.0.0 to 127.0.0.1 or failing to bind to a public IP) and an extraneous slash in the port part of the URL causing access failures. The implementation suggestion to add a `public_host` configuration is also provided, which helps in understanding the intended solution. However, there are minor ambiguities, such as the lack of detailed input/output formats for the configuration changes and insufficient mention of potential edge cases (e.g., handling multiple network interfaces or reverse proxy configurations). Additionally, the problem statement includes unrelated content (e.g., ChromaDB bug fix) that slightly distracts from the main focus. Overall, the core issue and solution direction are clear, but some finer details are missing.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, it requires understanding the networking logic within the application, specifically how API addresses are constructed and used for redirections between services (webui to API). The solution involves modifying configuration handling to support a public host/port setting, which impacts how URLs are generated, as seen in the code changes to `api_address()` in `server/utils.py`. This necessitates changes across multiple files, including configuration schemas and potentially UI components, as evidenced by the extensive diff provided. The problem also requires handling edge cases like ensuring compatibility with existing configurations and validating the public host/port values, though these are not deeply complex. Additionally, there is a need to understand the interaction between different parts of the codebase (e.g., API server settings and web UI). However, the changes do not significantly alter the system's architecture or require advanced domain-specific knowledge beyond typical web application networking concepts. Therefore, a score of 0.45 reflects a medium difficulty level, involving moderate complexity in understanding and modifying the codebase across several components.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use standardized version metadata across python packages\nCurrently we define the versions for packages in a few different places. In some cases, we define it in the `__init__.py` file as `__version__`, in some cases we define it in the `setup.cfg` file.\r\nDefining `__version__` is not recommended and was a [rejected PEP 396 standard](https://peps.python.org/pep-0396/#pep-rejection). Instead, we should define it in the `setup.cfg` or `pyproject.toml` file directly.\n", "patch": "diff --git a/plugins/json/setup.cfg b/plugins/json/setup.cfg\nindex db54ea426..362d8bbb4 100644\n--- a/plugins/json/setup.cfg\n+++ b/plugins/json/setup.cfg\n@@ -3,7 +3,7 @@ name = deephaven-plugin-json\n description = Deephaven Plugin for json\n long_description = file: README.md\n long_description_content_type = text/markdown\n-version = attr:deephaven.plugin.json.__version__\n+version = 0.0.2.dev0\n url = https://github.com/deephaven/deephaven-plugins\n project_urls =\n     Source Code = https://github.com/deephaven/deephaven-plugins\ndiff --git a/plugins/json/src/deephaven/plugin/json/__init__.py b/plugins/json/src/deephaven/plugin/json/__init__.py\nindex 1bf18e00c..562422e8a 100644\n--- a/plugins/json/src/deephaven/plugin/json/__init__.py\n+++ b/plugins/json/src/deephaven/plugin/json/__init__.py\n@@ -5,8 +5,6 @@\n \n from deephaven.plugin import Registration, Callback, object_type\n \n-__version__ = \"0.0.2.dev0\"\n-\n \n class Node:\n     def __init__(self, object, is_ref=False, **kw) -> None:\ndiff --git a/plugins/plotly/setup.cfg b/plugins/plotly/setup.cfg\nindex 64b135cc4..c9ab3f75f 100644\n--- a/plugins/plotly/setup.cfg\n+++ b/plugins/plotly/setup.cfg\n@@ -3,7 +3,7 @@ name = deephaven-plugin-plotly\n description = Deephaven Plugin for Plotly\n long_description = file: README.md\n long_description_content_type = text/markdown\n-version = attr:deephaven.plugin.plotly.__version__\n+version = 0.3.0.dev0\n url = https://github.com/deephaven/deephaven-plugins\n project_urls =\n     Source Code = https://github.com/deephaven/deephaven-plugins\ndiff --git a/plugins/plotly/src/deephaven/plugin/plotly/__init__.py b/plugins/plotly/src/deephaven/plugin/plotly/__init__.py\nindex d520ce352..c4a18cf16 100644\n--- a/plugins/plotly/src/deephaven/plugin/plotly/__init__.py\n+++ b/plugins/plotly/src/deephaven/plugin/plotly/__init__.py\n@@ -1,8 +1,6 @@\n from deephaven.plugin import Registration, Callback\n from plotly import io as pio\n \n-__version__ = \"0.3.0.dev0\"\n-\n \n def _init_theme():\n     # Set the Deephaven style globally\ndiff --git a/tools/update_version.sh b/tools/update_version.sh\nindex d1fd141d2..7d5f7bd69 100755\n--- a/tools/update_version.sh\n+++ b/tools/update_version.sh\n@@ -113,26 +113,8 @@ function update_file() {\n extra=\n [ \"$dev\" = true ] && extra=\".dev0\"\n case \"$package\" in\n-        json)\n-            update_file json/src/deephaven/plugin/json/__init__.py '__version__ = \"' '\"' \"$extra\"\n-            ;;\n-        matplotlib)\n-            update_file matplotlib/setup.cfg 'version = ' '' \"$extra\"\n-            ;;\n-        plotly)\n-            update_file plotly/src/deephaven/plugin/plotly/__init__.py '__version__ = \"' '\"' \"$extra\"\n-            ;;\n-        plotly-express)\n-            update_file plotly-express/setup.cfg 'version = ' '' \"$extra\"\n-            ;;\n-        ui)\n-            update_file ui/setup.cfg 'version = ' '' \"$extra\"\n-            ;;\n-        utilities)\n-            update_file utilities/setup.cfg 'version = ' '' \"$extra\"\n-            ;;\n-        packaging)\n-            update_file packaging/setup.cfg 'version = ' '' \"$extra\"\n+        json | matplotlib | plotly | plotly-express | ui | utilities | packaging)\n+            update_file \"${package}/setup.cfg\" 'version = ' '' \"$extra\"\n             ;;\n         auth-keycloak | dashboard-object-viewer | table-example)\n             # Packages that don't have any Python to publish, just ignore\n", "instance_id": "deephaven__deephaven-plugins-373", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to standardize version metadata across Python packages by moving version definitions from `__init__.py` files to `setup.cfg` or `pyproject.toml` files, as per the rejection of PEP 396. It identifies the current issue (version defined in multiple places) and provides a clear goal (standardize in configuration files). However, there are minor ambiguities and missing details. For instance, it does not explicitly mention whether this change applies to all packages in the repository or only specific ones, nor does it address potential backward compatibility concerns or how to handle discrepancies between existing version definitions. Additionally, there are no examples or detailed steps for handling cases where `pyproject.toml` might be preferred over `setup.cfg`. Despite these minor gaps, the overall intent and scope are understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes involve multiple files across different packages (e.g., `json` and `plotly` plugins), but the modifications are straightforward\u2014removing `__version__` definitions from `__init__.py` files and hardcoding version numbers in `setup.cfg`. Additionally, a shell script (`update_version.sh`) is updated to reflect the new versioning approach. The changes are localized to configuration and initialization files and do not impact the core functionality or architecture of the system. The overall amount of code change is minimal, mostly involving deletions and simple updates.\n\n2. **Number of Technical Concepts:** The problem requires basic knowledge of Python packaging standards (e.g., `setup.cfg`, PEP 396), understanding of version metadata, and familiarity with shell scripting for the `update_version.sh` modification. These concepts are not complex for a developer with moderate experience in Python development and packaging. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention specific edge cases or error handling requirements, and the code changes do not introduce new logic that would necessitate such considerations. The task is purely about restructuring metadata, with no apparent risk of runtime errors or complex edge cases arising from the changes.\n\n4. **Overall Complexity:** The task requires understanding the intent behind Python packaging best practices and applying a consistent change across a few files. While it involves multiple files, the logic is repetitive and does not require deep dives into the codebase's architecture or interactions between modules.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic modifications and a surface-level understanding of Python packaging conventions. It is slightly above the \"Very Easy\" range due to the need to coordinate changes across multiple files and update a script, but it remains a straightforward task for most developers.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "column_width is not respected everywhere\nI see this:\r\n\r\n```toml\r\n[tool.ruff]\r\n...\r\nlint.allowed-confusables = [\r\n  \"\u00d7\",\r\n  \"\u2019\",\r\n]\r\nlint.flake8-type-checking.exempt-modules = [\r\n]\r\nlint.per-file-ignores.\"*/__init__.py\" = [\r\n  \"F401\",\r\n]\r\nlint.per-file-ignores.\"docs/*\" = [\r\n  \"I\",\r\n]\r\nlint.per-file-ignores.\"tests/*\" = [\r\n  \"D\",\r\n]\r\n```\r\n\r\nand would rather see\r\n\r\n```toml\r\n[tool.ruff]\r\n...\r\nlint.allowed-confusables = [ \"\u00d7\", \"\u2019\" ]\r\nlint.flake8-type-checking.exempt-modules = []\r\nlint.per-file-ignores.\"*/__init__.py\" = [ \"F401\" ]\r\nlint.per-file-ignores.\"docs/*\" = [ \"I\" ]\r\nlint.per-file-ignores.\"tests/*\" = [ \"D\" ]\r\n```\r\n\r\n- `allowed-confusables` is like `tool.hatch.envs.*.features` in that it always contains a short list of strings \r\n- `lint.per-file-ignores.*` too, those lists are always very short\r\n- I think empty lists should just be on one line always, it looks super weird to expand them.\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 13c18a9..63ca0bb 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -520,7 +520,7 @@ dependencies = [\n \n [[package]]\n name = \"pyproject-fmt-rust\"\n-version = \"1.1.4\"\n+version = \"1.1.5\"\n dependencies = [\n  \"indoc\",\n  \"lexical-sort\",\ndiff --git a/rust/src/helpers/array.rs b/rust/src/helpers/array.rs\nindex 3cf4814..bc96672 100644\n--- a/rust/src/helpers/array.rs\n+++ b/rust/src/helpers/array.rs\n@@ -37,6 +37,10 @@ where\n                 .filter(|x| *x == COMMA || *x == VALUE)\n                 .last()\n                 == Some(COMMA);\n+            let multiline = array_node\n+                .children_with_tokens()\n+                .find(|e| e.kind() == NEWLINE)\n+                .is_some();\n             let mut value_set = Vec::<Vec<SyntaxElement>>::new();\n             let entry_set = RefCell::new(Vec::<SyntaxElement>::new());\n             let mut key_to_pos = HashMap::<String, usize>::new();\n@@ -67,7 +71,9 @@ where\n                 match &entry.kind() {\n                     SyntaxKind::BRACKET_START => {\n                         entries.push(entry);\n-                        entries.push(make_newline());\n+                        if multiline {\n+                            entries.push(make_newline());\n+                        }\n                         previous_is_bracket_open = true;\n                     }\n                     SyntaxKind::BRACKET_END => {\n@@ -80,7 +86,9 @@ where\n                     }\n                     VALUE => {\n                         if has_value {\n-                            entry_set.borrow_mut().push(make_newline());\n+                            if multiline {\n+                                entry_set.borrow_mut().push(make_newline());\n+                            }\n                             add_to_value_set(entry_value.clone());\n                         }\n                         has_value = true;\n@@ -117,7 +125,7 @@ where\n \n             let mut order: Vec<String> = key_to_pos.clone().into_keys().collect();\n             order.string_sort_unstable(natural_lexical_cmp);\n-            let end = entries.split_off(2);\n+            let end = entries.split_off(if multiline { 2 } else { 1 });\n             for key in order {\n                 entries.extend(value_set[key_to_pos[&key]].clone());\n             }\ndiff --git a/rust/src/main.rs b/rust/src/main.rs\nindex 1920e95..93d7e3a 100644\n--- a/rust/src/main.rs\n+++ b/rust/src/main.rs\n@@ -63,16 +63,16 @@ pub fn format_toml(content: &str, opt: &Settings) -> String {\n     reorder_tables(&root_ast, &mut tables);\n \n     let options = Options {\n-        align_entries: false,           // do not align by =\n-        align_comments: true,           // align inline comments\n-        align_single_comments: true,    // align comments after entries\n-        array_trailing_comma: true,     // ensure arrays finish with trailing comma\n-        array_auto_expand: true,        // arrays go to multi line for easier diffs\n-        array_auto_collapse: false,     // do not collapse for easier diffs\n-        compact_arrays: false,          // do not compact for easier diffs\n-        compact_inline_tables: false,   // do not compact for easier diffs\n-        compact_entries: false,         // do not compact for easier diffs\n-        column_width: opt.column_width, // always expand arrays per https://github.com/tamasfe/taplo/issues/390\n+        align_entries: false,         // do not align by =\n+        align_comments: true,         // align inline comments\n+        align_single_comments: true,  // align comments after entries\n+        array_trailing_comma: true,   // ensure arrays finish with trailing comma\n+        array_auto_expand: true,      // arrays go to multi line when too long\n+        array_auto_collapse: false,   // do not collapse for easier diffs\n+        compact_arrays: false,        // leave whitespace\n+        compact_inline_tables: false, // leave whitespace\n+        compact_entries: false,       // leave whitespace\n+        column_width: opt.column_width,\n         indent_tables: false,\n         indent_entries: false,\n         inline_table_expand: true,\n@@ -316,4 +316,47 @@ mod tests {\n         let second = format_toml(got.as_str(), &settings);\n         assert_eq!(second, got);\n     }\n+\n+    /// Test that the column width is respected,\n+    /// and that arrays are neither exploded nor collapsed without reason\n+    #[rstest]\n+    fn test_column_width() {\n+        let start = indoc! {r#\"\n+        [build-system]\n+        build-backend = \"backend\"\n+        requires = [\"c>=1.5\", \"d == 2\" ]\n+\n+        [project]\n+        name = \"beta\"\n+        dependencies = [\n+        \"e>=1.5\",\n+        ]\n+        \"#};\n+        let settings = Settings {\n+            column_width: 80,\n+            indent: 4,\n+            keep_full_version: false,\n+            max_supported_python: (3, 12),\n+            min_supported_python: (3, 12),\n+        };\n+        let got = format_toml(start, &settings);\n+        let expected = indoc! {r#\"\n+        [build-system]\n+        build-backend = \"backend\"\n+        requires = [ \"c>=1.5\", \"d==2\" ]\n+\n+        [project]\n+        name = \"beta\"\n+        classifiers = [\n+            \"Programming Language :: Python :: 3 :: Only\",\n+            \"Programming Language :: Python :: 3.12\",\n+        ]\n+        dependencies = [\n+            \"e>=1.5\",\n+        ]\n+        \"#};\n+        assert_eq!(got, expected);\n+        let second = format_toml(got.as_str(), &settings);\n+        assert_eq!(second, got);\n+    }\n }\n", "instance_id": "tox-dev__pyproject-fmt-rust-43", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in expressing the desired formatting changes for TOML configuration files, specifically regarding how arrays and lists should be formatted (e.g., short lists on a single line, empty lists not expanded). It provides examples of the current and desired output, which helps in understanding the goal. However, there are minor ambiguities: the statement does not explicitly define what constitutes a \"short list\" beyond subjective terms like \"always very short,\" nor does it address potential edge cases such as nested structures or conflicts with other formatting rules. Additionally, the problem lacks explicit constraints or requirements for when to apply these rules universally versus conditionally. Despite these minor gaps, the intent and examples make the problem mostly comprehensible.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves modifications across a few files in a Rust codebase, specifically altering formatting logic in the TOML parser/formatter (e.g., `array.rs` and `main.rs`). The changes are not trivial as they require understanding the syntax tree manipulation and formatting options within the codebase, such as detecting multiline arrays and adjusting newline insertions. Second, the technical concepts involved include familiarity with Rust's syntax tree handling (likely using a library like `syn` or a custom parser), conditional logic for formatting rules, and configuration settings for formatting behavior. Third, while the problem does not explicitly mention complex edge cases, the code changes suggest handling variations in array structures (e.g., multiline vs. single-line) and ensuring consistency with column width settings, which adds moderate complexity to error handling and testing (as seen in the added test case in `main.rs`). However, the impact on the overall architecture is limited, as this is primarily a formatting tweak rather than a structural change. The amount of code change is relatively small but requires precise logic adjustments. Overall, this problem requires a moderate level of understanding and effort, fitting within the 0.4-0.6 range, and I\u2019ve assigned a score of 0.45 to reflect the balance between the focused scope and the need for careful implementation.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deprecate the shapely.geos module\nTo do item for Shapely 1.8: find a way to raise deprecation warnings for functionality in the `shapely.geos` module (as this will be completely removed in Shapely 2.0). \r\nAlthough it's not really documented / recommended, it will be used in practice I think.\r\n\r\nEg the WKTReader / WKTWriter (eg https://stackoverflow.com/a/29548439/653364), the `lgeos` object to directly access GEOS functions (eg https://gis.stackexchange.com/a/226912/9828, we also did this in geopandas https://github.com/geopandas/geopandas/blob/8be1fd8b2a0b7691031d59b3a54275304db8aaa9/geopandas/io/sql.py#L58)\n", "patch": "diff --git a/CHANGES.txt b/CHANGES.txt\nindex c14f04e11..631394e03 100644\n--- a/CHANGES.txt\n+++ b/CHANGES.txt\n@@ -42,6 +42,12 @@ Breaking changes in GEOS 3.12:\n - The WKT representation of a MULTIPOINT changed from for example \"MULTIPOINT (0 0, 1 1)\"\n   to \"MULTIPOINT ((0 0), (1 1))\". (#1885)\n \n+Deprecations:\n+\n+- The ``shapely.geos`` module is deprecated. All GEOS-version related attributes are\n+  available directly from the top-level ``shapely`` namespace as well (already since\n+  shapely 2.0) (#2145).\n+\n Packaging:\n \n - Binary wheels are now built for musllinux (Alpine) x86_64 platforms (#1996).\ndiff --git a/shapely/geos.py b/shapely/geos.py\nindex 5fa3bf227..3667365ef 100644\n--- a/shapely/geos.py\n+++ b/shapely/geos.py\n@@ -1,7 +1,17 @@\n \"\"\"Proxies for libgeos, GEOS-specific exceptions, and utilities.\"\"\"\n \n+import warnings\n+\n import shapely\n \n+warnings.warn(\n+    \"The 'shapely.geos' module is deprecated, and will be removed in a future version. \"\n+    \"All attributes of 'shapely.geos' are available directly from the top-level \"\n+    \"'shapely' namespace (since shapely 2.0.0).\",\n+    DeprecationWarning,\n+    stacklevel=2,\n+)\n+\n geos_version_string = shapely.geos_capi_version_string\n geos_version = shapely.geos_version\n geos_capi_version = shapely.geos_capi_version\n", "instance_id": "shapely__shapely-2145", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to deprecate the `shapely.geos` module and raise deprecation warnings for its usage, as it will be removed in a future version (Shapely 2.0). The goal is explicitly stated, and references to real-world usage (via Stack Overflow and GitHub links) provide context on how the module is used in practice. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify how the deprecation warning should be implemented (e.g., should it be raised on import or on specific function calls?), nor does it mention any specific edge cases or constraints to consider during implementation. Additionally, while the intent to move attributes to the top-level `shapely` namespace is implied, it is not explicitly detailed in the problem statement itself (though it is clarified in the code changes). Overall, the statement is valid and mostly clear but lacks some precision in requirements and implementation details.", "difficulty_explanation": "The difficulty of this task is relatively low, falling into the \"Easy\" range (0.2-0.4). The scope of the code changes is minimal, involving only two files: a documentation update in `CHANGES.txt` and a small modification to `shapely/geos.py` to add a deprecation warning using Python's `warnings` module. The changes are localized and do not impact the broader architecture of the codebase, nor do they require deep understanding of complex interactions between modules. The technical concepts involved are straightforward\u2014primarily the use of Python's `warnings.warn()` function to issue a `DeprecationWarning` on module import. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic Python standard library usage are required. Edge cases and error handling are not a significant concern here, as the task is simply to notify users of deprecation without altering the module's functionality. The primary challenge might be ensuring the warning is visible to users at the right time (e.g., on import), but this is a minor consideration given the simplicity of the implementation. Overall, this is a straightforward task that requires minimal effort and understanding of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fixed bugs in repositories/manganato.py\n## Issues with the Manganato repository source\r\n\r\n### Issues\r\n\r\n1. In the ``get`` method, the format string \"%b %d,%Y - %I:%M %p\" expects the hour to be in the 12-hour format. However, Manganato displays time in the 24-hour format, which triggers a ValueError exception.\r\n\r\n```python\r\n\r\nfrom enma import Enma, DefaultAvailableSources\r\n\r\nenma = Enma[DefaultAvailableSources]()\r\nenma.source_manager.set_source(\"manganato\")\r\n\r\ntest = enma.get(identifier=\"manga-um997395\")\r\n\r\nr\"\"\"\r\n  File \"D:\\Archivos de Usuario\\Escritorio\\Stuff\\Projects\\external\\Enma\\enma\\infra\\adapters\\repositories\\manganato.py\", line 152, in get\r\n    created_at=datetime.strptime(updated_at, \"%b %d,%Y - %I:%M %p\") if updated_at else None,\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Archivos de Programas\\Python 3.11.2\\Lib\\_strptime.py\", line 568, in _strptime_datetime\r\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Archivos de Programas\\Python 3.11.2\\Lib\\_strptime.py\", line 349, in _strptime\r\n    raise ValueError(\"time data %r does not match format %r\" %\r\nValueError: time data 'Mar 08,2024 - 14:47 PM' does not match format '%b %d,%Y - %I:%M %p'\r\n\"\"\"\r\n\r\n```\r\n\r\n2. The ``__create_title`` method assumes that the manga title will contain at least two alternatives. However, certain posts in Manganato have a singular alternative. Trying to unpack the values triggers a ValueError exception.\r\n\r\n```python\r\nfrom enma import Enma, DefaultAvailableSources\r\n\r\nenma = Enma[DefaultAvailableSources]()\r\nenma.source_manager.set_source(\"manganato\")\r\n\r\ntest = enma.get(identifier=\"manga-dj980492\")\r\n\r\nr\"\"\"\r\n  File \"D:\\Archivos de Usuario\\Escritorio\\Stuff\\Projects\\external\\Enma\\enma\\infra\\adapters\\repositories\\manganato.py\", line 124, in get\r\n    title = self.__create_title(main_title=elem_title,\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Archivos de Usuario\\Escritorio\\Stuff\\Projects\\external\\Enma\\enma\\infra\\adapters\\repositories\\manganato.py\", line 48, in __create_title\r\n    jp, cn, *_ = alternative.split(';') if alternative.find(';') != -1 else alternative.split(',')\r\n    ^^^^^^^^^^\r\nValueError: not enough values to unpack (expected at least 2, got 1)\r\n\"\"\"\r\n```\r\n\r\n### Proposed solutions\r\n\r\n1. Change the format string from \"%b %d,%Y - %I:%M %p\" to \"%b %d,%Y - %H:%M %p\" to accept 24-hour formatted data.\r\n\r\n2. Add a check to see whether there are no multiple available alternative strings, and act accordingly.\r\n\r\n```python\r\nhas_many_alternatives = alternative.find(';') != -1 or alternative.find(',') != -1\r\n\r\nif not has_many_alternatives:\r\n    jp = alternative\r\n    return Title(english=main_title.strip(),\r\n                 japanese=jp.strip(),\r\n                 other=main_title.strip())\r\n```\r\n\n", "patch": "diff --git a/.python-version b/.python-version\nindex ac957df..92536a9 100644\n--- a/.python-version\n+++ b/.python-version\n@@ -1,1 +1,1 @@\n-3.10.6\n+3.12.0\ndiff --git a/.vscode/launch.json b/.vscode/launch.json\nnew file mode 100644\nindex 0000000..7430640\n--- /dev/null\n+++ b/.vscode/launch.json\n@@ -0,0 +1,15 @@\n+{\n+    // Use IntelliSense to learn about possible attributes.\n+    // Hover to view descriptions of existing attributes.\n+    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n+    \"version\": \"0.2.0\",\n+    \"configurations\": [\n+        {\n+            \"name\": \"Enma\",\n+            \"type\": \"debugpy\",\n+            \"request\": \"launch\",\n+            \"program\": \"/home/alexandresenpai/scripts/Enma/main.py\",\n+            \"console\": \"integratedTerminal\"\n+        }\n+    ]\n+}\n\\ No newline at end of file\ndiff --git a/.vscode/settings.json b/.vscode/settings.json\nindex ae93530..9426d01 100644\n--- a/.vscode/settings.json\n+++ b/.vscode/settings.json\n@@ -1,4 +1,9 @@\n {\n     \"python.analysis.typeCheckingMode\": \"basic\",\n-    \"cloudcode.duetAI.inlineSuggestions.enableAuto\": false\n+    \"cloudcode.duetAI.inlineSuggestions.enableAuto\": false,\n+    \"python.testing.pytestArgs\": [\n+        \"tests\"\n+    ],\n+    \"python.testing.unittestEnabled\": false,\n+    \"python.testing.pytestEnabled\": true\n }\n\\ No newline at end of file\ndiff --git a/README.md b/README.md\nindex 3090fb8..9412960 100644\n--- a/README.md\n+++ b/README.md\n@@ -18,6 +18,9 @@\n \n Enma is a Python library designed to fetch manga and doujinshi data from many sources. It provides a unified interface to interact with different manga repositories, making it easier to retrieve manga details, search for manga, paginate through results, and fetch random manga.\n \n+## :warning: Warning\n+> **:exclamation: Important: Enma is not intended for mass querying or placing heavy loads on supported sources. Please use responsibly, adhering to the terms of service of the data sources. Misuse may result in service disruption or access denial.**\n+\n ## Requirements\n \n - Python 3.9+\n@@ -41,17 +44,20 @@ except AssertionError:\n     raise RuntimeError(f\"{package_name!r} requires Python {python_major}.{python_minor}+ (You have Python {sys.version})\")\n ```\n \n+## Documentation\n+You can consult full Enma documentation at <strong><a href=\"https://enma.gitbook.io/enma\" target=\"_blank\">https://enma.gitbook.io/enma</a></strong>.\n+\n ## Features Comparison\n \n-Feature    | NHentai | Manganato\n------------|---------|-----------\n-search     |    \u2705   |     \u2705    \n-random     |    \u2705   |     \ud83d\udeab    \n-get        |    \u2705   |     \u2705    \n-paginate   |    \u2705   |     \u2705\n-download   |    \u2705   |     \u2705       \n-author_page|    \u2705   |     \ud83d\udeab       \n-set_config |    \u2705   |     \ud83d\udeab\n+Feature    | NHentai | Manganato | Mangadex\n+-----------|---------|-----------|-----------\n+search     |    \u2705   |     \u2705   |   \u2705\n+random     |    \u2705   |     \ud83d\udeab   |   \u2705  \n+get        |    \u2705   |     \u2705   |   \u2705\n+paginate   |    \u2705   |     \u2705   |   \u2705\n+download   |    \u2705   |     \u2705   |   \u2705  \n+author_page|    \u2705   |     \ud83d\udeab   |   \ud83d\udeab   \n+set_config |    \u2705   |     \ud83d\udeab   |   \ud83d\udeab\n \n ## Usage\n \n@@ -256,6 +262,18 @@ We welcome contributions! If you'd like to contribute:\n \n Ensure you follow the coding standards and write tests for new features.\n \n+## Disclaimer\n+\n+This software is provided \"as is\", without warranty of any kind, express or implied. The developers and contributors of the Enma library shall not be liable for any misuse, damages, or other consequences arising from the use of this software.\n+\n+It is important to emphasize that the Enma library was developed with the goal of facilitating efficient and responsible access and manipulation of data. We do not encourage or support the use of this tool for conducting mass queries or accesses that could overload, harm, or in any way negatively affect the servers or services of the supported sources.\n+\n+Users of the Enma library must always follow the guidelines, terms of use, and limitations imposed by the accessed data sources. We strongly recommend the implementation of responsible rate limiting practices and obtaining appropriate permissions when necessary, to ensure that the use of the library complies with all applicable laws and regulations, in addition to respecting ethical principles of data use.\n+\n+By using the Enma library, you agree to use the tool in an ethical and responsible manner, acknowledging that the developers of Enma will not be responsible for any use that violates these guidelines.\n+\n+We remind you that respect for the services and APIs of the supported sources is fundamental for the sustainability and longevity of both the Enma and the services used. We value the community and the development ecosystem and encourage all users to contribute to a safer, more respectful, and collaborative digital environment.\n+\n ## License\n \n MIT\ndiff --git a/enma/__init__.py b/enma/__init__.py\nindex 2d43d74..0d2af74 100644\n--- a/enma/__init__.py\n+++ b/enma/__init__.py\n@@ -1,18 +1,20 @@\n import sys\n from enma.application.core.utils.logger import LogMode, logger\n from enma.application.use_cases.download_chapter import Threaded\n-from enma.infra.entrypoints.lib import Enma, SourcesEnum, DefaultAvailableSources\n-from enma.infra.adapters.repositories.nhentai import CloudFlareConfig, NHentai, Sort\n+from enma.infra.entrypoints.lib import Enma, SourcesEnum\n+from enma.infra.adapters.repositories.nhentai import CloudFlareConfig, NHentai, Sort as NHentaiSort\n+from enma.infra.adapters.repositories.mangadex import Mangadex, Sort as MangadexSort\n from enma.infra.adapters.repositories.manganato import Manganato\n from enma.infra.adapters.downloaders.default import DefaultDownloader\n from enma.infra.adapters.downloaders.manganato import ManganatoDownloader\n from enma.application.core.interfaces.downloader_adapter import IDownloaderAdapter\n from enma.application.core.interfaces.saver_adapter import ISaverAdapter\n from enma.infra.adapters.storage.local import LocalStorage\n-from enma.domain.entities.manga import Manga\n+from enma.domain.entities.manga import Manga, Chapter, SymbolicLink\n from enma.domain.entities.search_result import SearchResult\n from enma.domain.entities.pagination import Pagination\n from enma.domain.entities.author_page import AuthorPage\n+from enma.application.core.interfaces.manga_repository import IMangaRepository\n \n package_name = \"enma\"\n python_major = \"3\"\ndiff --git a/enma/_version.py b/enma/_version.py\nindex ba0fb3f..54660c3 100644\n--- a/enma/_version.py\n+++ b/enma/_version.py\n@@ -1,1 +1,1 @@\n-__version__ = '2.3.0'\n\\ No newline at end of file\n+__version__ = '2.4.0'\n\\ No newline at end of file\ndiff --git a/enma/application/core/handlers/error.py b/enma/application/core/handlers/error.py\nindex a409f10..8b729d2 100644\n--- a/enma/application/core/handlers/error.py\n+++ b/enma/application/core/handlers/error.py\n@@ -7,6 +7,30 @@ def __init__(self, message: str) -> None:\n         self.desc: str = 'This error occurs when the client chooses nonexistent source.'\n         self.critical: bool = False\n \n+class Unknown(Exception):\n+    def __init__(self, message: str) -> None:\n+        super().__init__(message)\n+\n+        self.code: str = 'UNKNOWN'\n+        self.desc: str = 'This error occours when was not possible to determine the error root cause.'\n+        self.critical: bool = True\n+\n+class NotFound(Exception):\n+    def __init__(self, message: str) -> None:\n+        super().__init__(message)\n+\n+        self.code: str = 'NOT_FOUND'\n+        self.desc: str = 'This error occours when was not possible to find the requested resource.'\n+        self.critical: bool = True\n+\n+class Forbidden(Exception):\n+    def __init__(self, message: str) -> None:\n+        super().__init__(message)\n+\n+        self.code: str = 'FORBIDDEN'\n+        self.desc: str = 'This error occours when the client can\\'t perform a request to the source due lack of credentials.'\n+        self.critical: bool = True\n+\n class InvalidRequest(Exception):\n     def __init__(self, message: str) -> None:\n         super().__init__(message)\n@@ -67,4 +91,13 @@ def __init__(self, message: str) -> None:\n         self.message: str = message\n         self.code: str = 'EXCEED_RETRY_COUNT'\n         self.desc: str = 'This error occurs when enma tries perform some action but something went wrong.'\n-        self.critical: bool = True\n\\ No newline at end of file\n+        self.critical: bool = True\n+\n+class ExceedRateLimit(Exception):\n+    def __init__(self, message: str) -> None:\n+        super().__init__(message)\n+\n+        self.message: str = message\n+        self.code: str = 'EXCEED_RATE_EXCEED'\n+        self.desc: str = 'This error occurs when enma perform more requests than a server can handle. Cool down your requests to this source!'\n+        self.critical: bool = False\n\\ No newline at end of file\ndiff --git a/enma/application/use_cases/get_manga.py b/enma/application/use_cases/get_manga.py\nindex a0c0c19..ba78f9c 100644\n--- a/enma/application/use_cases/get_manga.py\n+++ b/enma/application/use_cases/get_manga.py\n@@ -2,7 +2,7 @@\n from typing import Union\n \n from pydantic import BaseModel, Field, validator\n-from enma.application.core.handlers.error import InvalidRequest\n+from enma.application.core.handlers import error\n from enma.application.core.interfaces.manga_repository import IMangaRepository\n from enma.application.core.interfaces.use_case import DTO, IUseCase\n from enma.application.core.utils.logger import logger\n@@ -24,9 +24,10 @@ def __init__(self, manga_repository: IMangaRepository):\n     def execute(self, dto: DTO[GetMangaRequestDTO]) -> GetMangaResponseDTO:\n         logger.info(f'Fetching manga with identifier: {dto.data.identifier}.')\n \n-        manga = self.__manga_repository.get(identifier=dto.data.identifier,\n-                                            with_symbolic_links=dto.data.with_symbolic_links)\n-        \n-        if manga is None: return GetMangaResponseDTO(found=False, manga=None)\n-        \n-        return GetMangaResponseDTO(found=True, manga=manga)\n\\ No newline at end of file\n+        try:\n+            manga = self.__manga_repository.get(identifier=dto.data.identifier,\n+                                                with_symbolic_links=dto.data.with_symbolic_links)\n+            return GetMangaResponseDTO(found=True, manga=manga)\n+        except error.NotFound:\n+            logger.error(f'Could not find the manga using the provided identifier: {dto.data.identifier}')\n+            return GetMangaResponseDTO(found=False, manga=None)\n\\ No newline at end of file\ndiff --git a/enma/domain/entities/base.py b/enma/domain/entities/base.py\nindex 64762c5..fbfe30a 100644\n--- a/enma/domain/entities/base.py\n+++ b/enma/domain/entities/base.py\n@@ -1,6 +1,6 @@\n from datetime import datetime\n-import json\n from typing import Generic, TypeVar, Union\n+from uuid import uuid4\n \n T = TypeVar('T')\n \n@@ -23,12 +23,12 @@ def __init__(self,\n         \"\"\"Initializes an Entity with given or default values.\n \n         Args:\n-            id: A Union of int, str, and None representing the entity's ID. Defaults to 0.\n+            id: A Union of int, str, and None representing the entity's ID. Defaults to uuidv4.\n             created_at: A Union of datetime and None representing when the entity was created. Defaults to current UTC time.\n             updated_at: A Union of datetime and None representing when the entity was last updated. Defaults to current UTC time.\n         \"\"\"\n \n-        self.id = id if id is not None else 0\n+        self.id = id if id is not None else uuid4()\n         self.created_at = created_at if created_at is not None else datetime.utcnow()\n         self.updated_at = updated_at if updated_at is not None else datetime.utcnow()\n \ndiff --git a/enma/domain/entities/manga.py b/enma/domain/entities/manga.py\nindex f2a777c..8cbfadd 100644\n--- a/enma/domain/entities/manga.py\n+++ b/enma/domain/entities/manga.py\n@@ -1,14 +1,18 @@\n from dataclasses import dataclass, field\n from datetime import datetime\n from enum import Enum\n-from typing import TypedDict, Union\n+from typing import Literal, TypedDict, Union\n \n from enma.domain.entities.base import Entity\n \n class MIME(Enum):\n+    JPG = 'jpg'\n     J = 'jpg'\n+    PNG = 'png'\n     P = 'png'\n+    GIF = 'gif'\n     G = 'gif'\n+    \n \n @dataclass\n class Image:\n@@ -59,6 +63,55 @@ class Genre:\n class Author(Genre):\n     ...\n \n+class ILanguage(TypedDict):\n+    ja: Literal['japanese']\n+    jp: Literal['japanese']\n+    japanese: Literal['japanese']\n+    portuguese: Literal['portuguese']\n+    pt: Literal['portuguese']\n+    pt_br: Literal['portuguese']\n+    english: Literal['english']\n+    en: Literal['english']\n+    en_us: Literal['english']\n+    chinese: Literal['chinese']\n+    cn: Literal['chinese']\n+    zh: Literal['chinese']\n+    russian: Literal['russian']\n+    ru: Literal['russian']\n+    turkish: Literal['turkish']\n+    tr: Literal['turkish']\n+    spanish: Literal['spanish']\n+    es_la: Literal['spanish']\n+    malay: Literal['malay']\n+    ms: Literal['malay']\n+    korean: Literal['korean']\n+    ko: Literal['korean']\n+\n+Language: ILanguage = {\n+    'ja': 'japanese',\n+    'jp': 'japanese',\n+    'japanese': 'japanese',\n+    'portuguese': 'portuguese',\n+    'pt': 'portuguese',\n+    'pt_br': 'portuguese',\n+    'english': 'english',\n+    'en': 'english',\n+    'en_us': 'english',\n+    'chinese': 'chinese',\n+    'cn': 'chinese',\n+    'zh': 'chinese',\n+    'ru': 'russian',\n+    'russian': 'russian',\n+    'turkish': 'turkish',\n+    'tr': 'turkish',\n+    'spanish': 'spanish',\n+    'es_la': 'spanish',\n+    'malay': 'malay',\n+    'ms': 'malay',\n+    'korean': 'korean',\n+    'ko': 'korean'\n+}\n+    \n class Manga(Entity[IMangaProps]):\n     def __init__(self,\n                  title: Title,\n@@ -85,3 +138,8 @@ def __init__(self,\n         self.chapters = chapters or []\n \n         self.chapters_count = len(self.chapters if self.chapters else [])\n+\n+    def add_chapter(self,\n+                    chapter: Chapter):\n+        self.chapters.append(chapter)\n+        self.chapters_count += 1\ndiff --git a/enma/infra/adapters/repositories/mangadex.py b/enma/infra/adapters/repositories/mangadex.py\nnew file mode 100644\nindex 0000000..abf1d37\n--- /dev/null\n+++ b/enma/infra/adapters/repositories/mangadex.py\n@@ -0,0 +1,540 @@\n+\"\"\"\n+This module provides an adapter for the mangadex repository.\n+It contains functions and classes to interact with the mangadex API and retrieve manga data.\n+\"\"\"\n+from datetime import datetime\n+from enum import Enum\n+from typing import Any, Optional, Union, cast\n+from urllib.parse import urljoin, urlparse\n+\n+from requests import Response\n+\n+import requests\n+\n+from enma.application.core.handlers.error import (ExceedRateLimit, \n+                                                  Forbidden, \n+                                                  NotFound, \n+                                                  Unknown)\n+from enma.application.core.interfaces.manga_repository import IMangaRepository\n+from enma.application.core.utils.logger import logger\n+from enma.domain.entities.author_page import AuthorPage\n+from enma.domain.entities.manga import (MIME, \n+                                        Chapter, \n+                                        Genre, \n+                                        Author, \n+                                        Image, \n+                                        Language, \n+                                        Manga, \n+                                        SymbolicLink,\n+                                        Title)\n+from enma.domain.entities.search_result import Pagination, SearchResult, Thumb\n+from enma.infra.core.interfaces.mangadex_response import (AuthorRelation, \n+                                                          CoverArtRelation, IAltTitles, \n+                                                          IGetResult, \n+                                                          IHash, IManga, IMangaTag, IRelations, \n+                                                          ISearchResult, \n+                                                          IVolumesResponse)\n+\n+\n+class Sort(Enum):\n+    ALL_TIME = 'relevance'\n+    RECENT = 'createdAt'\n+\n+class Mangadex(IMangaRepository):\n+    \"\"\"\n+    Repository class for interacting with the Mangadex API.\n+    Provides methods to fetch manga details, search for manga, etc.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self.__API_URL = 'https://api.mangadex.org/'\n+        self.__COVER_URL = 'https://mangadex.org/covers/'\n+        self.__HASH_URL = 'https://api.mangadex.org/at-home/server/'\n+        self.__CHAPTER_PAGE_URL = 'https://cmdxd98sb0x3yprd.mangadex.network/data/'\n+\n+    def __handle_source_response(self, response: Response):\n+        \"\"\"\n+        Evaluates the HTTP response from the Mangadex API, raising specific exceptions based on the HTTP status code\n+        to indicate various error conditions such as rate limits exceeded, forbidden access, or resource not found.\n+\n+        Args:\n+            response (Response): The HTTP response object from a request to the Mangadex API.\n+\n+        Raises:\n+            Forbidden: Indicates a 403 Forbidden HTTP status code.\n+            NotFound: Indicates a 404 Not Found HTTP status code.\n+            ExceedRateLimit: Indicates a 429 Too Many Requests HTTP status code.\n+            Unknown: Indicates any other unexpected HTTP status code.\n+        \"\"\"\n+\n+        logger.debug(f'Fetched {response.url} with response status code {response.status_code} and text {response.text}')\n+\n+        if response.status_code == 200:\n+            return\n+        if response.status_code == 403:\n+            raise Forbidden(message='Could not perform a successful request to the source due to credentials issues. Check your credentials and try again.')\n+        if response.status_code == 404:\n+            raise NotFound(message=f'Could not find the requested resource at \"{response.url}\". Check the provided request parameters and try again.')\n+        if response.status_code == 429:\n+            raise ExceedRateLimit(message='You have exceeded the Mangadex rate limit!')\n+        raise Unknown(message='Something unexpected happened while trying to fetch source content. Set the logging mode to debug and try again.')\n+\n+    def __make_request(self,\n+                       url: str,\n+                       headers: Union[dict[str, Any], None] = None,\n+                       params: Optional[Union[dict[str, Union[str, int]], list[tuple[str, Union[str, int]]]]] = None) -> requests.Response:\n+        \"\"\"\n+        Makes a request to the specified URL with the given headers and parameters.\n+\n+        Args:\n+            url (str): The URL to make the request to.\n+            headers (dict[str, Any], optional): The headers to include in the request. Defaults to None.\n+            params (Optional[Union[dict[str, Union[str, int]], list[tuple[str, Union[str, int]]]]], optional): The parameters to include in the request. Defaults to None.\n+\n+        Returns:\n+            requests.Response: The response object from the API request.\n+        \"\"\"\n+        headers = headers if headers is not None else {}\n+        params = params if params is not None else {}\n+\n+        logger.debug(f'Fetching {url} with headers {headers} and params {params}')\n+\n+        response = requests.get(url=urlparse(url).geturl(),\n+                                headers={**headers, \"User-Agent\": \"Enma/2.4.0\"},\n+                                params=params)\n+        \n+        self.__handle_source_response(response)\n+\n+        return response\n+\n+    def set_config(self, config) -> None:\n+        raise NotImplementedError('Manganato does not support set config')\n+\n+    def __create_cover_uri(self, \n+                           manga_id: str, \n+                           file_name: str) -> str:\n+        \"\"\"\n+        Constructs a URL for a manga's cover image based on its identifier and the file name of the cover image.\n+\n+        Args:\n+            manga_id (str): The unique identifier of the manga.\n+            file_name (str): The file name of the cover image.\n+\n+        Returns:\n+            str: The fully qualified URL to the cover image.\n+        \"\"\"\n+        return urljoin(self.__COVER_URL, f'{manga_id}/{file_name}.512.jpg')\n+    \n+    def fetch_chapter_by_symbolic_link(self, \n+                                       link: SymbolicLink) -> Chapter:\n+        \"\"\"\n+        Retrieves manga chapter details including pages and images by following a symbolic link. This method is particularly\n+        useful for fetching chapters that have been directly linked.\n+\n+        Args:\n+            link (SymbolicLink): An object representing the symbolic link to the chapter.\n+\n+        Returns:\n+            Chapter: An object containing the fetched chapter details such as pages and images.\n+        \"\"\"\n+        response = self.__make_request(url=link.link)\n+        \n+        ch: IHash = response.json()\n+        chapter = Chapter()\n+    \n+        for index, page in enumerate(ch.get('chapter').get('data')):\n+            extension = page.split('.')[-1]\n+            chapter.add_page(Image(uri=self.__create_chapter_page_uri(ch.get('chapter').get('hash'), page),\n+                                   name=f'{index}.{extension}',\n+                                   mime=MIME[extension.upper()]))\n+        return chapter\n+\n+    def __fetch_chapter_hashes(self, chapter_id: str) -> tuple[str, list[str]]:\n+        \"\"\"\n+        Fetches the chapter hashes and page file names for a given chapter ID. These details are necessary to construct\n+        the URLs for individual chapter pages.\n+\n+        Args:\n+            chapter_id (str): The unique identifier of the chapter.\n+\n+        Returns:\n+            tuple[str, list[str]]: A tuple containing the chapter hash and a list of page file names.\n+        \"\"\"\n+        response = self.__make_request(url=urljoin(self.__HASH_URL, chapter_id))\n+        data: IHash = response.json()\n+\n+        return (data.get('chapter').get('hash'), \n+                data.get('chapter').get('data'))\n+    \n+    def __create_chapter_page_uri(self, hash: str, filename: str) -> str:\n+        \"\"\"\n+        Constructs the URL for a chapter page given the chapter hash and the page file name.\n+\n+        Args:\n+            hash (str): The hash of the chapter, used as part of the URL path.\n+            filename (str): The file name of the chapter page.\n+\n+        Returns:\n+            str: The fully qualified URL to the chapter page.\n+        \"\"\"\n+        return urljoin(self.__CHAPTER_PAGE_URL, f'{hash}/{filename}')\n+\n+    def __create_chapter(self,\n+                         chapter: tuple[int, str],\n+                         with_symbolic_links: bool = False) -> Chapter:\n+        \"\"\"\n+        Constructs a Chapter object for a given chapter tuple, optionally using symbolic links. If symbolic links are\n+        used, chapter pages are not pre-fetched but are instead represented as links.\n+\n+        Args:\n+            chapter (tuple[int, str]): A tuple containing the chapter number and the chapter ID.\n+            with_symbolic_links (bool, optional): A flag indicating whether to use symbolic links for chapter pages. Defaults to False.\n+\n+        Returns:\n+            Chapter: The constructed Chapter object.\n+        \"\"\"        \n+        curr_chapter, chapter_id = chapter\n+\n+        if with_symbolic_links:\n+            return Chapter(id=curr_chapter, \n+                           link=SymbolicLink(link=urljoin(self.__HASH_URL, chapter_id)))\n+        else:\n+            ch = Chapter(id=curr_chapter)\n+            hash, files = self.__fetch_chapter_hashes(chapter_id)\n+        \n+            for index, page in enumerate(files):\n+                extension = page.split('.')[-1]\n+                ch.add_page(Image(uri=self.__create_chapter_page_uri(hash, page),\n+                                  name=f'{index}.{extension}',\n+                                  mime=MIME[extension.upper()]))\n+        \n+            return ch\n+    \n+    def __list_chapters(self, manga_id: str) -> list[tuple[int, str]]:\n+        \"\"\"\n+        Retrieves a list of chapters for a given manga ID. Each chapter is represented as a tuple containing the chapter\n+        number and the chapter ID.\n+\n+        Args:\n+            manga_id (str): The unique identifier of the manga.\n+\n+        Returns:\n+            list[tuple[int, str]]: A list of tuples, each representing a chapter of the manga.\n+        \"\"\"\n+        response = self.__make_request(url=urljoin(self.__API_URL, f'manga/{manga_id}/aggregate'))\n+        \n+        data: IVolumesResponse = response.json()\n+\n+        chapters = []\n+        for volume in data.get('volumes'):\n+            volume = data.get('volumes').get(volume)\n+            \n+            if volume is None: continue\n+\n+            volume_chapters = volume.get('chapters')\n+            \n+            for volume_key in volume_chapters:\n+                current_vol = volume_chapters.get(volume_key)\n+\n+                if current_vol is None: continue\n+\n+                chapters.append((current_vol.get('chapter'), current_vol.get('id')))\n+        \n+        return chapters\n+    \n+    def __extract_authors(self, relations: IRelations) -> list[Author]:\n+        \"\"\"\n+        Extracts author information from a list of relationships within manga metadata, constructing Author objects\n+        for each author found.\n+\n+        Args:\n+            relations (IRelations): A list of relationship objects from the manga metadata.\n+\n+        Returns:\n+            list[Author]: A list of Author objects extracted from the relationships.\n+        \"\"\"\n+        authors_data = [relationship for relationship in relations if relationship.get('type') == 'author']\n+        authors: list[Author] = []\n+\n+        if len(authors_data) > 0:\n+            for author in authors_data:\n+                author = cast(AuthorRelation, author)\n+                authors.append(Author(name=author.get('attributes').get('name'),\n+                                      id=author.get('id')))\n+                \n+        return authors\n+    \n+    def __extract_genres(self, tags: list[IMangaTag]) -> list[Genre]:\n+        \"\"\"\n+        Extracts genre information from a list of tags within manga metadata, constructing Genre objects for each tag\n+        that represents a genre.\n+\n+        Args:\n+            tags (list[IMangaTag]): A list of tag objects from the manga metadata.\n+\n+        Returns:\n+            list[Genre]: A list of Genre objects extracted from the tags.\n+        \"\"\"\n+        return [Genre(id=tag.get('id'),\n+                      name=tag.get('attributes', {}).get('name', {}).get('en', 'unknown')) \n+                    for tag in tags or []\n+                    if tag.get('type') == 'tag']\n+    \n+    def __get_cover(self, \n+                    manga_id: str, \n+                    relations: IRelations) -> Image:\n+        \"\"\"\n+        Retrieves the cover image for a given manga ID from a list of relationships. If a cover image is found, an\n+        Image object is constructed and returned.\n+\n+        Args:\n+            manga_id (str): The unique identifier of the manga.\n+            relations (IRelations): A list of relationship objects from the manga metadata.\n+\n+        Returns:\n+            Image: An Image object representing the manga's cover image. Returns an Image object with an empty URI if no cover is found.\n+        \"\"\"\n+        covers = [tag for tag in relations if tag.get('type') == 'cover_art']\n+        if len(covers) == 0: return Image(uri='')\n+        cover = cast(CoverArtRelation, covers[0])\n+        return Image(uri=self.__create_cover_uri(manga_id, cover.get(\"attributes\").get(\"fileName\")),\n+                     width=512)\n+    \n+    def __get_title(self, alt_titles: IAltTitles, title: str) -> Title:\n+        \"\"\"\n+        Constructs a Title object for the manga, incorporating the English title, a Japanese title if available,\n+        and an alternative title.\n+\n+        Args:\n+            alt_titles (IAltTitles): A list of alternative titles for the manga.\n+            title (str): The primary English title of the manga.\n+\n+        Returns:\n+            Title: A Title object containing the English, Japanese, and an alternative title for the manga.\n+        \"\"\"\n+        japanese_titles = [ title.get('ja-ro') for title in alt_titles if title.get('ja-ro') is not None ]\n+        japanese_title = japanese_titles[0] if len(japanese_titles) > 0 else None\n+\n+        other_keys = list(alt_titles[-1].keys())\n+        other_key = other_keys[0] if len(other_keys) > 0 else ''\n+\n+        return Title(english=title,\n+                     japanese=japanese_title or '',\n+                     other=alt_titles[-1].get(other_key) or '')\n+\n+    def __parse_full_manga(self, \n+                           manga_data: IManga, \n+                           with_symbolic_links: bool = False) -> Manga:\n+        \"\"\"\n+        Parses the complete manga data retrieved from the Mangadex API, constructing a Manga object that includes\n+        details such as title, authors, genres, cover image, and chapters.\n+\n+        Args:\n+            manga_data (IManga): The raw manga data from the Mangadex API.\n+            with_symbolic_links (bool, optional): Indicates whether to use symbolic links for chapters. Defaults to False.\n+\n+        Returns:\n+            Manga: A fully constructed Manga object.\n+        \"\"\"\n+        attrs = manga_data.get('attributes', dict())\n+\n+        thumbnail = self.__get_cover(manga_data.get('id'), \n+                                     manga_data.get('relationships'))\n+        \n+\n+        manga = Manga(title=self.__get_title(alt_titles=attrs.get('altTitles'),\n+                                             title=attrs.get('title', dict()).get('en') or ''),\n+                      id=manga_data.get('id'),\n+                      created_at=datetime.fromisoformat(attrs.get('createdAt')),\n+                      updated_at=datetime.fromisoformat(attrs.get('updatedAt')),\n+                      language=Language.get(attrs.get('originalLanguage').strip().lower().replace('-', '_'), 'unknown'),\n+                      authors=self.__extract_authors(manga_data.get('relationships', list())),\n+                      genres=self.__extract_genres(attrs.get('tags', list())),\n+                      thumbnail=thumbnail,\n+                      cover=thumbnail)\n+        \n+        chapter_list = self.__list_chapters(manga_id=str(manga.id))\n+\n+        for chapter in chapter_list:\n+            manga.add_chapter(self.__create_chapter(chapter=chapter,\n+                                                    with_symbolic_links=with_symbolic_links))\n+            \n+        return manga\n+    \n+    def __parse_thumb(self, manga: IManga) -> Thumb:\n+        \"\"\"\n+        Extracts minimal manga information to construct a Thumb object, primarily used for search results\n+        where detailed information is not necessary.\n+\n+        Args:\n+            manga (IManga): The raw manga data from the Mangadex API.\n+\n+        Returns:\n+            Thumb: A Thumb object containing the manga's ID, title, and cover image.\n+        \"\"\"\n+\n+        title = manga.get('attributes').get('title').get('en')\n+        return Thumb(id=manga.get('id'),\n+                     title=title,\n+                     cover=self.__get_cover(manga_id=manga.get('id'),\n+                                            relations=manga.get('relationships', list())))\n+    \n+    def get(self, \n+            identifier: str,\n+            with_symbolic_links: bool = False) -> Manga:\n+        \"\"\"\n+        Retrieves detailed information for a specific manga identified by its ID, constructing a Manga object.\n+\n+        Args:\n+            identifier (str): The unique identifier of the manga to retrieve.\n+            with_symbolic_links (bool, optional): Indicates whether to construct the Manga object with symbolic links for chapters. Defaults to False.\n+\n+        Returns:\n+            Manga: The Manga object containing detailed information about the specified manga.\n+        \"\"\"\n+        response = self.__make_request(url=urljoin(self.__API_URL, f'manga/{identifier}'),\n+                                       params=[('includes[]', 'cover_art'),\n+                                               ('includes[]', 'author'),\n+                                               ('includes[]', 'artist')])\n+\n+        result: IGetResult = response.json()\n+        manga_data = result.get('data')\n+\n+        return self.__parse_full_manga(manga_data=manga_data,\n+                                       with_symbolic_links=with_symbolic_links)\n+        \n+    def __make_sort_query(self, sort: Sort) -> dict[str, str]:\n+        \"\"\"\n+        Constructs a query parameter dictionary to define the sorting order for search results based on a Sort enumeration value.\n+\n+        Args:\n+            sort (Sort): An enumeration value specifying the desired sort order for search results.\n+\n+        Returns:\n+            dict[str, str]: A dictionary of query parameters to define the sorting order.\n+        \"\"\"\n+        return { f'order[{sort.value if isinstance(sort, Sort) else sort}]': 'desc' }\n+\n+    def search(self,\n+               query: str,\n+               page: int,\n+               sort: Sort = Sort.RECENT,\n+               per_page: int = 25) -> SearchResult:\n+        \"\"\"\n+        Searches the Mangadex API for manga that match the given query string, optionally sorting the results\n+        and paginating them. Constructs and returns a SearchResult object containing the search results.\n+        \n+        Args:\n+            query (str): The search query string.\n+            page (int): The page number of the search results to retrieve.\n+            sort (Sort, optional): The sorting order of the search results. Defaults to Sort.RECENT.\n+            per_page (int, optional): The number of results per page. Defaults to 25.\n+\n+        Returns:\n+            SearchResult: An object containing the paginated search results, including manga thumbnails.\n+        \"\"\"\n+        logger.debug(f'Searching into Mangadex with args query={query};page={page};sort={sort}')\n+\n+        params = [('title', query), *tuple(self.__make_sort_query(sort).items()),\n+                  ('includes[]', 'cover_art'), ('limit', per_page),\n+                  ('offset', per_page * (page - 1) if page > 1 else 0), ('contentRating[]', 'safe'),\n+                  ('contentRating[]', 'suggestive'), ('contentRating[]', 'erotica'),\n+                  ('order[createdAt]', 'desc'), ('hasAvailableChapters', 'true')]\n+        \n+        request_response = self.__make_request(url=urljoin(self.__API_URL, 'manga'),\n+                                               params=params)\n+        \n+        response: ISearchResult = request_response.json()\n+\n+        total_results = response.get('total')\n+        total_pages = int(total_results / per_page)\n+        \n+        search_result = SearchResult(query=query,\n+                                     total_pages=total_pages,\n+                                     page=page,\n+                                     total_results=total_results)\n+        \n+        for result in response.get('data', []):\n+            search_result.results.append(self.__parse_thumb(manga=result))\n+\n+        return search_result\n+\n+    def paginate(self, page: int) -> Pagination:\n+        \"\"\"\n+        Retrieves a specific page of manga listings from the Mangadex API, returning a Pagination object\n+        that includes a list of manga thumbnails for that page.\n+\n+        Args:\n+            page (int): The page number of manga listings to retrieve.\n+\n+        Returns:\n+            Pagination: An object containing the paginated list of manga thumbnails and pagination details.\n+        \"\"\"\n+        logger.debug(f'Paginating with args page={page}')\n+        per_page = 25\n+        request_response = self.__make_request(url=urljoin(self.__API_URL, 'manga'),\n+                                               params=[('limit', per_page),\n+                                                       ('offset', per_page * (page - 1) if page > 1 else 0),\n+                                                       ('order[createdAt]', 'desc'),\n+                                                       ('includes[]', 'cover_art'),\n+                                                       ('contentRating[]', 'safe'),\n+                                                       ('contentRating[]', 'suggestive'),\n+                                                       ('contentRating[]', 'erotica'),\n+                                                       ('order[createdAt]', 'desc'),\n+                                                       ('hasAvailableChapters', 'true')])\n+        \n+        response: ISearchResult = request_response.json()\n+\n+        pagination = Pagination(page=page,\n+                                total_pages=int(response.get('total') / per_page),\n+                                total_results=response.get('total'))\n+\n+        for result in response.get('data', []):            \n+            pagination.results.append(self.__parse_thumb(manga=result))\n+\n+        return pagination\n+\n+    def random(self, retry=0) -> Manga:\n+        \"\"\"\n+        Fetches a random manga from the Mangadex API. If the first attempt fails, it will retry up to a specified number of times.\n+\n+        Args:\n+            retry (int, optional): The number of retries to attempt in case of failure. Defaults to 0.\n+\n+        Returns:\n+            Manga: A Manga object for the randomly selected manga.\n+        \"\"\"\n+        response = self.__make_request(url=urljoin(self.__API_URL, f'manga/random'),\n+                                       params=[('includes[]', 'cover_art'),\n+                                               ('contentRating[]', 'safe'),\n+                                               ('contentRating[]', 'suggestive'),\n+                                               ('contentRating[]', 'erotica'),\n+                                               ('includes[]', 'author'),\n+                                               ('includes[]', 'artist'),\n+                                               ('hasAvailableChapters', 'true')])\n+\n+        result: IGetResult = response.json()\n+\n+        manga = result.get('data')\n+\n+        return self.__parse_full_manga(manga_data=manga,\n+                                       with_symbolic_links=True)\n+    \n+    def author_page(self,\n+                    author: str,\n+                    page: int) -> AuthorPage:\n+        \"\"\"\n+        Fetches manga authored by a specific author. This method is not currently implemented for Mangadex\n+        and serves as a placeholder for potential future functionality.\n+\n+        Args:\n+            author (str): The name or identifier of the author.\n+            page (int): The page number of results to retrieve.\n+\n+        Raises:\n+            NotImplementedError: Indicates that this method is not supported or implemented.\n+\n+        Returns:\n+            AuthorPage: An object containing a list of manga by the specified author. This is currently not implemented.\n+        \"\"\"\n+        raise NotImplementedError('Mangadex does not support author page.')\n\\ No newline at end of file\ndiff --git a/enma/infra/adapters/repositories/manganato.py b/enma/infra/adapters/repositories/manganato.py\nindex 105c435..5a8c116 100644\n--- a/enma/infra/adapters/repositories/manganato.py\n+++ b/enma/infra/adapters/repositories/manganato.py\n@@ -45,12 +45,21 @@ def __create_title(self,\n                        main_title: str, \n                        alternative: str) -> Title:\n         logger.debug(f'Building manga title main: {main_title} and alternative: {alternative}')\n+\n+        has_many_alternatives = alternative.find(';') != -1 or alternative.find(',') != -1\n+\n+        if not has_many_alternatives:\n+            jp = alternative\n+            return Title(english=main_title.strip(),\n+                         japanese=jp.strip(),\n+                         other=main_title.strip())\n+\n         jp, cn, *_ = alternative.split(';') if alternative.find(';') != -1 else alternative.split(',')\n         return Title(english=main_title.strip(),\n                      japanese=jp.strip(),\n                      other=cn.strip())\n     \n-    def __find_chapets_list(self, html: BeautifulSoup) -> list[str]:\n+    def __find_chapters_list(self, html: BeautifulSoup) -> list[str]:\n         chapter_list = cast(Tag, html.find('ul', {'class': 'row-content-chapter'}))\n         chapters = chapter_list.find_all('li') if chapter_list else []\n         return [chapter.find('a')['href'] for chapter in chapters]\n@@ -67,7 +76,6 @@ def __create_chapter(self, url: str, symbolic: bool = False) -> Union[Chapter, N\n             logger.error(f'Could not fetch the chapter with url: {url}. status code: {response.status_code}')\n             return\n         \n-        \n         chapter = Chapter(id=response.url.split('/')[-1])\n         html = BeautifulSoup(response.text, 'html.parser')\n         images_container = cast(Tag, html.find('div', {'class': 'container-chapter-reader'}))\n@@ -134,14 +142,14 @@ def get(self,\n             updated_at = updated_at_field.find('span', {'class': 'stre-value'}).text\n \n         if with_symbolic_links:\n-            chapters_links = self.__find_chapets_list(html=soup)\n+            chapters_links = self.__find_chapters_list(html=soup)\n             chapters = [self.__create_chapter(link, symbolic=True) for link in chapters_links]\n         else:\n             workers = cpu_count()\n             logger.debug(f'Initializing {workers} workers to fetch chapters of {identifier}.')\n \n             with ThreadPoolExecutor(max_workers=workers) as executor:\n-                chapters = executor.map(self.__create_chapter, self.__find_chapets_list(html=soup))\n+                chapters = executor.map(self.__create_chapter, self.__find_chapters_list(html=soup))\n                 chapters = list(filter(lambda x: isinstance(x, Chapter), list(chapters)))\n                 executor.shutdown()\n         \n@@ -149,8 +157,8 @@ def get(self,\n                      authors=[Author(name=author)] if author is not None else None,\n                      genres=[Genre(name=genre_name) for genre_name in genres],\n                      id=identifier,\n-                     created_at=datetime.strptime(updated_at, \"%b %d,%Y - %I:%M %p\") if updated_at else None,\n-                     updated_at=datetime.strptime(updated_at, \"%b %d,%Y - %I:%M %p\") if updated_at else None,\n+                     created_at=datetime.strptime(updated_at, \"%b %d,%Y - %H:%M %p\") if updated_at else None,\n+                     updated_at=datetime.strptime(updated_at, \"%b %d,%Y - %H:%M %p\") if updated_at else None,\n                      thumbnail=Image(uri=cover), # type: ignore\n                      cover=Image(uri=cover), # type: ignore\n                      chapters=chapters) # type: ignore\ndiff --git a/enma/infra/adapters/repositories/nhentai.py b/enma/infra/adapters/repositories/nhentai.py\nindex da22d26..724b473 100644\n--- a/enma/infra/adapters/repositories/nhentai.py\n+++ b/enma/infra/adapters/repositories/nhentai.py\n@@ -11,8 +11,8 @@\n import requests\n from bs4 import BeautifulSoup, Tag\n \n-from enma.application.core.handlers.error import (ExceedRetryCount, InvalidConfig, InvalidRequest,\n-                                                  NhentaiSourceWithoutConfig)\n+from enma.application.core.handlers.error import (ExceedRetryCount, Forbidden, InvalidConfig, InvalidRequest,\n+                                                  NhentaiSourceWithoutConfig, NotFound, Unknown)\n from enma.application.core.interfaces.manga_repository import IMangaRepository\n from enma.application.core.utils.logger import logger\n from enma.domain.entities.author_page import AuthorPage\n@@ -58,6 +58,18 @@ def __init__(self,\n         self.__AVATAR_URL = 'https://i5.nhentai.net/'\n         self.__TINY_IMAGE_BASE_URL = self.__IMAGE_BASE_URL.replace('/i.', '/t.')\n \n+    def __handle_source_response(self, response: requests.Response):\n+        logger.debug(f'Fetched {response.url} with response status code {response.status_code} and text {response.text}')\n+        if response.status_code == 200: return\n+        if response.status_code == 403: \n+            raise Forbidden(message='Could not perform a successfull request to the source due credentials issues. \\\n+Check your credentials and try again.')\n+        if response.status_code == 404:\n+            raise NotFound(message=f'Could not find the requested resource at \"{response.url}\". \\\n+Check the provided request parameters and try again.')\n+        raise Unknown(message='Something unexpected happened while trying to fetch source content. \\\n+Set the logging mode to debug and try again.')\n+\n     def __make_request(self,\n                        url: str,\n                        headers: Union[dict[str, Any], None] = None,\n@@ -72,21 +84,17 @@ def __make_request(self,\n         logger.debug(f'Fetching {url} with headers {headers} and params {params} the current config cf_clearance: {self.__config.cf_clearance}')\n \n         response = requests.get(url=urlparse(url).geturl(),\n-                                headers={**headers, 'User-Agent': self.__config.user_agent},\n+                                headers={**headers, 'User-Agent': f'{self.__config.user_agent}'},\n                                 params={**params},\n                                 cookies={'cf_clearance': self.__config.cf_clearance})\n         \n-        logger.debug(f'Fetched {url} with response status code {response.status_code} and text {response.text}')\n+        self.__handle_source_response(response)\n \n         return response\n \n     def set_config(self, config: CloudFlareConfig) -> None:\n         if not isinstance(config, CloudFlareConfig): raise InvalidConfig(message='You must provide a CloudFlareConfig object.') \n         self.__config = config\n-\n-    def __handle_request_error(self, msg: str) -> None:\n-        logger.error(msg)\n-        return None\n     \n     def __make_page_uri(self,\n                         type: Union[Literal['cover'], Literal['page'], Literal['thumbnail']],\n@@ -111,10 +119,6 @@ def __make_page_uri(self,\n     def fetch_chapter_by_symbolic_link(self, \n                                        link: SymbolicLink) -> Chapter:\n         response = self.__make_request(url=link.link)\n-\n-        if response.status_code != 200:\n-            self.__handle_request_error(msg=f'Could not fetch {link.link} because nhentai\\'s request ends up with {response.status_code} status code.')\n-            return Chapter()\n         \n         doujin: NHentaiResponse = response.json()\n \n@@ -155,9 +159,6 @@ def get(self,\n         url = f'{self.__API_URL}/gallery/{identifier}'\n         response = self.__make_request(url=url)\n \n-        if response.status_code != 200:\n-            return self.__handle_request_error(msg=f'Could not fetch {identifier} because nhentai\\'s request ends up with {response.status_code} status code.')\n-\n         doujin: NHentaiResponse = response.json()\n         media_id = doujin.get('media_id')\n \n@@ -221,10 +222,6 @@ def search(self,\n                                      total_results=0,\n                                      results=[])\n         \n-        if request_response.status_code != 200:\n-            self.__handle_request_error(f'Could not search by {query} because nhentai\\'s request ends up with {request_response.status_code} status code.')\n-            return search_result\n-\n         soup = BeautifulSoup(request_response.text, 'html.parser')\n \n         search_results_container = soup.find('div', {'class': 'container'})\n@@ -290,10 +287,6 @@ def paginate(self, page: int) -> Pagination:\n         response = self.__make_request(url=urljoin(self.__API_URL, f'galleries/all'),\n                                        params={'page': page})\n \n-        if response.status_code != 200:\n-            self.__handle_request_error(f'Could not paginate to page {page} because nhentai\\'s request ends up with {response.status_code} status code.')\n-            return Pagination(page=page)\n-\n         data = response.json()\n \n         PAGES = data.get('num_pages', 0)\n@@ -315,9 +308,6 @@ def paginate(self, page: int) -> Pagination:\n     def random(self, retry=0) -> Manga:\n         response = self.__make_request(url=urljoin(self.__BASE_URL, 'random'))\n \n-        if response.status_code != 200:\n-            self.__handle_request_error(f'Could not fetch a random manga because nhentai\\'s request ends up with {response.status_code} status code.')\n-\n         soup = BeautifulSoup(response.text, 'html.parser')\n \n         id = cast(Tag, soup.find('h3', id='gallery_id')).text.replace('#', '')\n@@ -345,11 +335,7 @@ def author_page(self,\n                             total_pages=0,\n                             page=page,\n                             total_results=0,\n-                            results=[])\n-        \n-        if request_response.status_code != 200:\n-            logger.error('Could not fetch author page properly')\n-            return result    \n+                            results=[]) \n    \n         soup = BeautifulSoup(request_response.text, 'html.parser')\n \ndiff --git a/enma/infra/core/interfaces/mangadex_response.py b/enma/infra/core/interfaces/mangadex_response.py\nnew file mode 100644\nindex 0000000..777ed0c\n--- /dev/null\n+++ b/enma/infra/core/interfaces/mangadex_response.py\n@@ -0,0 +1,134 @@\n+from typing import Any, Literal, TypedDict, Union, TypeVar\n+\n+\n+class Title(TypedDict):\n+    en: str\n+\n+class MangaDesc(TypedDict):\n+    en: str\n+\n+class TagAttrs(TypedDict):\n+    name: dict[str, str]\n+    description: dict[str, str]\n+    group: Union[Literal[\"theme\"], Literal[\"genre\"], \n+                 Literal[\"format\"]]\n+    version: int\n+\n+class IMangaTag(TypedDict):\n+    id: str\n+    type: str\n+    attributes: TagAttrs\n+    relationships: list[Any]\n+\n+IAltTitles = list[dict[str, str]]\n+\n+class MangaAttrs(TypedDict):\n+    title: Title\n+    altTitles: IAltTitles\n+    description: MangaDesc\n+    isLocked: bool\n+    links: dict[str, str]\n+    originalLanguage: str\n+    lastVolume: str\n+    lastChapter: str\n+    publicationDemographic: Any\n+    status: str\n+    year: int\n+    contentRating: str\n+    tags: list[IMangaTag]\n+    state: str\n+    chapterNumbersResetOnNewVolume: bool\n+    createdAt: str\n+    updatedAt: str\n+    version: int\n+    availableTranslatedLanguages: list[str]\n+    latestUploadedChapter: str\n+\n+class CoverAttrs(TypedDict):\n+    description: str\n+    volume: int\n+    fileName: str\n+    locale: str\n+    createdAt: str\n+    updatedAt: str\n+    version: int\n+\n+class CoverArtRelation(TypedDict):\n+    id: str\n+    type: Literal[\"cover_art\"]\n+    attributes: CoverAttrs\n+\n+class PersonAttrs(TypedDict):\n+    name: str\n+    imageUrl: str\n+    biography: dict[str, str]\n+    twitter: str\n+    pixiv: str\n+    melonBook: str\n+    fanBox: str\n+    booth: str\n+    namicomi: str\n+    nicoVideo: str\n+    skeb: str\n+    fantia: str\n+    tumblr: str\n+    youtube: str\n+    weibo: str\n+    naver: str\n+    website: str\n+    createdAt: str\n+    updatedAt: str\n+    version: int\n+\n+class AuthorRelation(TypedDict):\n+    id: str\n+    type: Union[Literal[\"author\"], Literal[\"artist\"]]\n+    attributes: PersonAttrs\n+\n+IRelations = list[Union[CoverArtRelation,\n+                       AuthorRelation,\n+                       dict[str, str]]]\n+\n+class IManga(TypedDict):\n+    id: str\n+    type: str\n+    attributes: MangaAttrs\n+    relationships: IRelations\n+\n+class IGetResult(TypedDict):\n+    result: str\n+    response: str\n+    data: IManga\n+\n+class ISearchResult(IGetResult):\n+    limit: int\n+    offset: int\n+    total: int\n+    result: str\n+    response: str\n+    data: list[IManga]\n+\n+class IChapter(TypedDict):\n+    chapter: str\n+    id: str\n+    others: list\n+    count: int\n+\n+class IVolume(TypedDict):\n+    volume: str\n+    count: int\n+    chapters: dict[str, IChapter]\n+\n+class IVolumesResponse(TypedDict):\n+    result: str\n+    volumes: dict[str, IVolume]\n+\n+class IChapterHash(TypedDict):\n+    hash: str\n+    data: list[str]\n+    dataSaver: list[str]\n+\n+class IHash(TypedDict):\n+    result: str\n+    baseUrl: str\n+    chapter: IChapterHash\n\\ No newline at end of file\ndiff --git a/enma/infra/entrypoints/lib/__init__.py b/enma/infra/entrypoints/lib/__init__.py\nindex ad4e952..9d5e4d3 100644\n--- a/enma/infra/entrypoints/lib/__init__.py\n+++ b/enma/infra/entrypoints/lib/__init__.py\n@@ -5,7 +5,7 @@\n from enum import Enum\n from typing import Any, Generic, Optional, TypeVar, TypedDict, Union\n \n-from enma.application.core.handlers.error import InstanceError, SourceNotAvailable, SourceWasNotDefined\n+from enma.application.core.handlers.error import InstanceError, InvalidResource, SourceNotAvailable, SourceWasNotDefined\n from enma.application.core.interfaces.downloader_adapter import IDownloaderAdapter\n from enma.application.core.interfaces.manga_repository import IMangaRepository\n from enma.application.core.interfaces.saver_adapter import ISaverAdapter\n@@ -18,18 +18,17 @@\n from enma.application.use_cases.paginate import PaginateRequestDTO, PaginateResponseDTO, PaginateUseCase\n from enma.application.use_cases.search_manga import SearchMangaRequestDTO, SearchMangaResponseDTO, SearchMangaUseCase\n from enma.domain.entities.author_page import AuthorPage\n-from enma.domain.entities.manga import Chapter, Manga, SymbolicLink\n+from enma.domain.entities.manga import Chapter, Manga\n from enma.domain.entities.pagination import Pagination\n from enma.domain.entities.search_result import SearchResult\n+from enma.infra.adapters.repositories.mangadex import Mangadex\n from enma.infra.adapters.repositories.manganato import Manganato\n from enma.infra.adapters.repositories.nhentai import NHentai, CloudFlareConfig\n from enma.infra.core.interfaces.lib import IEnma\n \n-class SourcesEnum(str, Enum):\n-    ...\n-\n-class DefaultAvailableSources(SourcesEnum):\n+class SourcesEnum(Enum):\n     NHENTAI = 'nhentai'\n+    MANGADEX = 'mangadex'\n     MANGANATO = 'manganato'\n \n class ExtraConfigs(TypedDict):\n@@ -38,15 +37,37 @@ class ExtraConfigs(TypedDict):\n AvailableSources = TypeVar('AvailableSources', bound=SourcesEnum)\n \n class SourceManager(Generic[AvailableSources]):\n+    \"\"\"\n+    Manages manga source repositories available to the Enma application, allowing for dynamic source selection at runtime.\n+\n+    Attributes:\n+        source (Union[IMangaRepository, None]): The currently selected manga repository source.\n+        source_name (str): The name of the currently selected source.\n+    \"\"\"\n+\n     def __init__(self, **kwargs) -> None:\n-        self.__SOURCES: dict[str, IMangaRepository] = {'nhentai': NHentai(config=kwargs.get('cloudflare_config')),\n-                                                       'manganato': Manganato()}\n+        \"\"\"\n+        Initializes the SourceManager with empty sources and no selected source.\n+        \"\"\"\n+        self.__SOURCES: dict[str, IMangaRepository] = {}\n         self.source: Union[IMangaRepository, None] = None\n         self.source_name = ''\n     \n     def get_source(self,\n                    source_name: Union[AvailableSources, str]) -> IMangaRepository:\n-        \n+        \"\"\"\n+        Retrieves a source repository by name.\n+\n+        Args:\n+            source_name (Union[AvailableSources, str]): The name of the source to retrieve, either as a string or an enum.\n+\n+        Returns:\n+            IMangaRepository: The manga repository source.\n+\n+        Raises:\n+            SourceNotAvailable: If the requested source is not available.\n+        \"\"\"\n+\n         source_name = source_name.value if isinstance(source_name, Enum) else source_name\n         source = self.__SOURCES.get(source_name)\n \n@@ -57,20 +78,45 @@ def get_source(self,\n     \n     def set_source(self,\n                    source_name: Union[AvailableSources, str]) -> None:\n+        \"\"\"\n+        Sets the currently active source to the specified source name.\n+\n+        Args:\n+            source_name (Union[AvailableSources, str]): The name of the source to activate, either as a string or an enum.\n+        \"\"\"\n         source = self.get_source(source_name=source_name)\n         self.source = source\n         self.source_name = source_name\n \n     def add_source(self,\n-                   source_name: str,\n+                   source_name: Union[str, SourcesEnum],\n                    source: IMangaRepository) -> None:\n+        \"\"\"\n+        Adds a new source repository to the available sources.\n+\n+        Args:\n+            source_name (Union[str, SourcesEnum]): The name of the source to add.\n+            source (IMangaRepository): The manga repository source instance.\n \n+        Raises:\n+            InstanceError: If the provided source is not an instance of IMangaRepository.\n+        \"\"\"\n         if not isinstance(source, IMangaRepository):\n             raise InstanceError('Provided source is not an instance of IMangaRepository.')\n         \n-        self.__SOURCES[source_name] = source\n+        self.__SOURCES[source_name if isinstance(source_name, str) else source_name.value] = source\n \n def instantiate_source(callable):\n+    \"\"\"\n+    Decorator function to ensure the current use case is instantiated with the current source.\n+    This is used to decorate methods of the Enma class that require a source to have been set.\n+\n+    Args:\n+        callable: The method to be decorated.\n+    \n+    Returns:\n+        The wrapped method with source initialization logic.\n+    \"\"\"\n     def wrapper(self, *args, **kwargs):\n         if self.source_manager.source is not None and \\\n             self._Enma__current_source_name != self.source_manager.source_name:\n@@ -81,10 +127,22 @@ def wrapper(self, *args, **kwargs):\n     return wrapper\n \n class Enma(IEnma, Generic[AvailableSources]):\n+    \"\"\"\n+    Main application class for Enma, providing interfaces to execute various manga-related use cases.\n+    Allows dynamic selection of manga sources and performs actions like fetching manga, searching, and downloading chapters.\n+\n+    Attributes:\n+        source_manager (SourceManager[AvailableSources]): Manages the available sources and the current source selection.\n+    \"\"\"\n     def __init__(self, \n                  source: Optional[AvailableSources] = None, \n                  **kwargs) -> None:\n+        \"\"\"\n+        Initializes the Enma application with optional default source selection and extra configurations.\n \n+        Args:\n+            source (Optional[AvailableSources], optional): The default source to be used. If provided, use cases will be initialized with this source.\n+        \"\"\"\n         self.__get_manga_use_case: Optional[IUseCase[GetMangaRequestDTO, GetMangaResponseDTO]] = None\n         self.__search_manga_use_case: Optional[IUseCase[SearchMangaRequestDTO, SearchMangaResponseDTO]] = None\n         self.__paginate_use_case: Optional[IUseCase[PaginateRequestDTO, PaginateResponseDTO]] = None\n@@ -92,12 +150,27 @@ def __init__(self,\n         self.__downloader_use_case: Optional[IUseCase[DownloadChapterRequestDTO, DownloadChapterResponseDTO]] = None\n         self.__get_author_page_use_case: Optional[IUseCase[GetAuthorPageRequestDTO, GetAuthorPageResponseDTO]] = None\n         self.__fetch_chapter_by_symbolic_link_use_case: Optional[IUseCase[FetchChapterBySymbolicLinkRequestDTO, FetchChapterBySymbolicLinkResponseDTO]] = None\n-        self.__current_source_name = None\n+        self.__current_source_name: Optional[str] = None\n \n         self.source_manager = SourceManager[AvailableSources](**kwargs)\n+        self.__create_default_sources()\n         if source is not None: self.__initialize_use_case(source=self.source_manager.get_source(source_name=source))\n \n+    def __create_default_sources(self) -> None:\n+        \"\"\"\n+        Creates and adds the default manga sources to the source manager. Currently, NHentai, Manganato, and Mangadex are added.\n+        \"\"\"\n+        self.source_manager.add_source(SourcesEnum.NHENTAI, NHentai())\n+        self.source_manager.add_source(SourcesEnum.MANGANATO, Manganato())\n+        self.source_manager.add_source(SourcesEnum.MANGADEX, Mangadex())\n+\n     def __initialize_use_case(self, source: IMangaRepository) -> None:\n+        \"\"\"\n+        Initializes the use cases with the given source repository. This method sets up all use cases available in Enma.\n+\n+        Args:\n+            source (IMangaRepository): The source repository to initialize use cases with.\n+        \"\"\"\n         self.__get_manga_use_case = GetMangaUseCase(manga_repository=source)\n         self.__search_manga_use_case = SearchMangaUseCase(manga_repository=source)     \n         self.__paginate_use_case = PaginateUseCase(manga_repository=source)     \n@@ -110,7 +183,20 @@ def __initialize_use_case(self, source: IMangaRepository) -> None:\n     @instantiate_source\n     def get(self, \n             identifier: str,\n-            with_symbolic_links: bool = False) -> Union[Manga, None]:\n+            with_symbolic_links: bool = True) -> Union[Manga, None]:\n+        \"\"\"\n+        Retrieves detailed information for a specific manga identified by its ID.\n+\n+        Args:\n+            identifier (str): The unique identifier of the manga to retrieve.\n+            with_symbolic_links (bool, optional): If True, fetches the manga with symbolic links to chapters. Defaults to True.\n+\n+        Returns:\n+            Union[Manga, None]: The Manga object if found, None otherwise.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+        \"\"\"\n         if self.__get_manga_use_case is None:\n             raise SourceWasNotDefined('You must define a source before of performing actions.')\n \n@@ -121,7 +207,24 @@ def get(self,\n         return response.manga\n     \n     @instantiate_source\n-    def search(self, query: str, page: int=1, **kwargs) -> SearchResult:\n+    def search(self, \n+               query: str, \n+               page: int=1, \n+               **kwargs) -> SearchResult:\n+        \"\"\"\n+        Searches for manga that match the given query string.\n+\n+        Args:\n+            query (str): The search query string.\n+            page (int, optional): The page number of the search results to retrieve. Defaults to 1.\n+            **kwargs: Additional parameters for search customization.\n+\n+        Returns:\n+            SearchResult: An object containing the paginated search results, including manga thumbnails.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+        \"\"\"\n         if self.__search_manga_use_case is None:\n             raise SourceWasNotDefined('You must define a source before of performing actions.')\n         \n@@ -132,7 +235,20 @@ def search(self, query: str, page: int=1, **kwargs) -> SearchResult:\n         return response.result\n     \n     @instantiate_source\n-    def paginate(self, page: int) -> Pagination:\n+    def paginate(self, \n+                 page: int) -> Pagination:\n+        \"\"\"\n+        Retrieves a specific page of manga listings.\n+\n+        Args:\n+            page (int): The page number of manga listings to retrieve.\n+\n+        Returns:\n+            Pagination: An object containing the paginated list of manga thumbnails and pagination details.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+        \"\"\"\n         if self.__paginate_use_case is None:\n             raise SourceWasNotDefined('You must define a source before of performing actions.')\n         \n@@ -142,7 +258,20 @@ def paginate(self, page: int) -> Pagination:\n     \n     @instantiate_source\n     def random(self) -> Manga:\n-        response = self.__random_use_case.execute() # type: ignore\n+        \"\"\"\n+        Fetches a random manga from the currently selected source.\n+\n+        Returns:\n+            Manga: A Manga object for the randomly selected manga.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+            NotImplementedError: If the current source does not support fetching an author's page.\n+        \"\"\"\n+        if self.__random_use_case is None:\n+            raise SourceWasNotDefined('You must define a source before of performing actions.')\n+        \n+        response = self.__random_use_case.execute() \n         return response.result\n     \n     \n@@ -153,6 +282,19 @@ def download_chapter(self,\n                          downloader: IDownloaderAdapter,\n                          saver: ISaverAdapter,\n                          threaded: Threaded) -> None:\n+        \"\"\"\n+        Downloads a manga chapter to the specified path using the provided downloader and saver adapters.\n+\n+        Args:\n+            path (str): The filesystem path where the chapter should be saved.\n+            chapter (Chapter): The manga chapter to download.\n+            downloader (IDownloaderAdapter): The adapter to use for downloading the chapter pages.\n+            saver (ISaverAdapter): The adapter to use for saving the downloaded pages.\n+            threaded (Threaded): Determines whether the download should be performed in a threaded manner for concurrency.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+        \"\"\"\n         if self.__downloader_use_case is None:\n             raise SourceWasNotDefined('You must define a source before of performing actions.')\n         \n@@ -164,6 +306,20 @@ def download_chapter(self,\n     \n     @instantiate_source\n     def author_page(self, author: str, page: int) -> AuthorPage:\n+        \"\"\"\n+        Fetches manga authored by a specific author.\n+\n+        Args:\n+            author (str): The name or identifier of the author.\n+            page (int): The page number of results to retrieve.\n+\n+        Returns:\n+            AuthorPage: An object containing a list of manga by the specified author.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+            NotImplementedError: If the current source does not support fetching an author's page.\n+        \"\"\"\n         if self.__get_author_page_use_case is None:\n             raise SourceWasNotDefined('You must define a source before of performing actions.')\n         \n@@ -171,10 +327,29 @@ def author_page(self, author: str, page: int) -> AuthorPage:\n                                                                                             page=page))).result\n \n     @instantiate_source\n-    def fetch_chapter_by_symbolic_link(self, link: SymbolicLink) -> Chapter:\n+    def fetch_chapter_by_symbolic_link(self, \n+                                       chapter: Chapter) -> Chapter:\n+        \"\"\"\n+        Fetches a manga chapter's details including pages and images by its symbolic link.\n+\n+        Args:\n+            chapter (Chapter): The manga chapter to fetch, which must include a valid symbolic link.\n+\n+        Returns:\n+            Chapter: An object containing the fetched chapter details such as pages and images.\n+\n+        Raises:\n+            SourceWasNotDefined: If no source has been defined prior to calling this method.\n+            InvalidResource: If the provided chapter does not have a valid symbolic link.\n+        \"\"\"\n         if self.__fetch_chapter_by_symbolic_link_use_case is None:\n             raise SourceWasNotDefined('You must define a source before of performing actions.')\n         \n-        response = self.__fetch_chapter_by_symbolic_link_use_case.execute(dto=DTO(data=FetchChapterBySymbolicLinkRequestDTO(link=link)))\n+        if chapter.link is None or chapter.link.link is None:\n+            raise InvalidResource('Chapter does not have a symbolic link.')\n         \n+        response = self.__fetch_chapter_by_symbolic_link_use_case.execute(dto=DTO(data=FetchChapterBySymbolicLinkRequestDTO(link=chapter.link)))\n+        \n+        response.chapter.id = chapter.id\n+\n         return response.chapter\ndiff --git a/requirements.txt b/requirements.txt\nindex c42522e..a0cda0d 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,3 +1,4 @@\n requests==2.31.0\n beautifulsoup4==4.10.0\n-pydantic==2.5.3\n\\ No newline at end of file\n+pydantic==2.5.3\n+pytest==8.0.2\n", "instance_id": "AlexandreSenpai__Enma-70", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly identifies two specific issues in the `manganato.py` file related to date parsing and title alternative handling. The issues are described with detailed error messages and stack traces, which help in understanding the root cause. Additionally, the proposed solutions are explicitly provided with code snippets, making the intent and expected resolution very clear. The input and output expectations are implicitly defined through the context of the errors and fixes. There are no significant ambiguities, and the statement includes examples of the failing scenarios. The only minor omission is the lack of explicit mention of potential edge cases beyond the ones already causing errors, but this does not detract from the overall clarity.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) due to the following reasons based on the evaluated factors:\n\n1. **Scope and Depth of Code Changes**: The required modifications are localized to a single file (`manganato.py`) and involve straightforward changes to two specific methods (`get` for date format string and `__create_title` for handling alternative titles). The changes do not impact the broader architecture of the system or require understanding complex interactions between multiple modules. The amount of code change is minimal, focusing on updating a format string and adding a conditional check.\n\n2. **Number of Technical Concepts**: The problem requires basic knowledge of Python, specifically string formatting for `datetime.strptime` and conditional logic for string splitting. No advanced language features, libraries, algorithms, or design patterns are needed. The concepts involved are fundamental and widely understood by developers with basic to intermediate Python experience.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement highlights specific edge cases (24-hour time format and single alternative titles) that are directly addressed by the proposed solutions. The fixes are simple and do not introduce complex error handling logic beyond basic conditional checks. There are no indications of additional unaddressed edge cases that would significantly complicate the solution.\n\n4. **Overall Complexity**: The task involves simple bug fixes rather than implementing new features or refactoring large parts of the codebase. The changes are well-contained and do not require deep understanding of the entire codebase or domain-specific knowledge beyond basic web scraping and data parsing.\n\nWhile the broader code changes in the diff (e.g., addition of Mangadex support, error handling enhancements) are extensive, they appear to be unrelated to the specific problem statement focused on `manganato.py` fixes. Therefore, the difficulty assessment is based solely on the described issues and their corresponding solutions, which are straightforward and require minimal effort to implement and test.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`skip_if` not passed down in container hierarchies\nHey @hanjinliu,\r\nI wanted to use `magicclass.serialize.serialize()` to save widget states in a container, which contains nested containers. Some of these have Image widgets, and I wanted to skip these in serialization using the `skip_if` argument. The problem is that the value of `skip_if` is not passed down in the recursive call. I believe that adding it to lines 80 & 93 in `magicclass.serialize.py` would solve this issue.\n", "patch": "diff --git a/magicclass/serialize.py b/magicclass/serialize.py\nindex 95e00580..ff09a484 100644\n--- a/magicclass/serialize.py\n+++ b/magicclass/serialize.py\n@@ -77,7 +77,12 @@ def _serialize_value(widget: ValueWidget | WidgetAction):\n                 if (_value := _serialize_value(child)) is not _missing:\n                     out[child.name] = _value\n             else:\n-                ser = serialize(child, skip_empty=skip_empty, skip_null=skip_null)\n+                ser = serialize(\n+                    child,\n+                    skip_empty=skip_empty,\n+                    skip_null=skip_null,\n+                    skip_if=skip_if,\n+                )\n                 if len(ser) > 0 or not skip_empty:\n                     out[child.name] = ser\n             processed.add(id(child))\n@@ -90,7 +95,10 @@ def _serialize_value(widget: ValueWidget | WidgetAction):\n                 out[widget.name] = _value\n         elif isinstance(widget, (Container, MenuGuiBase, ToolBarGui)):\n             out[widget.name] = serialize(\n-                widget, skip_empty=skip_empty, skip_null=skip_null\n+                widget,\n+                skip_empty=skip_empty,\n+                skip_null=skip_null,\n+                skip_if=skip_if,\n             )\n         elif isinstance(widget, WidgetAction) and widget.support_value:\n             if (_value := _serialize_value(widget.widget)) is not _missing:\n@@ -154,7 +162,9 @@ def deserialize(\n                 if _is_value_widget_like(child):\n                     child.value = val\n                 else:\n-                    deserialize(child, val)\n+                    deserialize(\n+                        child, val, missing_ok=missing_ok, record=record, emit=emit\n+                    )\n \n     for widget in ui:\n         if isinstance(widget, (PushButton, Action)):\n@@ -164,7 +174,9 @@ def deserialize(\n                 widget.value = value\n         elif isinstance(widget, (Container, MenuGuiBase, ToolBarGui)):\n             if (val := _dict_get(data, widget.name, missing_ok)) is not _missing:\n-                deserialize(widget, val)\n+                deserialize(\n+                    widget, val, missing_ok=missing_ok, record=record, emit=emit\n+                )\n         elif isinstance(widget, WidgetAction) and widget.support_value:\n             if (value := _dict_get(data, widget.name, missing_ok)) is not _missing:\n                 widget.value = value\n", "instance_id": "hanjinliu__magic-class-156", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `skip_if` argument is not being passed down through recursive calls in the `serialize()` function of a widget container hierarchy. The user points out specific lines in the code (lines 80 and 93) where they believe the fix should be applied, which helps in understanding the intended solution. However, the statement lacks critical details such as examples of input/output, specific use cases where the issue manifests, or any mention of potential edge cases (e.g., how `skip_if` should behave with different widget types or nested levels). Additionally, there is no explanation of the expected behavior of `skip_if` in nested containers, which could lead to ambiguity in interpreting the requirements. Thus, while the problem is valid and mostly clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to several factors. First, the scope of the code changes is minimal and localized to a single file (`magicclass/serialize.py`), involving straightforward modifications to function calls by adding the `skip_if` parameter in two places for the `serialize()` function and similarly passing additional parameters in the `deserialize()` function. The changes do not impact the broader system architecture or require understanding complex interactions across multiple modules. Second, the technical concepts involved are basic: understanding function arguments and recursive calls, which are fundamental programming concepts not specific to advanced language features, libraries, or algorithms. Third, the problem statement and code changes do not explicitly mention edge cases or additional error handling requirements beyond passing the parameters, and the modifications appear to be a simple bug fix without introducing new logic. While the developer needs to understand the purpose of `skip_if` and ensure it behaves correctly in nested calls, this does not significantly elevate the complexity. Therefore, I assign a difficulty score of 0.25, reflecting an easy task that requires minimal effort and basic code logic understanding.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "to_dict() doesn't convert inner_hits\nMaybe related to https://github.com/elastic/elasticsearch-dsl-py/issues/291, I'm attempting to dump my elasticsearch-dsl response by calling `to_dict()`, but it doesn't appear to walk the entire structure. `inner_hits` appear to be `Response` instances, but calling `to_dict()` on the top-level response doesn't convert them. Why does it convert hits but not inner_hits? Is there a simple way to convert the _entire_ response without having to pick apart the structure and manually call `to_dict()` as needed?\r\n\r\nI'm using Python 3.8 and elasticsearch-dsl 7.4.0.\r\n\r\nThanks so much for this library! \u2764\ufe0f It's incredibly useful.\n", "patch": "diff --git a/elasticsearch_dsl/utils.py b/elasticsearch_dsl/utils.py\nindex 021afc99..d9727f1e 100644\n--- a/elasticsearch_dsl/utils.py\n+++ b/elasticsearch_dsl/utils.py\n@@ -86,6 +86,17 @@ def _wrap(val: Any, obj_wrapper: Optional[Callable[[Any], Any]] = None) -> Any:\n     return val\n \n \n+def _recursive_to_dict(value: Any) -> Any:\n+    if hasattr(value, \"to_dict\"):\n+        return value.to_dict()\n+    elif isinstance(value, dict) or isinstance(value, AttrDict):\n+        return {k: _recursive_to_dict(v) for k, v in value.items()}\n+    elif isinstance(value, list) or isinstance(value, AttrList):\n+        return [recursive_to_dict(elem) for elem in value]\n+    else:\n+        return value\n+\n+\n class AttrList(Generic[_ValT]):\n     def __init__(\n         self, l: List[_ValT], obj_wrapper: Optional[Callable[[_ValT], Any]] = None\n@@ -228,8 +239,10 @@ def __setattr__(self, name: str, value: _ValT) -> None:\n     def __iter__(self) -> Iterator[str]:\n         return iter(self._d_)\n \n-    def to_dict(self) -> Dict[str, _ValT]:\n-        return self._d_\n+    def to_dict(self, recursive: bool = False) -> Dict[str, _ValT]:\n+        return cast(\n+            Dict[str, _ValT], _recursive_to_dict(self._d_) if recursive else self._d_\n+        )\n \n     def keys(self) -> Iterable[str]:\n         return self._d_.keys()\n", "instance_id": "elastic__elasticsearch-dsl-py-1894", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `to_dict()` method in the `elasticsearch-dsl` library does not convert `inner_hits` to dictionaries as expected, unlike `hits`. The user provides context about their environment (Python 3.8, elasticsearch-dsl 7.4.0) and references a related GitHub issue, which adds some clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected output format for `inner_hits` after conversion, nor does it mention specific edge cases or constraints (e.g., nested structures beyond `inner_hits`, performance expectations). Additionally, the statement is framed as a question rather than a formal requirement, which introduces slight uncertainty about the exact goal (e.g., whether a full recursive conversion is desired or just for `inner_hits`). Despite these minor gaps, the intent is understandable, and the provided code changes align with addressing the issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are localized to a single file (`elasticsearch_dsl/utils.py`) and involve modifying the `to_dict()` method of the `AttrDict` class to support recursive conversion, along with adding a helper function `_recursive_to_dict()`. The changes are relatively small (about 15 lines of code) and do not impact the broader architecture of the library. They focus on enhancing an existing utility without requiring deep interaction with other modules or components.\n\n2. **Technical Concepts Involved**: Solving this requires a moderate understanding of Python's object model (e.g., checking for `to_dict()` method existence via `hasattr`), handling recursive data structures (dictionaries and lists), and familiarity with the library's custom types like `AttrDict` and `AttrList`. These concepts are not overly complex for an intermediate Python developer, though they do require some understanding of the library's internal design to ensure the recursive conversion works as intended.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes introduce a recursive approach that could potentially encounter issues like circular references or deeply nested structures leading to stack overflow. However, the provided solution does not address these, and the complexity of handling such cases is not inherently required by the problem as stated. Basic error handling (e.g., type checking) is implicitly handled by the structure of the code, but no advanced error management is needed.\n\n4. **Overall Complexity**: The task involves understanding the intent of `to_dict()` and extending it to handle nested structures recursively. While this requires some thought about how to traverse nested data, it is a straightforward enhancement to existing functionality. The solution does not demand advanced algorithms, performance optimization, or deep domain knowledge of Elasticsearch beyond the library's API.\n\nGiven these considerations, I assign a difficulty score of 0.35, placing it on the higher end of Easy. It requires more than trivial changes (e.g., beyond fixing a typo) but does not reach the complexity of a Medium difficulty task, as it is confined to a single utility function and does not involve significant architectural or cross-module considerations.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ImportError: cannot import name 'AutoRegister' from 'class_registry' on Ubuntu 24.04\nBonjour Aur\u00e9lien, \r\n\r\nI tried to install on a fresh Ubuntu 24.04 desktop (up to date @ 2024-09-11, nothing installed), from the git version and pip version, and got the same issue : \r\n\r\n```(my_venv) gis@ubuntu-otm:~$ pyhgtmap\r\nTraceback (most recent call last):\r\n  File \"/home/gis/my_venv/bin/pyhgtmap\", line 5, in <module>\r\n    from pyhgtmap.main import main\r\n  File \"/home/gis/my_venv/lib/python3.12/site-packages/pyhgtmap/main.py\", line 10, in <module>\r\n    from pyhgtmap import NASASRTMUtil\r\n  File \"/home/gis/my_venv/lib/python3.12/site-packages/pyhgtmap/NASASRTMUtil.py\", line 16, in <module>\r\n    from pyhgtmap.sources.pool import Pool\r\n  File \"/home/gis/my_venv/lib/python3.12/site-packages/pyhgtmap/sources/__init__.py\", line 11, in <module>\r\n    from class_registry import AutoRegister, ClassRegistry\r\nImportError: cannot import name 'AutoRegister' from 'class_registry' (/home/gis/my_venv/lib/python3.12/site-packages/class_registry/__init__.py)\r\n```\r\nI googled a little bit, found [this](https://class-registry.readthedocs.io/en/latest/advanced_topics.html) and try to replace, in pyhgtmap/sources/__init__.py\r\n```\r\nfrom class_registry import AutoRegister, ClassRegistry\r\n```\r\nby \r\n```\r\nfrom class_registry import ClassRegistry\r\nfrom class_registry.base import AutoRegister\r\n```\r\n\r\nand then I get \r\n\r\n```(my_venv) gis@ubuntu-otm:~$ pyhgtmap\r\nTraceback (most recent call last):\r\n  File \"/home/gis/my_venv/bin/pyhgtmap\", line 5, in <module>\r\n    from pyhgtmap.main import main\r\n  File \"/home/gis/my_venv/lib/python3.12/site-packages/pyhgtmap/main.py\", line 10, in <module>\r\n    from pyhgtmap import NASASRTMUtil\r\n  File \"/home/gis/my_venv/lib/python3.12/site-packages/pyhgtmap/NASASRTMUtil.py\", line 16, in <module>\r\n    from pyhgtmap.sources.pool import Pool\r\n  File \"/home/gis/my_venv/lib/python3.12/site-packages/pyhgtmap/sources/__init__.py\", line 29, in <module>\r\n    class Source(ABC, metaclass=AutoRegister(SOURCES_TYPES_REGISTRY)):  # type: ignore[misc] # Mypy does not understand dynamically-computed metaclasses\r\nTypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\r\n````\r\nAnd then, it's beyond my skills scope, sorry ;-)\r\n\r\nAm I doing something wrong? Do I have to force a previous version of classregistry as a temporary workaround?\r\n\r\nThank you,\r\n\r\n\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex 7e70037..79c4ddf 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -28,7 +28,7 @@ dependencies = [\n   \"numpy>=1.24.2\",\n   \"nptyping>=2.5.0\",\n   \"npyosmium>=3.6.1\",\n-  \"phx-class-registry>=4.0.6\",\n+  \"phx-class-registry>=4.0.6, <5\",\n   \"pybind11-rdp>=0.1.3\",\n   \"PyDrive2>=1.20.0\",\n   \"scipy>=1.8.0\",\n@@ -72,7 +72,7 @@ dependencies = [\n   \"types-beautifulsoup4>=4\",\n   \"mypy>=1.0.1\",\n   \"mypy-extensions~=1.0.0\",\n-  \"ruff>=0.3.0\",\n+  \"ruff>=0.6.4\",\n ]\n \n [tool.hatch.envs.default.scripts]\n", "instance_id": "agrenott__pyhgtmap-61", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an `ImportError` when trying to run `pyhgtmap` on Ubuntu 24.04 due to a failure to import `AutoRegister` from the `class_registry` module. The user provides a detailed error traceback, their attempt to fix the issue by modifying the import statement, and the subsequent error they encountered. This gives a good starting point for understanding the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the version of Python (though it can be inferred as 3.12 from the traceback) or the exact version of the `class_registry` library causing the issue. Additionally, there are no examples of expected behavior or desired output beyond resolving the error, and edge cases or constraints are not discussed. Despite these minor gaps, the issue is well-documented enough to proceed with a solution.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change in `pyproject.toml` is minimal, involving only a version constraint update for the `phx-class-registry` dependency to `<5`. This suggests the issue is likely a compatibility problem with a newer version of the library, and the fix is localized to a single configuration file. There is no indication of needing to modify multiple modules or impact the broader system architecture. However, if the version pinning does not resolve the issue, further investigation into the `class_registry` library or the codebase might be required, though this is not reflected in the current diff.\n\n2. **Number of Technical Concepts**: Solving this issue requires basic knowledge of Python dependency management (e.g., using `pip` and version constraints in `pyproject.toml`) and familiarity with how Python imports work. Understanding the error message and the concept of metaclasses (as hinted by the second traceback) might be necessary if the version pinning does not work, but this is still within the realm of intermediate Python knowledge. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the error handling requirements appear minimal at this stage. The issue is a straightforward import error likely caused by a library update or API change. If further code changes are needed beyond dependency versioning, some error handling might be required, but this is not evident from the current information.\n\n4. **Overall Complexity**: The problem seems to be a compatibility issue that can likely be resolved by pinning the dependency version, as shown in the code change. While the second error message about a metaclass conflict suggests potential complexity, the provided solution does not delve into modifying the codebase to handle this, and the fix remains simple. If deeper changes are needed (e.g., adapting to a new API or refactoring metaclass usage), the difficulty could increase, but based on the current scope, it remains low.\n\nIn summary, this problem requires understanding some code logic and making a simple modification to a configuration file, fitting the \"Easy\" category. I assign a difficulty score of 0.3 to account for the minor complexity introduced by the need to understand dependency issues and potential follow-up if the version pinning does not fully resolve the problem.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Test failure in test_filehandler_reopen_thread_safety on Python 3.13\n## BUG/PROBLEM REPORT / FEATURE REQUEST\r\n\r\n<!--\r\n\r\nPlease do not report security-related issues here. Report them by email to security@plone.org. The Security Team will contact the relevant maintainer if necessary.\r\n\r\nInclude tracebacks, screenshots, code of debugging sessions or code that reproduces the issue if possible.\r\nThe best reproductions are in plain Zope installations without addons or at least with minimal needed addons installed.\r\n\r\n-->\r\n\r\n### What I did:\r\n\r\n<!-- Enter a reproducible description, including preconditions. -->\r\n1. Check out current `master`, abf6f26fd27b79a23978058b274e397cca72b504\r\n2. Run `tox -e py313`\r\n\r\n### What I expect to happen:\r\n\r\nAll tests pass, as they do for `tox -e py312`.\r\n\r\n### What actually happened:\r\n\r\n```\r\nFailure in test test_filehandler_reopen_thread_safety (ZConfig.components.logger.tests.test_logger.TestReopeningLogfiles.test_filehandler_reopen_thread_safety)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 58, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 651, in run\r\n    self._callTestMethod(testMethod)\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 606, in _callTestMethod\r\n    if method() is not None:\r\n  File \"/home/ben/src/forks/ZConfig/src/ZConfig/components/logger/tests/test_logger.py\", line 714, in test_filehandler_reopen_thread_safety\r\n    self.assertEqual(calls, [\"acquire\", \"acquire\", \"release\", \"release\"])\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 907, in assertEqual\r\n    assertion_func(first, second, msg=msg)\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 1113, in assertListEqual\r\n    self.assertSequenceEqual(list1, list2, msg, seq_type=list)\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 1095, in assertSequenceEqual\r\n    self.fail(msg)\r\n  File \"/usr/lib64/python3.13/unittest/case.py\", line 732, in fail\r\n    raise self.failureException(msg)\r\nAssertionError: Lists differ: [] != ['acquire', 'acquire', 'release', 'release']\r\n\r\nSecond list contains 4 additional elements.\r\nFirst extra element 0:\r\n'acquire'\r\n\r\n- []\r\n+ ['acquire', 'acquire', 'release', 'release']\r\n\r\n------\r\n2024-05-11T11:12:45 INFO root An info message\r\n------\r\n2024-05-11T11:12:45 INFO root We see an info message\r\n------\r\n2024-05-11T11:12:45 INFO root Here is another info message\r\n/home/ben/src/forks/ZConfig/src/ZConfig/tests/test_schema2html.py:106: DeprecationWarning: The frontend.OptionParser class will be replaced by a subclass of argparse.ArgumentParser in Docutils 0.21 or later.\r\n  settings=docutils.frontend.OptionParser(\r\n/usr/lib64/python3.13/optparse.py:1000: DeprecationWarning: The frontend.Option class will be removed in Docutils 0.21 or later.\r\n  option = self.option_class(*args, **kwargs)\r\n/home/ben/src/forks/ZConfig/src/ZConfig/tests/test_schema2html.py:106: DeprecationWarning: The frontend.OptionParser class will be replaced by a subclass of argparse.ArgumentParser in Docutils 0.21 or later.\r\n  settings=docutils.frontend.OptionParser(\r\n/usr/lib64/python3.13/optparse.py:1000: DeprecationWarning: The frontend.Option class will be removed in Docutils 0.21 or later.\r\n  option = self.option_class(*args, **kwargs)\r\n/home/ben/src/forks/ZConfig/src/ZConfig/tests/test_schema2html.py:106: DeprecationWarning: The frontend.OptionParser class will be replaced by a subclass of argparse.ArgumentParser in Docutils 0.21 or later.\r\n  settings=docutils.frontend.OptionParser(\r\n/usr/lib64/python3.13/optparse.py:1000: DeprecationWarning: The frontend.Option class will be removed in Docutils 0.21 or later.\r\n  option = self.option_class(*args, **kwargs)\r\n/home/ben/src/forks/ZConfig/src/ZConfig/tests/test_schema2html.py:106: DeprecationWarning: The frontend.OptionParser class will be replaced by a subclass of argparse.ArgumentParser in Docutils 0.21 or later.\r\n  settings=docutils.frontend.OptionParser(\r\n/usr/lib64/python3.13/optparse.py:1000: DeprecationWarning: The frontend.Option class will be removed in Docutils 0.21 or later.\r\n  option = self.option_class(*args, **kwargs)\r\n/home/ben/src/forks/ZConfig/src/ZConfig/tests/test_schema2html.py:106: DeprecationWarning: The frontend.OptionParser class will be replaced by a subclass of argparse.ArgumentParser in Docutils 0.21 or later.\r\n  settings=docutils.frontend.OptionParser(\r\n/usr/lib64/python3.13/optparse.py:1000: DeprecationWarning: The frontend.Option class will be removed in Docutils 0.21 or later.\r\n  option = self.option_class(*args, **kwargs)\r\n```\r\n\r\n### What version of Python and Zope/Addons I am using:\r\n\r\n<!-- Enter Operating system, Python and Zope versions you are using -->\r\n\r\nFedora Linux 39, Python 3.13.0a6, Zope versions as installed to the virtualenv by default: zope.exceptions 5.0.1, zope.interface  6.3, zope.testrunner 6.4\r\n\r\n-----\r\n\r\nIt wasn\u2019t immediately obvious to me what the best way to fix this should be. I believe it may be related to https://github.com/python/cpython/issues/109461 / https://github.com/python/cpython/pull/109462 / https://github.com/python/cpython/pull/109462.\n", "patch": "diff --git a/.meta.toml b/.meta.toml\nindex f5a50fb..9904c8b 100644\n--- a/.meta.toml\n+++ b/.meta.toml\n@@ -2,12 +2,12 @@\n # https://github.com/zopefoundation/meta/tree/master/config/pure-python\n [meta]\n template = \"pure-python\"\n-commit-id = \"7713fd86\"\n+commit-id = \"e7051a16\"\n \n [python]\n with-windows = false\n with-pypy = true\n-with-future-python = false\n+with-future-python = true\n with-docs = true\n with-sphinx-doctests = false\n with-macos = false\ndiff --git a/CHANGES.rst b/CHANGES.rst\nindex baa0ce4..2d0c9b6 100644\n--- a/CHANGES.rst\n+++ b/CHANGES.rst\n@@ -5,6 +5,8 @@\n 4.2 (unreleased)\n ================\n \n+- Add support for Python 3.13.\n+\n \n 4.1 (2024-05-03)\n ================\ndiff --git a/setup.py b/setup.py\nindex d5fa795..c272965 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -68,6 +68,7 @@\n         'Programming Language :: Python :: 3.10',\n         'Programming Language :: Python :: 3.11',\n         'Programming Language :: Python :: 3.12',\n+        'Programming Language :: Python :: 3.13',\n         'Programming Language :: Python :: Implementation :: CPython',\n         'Programming Language :: Python :: Implementation :: PyPy',\n         'Operating System :: OS Independent',\ndiff --git a/tox.ini b/tox.ini\nindex 040f33c..29ffc2d 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -11,6 +11,7 @@ envlist =\n     py310\n     py311\n     py312\n+    py313\n     pypy3\n     docs\n     coverage\n@@ -19,6 +20,7 @@ envlist =\n usedevelop = true\n package = wheel\n wheel_build_env = .pkg\n+pip_pre = py313: true\n deps =\n setenv =\n     py312: VIRTUALENV_PIP=23.1.2\n", "instance_id": "zopefoundation__ZConfig-97", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a test failure in `test_filehandler_reopen_thread_safety` when running on Python 3.13. It provides steps to reproduce the issue, the expected outcome, the actual outcome with a detailed traceback, and relevant environment details (Python version, OS, and Zope versions). However, there are minor ambiguities that prevent it from being comprehensive. The problem statement does not explicitly explain the root cause of the failure beyond referencing a related CPython issue/PR, and it lacks specific guidance on what changes are needed in the codebase to resolve the issue. Additionally, edge cases or specific constraints related to thread safety or file handler reopening are not mentioned, which could be critical for a complete understanding of the problem. Overall, while the issue is well-documented, it leaves some interpretation to the developer regarding the exact fix.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the issue involves a test failure related to thread safety in file handler reopening, which suggests a need to understand and potentially modify concurrency-related logic in the codebase. The provided code changes are minimal and primarily involve configuration updates (e.g., adding Python 3.13 support in `setup.py` and `tox.ini`), but these changes do not address the core test failure. This indicates that the actual fix likely requires deeper modifications in the test or implementation code, possibly in `ZConfig/components/logger/tests/test_logger.py` or related modules, which are not shown in the diff. Second, the technical concepts involved include thread safety, file handling, and Python's internal changes in version 3.13 (potentially related to the referenced CPython issue on locking mechanisms), which require a solid understanding of Python's threading model and logging system. Third, the problem likely involves edge cases related to concurrent access to file handlers, though these are not explicitly detailed in the statement. Finally, while the changes do not appear to impact the overall system architecture, resolving the test failure may require a nuanced understanding of interactions between logging components and threading in the ZConfig library. Given these factors, I rate the difficulty as 0.65, reflecting a challenging problem that demands a deep dive into specific Python internals and concurrency issues, but not reaching the level of a complete architectural overhaul or extremely advanced domain-specific knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove the yellow star (main identity)\nThe main identity of an individual has no practical effect for UI users.\r\n\r\nThe yellow star marking it in the UI calls the users' attention and makes them wonder if they should pick another identity as the main one (which cannot be done easily in the UI).\r\n\r\nOnly for advanced technical users might it make any sense, and even in such case, choosing a more subtle visual indicator (like a small dot) would prevent misleading the average users.\r\n\n", "patch": "diff --git a/releases/unreleased/less-distracting-main-identity-indicator.yml b/releases/unreleased/less-distracting-main-identity-indicator.yml\nnew file mode 100644\nindex 00000000..5df1ebaf\n--- /dev/null\n+++ b/releases/unreleased/less-distracting-main-identity-indicator.yml\n@@ -0,0 +1,10 @@\n+---\n+title: Less distracting main identity indicator\n+category: added\n+author: Eva Mill\u00e1n <evamillan@bitergia.com>\n+issue: 920\n+notes: >\n+  The star icon that marked an individual's main identity\n+  was distracting and potentially misleading for users.\n+  It has changed to a more subtle indicator since it is not\n+  relevant for most users.\ndiff --git a/ui/src/components/Identity.vue b/ui/src/components/Identity.vue\nindex fb2956e9..e97907a2 100644\n--- a/ui/src/components/Identity.vue\n+++ b/ui/src/components/Identity.vue\n@@ -1,30 +1,30 @@\n <template>\n   <v-row class=\"d-flex align-center flex-nowrap\" no-gutters>\n     <v-col class=\"uuid d-flex align-center\">\n-      <v-tooltip open-delay=\"100\" bottom>\n-        <template v-slot:activator=\"{ props }\">\n-          <v-chip\n-            class=\"text-center\"\n-            :class=\"{ 'mr-6': !isMain }\"\n-            v-bind=\"props\"\n-            variant=\"outlined\"\n-            @click=\"copy(uuid)\"\n-            @mouseenter=\"resetCopyText\"\n-          >\n-            <span class=\"clip\">{{ uuid }}</span>\n-            <v-icon size=\"small\" end>mdi-content-copy</v-icon>\n-          </v-chip>\n-        </template>\n-        <span>{{ tooltip }}</span>\n-      </v-tooltip>\n-      <v-tooltip v-if=\"isMain\" bottom>\n-        <template v-slot:activator=\"{ props }\">\n-          <v-icon v-bind=\"props\" color=\"secondary\" size=\"small\" end>\n-            mdi-star\n-          </v-icon>\n-        </template>\n-        <span>Main identity</span>\n-      </v-tooltip>\n+      <v-chip\n+        class=\"text-center pr-2\"\n+        :class=\"{ 'v-chip--border': isMain }\"\n+        :variant=\"isMain ? 'tonal' : 'outlined'\"\n+      >\n+        <span class=\"clip mr-1\">{{ uuid }}</span>\n+        <span v-if=\"isMain\" class=\"d-sr-only\">\n+          Main identity\n+        </span>\n+        <v-tooltip open-delay=\"100\" location=\"bottom\">\n+          <template v-slot:activator=\"{ props }\">\n+            <v-btn\n+              v-bind=\"props\"\n+              density=\"comfortable\"\n+              icon=\"mdi-content-copy\"\n+              size=\"small\"\n+              variant=\"text\"\n+              @click=\"copy(uuid)\"\n+              @mouseenter=\"resetCopyText\"\n+            />\n+          </template>\n+          <span>{{ tooltip }}</span>\n+        </v-tooltip>\n+      </v-chip>\n     </v-col>\n     <v-col class=\"ma-2\" md=\"2\">\n       <span>{{ name }}</span>\n@@ -118,4 +118,8 @@ export default {\n .v-tooltip__content {\n   font-size: 12px;\n }\n+\n+:deep(.v-chip--variant-tonal) .v-chip__underlay {\n+  opacity: 0.04;\n+}\n </style>\n", "instance_id": "chaoss__grimoirelab-sortinghat-926", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to address a UI issue by changing the visual indicator for the main identity from a distracting yellow star to a more subtle indicator. The goal is explicitly stated, and the reasoning behind the change (avoiding user confusion) is provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the exact nature of the new indicator (though the code changes reveal it), nor does it mention any specific constraints or requirements for backward compatibility, accessibility, or user experience testing. Additionally, there are no examples or mockups provided to illustrate the desired outcome. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. The code changes are confined to a single file (Identity.vue) and involve straightforward modifications to the UI component using Vue.js and Vuetify. The scope of the change is limited to updating the visual representation of the main identity indicator by replacing an icon with a different styling approach (using tonal variant and subtle underlay opacity). It does not impact the broader system architecture or require understanding complex interactions between modules. The technical concepts involved are basic: familiarity with Vue.js component syntax, Vuetify component properties, and CSS styling. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code changes. The amount of code modified is small, and the task primarily involves UI tweaking rather than deep logical or algorithmic work. A score of 0.25 reflects the simplicity of the task while acknowledging that it requires some understanding of the UI framework and component styling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Array creation methods\n### From shape or value\r\n- ~`empty`~\r\n- ~`empty_like`~\r\n- [x] `eye` (#491)\r\n- [x] `identity` (#491)\r\n- [x] `ones` (#491)\r\n- [x] `ones_like` (#491)\r\n- [x] `zeros` (#491)\r\n- [x] `zeros_like` (#491)\r\n- [x] `full` (#491)\r\n- [x] `full_like` (#491)\r\n\r\n`ones` and `zeros` should be static methods on each array class. `ones_like` and `zeros_like` can be methods on the array class and have Python methods. `full` and `full_like` should take both floats/ints and the corresponding APy-scalar and should probably warn if the APy-scalar is quantized (has a longer word length than the resulting array).\r\n\r\n### From existing data\r\n- ~`array`~\r\n- ~`asarray`~\r\n- ~`asanyarray`~\r\n- ~`ascontiguousarray`~ ?\r\n- ~`asmatrix`~\r\n- ~`astype`~\r\n- [ ] `copy` can have a Python function.\r\n- ~`frombuffer`~\r\n- ~`from_dlpack`~ ?\r\n- [ ] `fromfile` \r\n- [ ] `fromfunction`\r\n- [ ] `fromiter`\r\n- [ ] `fromstring`\r\n- [ ] `loadtxt`\r\n\r\n### ~Creating record arrays~\r\n\r\n### ~Creating character arrays~\r\n\r\n### Numerical ranges\r\n- [x] `arange` (#491,  #513)\r\n- [ ] `linspace`\r\n- [ ] `logspace`\r\n- [ ] `geomspace`\r\n- [ ] `meshgrid`\r\n- [ ] `mgrid`\r\n- [ ] `ogrid`\r\n\r\n### Building matrices\r\n- [ ] `diag`\r\n- [ ] `diagflat`\r\n- [ ] `tri`\r\n- [ ] `tril`\r\n- [ ] `triu`\r\n- [ ] `vander`\r\n\r\n### ~The matrix class~\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 6c38e0f3b..e6a2d008d 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -42,6 +42,15 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n   - `min()`\n   - `nanmax()`\n   - `nanmin()`\n+- Array creation functions added:\n+  - `eye()`\n+  - `identity()`\n+  - `ones()`\n+  - `ones_like()`\n+  - `zeros()`\n+  - `zeros_like()`\n+  - `full()`\n+  - `full_like()`\n - Added `array_utils.h` to contain shared functions between float and fixed arrays.\n - Added support for higher dimensions (ndim > 2) in `transpose()`.\n \ndiff --git a/docs/api/apyfixedarray.rst b/docs/api/apyfixedarray.rst\nindex ad7351a9e..c122b20ce 100644\n--- a/docs/api/apyfixedarray.rst\n+++ b/docs/api/apyfixedarray.rst\n@@ -15,6 +15,19 @@\n \n    .. automethod:: from_float\n \n+   Other creation functions\n+   ------------------------\n+\n+   .. automethod:: zeros\n+\n+   .. automethod:: ones\n+\n+   .. automethod:: eye\n+\n+   .. automethod:: identity\n+\n+   .. automethod:: full\n+\n    Change word length\n    ------------------\n \ndiff --git a/docs/api/apyfloatarray.rst b/docs/api/apyfloatarray.rst\nindex 5028f64e5..26388c021 100644\n--- a/docs/api/apyfloatarray.rst\n+++ b/docs/api/apyfloatarray.rst\n@@ -15,6 +15,19 @@\n \n    .. automethod:: from_float\n \n+   Other creation functions\n+   ------------------------\n+\n+   .. automethod:: zeros\n+\n+   .. automethod:: ones\n+\n+   .. automethod:: eye\n+\n+   .. automethod:: identity\n+\n+   .. automethod:: full\n+\n    Change word length\n    ------------------\n \ndiff --git a/docs/api/arrayfunctions.rst b/docs/api/arrayfunctions.rst\nindex b0aa0b651..0619e5618 100644\n--- a/docs/api/arrayfunctions.rst\n+++ b/docs/api/arrayfunctions.rst\n@@ -14,3 +14,19 @@ Array functions\n .. autofunction:: apytypes.swapaxes\n \n .. autofunction:: apytypes.expand_dims\n+\n+.. autofunction:: apytypes.zeros\n+\n+.. autofunction:: apytypes.ones\n+\n+.. autofunction:: apytypes.eye\n+\n+.. autofunction:: apytypes.identity\n+\n+.. autofunction:: apytypes.full\n+\n+.. autofunction:: apytypes.zeros_like\n+\n+.. autofunction:: apytypes.ones_like\n+\n+.. autofunction:: apytypes.full_like\ndiff --git a/lib/apytypes/__init__.py b/lib/apytypes/__init__.py\nindex 601bbae4a..689061dfe 100644\n--- a/lib/apytypes/__init__.py\n+++ b/lib/apytypes/__init__.py\n@@ -28,6 +28,15 @@\n     moveaxis,\n     swapaxes,\n     expand_dims,\n+    zeros,\n+    zeros_like,\n+    ones,\n+    ones_like,\n+    eye,\n+    identity,\n+    full,\n+    full_like,\n+    arange,\n )\n \n from apytypes._version import version as __version__\n@@ -58,6 +67,15 @@\n     \"moveaxis\",\n     \"swapaxes\",\n     \"expand_dims\",\n+    \"zeros\",\n+    \"zeros_like\",\n+    \"ones\",\n+    \"ones_like\",\n+    \"eye\",\n+    \"identity\",\n+    \"full\",\n+    \"full_like\",\n+    \"arange\",\n ]\n \n APyFloat.__doc__ = r\"\"\"\ndiff --git a/lib/apytypes/__init__.pyi b/lib/apytypes/__init__.pyi\nindex e2497c146..dbdee15c0 100644\n--- a/lib/apytypes/__init__.pyi\n+++ b/lib/apytypes/__init__.pyi\n@@ -23,15 +23,24 @@ from ._apytypes import (\n     set_float_quantization_seed as set_float_quantization_seed,\n )\n from ._array_functions import (\n+    arange as arange,\n     convolve as convolve,\n     expand_dims as expand_dims,\n+    eye as eye,\n+    full as full,\n+    full_like as full_like,\n+    identity as identity,\n     moveaxis as moveaxis,\n+    ones as ones,\n+    ones_like as ones_like,\n     ravel as ravel,\n     reshape as reshape,\n     shape as shape,\n     squeeze as squeeze,\n     swapaxes as swapaxes,\n     transpose as transpose,\n+    zeros as zeros,\n+    zeros_like as zeros_like,\n )\n \n __all__: list = [\n@@ -60,6 +69,15 @@ __all__: list = [\n     \"moveaxis\",\n     \"swapaxes\",\n     \"expand_dims\",\n+    \"zeros\",\n+    \"zeros_like\",\n+    \"ones\",\n+    \"ones_like\",\n+    \"eye\",\n+    \"identity\",\n+    \"full\",\n+    \"full_like\",\n+    \"arange\",\n ]\n \n annotations: __future__._Feature = ...\ndiff --git a/lib/apytypes/_apytypes.pyi b/lib/apytypes/_apytypes.pyi\nindex 65171b11a..346aa3343 100644\n--- a/lib/apytypes/_apytypes.pyi\n+++ b/lib/apytypes/_apytypes.pyi\n@@ -1330,6 +1330,135 @@ class APyFixedArray:\n         :class:`APyFixedArray`\n         \"\"\"\n \n+    @staticmethod\n+    def zeros(\n+        shape: tuple,\n+        int_bits: int | None = None,\n+        frac_bits: int | None = None,\n+        bits: int | None = None,\n+    ) -> APyFixedArray:\n+        \"\"\"\n+        Initializes an array with zeros.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array initialized with zeros.\n+        \"\"\"\n+\n+    @staticmethod\n+    def ones(\n+        shape: tuple,\n+        int_bits: int | None = None,\n+        frac_bits: int | None = None,\n+        bits: int | None = None,\n+    ) -> APyFixedArray:\n+        \"\"\"\n+        Initializes an array with ones.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array initialized with ones.\n+        \"\"\"\n+\n+    @staticmethod\n+    def eye(\n+        n: int,\n+        m: int | None = None,\n+        int_bits: int | None = None,\n+        frac_bits: int | None = None,\n+        bits: int | None = None,\n+    ) -> APyFixedArray:\n+        \"\"\"\n+        Initializes an array with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows.\n+        m : int, optional\n+            Number of columns. Default is None.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array with the specified value on the diagonal.\n+        \"\"\"\n+\n+    @staticmethod\n+    def identity(\n+        n: int,\n+        int_bits: int | None = None,\n+        frac_bits: int | None = None,\n+        bits: int | None = None,\n+    ) -> APyFixedArray:\n+        \"\"\"\n+        Initializes an identity matrix with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows (and columns) in n x n output.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An identity matrix with ones on the diagonal.\n+        \"\"\"\n+\n+    @staticmethod\n+    def full(shape: tuple, fill_value: APyFixed) -> APyFixedArray:\n+        \"\"\"\n+        Initializes an array with the specified value.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        fill_value : APyFixed\n+            Value to fill the array.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array filled with the specified value.\n+        \"\"\"\n+\n     def __lshift__(self, shift_amnt: int) -> APyFixedArray: ...\n     def __matmul__(self, rhs: APyFixedArray) -> APyFixedArray: ...\n     def __repr__(self) -> str: ...\n@@ -2390,6 +2519,126 @@ class APyFloatArray:\n         :class:`APyFloatArray`\n         \"\"\"\n \n+    @staticmethod\n+    def zeros(\n+        shape: tuple, exp_bits: int, man_bits: int, bias: int | None = None\n+    ) -> APyFloatArray:\n+        \"\"\"\n+        Initializes an array with zeros.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array filled with zeros.\n+        \"\"\"\n+\n+    @staticmethod\n+    def ones(\n+        shape: tuple, exp_bits: int, man_bits: int, bias: int | None = None\n+    ) -> APyFloatArray:\n+        \"\"\"\n+        Initializes an array with ones.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array filled with ones.\n+        \"\"\"\n+\n+    @staticmethod\n+    def eye(\n+        n: int,\n+        exp_bits: int,\n+        man_bits: int,\n+        m: int | None = None,\n+        bias: int | None = None,\n+    ) -> APyFloatArray:\n+        \"\"\"\n+        Initializes an array with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows (and columns) in the n x n output.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        m : int, optional\n+            Number of columns. Default is None, which results in an n x n output.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array with the specified value on the diagonal.\n+        \"\"\"\n+\n+    @staticmethod\n+    def identity(\n+        n: int, exp_bits: int, man_bits: int, bias: int | None = None\n+    ) -> APyFloatArray:\n+        \"\"\"\n+        Initializes an identity matrix with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows (and columns) in the n x n output.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An identity matrix with ones on the diagonal.\n+        \"\"\"\n+\n+    @staticmethod\n+    def full(shape: tuple, fill_value: APyFloat) -> APyFloatArray:\n+        \"\"\"\n+        Initializes an array filled with the specified value.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        fill_value : APyFloat\n+            Value to fill the array.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array filled with the specified value.\n+        \"\"\"\n+\n     def __matmul__(self, rhs: APyFloatArray) -> APyFloatArray | APyFloat: ...\n     def __repr__(self) -> str: ...\n     def __len__(self) -> int: ...\ndiff --git a/lib/apytypes/_array_functions.py b/lib/apytypes/_array_functions.py\nindex 00b9ded37..f9e6bf467 100644\n--- a/lib/apytypes/_array_functions.py\n+++ b/lib/apytypes/_array_functions.py\n@@ -411,9 +411,478 @@ def expand_dims(a, axis):\n     return a.reshape(shape)\n \n \n+# =============================================================================\n+# array creation\n+# =============================================================================\n+\n+\n+def zeros(shape, int_bits=None, frac_bits=None, exp_bits=None, mantissa_bits=None):\n+    \"\"\"\n+    Initializes an array with zeros.\n+\n+    Word lengths need to be specified and decide the return type of the array.\n+\n+    Parameters\n+    ----------\n+    shape : tuple\n+        Shape of the array.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The initialized array with zeros.\n+    \"\"\"\n+\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    a_type = _determine_array_type(int_bits, frac_bits, exp_bits, mantissa_bits)\n+    try:\n+        zeros = a_type.zeros\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make zeros array of type {type(a_type)}\")\n+    if a_type is APyFixedArray:\n+        return zeros(shape=shape, int_bits=int_bits, frac_bits=frac_bits)\n+\n+    if a_type is APyFloatArray:\n+        return zeros(shape=shape, exp_bits=exp_bits, man_bits=mantissa_bits)\n+\n+    raise ValueError(\"Only 'fixed' and 'float' array_types are defined\")\n+\n+\n+def ones(shape, int_bits=None, frac_bits=None, exp_bits=None, mantissa_bits=None):\n+    \"\"\"\n+    Initializes an array with ones.\n+\n+    Word lengths need to be specified and decide the return type of the array.\n+\n+    Parameters\n+    ----------\n+    shape : tuple\n+        Shape of the array.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The array initialized filled with ones.\n+    \"\"\"\n+\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    a_type = _determine_array_type(int_bits, frac_bits, exp_bits, mantissa_bits)\n+    try:\n+        ones = a_type.ones\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make ones array of type {type(a_type)}\")\n+    if a_type is APyFixedArray or isinstance(a_type, APyFixedArray):\n+        return ones(shape=shape, int_bits=int_bits, frac_bits=frac_bits)\n+\n+    if a_type is APyFloatArray or isinstance(a_type, APyFloatArray):\n+        return ones(shape=shape, exp_bits=exp_bits, man_bits=mantissa_bits)\n+\n+    raise ValueError(\"Only 'fixed' and 'float' array_types are defined\")\n+\n+\n+def eye(\n+    n: int, m=None, int_bits=None, frac_bits=None, exp_bits=None, mantissa_bits=None\n+):\n+    \"\"\"\n+    Return a 2-D array with ones on the diagonal and zeros elsewhere.\n+\n+    Word lengths need to be specified and decide the return type of the array.\n+\n+    Parameters\n+    ----------\n+    a_type : :class:`APyFloatArray` or :class:`APyFixedArray`, optional\n+        The type of array to initialize.\n+    n : int\n+        Number of rows in the output.\n+    m : int, optional\n+        Number of columns in the output. If None, defaults to N.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        An array where all elements are equal to zero, except for the k-th diagonal, whose values are equal to one.\n+    \"\"\"\n+\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    a_type = _determine_array_type(int_bits, frac_bits, exp_bits, mantissa_bits)\n+    try:\n+        eye = a_type.eye\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make eye array of type {type(a_type)}\")\n+    if a_type is APyFixedArray or isinstance(a_type, APyFixedArray):\n+        return eye(n=n, m=m, int_bits=int_bits, frac_bits=frac_bits)\n+\n+    if a_type is APyFloatArray or isinstance(a_type, APyFloatArray):\n+        return eye(n=n, m=m, exp_bits=exp_bits, man_bits=mantissa_bits)\n+\n+    raise ValueError(\"Only 'fixed' and 'float' array_types are defined\")\n+\n+\n+def identity(n, int_bits=None, frac_bits=None, exp_bits=None, mantissa_bits=None):\n+    \"\"\"\n+    Return the identity array.\n+\n+    Word lengths need to be specified and decides return type of Array.\n+\n+    Parameters\n+    ----------\n+    a_type : :class:`APyFloatArray` or :class:`APyFixedArray`, optional\n+        The type of array to initialize.\n+    n : int\n+        Number of rows (and columns) in n x n output.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        n x n array with its main diagonal set to one, and all other elements 0.\n+    \"\"\"\n+\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    a_type = _determine_array_type(int_bits, frac_bits, exp_bits, mantissa_bits)\n+    try:\n+        identity = a_type.identity\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make identity array of type {type(a_type)}\")\n+\n+    if a_type is APyFixedArray or isinstance(a_type, APyFixedArray):\n+        return identity(n=n, int_bits=int_bits, frac_bits=frac_bits)\n+\n+    if a_type is APyFloatArray or isinstance(a_type, APyFloatArray):\n+        return identity(n=n, exp_bits=exp_bits, man_bits=mantissa_bits)\n+\n+    raise ValueError(\"Only 'fixed' and 'float' array_types are defined\")\n+\n+\n+def full(\n+    shape,\n+    fill_value,\n+    int_bits=None,\n+    frac_bits=None,\n+    exp_bits=None,\n+    mantissa_bits=None,\n+):\n+    \"\"\"\n+    Return a new array of given shape and type, filled with fill_value.\n+\n+    If fill_value is an int or float, you must specify the word lengths (int_bits, frac_bits or exp_bits, mantissa_bits).\n+    If fill_value is an APyFloat or APyFixed, the array will use the provided word lengths if specified.\n+    If no word lengths are specified, the resulting array will inherit the word lengths from fill_value.\n+\n+    Parameters\n+    ----------\n+    shape : tuple\n+        Shape of the array.\n+    fill_value : :class:`APyFloat` or :class:`APyFixedArray` or int or float\n+        Fill value.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        Array of fill_value with the given shape.\n+    \"\"\"\n+\n+    from apytypes import APyFixedArray, APyFixed, APyFloat, APyFloatArray\n+\n+    fill_value = _normalize_fill_value(\n+        fill_value, int_bits, frac_bits, exp_bits, mantissa_bits\n+    )\n+    try:\n+        if isinstance(fill_value, APyFixed):\n+            full = APyFixedArray.full\n+        elif isinstance(fill_value, APyFloat):\n+            full = APyFloatArray.full\n+        else:\n+            raise (AttributeError)\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make full array of type {type(fill_value)}\")\n+    return full(shape, fill_value)\n+\n+\n+def zeros_like(a, int_bits=None, frac_bits=None, exp_bits=None, mantissa_bits=None):\n+    \"\"\"\n+    Return an array of zeros with the same shape and type as a given array.\n+    Defaults to `a` wordlength\n+\n+    Parameters\n+    ----------\n+    a : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The shape and data-type define these same attributes of the returned array.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The initialized array with zeros.\n+    \"\"\"\n+    try:\n+        zeros = a.zeros\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make zeros array of type {type(a)}\")\n+\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    if isinstance(a, APyFixedArray):\n+        int_bits = int_bits if int_bits is not None else a.int_bits\n+        frac_bits = frac_bits if int_bits is not None else a.frac_bits\n+        return zeros(shape=a.shape, int_bits=int_bits, frac_bits=frac_bits)\n+\n+    if isinstance(a, APyFloatArray):\n+        exp_bits = exp_bits if exp_bits is not None else a.exp_bits\n+        mantissa_bits = mantissa_bits if mantissa_bits is not None else a.man_bits\n+        return zeros(shape=a.shape, exp_bits=exp_bits, man_bits=mantissa_bits)\n+\n+    raise ValueError(\"Only 'fixed' and 'float' array_types are defined\")\n+\n+\n+def ones_like(a, int_bits=None, frac_bits=None, exp_bits=None, mantissa_bits=None):\n+    \"\"\"\n+    Return an array of ones with the same shape and type as a given array.\n+    Defaults to `a` wordlength\n+\n+    Parameters\n+    ----------\n+    a : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The shape and data-type of a define these same attributes of the returned array.\n+    int_bits : int, optional\n+        Number of integer bits.\n+    frac_bits : int, optional\n+        Number of fractional bits.\n+    exp_bits : int, optional\n+        Number of exponential bits.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The array initialized filled with ones.\n+    \"\"\"\n+    try:\n+        ones = a.ones\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make ones array of type {type(a)}\")\n+\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    if isinstance(a, APyFixedArray):\n+        int_bits = int_bits if int_bits is not None else a.int_bits\n+        frac_bits = frac_bits if frac_bits is not None else a.frac_bits\n+        return ones(shape=a.shape, int_bits=int_bits, frac_bits=frac_bits)\n+\n+    if isinstance(a, APyFloatArray):\n+        exp_bits = exp_bits if exp_bits is not None else a.exp_bits\n+        mantissa_bits = mantissa_bits if mantissa_bits is not None else a.man_bits\n+        return ones(shape=a.shape, exp_bits=exp_bits, man_bits=mantissa_bits)\n+\n+    raise ValueError(\"Only 'fixed' and 'float' array_types are defined\")\n+\n+\n+def full_like(\n+    a,\n+    fill_value,\n+    int_bits=None,\n+    frac_bits=None,\n+    exp_bits=None,\n+    mantissa_bits=None,\n+):\n+    \"\"\"\n+    Return a full array with the same shape and type as a given array.\n+\n+    If fill_value is an int or float, you must specify the word lengths (int_bits, frac_bits or exp_bits, mantissa_bits).\n+    If fill_value is an APyFloat or APyFixed, the array will use the provided word lengths if specified.\n+    If no word lengths are specified, the resulting array will inherit the word lengths from fill_value.\n+\n+    Parameters\n+    ----------\n+    a : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        The shape and array type of the returned array.\n+    fill_value : :class:`APyFloat` or :class:`APyFixed` or int or float\n+        The value to fill the array with.\n+    int_bits : int, optional\n+        Number of integer bits for APyFixed.\n+    frac_bits : int, optional\n+        Number of fractional bits for APyFixed.\n+    exp_bits : int, optional\n+        Number of exponent bits for APyFloat.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits for APyFloat.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        Array filled with fill_value, having the same shape and type as `a`.\n+    \"\"\"\n+    try:\n+        full = a.full\n+    except AttributeError:\n+        raise TypeError(f\"Cannot make full array of type {type(a)}\")\n+\n+    fill_value = _normalize_fill_value(\n+        fill_value, int_bits, frac_bits, exp_bits, mantissa_bits\n+    )\n+    return full(a.shape, fill_value)\n+\n+\n+def arange(\n+    start,\n+    stop=None,\n+    step=1,\n+    int_bits=None,\n+    frac_bits=None,\n+    exp_bits=None,\n+    mantissa_bits=None,\n+):\n+    \"\"\"\n+    Simple and naive version of arange.\n+\n+    Parameters\n+    ----------\n+    start : int, float\n+        start number\n+    stop : int, optional\n+        stop number\n+    step : int, float, optional\n+        step size in range\n+    int_bits : int, optional\n+        Number of integer bits for APyFixed.\n+    frac_bits : int, optional\n+        Number of fractional bits for APyFixed.\n+    exp_bits : int, optional\n+        Number of exponent bits for APyFloat.\n+    mantissa_bits : int, optional\n+        Number of mantissa bits for APyFloat.\n+\n+    Returns\n+    -------\n+    result : :class:`APyFloatArray` or :class:`APyFixedArray`\n+        Array filled with fill_value, having the same shape and type as `a`.\n+    \"\"\"\n+    if stop is None:\n+        start, stop = 0, start\n+    if step == 0:\n+        raise ValueError(\"Step must not be zero\")\n+\n+    floats = []\n+    value = start\n+    while (step > 0 and value < stop) or (step < 0 and value > stop):\n+        floats.append(value)\n+        value += step\n+\n+    a_type = _determine_array_type(int_bits, frac_bits, exp_bits, mantissa_bits)\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    if a_type is APyFixedArray:\n+        return APyFixedArray.from_float(floats, int_bits=int_bits, frac_bits=frac_bits)\n+    elif a_type is APyFloatArray:\n+        return APyFloatArray.from_float(\n+            floats, exp_bits=exp_bits, man_bits=mantissa_bits\n+        )\n+    raise ValueError(\"Undefined inputs\")\n+\n+\n # =============================================================================\n # Helpers\n # =============================================================================\n+def _determine_array_type(int_bits, frac_bits, exp_bits, mantissa_bits):\n+    from apytypes import APyFixedArray, APyFloatArray\n+\n+    if int_bits is not None and frac_bits is not None:\n+        return APyFixedArray\n+    elif exp_bits is not None and mantissa_bits is not None:\n+        return APyFloatArray\n+    raise ValueError(\"You need to specify wordlengths in this function\")\n+\n+\n+def _normalize_fill_value(\n+    fill_value,\n+    int_bits=None,\n+    frac_bits=None,\n+    exp_bits=None,\n+    mantissa_bits=None,\n+):\n+    from apytypes import APyFixed, APyFloat\n+\n+    if isinstance(fill_value, (int, float)):\n+        if int_bits is not None and frac_bits is not None:\n+            return APyFixed.from_float(\n+                fill_value, int_bits=int_bits, frac_bits=frac_bits\n+            )\n+        elif exp_bits is not None and mantissa_bits is not None:\n+            return APyFloat.from_float(\n+                fill_value, exp_bits=exp_bits, man_bits=mantissa_bits\n+            )\n+        else:\n+            raise ValueError(\n+                \"You need to specify wordlengths if input is a Python float or integer\"\n+            )\n+\n+    elif isinstance(fill_value, APyFixed):\n+        int_bits = int_bits if int_bits is not None else fill_value.int_bits\n+        frac_bits = frac_bits if frac_bits is not None else fill_value.frac_bits\n+        return fill_value.cast(int_bits, frac_bits)\n+\n+    elif isinstance(fill_value, APyFloat):\n+        exp_bits = exp_bits if exp_bits is not None else fill_value.exp_bits\n+        mantissa_bits = (\n+            mantissa_bits if mantissa_bits is not None else fill_value.man_bits\n+        )\n+        return fill_value.cast(exp_bits, mantissa_bits, fill_value.bias)\n+\n+    else:\n+        raise ValueError(\n+            \"Only int, float, APyFloat and APyFixed are supported for fill_value\"\n+        )\n \n \n def _normalize_axis(axis: int, ndim: int) -> int:\ndiff --git a/lib/apytypes/_array_functions.pyi b/lib/apytypes/_array_functions.pyi\nindex 47f5887fe..8b1aa65cb 100644\n--- a/lib/apytypes/_array_functions.pyi\n+++ b/lib/apytypes/_array_functions.pyi\n@@ -1,6 +1,6 @@\n-from typing import overload, Sequence\n-from apytypes import APyFloatArray, APyFixedArray\n-from apytypes.typing import APyArray\n+from typing import overload, Sequence, Union\n+from apytypes import APyFloatArray, APyFixedArray, APyFixed, APyFloat\n+from apytypes.typing import APyArray, APyScalar\n \n @overload\n def squeeze(\n@@ -61,3 +61,220 @@ def expand_dims(a: APyFloatArray, axis: int | Sequence[int]) -> APyFloatArray: .\n @overload\n def expand_dims(a: APyFixedArray, axis: int | Sequence[int]) -> APyFixedArray: ...\n def expand_dims(a: APyArray, axis: int | Sequence[int]) -> APyArray: ...\n+@overload\n+def identity(\n+    n: int,\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def identity(\n+    n: int,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\n+@overload\n+def eye(\n+    n: int,\n+    m: int = None,\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def eye(\n+    n: int,\n+    m: int = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\n+@overload\n+def zeros(\n+    shape: tuple[int, ...],\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def zeros(\n+    shape: tuple[int, ...],\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\n+@overload\n+def ones(\n+    shape: tuple[int, ...],\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def ones(\n+    shape: tuple[int, ...],\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\n+@overload\n+def full(\n+    shape: tuple[int, ...],\n+    fill_value: APyScalar = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyArray: ...\n+@overload\n+def full(\n+    shape: tuple[int, ...],\n+    fill_value: APyFixed = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+@overload\n+def full(\n+    shape: tuple[int, ...],\n+    fill_value: APyFloat = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFloatArray: ...\n+@overload\n+def full(\n+    shape: tuple[int, ...],\n+    fill_value: Union[int, float] = None,\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+@overload\n+def full(\n+    shape: tuple[int, ...],\n+    fill_value: Union[int, float] = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\n+@overload\n+def zeros_like(\n+    a: APyArray,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyArray: ...\n+@overload\n+def zeros_like(\n+    a: APyFixedArray,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def zeros_like(\n+    a: APyFloatArray,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFloatArray: ...\n+@overload\n+def ones_like(\n+    a: APyArray,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyArray: ...\n+@overload\n+def ones_like(\n+    a: APyFixedArray,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def ones_like(\n+    a: APyFloatArray,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFloatArray: ...\n+@overload\n+def full_like(\n+    a: APyArray,\n+    fill_value: APyScalar = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyArray: ...\n+@overload\n+def full_like(\n+    a: APyArray,\n+    fill_value: APyFixed = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+@overload\n+def full_like(\n+    a: APyArray,\n+    fill_value: APyFloat = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFloatArray: ...\n+@overload\n+def full_like(\n+    a: APyArray,\n+    fill_value: Union[int, float] = None,\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+def full_like(\n+    a: APyArray,\n+    fill_value: Union[int, float] = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\n+@overload\n+def arange(\n+    start: Union[int, float],\n+    stop: Union[int, float] = None,\n+    step: Union[int, float, None] = None,\n+    int_bits: int = None,\n+    frac_bits: int = None,\n+    exp_bits: None = None,\n+    mantissa_bits: None = None,\n+) -> APyFixedArray: ...\n+@overload\n+def arange(\n+    start: Union[int, float],\n+    stop: Union[int, float] = None,\n+    step: Union[int, float, None] = None,\n+    int_bits: None = None,\n+    frac_bits: None = None,\n+    exp_bits: int = None,\n+    mantissa_bits: int = None,\n+) -> APyFloatArray: ...\ndiff --git a/lib/apytypes/_typing.py b/lib/apytypes/_typing.py\nindex 139444794..82d42d8cd 100644\n--- a/lib/apytypes/_typing.py\n+++ b/lib/apytypes/_typing.py\n@@ -1,6 +1,7 @@\n from typing import Union\n-from apytypes import APyFloatArray, APyFixedArray\n+from apytypes import APyFloatArray, APyFixedArray, APyFloat, APyFixed\n # The following are type aliases. Once Python 3.9 is dropped, they should be annotated\n # using ``typing.TypeAlias`` and Unions should be converted to using ``|`` syntax.\n \n APyArray = Union[APyFixedArray, APyFloatArray]\n+APyScalar = Union[APyFixed, APyFloat]\ndiff --git a/src/apyfixed_util.h b/src/apyfixed_util.h\nindex 0cab319dd..6ca2e9e29 100644\n--- a/src/apyfixed_util.h\n+++ b/src/apyfixed_util.h\n@@ -20,6 +20,22 @@ mp_limb_t get_data_from_double(double value, int bits, int frac_bits, int shift_\n //! Fast integer power by squaring.\n APyFixed ipow(APyFixed base, unsigned int n);\n \n+//! Get bit pattern for the value one\n+static APY_INLINE APyFixed one(int bits, int int_bits)\n+{\n+    std::size_t bit_index = bits - int_bits;\n+    std::size_t limb_bits = sizeof(mp_limb_t) * 8;\n+    std::size_t limb_index = bit_index / limb_bits;\n+    std::size_t bit_offset = bit_index % limb_bits;\n+\n+    std::size_t num_limbs = limb_index + 1;\n+    std::vector<mp_limb_t> data(num_limbs, static_cast<mp_limb_t>(0));\n+\n+    // Set the specified bit to 1\n+    data[limb_index] |= static_cast<mp_limb_t>(1) << bit_offset;\n+\n+    return APyFixed(bits, int_bits, data);\n+}\n /* ********************************************************************************** *\n  * *    Fixed-point iterator based in-place quantization with multi-limb support    * *\n  * ********************************************************************************** */\ndiff --git a/src/apyfixedarray.cc b/src/apyfixedarray.cc\nindex 9d2cf8d1e..63435c283 100644\n--- a/src/apyfixedarray.cc\n+++ b/src/apyfixedarray.cc\n@@ -1,5 +1,4 @@\n-// Python object access through Nanobind\n-#include <nanobind/nanobind.h>\n+#include \"nanobind/nanobind.h\"\n #include <nanobind/ndarray.h>\n #include <nanobind/stl/variant.h> // std::variant (with nanobind support)\n #include <variant>\n@@ -1762,6 +1761,82 @@ APyFixedArray APyFixedArray::from_array(\n     return result;\n }\n \n+/* ****************************************************************************** *\n+ *                    Static methods for creating arrays                          *\n+ * ****************************************************************************** */\n+\n+APyFixedArray APyFixedArray::zeros(\n+    const nb::tuple& shape,\n+    std::optional<int> int_bits,\n+    std::optional<int> frac_bits,\n+    std::optional<int> bits\n+)\n+{\n+    std::vector<std::size_t> new_shape = ::shape_from_tuple(shape);\n+    APyFixedArray result(new_shape, int_bits, frac_bits, bits);\n+    return result;\n+}\n+\n+APyFixedArray APyFixedArray::ones(\n+    const nb::tuple& shape,\n+    std::optional<int> int_bits,\n+    std::optional<int> frac_bits,\n+    std::optional<int> bits\n+)\n+{\n+    const int final_int_bits\n+        = int_bits ? int_bits.value() : bits.value() - frac_bits.value();\n+    const int final_bits = bits ? bits.value() : int_bits.value() + frac_bits.value();\n+\n+    APyFixed fixed_one = one(final_bits, final_int_bits);\n+    return full(shape, fixed_one);\n+}\n+\n+APyFixedArray APyFixedArray::eye(\n+    const nb::int_& N,\n+    std::optional<nb::int_> M,\n+    std::optional<int> int_bits,\n+    std::optional<int> frac_bits,\n+    std::optional<int> bits\n+)\n+{\n+    const int final_int_bits\n+        = int_bits ? int_bits.value() : bits.value() - frac_bits.value();\n+    const int final_bits = bits ? bits.value() : int_bits.value() + frac_bits.value();\n+    APyFixed fixed_one = one(final_bits, final_int_bits);\n+\n+    // Use N for both dimensions if M is not provided\n+    nb::tuple shape = M ? nb::make_tuple(N, M.value()) : nb::make_tuple(N, N);\n+\n+    return diagonal(shape, fixed_one);\n+}\n+\n+APyFixedArray APyFixedArray::identity(\n+    const nb::int_& N,\n+    std::optional<int> int_bits,\n+    std::optional<int> frac_bits,\n+    std::optional<int> bits\n+)\n+{\n+    return eye(N, std::nullopt, int_bits, frac_bits, bits);\n+}\n+\n+APyFixedArray APyFixedArray::full(const nb::tuple& shape, const APyFixed& fill_value)\n+{\n+    std::vector<std::size_t> new_shape = ::shape_from_tuple(shape);\n+    APyFixedArray result(new_shape, fill_value.bits(), fill_value.int_bits());\n+\n+    std::size_t num_elem = fold_shape(new_shape);\n+    std::size_t itemsize = result._itemsize;\n+\n+    for (std::size_t index = 0; index < num_elem; ++index) {\n+        std::copy_n(\n+            fill_value._data.begin(), itemsize, result._data.begin() + index * itemsize\n+        );\n+    }\n+    return result;\n+}\n+\n /* ********************************************************************************** *\n  * *                            Private member functions                            * *\n  * ********************************************************************************** */\n@@ -2326,3 +2401,27 @@ void APyFixedArray::_set_values_from_ndarray(const nb::ndarray<nb::c_contig>& nd\n         \"unsupported `dtype` expecting integer/float\"\n     );\n }\n+\n+APyFixedArray\n+APyFixedArray::diagonal(const nb::tuple& shape, const APyFixed& fill_value)\n+{\n+    std::vector<std::size_t> new_shape = ::shape_from_tuple(shape);\n+    if (new_shape.size() > 2) {\n+        throw nb::value_error(\n+            \"Creating higher dimensional diagonal arrays are not yet defined\"\n+        );\n+    }\n+    APyFixedArray result(new_shape, fill_value.bits(), fill_value.int_bits());\n+    std::size_t itemsize = result._itemsize;\n+\n+    std::size_t min_dim = *std::min_element(new_shape.begin(), new_shape.end());\n+    std::vector<std::size_t> strides = ::strides_from_shape(new_shape);\n+\n+    for (std::size_t i = 0; i < min_dim; ++i) {\n+        std::size_t index = i * std::accumulate(strides.begin(), strides.end(), 0);\n+        std::copy_n(\n+            fill_value._data.begin(), itemsize, result._data.begin() + index * itemsize\n+        );\n+    }\n+    return result;\n+}\ndiff --git a/src/apyfixedarray.h b/src/apyfixedarray.h\nindex 36725bec3..7f2b0b1f9 100644\n--- a/src/apyfixedarray.h\n+++ b/src/apyfixedarray.h\n@@ -318,6 +318,47 @@ class APyFixedArray : public APyBuffer<mp_limb_t> {\n         std::optional<int> bits = std::nullopt\n     );\n \n+    /* ****************************************************************************** *\n+     *                     Static methods for array initialization                    *\n+     * ****************************************************************************** */\n+\n+    //! Create an `APyFixedArray` initialized with zeros\n+    static APyFixedArray zeros(\n+        const nb::tuple& shape,\n+        std::optional<int> int_bits = std::nullopt,\n+        std::optional<int> frac_bits = std::nullopt,\n+        std::optional<int> bits = std::nullopt\n+    );\n+    //! Create an `APyFixedArray` initialized with ones\n+    static APyFixedArray ones(\n+        const nb::tuple& shape,\n+        std::optional<int> int_bits = std::nullopt,\n+        std::optional<int> frac_bits = std::nullopt,\n+        std::optional<int> bits = std::nullopt\n+    );\n+\n+    //! Create an `APyFixedArray` with ones on the diagonal and zeros elsewhere\n+    static APyFixedArray\n+    eye(const nb::int_& N,\n+        std::optional<nb::int_> M = std::nullopt,\n+        std::optional<int> int_bits = std::nullopt,\n+        std::optional<int> frac_bits = std::nullopt,\n+        std::optional<int> bits = std::nullopt);\n+\n+    //! Create a square `APyFixedArray` with ones on the diagonal and zeros elsewhere\n+    static APyFixedArray identity(\n+        const nb::int_& N,\n+        std::optional<int> int_bits = std::nullopt,\n+        std::optional<int> frac_bits = std::nullopt,\n+        std::optional<int> bits = std::nullopt\n+    );\n+\n+    //! Create an `APyFixedArray` initialized with a specified fill value\n+    static APyFixedArray full(const nb::tuple& shape, const APyFixed& fill_value);\n+\n+    //! Create an `APyFixedArray` with a specified diagonal value\n+    static APyFixedArray diagonal(const nb::tuple& shape, const APyFixed& fill_value);\n+\n private:\n     /* ****************************************************************************** *\n      * *                          Private member functions                          * *\ndiff --git a/src/apyfixedarray_wrapper.cc b/src/apyfixedarray_wrapper.cc\nindex 6cb778c2b..0a3a09c74 100644\n--- a/src/apyfixedarray_wrapper.cc\n+++ b/src/apyfixedarray_wrapper.cc\n@@ -1083,6 +1083,138 @@ void bind_fixed_array(nb::module_& m)\n             :class:`APyFixedArray`\n             )pbdoc\"\n         )\n+        .def_static(\n+            \"zeros\",\n+            &APyFixedArray::zeros,\n+            nb::arg(\"shape\"),\n+            nb::arg(\"int_bits\") = nb::none(),\n+            nb::arg(\"frac_bits\") = nb::none(),\n+            nb::arg(\"bits\") = nb::none(),\n+            R\"pbdoc(\n+        Initializes an array with zeros.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array initialized with zeros.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"ones\",\n+            &APyFixedArray::ones,\n+            nb::arg(\"shape\"),\n+            nb::arg(\"int_bits\") = nb::none(),\n+            nb::arg(\"frac_bits\") = nb::none(),\n+            nb::arg(\"bits\") = nb::none(),\n+            R\"pbdoc(\n+        Initializes an array with ones.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array initialized with ones.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"eye\",\n+            &APyFixedArray::eye,\n+            nb::arg(\"n\"),\n+            nb::arg(\"m\") = nb::none(),\n+            nb::arg(\"int_bits\") = nb::none(),\n+            nb::arg(\"frac_bits\") = nb::none(),\n+            nb::arg(\"bits\") = nb::none(),\n+            R\"pbdoc(\n+        Initializes an array with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows.\n+        m : int, optional\n+            Number of columns. Default is None.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array with the specified value on the diagonal.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"identity\",\n+            &APyFixedArray::identity,\n+            nb::arg(\"n\"),\n+            nb::arg(\"int_bits\") = nb::none(),\n+            nb::arg(\"frac_bits\") = nb::none(),\n+            nb::arg(\"bits\") = nb::none(),\n+            R\"pbdoc(\n+        Initializes an identity matrix with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows (and columns) in n x n output.\n+        int_bits : int, optional\n+            Number of integer bits. Default is None.\n+        frac_bits : int, optional\n+            Number of fractional bits. Default is None.\n+        bits : int, optional\n+            Total number of bits. Default is None.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An identity matrix with ones on the diagonal.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"full\",\n+            &APyFixedArray::full,\n+            nb::arg(\"shape\"),\n+            nb::arg(\"fill_value\"),\n+            R\"pbdoc(\n+        Initializes an array with the specified value.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        fill_value : APyFixed\n+            Value to fill the array.\n+\n+        Returns\n+        -------\n+        APyFixedArray\n+            An array filled with the specified value.\n+    )pbdoc\"\n+        )\n \n         /*\n          * Dunder methods\ndiff --git a/src/apyfloat_util.h b/src/apyfloat_util.h\nindex 549d47ed7..c1f42b074 100644\n--- a/src/apyfloat_util.h\n+++ b/src/apyfloat_util.h\n@@ -5,6 +5,8 @@\n #include \"apyfixed_util.h\"\n #include \"apyfloat.h\"\n #include \"apytypes_common.h\"\n+#include <optional>\n+#include <vector>\n \n /*!\n  * Sizes of APyFloat datatypes\n@@ -53,6 +55,18 @@ exp_t calc_bias_general(\n     int new_exp_bits, int exp_bits1, exp_t bias1, int exp_bits2, exp_t bias2\n );\n \n+//! Get bit pattern for the value one\n+static APY_INLINE APyFloat\n+one(std::uint8_t exp_bits,\n+    std::uint8_t man_bits,\n+    std::optional<exp_t> bias = std::nullopt)\n+{\n+    APyFloat result(exp_bits, man_bits, bias.value_or(APyFloat::ieee_bias(exp_bits)));\n+    APyFloatData data = { 0, result.get_bias(), 0 };\n+    result.set_data(data);\n+    return result;\n+}\n+\n //! Quantize mantissa\n APY_INLINE void quantize_mantissa(\n     man_t& man,\ndiff --git a/src/apyfloatarray.cc b/src/apyfloatarray.cc\nindex 3a3b86b4b..e1eafbf54 100644\n--- a/src/apyfloatarray.cc\n+++ b/src/apyfloatarray.cc\n@@ -995,6 +995,102 @@ std::variant<APyFloatArray, APyFloat> APyFloatArray::matmul(const APyFloatArray&\n     ));\n }\n \n+/* ****************************************************************************** *\n+ *                     Static methods for array initialization                    *\n+ * ****************************************************************************** */\n+\n+APyFloatArray APyFloatArray::zeros(\n+    const nb::tuple& shape,\n+    std::uint8_t exp_bits,\n+    std::uint8_t man_bits,\n+    std::optional<exp_t> bias\n+)\n+{\n+    std::vector<std::size_t> new_shape = ::shape_from_tuple(shape);\n+    APyFloatArray result(new_shape, exp_bits, man_bits, bias);\n+    return result;\n+}\n+\n+APyFloatArray APyFloatArray::ones(\n+    const nb::tuple& shape,\n+    std::uint8_t exp_bits,\n+    std::uint8_t man_bits,\n+    std::optional<exp_t> bias\n+)\n+{\n+    APyFloat float_one = one(exp_bits, man_bits, bias);\n+    return full(shape, float_one);\n+}\n+\n+APyFloatArray APyFloatArray::eye(\n+    const nb::int_& N,\n+    std::uint8_t exp_bits,\n+    std::uint8_t man_bits,\n+    std::optional<nb::int_> M,\n+    std::optional<exp_t> bias\n+)\n+{\n+    APyFloat float_one = one(exp_bits, man_bits, bias);\n+    // Use N for both dimensions if M is not provided\n+    nb::tuple shape = M ? nb::make_tuple(N, M.value()) : nb::make_tuple(N, N);\n+    return diagonal(shape, float_one);\n+}\n+\n+APyFloatArray APyFloatArray::identity(\n+    const nb::int_& N,\n+    std::uint8_t exp_bits,\n+    std::uint8_t man_bits,\n+    std::optional<exp_t> bias\n+)\n+{\n+    return eye(N, exp_bits, man_bits, std::nullopt, bias);\n+}\n+\n+APyFloatArray APyFloatArray::full(const nb::tuple& shape, const APyFloat& fill_value)\n+{\n+    std::vector<std::size_t> new_shape = ::shape_from_tuple(shape);\n+    APyFloatArray result(\n+        new_shape,\n+        fill_value.get_exp_bits(),\n+        fill_value.get_man_bits(),\n+        fill_value.get_bias()\n+    );\n+\n+    std::size_t num_elem = ::fold_shape(new_shape);\n+    result.data = std::vector<APyFloatData>(num_elem, fill_value.get_data());\n+    return result;\n+}\n+\n+APyFloatArray\n+APyFloatArray::diagonal(const nb::tuple& shape, const APyFloat& fill_value)\n+{\n+    std::vector<std::size_t> new_shape = ::shape_from_tuple(shape);\n+    if (new_shape.size() > 2) {\n+        throw nb::value_error(\n+            \"Creating higher dimensional diagonal arrays are not yet defined\"\n+        );\n+    }\n+    APyFloatArray result(\n+        new_shape,\n+        fill_value.get_exp_bits(),\n+        fill_value.get_man_bits(),\n+        fill_value.get_bias()\n+    );\n+\n+    std::size_t min_dim = *std::min_element(new_shape.begin(), new_shape.end());\n+    std::vector<std::size_t> strides = ::strides_from_shape(new_shape);\n+    std::size_t multiplier = std::accumulate(strides.begin(), strides.end(), 0);\n+    for (std::size_t i = 0; i < min_dim; ++i) {\n+        std::size_t index = i * multiplier;\n+        result.data[index] = fill_value.get_data();\n+    }\n+    return result;\n+}\n+\n+/* ****************************************************************************** *\n+ *                            Methods for array modification                      *\n+ * ****************************************************************************** */\n+\n APyFloatArray\n APyFloatArray::squeeze(std::optional<std::variant<nb::int_, nb::tuple>> axis) const\n {\ndiff --git a/src/apyfloatarray.h b/src/apyfloatarray.h\nindex 3cad0addb..f596243c8 100644\n--- a/src/apyfloatarray.h\n+++ b/src/apyfloatarray.h\n@@ -95,6 +95,43 @@ class APyFloatArray {\n     //! Set data fields based on an and-array of doubles\n     void _set_values_from_ndarray(const nanobind::ndarray<nanobind::c_contig>& ndarray);\n \n+    /* ****************************************************************************** *\n+     *                     Static methods for array initialization                    *\n+     * ****************************************************************************** */\n+\n+    static APyFloatArray zeros(\n+        const nb::tuple& shape,\n+        std::uint8_t exp_bits,\n+        std::uint8_t man_bits,\n+        std::optional<exp_t> bias = std::nullopt\n+    );\n+\n+    static APyFloatArray ones(\n+        const nb::tuple& shape,\n+        std::uint8_t exp_bits,\n+        std::uint8_t man_bits,\n+        std::optional<exp_t> bias = std::nullopt\n+    );\n+\n+    static APyFloatArray\n+    eye(const nb::int_& N,\n+        std::uint8_t exp_bits,\n+        std::uint8_t man_bits,\n+        std::optional<nb::int_> M,\n+        std::optional<exp_t> bias = std::nullopt);\n+\n+    static APyFloatArray identity(\n+        const nb::int_& N,\n+        std::uint8_t exp_bits,\n+        std::uint8_t man_bits,\n+        std::optional<exp_t> bias = std::nullopt\n+    );\n+\n+    //! Create an `APyFloatArray` initialized with a specified fill value\n+    static APyFloatArray full(const nb::tuple& shape, const APyFloat& fill_value);\n+\n+    static APyFloatArray diagonal(const nb::tuple& shape, const APyFloat& fill_value);\n+\n     /* ****************************************************************************** *\n      * *                          Public member functions                           * *\n      * ****************************************************************************** */\ndiff --git a/src/apyfloatarray_wrapper.cc b/src/apyfloatarray_wrapper.cc\nindex cd0634d6f..77951bf9d 100644\n--- a/src/apyfloatarray_wrapper.cc\n+++ b/src/apyfloatarray_wrapper.cc\n@@ -505,7 +505,139 @@ void bind_float_array(nb::module_& m)\n             :class:`APyFloatArray`\n             )pbdoc\"\n         )\n+        .def_static(\n+            \"zeros\",\n+            &APyFloatArray::zeros,\n+            nb::arg(\"shape\"),\n+            nb::arg(\"exp_bits\"),\n+            nb::arg(\"man_bits\"),\n+            nb::arg(\"bias\") = std::nullopt,\n+            R\"pbdoc(\n+\n+            Initializes an array with zeros.\n+\n+            Parameters\n+            ----------\n+            shape : tuple\n+                Shape of the array.\n+            exp_bits : int\n+                Number of exponent bits.\n+            man_bits : int\n+                Number of mantissa bits.\n+            bias : optional\n+                Set bias. Default is None.\n+\n+            Returns\n+            -------\n+            APyFloatArray\n+                An array filled with zeros.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"ones\",\n+            &APyFloatArray::ones,\n+            nb::arg(\"shape\"),\n+            nb::arg(\"exp_bits\"),\n+            nb::arg(\"man_bits\"),\n+            nb::arg(\"bias\") = std::nullopt,\n+            R\"pbdoc(\n+        Initializes an array with ones.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array filled with ones.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"eye\",\n+            &APyFloatArray::eye,\n+            nb::arg(\"n\"),\n+            nb::arg(\"exp_bits\"),\n+            nb::arg(\"man_bits\"),\n+            nb::arg(\"m\") = nb::none(),\n+            nb::arg(\"bias\") = std::nullopt,\n+            R\"pbdoc(\n+        Initializes an array with ones on the diagonal.\n \n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows (and columns) in the n x n output.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        m : int, optional\n+            Number of columns. Default is None, which results in an n x n output.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array with the specified value on the diagonal.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"identity\",\n+            &APyFloatArray::identity,\n+            nb::arg(\"n\"),\n+            nb::arg(\"exp_bits\"),\n+            nb::arg(\"man_bits\"),\n+            nb::arg(\"bias\") = std::nullopt,\n+            R\"pbdoc(\n+        Initializes an identity matrix with ones on the diagonal.\n+\n+        Parameters\n+        ----------\n+        n : int\n+            Number of rows (and columns) in the n x n output.\n+        exp_bits : int\n+            Number of exponent bits.\n+        man_bits : int\n+            Number of mantissa bits.\n+        bias : optional\n+            Set bias. Default is None.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An identity matrix with ones on the diagonal.\n+    )pbdoc\"\n+        )\n+        .def_static(\n+            \"full\",\n+            &APyFloatArray::full,\n+            nb::arg(\"shape\"),\n+            nb::arg(\"fill_value\"),\n+            R\"pbdoc(\n+        Initializes an array filled with the specified value.\n+\n+        Parameters\n+        ----------\n+        shape : tuple\n+            Shape of the array.\n+        fill_value : APyFloat\n+            Value to fill the array.\n+\n+        Returns\n+        -------\n+        APyFloatArray\n+            An array filled with the specified value.\n+    )pbdoc\"\n+        )\n         /*\n          * Dunder methods\n          */\ndiff --git a/src/array_utils.h b/src/array_utils.h\nindex 93f3164a4..ed51c2f39 100644\n--- a/src/array_utils.h\n+++ b/src/array_utils.h\n@@ -63,6 +63,20 @@ shape_from_tuple(nb::tuple new_shape, size_t elem_count)\n     return new_shape_vec;\n }\n \n+static APY_INLINE std::vector<std::size_t> shape_from_tuple(nb::tuple new_shape)\n+{\n+    std::vector<std::size_t> new_shape_vec;\n+    for (auto it = new_shape.begin(); it != new_shape.end(); ++it) {\n+        int current_value = nb::cast<int>(*it);\n+        if (current_value < 0) {\n+            throw nb::value_error(\"Negative dimensions or are not allowed.\");\n+        } else {\n+            new_shape_vec.push_back(static_cast<std::size_t>(current_value));\n+        }\n+    }\n+    return new_shape_vec;\n+}\n+\n /**\n  * @brief Converts the provided axes into a vector of positive indices.\n  *\n", "instance_id": "apytypes__apytypes-491", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in outlining the goal of implementing various array creation methods for a library dealing with fixed-point and floating-point arrays (APyFixedArray and APyFloatArray). It specifies which methods are already implemented (e.g., `eye`, `identity`, `ones`, etc.) and provides some guidance on their expected behavior, such as `ones` and `zeros` being static methods and handling of quantization warnings for `full` and `full_like`. However, there are minor ambiguities and missing details. For instance, the problem statement does not fully clarify the expected behavior for edge cases (e.g., invalid shapes, negative dimensions, or overflow conditions). Additionally, while it mentions that `full` should warn on quantized APy-scalars, it does not specify how or where this warning should be implemented. Constraints on input parameters (e.g., valid ranges for `int_bits`, `frac_bits`, etc.) are also not explicitly defined. Despite these minor gaps, the intent and scope of the task are understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is significant, as it involves multiple files across Python and C++ layers (`apyfixedarray.cc`, `apyfloatarray.cc`, Python wrappers, and utility headers), requiring an understanding of the interaction between Python bindings (via Nanobind) and the underlying C++ implementation. The changes include adding static methods for array initialization (`zeros`, `ones`, `eye`, `identity`, `full`, etc.), which necessitates handling different data types (fixed-point and floating-point) and their specific parameters (e.g., `int_bits`, `frac_bits`, `exp_bits`, `man_bits`). This requires familiarity with several technical concepts, including type handling in Python/C++ interop, bit-level manipulations for fixed-point and floating-point representations, and array shape/strides calculations. While the logic for most methods (e.g., filling arrays with zeros or ones) is straightforward, implementing `eye` and `identity` involves slightly more complex diagonal filling logic, and `full` requires handling type casting and potential quantization issues. Edge case handling (e.g., invalid shapes or bit configurations) is partially addressed in the code but not comprehensively specified in the problem statement, adding moderate complexity. However, the problem does not appear to impact core system architecture or require advanced algorithmic design, keeping it from being classified as hard. Overall, this task demands a solid understanding of the codebase and moderate effort across multiple components, justifying a difficulty score of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Comparison with long integers\nAfter #307 the comparison is done at bit-level by converting the incoming double to an APyFloat (and in the case of APyFixed comparison, the APyFloat to an APyFixed). However, for long Python integers this will not work correctly. There should be a method that creates a \"long enough\" APyFixed and then uses that for comparison. (The creation method should not be exported...)\n", "patch": "diff --git a/lib/apytypes/_apytypes.pyi b/lib/apytypes/_apytypes.pyi\nindex 0c8283846..1a3ed9ea7 100644\n--- a/lib/apytypes/_apytypes.pyi\n+++ b/lib/apytypes/_apytypes.pyi\n@@ -125,15 +125,21 @@ class APyFixed:\n     def __eq__(self, arg0: APyFixed) -> bool: ...\n     @typing.overload\n     def __eq__(self, arg0: float) -> bool: ...\n+    @typing.overload\n+    def __eq__(self, arg0: int) -> bool: ...\n     def __float__(self) -> float: ...\n     @typing.overload\n     def __ge__(self, arg0: APyFixed) -> bool: ...\n     @typing.overload\n     def __ge__(self, arg0: float) -> bool: ...\n     @typing.overload\n+    def __ge__(self, arg0: int) -> bool: ...\n+    @typing.overload\n     def __gt__(self, arg0: APyFixed) -> bool: ...\n     @typing.overload\n     def __gt__(self, arg0: float) -> bool: ...\n+    @typing.overload\n+    def __gt__(self, arg0: int) -> bool: ...\n     def __init__(\n         self,\n         bit_pattern: int,\n@@ -145,12 +151,16 @@ class APyFixed:\n     def __le__(self, arg0: APyFixed) -> bool: ...\n     @typing.overload\n     def __le__(self, arg0: float) -> bool: ...\n+    @typing.overload\n+    def __le__(self, arg0: int) -> bool: ...\n     def __lshift__(self, shift_amnt: int) -> APyFixed: ...\n     @typing.overload\n     def __lt__(self, arg0: APyFixed) -> bool: ...\n     @typing.overload\n     def __lt__(self, arg0: float) -> bool: ...\n     @typing.overload\n+    def __lt__(self, arg0: int) -> bool: ...\n+    @typing.overload\n     def __mul__(self, arg0: APyFixed) -> APyFixed: ...\n     @typing.overload\n     def __mul__(self, arg0: int) -> APyFixed: ...\n@@ -159,6 +169,8 @@ class APyFixed:\n     @typing.overload\n     def __ne__(self, arg0: float) -> bool: ...\n     @typing.overload\n+    def __ne__(self, arg0: int) -> bool: ...\n+    @typing.overload\n     def __neg__(self) -> APyFixed: ...\n     @typing.overload\n     def __neg__(self) -> APyFixed: ...\ndiff --git a/src/apyfixed.cc b/src/apyfixed.cc\nindex 5ec3ab98a..63211f455 100644\n--- a/src/apyfixed.cc\n+++ b/src/apyfixed.cc\n@@ -47,7 +47,7 @@ APyFixed::APyFixed(const APyFixed& other)\n }\n \n APyFixed::APyFixed(\n-    nb::int_ python_long_int_bit_pattern,\n+    const nb::int_& python_long_int_bit_pattern,\n     std::optional<int> int_bits,\n     std::optional<int> frac_bits,\n     std::optional<int> bits\n@@ -83,21 +83,11 @@ APyFixed::APyFixed(int bits, int int_bits, _IT begin, _IT end)\n     : APyFixed(bits, int_bits)\n {\n     assert(std::distance(begin, end) > 0);\n-    assert(std::distance(begin, end) <= ptrdiff_t(bits_to_limbs(bits)));\n+    assert(std::distance(begin, end) == ptrdiff_t(bits_to_limbs(bits)));\n \n     // Copy data into resulting vector\n     std::copy(begin, end, _data.begin());\n \n-    // Sign-extend if necessary\n-    if (std::size_t(std::distance(begin, end)) < bits_to_limbs(bits)) {\n-        mp_limb_t sign_limb = mp_limb_signed_t(*std::prev(end)) < 0 ? -1 : 0;\n-        std::fill_n(\n-            _data.begin() + std::distance(begin, end),\n-            bits_to_limbs(bits) - std::distance(begin, end),\n-            sign_limb\n-        );\n-    }\n-\n     // Two's-complements overflow bits outside of the range\n     _overflow_twos_complement(_data.begin(), _data.end(), _bits, _int_bits);\n }\n@@ -341,6 +331,60 @@ bool APyFixed::operator>=(const APyFixed& rhs) const\n     return diff.is_negative() || diff.is_zero();\n }\n \n+bool APyFixed::operator==(const nb::int_& rhs) const\n+{\n+    const std::vector<mp_limb_t> limb_vec = python_long_to_limb_vec(rhs);\n+    APyFixed rhs_fixed(\n+        _LIMB_SIZE_BITS * limb_vec.size(), _LIMB_SIZE_BITS * limb_vec.size(), limb_vec\n+    );\n+    return *this == rhs_fixed;\n+}\n+\n+bool APyFixed::operator!=(const nb::int_& rhs) const\n+{\n+    const std::vector<mp_limb_t> limb_vec = python_long_to_limb_vec(rhs);\n+    APyFixed rhs_fixed(\n+        _LIMB_SIZE_BITS * limb_vec.size(), _LIMB_SIZE_BITS * limb_vec.size(), limb_vec\n+    );\n+    return *this != rhs_fixed;\n+}\n+\n+bool APyFixed::operator<(const nb::int_& rhs) const\n+{\n+    const std::vector<mp_limb_t> limb_vec = python_long_to_limb_vec(rhs);\n+    APyFixed rhs_fixed(\n+        _LIMB_SIZE_BITS * limb_vec.size(), _LIMB_SIZE_BITS * limb_vec.size(), limb_vec\n+    );\n+    return *this < rhs_fixed;\n+}\n+\n+bool APyFixed::operator<=(const nb::int_& rhs) const\n+{\n+    const std::vector<mp_limb_t> limb_vec = python_long_to_limb_vec(rhs);\n+    APyFixed rhs_fixed(\n+        _LIMB_SIZE_BITS * limb_vec.size(), _LIMB_SIZE_BITS * limb_vec.size(), limb_vec\n+    );\n+    return *this <= rhs_fixed;\n+}\n+\n+bool APyFixed::operator>(const nb::int_& rhs) const\n+{\n+    const std::vector<mp_limb_t> limb_vec = python_long_to_limb_vec(rhs);\n+    APyFixed rhs_fixed(\n+        _LIMB_SIZE_BITS * limb_vec.size(), _LIMB_SIZE_BITS * limb_vec.size(), limb_vec\n+    );\n+    return *this > rhs_fixed;\n+}\n+\n+bool APyFixed::operator>=(const nb::int_& rhs) const\n+{\n+    const std::vector<mp_limb_t> limb_vec = python_long_to_limb_vec(rhs);\n+    APyFixed rhs_fixed(\n+        _LIMB_SIZE_BITS * limb_vec.size(), _LIMB_SIZE_BITS * limb_vec.size(), limb_vec\n+    );\n+    return *this >= rhs_fixed;\n+}\n+\n bool APyFixed::operator==(const double rhs) const\n {\n     APyFloat rhs_fp(\n@@ -505,9 +549,15 @@ std::string APyFixed::to_string_dec() const\n     return result;\n }\n \n-std::string APyFixed::to_string_hex() const { throw NotImplementedException(); }\n+std::string APyFixed::to_string_hex() const\n+{\n+    throw NotImplementedException(\"APyFixed::to_string_hex()\");\n+}\n \n-std::string APyFixed::to_string_oct() const { throw NotImplementedException(); }\n+std::string APyFixed::to_string_oct() const\n+{\n+    throw NotImplementedException(\"APyFixed::to_string_oct()\");\n+}\n \n std::string APyFixed::to_string(int base) const\n {\n@@ -522,7 +572,10 @@ std::string APyFixed::to_string(int base) const\n         return to_string_dec();\n         break;\n     default:\n-        throw NotImplementedException();\n+        throw nb::value_error(\n+            fmt::format(\"APyFixed::to_string(base={}): base is not supported\", base)\n+                .c_str()\n+        );\n         break;\n     }\n }\ndiff --git a/src/apyfixed.h b/src/apyfixed.h\nindex 1a7681e0c..b651e8dff 100644\n--- a/src/apyfixed.h\n+++ b/src/apyfixed.h\n@@ -65,9 +65,12 @@ class APyFixed {\n     //! No default constructed `APyFixed` objects\n     APyFixed() = delete;\n \n+    //! Construct a copy from `other`.\n+    APyFixed(const APyFixed& other);\n+\n     //! Main Python-exposed `APyFixed` constructor\n     explicit APyFixed(\n-        nanobind::int_ python_long_int_bit_pattern,\n+        const nanobind::int_& python_long_int_bit_pattern,\n         std::optional<int> int_bits = std::nullopt,\n         std::optional<int> frac_bits = std::nullopt,\n         std::optional<int> bits = std::nullopt\n@@ -78,9 +81,6 @@ class APyFixed {\n      * ****************************************************************************** */\n \n public:\n-    //! Construct a copy from `other`.\n-    APyFixed(const APyFixed& other);\n-\n     //! Construct a zero-initialized number with `bits` and `int_bits`. Undefined\n     //! behaviour if `bits < 1`.\n     explicit APyFixed(int bits, int int_bits);\n@@ -139,12 +139,21 @@ class APyFixed {\n     bool operator<=(const APyFixed& rhs) const;\n     bool operator>(const APyFixed& rhs) const;\n     bool operator>=(const APyFixed& rhs) const;\n+\n+    bool operator==(const nanobind::int_& rhs) const;\n+    bool operator!=(const nanobind::int_& rhs) const;\n+    bool operator<(const nanobind::int_& rhs) const;\n+    bool operator<=(const nanobind::int_& rhs) const;\n+    bool operator>(const nanobind::int_& rhs) const;\n+    bool operator>=(const nanobind::int_& rhs) const;\n+\n     bool operator==(const double rhs) const;\n     bool operator!=(const double rhs) const;\n     bool operator<=(const double rhs) const;\n     bool operator<(const double rhs) const;\n     bool operator>=(const double rhs) const;\n     bool operator>(const double rhs) const;\n+\n     bool operator==(const float rhs) const;\n     bool operator!=(const float rhs) const;\n     bool operator<=(const float rhs) const;\ndiff --git a/src/apyfixed_wrapper.cc b/src/apyfixed_wrapper.cc\nindex e22e49e4a..01948c724 100644\n--- a/src/apyfixed_wrapper.cc\n+++ b/src/apyfixed_wrapper.cc\n@@ -36,6 +36,13 @@ void bind_fixed(nb::module_& m)\n         .def(nb::self > nb::self)\n         .def(nb::self >= nb::self)\n \n+        .def(nb::self == nb::int_())\n+        .def(nb::self != nb::int_())\n+        .def(nb::self < nb::int_())\n+        .def(nb::self <= nb::int_())\n+        .def(nb::self > nb::int_())\n+        .def(nb::self >= nb::int_())\n+\n         .def(nb::self == float())\n         .def(nb::self != float())\n         .def(nb::self < float())\ndiff --git a/src/python_util.h b/src/python_util.h\nindex f34de4af6..56f7ded66 100644\n--- a/src/python_util.h\n+++ b/src/python_util.h\n@@ -91,10 +91,11 @@\n  * Python arbitrary long integer object to GMP limb vector\n  */\n [[maybe_unused]] static APY_INLINE std::vector<mp_limb_t> python_long_to_limb_vec(\n-    nanobind::int_ py_long_int, std::optional<std::size_t> n_exact_limbs = std::nullopt\n+    const nanobind::int_& py_long_int,\n+    std::optional<std::size_t> n_exact_limbs = std::nullopt\n )\n {\n-    PyLongObject* py_long = (PyLongObject*)py_long_int.ptr();\n+    const PyLongObject* py_long = (const PyLongObject*)py_long_int.ptr();\n     long py_long_digits = PyLong_DigitCount(py_long);\n     bool py_long_is_negative = PyLong_IsNegative(py_long);\n \n", "instance_id": "apytypes__apytypes-389", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: comparisons with long Python integers are not handled correctly in the current implementation, and a method is needed to create a \"long enough\" APyFixed for accurate comparison. The goal is evident\u2014fixing comparison operations between APyFixed and Python long integers. However, critical details are missing, such as specific constraints on what \"long enough\" means (e.g., how many bits should be allocated), how to handle overflow or underflow during conversion, and whether there are specific performance requirements for this operation. Additionally, edge cases (e.g., very large integers, negative numbers) are not explicitly mentioned, which could lead to ambiguity during implementation. Despite these gaps, the intent and general approach are understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files (apyfixed.cc, apyfixed.h, apyfixed_wrapper.cc, python_util.h, and a type definition file), indicating a moderate level of interaction across the codebase. However, the changes are mostly localized to adding comparison operators for Python long integers by converting them to APyFixed objects, which reuses existing logic. Second, the technical concepts required include understanding Python's long integer representation (via PyLongObject), bit-level operations, and the internal representation of APyFixed (using GMP limbs for arbitrary-precision arithmetic). These concepts are moderately complex, especially for someone unfamiliar with low-level bit manipulation or Python's C API. Third, the problem requires handling potential edge cases, such as very large integers or negative numbers, though these are not explicitly detailed in the statement. The code changes show a straightforward approach (converting Python integers to APyFixed with sufficient bits), but ensuring correctness across all edge cases adds some complexity. Finally, the impact on the system's architecture is minimal, as this is an extension of existing functionality rather than a fundamental redesign. Overall, this task requires a solid understanding of the codebase and moderate effort to implement and test, placing it in the 0.4-0.6 range, with a slight bump to 0.55 due to the cross-file changes and bit-level operations.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Enable Strict Mode for Pyright\nAs discussed with @coretl we would like to use the scanspec repository to prototype a number of modern python correctness tooling including pyright strict mode (see [docs](https://github.com/microsoft/pyright/blob/main/docs/configuration.md)).\r\n\r\n## Acceptance Criteria\r\n- Strict mode enabled per https://diamondlightsource.github.io/python-copier-template/main/how-to/strict-mode.html\r\n- `tox -e type-checking` gives no errors\n", "patch": "diff --git a/docs/conf.py b/docs/conf.py\nindex acfda3be..4e376ee9 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -72,6 +72,7 @@\n # ('envvar', 'LD_LIBRARY_PATH').\n nitpick_ignore = [\n     (\"py:class\", \"scanspec.core.C\"),\n+    (\"py:class\", \"scanspec.core.T\"),\n     (\"py:class\", \"pydantic.config.ConfigDict\"),\n ]\n \ndiff --git a/pyproject.toml b/pyproject.toml\nindex b0bf50e6..820f6aaf 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -63,8 +63,8 @@ name = \"Tom Cobb\"\n version_file = \"src/scanspec/_version.py\"\n \n [tool.pyright]\n-typeCheckingMode = \"standard\"\n-reportMissingImports = false  # Ignore missing stubs in imported modules\n+typeCheckingMode = \"strict\"\n+reportMissingImports = false # Ignore missing stubs in imported modules\n \n [tool.pytest.ini_options]\n # Run pytest with all our checkers, and don't spam us with massive tracebacks on error\n@@ -135,9 +135,5 @@ convention = \"google\"\n [tool.ruff.lint.per-file-ignores]\n \n \"tests/**/*\" = [\n-    # By default, private member access is allowed in tests\n-    # See https://github.com/DiamondLightSource/python-copier-template/issues/154\n-    # Remove this line to forbid private member access in tests\n-    \"SLF001\",\n-    \"D\",      # Don't check docstrings in tests\n+    \"D\", # Don't check docstrings in tests\n ]\ndiff --git a/schema.json b/schema.json\nindex c58c4fef..459f28a5 100644\n--- a/schema.json\n+++ b/schema.json\n@@ -1,1862 +1,1 @@\n-{\n-    \"openapi\": \"3.1.0\",\n-    \"info\": {\n-        \"title\": \"FastAPI\",\n-        \"version\": \"0.1.1\"\n-    },\n-    \"paths\": {\n-        \"/valid\": {\n-            \"post\": {\n-                \"summary\": \"Valid\",\n-                \"description\": \"Validate wether a ScanSpec can produce a viable scan.\\n\\nArgs:\\n    spec: The scanspec to validate\\n\\nReturns:\\n    ValidResponse: A canonical version of the spec if it is valid.\\n        An error otherwise.\",\n-                \"operationId\": \"valid_valid_post\",\n-                \"requestBody\": {\n-                    \"content\": {\n-                        \"application/json\": {\n-                            \"schema\": {\n-                                \"$ref\": \"#/components/schemas/Spec-Input\",\n-                                \"examples\": [\n-                                    {\n-                                        \"outer\": {\n-                                            \"axis\": \"y\",\n-                                            \"start\": 0.0,\n-                                            \"stop\": 10.0,\n-                                            \"num\": 3,\n-                                            \"type\": \"Line\"\n-                                        },\n-                                        \"inner\": {\n-                                            \"axis\": \"x\",\n-                                            \"start\": 0.0,\n-                                            \"stop\": 10.0,\n-                                            \"num\": 4,\n-                                            \"type\": \"Line\"\n-                                        },\n-                                        \"type\": \"Product\"\n-                                    }\n-                                ]\n-                            }\n-                        }\n-                    },\n-                    \"required\": true\n-                },\n-                \"responses\": {\n-                    \"200\": {\n-                        \"description\": \"Successful Response\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/ValidResponse\"\n-                                }\n-                            }\n-                        }\n-                    },\n-                    \"422\": {\n-                        \"description\": \"Validation Error\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/HTTPValidationError\"\n-                                }\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        },\n-        \"/midpoints\": {\n-            \"post\": {\n-                \"summary\": \"Midpoints\",\n-                \"description\": \"Generate midpoints from a scanspec.\\n\\nA scanspec can produce bounded points (i.e. a point is valid if an\\naxis is between a minimum and and a maximum, see /bounds). The midpoints\\nare the middle of each set of bounds.\\n\\nArgs:\\n    request: Scanspec and formatting info.\\n\\nReturns:\\n    MidpointsResponse: Midpoints of the scan\",\n-                \"operationId\": \"midpoints_midpoints_post\",\n-                \"requestBody\": {\n-                    \"content\": {\n-                        \"application/json\": {\n-                            \"schema\": {\n-                                \"$ref\": \"#/components/schemas/PointsRequest\",\n-                                \"examples\": [\n-                                    {\n-                                        \"spec\": {\n-                                            \"outer\": {\n-                                                \"axis\": \"y\",\n-                                                \"start\": 0.0,\n-                                                \"stop\": 10.0,\n-                                                \"num\": 3,\n-                                                \"type\": \"Line\"\n-                                            },\n-                                            \"inner\": {\n-                                                \"axis\": \"x\",\n-                                                \"start\": 0.0,\n-                                                \"stop\": 10.0,\n-                                                \"num\": 4,\n-                                                \"type\": \"Line\"\n-                                            },\n-                                            \"type\": \"Product\"\n-                                        },\n-                                        \"max_frames\": 1024,\n-                                        \"format\": \"FLOAT_LIST\"\n-                                    }\n-                                ]\n-                            }\n-                        }\n-                    },\n-                    \"required\": true\n-                },\n-                \"responses\": {\n-                    \"200\": {\n-                        \"description\": \"Successful Response\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/MidpointsResponse\"\n-                                }\n-                            }\n-                        }\n-                    },\n-                    \"422\": {\n-                        \"description\": \"Validation Error\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/HTTPValidationError\"\n-                                }\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        },\n-        \"/bounds\": {\n-            \"post\": {\n-                \"summary\": \"Bounds\",\n-                \"description\": \"Generate bounds from a scanspec.\\n\\nA scanspec can produce points with lower and upper bounds.\\n\\nArgs:\\n    request: Scanspec and formatting info.\\n\\nReturns:\\n    BoundsResponse: Bounds of the scan\",\n-                \"operationId\": \"bounds_bounds_post\",\n-                \"requestBody\": {\n-                    \"content\": {\n-                        \"application/json\": {\n-                            \"schema\": {\n-                                \"$ref\": \"#/components/schemas/PointsRequest\",\n-                                \"examples\": [\n-                                    {\n-                                        \"spec\": {\n-                                            \"outer\": {\n-                                                \"axis\": \"y\",\n-                                                \"start\": 0.0,\n-                                                \"stop\": 10.0,\n-                                                \"num\": 3,\n-                                                \"type\": \"Line\"\n-                                            },\n-                                            \"inner\": {\n-                                                \"axis\": \"x\",\n-                                                \"start\": 0.0,\n-                                                \"stop\": 10.0,\n-                                                \"num\": 4,\n-                                                \"type\": \"Line\"\n-                                            },\n-                                            \"type\": \"Product\"\n-                                        },\n-                                        \"max_frames\": 1024,\n-                                        \"format\": \"FLOAT_LIST\"\n-                                    }\n-                                ]\n-                            }\n-                        }\n-                    },\n-                    \"required\": true\n-                },\n-                \"responses\": {\n-                    \"200\": {\n-                        \"description\": \"Successful Response\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/BoundsResponse\"\n-                                }\n-                            }\n-                        }\n-                    },\n-                    \"422\": {\n-                        \"description\": \"Validation Error\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/HTTPValidationError\"\n-                                }\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        },\n-        \"/gap\": {\n-            \"post\": {\n-                \"summary\": \"Gap\",\n-                \"description\": \"Generate gaps from a scanspec.\\n\\nA scanspec may indicate if there is a gap between two frames.\\nThe array returned corresponds to whether or not there is a gap\\nafter each frame.\\n\\nArgs:\\n    spec: Scanspec and formatting info.\\n\\nReturns:\\n    GapResponse: Bounds of the scan\",\n-                \"operationId\": \"gap_gap_post\",\n-                \"requestBody\": {\n-                    \"content\": {\n-                        \"application/json\": {\n-                            \"schema\": {\n-                                \"$ref\": \"#/components/schemas/Spec-Input\",\n-                                \"examples\": [\n-                                    {\n-                                        \"outer\": {\n-                                            \"axis\": \"y\",\n-                                            \"start\": 0.0,\n-                                            \"stop\": 10.0,\n-                                            \"num\": 3,\n-                                            \"type\": \"Line\"\n-                                        },\n-                                        \"inner\": {\n-                                            \"axis\": \"x\",\n-                                            \"start\": 0.0,\n-                                            \"stop\": 10.0,\n-                                            \"num\": 4,\n-                                            \"type\": \"Line\"\n-                                        },\n-                                        \"type\": \"Product\"\n-                                    }\n-                                ]\n-                            }\n-                        }\n-                    },\n-                    \"required\": true\n-                },\n-                \"responses\": {\n-                    \"200\": {\n-                        \"description\": \"Successful Response\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/GapResponse\"\n-                                }\n-                            }\n-                        }\n-                    },\n-                    \"422\": {\n-                        \"description\": \"Validation Error\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/HTTPValidationError\"\n-                                }\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        },\n-        \"/smalleststep\": {\n-            \"post\": {\n-                \"summary\": \"Smallest Step\",\n-                \"description\": \"Calculate the smallest step in a scan, both absolutely and per-axis.\\n\\nIgnore any steps of size 0.\\n\\nArgs:\\n    spec: The spec of the scan\\n\\nReturns:\\n    SmallestStepResponse: A description of the smallest steps in the spec\",\n-                \"operationId\": \"smallest_step_smalleststep_post\",\n-                \"requestBody\": {\n-                    \"content\": {\n-                        \"application/json\": {\n-                            \"schema\": {\n-                                \"$ref\": \"#/components/schemas/Spec-Input\",\n-                                \"examples\": [\n-                                    {\n-                                        \"outer\": {\n-                                            \"axis\": \"y\",\n-                                            \"start\": 0.0,\n-                                            \"stop\": 10.0,\n-                                            \"num\": 3,\n-                                            \"type\": \"Line\"\n-                                        },\n-                                        \"inner\": {\n-                                            \"axis\": \"x\",\n-                                            \"start\": 0.0,\n-                                            \"stop\": 10.0,\n-                                            \"num\": 4,\n-                                            \"type\": \"Line\"\n-                                        },\n-                                        \"type\": \"Product\"\n-                                    }\n-                                ]\n-                            }\n-                        }\n-                    },\n-                    \"required\": true\n-                },\n-                \"responses\": {\n-                    \"200\": {\n-                        \"description\": \"Successful Response\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/SmallestStepResponse\"\n-                                }\n-                            }\n-                        }\n-                    },\n-                    \"422\": {\n-                        \"description\": \"Validation Error\",\n-                        \"content\": {\n-                            \"application/json\": {\n-                                \"schema\": {\n-                                    \"$ref\": \"#/components/schemas/HTTPValidationError\"\n-                                }\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        }\n-    },\n-    \"components\": {\n-        \"schemas\": {\n-            \"BoundsResponse\": {\n-                \"properties\": {\n-                    \"total_frames\": {\n-                        \"type\": \"integer\",\n-                        \"title\": \"Total Frames\",\n-                        \"description\": \"Total number of frames in spec\"\n-                    },\n-                    \"returned_frames\": {\n-                        \"type\": \"integer\",\n-                        \"title\": \"Returned Frames\",\n-                        \"description\": \"Total of number of frames in this response, may be less than total_frames due to downsampling etc.\"\n-                    },\n-                    \"format\": {\n-                        \"$ref\": \"#/components/schemas/PointsFormat\",\n-                        \"description\": \"Format of returned point data\"\n-                    },\n-                    \"lower\": {\n-                        \"additionalProperties\": {\n-                            \"anyOf\": [\n-                                {\n-                                    \"type\": \"string\"\n-                                },\n-                                {\n-                                    \"items\": {\n-                                        \"type\": \"number\"\n-                                    },\n-                                    \"type\": \"array\"\n-                                }\n-                            ]\n-                        },\n-                        \"type\": \"object\",\n-                        \"title\": \"Lower\",\n-                        \"description\": \"Lower bounds of scan frames if different from midpoints\"\n-                    },\n-                    \"upper\": {\n-                        \"additionalProperties\": {\n-                            \"anyOf\": [\n-                                {\n-                                    \"type\": \"string\"\n-                                },\n-                                {\n-                                    \"items\": {\n-                                        \"type\": \"number\"\n-                                    },\n-                                    \"type\": \"array\"\n-                                }\n-                            ]\n-                        },\n-                        \"type\": \"object\",\n-                        \"title\": \"Upper\",\n-                        \"description\": \"Upper bounds of scan frames if different from midpoints\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"total_frames\",\n-                    \"returned_frames\",\n-                    \"format\",\n-                    \"lower\",\n-                    \"upper\"\n-                ],\n-                \"title\": \"BoundsResponse\",\n-                \"description\": \"Bounds of a generated scan.\"\n-            },\n-            \"Circle\": {\n-                \"properties\": {\n-                    \"x_axis\": {\n-                        \"title\": \"X Axis\",\n-                        \"description\": \"The name matching the x axis of the spec\"\n-                    },\n-                    \"y_axis\": {\n-                        \"title\": \"Y Axis\",\n-                        \"description\": \"The name matching the y axis of the spec\"\n-                    },\n-                    \"x_middle\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"X Middle\",\n-                        \"description\": \"The central x point of the circle\"\n-                    },\n-                    \"y_middle\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Y Middle\",\n-                        \"description\": \"The central y point of the circle\"\n-                    },\n-                    \"radius\": {\n-                        \"type\": \"number\",\n-                        \"exclusiveMinimum\": 0.0,\n-                        \"title\": \"Radius\",\n-                        \"description\": \"Radius of the circle\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Circle\"\n-                        ],\n-                        \"const\": \"Circle\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Circle\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"x_axis\",\n-                    \"y_axis\",\n-                    \"x_middle\",\n-                    \"y_middle\",\n-                    \"radius\"\n-                ],\n-                \"title\": \"Circle\",\n-                \"description\": \"Mask contains points of axis within an xy circle of given radius.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Circle\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 1, 3, 10) * ~Line(\\\"x\\\", 0, 2, 10)\\n    spec = grid & Circle(\\\"x\\\", \\\"y\\\", 1, 2, 0.9)\"\n-            },\n-            \"CombinationOf-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"CombinationOf\"\n-                        ],\n-                        \"const\": \"CombinationOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"CombinationOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"CombinationOf\",\n-                \"description\": \"Abstract baseclass for a combination of two regions, left and right.\"\n-            },\n-            \"CombinationOf-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"CombinationOf\"\n-                        ],\n-                        \"const\": \"CombinationOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"CombinationOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"CombinationOf\",\n-                \"description\": \"Abstract baseclass for a combination of two regions, left and right.\"\n-            },\n-            \"Concat-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The left-hand Spec to Concat, midpoints will appear earlier\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The right-hand Spec to Concat, midpoints will appear later\"\n-                    },\n-                    \"gap\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Gap\",\n-                        \"description\": \"If True, force a gap in the output at the join\",\n-                        \"default\": false\n-                    },\n-                    \"check_path_changes\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Check Path Changes\",\n-                        \"description\": \"If True path through scan will not be modified by squash\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Concat\"\n-                        ],\n-                        \"const\": \"Concat\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Concat\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"Concat\",\n-                \"description\": \"Concatenate two Specs together, running one after the other.\\n\\nEach Dimension of left and right must contain the same axes. Typically\\nformed using `Spec.concat`.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"x\\\", 1, 3, 3).concat(Line(\\\"x\\\", 4, 5, 5))\"\n-            },\n-            \"Concat-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The left-hand Spec to Concat, midpoints will appear earlier\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The right-hand Spec to Concat, midpoints will appear later\"\n-                    },\n-                    \"gap\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Gap\",\n-                        \"description\": \"If True, force a gap in the output at the join\",\n-                        \"default\": false\n-                    },\n-                    \"check_path_changes\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Check Path Changes\",\n-                        \"description\": \"If True path through scan will not be modified by squash\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Concat\"\n-                        ],\n-                        \"const\": \"Concat\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Concat\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"Concat\",\n-                \"description\": \"Concatenate two Specs together, running one after the other.\\n\\nEach Dimension of left and right must contain the same axes. Typically\\nformed using `Spec.concat`.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"x\\\", 1, 3, 3).concat(Line(\\\"x\\\", 4, 5, 5))\"\n-            },\n-            \"DifferenceOf-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"DifferenceOf\"\n-                        ],\n-                        \"const\": \"DifferenceOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"DifferenceOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"DifferenceOf\",\n-                \"description\": \"A point is in DifferenceOf(a, b) if in a and not in b.\\n\\nTypically created with the ``-`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) - Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False, False, False])\"\n-            },\n-            \"DifferenceOf-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"DifferenceOf\"\n-                        ],\n-                        \"const\": \"DifferenceOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"DifferenceOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"DifferenceOf\",\n-                \"description\": \"A point is in DifferenceOf(a, b) if in a and not in b.\\n\\nTypically created with the ``-`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) - Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False, False, False])\"\n-            },\n-            \"Ellipse\": {\n-                \"properties\": {\n-                    \"x_axis\": {\n-                        \"title\": \"X Axis\",\n-                        \"description\": \"The name matching the x axis of the spec\"\n-                    },\n-                    \"y_axis\": {\n-                        \"title\": \"Y Axis\",\n-                        \"description\": \"The name matching the y axis of the spec\"\n-                    },\n-                    \"x_middle\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"X Middle\",\n-                        \"description\": \"The central x point of the ellipse\"\n-                    },\n-                    \"y_middle\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Y Middle\",\n-                        \"description\": \"The central y point of the ellipse\"\n-                    },\n-                    \"x_radius\": {\n-                        \"type\": \"number\",\n-                        \"exclusiveMinimum\": 0.0,\n-                        \"title\": \"X Radius\",\n-                        \"description\": \"The radius along the x axis of the ellipse\"\n-                    },\n-                    \"y_radius\": {\n-                        \"type\": \"number\",\n-                        \"exclusiveMinimum\": 0.0,\n-                        \"title\": \"Y Radius\",\n-                        \"description\": \"The radius along the y axis of the ellipse\"\n-                    },\n-                    \"angle\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Angle\",\n-                        \"description\": \"The angle of the ellipse (degrees)\",\n-                        \"default\": 0.0\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Ellipse\"\n-                        ],\n-                        \"const\": \"Ellipse\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Ellipse\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"x_axis\",\n-                    \"y_axis\",\n-                    \"x_middle\",\n-                    \"y_middle\",\n-                    \"x_radius\",\n-                    \"y_radius\"\n-                ],\n-                \"title\": \"Ellipse\",\n-                \"description\": \"Mask contains points of axis within an xy ellipse of given radius.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Ellipse\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 3, 8, 10) * ~Line(\\\"x\\\", 1 ,8, 10)\\n    spec = grid & Ellipse(\\\"x\\\", \\\"y\\\", 5, 5, 2, 3, 75)\"\n-            },\n-            \"GapResponse\": {\n-                \"properties\": {\n-                    \"gap\": {\n-                        \"items\": {\n-                            \"type\": \"boolean\"\n-                        },\n-                        \"type\": \"array\",\n-                        \"title\": \"Gap\",\n-                        \"description\": \"Boolean array indicating if there is a gap between each frame\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"gap\"\n-                ],\n-                \"title\": \"GapResponse\",\n-                \"description\": \"Presence of gaps in a generated scan.\"\n-            },\n-            \"HTTPValidationError\": {\n-                \"properties\": {\n-                    \"detail\": {\n-                        \"items\": {\n-                            \"$ref\": \"#/components/schemas/ValidationError\"\n-                        },\n-                        \"type\": \"array\",\n-                        \"title\": \"Detail\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"title\": \"HTTPValidationError\"\n-            },\n-            \"IntersectionOf-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"IntersectionOf\"\n-                        ],\n-                        \"const\": \"IntersectionOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"IntersectionOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"IntersectionOf\",\n-                \"description\": \"A point is in IntersectionOf(a, b) if in both a and b.\\n\\nTypically created with the ``&`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) & Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False, False,  True, False, False])\"\n-            },\n-            \"IntersectionOf-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"IntersectionOf\"\n-                        ],\n-                        \"const\": \"IntersectionOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"IntersectionOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"IntersectionOf\",\n-                \"description\": \"A point is in IntersectionOf(a, b) if in both a and b.\\n\\nTypically created with the ``&`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) & Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False, False,  True, False, False])\"\n-            },\n-            \"Line\": {\n-                \"properties\": {\n-                    \"axis\": {\n-                        \"title\": \"Axis\",\n-                        \"description\": \"An identifier for what to move\"\n-                    },\n-                    \"start\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Start\",\n-                        \"description\": \"Midpoint of the first point of the line\"\n-                    },\n-                    \"stop\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Stop\",\n-                        \"description\": \"Midpoint of the last point of the line\"\n-                    },\n-                    \"num\": {\n-                        \"type\": \"integer\",\n-                        \"minimum\": 1.0,\n-                        \"title\": \"Num\",\n-                        \"description\": \"Number of frames to produce\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Line\"\n-                        ],\n-                        \"const\": \"Line\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Line\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"axis\",\n-                    \"start\",\n-                    \"stop\",\n-                    \"num\"\n-                ],\n-                \"title\": \"Line\",\n-                \"description\": \"Linearly spaced frames with start and stop as first and last midpoints.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"x\\\", 1, 2, 5)\"\n-            },\n-            \"Mask-Input\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The Spec containing the source midpoints\"\n-                    },\n-                    \"region\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The Region that midpoints will be inside\"\n-                    },\n-                    \"check_path_changes\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Check Path Changes\",\n-                        \"description\": \"If True path through scan will not be modified by squash\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Mask\"\n-                        ],\n-                        \"const\": \"Mask\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Mask\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\",\n-                    \"region\"\n-                ],\n-                \"title\": \"Mask\",\n-                \"description\": \"Restrict Spec to only midpoints that fall inside the given Region.\\n\\nTypically created with the ``&`` operator. It also pushes down the\\n``& | ^ -`` operators to its `Region` to avoid the need for brackets on\\ncombinations of Regions.\\n\\nIf a Region spans multiple Frames objects, they will be squashed together.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Circle\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * Line(\\\"x\\\", 3, 5, 5) & Circle(\\\"x\\\", \\\"y\\\", 4, 2, 1.2)\\n\\nSee Also: `why-squash-can-change-path`\"\n-            },\n-            \"Mask-Output\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The Spec containing the source midpoints\"\n-                    },\n-                    \"region\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The Region that midpoints will be inside\"\n-                    },\n-                    \"check_path_changes\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Check Path Changes\",\n-                        \"description\": \"If True path through scan will not be modified by squash\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Mask\"\n-                        ],\n-                        \"const\": \"Mask\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Mask\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\",\n-                    \"region\"\n-                ],\n-                \"title\": \"Mask\",\n-                \"description\": \"Restrict Spec to only midpoints that fall inside the given Region.\\n\\nTypically created with the ``&`` operator. It also pushes down the\\n``& | ^ -`` operators to its `Region` to avoid the need for brackets on\\ncombinations of Regions.\\n\\nIf a Region spans multiple Frames objects, they will be squashed together.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Circle\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * Line(\\\"x\\\", 3, 5, 5) & Circle(\\\"x\\\", \\\"y\\\", 4, 2, 1.2)\\n\\nSee Also: `why-squash-can-change-path`\"\n-            },\n-            \"MidpointsResponse\": {\n-                \"properties\": {\n-                    \"total_frames\": {\n-                        \"type\": \"integer\",\n-                        \"title\": \"Total Frames\",\n-                        \"description\": \"Total number of frames in spec\"\n-                    },\n-                    \"returned_frames\": {\n-                        \"type\": \"integer\",\n-                        \"title\": \"Returned Frames\",\n-                        \"description\": \"Total of number of frames in this response, may be less than total_frames due to downsampling etc.\"\n-                    },\n-                    \"format\": {\n-                        \"$ref\": \"#/components/schemas/PointsFormat\",\n-                        \"description\": \"Format of returned point data\"\n-                    },\n-                    \"midpoints\": {\n-                        \"additionalProperties\": {\n-                            \"anyOf\": [\n-                                {\n-                                    \"type\": \"string\"\n-                                },\n-                                {\n-                                    \"items\": {\n-                                        \"type\": \"number\"\n-                                    },\n-                                    \"type\": \"array\"\n-                                }\n-                            ]\n-                        },\n-                        \"type\": \"object\",\n-                        \"title\": \"Midpoints\",\n-                        \"description\": \"The midpoints of scan frames for each axis\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"total_frames\",\n-                    \"returned_frames\",\n-                    \"format\",\n-                    \"midpoints\"\n-                ],\n-                \"title\": \"MidpointsResponse\",\n-                \"description\": \"Midpoints of a generated scan.\"\n-            },\n-            \"PointsFormat\": {\n-                \"type\": \"string\",\n-                \"enum\": [\n-                    \"STRING\",\n-                    \"FLOAT_LIST\",\n-                    \"BASE64_ENCODED\"\n-                ],\n-                \"title\": \"PointsFormat\",\n-                \"description\": \"Formats in which we can return points.\"\n-            },\n-            \"PointsRequest\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The spec from which to generate points\"\n-                    },\n-                    \"max_frames\": {\n-                        \"anyOf\": [\n-                            {\n-                                \"type\": \"integer\"\n-                            },\n-                            {\n-                                \"type\": \"null\"\n-                            }\n-                        ],\n-                        \"title\": \"Max Frames\",\n-                        \"description\": \"The maximum number of points to return, if None will return as many as calculated\",\n-                        \"default\": 100000\n-                    },\n-                    \"format\": {\n-                        \"$ref\": \"#/components/schemas/PointsFormat\",\n-                        \"description\": \"The format in which to output the points data\",\n-                        \"default\": \"FLOAT_LIST\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\"\n-                ],\n-                \"title\": \"PointsRequest\",\n-                \"description\": \"A request for generated scan points.\"\n-            },\n-            \"Polygon\": {\n-                \"properties\": {\n-                    \"x_axis\": {\n-                        \"title\": \"X Axis\",\n-                        \"description\": \"The name matching the x axis of the spec\"\n-                    },\n-                    \"y_axis\": {\n-                        \"title\": \"Y Axis\",\n-                        \"description\": \"The name matching the y axis of the spec\"\n-                    },\n-                    \"x_verts\": {\n-                        \"items\": {\n-                            \"type\": \"number\"\n-                        },\n-                        \"type\": \"array\",\n-                        \"minItems\": 3,\n-                        \"title\": \"X Verts\",\n-                        \"description\": \"The Nx1 x coordinates of the polygons vertices\"\n-                    },\n-                    \"y_verts\": {\n-                        \"items\": {\n-                            \"type\": \"number\"\n-                        },\n-                        \"type\": \"array\",\n-                        \"minItems\": 3,\n-                        \"title\": \"Y Verts\",\n-                        \"description\": \"The Nx1 y coordinates of the polygons vertices\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Polygon\"\n-                        ],\n-                        \"const\": \"Polygon\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Polygon\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"x_axis\",\n-                    \"y_axis\",\n-                    \"x_verts\",\n-                    \"y_verts\"\n-                ],\n-                \"title\": \"Polygon\",\n-                \"description\": \"Mask contains points of axis within a rotated xy polygon.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Polygon\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 3, 8, 10) * ~Line(\\\"x\\\", 1 ,8, 10)\\n    spec = grid & Polygon(\\\"x\\\", \\\"y\\\", [1.0, 6.0, 8.0, 2.0], [4.0, 10.0, 6.0, 1.0])\"\n-            },\n-            \"Product-Input\": {\n-                \"properties\": {\n-                    \"outer\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"Will be executed once\"\n-                    },\n-                    \"inner\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"Will be executed len(outer) times\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Product\"\n-                        ],\n-                        \"const\": \"Product\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Product\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"outer\",\n-                    \"inner\"\n-                ],\n-                \"title\": \"Product\",\n-                \"description\": \"Outer product of two Specs, nesting inner within outer.\\n\\nThis means that inner will run in its entirety at each point in outer.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 3, 4, 12)\"\n-            },\n-            \"Product-Output\": {\n-                \"properties\": {\n-                    \"outer\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"Will be executed once\"\n-                    },\n-                    \"inner\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"Will be executed len(outer) times\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Product\"\n-                        ],\n-                        \"const\": \"Product\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Product\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"outer\",\n-                    \"inner\"\n-                ],\n-                \"title\": \"Product\",\n-                \"description\": \"Outer product of two Specs, nesting inner within outer.\\n\\nThis means that inner will run in its entirety at each point in outer.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 3, 4, 12)\"\n-            },\n-            \"Range\": {\n-                \"properties\": {\n-                    \"axis\": {\n-                        \"title\": \"Axis\",\n-                        \"description\": \"The name matching the axis to mask in spec\"\n-                    },\n-                    \"min\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Min\",\n-                        \"description\": \"The minimum inclusive value in the region\"\n-                    },\n-                    \"max\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Max\",\n-                        \"description\": \"The minimum inclusive value in the region\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Range\"\n-                        ],\n-                        \"const\": \"Range\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Range\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"axis\",\n-                    \"min\",\n-                    \"max\"\n-                ],\n-                \"title\": \"Range\",\n-                \"description\": \"Mask contains points of axis >= min and <= max.\\n\\n>>> r = Range(\\\"x\\\", 1, 2)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True,  True, False, False])\"\n-            },\n-            \"Rectangle\": {\n-                \"properties\": {\n-                    \"x_axis\": {\n-                        \"title\": \"X Axis\",\n-                        \"description\": \"The name matching the x axis of the spec\"\n-                    },\n-                    \"y_axis\": {\n-                        \"title\": \"Y Axis\",\n-                        \"description\": \"The name matching the y axis of the spec\"\n-                    },\n-                    \"x_min\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"X Min\",\n-                        \"description\": \"Minimum inclusive x value in the region\"\n-                    },\n-                    \"y_min\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Y Min\",\n-                        \"description\": \"Minimum inclusive y value in the region\"\n-                    },\n-                    \"x_max\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"X Max\",\n-                        \"description\": \"Maximum inclusive x value in the region\"\n-                    },\n-                    \"y_max\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Y Max\",\n-                        \"description\": \"Maximum inclusive y value in the region\"\n-                    },\n-                    \"angle\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Angle\",\n-                        \"description\": \"Clockwise rotation angle of the rectangle\",\n-                        \"default\": 0.0\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Rectangle\"\n-                        ],\n-                        \"const\": \"Rectangle\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Rectangle\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"x_axis\",\n-                    \"y_axis\",\n-                    \"x_min\",\n-                    \"y_min\",\n-                    \"x_max\",\n-                    \"y_max\"\n-                ],\n-                \"title\": \"Rectangle\",\n-                \"description\": \"Mask contains points of axis within a rotated xy rectangle.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Rectangle\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 1, 3, 10) * ~Line(\\\"x\\\", 0, 2, 10)\\n    spec = grid & Rectangle(\\\"x\\\", \\\"y\\\", 0, 1.1, 1.5, 2.1, 30)\"\n-            },\n-            \"Region-Input\": {\n-                \"oneOf\": [\n-                    {\n-                        \"$ref\": \"#/components/schemas/CombinationOf-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/UnionOf-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/IntersectionOf-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/DifferenceOf-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/SymmetricDifferenceOf-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Range\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Rectangle\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Polygon\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Circle\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Ellipse\"\n-                    }\n-                ],\n-                \"discriminator\": {\n-                    \"propertyName\": \"type\",\n-                    \"mapping\": {\n-                        \"Circle\": \"#/components/schemas/Circle\",\n-                        \"CombinationOf\": \"#/components/schemas/CombinationOf-Input\",\n-                        \"DifferenceOf\": \"#/components/schemas/DifferenceOf-Input\",\n-                        \"Ellipse\": \"#/components/schemas/Ellipse\",\n-                        \"IntersectionOf\": \"#/components/schemas/IntersectionOf-Input\",\n-                        \"Polygon\": \"#/components/schemas/Polygon\",\n-                        \"Range\": \"#/components/schemas/Range\",\n-                        \"Rectangle\": \"#/components/schemas/Rectangle\",\n-                        \"SymmetricDifferenceOf\": \"#/components/schemas/SymmetricDifferenceOf-Input\",\n-                        \"UnionOf\": \"#/components/schemas/UnionOf-Input\"\n-                    }\n-                }\n-            },\n-            \"Region-Output\": {\n-                \"oneOf\": [\n-                    {\n-                        \"$ref\": \"#/components/schemas/CombinationOf-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/UnionOf-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/IntersectionOf-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/DifferenceOf-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/SymmetricDifferenceOf-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Range\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Rectangle\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Polygon\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Circle\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Ellipse\"\n-                    }\n-                ],\n-                \"discriminator\": {\n-                    \"propertyName\": \"type\",\n-                    \"mapping\": {\n-                        \"Circle\": \"#/components/schemas/Circle\",\n-                        \"CombinationOf\": \"#/components/schemas/CombinationOf-Output\",\n-                        \"DifferenceOf\": \"#/components/schemas/DifferenceOf-Output\",\n-                        \"Ellipse\": \"#/components/schemas/Ellipse\",\n-                        \"IntersectionOf\": \"#/components/schemas/IntersectionOf-Output\",\n-                        \"Polygon\": \"#/components/schemas/Polygon\",\n-                        \"Range\": \"#/components/schemas/Range\",\n-                        \"Rectangle\": \"#/components/schemas/Rectangle\",\n-                        \"SymmetricDifferenceOf\": \"#/components/schemas/SymmetricDifferenceOf-Output\",\n-                        \"UnionOf\": \"#/components/schemas/UnionOf-Output\"\n-                    }\n-                }\n-            },\n-            \"Repeat\": {\n-                \"properties\": {\n-                    \"num\": {\n-                        \"type\": \"integer\",\n-                        \"minimum\": 1.0,\n-                        \"title\": \"Num\",\n-                        \"description\": \"Number of frames to produce\"\n-                    },\n-                    \"gap\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Gap\",\n-                        \"description\": \"If False and the slowest of the stack of Frames is snaked then the end and start of consecutive iterations of Spec will have no gap\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Repeat\"\n-                        ],\n-                        \"const\": \"Repeat\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Repeat\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"num\"\n-                ],\n-                \"title\": \"Repeat\",\n-                \"description\": \"Repeat an empty frame num times.\\n\\nCan be used on the outside of a scan to repeat the same scan many times.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = 2 * ~Line.bounded(\\\"x\\\", 3, 4, 1)\\n\\nIf you want snaked axes to have no gap between iterations you can do:\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Repeat\\n\\n    spec = Repeat(2, gap=False) * ~Line.bounded(\\\"x\\\", 3, 4, 1)\\n\\n.. note:: There is no turnaround arrow at x=4\"\n-            },\n-            \"SmallestStepResponse\": {\n-                \"properties\": {\n-                    \"absolute\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Absolute\",\n-                        \"description\": \"Absolute smallest distance between two points on a single axis\"\n-                    },\n-                    \"per_axis\": {\n-                        \"additionalProperties\": {\n-                            \"type\": \"number\"\n-                        },\n-                        \"type\": \"object\",\n-                        \"title\": \"Per Axis\",\n-                        \"description\": \"Smallest distance between two points on each axis\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"absolute\",\n-                    \"per_axis\"\n-                ],\n-                \"title\": \"SmallestStepResponse\",\n-                \"description\": \"Information about the smallest steps between points in a spec.\"\n-            },\n-            \"Snake-Input\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The Spec to run in reverse every other iteration\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Snake\"\n-                        ],\n-                        \"const\": \"Snake\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Snake\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\"\n-                ],\n-                \"title\": \"Snake\",\n-                \"description\": \"Run the Spec in reverse on every other iteration when nested.\\n\\nTypically created with the ``~`` operator.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * ~Line(\\\"x\\\", 3, 5, 5)\"\n-            },\n-            \"Snake-Output\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The Spec to run in reverse every other iteration\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Snake\"\n-                        ],\n-                        \"const\": \"Snake\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Snake\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\"\n-                ],\n-                \"title\": \"Snake\",\n-                \"description\": \"Run the Spec in reverse on every other iteration when nested.\\n\\nTypically created with the ``~`` operator.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * ~Line(\\\"x\\\", 3, 5, 5)\"\n-            },\n-            \"Spec-Input\": {\n-                \"oneOf\": [\n-                    {\n-                        \"$ref\": \"#/components/schemas/Product-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Repeat\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Zip-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Mask-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Snake-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Concat-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Squash-Input\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Line\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Static\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Spiral\"\n-                    }\n-                ],\n-                \"discriminator\": {\n-                    \"propertyName\": \"type\",\n-                    \"mapping\": {\n-                        \"Concat\": \"#/components/schemas/Concat-Input\",\n-                        \"Line\": \"#/components/schemas/Line\",\n-                        \"Mask\": \"#/components/schemas/Mask-Input\",\n-                        \"Product\": \"#/components/schemas/Product-Input\",\n-                        \"Repeat\": \"#/components/schemas/Repeat\",\n-                        \"Snake\": \"#/components/schemas/Snake-Input\",\n-                        \"Spiral\": \"#/components/schemas/Spiral\",\n-                        \"Squash\": \"#/components/schemas/Squash-Input\",\n-                        \"Static\": \"#/components/schemas/Static\",\n-                        \"Zip\": \"#/components/schemas/Zip-Input\"\n-                    }\n-                }\n-            },\n-            \"Spec-Output\": {\n-                \"oneOf\": [\n-                    {\n-                        \"$ref\": \"#/components/schemas/Product-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Repeat\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Zip-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Mask-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Snake-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Concat-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Squash-Output\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Line\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Static\"\n-                    },\n-                    {\n-                        \"$ref\": \"#/components/schemas/Spiral\"\n-                    }\n-                ],\n-                \"discriminator\": {\n-                    \"propertyName\": \"type\",\n-                    \"mapping\": {\n-                        \"Concat\": \"#/components/schemas/Concat-Output\",\n-                        \"Line\": \"#/components/schemas/Line\",\n-                        \"Mask\": \"#/components/schemas/Mask-Output\",\n-                        \"Product\": \"#/components/schemas/Product-Output\",\n-                        \"Repeat\": \"#/components/schemas/Repeat\",\n-                        \"Snake\": \"#/components/schemas/Snake-Output\",\n-                        \"Spiral\": \"#/components/schemas/Spiral\",\n-                        \"Squash\": \"#/components/schemas/Squash-Output\",\n-                        \"Static\": \"#/components/schemas/Static\",\n-                        \"Zip\": \"#/components/schemas/Zip-Output\"\n-                    }\n-                }\n-            },\n-            \"Spiral\": {\n-                \"properties\": {\n-                    \"x_axis\": {\n-                        \"title\": \"X Axis\",\n-                        \"description\": \"An identifier for what to move for x\"\n-                    },\n-                    \"y_axis\": {\n-                        \"title\": \"Y Axis\",\n-                        \"description\": \"An identifier for what to move for y\"\n-                    },\n-                    \"x_start\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"X Start\",\n-                        \"description\": \"x centre of the spiral\"\n-                    },\n-                    \"y_start\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Y Start\",\n-                        \"description\": \"y centre of the spiral\"\n-                    },\n-                    \"x_range\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"X Range\",\n-                        \"description\": \"x width of the spiral\"\n-                    },\n-                    \"y_range\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Y Range\",\n-                        \"description\": \"y width of the spiral\"\n-                    },\n-                    \"num\": {\n-                        \"type\": \"integer\",\n-                        \"minimum\": 1.0,\n-                        \"title\": \"Num\",\n-                        \"description\": \"Number of frames to produce\"\n-                    },\n-                    \"rotate\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Rotate\",\n-                        \"description\": \"How much to rotate the angle of the spiral\",\n-                        \"default\": 0.0\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Spiral\"\n-                        ],\n-                        \"const\": \"Spiral\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Spiral\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"x_axis\",\n-                    \"y_axis\",\n-                    \"x_start\",\n-                    \"y_start\",\n-                    \"x_range\",\n-                    \"y_range\",\n-                    \"num\"\n-                ],\n-                \"title\": \"Spiral\",\n-                \"description\": \"Archimedean spiral of \\\"x_axis\\\" and \\\"y_axis\\\".\\n\\nStarts at centre point (\\\"x_start\\\", \\\"y_start\\\") with angle \\\"rotate\\\". Produces\\n\\\"num\\\" points in a spiral spanning width of \\\"x_range\\\" and height of \\\"y_range\\\"\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Spiral\\n\\n    spec = Spiral(\\\"x\\\", \\\"y\\\", 1, 5, 10, 50, 30)\"\n-            },\n-            \"Squash-Input\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The Spec to squash the dimensions of\"\n-                    },\n-                    \"check_path_changes\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Check Path Changes\",\n-                        \"description\": \"If True path through scan will not be modified by squash\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Squash\"\n-                        ],\n-                        \"const\": \"Squash\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Squash\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\"\n-                ],\n-                \"title\": \"Squash\",\n-                \"description\": \"Squash a stack of Frames together into a single expanded Frames object.\\n\\nSee Also:\\n    `why-squash-can-change-path`\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Squash\\n\\n    spec = Squash(Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 0, 1, 4))\"\n-            },\n-            \"Squash-Output\": {\n-                \"properties\": {\n-                    \"spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The Spec to squash the dimensions of\"\n-                    },\n-                    \"check_path_changes\": {\n-                        \"type\": \"boolean\",\n-                        \"title\": \"Check Path Changes\",\n-                        \"description\": \"If True path through scan will not be modified by squash\",\n-                        \"default\": true\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Squash\"\n-                        ],\n-                        \"const\": \"Squash\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Squash\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"spec\"\n-                ],\n-                \"title\": \"Squash\",\n-                \"description\": \"Squash a stack of Frames together into a single expanded Frames object.\\n\\nSee Also:\\n    `why-squash-can-change-path`\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Squash\\n\\n    spec = Squash(Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 0, 1, 4))\"\n-            },\n-            \"Static\": {\n-                \"properties\": {\n-                    \"axis\": {\n-                        \"title\": \"Axis\",\n-                        \"description\": \"An identifier for what to move\"\n-                    },\n-                    \"value\": {\n-                        \"type\": \"number\",\n-                        \"title\": \"Value\",\n-                        \"description\": \"The value at each point\"\n-                    },\n-                    \"num\": {\n-                        \"type\": \"integer\",\n-                        \"minimum\": 1.0,\n-                        \"title\": \"Num\",\n-                        \"description\": \"Number of frames to produce\",\n-                        \"default\": 1\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Static\"\n-                        ],\n-                        \"const\": \"Static\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Static\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"axis\",\n-                    \"value\"\n-                ],\n-                \"title\": \"Static\",\n-                \"description\": \"A static frame, repeated num times, with axis at value.\\n\\nCan be used to set axis=value at every point in a scan.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Static\\n\\n    spec = Line(\\\"y\\\", 1, 2, 3).zip(Static(\\\"x\\\", 3))\"\n-            },\n-            \"SymmetricDifferenceOf-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"SymmetricDifferenceOf\"\n-                        ],\n-                        \"const\": \"SymmetricDifferenceOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"SymmetricDifferenceOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"SymmetricDifferenceOf\",\n-                \"description\": \"A point is in SymmetricDifferenceOf(a, b) if in either a or b, but not both.\\n\\nTypically created with the ``^`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) ^ Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False,  True, False])\"\n-            },\n-            \"SymmetricDifferenceOf-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"SymmetricDifferenceOf\"\n-                        ],\n-                        \"const\": \"SymmetricDifferenceOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"SymmetricDifferenceOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"SymmetricDifferenceOf\",\n-                \"description\": \"A point is in SymmetricDifferenceOf(a, b) if in either a or b, but not both.\\n\\nTypically created with the ``^`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) ^ Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False,  True, False])\"\n-            },\n-            \"UnionOf-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Input\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"UnionOf\"\n-                        ],\n-                        \"const\": \"UnionOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"UnionOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"UnionOf\",\n-                \"description\": \"A point is in UnionOf(a, b) if in either a or b.\\n\\nTypically created with the ``|`` operator\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) | Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True,  True,  True, False])\"\n-            },\n-            \"UnionOf-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The left-hand Region to combine\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Region-Output\",\n-                        \"description\": \"The right-hand Region to combine\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"UnionOf\"\n-                        ],\n-                        \"const\": \"UnionOf\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"UnionOf\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"UnionOf\",\n-                \"description\": \"A point is in UnionOf(a, b) if in either a or b.\\n\\nTypically created with the ``|`` operator\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) | Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True,  True,  True, False])\"\n-            },\n-            \"ValidResponse\": {\n-                \"properties\": {\n-                    \"input_spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The input scanspec\"\n-                    },\n-                    \"valid_spec\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The validated version of the spec\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"input_spec\",\n-                    \"valid_spec\"\n-                ],\n-                \"title\": \"ValidResponse\",\n-                \"description\": \"Response model for spec validation.\"\n-            },\n-            \"ValidationError\": {\n-                \"properties\": {\n-                    \"loc\": {\n-                        \"items\": {\n-                            \"anyOf\": [\n-                                {\n-                                    \"type\": \"string\"\n-                                },\n-                                {\n-                                    \"type\": \"integer\"\n-                                }\n-                            ]\n-                        },\n-                        \"type\": \"array\",\n-                        \"title\": \"Location\"\n-                    },\n-                    \"msg\": {\n-                        \"type\": \"string\",\n-                        \"title\": \"Message\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"title\": \"Error Type\"\n-                    }\n-                },\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"loc\",\n-                    \"msg\",\n-                    \"type\"\n-                ],\n-                \"title\": \"ValidationError\"\n-            },\n-            \"Zip-Input\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The left-hand Spec to Zip, will appear earlier in axes\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Input\",\n-                        \"description\": \"The right-hand Spec to Zip, will appear later in axes\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Zip\"\n-                        ],\n-                        \"const\": \"Zip\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Zip\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"Zip\",\n-                \"description\": \"Run two Specs in parallel, merging their midpoints together.\\n\\nTypically formed using `Spec.zip`.\\n\\nStacks of Frames are merged by:\\n\\n- If right creates a stack of a single Frames object of size 1, expand it to\\n  the size of the fastest Frames object created by left\\n- Merge individual Frames objects together from fastest to slowest\\n\\nThis means that Zipping a Spec producing stack [l2, l1] with a Spec\\nproducing stack [r1] will assert len(l1)==len(r1), and produce\\nstack [l2, l1.zip(r1)].\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"z\\\", 1, 2, 3) * Line(\\\"y\\\", 3, 4, 5).zip(Line(\\\"x\\\", 4, 5, 5))\"\n-            },\n-            \"Zip-Output\": {\n-                \"properties\": {\n-                    \"left\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The left-hand Spec to Zip, will appear earlier in axes\"\n-                    },\n-                    \"right\": {\n-                        \"$ref\": \"#/components/schemas/Spec-Output\",\n-                        \"description\": \"The right-hand Spec to Zip, will appear later in axes\"\n-                    },\n-                    \"type\": {\n-                        \"type\": \"string\",\n-                        \"enum\": [\n-                            \"Zip\"\n-                        ],\n-                        \"const\": \"Zip\",\n-                        \"title\": \"Type\",\n-                        \"default\": \"Zip\"\n-                    }\n-                },\n-                \"additionalProperties\": false,\n-                \"type\": \"object\",\n-                \"required\": [\n-                    \"left\",\n-                    \"right\"\n-                ],\n-                \"title\": \"Zip\",\n-                \"description\": \"Run two Specs in parallel, merging their midpoints together.\\n\\nTypically formed using `Spec.zip`.\\n\\nStacks of Frames are merged by:\\n\\n- If right creates a stack of a single Frames object of size 1, expand it to\\n  the size of the fastest Frames object created by left\\n- Merge individual Frames objects together from fastest to slowest\\n\\nThis means that Zipping a Spec producing stack [l2, l1] with a Spec\\nproducing stack [r1] will assert len(l1)==len(r1), and produce\\nstack [l2, l1.zip(r1)].\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"z\\\", 1, 2, 3) * Line(\\\"y\\\", 3, 4, 5).zip(Line(\\\"x\\\", 4, 5, 5))\"\n-            }\n-        }\n-    }\n-}\n+{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"FastAPI\", \"version\": \"0.1.1\"}, \"paths\": {\"/valid\": {\"post\": {\"summary\": \"Valid\", \"description\": \"Validate wether a ScanSpec[str] can produce a viable scan.\\n\\nArgs:\\n    spec: The scanspec to validate\\n\\nReturns:\\n    ValidResponse: A canonical version of the spec if it is valid.\\n        An error otherwise.\", \"operationId\": \"valid_valid_post\", \"requestBody\": {\"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"examples\": [{\"outer\": {\"axis\": \"y\", \"start\": 0.0, \"stop\": 10.0, \"num\": 3, \"type\": \"Line\"}, \"inner\": {\"axis\": \"x\", \"start\": 0.0, \"stop\": 10.0, \"num\": 4, \"type\": \"Line\"}, \"type\": \"Product\"}]}}}, \"required\": true}, \"responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/ValidResponse\"}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}}, \"/midpoints\": {\"post\": {\"summary\": \"Midpoints\", \"description\": \"Generate midpoints from a scanspec.\\n\\nA scanspec can produce bounded points (i.e. a point is valid if an\\naxis is between a minimum and and a maximum, see /bounds). The midpoints\\nare the middle of each set of bounds.\\n\\nArgs:\\n    request: Scanspec and formatting info.\\n\\nReturns:\\n    MidpointsResponse: Midpoints of the scan\", \"operationId\": \"midpoints_midpoints_post\", \"requestBody\": {\"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/PointsRequest\", \"examples\": [{\"spec\": {\"outer\": {\"axis\": \"y\", \"start\": 0.0, \"stop\": 10.0, \"num\": 3, \"type\": \"Line\"}, \"inner\": {\"axis\": \"x\", \"start\": 0.0, \"stop\": 10.0, \"num\": 4, \"type\": \"Line\"}, \"type\": \"Product\"}, \"max_frames\": 1024, \"format\": \"FLOAT_LIST\"}]}}}, \"required\": true}, \"responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/MidpointsResponse\"}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}}, \"/bounds\": {\"post\": {\"summary\": \"Bounds\", \"description\": \"Generate bounds from a scanspec.\\n\\nA scanspec can produce points with lower and upper bounds.\\n\\nArgs:\\n    request: Scanspec and formatting info.\\n\\nReturns:\\n    BoundsResponse: Bounds of the scan\", \"operationId\": \"bounds_bounds_post\", \"requestBody\": {\"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/PointsRequest\", \"examples\": [{\"spec\": {\"outer\": {\"axis\": \"y\", \"start\": 0.0, \"stop\": 10.0, \"num\": 3, \"type\": \"Line\"}, \"inner\": {\"axis\": \"x\", \"start\": 0.0, \"stop\": 10.0, \"num\": 4, \"type\": \"Line\"}, \"type\": \"Product\"}, \"max_frames\": 1024, \"format\": \"FLOAT_LIST\"}]}}}, \"required\": true}, \"responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/BoundsResponse\"}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}}, \"/gap\": {\"post\": {\"summary\": \"Gap\", \"description\": \"Generate gaps from a scanspec.\\n\\nA scanspec may indicate if there is a gap between two frames.\\nThe array returned corresponds to whether or not there is a gap\\nafter each frame.\\n\\nArgs:\\n    spec: Scanspec and formatting info.\\n\\nReturns:\\n    GapResponse: Bounds of the scan\", \"operationId\": \"gap_gap_post\", \"requestBody\": {\"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"examples\": [{\"outer\": {\"axis\": \"y\", \"start\": 0.0, \"stop\": 10.0, \"num\": 3, \"type\": \"Line\"}, \"inner\": {\"axis\": \"x\", \"start\": 0.0, \"stop\": 10.0, \"num\": 4, \"type\": \"Line\"}, \"type\": \"Product\"}]}}}, \"required\": true}, \"responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/GapResponse\"}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}}, \"/smalleststep\": {\"post\": {\"summary\": \"Smallest Step\", \"description\": \"Calculate the smallest step in a scan, both absolutely and per-axis.\\n\\nIgnore any steps of size 0.\\n\\nArgs:\\n    spec: The spec of the scan\\n\\nReturns:\\n    SmallestStepResponse: A description of the smallest steps in the spec\", \"operationId\": \"smallest_step_smalleststep_post\", \"requestBody\": {\"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"examples\": [{\"outer\": {\"axis\": \"y\", \"start\": 0.0, \"stop\": 10.0, \"num\": 3, \"type\": \"Line\"}, \"inner\": {\"axis\": \"x\", \"start\": 0.0, \"stop\": 10.0, \"num\": 4, \"type\": \"Line\"}, \"type\": \"Product\"}]}}}, \"required\": true}, \"responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/SmallestStepResponse\"}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}}}, \"components\": {\"schemas\": {\"BoundsResponse\": {\"properties\": {\"total_frames\": {\"type\": \"integer\", \"title\": \"Total Frames\", \"description\": \"Total number of frames in spec\"}, \"returned_frames\": {\"type\": \"integer\", \"title\": \"Returned Frames\", \"description\": \"Total of number of frames in this response, may be less than total_frames due to downsampling etc.\"}, \"format\": {\"$ref\": \"#/components/schemas/PointsFormat\", \"description\": \"Format of returned point data\"}, \"lower\": {\"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"items\": {\"type\": \"number\"}, \"type\": \"array\"}]}, \"type\": \"object\", \"title\": \"Lower\", \"description\": \"Lower bounds of scan frames if different from midpoints\"}, \"upper\": {\"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"items\": {\"type\": \"number\"}, \"type\": \"array\"}]}, \"type\": \"object\", \"title\": \"Upper\", \"description\": \"Upper bounds of scan frames if different from midpoints\"}}, \"type\": \"object\", \"required\": [\"total_frames\", \"returned_frames\", \"format\", \"lower\", \"upper\"], \"title\": \"BoundsResponse\", \"description\": \"Bounds of a generated scan.\"}, \"Circle\": {\"properties\": {\"x_axis\": {\"title\": \"X Axis\", \"description\": \"The name matching the x axis of the spec\"}, \"y_axis\": {\"title\": \"Y Axis\", \"description\": \"The name matching the y axis of the spec\"}, \"x_middle\": {\"type\": \"number\", \"title\": \"X Middle\", \"description\": \"The central x point of the circle\"}, \"y_middle\": {\"type\": \"number\", \"title\": \"Y Middle\", \"description\": \"The central y point of the circle\"}, \"radius\": {\"type\": \"number\", \"exclusiveMinimum\": 0.0, \"title\": \"Radius\", \"description\": \"Radius of the circle\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Circle\"], \"const\": \"Circle\", \"title\": \"Type\", \"default\": \"Circle\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"x_axis\", \"y_axis\", \"x_middle\", \"y_middle\", \"radius\"], \"title\": \"Circle\", \"description\": \"Mask contains points of axis within an xy circle of given radius.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Circle\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 1, 3, 10) * ~Line(\\\"x\\\", 0, 2, 10)\\n    spec = grid & Circle(\\\"x\\\", \\\"y\\\", 1, 2, 0.9)\"}, \"CombinationOf-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"CombinationOf\"], \"const\": \"CombinationOf\", \"title\": \"Type\", \"default\": \"CombinationOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"CombinationOf\", \"description\": \"Abstract baseclass for a combination of two regions, left and right.\"}, \"CombinationOf-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"CombinationOf\"], \"const\": \"CombinationOf\", \"title\": \"Type\", \"default\": \"CombinationOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"CombinationOf\", \"description\": \"Abstract baseclass for a combination of two regions, left and right.\"}, \"Concat-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The left-hand Spec to Concat, midpoints will appear earlier\"}, \"right\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The right-hand Spec to Concat, midpoints will appear later\"}, \"gap\": {\"type\": \"boolean\", \"title\": \"Gap\", \"description\": \"If True, force a gap in the output at the join\", \"default\": false}, \"check_path_changes\": {\"type\": \"boolean\", \"title\": \"Check Path Changes\", \"description\": \"If True path through scan will not be modified by squash\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Concat\"], \"const\": \"Concat\", \"title\": \"Type\", \"default\": \"Concat\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"Concat\", \"description\": \"Concatenate two Specs together, running one after the other.\\n\\nEach Dimension of left and right must contain the same axes. Typically\\nformed using `Spec.concat`.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"x\\\", 1, 3, 3).concat(Line(\\\"x\\\", 4, 5, 5))\"}, \"Concat-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The left-hand Spec to Concat, midpoints will appear earlier\"}, \"right\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The right-hand Spec to Concat, midpoints will appear later\"}, \"gap\": {\"type\": \"boolean\", \"title\": \"Gap\", \"description\": \"If True, force a gap in the output at the join\", \"default\": false}, \"check_path_changes\": {\"type\": \"boolean\", \"title\": \"Check Path Changes\", \"description\": \"If True path through scan will not be modified by squash\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Concat\"], \"const\": \"Concat\", \"title\": \"Type\", \"default\": \"Concat\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"Concat\", \"description\": \"Concatenate two Specs together, running one after the other.\\n\\nEach Dimension of left and right must contain the same axes. Typically\\nformed using `Spec.concat`.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"x\\\", 1, 3, 3).concat(Line(\\\"x\\\", 4, 5, 5))\"}, \"DifferenceOf-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"DifferenceOf\"], \"const\": \"DifferenceOf\", \"title\": \"Type\", \"default\": \"DifferenceOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"DifferenceOf\", \"description\": \"A point is in DifferenceOf(a, b) if in a and not in b.\\n\\nTypically created with the ``-`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) - Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False, False, False])\"}, \"DifferenceOf-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"DifferenceOf\"], \"const\": \"DifferenceOf\", \"title\": \"Type\", \"default\": \"DifferenceOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"DifferenceOf\", \"description\": \"A point is in DifferenceOf(a, b) if in a and not in b.\\n\\nTypically created with the ``-`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) - Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False, False, False])\"}, \"Ellipse\": {\"properties\": {\"x_axis\": {\"title\": \"X Axis\", \"description\": \"The name matching the x axis of the spec\"}, \"y_axis\": {\"title\": \"Y Axis\", \"description\": \"The name matching the y axis of the spec\"}, \"x_middle\": {\"type\": \"number\", \"title\": \"X Middle\", \"description\": \"The central x point of the ellipse\"}, \"y_middle\": {\"type\": \"number\", \"title\": \"Y Middle\", \"description\": \"The central y point of the ellipse\"}, \"x_radius\": {\"type\": \"number\", \"exclusiveMinimum\": 0.0, \"title\": \"X Radius\", \"description\": \"The radius along the x axis of the ellipse\"}, \"y_radius\": {\"type\": \"number\", \"exclusiveMinimum\": 0.0, \"title\": \"Y Radius\", \"description\": \"The radius along the y axis of the ellipse\"}, \"angle\": {\"type\": \"number\", \"title\": \"Angle\", \"description\": \"The angle of the ellipse (degrees)\", \"default\": 0.0}, \"type\": {\"type\": \"string\", \"enum\": [\"Ellipse\"], \"const\": \"Ellipse\", \"title\": \"Type\", \"default\": \"Ellipse\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"x_axis\", \"y_axis\", \"x_middle\", \"y_middle\", \"x_radius\", \"y_radius\"], \"title\": \"Ellipse\", \"description\": \"Mask contains points of axis within an xy ellipse of given radius.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Ellipse\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 3, 8, 10) * ~Line(\\\"x\\\", 1 ,8, 10)\\n    spec = grid & Ellipse(\\\"x\\\", \\\"y\\\", 5, 5, 2, 3, 75)\"}, \"GapResponse\": {\"properties\": {\"gap\": {\"items\": {\"type\": \"boolean\"}, \"type\": \"array\", \"title\": \"Gap\", \"description\": \"Boolean array indicating if there is a gap between each frame\"}}, \"type\": \"object\", \"required\": [\"gap\"], \"title\": \"GapResponse\", \"description\": \"Presence of gaps in a generated scan.\"}, \"HTTPValidationError\": {\"properties\": {\"detail\": {\"items\": {\"$ref\": \"#/components/schemas/ValidationError\"}, \"type\": \"array\", \"title\": \"Detail\"}}, \"type\": \"object\", \"title\": \"HTTPValidationError\"}, \"IntersectionOf-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"IntersectionOf\"], \"const\": \"IntersectionOf\", \"title\": \"Type\", \"default\": \"IntersectionOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"IntersectionOf\", \"description\": \"A point is in IntersectionOf(a, b) if in both a and b.\\n\\nTypically created with the ``&`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) & Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False, False,  True, False, False])\"}, \"IntersectionOf-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"IntersectionOf\"], \"const\": \"IntersectionOf\", \"title\": \"Type\", \"default\": \"IntersectionOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"IntersectionOf\", \"description\": \"A point is in IntersectionOf(a, b) if in both a and b.\\n\\nTypically created with the ``&`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) & Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False, False,  True, False, False])\"}, \"Line\": {\"properties\": {\"axis\": {\"title\": \"Axis\", \"description\": \"An identifier for what to move\"}, \"start\": {\"type\": \"number\", \"title\": \"Start\", \"description\": \"Midpoint of the first point of the line\"}, \"stop\": {\"type\": \"number\", \"title\": \"Stop\", \"description\": \"Midpoint of the last point of the line\"}, \"num\": {\"type\": \"integer\", \"minimum\": 1.0, \"title\": \"Num\", \"description\": \"Number of frames to produce\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Line\"], \"const\": \"Line\", \"title\": \"Type\", \"default\": \"Line\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"axis\", \"start\", \"stop\", \"num\"], \"title\": \"Line\", \"description\": \"Linearly spaced frames with start and stop as first and last midpoints.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"x\\\", 1, 2, 5)\"}, \"Mask-Input\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The Spec containing the source midpoints\"}, \"region\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The Region that midpoints will be inside\"}, \"check_path_changes\": {\"type\": \"boolean\", \"title\": \"Check Path Changes\", \"description\": \"If True path through scan will not be modified by squash\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Mask\"], \"const\": \"Mask\", \"title\": \"Type\", \"default\": \"Mask\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"spec\", \"region\"], \"title\": \"Mask\", \"description\": \"Restrict Spec to only midpoints that fall inside the given Region.\\n\\nTypically created with the ``&`` operator. It also pushes down the\\n``& | ^ -`` operators to its `Region` to avoid the need for brackets on\\ncombinations of Regions.\\n\\nIf a Region spans multiple Frames objects, they will be squashed together.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Circle\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * Line(\\\"x\\\", 3, 5, 5) & Circle(\\\"x\\\", \\\"y\\\", 4, 2, 1.2)\\n\\nSee Also: `why-squash-can-change-path`\"}, \"Mask-Output\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The Spec containing the source midpoints\"}, \"region\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The Region that midpoints will be inside\"}, \"check_path_changes\": {\"type\": \"boolean\", \"title\": \"Check Path Changes\", \"description\": \"If True path through scan will not be modified by squash\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Mask\"], \"const\": \"Mask\", \"title\": \"Type\", \"default\": \"Mask\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"spec\", \"region\"], \"title\": \"Mask\", \"description\": \"Restrict Spec to only midpoints that fall inside the given Region.\\n\\nTypically created with the ``&`` operator. It also pushes down the\\n``& | ^ -`` operators to its `Region` to avoid the need for brackets on\\ncombinations of Regions.\\n\\nIf a Region spans multiple Frames objects, they will be squashed together.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Circle\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * Line(\\\"x\\\", 3, 5, 5) & Circle(\\\"x\\\", \\\"y\\\", 4, 2, 1.2)\\n\\nSee Also: `why-squash-can-change-path`\"}, \"MidpointsResponse\": {\"properties\": {\"total_frames\": {\"type\": \"integer\", \"title\": \"Total Frames\", \"description\": \"Total number of frames in spec\"}, \"returned_frames\": {\"type\": \"integer\", \"title\": \"Returned Frames\", \"description\": \"Total of number of frames in this response, may be less than total_frames due to downsampling etc.\"}, \"format\": {\"$ref\": \"#/components/schemas/PointsFormat\", \"description\": \"Format of returned point data\"}, \"midpoints\": {\"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"items\": {\"type\": \"number\"}, \"type\": \"array\"}]}, \"type\": \"object\", \"title\": \"Midpoints\", \"description\": \"The midpoints of scan frames for each axis\"}}, \"type\": \"object\", \"required\": [\"total_frames\", \"returned_frames\", \"format\", \"midpoints\"], \"title\": \"MidpointsResponse\", \"description\": \"Midpoints of a generated scan.\"}, \"PointsFormat\": {\"type\": \"string\", \"enum\": [\"STRING\", \"FLOAT_LIST\", \"BASE64_ENCODED\"], \"title\": \"PointsFormat\", \"description\": \"Formats in which we can return points.\"}, \"PointsRequest\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The spec from which to generate points\"}, \"max_frames\": {\"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"null\"}], \"title\": \"Max Frames\", \"description\": \"The maximum number of points to return, if None will return as many as calculated\", \"default\": 100000}, \"format\": {\"$ref\": \"#/components/schemas/PointsFormat\", \"description\": \"The format in which to output the points data\", \"default\": \"FLOAT_LIST\"}}, \"type\": \"object\", \"required\": [\"spec\"], \"title\": \"PointsRequest\", \"description\": \"A request for generated scan points.\"}, \"Polygon\": {\"properties\": {\"x_axis\": {\"title\": \"X Axis\", \"description\": \"The name matching the x axis of the spec\"}, \"y_axis\": {\"title\": \"Y Axis\", \"description\": \"The name matching the y axis of the spec\"}, \"x_verts\": {\"items\": {\"type\": \"number\"}, \"type\": \"array\", \"minItems\": 3, \"title\": \"X Verts\", \"description\": \"The Nx1 x coordinates of the polygons vertices\"}, \"y_verts\": {\"items\": {\"type\": \"number\"}, \"type\": \"array\", \"minItems\": 3, \"title\": \"Y Verts\", \"description\": \"The Nx1 y coordinates of the polygons vertices\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Polygon\"], \"const\": \"Polygon\", \"title\": \"Type\", \"default\": \"Polygon\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"x_axis\", \"y_axis\", \"x_verts\", \"y_verts\"], \"title\": \"Polygon\", \"description\": \"Mask contains points of axis within a rotated xy polygon.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Polygon\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 3, 8, 10) * ~Line(\\\"x\\\", 1 ,8, 10)\\n    spec = grid & Polygon(\\\"x\\\", \\\"y\\\", [1.0, 6.0, 8.0, 2.0], [4.0, 10.0, 6.0, 1.0])\"}, \"Product-Input\": {\"properties\": {\"outer\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"Will be executed once\"}, \"inner\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"Will be executed len(outer) times\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Product\"], \"const\": \"Product\", \"title\": \"Type\", \"default\": \"Product\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"outer\", \"inner\"], \"title\": \"Product\", \"description\": \"Outer product of two Specs, nesting inner within outer.\\n\\nThis means that inner will run in its entirety at each point in outer.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 3, 4, 12)\"}, \"Product-Output\": {\"properties\": {\"outer\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"Will be executed once\"}, \"inner\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"Will be executed len(outer) times\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Product\"], \"const\": \"Product\", \"title\": \"Type\", \"default\": \"Product\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"outer\", \"inner\"], \"title\": \"Product\", \"description\": \"Outer product of two Specs, nesting inner within outer.\\n\\nThis means that inner will run in its entirety at each point in outer.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 3, 4, 12)\"}, \"Range\": {\"properties\": {\"axis\": {\"title\": \"Axis\", \"description\": \"The name matching the axis to mask in spec\"}, \"min\": {\"type\": \"number\", \"title\": \"Min\", \"description\": \"The minimum inclusive value in the region\"}, \"max\": {\"type\": \"number\", \"title\": \"Max\", \"description\": \"The minimum inclusive value in the region\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Range\"], \"const\": \"Range\", \"title\": \"Type\", \"default\": \"Range\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"axis\", \"min\", \"max\"], \"title\": \"Range\", \"description\": \"Mask contains points of axis >= min and <= max.\\n\\n>>> r = Range(\\\"x\\\", 1, 2)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True,  True, False, False])\"}, \"Rectangle\": {\"properties\": {\"x_axis\": {\"title\": \"X Axis\", \"description\": \"The name matching the x axis of the spec\"}, \"y_axis\": {\"title\": \"Y Axis\", \"description\": \"The name matching the y axis of the spec\"}, \"x_min\": {\"type\": \"number\", \"title\": \"X Min\", \"description\": \"Minimum inclusive x value in the region\"}, \"y_min\": {\"type\": \"number\", \"title\": \"Y Min\", \"description\": \"Minimum inclusive y value in the region\"}, \"x_max\": {\"type\": \"number\", \"title\": \"X Max\", \"description\": \"Maximum inclusive x value in the region\"}, \"y_max\": {\"type\": \"number\", \"title\": \"Y Max\", \"description\": \"Maximum inclusive y value in the region\"}, \"angle\": {\"type\": \"number\", \"title\": \"Angle\", \"description\": \"Clockwise rotation angle of the rectangle\", \"default\": 0.0}, \"type\": {\"type\": \"string\", \"enum\": [\"Rectangle\"], \"const\": \"Rectangle\", \"title\": \"Type\", \"default\": \"Rectangle\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"x_axis\", \"y_axis\", \"x_min\", \"y_min\", \"x_max\", \"y_max\"], \"title\": \"Rectangle\", \"description\": \"Mask contains points of axis within a rotated xy rectangle.\\n\\n.. example_spec::\\n\\n    from scanspec.regions import Rectangle\\n    from scanspec.specs import Line\\n\\n    grid = Line(\\\"y\\\", 1, 3, 10) * ~Line(\\\"x\\\", 0, 2, 10)\\n    spec = grid & Rectangle(\\\"x\\\", \\\"y\\\", 0, 1.1, 1.5, 2.1, 30)\"}, \"Region-Input\": {\"oneOf\": [{\"$ref\": \"#/components/schemas/CombinationOf-Input\"}, {\"$ref\": \"#/components/schemas/UnionOf-Input\"}, {\"$ref\": \"#/components/schemas/IntersectionOf-Input\"}, {\"$ref\": \"#/components/schemas/DifferenceOf-Input\"}, {\"$ref\": \"#/components/schemas/SymmetricDifferenceOf-Input\"}, {\"$ref\": \"#/components/schemas/Range\"}, {\"$ref\": \"#/components/schemas/Rectangle\"}, {\"$ref\": \"#/components/schemas/Polygon\"}, {\"$ref\": \"#/components/schemas/Circle\"}, {\"$ref\": \"#/components/schemas/Ellipse\"}], \"discriminator\": {\"propertyName\": \"type\", \"mapping\": {\"Circle\": \"#/components/schemas/Circle\", \"CombinationOf\": \"#/components/schemas/CombinationOf-Input\", \"DifferenceOf\": \"#/components/schemas/DifferenceOf-Input\", \"Ellipse\": \"#/components/schemas/Ellipse\", \"IntersectionOf\": \"#/components/schemas/IntersectionOf-Input\", \"Polygon\": \"#/components/schemas/Polygon\", \"Range\": \"#/components/schemas/Range\", \"Rectangle\": \"#/components/schemas/Rectangle\", \"SymmetricDifferenceOf\": \"#/components/schemas/SymmetricDifferenceOf-Input\", \"UnionOf\": \"#/components/schemas/UnionOf-Input\"}}}, \"Region-Output\": {\"oneOf\": [{\"$ref\": \"#/components/schemas/CombinationOf-Output\"}, {\"$ref\": \"#/components/schemas/UnionOf-Output\"}, {\"$ref\": \"#/components/schemas/IntersectionOf-Output\"}, {\"$ref\": \"#/components/schemas/DifferenceOf-Output\"}, {\"$ref\": \"#/components/schemas/SymmetricDifferenceOf-Output\"}, {\"$ref\": \"#/components/schemas/Range\"}, {\"$ref\": \"#/components/schemas/Rectangle\"}, {\"$ref\": \"#/components/schemas/Polygon\"}, {\"$ref\": \"#/components/schemas/Circle\"}, {\"$ref\": \"#/components/schemas/Ellipse\"}], \"discriminator\": {\"propertyName\": \"type\", \"mapping\": {\"Circle\": \"#/components/schemas/Circle\", \"CombinationOf\": \"#/components/schemas/CombinationOf-Output\", \"DifferenceOf\": \"#/components/schemas/DifferenceOf-Output\", \"Ellipse\": \"#/components/schemas/Ellipse\", \"IntersectionOf\": \"#/components/schemas/IntersectionOf-Output\", \"Polygon\": \"#/components/schemas/Polygon\", \"Range\": \"#/components/schemas/Range\", \"Rectangle\": \"#/components/schemas/Rectangle\", \"SymmetricDifferenceOf\": \"#/components/schemas/SymmetricDifferenceOf-Output\", \"UnionOf\": \"#/components/schemas/UnionOf-Output\"}}}, \"Repeat\": {\"properties\": {\"num\": {\"type\": \"integer\", \"minimum\": 1.0, \"title\": \"Num\", \"description\": \"Number of frames to produce\"}, \"gap\": {\"type\": \"boolean\", \"title\": \"Gap\", \"description\": \"If False and the slowest of the stack of Frames is snaked then the end and start of consecutive iterations of Spec will have no gap\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Repeat\"], \"const\": \"Repeat\", \"title\": \"Type\", \"default\": \"Repeat\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"num\"], \"title\": \"Repeat\", \"description\": \"Repeat an empty frame num times.\\n\\nCan be used on the outside of a scan to repeat the same scan many times.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = 2 * ~Line.bounded(\\\"x\\\", 3, 4, 1)\\n\\nIf you want snaked axes to have no gap between iterations you can do:\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Repeat\\n\\n    spec = Repeat(2, gap=False) * ~Line.bounded(\\\"x\\\", 3, 4, 1)\\n\\n.. note:: There is no turnaround arrow at x=4\"}, \"SmallestStepResponse\": {\"properties\": {\"absolute\": {\"type\": \"number\", \"title\": \"Absolute\", \"description\": \"Absolute smallest distance between two points on a single axis\"}, \"per_axis\": {\"additionalProperties\": {\"type\": \"number\"}, \"type\": \"object\", \"title\": \"Per Axis\", \"description\": \"Smallest distance between two points on each axis\"}}, \"type\": \"object\", \"required\": [\"absolute\", \"per_axis\"], \"title\": \"SmallestStepResponse\", \"description\": \"Information about the smallest steps between points in a spec.\"}, \"Snake-Input\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The Spec to run in reverse every other iteration\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Snake\"], \"const\": \"Snake\", \"title\": \"Type\", \"default\": \"Snake\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"spec\"], \"title\": \"Snake\", \"description\": \"Run the Spec in reverse on every other iteration when nested.\\n\\nTypically created with the ``~`` operator.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * ~Line(\\\"x\\\", 3, 5, 5)\"}, \"Snake-Output\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The Spec to run in reverse every other iteration\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Snake\"], \"const\": \"Snake\", \"title\": \"Type\", \"default\": \"Snake\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"spec\"], \"title\": \"Snake\", \"description\": \"Run the Spec in reverse on every other iteration when nested.\\n\\nTypically created with the ``~`` operator.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"y\\\", 1, 3, 3) * ~Line(\\\"x\\\", 3, 5, 5)\"}, \"Spec-Input\": {\"oneOf\": [{\"$ref\": \"#/components/schemas/Product-Input\"}, {\"$ref\": \"#/components/schemas/Repeat\"}, {\"$ref\": \"#/components/schemas/Zip-Input\"}, {\"$ref\": \"#/components/schemas/Mask-Input\"}, {\"$ref\": \"#/components/schemas/Snake-Input\"}, {\"$ref\": \"#/components/schemas/Concat-Input\"}, {\"$ref\": \"#/components/schemas/Squash-Input\"}, {\"$ref\": \"#/components/schemas/Line\"}, {\"$ref\": \"#/components/schemas/Static\"}, {\"$ref\": \"#/components/schemas/Spiral\"}], \"discriminator\": {\"propertyName\": \"type\", \"mapping\": {\"Concat\": \"#/components/schemas/Concat-Input\", \"Line\": \"#/components/schemas/Line\", \"Mask\": \"#/components/schemas/Mask-Input\", \"Product\": \"#/components/schemas/Product-Input\", \"Repeat\": \"#/components/schemas/Repeat\", \"Snake\": \"#/components/schemas/Snake-Input\", \"Spiral\": \"#/components/schemas/Spiral\", \"Squash\": \"#/components/schemas/Squash-Input\", \"Static\": \"#/components/schemas/Static\", \"Zip\": \"#/components/schemas/Zip-Input\"}}}, \"Spec-Output\": {\"oneOf\": [{\"$ref\": \"#/components/schemas/Product-Output\"}, {\"$ref\": \"#/components/schemas/Repeat\"}, {\"$ref\": \"#/components/schemas/Zip-Output\"}, {\"$ref\": \"#/components/schemas/Mask-Output\"}, {\"$ref\": \"#/components/schemas/Snake-Output\"}, {\"$ref\": \"#/components/schemas/Concat-Output\"}, {\"$ref\": \"#/components/schemas/Squash-Output\"}, {\"$ref\": \"#/components/schemas/Line\"}, {\"$ref\": \"#/components/schemas/Static\"}, {\"$ref\": \"#/components/schemas/Spiral\"}], \"discriminator\": {\"propertyName\": \"type\", \"mapping\": {\"Concat\": \"#/components/schemas/Concat-Output\", \"Line\": \"#/components/schemas/Line\", \"Mask\": \"#/components/schemas/Mask-Output\", \"Product\": \"#/components/schemas/Product-Output\", \"Repeat\": \"#/components/schemas/Repeat\", \"Snake\": \"#/components/schemas/Snake-Output\", \"Spiral\": \"#/components/schemas/Spiral\", \"Squash\": \"#/components/schemas/Squash-Output\", \"Static\": \"#/components/schemas/Static\", \"Zip\": \"#/components/schemas/Zip-Output\"}}}, \"Spiral\": {\"properties\": {\"x_axis\": {\"title\": \"X Axis\", \"description\": \"An identifier for what to move for x\"}, \"y_axis\": {\"title\": \"Y Axis\", \"description\": \"An identifier for what to move for y\"}, \"x_start\": {\"type\": \"number\", \"title\": \"X Start\", \"description\": \"x centre of the spiral\"}, \"y_start\": {\"type\": \"number\", \"title\": \"Y Start\", \"description\": \"y centre of the spiral\"}, \"x_range\": {\"type\": \"number\", \"title\": \"X Range\", \"description\": \"x width of the spiral\"}, \"y_range\": {\"type\": \"number\", \"title\": \"Y Range\", \"description\": \"y width of the spiral\"}, \"num\": {\"type\": \"integer\", \"minimum\": 1.0, \"title\": \"Num\", \"description\": \"Number of frames to produce\"}, \"rotate\": {\"type\": \"number\", \"title\": \"Rotate\", \"description\": \"How much to rotate the angle of the spiral\", \"default\": 0.0}, \"type\": {\"type\": \"string\", \"enum\": [\"Spiral\"], \"const\": \"Spiral\", \"title\": \"Type\", \"default\": \"Spiral\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"x_axis\", \"y_axis\", \"x_start\", \"y_start\", \"x_range\", \"y_range\", \"num\"], \"title\": \"Spiral\", \"description\": \"Archimedean spiral of \\\"x_axis\\\" and \\\"y_axis\\\".\\n\\nStarts at centre point (\\\"x_start\\\", \\\"y_start\\\") with angle \\\"rotate\\\". Produces\\n\\\"num\\\" points in a spiral spanning width of \\\"x_range\\\" and height of \\\"y_range\\\"\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Spiral\\n\\n    spec = Spiral(\\\"x\\\", \\\"y\\\", 1, 5, 10, 50, 30)\"}, \"Squash-Input\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The Spec to squash the dimensions of\"}, \"check_path_changes\": {\"type\": \"boolean\", \"title\": \"Check Path Changes\", \"description\": \"If True path through scan will not be modified by squash\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Squash\"], \"const\": \"Squash\", \"title\": \"Type\", \"default\": \"Squash\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"spec\"], \"title\": \"Squash\", \"description\": \"Squash a stack of Frames together into a single expanded Frames object.\\n\\nSee Also:\\n    `why-squash-can-change-path`\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Squash\\n\\n    spec = Squash(Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 0, 1, 4))\"}, \"Squash-Output\": {\"properties\": {\"spec\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The Spec to squash the dimensions of\"}, \"check_path_changes\": {\"type\": \"boolean\", \"title\": \"Check Path Changes\", \"description\": \"If True path through scan will not be modified by squash\", \"default\": true}, \"type\": {\"type\": \"string\", \"enum\": [\"Squash\"], \"const\": \"Squash\", \"title\": \"Type\", \"default\": \"Squash\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"spec\"], \"title\": \"Squash\", \"description\": \"Squash a stack of Frames together into a single expanded Frames object.\\n\\nSee Also:\\n    `why-squash-can-change-path`\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Squash\\n\\n    spec = Squash(Line(\\\"y\\\", 1, 2, 3) * Line(\\\"x\\\", 0, 1, 4))\"}, \"Static\": {\"properties\": {\"axis\": {\"title\": \"Axis\", \"description\": \"An identifier for what to move\"}, \"value\": {\"type\": \"number\", \"title\": \"Value\", \"description\": \"The value at each point\"}, \"num\": {\"type\": \"integer\", \"minimum\": 1.0, \"title\": \"Num\", \"description\": \"Number of frames to produce\", \"default\": 1}, \"type\": {\"type\": \"string\", \"enum\": [\"Static\"], \"const\": \"Static\", \"title\": \"Type\", \"default\": \"Static\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"axis\", \"value\"], \"title\": \"Static\", \"description\": \"A static frame, repeated num times, with axis at value.\\n\\nCan be used to set axis=value at every point in a scan.\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line, Static\\n\\n    spec = Line(\\\"y\\\", 1, 2, 3).zip(Static(\\\"x\\\", 3))\"}, \"SymmetricDifferenceOf-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"SymmetricDifferenceOf\"], \"const\": \"SymmetricDifferenceOf\", \"title\": \"Type\", \"default\": \"SymmetricDifferenceOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"SymmetricDifferenceOf\", \"description\": \"A point is in SymmetricDifferenceOf(a, b) if in either a or b, but not both.\\n\\nTypically created with the ``^`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) ^ Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False,  True, False])\"}, \"SymmetricDifferenceOf-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"SymmetricDifferenceOf\"], \"const\": \"SymmetricDifferenceOf\", \"title\": \"Type\", \"default\": \"SymmetricDifferenceOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"SymmetricDifferenceOf\", \"description\": \"A point is in SymmetricDifferenceOf(a, b) if in either a or b, but not both.\\n\\nTypically created with the ``^`` operator.\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) ^ Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True, False,  True, False])\"}, \"UnionOf-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Input\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"UnionOf\"], \"const\": \"UnionOf\", \"title\": \"Type\", \"default\": \"UnionOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"UnionOf\", \"description\": \"A point is in UnionOf(a, b) if in either a or b.\\n\\nTypically created with the ``|`` operator\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) | Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True,  True,  True, False])\"}, \"UnionOf-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The left-hand Region to combine\"}, \"right\": {\"$ref\": \"#/components/schemas/Region-Output\", \"description\": \"The right-hand Region to combine\"}, \"type\": {\"type\": \"string\", \"enum\": [\"UnionOf\"], \"const\": \"UnionOf\", \"title\": \"Type\", \"default\": \"UnionOf\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"UnionOf\", \"description\": \"A point is in UnionOf(a, b) if in either a or b.\\n\\nTypically created with the ``|`` operator\\n\\n>>> r = Range(\\\"x\\\", 0.5, 2.5) | Range(\\\"x\\\", 1.5, 3.5)\\n>>> r.mask({\\\"x\\\": np.array([0, 1, 2, 3, 4])})\\narray([False,  True,  True,  True, False])\"}, \"ValidResponse\": {\"properties\": {\"input_spec\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The input scanspec\"}, \"valid_spec\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The validated version of the spec\"}}, \"type\": \"object\", \"required\": [\"input_spec\", \"valid_spec\"], \"title\": \"ValidResponse\", \"description\": \"Response model for spec validation.\"}, \"ValidationError\": {\"properties\": {\"loc\": {\"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]}, \"type\": \"array\", \"title\": \"Location\"}, \"msg\": {\"type\": \"string\", \"title\": \"Message\"}, \"type\": {\"type\": \"string\", \"title\": \"Error Type\"}}, \"type\": \"object\", \"required\": [\"loc\", \"msg\", \"type\"], \"title\": \"ValidationError\"}, \"Zip-Input\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The left-hand Spec to Zip, will appear earlier in axes\"}, \"right\": {\"$ref\": \"#/components/schemas/Spec-Input\", \"description\": \"The right-hand Spec to Zip, will appear later in axes\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Zip\"], \"const\": \"Zip\", \"title\": \"Type\", \"default\": \"Zip\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"Zip\", \"description\": \"Run two Specs in parallel, merging their midpoints together.\\n\\nTypically formed using `Spec.zip`.\\n\\nStacks of Frames are merged by:\\n\\n- If right creates a stack of a single Frames object of size 1, expand it to\\n  the size of the fastest Frames object created by left\\n- Merge individual Frames objects together from fastest to slowest\\n\\nThis means that Zipping a Spec producing stack [l2, l1] with a Spec\\nproducing stack [r1] will assert len(l1)==len(r1), and produce\\nstack [l2, l1.zip(r1)].\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"z\\\", 1, 2, 3) * Line(\\\"y\\\", 3, 4, 5).zip(Line(\\\"x\\\", 4, 5, 5))\"}, \"Zip-Output\": {\"properties\": {\"left\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The left-hand Spec to Zip, will appear earlier in axes\"}, \"right\": {\"$ref\": \"#/components/schemas/Spec-Output\", \"description\": \"The right-hand Spec to Zip, will appear later in axes\"}, \"type\": {\"type\": \"string\", \"enum\": [\"Zip\"], \"const\": \"Zip\", \"title\": \"Type\", \"default\": \"Zip\"}}, \"additionalProperties\": false, \"type\": \"object\", \"required\": [\"left\", \"right\"], \"title\": \"Zip\", \"description\": \"Run two Specs in parallel, merging their midpoints together.\\n\\nTypically formed using `Spec.zip`.\\n\\nStacks of Frames are merged by:\\n\\n- If right creates a stack of a single Frames object of size 1, expand it to\\n  the size of the fastest Frames object created by left\\n- Merge individual Frames objects together from fastest to slowest\\n\\nThis means that Zipping a Spec producing stack [l2, l1] with a Spec\\nproducing stack [r1] will assert len(l1)==len(r1), and produce\\nstack [l2, l1.zip(r1)].\\n\\n.. example_spec::\\n\\n    from scanspec.specs import Line\\n\\n    spec = Line(\\\"z\\\", 1, 2, 3) * Line(\\\"y\\\", 3, 4, 5).zip(Line(\\\"x\\\", 4, 5, 5))\"}}}}\ndiff --git a/src/scanspec/cli.py b/src/scanspec/cli.py\nindex 82a57cce..75e582d2 100644\n--- a/src/scanspec/cli.py\n+++ b/src/scanspec/cli.py\n@@ -20,7 +20,7 @@\n )\n @click.version_option(prog_name=\"scanspec\", message=\"%(version)s\")\n @click.pass_context\n-def cli(ctx, log_level: str):\n+def cli(ctx: click.Context, log_level: str):\n     \"\"\"Top level scanspec command line interface.\"\"\"\n     level = getattr(logging, log_level.upper(), None)\n     logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=level)\n@@ -50,7 +50,7 @@ def plot(spec: str):\n @click.option(\n     \"--port\", default=8080, help=\"The port that the scanspec service will be hosted on.\"\n )\n-def service(cors, port):\n+def service(cors: bool, port: int):\n     \"\"\"Run up a REST service.\"\"\"\n     from scanspec.service import run_app\n \ndiff --git a/src/scanspec/core.py b/src/scanspec/core.py\nindex a15b644c..4a4edb69 100644\n--- a/src/scanspec/core.py\n+++ b/src/scanspec/core.py\n@@ -2,20 +2,28 @@\n \n from __future__ import annotations\n \n+import itertools\n from collections.abc import Callable, Iterable, Iterator, Sequence\n from functools import lru_cache\n from inspect import isclass\n-from typing import Any, Generic, Literal, TypeVar\n+from typing import (\n+    Any,\n+    Generic,\n+    Literal,\n+    TypeVar,\n+)\n \n import numpy as np\n+import numpy.typing as npt\n from pydantic import BaseModel, ConfigDict, Field, GetCoreSchemaHandler\n from pydantic.dataclasses import is_pydantic_dataclass, rebuild_dataclass\n from pydantic_core import CoreSchema\n from pydantic_core.core_schema import tagged_union_schema\n \n __all__ = [\n-    \"if_instance_do\",\n     \"Axis\",\n+    \"OtherAxis\",\n+    \"if_instance_do\",\n     \"AxesPoints\",\n     \"Frames\",\n     \"SnakedFrames\",\n@@ -31,6 +39,9 @@\n StrictConfig: ConfigDict = {\"extra\": \"forbid\"}\n \n C = TypeVar(\"C\")\n+T = TypeVar(\"T\")\n+\n+GapArray = npt.NDArray[np.bool]\n \n \n def discriminated_union_of_subclasses(\n@@ -111,7 +122,7 @@ def calculate(self) -> int:\n     tagged_union = _TaggedUnion(super_cls, discriminator)\n     _tagged_unions[super_cls] = tagged_union\n \n-    def add_subclass_to_union(subclass):\n+    def add_subclass_to_union(subclass: type[C]):\n         # Add a discriminator field to a subclass so it can\n         # be identified when deserializing\n         subclass.__annotations__ = {\n@@ -120,7 +131,9 @@ def add_subclass_to_union(subclass):\n         }\n         setattr(subclass, discriminator, Field(subclass.__name__, repr=False))  # type: ignore\n \n-    def get_schema_of_union(cls, source_type: Any, handler: GetCoreSchemaHandler):\n+    def get_schema_of_union(\n+        cls: type[C], source_type: Any, handler: GetCoreSchemaHandler\n+    ):\n         if cls is not super_cls:\n             tagged_union.add_member(cls)\n             return handler(cls)\n@@ -138,7 +151,7 @@ def get_schema_of_union(cls, source_type: Any, handler: GetCoreSchemaHandler):\n \n \n class _TaggedUnion:\n-    def __init__(self, base_class: type, discriminator: str):\n+    def __init__(self, base_class: type[Any], discriminator: str):\n         self._base_class = base_class\n         # Classes and their field names that refer to this tagged union\n         self._discriminator = discriminator\n@@ -154,7 +167,7 @@ def add_member(self, cls: type):\n                 _TaggedUnion._rebuild(member)\n \n     @staticmethod\n-    def _rebuild(cls_or_func: type | Callable):\n+    def _rebuild(cls_or_func: Callable[..., T]) -> None:\n         if isclass(cls_or_func):\n             if is_pydantic_dataclass(cls_or_func):\n                 rebuild_dataclass(cls_or_func, force=True)\n@@ -170,11 +183,13 @@ def schema(self, handler: GetCoreSchemaHandler) -> CoreSchema:\n \n \n @lru_cache(1)\n-def _make_schema(members: tuple[type, ...], handler):\n+def _make_schema(\n+    members: tuple[type[Any], ...], handler: Callable[[Any], CoreSchema]\n+) -> dict[str, CoreSchema]:\n     return {member.__name__: handler(member) for member in members}\n \n \n-def if_instance_do(x: Any, cls: type, func: Callable):\n+def if_instance_do(x: C, cls: type[C], func: Callable[[C], T]) -> T:\n     \"\"\"If x is of type cls then return func(x), otherwise return NotImplemented.\n \n     Used as a helper when implementing operator overloading.\n@@ -186,11 +201,14 @@ def if_instance_do(x: Any, cls: type, func: Callable):\n \n \n #: A type variable for an `axis_` that can be specified for a scan\n-Axis = TypeVar(\"Axis\")\n+Axis = TypeVar(\"Axis\", covariant=True)\n+\n+#: Alternative axis variable to be used when two are required in the same type binding\n+OtherAxis = TypeVar(\"OtherAxis\")\n \n #: Map of axes to float ndarray of points\n #: E.g. {xmotor: array([0, 1, 2]), ymotor: array([2, 2, 2])}\n-AxesPoints = dict[Axis, np.ndarray]\n+AxesPoints = dict[Axis, npt.NDArray[np.floating[Any]]]\n \n \n class Frames(Generic[Axis]):\n@@ -226,7 +244,7 @@ def __init__(\n         midpoints: AxesPoints[Axis],\n         lower: AxesPoints[Axis] | None = None,\n         upper: AxesPoints[Axis] | None = None,\n-        gap: np.ndarray | None = None,\n+        gap: GapArray | None = None,\n     ):\n         #: The midpoints of scan frames for each axis\n         self.midpoints = midpoints\n@@ -274,7 +292,9 @@ def __len__(self) -> int:\n         # All axespoints arrays are same length, pick the first one\n         return len(self.gap)\n \n-    def extract(self, indices: np.ndarray, calculate_gap=True) -> Frames[Axis]:\n+    def extract(\n+        self, indices: npt.NDArray[np.signedinteger[Any]], calculate_gap: bool = True\n+    ) -> Frames[Axis]:\n         \"\"\"Return a new Frames object restricted to the indices provided.\n \n         Args:\n@@ -293,7 +313,7 @@ def extract_dict(ds: Iterable[AxesPoints[Axis]]) -> AxesPoints[Axis]:\n                 return {k: v[dim_indices] for k, v in d.items()}\n             return {}\n \n-        def extract_gap(gaps: Iterable[np.ndarray]) -> np.ndarray | None:\n+        def extract_gap(gaps: Iterable[GapArray]) -> GapArray | None:\n             for gap in gaps:\n                 if not calculate_gap:\n                     return gap[dim_indices]\n@@ -326,7 +346,7 @@ def concat_dict(ds: Sequence[AxesPoints[Axis]]) -> AxesPoints[Axis]:\n             # lower[ax] = np.concatenate(self.lower[ax], other.lower[ax])\n             return {a: np.concatenate([d[a] for d in ds]) for a in self.axes()}\n \n-        def concat_gap(gaps: Sequence[np.ndarray]) -> np.ndarray:\n+        def concat_gap(gaps: Sequence[GapArray]) -> GapArray:\n             g = np.concatenate(gaps)\n             # Calc the first frame\n             g[0] = gap_between_frames(other, self)\n@@ -354,7 +374,7 @@ def zip_dict(ds: Sequence[AxesPoints[Axis]]) -> AxesPoints[Axis]:\n             # lower[ax] = {**self.lower[ax], **other.lower[ax]}\n             return dict(kv for d in ds for kv in d.items())\n \n-        def zip_gap(gaps: Sequence[np.ndarray]) -> np.ndarray:\n+        def zip_gap(gaps: Sequence[GapArray]) -> GapArray:\n             # Gap if either frames has a gap. E.g.\n             # gap[i] = self.gap[i] | other.gap[i]\n             return np.logical_or.reduce(gaps)\n@@ -364,24 +384,24 @@ def zip_gap(gaps: Sequence[np.ndarray]) -> np.ndarray:\n \n def _merge_frames(\n     *stack: Frames[Axis],\n-    dict_merge=Callable[[Sequence[AxesPoints[Axis]]], AxesPoints[Axis]],  # type: ignore\n-    gap_merge=Callable[[Sequence[np.ndarray]], np.ndarray | None],\n+    dict_merge: Callable[[Sequence[AxesPoints[Axis]]], AxesPoints[Axis]],  # type: ignore\n+    gap_merge: Callable[[Sequence[GapArray]], GapArray | None],\n ) -> Frames[Axis]:\n     types = {type(fs) for fs in stack}\n     assert len(types) == 1, f\"Mismatching types for {stack}\"\n     cls = types.pop()\n \n-    # If any lower or upper are different, apply to those\n-    kwargs = {}\n-    for a in (\"lower\", \"upper\"):\n-        if any(fs.midpoints is not getattr(fs, a) for fs in stack):\n-            kwargs[a] = dict_merge([getattr(fs, a) for fs in stack])\n-\n     # Apply to midpoints, force calculation of gap\n     return cls(\n         midpoints=dict_merge([fs.midpoints for fs in stack]),\n         gap=gap_merge([fs.gap for fs in stack]),\n-        **kwargs,\n+        # If any lower or upper are different, apply to those\n+        lower=dict_merge([fs.lower for fs in stack])\n+        if any(fs.midpoints is not fs.lower for fs in stack)\n+        else None,\n+        upper=dict_merge([fs.upper for fs in stack])\n+        if any(fs.midpoints is not fs.upper for fs in stack)\n+        else None,\n     )\n \n \n@@ -393,7 +413,7 @@ def __init__(\n         midpoints: AxesPoints[Axis],\n         lower: AxesPoints[Axis] | None = None,\n         upper: AxesPoints[Axis] | None = None,\n-        gap: np.ndarray | None = None,\n+        gap: GapArray | None = None,\n     ):\n         super().__init__(midpoints, lower=lower, upper=upper, gap=gap)\n         # Override first element of gap to be True, as subsequent runs\n@@ -401,11 +421,15 @@ def __init__(\n         self.gap[0] = False\n \n     @classmethod\n-    def from_frames(cls, frames: Frames[Axis]) -> SnakedFrames[Axis]:\n+    def from_frames(\n+        cls: type[SnakedFrames[Any]], frames: Frames[OtherAxis]\n+    ) -> SnakedFrames[OtherAxis]:\n         \"\"\"Create a snaked version of a `Frames` object.\"\"\"\n         return cls(frames.midpoints, frames.lower, frames.upper, frames.gap)\n \n-    def extract(self, indices: np.ndarray, calculate_gap=True) -> Frames[Axis]:\n+    def extract(\n+        self, indices: npt.NDArray[np.signedinteger[Any]], calculate_gap: bool = True\n+    ) -> Frames[Axis]:\n         \"\"\"Return a new Frames object restricted to the indices provided.\n \n         Args:\n@@ -434,23 +458,23 @@ def extract(self, indices: np.ndarray, calculate_gap=True) -> Frames[Axis]:\n             cls = type(self)\n             gap = None\n \n-        # If lower or upper are different, apply to those\n-        kwargs = {}\n-        if self.midpoints is not self.lower:\n-            # If going backwards select from the opposite bound\n-            kwargs[\"lower\"] = {\n+        # Apply to midpoints\n+        return cls(\n+            {k: v[snake_indices] for k, v in self.midpoints.items()},\n+            gap=gap,\n+            # If lower or upper are different, apply to those\n+            lower={\n                 k: np.where(backwards, self.upper[k][snake_indices], v[snake_indices])\n                 for k, v in self.lower.items()\n             }\n-        if self.midpoints is not self.upper:\n-            kwargs[\"upper\"] = {\n+            if self.midpoints is not self.lower\n+            else None,\n+            upper={\n                 k: np.where(backwards, self.lower[k][snake_indices], v[snake_indices])\n                 for k, v in self.upper.items()\n             }\n-\n-        # Apply to midpoints\n-        return cls(\n-            {k: v[snake_indices] for k, v in self.midpoints.items()}, gap=gap, **kwargs\n+            if self.midpoints is not self.upper\n+            else None,\n         )\n \n \n@@ -459,7 +483,9 @@ def gap_between_frames(frames1: Frames[Axis], frames2: Frames[Axis]) -> bool:\n     return any(frames1.upper[a][-1] != frames2.lower[a][0] for a in frames1.axes())\n \n \n-def squash_frames(stack: list[Frames[Axis]], check_path_changes=True) -> Frames[Axis]:\n+def squash_frames(\n+    stack: list[Frames[Axis]], check_path_changes: bool = True\n+) -> Frames[Axis]:\n     \"\"\"Squash a stack of nested Frames into a single one.\n \n     Args:\n@@ -624,10 +650,7 @@ def __init__(self, stack: list[Frames[Axis]]):\n     @property\n     def axes(self) -> list[Axis]:\n         \"\"\"The axes that will be present in each points dictionary.\"\"\"\n-        axes = []\n-        for frames in self.stack:\n-            axes += frames.axes()\n-        return axes\n+        return list(itertools.chain(*(frames.axes() for frames in self.stack)))\n \n     def __len__(self) -> int:\n         \"\"\"The number of dictionaries that will be produced if iterated over.\"\"\"\ndiff --git a/src/scanspec/plot.py b/src/scanspec/plot.py\nindex f81c3447..44483655 100644\n--- a/src/scanspec/plot.py\n+++ b/src/scanspec/plot.py\n@@ -1,14 +1,16 @@\n \"\"\"`plot_spec` to visualize a scan.\"\"\"\n \n-from collections.abc import Iterator\n+from collections.abc import Iterable, Iterator\n from itertools import cycle\n from typing import Any\n \n import numpy as np\n+import numpy.typing as npt\n from matplotlib import colors, patches\n from matplotlib import pyplot as plt\n-from mpl_toolkits.mplot3d import Axes3D, proj3d\n-from scipy import interpolate\n+from matplotlib.axes import Axes\n+from mpl_toolkits.mplot3d import Axes3D, proj3d  # type: ignore\n+from scipy import interpolate  # type: ignore\n \n from .core import Path\n from .regions import Circle, Ellipse, Polygon, Rectangle, Region, find_regions\n@@ -17,51 +19,73 @@\n __all__ = [\"plot_spec\"]\n \n \n-def _plot_arrays(axes, arrays: list[np.ndarray], **kwargs):\n+def _plot_arrays(\n+    axes: Axes, arrays: list[npt.NDArray[np.floating[Any]]], **kwargs: Any\n+):\n     if len(arrays) > 2:\n-        axes.plot3D(arrays[2], arrays[1], arrays[0], **kwargs)\n+        axes.plot3D(arrays[2], arrays[1], arrays[0], **kwargs)  # type: ignore\n     elif len(arrays) == 2:\n-        axes.plot(arrays[1], arrays[0], **kwargs)\n+        axes.plot(arrays[1], arrays[0], **kwargs)  # type: ignore\n     else:\n-        axes.plot(arrays[0], np.zeros(len(arrays[0])), **kwargs)\n+        axes.plot(arrays[0], np.zeros(len(arrays[0])), **kwargs)  # type: ignore\n \n \n # https://stackoverflow.com/a/11156353\n-class _Arrow3D(patches.FancyArrowPatch):\n-    def __init__(self, xs, ys, zs, *args, **kwargs):\n-        super().__init__((0, 0), (0, 0), *args, **kwargs)\n+class Arrow3D(patches.FancyArrowPatch):\n+    def __init__(\n+        self,\n+        xs: npt.NDArray[np.floating[Any]],\n+        ys: npt.NDArray[np.floating[Any]],\n+        zs: npt.NDArray[np.floating[Any]],\n+        *args: Any,\n+        **kwargs: Any,\n+    ):\n+        super().__init__((0, 0), (0, 0), *args, **kwargs)  # type: ignore\n         self._verts3d = xs, ys, zs\n \n     # Added here because of https://github.com/matplotlib/matplotlib/issues/21688\n-    def do_3d_projection(self, renderer=None):\n+    def do_3d_projection(self, renderer: Any = None):\n         xs3d, ys3d, zs3d = self._verts3d\n         xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)  # type: ignore\n         self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n \n         return np.min(zs)\n \n+    @property\n+    def verts3d(\n+        self,\n+    ) -> tuple[\n+        npt.NDArray[np.floating[Any]],\n+        npt.NDArray[np.floating[Any]],\n+        npt.NDArray[np.floating[Any]],\n+    ]:\n+        return self._verts3d\n \n-def _plot_arrow(axes, arrays: list[np.ndarray]):\n+\n+def _plot_arrow(axes: Axes, arrays: list[npt.NDArray[np.floating[Any]]]):\n     if len(arrays) == 1:\n         arrays = [np.array([0, 0])] + arrays\n     if len(arrays) == 2:\n         head = [a[-1] for a in reversed(arrays)]\n         tail = [a[-1] - (a[-1] - a[-2]) * 0.1 for a in reversed(arrays)]\n-        axes.annotate(\n+        axes.annotate(  # type: ignore\n             \"\",\n-            head[:2],\n-            tail[:2],\n+            tuple(head[:2]),\n+            tuple(tail[:2]),\n             arrowprops={\"color\": \"lightgrey\", \"arrowstyle\": \"-|>\"},\n         )\n     elif len(arrays) == 3:\n         arrows = [a[-2:] for a in reversed(arrays)]\n-        a = _Arrow3D(\n-            *arrows[:3], mutation_scale=10, arrowstyle=\"-|>\", color=\"lightgrey\"\n-        )\n+        a = Arrow3D(*arrows[:3], mutation_scale=10, arrowstyle=\"-|>\", color=\"lightgrey\")\n         axes.add_artist(a)\n \n \n-def _plot_spline(axes, ranges, arrays: list[np.ndarray], index_colours: dict[int, str]):\n+def _plot_spline(\n+    axes: Axes,\n+    ranges: list[float],\n+    arrays: list[npt.NDArray[np.floating[Any]]],\n+    index_colours: dict[int, str],\n+) -> Iterable[list[npt.NDArray[np.floating[Any]]]]:\n     scaled_arrays = [a / r for a, r in zip(arrays, ranges, strict=False)]\n     # Define curves parametrically\n     t = np.zeros(len(arrays[0]))\n@@ -76,12 +100,14 @@ def _plot_spline(axes, ranges, arrays: list[np.ndarray], index_colours: dict[int\n         # There are no duplicated points, plot a spline\n         t /= t[-1]\n         # Scale the arrays so splines don't favour larger scaled axes\n-        tck, _ = interpolate.splprep(scaled_arrays, k=2, s=0)\n+        tck, _ = interpolate.splprep(scaled_arrays, k=2, s=0)  # type: ignore\n         starts = sorted(index_colours)\n         stops = starts[1:] + [len(arrays[0]) - 1]\n         for start, stop in zip(starts, stops, strict=False):\n-            tnew = np.linspace(t[start], t[stop], num=1001)\n-            spline = interpolate.splev(tnew, tck)\n+            start_value: float = t[start]\n+            stop_value: float = t[stop]\n+            tnew = np.linspace(start_value, stop_value, num=1001)\n+            spline: npt.NDArray[np.floating[Any]] = interpolate.splev(tnew, tck)  # type: ignore\n             # Scale the splines back to the original scaling\n             unscaled_splines = [a * r for a, r in zip(spline, ranges, strict=False)]\n             _plot_arrays(axes, unscaled_splines, color=index_colours[start])\n@@ -110,31 +136,31 @@ def plot_spec(spec: Spec[Any], title: str | None = None):\n \n     # Setup axes\n     if ndims > 2:\n-        plt.figure(figsize=(6, 6))\n-        plt_axes = plt.axes(projection=\"3d\")\n-        plt_axes.grid(False)\n+        plt.figure(figsize=(6, 6))  # type: ignore\n+        plt_axes: Axes = plt.axes(projection=\"3d\")  # type: ignore\n+        plt_axes.grid(False)  # type: ignore\n         if isinstance(plt_axes, Axes3D):\n-            plt_axes.set_zlabel(axes[-3])\n-            plt_axes.set_ylabel(axes[-2])\n-            plt_axes.view_init(elev=15)\n+            plt_axes.set_zlabel(axes[-3])  # type: ignore\n+            plt_axes.set_ylabel(axes[-2])  # type: ignore\n+            plt_axes.view_init(elev=15)  # type: ignore\n         else:\n             raise TypeError(\n                 \"Expected matplotlib to create an Axes3D object, \"\n                 f\"instead got: {plt_axes}\"\n             )\n     elif ndims == 2:\n-        plt.figure(figsize=(6, 6))\n-        plt_axes = plt.axes()\n-        plt_axes.set_ylabel(axes[-2])\n+        plt.figure(figsize=(6, 6))  # type: ignore\n+        plt_axes = plt.axes()  # type: ignore\n+        plt_axes.set_ylabel(axes[-2])  # type: ignore\n     else:\n-        plt.figure(figsize=(6, 2))\n-        plt_axes = plt.axes()\n+        plt.figure(figsize=(6, 2))  # type: ignore\n+        plt_axes = plt.axes()  # type: ignore\n         plt_axes.yaxis.set_visible(False)\n-    plt_axes.set_xlabel(axes[-1])\n+    plt_axes.set_xlabel(axes[-1])  # type: ignore\n \n     # Title with dimension sizes\n     title = title or \", \".join(f\"Dim[{' '.join(d.axes())} len={len(d)}]\" for d in dims)\n-    plt.title(title)\n+    plt.title(title)  # type: ignore\n \n     # Plot any Regions\n     if ndims <= 2:\n@@ -164,8 +190,8 @@ def plot_spec(spec: Spec[Any], title: str | None = None):\n                 plt_axes.add_patch(patches.Polygon(xy_verts, fill=False))\n \n     # Plot the splines\n-    tail: Any = {a: None for a in axes}\n-    ranges = [max(np.max(v) - np.min(v), 0.0001) for k, v in dim.midpoints.items()]\n+    tail: dict[str, npt.NDArray[np.floating[Any]] | None] = {a: None for a in axes}\n+    ranges = [max(float(np.max(v) - np.min(v)), 0.0001) for v in dim.midpoints.values()]\n     seg_col = cycle(colors.TABLEAU_COLORS)\n     last_index = 0\n     splines = None\n@@ -174,8 +200,8 @@ def plot_spec(spec: Spec[Any], title: str | None = None):\n     gap_indices = list(np.nonzero(dim.gap[1:])[0] + 1)\n     for index in gap_indices + [len(dim)]:\n         num_points = index - last_index\n-        arrays = []\n-        turnaround = []\n+        arrays: list[npt.NDArray[np.floating[Any]]] = []\n+        turnaround: list[npt.NDArray[np.floating[Any]]] = []\n         for a in axes:\n             # Add the midpoints and the lower and upper bounds\n             arr = np.empty(num_points * 2 + 1)\n@@ -184,13 +210,15 @@ def plot_spec(spec: Spec[Any], title: str | None = None):\n             arr[-1] = dim.upper[a][index - 1]\n             arrays.append(arr)\n             # Add the turnaround\n-            if tail[a] is not None:\n+            axis_tail = tail[a]\n+            if axis_tail is not None:\n                 # Already had a tail, add lead in points\n-                tail[a][2:] = np.linspace(-0.01, 0, 2) * (arr[1] - arr[0]) + arr[0]\n-                turnaround.append(tail[a])\n+                axis_tail[2:] = np.linspace(-0.01, 0, 2) * (arr[1] - arr[0]) + arr[0]\n+                turnaround.append(axis_tail)\n             # Add tail off points\n-            tail[a] = np.empty(4)\n-            tail[a][:2] = np.linspace(0, 0.01, 2) * (arr[-1] - arr[-2]) + arr[-1]\n+            axis_tail = np.empty(4)\n+            axis_tail[:2] = np.linspace(0, 0.01, 2) * (arr[-1] - arr[-2]) + arr[-1]\n+            tail[a] = axis_tail\n         last_index = index\n \n         arrow_arr = None\n@@ -240,4 +268,4 @@ def plot_spec(spec: Spec[Any], title: str | None = None):\n         color=\"lightgrey\",\n     )\n \n-    plt.show()\n+    plt.show()  # type: ignore\ndiff --git a/src/scanspec/regions.py b/src/scanspec/regions.py\nindex 0f8e6872..b9f2ba42 100644\n--- a/src/scanspec/regions.py\n+++ b/src/scanspec/regions.py\n@@ -9,9 +9,10 @@\n \n from collections.abc import Iterator, Mapping\n from dataclasses import is_dataclass\n-from typing import Any, Generic\n+from typing import Any, Generic, cast\n \n import numpy as np\n+import numpy.typing as npt\n from pydantic import BaseModel, Field, TypeAdapter\n from pydantic.dataclasses import dataclass\n \n@@ -39,6 +40,8 @@\n     \"find_regions\",\n ]\n \n+NpMask = npt.NDArray[np.bool]\n+\n \n @discriminated_union_of_subclasses\n class Region(Generic[Axis]):\n@@ -56,33 +59,33 @@ def axis_sets(self) -> list[set[Axis]]:  # noqa: D102\n         \"\"\"Produce the non-overlapping sets of axes this region spans.\"\"\"\n         raise NotImplementedError(self)\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         \"\"\"Produce a mask of which points are in the region.\"\"\"\n         raise NotImplementedError(self)\n \n-    def __or__(self, other) -> UnionOf[Axis]:\n+    def __or__(self, other: Region[Axis]) -> UnionOf[Axis]:\n         return if_instance_do(other, Region, lambda o: UnionOf(self, o))\n \n-    def __and__(self, other) -> IntersectionOf[Axis]:\n+    def __and__(self, other: Region[Axis]) -> IntersectionOf[Axis]:\n         return if_instance_do(other, Region, lambda o: IntersectionOf(self, o))\n \n-    def __sub__(self, other) -> DifferenceOf[Axis]:\n+    def __sub__(self, other: Region[Axis]) -> DifferenceOf[Axis]:\n         return if_instance_do(other, Region, lambda o: DifferenceOf(self, o))\n \n-    def __xor__(self, other) -> SymmetricDifferenceOf[Axis]:\n+    def __xor__(self, other: Region[Axis]) -> SymmetricDifferenceOf[Axis]:\n         return if_instance_do(other, Region, lambda o: SymmetricDifferenceOf(self, o))\n \n     def serialize(self) -> Mapping[str, Any]:\n         \"\"\"Serialize the Region to a dictionary.\"\"\"\n-        return TypeAdapter(Region).dump_python(self)\n+        return TypeAdapter(Region[Any]).dump_python(self)\n \n     @staticmethod\n-    def deserialize(obj) -> Region:\n+    def deserialize(obj: Any) -> Region[Any]:\n         \"\"\"Deserialize a Region from a dictionary.\"\"\"\n-        return TypeAdapter(Region).validate_python(obj)\n+        return TypeAdapter(Region[Any]).validate_python(obj)\n \n \n-def get_mask(region: Region[Axis], points: AxesPoints[Axis]) -> np.ndarray:\n+def get_mask(region: Region[Axis], points: AxesPoints[Axis]) -> NpMask:\n     \"\"\"Return a mask of the points inside the region.\n \n     If there is an overlap of axes of region and points return a\n@@ -93,7 +96,7 @@ def get_mask(region: Region[Axis], points: AxesPoints[Axis]) -> np.ndarray:\n     if needs_mask:\n         return region.mask(points)\n     else:\n-        return np.ones(len(list(points.values())[0]))\n+        return np.ones(len(list(points.values())[0]), dtype=np.bool)\n \n \n def _merge_axis_sets(axis_sets: list[set[Axis]]) -> Iterator[set[Axis]]:\n@@ -137,7 +140,7 @@ class UnionOf(CombinationOf[Axis]):\n     array([False,  True,  True,  True, False])\n     \"\"\"\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         mask = get_mask(self.left, points) | get_mask(self.right, points)\n         return mask\n \n@@ -153,7 +156,7 @@ class IntersectionOf(CombinationOf[Axis]):\n     array([False, False,  True, False, False])\n     \"\"\"\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         mask = get_mask(self.left, points) & get_mask(self.right, points)\n         return mask\n \n@@ -169,7 +172,7 @@ class DifferenceOf(CombinationOf[Axis]):\n     array([False,  True, False, False, False])\n     \"\"\"\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         left_mask = get_mask(self.left, points)\n         # Return the xor restricted to the left region\n         mask = left_mask ^ get_mask(self.right, points) & left_mask\n@@ -187,7 +190,7 @@ class SymmetricDifferenceOf(CombinationOf[Axis]):\n     array([False,  True, False,  True, False])\n     \"\"\"\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         mask = get_mask(self.left, points) ^ get_mask(self.right, points)\n         return mask\n \n@@ -208,7 +211,7 @@ class Range(Region[Axis]):\n     def axis_sets(self) -> list[set[Axis]]:  # noqa: D102\n         return [{self.axis}]\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         v = points[self.axis]\n         mask = np.bitwise_and(v >= self.min, v <= self.max)\n         return mask\n@@ -240,7 +243,7 @@ class Rectangle(Region[Axis]):\n     def axis_sets(self) -> list[set[Axis]]:  # noqa: D102\n         return [{self.x_axis, self.y_axis}]\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         x = points[self.x_axis] - self.x_min\n         y = points[self.y_axis] - self.y_min\n         if self.angle != 0:\n@@ -280,15 +283,15 @@ class Polygon(Region[Axis]):\n     def axis_sets(self) -> list[set[Axis]]:  # noqa: D102\n         return [{self.x_axis, self.y_axis}]\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         x = points[self.x_axis]\n         y = points[self.y_axis]\n         v1x, v1y = self.x_verts[-1], self.y_verts[-1]\n-        mask = np.full(len(x), False, dtype=np.int8)\n+        mask = np.full(len(x), False, dtype=np.bool)\n         for v2x, v2y in zip(self.x_verts, self.y_verts, strict=False):\n             # skip horizontal edges\n             if v2y != v1y:\n-                vmask = np.full(len(x), False, dtype=np.int8)\n+                vmask = np.full(len(x), False, dtype=np.bool)\n                 vmask |= (y < v2y) & (y >= v1y)\n                 vmask |= (y < v1y) & (y >= v2y)\n                 t = (y - v1y) / (v2y - v1y)\n@@ -320,7 +323,7 @@ class Circle(Region[Axis]):\n     def axis_sets(self) -> list[set[Axis]]:  # noqa: D102\n         return [{self.x_axis, self.y_axis}]\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         x = points[self.x_axis] - self.x_middle\n         y = points[self.y_axis] - self.y_middle\n         mask = x * x + y * y <= (self.radius * self.radius)\n@@ -355,7 +358,7 @@ class Ellipse(Region[Axis]):\n     def axis_sets(self) -> list[set[Axis]]:  # noqa: D102\n         return [{self.x_axis, self.y_axis}]\n \n-    def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n+    def mask(self, points: AxesPoints[Axis]) -> NpMask:  # noqa: D102\n         x = points[self.x_axis] - self.x_middle\n         y = points[self.y_axis] - self.y_middle\n         if self.angle != 0:\n@@ -369,7 +372,7 @@ def mask(self, points: AxesPoints[Axis]) -> np.ndarray:  # noqa: D102\n         return mask\n \n \n-def find_regions(obj) -> Iterator[Region]:\n+def find_regions(obj: Any) -> Iterator[Region[Any]]:  # noqa: D102\n     \"\"\"Recursively yield Regions from obj and its children.\"\"\"\n     if (\n         hasattr(obj, \"__pydantic_model__\")\n@@ -379,5 +382,5 @@ def find_regions(obj) -> Iterator[Region]:\n         if isinstance(obj, Region):\n             yield obj\n         for name in obj.__dict__.keys():\n-            regions: Iterator[Region] = find_regions(getattr(obj, name))\n+            regions: Iterator[Region[Any]] = find_regions(getattr(cast(Any, obj), name))\n             yield from regions\ndiff --git a/src/scanspec/service.py b/src/scanspec/service.py\nindex ce628de5..dcf152ed 100644\n--- a/src/scanspec/service.py\n+++ b/src/scanspec/service.py\n@@ -4,8 +4,10 @@\n import json\n from collections.abc import Mapping\n from enum import Enum\n+from typing import Any\n \n import numpy as np\n+import numpy.typing as npt\n from fastapi import Body, FastAPI\n from fastapi.middleware.cors import CORSMiddleware\n from fastapi.openapi.utils import get_openapi\n@@ -31,8 +33,8 @@\n class ValidResponse:\n     \"\"\"Response model for spec validation.\"\"\"\n \n-    input_spec: Spec = Field(description=\"The input scanspec\")\n-    valid_spec: Spec = Field(description=\"The validated version of the spec\")\n+    input_spec: Spec[str] = Field(description=\"The input scanspec\")\n+    valid_spec: Spec[str] = Field(description=\"The validated version of the spec\")\n \n \n class PointsFormat(str, Enum):\n@@ -47,7 +49,7 @@ class PointsFormat(str, Enum):\n class PointsRequest:\n     \"\"\"A request for generated scan points.\"\"\"\n \n-    spec: Spec = Field(description=\"The spec from which to generate points\")\n+    spec: Spec[str] = Field(description=\"The spec from which to generate points\")\n     max_frames: int | None = Field(\n         description=\"The maximum number of points to return, if None will return \"\n         \"as many as calculated\",\n@@ -125,9 +127,9 @@ class SmallestStepResponse:\n \n @app.post(\"/valid\", response_model=ValidResponse)\n def valid(\n-    spec: Spec = Body(..., examples=[_EXAMPLE_SPEC]),\n+    spec: Spec[str] = Body(..., examples=[_EXAMPLE_SPEC]),\n ) -> ValidResponse:\n-    \"\"\"Validate wether a ScanSpec can produce a viable scan.\n+    \"\"\"Validate wether a ScanSpec[str] can produce a viable scan.\n \n     Args:\n         spec: The scanspec to validate\n@@ -200,7 +202,7 @@ def bounds(\n \n @app.post(\"/gap\", response_model=GapResponse)\n def gap(\n-    spec: Spec = Body(\n+    spec: Spec[str] = Body(\n         ...,\n         examples=[_EXAMPLE_SPEC],\n     ),\n@@ -226,7 +228,7 @@ def gap(\n \n @app.post(\"/smalleststep\", response_model=SmallestStepResponse)\n def smallest_step(\n-    spec: Spec = Body(..., examples=[_EXAMPLE_SPEC]),\n+    spec: Spec[str] = Body(..., examples=[_EXAMPLE_SPEC]),\n ) -> SmallestStepResponse:\n     \"\"\"Calculate the smallest step in a scan, both absolutely and per-axis.\n \n@@ -256,7 +258,7 @@ def smallest_step(\n #\n \n \n-def _to_chunk(request: PointsRequest) -> tuple[Frames, int]:\n+def _to_chunk(request: PointsRequest) -> tuple[Frames[str], int]:\n     spec = Spec.deserialize(request.spec)\n     dims = spec.calculate()  # Grab dimensions from spec\n     path = Path(dims)  # Convert to a path\n@@ -303,7 +305,7 @@ def _format_axes_points(\n         raise KeyError(f\"Unknown format: {format}\")\n \n \n-def _reduce_frames(stack: list[Frames[str]], max_frames: int) -> Path:\n+def _reduce_frames(stack: list[Frames[str]], max_frames: int) -> Path[str]:\n     \"\"\"Removes frames from a spec so len(path) < max_frames.\n \n     Args:\n@@ -323,7 +325,7 @@ def _reduce_frames(stack: list[Frames[str]], max_frames: int) -> Path:\n     return Path(sub_frames)\n \n \n-def _sub_sample(frames: Frames[str], ratio: float) -> Frames:\n+def _sub_sample(frames: Frames[str], ratio: float) -> Frames[str]:\n     \"\"\"Provides a sub-sample Frames object whilst preserving its core structure.\n \n     Args:\n@@ -336,7 +338,7 @@ def _sub_sample(frames: Frames[str], ratio: float) -> Frames:\n     return frames.extract(indexes, calculate_gap=False)\n \n \n-def _calc_smallest_step(points: list[np.ndarray]) -> float:\n+def _calc_smallest_step(points: list[npt.NDArray[np.float64]]) -> float:\n     # Calc abs diffs of all axes, ignoring any zero values\n     absolute_diffs = [_abs_diffs(axis_midpoints) for axis_midpoints in points]\n     # Normalize and remove zeros\n@@ -346,7 +348,7 @@ def _calc_smallest_step(points: list[np.ndarray]) -> float:\n     return np.amin(norm_diffs)\n \n \n-def _abs_diffs(array: np.ndarray) -> np.ndarray:\n+def _abs_diffs(array: npt.NDArray[np.number[Any]]) -> npt.NDArray[np.number[Any]]:\n     \"\"\"Calculates the absolute differences between adjacent elements in the array.\n \n     Args:\n@@ -374,7 +376,7 @@ def run_app(cors: bool = False, port: int = 8080) -> None:\n \n     import uvicorn\n \n-    uvicorn.run(app, port=port)\n+    uvicorn.run(app=app, port=port)\n \n \n def scanspec_schema_text() -> str:\ndiff --git a/src/scanspec/specs.py b/src/scanspec/specs.py\nindex adfec1a7..5a7807e9 100644\n--- a/src/scanspec/specs.py\n+++ b/src/scanspec/specs.py\n@@ -8,12 +8,10 @@\n from __future__ import annotations\n \n from collections.abc import Callable, Mapping\n-from typing import (\n-    Any,\n-    Generic,\n-)\n+from typing import Any, Generic, overload\n \n import numpy as np\n+import numpy.typing as npt\n from pydantic import Field, TypeAdapter, validate_call\n from pydantic.dataclasses import dataclass\n \n@@ -21,6 +19,7 @@\n     Axis,\n     Frames,\n     Midpoints,\n+    OtherAxis,\n     Path,\n     SnakedFrames,\n     StrictConfig,\n@@ -72,7 +71,9 @@ def axes(self) -> list[Axis]:  # noqa: D102\n         \"\"\"\n         raise NotImplementedError(self)\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:  # noqa: D102\n         \"\"\"Produce a stack of nested `Frames` that form the scan.\n \n         Ordered from slowest moving to fastest moving.\n@@ -91,34 +92,42 @@ def shape(self) -> tuple[int, ...]:\n         \"\"\"Return the final, simplified shape of the scan.\"\"\"\n         return tuple(len(dim) for dim in self.calculate())\n \n-    def __rmul__(self, other) -> Product[Axis]:\n+    def __rmul__(self, other: int) -> Product[Axis]:\n         return if_instance_do(other, int, lambda o: Product(Repeat(o), self))\n \n-    def __mul__(self, other) -> Product[Axis]:\n+    @overload\n+    def __mul__(self, other: Spec[Axis]) -> Product[Axis]: ...\n+\n+    @overload\n+    def __mul__(self, other: Spec[OtherAxis]) -> Product[Axis | OtherAxis]: ...\n+\n+    def __mul__(\n+        self, other: Spec[Axis] | Spec[OtherAxis]\n+    ) -> Product[Axis] | Product[Axis | OtherAxis]:\n         return if_instance_do(other, Spec, lambda o: Product(self, o))\n \n-    def __and__(self, other) -> Mask[Axis]:\n+    def __and__(self, other: Region[Axis]) -> Mask[Axis]:\n         return if_instance_do(other, Region, lambda o: Mask(self, o))\n \n     def __invert__(self) -> Snake[Axis]:\n         return Snake(self)\n \n-    def zip(self, other: Spec) -> Zip[Axis]:\n+    def zip(self, other: Spec[OtherAxis]) -> Zip[Axis | OtherAxis]:\n         \"\"\"`Zip` the Spec with another, iterating in tandem.\"\"\"\n-        return Zip(self, other)\n+        return Zip(left=self, right=other)\n \n-    def concat(self, other: Spec) -> Concat[Axis]:\n+    def concat(self, other: Spec[Axis]) -> Concat[Axis]:\n         \"\"\"`Concat` the Spec with another, iterating one after the other.\"\"\"\n         return Concat(self, other)\n \n     def serialize(self) -> Mapping[str, Any]:\n         \"\"\"Serialize the Spec to a dictionary.\"\"\"\n-        return TypeAdapter(Spec).dump_python(self)\n+        return TypeAdapter(Spec[Any]).dump_python(self)\n \n     @staticmethod\n-    def deserialize(obj) -> Spec:\n+    def deserialize(obj: Any) -> Spec[Any]:\n         \"\"\"Deserialize a Spec from a dictionary.\"\"\"\n-        return TypeAdapter(Spec).validate_python(obj)\n+        return TypeAdapter(Spec[Any]).validate_python(obj)\n \n \n @dataclass(config=StrictConfig)\n@@ -140,7 +149,9 @@ class Product(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return self.outer.axes() + self.inner.axes()\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         frames_outer = self.outer.calculate(bounds=False, nested=nested)\n         frames_inner = self.inner.calculate(bounds, nested=True)\n         return frames_outer + frames_inner\n@@ -179,7 +190,9 @@ class Repeat(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return []\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         return [Frames({}, gap=np.full(self.num, self.gap))]\n \n \n@@ -216,7 +229,9 @@ class Zip(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return self.left.axes() + self.right.axes()\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         frames_left = self.left.calculate(bounds, nested)\n         frames_right = self.right.calculate(bounds, nested)\n         assert len(frames_left) >= len(\n@@ -241,7 +256,7 @@ def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D\n         padded_right += frames_right  # type: ignore\n \n         # Work through, zipping them together one by one\n-        frames = []\n+        frames: list[Frames[Axis]] = []\n         for left, right in zip(frames_left, padded_right, strict=False):\n             if right is None:\n                 combined = left\n@@ -284,7 +299,9 @@ class Mask(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return self.spec.axes()\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         frames = self.spec.calculate(bounds, nested)\n         for axis_set in self.region.axis_sets():\n             # Find the start and end index of any dimensions containing these axes\n@@ -299,7 +316,7 @@ def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D\n                 squashed = squash_frames(frames[si : ei + 1], check_path_changes)\n                 frames = frames[:si] + [squashed] + frames[ei + 1 :]\n         # Generate masks from the midpoints showing what's inside\n-        masked_frames = []\n+        masked_frames: list[Frames[Axis]] = []\n         for f in frames:\n             indices = get_mask(self.region, f.midpoints).nonzero()[0]\n             masked_frames.append(f.extract(indices))\n@@ -342,7 +359,9 @@ class Snake(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return self.spec.axes()\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         return [\n             SnakedFrames.from_frames(segment)\n             for segment in self.spec.calculate(bounds, nested)\n@@ -385,7 +404,9 @@ def axes(self) -> list[Axis]:  # noqa: D102\n         assert set(left_axes) == set(right_axes), f\"axes {left_axes} != {right_axes}\"\n         return left_axes\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         dim_left = squash_frames(\n             self.left.calculate(bounds, nested), nested and self.check_path_changes\n         )\n@@ -420,15 +441,17 @@ class Squash(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return self.spec.axes()\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         dims = self.spec.calculate(bounds, nested)\n         dim = squash_frames(dims, nested and self.check_path_changes)\n         return [dim]\n \n \n def _dimensions_from_indexes(\n-    func: Callable[[np.ndarray], dict[Axis, np.ndarray]],\n-    axes: list,\n+    func: Callable[[npt.NDArray[np.float64]], dict[Axis, npt.NDArray[np.float64]]],\n+    axes: list[Axis],\n     num: int,\n     bounds: bool,\n ) -> list[Frames[Axis]]:\n@@ -472,7 +495,9 @@ class Line(Spec[Axis]):\n     def axes(self) -> list[Axis]:  # noqa: D102\n         return [self.axis]\n \n-    def _line_from_indexes(self, indexes: np.ndarray) -> dict[Axis, np.ndarray]:\n+    def _line_from_indexes(\n+        self, indexes: npt.NDArray[np.float64]\n+    ) -> dict[Axis, npt.NDArray[np.float64]]:\n         if self.num == 1:\n             # Only one point, stop-start gives length of one point\n             step = self.stop - self.start\n@@ -484,19 +509,21 @@ def _line_from_indexes(self, indexes: np.ndarray) -> dict[Axis, np.ndarray]:\n         first = self.start - step / 2\n         return {self.axis: indexes * step + first}\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         return _dimensions_from_indexes(\n             self._line_from_indexes, self.axes(), self.num, bounds\n         )\n \n     @classmethod\n     def bounded(\n-        cls,\n-        axis: Axis = Field(description=\"An identifier for what to move\"),\n+        cls: type[Line[Any]],\n+        axis: OtherAxis = Field(description=\"An identifier for what to move\"),\n         lower: float = Field(description=\"Lower bound of the first point of the line\"),\n         upper: float = Field(description=\"Upper bound of the last point of the line\"),\n         num: int = Field(ge=1, description=\"Number of frames to produce\"),\n-    ) -> Line[Axis]:\n+    ) -> Line[OtherAxis]:\n         \"\"\"Specify a Line by extreme bounds instead of midpoints.\n \n         .. example_spec::\n@@ -541,7 +568,7 @@ class Static(Spec[Axis]):\n \n     @classmethod\n     def duration(\n-        cls: type[Static],\n+        cls: type[Static[Any]],\n         duration: float = Field(description=\"The duration of each static point\"),\n         num: int = Field(ge=1, description=\"Number of frames to produce\", default=1),\n     ) -> Static[str]:\n@@ -553,15 +580,19 @@ def duration(\n \n             spec = Line(\"y\", 1, 2, 3).zip(Static.duration(0.1))\n         \"\"\"\n-        return cls(DURATION, duration, num)\n+        return Static(DURATION, duration, num)\n \n     def axes(self) -> list[Axis]:  # noqa: D102\n         return [self.axis]\n \n-    def _repeats_from_indexes(self, indexes: np.ndarray) -> dict[Axis, np.ndarray]:\n+    def _repeats_from_indexes(\n+        self, indexes: npt.NDArray[np.float64]\n+    ) -> dict[Axis, npt.NDArray[np.float64]]:\n         return {self.axis: np.full(len(indexes), self.value)}\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         return _dimensions_from_indexes(\n             self._repeats_from_indexes, self.axes(), self.num, bounds\n         )\n@@ -601,7 +632,9 @@ def axes(self) -> list[Axis]:  # noqa: D102\n         # TODO: reversed from __init__ args, a good idea?\n         return [self.y_axis, self.x_axis]\n \n-    def _spiral_from_indexes(self, indexes: np.ndarray) -> dict[Axis, np.ndarray]:\n+    def _spiral_from_indexes(\n+        self, indexes: npt.NDArray[np.float64]\n+    ) -> dict[Axis, npt.NDArray[np.float64]]:\n         # simplest spiral equation: r = phi\n         # we want point spacing across area to be the same as between rings\n         # so: sqrt(area / num) = ring_spacing\n@@ -618,16 +651,18 @@ def _spiral_from_indexes(self, indexes: np.ndarray) -> dict[Axis, np.ndarray]:\n             self.x_axis: self.x_start + x_scale * phi * np.sin(phi + self.rotate),\n         }\n \n-    def calculate(self, bounds=True, nested=False) -> list[Frames[Axis]]:  # noqa: D102\n+    def calculate(  # noqa: D102\n+        self, bounds: bool = True, nested: bool = False\n+    ) -> list[Frames[Axis]]:\n         return _dimensions_from_indexes(\n             self._spiral_from_indexes, self.axes(), self.num, bounds\n         )\n \n     @classmethod\n     def spaced(\n-        cls,\n-        x_axis: Axis = Field(description=\"An identifier for what to move for x\"),\n-        y_axis: Axis = Field(description=\"An identifier for what to move for y\"),\n+        cls: type[Spiral[Any]],\n+        x_axis: OtherAxis = Field(description=\"An identifier for what to move for x\"),\n+        y_axis: OtherAxis = Field(description=\"An identifier for what to move for y\"),\n         x_start: float = Field(description=\"x centre of the spiral\"),\n         y_start: float = Field(description=\"y centre of the spiral\"),\n         radius: float = Field(description=\"radius of the spiral\"),\n@@ -635,7 +670,7 @@ def spaced(\n         rotate: float = Field(\n             description=\"How much to rotate the angle of the spiral\", default=0.0\n         ),\n-    ) -> Spiral[Axis]:\n+    ) -> Spiral[OtherAxis]:\n         \"\"\"Specify a Spiral equally spaced in \"x_axis\" and \"y_axis\".\n \n         .. example_spec::\n@@ -651,14 +686,21 @@ def spaced(\n         n_rings = radius / dr\n         num = int(n_rings**2 * np.pi)\n         return cls(\n-            x_axis, y_axis, x_start, y_start, radius * 2, radius * 2, num, rotate\n+            x_axis,\n+            y_axis,\n+            x_start,\n+            y_start,\n+            radius * 2,\n+            radius * 2,\n+            num,\n+            rotate,\n         )\n \n \n Spiral.spaced = validate_call(Spiral.spaced)  # type:ignore\n \n \n-def fly(spec: Spec[Axis], duration: float) -> Spec[Axis]:\n+def fly(spec: Spec[Axis], duration: float) -> Spec[Axis | str]:\n     \"\"\"Flyscan, zipping with fixed duration for every frame.\n \n     Args:\n@@ -675,7 +717,7 @@ def fly(spec: Spec[Axis], duration: float) -> Spec[Axis]:\n     return spec.zip(Static.duration(duration))\n \n \n-def step(spec: Spec[Axis], duration: float, num: int = 1) -> Spec[Axis]:\n+def step(spec: Spec[Axis], duration: float, num: int = 1) -> Spec[Axis | str]:\n     \"\"\"Step scan, with num frames of given duration at each frame in the spec.\n \n     Args:\n@@ -694,7 +736,7 @@ def step(spec: Spec[Axis], duration: float, num: int = 1) -> Spec[Axis]:\n     return spec * Static.duration(duration, num)\n \n \n-def get_constant_duration(frames: list[Frames]) -> float | None:\n+def get_constant_duration(frames: list[Frames[Any]]) -> float | None:\n     \"\"\"Returns the duration of a number of ScanSpec frames, if known and consistent.\n \n     Args:\ndiff --git a/src/scanspec/sphinxext.py b/src/scanspec/sphinxext.py\nindex 69500422..9362a983 100644\n--- a/src/scanspec/sphinxext.py\n+++ b/src/scanspec/sphinxext.py\n@@ -1,9 +1,11 @@\n \"\"\"An example_spec directive.\"\"\"\n \n from contextlib import contextmanager\n+from typing import Any, cast\n \n from docutils.statemachine import StringList\n from matplotlib.sphinxext import plot_directive\n+from sphinx.application import Sphinx\n \n from . import __version__\n \n@@ -15,9 +17,18 @@ def always_create_figures():\n     This is needed even if source rst hasn't changed, as we often use\n     example_spec from within docstrings\n     \"\"\"\n-    orig_f = plot_directive.out_of_date\n+\n+    def always_true(\n+        original: Any,\n+        derived: Any,\n+        includes: Any = None,\n+    ) -> bool:\n+        return True\n+\n+    # Type ignored because we never manipulate this object\n+    orig_f = plot_directive.out_of_date  # type: ignore\n     # Patch the plot directive so it thinks all sources are out of date\n-    plot_directive.out_of_date = lambda *args, **kwargs: True\n+    plot_directive.out_of_date = always_true\n     try:\n         yield\n     finally:\n@@ -27,7 +38,7 @@ def always_create_figures():\n class ExampleSpecDirective(plot_directive.PlotDirective):\n     \"\"\"Runs `plot_spec` on the ``spec`` definied in the content.\"\"\"\n \n-    def run(self):\n+    def run(self) -> Any:\n         \"\"\"Run the directive.\"\"\"\n         self.content = StringList(\n             [\"# Example Spec\", \"\", \"from scanspec.plot import plot_spec\"]\n@@ -35,10 +46,10 @@ def run(self):\n             + [\"plot_spec(spec)\"]\n         )\n         with always_create_figures():\n-            return super().run()\n+            return cast(Any, super().run())\n \n \n-def setup(app):\n+def setup(app: Sphinx):\n     \"\"\"Setup this extension in sphinx.\"\"\"\n     app.add_directive(\"example_spec\", ExampleSpecDirective)\n \n", "instance_id": "bluesky__scanspec-143", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear, with a defined goal of enabling strict mode for Pyright in the scanspec repository as a prototype for modern Python correctness tooling. It provides acceptance criteria, such as enabling strict mode per a specific guideline and ensuring no errors with `tox -e type-checking`. References to external documentation are included, which helps in understanding the task. However, there are minor ambiguities: the problem does not explicitly detail the expected impact on the codebase beyond configuration changes, nor does it specify potential challenges or edge cases (e.g., existing type issues that might surface with strict mode). Additionally, it lacks explicit mention of how strict mode might interact with the existing code or dependencies, which could be critical for a complete understanding. Overall, it is valid and clear but misses some minor contextual details.", "difficulty_explanation": "The difficulty of this task falls in the easy range (0.2-0.4) due to the nature of the changes required and the level of understanding needed. The code changes primarily involve configuration updates in `pyproject.toml` to switch Pyright's `typeCheckingMode` from \"standard\" to \"strict\", alongside minor adjustments in other files like `docs/conf.py` (adding a nitpick ignore), `schema.json` (formatting changes), and various source files (adding type hints to resolve strict mode errors). The scope of changes spans multiple files but is relatively straightforward, focusing on type annotations and configuration rather than complex logic or architectural modifications. It requires a basic understanding of Python type checking with Pyright, familiarity with the project's structure, and the ability to address type errors by adding annotations, which is not overly complex for a developer with moderate experience. However, it edges slightly higher within the easy range due to the need to understand and resolve type issues across a non-trivial codebase, potentially involving some trial and error to ensure `tox -e type-checking` passes. There are no significant edge cases or performance considerations mentioned, and the impact on the system's architecture is minimal. Thus, a score of 0.35 reflects the task's simplicity with a slight increase for the breadth of minor code adjustments needed.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "make conda installs in CI stricter\n## Description\r\n\r\nContributes to https://github.com/rapidsai/build-planning/issues/106\r\n\r\nProposes specifying the RAPIDS version in `conda install` calls in CI that install CI artifacts, to reduce the risk of CI jobs picking up artifacts from other releases.\r\n\r\n## Checklist\r\n- [x] I am familiar with the [Contributing Guidelines](https://github.com/rapidsai/rmm/blob/HEAD/CONTRIBUTING.md).\r\n- [x] New or existing tests cover these changes.\r\n- [x] The documentation is up to date with these changes.\r\n\nmake conda installs in CI stricter\n## Description\r\n\r\nContributes to https://github.com/rapidsai/build-planning/issues/106\r\n\r\nProposes specifying the RAPIDS version in `conda install` calls in CI that install CI artifacts, to reduce the risk of CI jobs picking up artifacts from other releases.\r\n\r\n## Checklist\r\n- [x] I am familiar with the [Contributing Guidelines](https://github.com/rapidsai/rmm/blob/HEAD/CONTRIBUTING.md).\r\n- [x] New or existing tests cover these changes.\r\n- [x] The documentation is up to date with these changes.\r\n\n", "patch": "diff --git a/ci/build_docs.sh b/ci/build_docs.sh\nindex fadaf0f27..844dae1c6 100755\n--- a/ci/build_docs.sh\n+++ b/ci/build_docs.sh\n@@ -6,8 +6,8 @@ set -euo pipefail\n rapids-logger \"Create test conda environment\"\n . /opt/conda/etc/profile.d/conda.sh\n \n-RAPIDS_VERSION_MAJOR_MINOR=\"$(rapids-version-major-minor)\"\n-export RAPIDS_VERSION_NUMBER=\"$RAPIDS_VERSION_MAJOR_MINOR\"\n+RAPIDS_VERSION=\"$(rapids-version)\"\n+export RAPIDS_VERSION_MAJOR_MINOR=\"$(rapids-version-major-minor)\"\n \n rapids-dependency-file-generator \\\n   --output conda \\\n@@ -26,8 +26,8 @@ PYTHON_CHANNEL=$(rapids-download-conda-from-s3 python)\n rapids-mamba-retry install \\\n   --channel \"${CPP_CHANNEL}\" \\\n   --channel \"${PYTHON_CHANNEL}\" \\\n-  \"rmm=${RAPIDS_VERSION_MAJOR_MINOR}\" \\\n-  \"librmm=${RAPIDS_VERSION_MAJOR_MINOR}\"\n+  \"rmm=${RAPIDS_VERSION}\" \\\n+  \"librmm=${RAPIDS_VERSION}\"\n \n export RAPIDS_DOCS_DIR=\"$(mktemp -d)\"\n \n@@ -45,4 +45,4 @@ mkdir -p \"${RAPIDS_DOCS_DIR}/rmm/html\"\n mv _build/dirhtml/* \"${RAPIDS_DOCS_DIR}/rmm/html\"\n popd\n \n-rapids-upload-docs\n+RAPIDS_VERSION_NUMBER=\"${RAPIDS_VERSION_MAJOR_MINOR}\" rapids-upload-docs\n", "instance_id": "rapidsai__rmm-1703", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to make conda installs in CI stricter by specifying the RAPIDS version in `conda install` calls to avoid picking up artifacts from other releases. It references a related issue for context, which is helpful. However, there are minor ambiguities and missing details. For instance, it does not explicitly explain why using the full RAPIDS version (instead of just major.minor) is necessary or how this change impacts the CI process in terms of compatibility or potential risks. Additionally, there are no examples or detailed scenarios provided to illustrate the problem (e.g., what happens when artifacts from other releases are picked up). Edge cases or potential failure modes are also not mentioned. Despite these minor gaps, the overall goal and scope of the change are understandable, especially when combined with the provided code changes.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The code changes are confined to a single script (`ci/build_docs.sh`) and involve straightforward modifications, such as updating environment variable names and adjusting the version string used in `conda install` commands. The scope of the change is minimal, with no impact on the broader system architecture or interactions between multiple modules. The technical concepts required are basic: familiarity with shell scripting, environment variables, and conda package management. There is no indication of complex algorithms, design patterns, or domain-specific knowledge beyond understanding CI workflows and version specifications. Edge cases and error handling are not explicitly mentioned in the problem statement or evident in the code changes, suggesting that the modification does not introduce significant complexity in this regard. Overall, this task requires minimal effort and a basic understanding of the CI script's logic to implement and verify the change.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Can't get help for install if no device detected\n```\r\njepler@bert:~$ circup --version\r\nCircUp, A CircuitPython module updater. Version 1.5.3\r\njepler@bert:~$ circup install --help\r\nCould not find a connected CircuitPython device.\r\n```\r\n\r\nInstead of giving an error I'd like it to show the help.\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex 4c1aa22..ac2069c 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -19,6 +19,7 @@ downloads/\n eggs/\n .eggs/\n lib/\n+!tests/mock_device/lib/\n lib64/\n parts/\n sdist/\n@@ -93,6 +94,7 @@ venv/\n ENV/\n env.bak/\n venv.bak/\n+*_venv/\n \n # Spyder project settings\n .spyderproject\ndiff --git a/circup/__init__.py b/circup/__init__.py\nindex 8fb76e6..41016b4 100644\n--- a/circup/__init__.py\n+++ b/circup/__init__.py\n@@ -5,1578 +5,20 @@\n CircUp -- a utility to manage and update libraries on a CircuitPython device.\n \"\"\"\n \n-import ctypes\n-import glob\n-import json\n-import logging\n-import time\n-from logging.handlers import RotatingFileHandler\n-import os\n-import re\n-import shutil\n-import socket\n-import sys\n-import tempfile\n-import zipfile\n-from subprocess import check_output\n-from urllib.parse import urlparse, urljoin\n-\n-import appdirs\n-import click\n-import findimports\n-import pkg_resources\n-import requests\n-import toml\n-import update_checker\n-from requests.auth import HTTPBasicAuth\n-from semver import VersionInfo\n \n from circup.shared import DATA_DIR, BAD_FILE_FORMAT, extract_metadata, _get_modules_file\n from circup.backends import WebBackend, DiskBackend\n+from circup.logging import logger\n \n-#: The version of CircuitPython found on the connected device.\n-CPY_VERSION = \"\"\n \n # Useful constants.\n-#: Flag to indicate if the command is being run in verbose mode.\n-VERBOSE = False\n-\n-#: The path to the JSON file containing the metadata about the bundles.\n-BUNDLE_CONFIG_FILE = pkg_resources.resource_filename(\n-    \"circup\", \"config/bundle_config.json\"\n-)\n-#: Overwrite the bundles list with this file (only done manually)\n-BUNDLE_CONFIG_OVERWRITE = os.path.join(DATA_DIR, \"bundle_config.json\")\n-#: The path to the JSON file containing the local list of bundles.\n-BUNDLE_CONFIG_LOCAL = os.path.join(DATA_DIR, \"bundle_config_local.json\")\n-#: The path to the JSON file containing the metadata about the bundles.\n-BUNDLE_DATA = os.path.join(DATA_DIR, \"circup.json\")\n-#: The directory containing the utility's log file.\n-LOG_DIR = appdirs.user_log_dir(appname=\"circup\", appauthor=\"adafruit\")\n-#: The location of the log file for the utility.\n-LOGFILE = os.path.join(LOG_DIR, \"circup.log\")\n-\n-#:  The libraries (and blank lines) which don't go on devices\n-NOT_MCU_LIBRARIES = [\n-    \"\",\n-    \"adafruit-blinka\",\n-    \"adafruit-blinka-bleio\",\n-    \"adafruit-blinka-displayio\",\n-    \"adafruit-circuitpython-typing\",\n-    \"circuitpython_typing\",\n-    \"pyserial\",\n-]\n-\n-#: Module formats list (and the other form used in github files)\n-PLATFORMS = {\"py\": \"py\", \"8mpy\": \"8.x-mpy\", \"9mpy\": \"9.x-mpy\"}\n-#: Commands that do not require an attached board\n-BOARDLESS_COMMANDS = [\"show\", \"bundle-add\", \"bundle-remove\", \"bundle-show\"]\n-\n-#: Timeout for requests calls like get()\n-REQUESTS_TIMEOUT = 30\n-\n-# Ensure DATA_DIR / LOG_DIR related directories and files exist.\n-if not os.path.exists(DATA_DIR):  # pragma: no cover\n-    os.makedirs(DATA_DIR)\n-if not os.path.exists(LOG_DIR):  # pragma: no cover\n-    os.makedirs(LOG_DIR)\n-\n-\n-# Setup logging.\n-logger = logging.getLogger(__name__)\n-logger.setLevel(logging.INFO)\n-logfile_handler = RotatingFileHandler(LOGFILE, maxBytes=10_000_000, backupCount=0)\n-log_formatter = logging.Formatter(\n-    \"%(asctime)s %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\"\n-)\n-logfile_handler.setFormatter(log_formatter)\n-logger.addHandler(logfile_handler)\n \n \n __version__ = \"0.0.0-auto.0\"\n __repo__ = \"https://github.com/adafruit/circup.git\"\n \n \n-class Bundle:\n-    \"\"\"\n-    All the links and file names for a bundle\n-    \"\"\"\n-\n-    def __init__(self, repo):\n-        \"\"\"\n-        Initialise a Bundle created from its github info.\n-        Construct all the strings in one place.\n-\n-        :param str repo: Repository string for github: \"user/repository\"\n-        \"\"\"\n-        vendor, bundle_id = repo.split(\"/\")\n-        bundle_id = bundle_id.lower().replace(\"_\", \"-\")\n-        self.key = repo\n-        #\n-        self.url = \"https://github.com/\" + repo\n-        self.basename = bundle_id + \"-{platform}-{tag}\"\n-        self.urlzip = self.basename + \".zip\"\n-        self.dir = os.path.join(DATA_DIR, vendor, bundle_id + \"-{platform}\")\n-        self.zip = os.path.join(DATA_DIR, bundle_id + \"-{platform}.zip\")\n-        self.url_format = self.url + \"/releases/download/{tag}/\" + self.urlzip\n-        # tag\n-        self._current = None\n-        self._latest = None\n-\n-    def lib_dir(self, platform):\n-        \"\"\"\n-        This bundle's lib directory for the platform.\n-\n-        :param str platform: The platform identifier (py/6mpy/...).\n-        :return: The path to the lib directory for the platform.\n-        \"\"\"\n-        tag = self.current_tag\n-        return os.path.join(\n-            self.dir.format(platform=platform),\n-            self.basename.format(platform=PLATFORMS[platform], tag=tag),\n-            \"lib\",\n-        )\n-\n-    def requirements_for(self, library_name, toml_file=False):\n-        \"\"\"\n-        The requirements file for this library.\n-\n-        :param str library_name: The name of the library.\n-        :return: The path to the requirements.txt file.\n-        \"\"\"\n-        platform = \"py\"\n-        tag = self.current_tag\n-        found_file = os.path.join(\n-            self.dir.format(platform=platform),\n-            self.basename.format(platform=PLATFORMS[platform], tag=tag),\n-            \"requirements\",\n-            library_name,\n-            \"requirements.txt\" if not toml_file else \"pyproject.toml\",\n-        )\n-        if os.path.isfile(found_file):\n-            with open(found_file, \"r\", encoding=\"utf-8\") as read_this:\n-                return read_this.read()\n-        return None\n-\n-    @property\n-    def current_tag(self):\n-        \"\"\"\n-        Lazy load current cached tag from the BUNDLE_DATA json file.\n-\n-        :return: The current cached tag value for the project.\n-        \"\"\"\n-        if self._current is None:\n-            self._current = tags_data_load().get(self.key, \"0\")\n-        return self._current\n-\n-    @current_tag.setter\n-    def current_tag(self, tag):\n-        \"\"\"\n-        Set the current cached tag (after updating).\n-\n-        :param str tag: The new value for the current tag.\n-        :return: The current cached tag value for the project.\n-        \"\"\"\n-        self._current = tag\n-\n-    @property\n-    def latest_tag(self):\n-        \"\"\"\n-        Lazy find the value of the latest tag for the bundle.\n-\n-        :return: The most recent tag value for the project.\n-        \"\"\"\n-        if self._latest is None:\n-            self._latest = get_latest_release_from_url(self.url + \"/releases/latest\")\n-        return self._latest\n-\n-    def validate(self):\n-        \"\"\"\n-        Test the existence of the expected URLs (not their content)\n-        \"\"\"\n-        tag = self.latest_tag\n-        if not tag or tag == \"releases\":\n-            if VERBOSE:\n-                click.secho(f'  Invalid tag \"{tag}\"', fg=\"red\")\n-            return False\n-        for platform in PLATFORMS.values():\n-            url = self.url_format.format(platform=platform, tag=tag)\n-            r = requests.get(url, stream=True, timeout=REQUESTS_TIMEOUT)\n-            # pylint: disable=no-member\n-            if r.status_code != requests.codes.ok:\n-                if VERBOSE:\n-                    click.secho(f\"  Unable to find {os.path.split(url)[1]}\", fg=\"red\")\n-                return False\n-            # pylint: enable=no-member\n-        return True\n-\n-    def __repr__(self):\n-        \"\"\"\n-        Helps with log files.\n-\n-        :return: A repr of a dictionary containing the Bundles's metadata.\n-        \"\"\"\n-        return repr(\n-            {\n-                \"key\": self.key,\n-                \"url\": self.url,\n-                \"urlzip\": self.urlzip,\n-                \"dir\": self.dir,\n-                \"zip\": self.zip,\n-                \"url_format\": self.url_format,\n-                \"current\": self._current,\n-                \"latest\": self._latest,\n-            }\n-        )\n-\n-\n-class Module:\n-    \"\"\"\n-    Represents a CircuitPython module.\n-    \"\"\"\n-\n-    # pylint: disable=too-many-arguments\n-\n-    def __init__(\n-        self,\n-        name,\n-        backend,\n-        repo,\n-        device_version,\n-        bundle_version,\n-        mpy,\n-        bundle,\n-        compatibility,\n-    ):\n-        \"\"\"\n-        The ``self.file`` and ``self.name`` attributes are constructed from\n-        the ``path`` value. If the path is to a directory based module, the\n-        resulting self.file value will be None, and the name will be the\n-        basename of the directory path.\n-\n-        :param str name: The file name of the module.\n-        :param Backend backend: The backend that the module is on.\n-        :param str repo: The URL of the Git repository for this module.\n-        :param str device_version: The semver value for the version on device.\n-        :param str bundle_version: The semver value for the version in bundle.\n-        :param bool mpy: Flag to indicate if the module is byte-code compiled.\n-        :param Bundle bundle: Bundle object where the module is located.\n-        :param (str,str) compatibility: Min and max versions of CP compatible with the mpy.\n-        \"\"\"\n-        self.name = name\n-        self.backend = backend\n-        self.path = (\n-            urljoin(backend.library_path, name, allow_fragments=False)\n-            if isinstance(backend, WebBackend)\n-            else os.path.join(backend.library_path, name)\n-        )\n-\n-        url = urlparse(self.path, allow_fragments=False)\n-\n-        if (\n-            url.path.endswith(\"/\")\n-            if isinstance(backend, WebBackend)\n-            else self.path.endswith(os.sep)\n-        ):\n-            self.file = None\n-            self.name = self.path.split(\n-                \"/\" if isinstance(backend, WebBackend) else os.sep\n-            )[-2]\n-        else:\n-            self.file = os.path.basename(url.path)\n-            self.name = (\n-                os.path.basename(url.path).replace(\".py\", \"\").replace(\".mpy\", \"\")\n-            )\n-\n-        self.repo = repo\n-        self.device_version = device_version\n-        self.bundle_version = bundle_version\n-        self.mpy = mpy\n-        self.min_version = compatibility[0]\n-        self.max_version = compatibility[1]\n-        # Figure out the bundle path.\n-        self.bundle_path = None\n-        if self.mpy:\n-            # Byte compiled, now check CircuitPython version.\n-            major_version = CPY_VERSION.split(\".\")[0]\n-            bundle_platform = \"{}mpy\".format(major_version)\n-        else:\n-            # Regular Python\n-            bundle_platform = \"py\"\n-        # module path in the bundle\n-        search_path = bundle.lib_dir(bundle_platform)\n-        if self.file:\n-            self.bundle_path = os.path.join(search_path, self.file)\n-        else:\n-            self.bundle_path = os.path.join(search_path, self.name)\n-        logger.info(self)\n-\n-    # pylint: enable=too-many-arguments\n-\n-    @property\n-    def outofdate(self):\n-        \"\"\"\n-        Returns a boolean to indicate if this module is out of date.\n-        Treat mismatched MPY versions as out of date.\n-\n-        :return: Truthy indication if the module is out of date.\n-        \"\"\"\n-        if self.mpy_mismatch:\n-            return True\n-        if self.device_version and self.bundle_version:\n-            try:\n-                return VersionInfo.parse(self.device_version) < VersionInfo.parse(\n-                    self.bundle_version\n-                )\n-            except ValueError as ex:\n-                logger.warning(\"Module '%s' has incorrect semver value.\", self.name)\n-                logger.warning(ex)\n-        return True  # Assume out of date to try to update.\n-\n-    @property\n-    def bad_format(self):\n-        \"\"\"A boolean indicating that the mpy file format could not be identified\"\"\"\n-        return self.mpy and self.device_version == BAD_FILE_FORMAT\n-\n-    @property\n-    def mpy_mismatch(self):\n-        \"\"\"\n-        Returns a boolean to indicate if this module's MPY version is compatible\n-        with the board's current version of Circuitpython. A min or max version\n-        that evals to False means no limit.\n-\n-        :return: Boolean indicating if the MPY versions don't match.\n-        \"\"\"\n-        if not self.mpy:\n-            return False\n-        try:\n-            cpv = VersionInfo.parse(CPY_VERSION)\n-        except ValueError as ex:\n-            logger.warning(\"CircuitPython has incorrect semver value.\")\n-            logger.warning(ex)\n-        try:\n-            if self.min_version and cpv < VersionInfo.parse(self.min_version):\n-                return True  # CP version too old\n-            if self.max_version and cpv >= VersionInfo.parse(self.max_version):\n-                return True  # MPY version too old\n-        except (TypeError, ValueError) as ex:\n-            logger.warning(\n-                \"Module '%s' has incorrect MPY compatibility information.\", self.name\n-            )\n-            logger.warning(ex)\n-        return False\n-\n-    @property\n-    def major_update(self):\n-        \"\"\"\n-        Returns a boolean to indicate if this is a major version update.\n-\n-        :return: Boolean indicating if this is a major version upgrade\n-        \"\"\"\n-        try:\n-            if (\n-                VersionInfo.parse(self.device_version).major\n-                == VersionInfo.parse(self.bundle_version).major\n-            ):\n-                return False\n-        except (TypeError, ValueError) as ex:\n-            logger.warning(\"Module '%s' has incorrect semver value.\", self.name)\n-            logger.warning(ex)\n-        return True  # Assume Major Version udpate.\n-\n-    @property\n-    def row(self):\n-        \"\"\"\n-        Returns a tuple of items to display in a table row to show the module's\n-        name, local version and remote version, and reason to update.\n-\n-        :return: A tuple containing the module's name, version on the connected\n-                 device, version in the latest bundle and reason to update.\n-        \"\"\"\n-        loc = self.device_version if self.device_version else \"unknown\"\n-        rem = self.bundle_version if self.bundle_version else \"unknown\"\n-        if self.mpy_mismatch:\n-            update_reason = \"MPY Format\"\n-        elif self.major_update:\n-            update_reason = \"Major Version\"\n-        else:\n-            update_reason = \"Minor Version\"\n-        return (self.name, loc, rem, update_reason)\n-\n-    def __repr__(self):\n-        \"\"\"\n-        Helps with log files.\n-\n-        :return: A repr of a dictionary containing the module's metadata.\n-        \"\"\"\n-        return repr(\n-            {\n-                \"path\": self.path,\n-                \"file\": self.file,\n-                \"name\": self.name,\n-                \"repo\": self.repo,\n-                \"device_version\": self.device_version,\n-                \"bundle_version\": self.bundle_version,\n-                \"bundle_path\": self.bundle_path,\n-                \"mpy\": self.mpy,\n-                \"min_version\": self.min_version,\n-                \"max_version\": self.max_version,\n-            }\n-        )\n-\n-\n-def clean_library_name(assumed_library_name):\n-    \"\"\"\n-    Most CP repos and library names are look like this:\n-\n-        repo: Adafruit_CircuitPython_LC709203F\n-        library: adafruit_lc709203f\n-\n-    But some do not and this handles cleaning that up.\n-    Also cleans up if the pypi or reponame is passed in instead of the\n-    CP library name.\n-\n-    :param str assumed_library_name: An assumed name of a library from user\n-        or requirements.txt entry\n-    :return: str proper library name\n-    \"\"\"\n-    not_standard_names = {\n-        # Assumed Name : Actual Name\n-        \"adafruit_adafruitio\": \"adafruit_io\",\n-        \"adafruit_asyncio\": \"asyncio\",\n-        \"adafruit_busdevice\": \"adafruit_bus_device\",\n-        \"adafruit_connectionmanager\": \"adafruit_connection_manager\",\n-        \"adafruit_display_button\": \"adafruit_button\",\n-        \"adafruit_neopixel\": \"neopixel\",\n-        \"adafruit_sd\": \"adafruit_sdcard\",\n-        \"adafruit_simpleio\": \"simpleio\",\n-        \"pimoroni_ltr559\": \"pimoroni_circuitpython_ltr559\",\n-    }\n-    if \"circuitpython\" in assumed_library_name:\n-        # convert repo or pypi name to common library name\n-        assumed_library_name = (\n-            assumed_library_name.replace(\"-circuitpython-\", \"_\")\n-            .replace(\"_circuitpython_\", \"_\")\n-            .replace(\"-\", \"_\")\n-        )\n-    if assumed_library_name in not_standard_names:\n-        return not_standard_names[assumed_library_name]\n-    return assumed_library_name\n-\n-\n-def completion_for_install(ctx, param, incomplete):\n-    \"\"\"\n-    Returns the list of available modules for the command line tab-completion\n-    with the ``circup install`` command.\n-    \"\"\"\n-    # pylint: disable=unused-argument\n-    available_modules = get_bundle_versions(get_bundles_list(), avoid_download=True)\n-    module_names = {m.replace(\".py\", \"\") for m in available_modules}\n-    if incomplete:\n-        module_names = [name for name in module_names if name.startswith(incomplete)]\n-    return sorted(module_names)\n-\n-\n-def ensure_latest_bundle(bundle):\n-    \"\"\"\n-    Ensure that there's a copy of the latest library bundle available so circup\n-    can check the metadata contained therein.\n-\n-    :param Bundle bundle: the target Bundle object.\n-    \"\"\"\n-    logger.info(\"Checking library updates for %s.\", bundle.key)\n-    tag = bundle.latest_tag\n-    do_update = False\n-    if tag == bundle.current_tag:\n-        for platform in PLATFORMS:\n-            # missing directories (new platform added on an existing install\n-            # or side effect of pytest or network errors)\n-            do_update = do_update or not os.path.isdir(bundle.lib_dir(platform))\n-    else:\n-        do_update = True\n-\n-    if do_update:\n-        logger.info(\"New version available (%s).\", tag)\n-        try:\n-            get_bundle(bundle, tag)\n-            tags_data_save_tag(bundle.key, tag)\n-        except requests.exceptions.HTTPError as ex:\n-            # See #20 for reason for this\n-            click.secho(\n-                (\n-                    \"There was a problem downloading that platform bundle. \"\n-                    \"Skipping and using existing download if available.\"\n-                ),\n-                fg=\"red\",\n-            )\n-            logger.exception(ex)\n-    else:\n-        logger.info(\"Current bundle up to date %s.\", tag)\n-\n-\n-def find_device():\n-    \"\"\"\n-    Return the location on the filesystem for the connected CircuitPython device.\n-    This is based upon how Mu discovers this information.\n-\n-    :return: The path to the device on the local filesystem.\n-    \"\"\"\n-    device_dir = None\n-    # Attempt to find the path on the filesystem that represents the plugged in\n-    # CIRCUITPY board.\n-    if os.name == \"posix\":\n-        # Linux / OSX\n-        for mount_command in [\"mount\", \"/sbin/mount\"]:\n-            try:\n-                mount_output = check_output(mount_command).splitlines()\n-                mounted_volumes = [x.split()[2] for x in mount_output]\n-                for volume in mounted_volumes:\n-                    if volume.endswith(b\"CIRCUITPY\"):\n-                        device_dir = volume.decode(\"utf-8\")\n-            except FileNotFoundError:\n-                continue\n-    elif os.name == \"nt\":\n-        # Windows\n-\n-        def get_volume_name(disk_name):\n-            \"\"\"\n-            Each disk or external device connected to windows has an attribute\n-            called \"volume name\". This function returns the volume name for the\n-            given disk/device.\n-\n-            Based upon answer given here: http://stackoverflow.com/a/12056414\n-            \"\"\"\n-            vol_name_buf = ctypes.create_unicode_buffer(1024)\n-            ctypes.windll.kernel32.GetVolumeInformationW(\n-                ctypes.c_wchar_p(disk_name),\n-                vol_name_buf,\n-                ctypes.sizeof(vol_name_buf),\n-                None,\n-                None,\n-                None,\n-                None,\n-                0,\n-            )\n-            return vol_name_buf.value\n-\n-        #\n-        # In certain circumstances, volumes are allocated to USB\n-        # storage devices which cause a Windows popup to raise if their\n-        # volume contains no media. Wrapping the check in SetErrorMode\n-        # with SEM_FAILCRITICALERRORS (1) prevents this popup.\n-        #\n-        old_mode = ctypes.windll.kernel32.SetErrorMode(1)\n-        try:\n-            for disk in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n-                path = \"{}:\\\\\".format(disk)\n-                if os.path.exists(path) and get_volume_name(path) == \"CIRCUITPY\":\n-                    device_dir = path\n-                    # Report only the FIRST device found.\n-                    break\n-        finally:\n-            ctypes.windll.kernel32.SetErrorMode(old_mode)\n-    else:\n-        # No support for unknown operating systems.\n-        raise NotImplementedError('OS \"{}\" not supported.'.format(os.name))\n-    logger.info(\"Found device: %s\", device_dir)\n-    return device_dir\n-\n-\n-def find_modules(backend, bundles_list):\n-    \"\"\"\n-    Extracts metadata from the connected device and available bundles and\n-    returns this as a list of Module instances representing the modules on the\n-    device.\n-\n-    :param Backend backend: Backend with the device connection.\n-    :param List[Bundle] bundles_list: List of supported bundles as Bundle objects.\n-    :return: A list of Module instances describing the current state of the\n-             modules on the connected device.\n-    \"\"\"\n-    # pylint: disable=broad-except,too-many-locals\n-    try:\n-        device_modules = backend.get_device_versions()\n-        bundle_modules = get_bundle_versions(bundles_list)\n-        result = []\n-        for key, device_metadata in device_modules.items():\n-\n-            if key in bundle_modules:\n-                path = device_metadata[\"path\"]\n-                bundle_metadata = bundle_modules[key]\n-                repo = bundle_metadata.get(\"__repo__\")\n-                bundle = bundle_metadata.get(\"bundle\")\n-                device_version = device_metadata.get(\"__version__\")\n-                bundle_version = bundle_metadata.get(\"__version__\")\n-                mpy = device_metadata[\"mpy\"]\n-                compatibility = device_metadata.get(\"compatibility\", (None, None))\n-                module_name = (\n-                    path.split(os.sep)[-1]\n-                    if not path.endswith(os.sep)\n-                    else path[:-1].split(os.sep)[-1] + os.sep\n-                )\n-\n-                m = Module(\n-                    module_name,\n-                    backend,\n-                    repo,\n-                    device_version,\n-                    bundle_version,\n-                    mpy,\n-                    bundle,\n-                    compatibility,\n-                )\n-                result.append(m)\n-        return result\n-    except Exception as ex:\n-        # If it's not possible to get the device and bundle metadata, bail out\n-        # with a friendly message and indication of what's gone wrong.\n-        logger.exception(ex)\n-        click.echo(\"There was a problem: {}\".format(ex))\n-        sys.exit(1)\n-    # pylint: enable=broad-except,too-many-locals\n-\n-\n-def get_bundle(bundle, tag):\n-    \"\"\"\n-    Downloads and extracts the version of the bundle with the referenced tag.\n-    The resulting zip file is saved on the local filesystem.\n-\n-    :param Bundle bundle: the target Bundle object.\n-    :param str tag: The GIT tag to use to download the bundle.\n-    \"\"\"\n-    click.echo(f\"Downloading latest bundles for {bundle.key} ({tag}).\")\n-    for platform, github_string in PLATFORMS.items():\n-        # Report the platform: \"8.x-mpy\", etc.\n-        click.echo(f\"{github_string}:\")\n-        url = bundle.url_format.format(platform=github_string, tag=tag)\n-        logger.info(\"Downloading bundle: %s\", url)\n-        r = requests.get(url, stream=True, timeout=REQUESTS_TIMEOUT)\n-        # pylint: disable=no-member\n-        if r.status_code != requests.codes.ok:\n-            logger.warning(\"Unable to connect to %s\", url)\n-            r.raise_for_status()\n-        # pylint: enable=no-member\n-        total_size = int(r.headers.get(\"Content-Length\"))\n-        temp_zip = bundle.zip.format(platform=platform)\n-        with click.progressbar(\n-            r.iter_content(1024), label=\"Extracting:\", length=total_size\n-        ) as pbar, open(temp_zip, \"wb\") as zip_fp:\n-            for chunk in pbar:\n-                zip_fp.write(chunk)\n-                pbar.update(len(chunk))\n-        logger.info(\"Saved to %s\", temp_zip)\n-        temp_dir = bundle.dir.format(platform=platform)\n-        if os.path.isdir(temp_dir):\n-            shutil.rmtree(temp_dir)\n-        with zipfile.ZipFile(temp_zip, \"r\") as zfile:\n-            zfile.extractall(temp_dir)\n-    bundle.current_tag = tag\n-    click.echo(\"\\nOK\\n\")\n-\n-\n-def get_bundle_versions(bundles_list, avoid_download=False):\n-    \"\"\"\n-    Returns a dictionary of metadata from modules in the latest known release\n-    of the library bundle. Uses the Python version (rather than the compiled\n-    version) of the library modules.\n-\n-    :param List[Bundle] bundles_list: List of supported bundles as Bundle objects.\n-    :param bool avoid_download: if True, download the bundle only if missing.\n-    :return: A dictionary of metadata about the modules available in the\n-             library bundle.\n-    \"\"\"\n-    all_the_modules = dict()\n-    for bundle in bundles_list:\n-        if not avoid_download or not os.path.isdir(bundle.lib_dir(\"py\")):\n-            ensure_latest_bundle(bundle)\n-        path = bundle.lib_dir(\"py\")\n-        path_modules = _get_modules_file(path, logger)\n-        for name, module in path_modules.items():\n-            module[\"bundle\"] = bundle\n-            if name not in all_the_modules:  # here we decide the order of priority\n-                all_the_modules[name] = module\n-    return all_the_modules\n-\n-\n-def get_bundles_dict():\n-    \"\"\"\n-    Retrieve the dictionary from BUNDLE_CONFIG_FILE (JSON).\n-    Put the local dictionary in front, so it gets priority.\n-    It's a dictionary of bundle string identifiers.\n-\n-    :return: Combined dictionaries from the config files.\n-    \"\"\"\n-    bundle_dict = get_bundles_local_dict()\n-    try:\n-        with open(BUNDLE_CONFIG_OVERWRITE, \"rb\") as bundle_config_json:\n-            bundle_config = json.load(bundle_config_json)\n-    except (FileNotFoundError, json.decoder.JSONDecodeError):\n-        with open(BUNDLE_CONFIG_FILE, \"rb\") as bundle_config_json:\n-            bundle_config = json.load(bundle_config_json)\n-    for name, bundle in bundle_config.items():\n-        if bundle not in bundle_dict.values():\n-            bundle_dict[name] = bundle\n-    return bundle_dict\n-\n-\n-def get_bundles_local_dict():\n-    \"\"\"\n-    Retrieve the local bundles from BUNDLE_CONFIG_LOCAL (JSON).\n-\n-    :return: Raw dictionary from the config file(s).\n-    \"\"\"\n-    try:\n-        with open(BUNDLE_CONFIG_LOCAL, \"rb\") as bundle_config_json:\n-            bundle_config = json.load(bundle_config_json)\n-        if not isinstance(bundle_config, dict) or not bundle_config:\n-            logger.error(\"Local bundle list invalid. Skipped.\")\n-            raise FileNotFoundError(\"Bad local bundle list\")\n-        return bundle_config\n-    except (FileNotFoundError, json.decoder.JSONDecodeError):\n-        return dict()\n-\n-\n-def get_bundles_list():\n-    \"\"\"\n-    Retrieve the list of bundles from the config dictionary.\n-\n-    :return: List of supported bundles as Bundle objects.\n-    \"\"\"\n-    bundle_config = get_bundles_dict()\n-    bundles_list = [Bundle(bundle_config[b]) for b in bundle_config]\n-    logger.info(\"Using bundles: %s\", \", \".join(b.key for b in bundles_list))\n-    return bundles_list\n-\n-\n-def get_circup_version():\n-    \"\"\"Return the version of circup that is running. If not available, return None.\n-\n-    :return: Current version of circup, or None.\n-    \"\"\"\n-    try:\n-        from importlib import metadata  # pylint: disable=import-outside-toplevel\n-    except ImportError:\n-        try:\n-            import importlib_metadata as metadata  # pylint: disable=import-outside-toplevel\n-        except ImportError:\n-            return None\n-    try:\n-        return metadata.version(\"circup\")\n-    except metadata.PackageNotFoundError:\n-        return None\n-\n-\n-def get_dependencies(*requested_libraries, mod_names, to_install=()):\n-    \"\"\"\n-    Return a list of other CircuitPython libraries required by the given list\n-    of libraries\n-\n-    :param tuple requested_libraries: The libraries to search for dependencies\n-    :param object mod_names:  All the modules metadata from bundle\n-    :param list(str) to_install: Modules already selected for installation.\n-    :return: tuple of module names to install which we build\n-    \"\"\"\n-    # Internal variables\n-    _to_install = to_install\n-    _requested_libraries = []\n-    _rl = requested_libraries[0]\n-\n-    if not requested_libraries[0]:\n-        # If nothing is requested, we're done\n-        return _to_install\n-\n-    for lib_name in _rl:\n-        lower_lib_name = lib_name.lower()\n-        if lower_lib_name in NOT_MCU_LIBRARIES:\n-            logger.info(\n-                \"Skipping %s. It is not for microcontroller installs.\", lib_name\n-            )\n-        else:\n-            # Canonicalize, with some exceptions:\n-            # adafruit-circuitpython-something => adafruit_something\n-            canonical_lib_name = clean_library_name(lower_lib_name)\n-            try:\n-                # Don't process any names we can't find in mod_names\n-                mod_names[canonical_lib_name]  # pylint: disable=pointless-statement\n-                _requested_libraries.append(canonical_lib_name)\n-            except KeyError:\n-                click.secho(\n-                    f\"WARNING:\\n\\t{canonical_lib_name} is not a known CircuitPython library.\",\n-                    fg=\"yellow\",\n-                )\n-\n-    if not _requested_libraries:\n-        # If nothing is requested, we're done\n-        return _to_install\n-\n-    for library in list(_requested_libraries):\n-        if library not in _to_install:\n-            _to_install = _to_install + (library,)\n-            # get the requirements.txt from bundle\n-            bundle = mod_names[library][\"bundle\"]\n-            requirements_txt = bundle.requirements_for(library)\n-            if requirements_txt:\n-                _requested_libraries.extend(\n-                    libraries_from_requirements(requirements_txt)\n-                )\n-\n-            circup_dependencies = get_circup_dependencies(bundle, library)\n-            for circup_dependency in circup_dependencies:\n-                _requested_libraries.append(circup_dependency)\n-\n-        # we've processed this library, remove it from the list\n-        _requested_libraries.remove(library)\n-\n-        return get_dependencies(\n-            tuple(_requested_libraries), mod_names=mod_names, to_install=_to_install\n-        )\n-\n-\n-def get_circup_dependencies(bundle, library):\n-    \"\"\"\n-    Get the list of circup dependencies from pyproject.toml\n-    e.g.\n-    [circup]\n-    circup_dependencies = [\"dependency_name_here\"]\n-\n-    :param bundle: The Bundle to look within\n-    :param library: The Library to find pyproject.toml for and get dependencies from\n-\n-    :return: The list of dependency libraries that were found\n-    \"\"\"\n-    try:\n-        pyproj_toml = bundle.requirements_for(library, toml_file=True)\n-        if pyproj_toml:\n-            pyproj_toml_data = toml.loads(pyproj_toml)\n-            dependencies = pyproj_toml_data[\"circup\"][\"circup_dependencies\"]\n-            if isinstance(dependencies, list):\n-                return dependencies\n-\n-            if isinstance(dependencies, str):\n-                return (dependencies,)\n-\n-        return tuple()\n-\n-    except KeyError:\n-        # no circup_dependencies in pyproject.toml\n-        return tuple()\n-\n-\n-def get_latest_release_from_url(url):\n-    \"\"\"\n-    Find the tag name of the latest release by using HTTP HEAD and decoding the redirect.\n-\n-    :param str url: URL to the latest release page on a git repository.\n-    :return: The most recent tag value for the release.\n-    \"\"\"\n-\n-    logger.info(\"Requesting redirect information: %s\", url)\n-    response = requests.head(url, timeout=REQUESTS_TIMEOUT)\n-    responseurl = response.url\n-    if response.is_redirect:\n-        responseurl = response.headers[\"Location\"]\n-    tag = responseurl.rsplit(\"/\", 1)[-1]\n-    logger.info(\"Tag: '%s'\", tag)\n-    return tag\n-\n-\n-def libraries_from_requirements(requirements):\n-    \"\"\"\n-    Clean up supplied requirements.txt and turn into tuple of CP libraries\n-\n-    :param str requirements: A string version of a requirements.txt\n-    :return: tuple of library names\n-    \"\"\"\n-    libraries = ()\n-    for line in requirements.split(\"\\n\"):\n-        line = line.lower().strip()\n-        if line.startswith(\"#\") or line == \"\":\n-            # skip comments\n-            pass\n-        else:\n-            # Remove everything after any pip style version specifiers\n-            line = re.split(\"[<>=~[;]\", line)[0].strip()\n-            libraries = libraries + (line,)\n-    return libraries\n-\n-\n-def save_local_bundles(bundles_data):\n-    \"\"\"\n-    Save the list of local bundles to the settings.\n-\n-    :param str key: The bundle's identifier/key.\n-    \"\"\"\n-    if len(bundles_data) > 0:\n-        with open(BUNDLE_CONFIG_LOCAL, \"w\", encoding=\"utf-8\") as data:\n-            json.dump(bundles_data, data)\n-    else:\n-        if os.path.isfile(BUNDLE_CONFIG_LOCAL):\n-            os.unlink(BUNDLE_CONFIG_LOCAL)\n-\n-\n-def tags_data_load():\n-    \"\"\"\n-    Load the list of the version tags of the bundles on disk.\n-\n-    :return: a dict() of tags indexed by Bundle identifiers/keys.\n-    \"\"\"\n-    tags_data = None\n-    try:\n-        with open(BUNDLE_DATA, encoding=\"utf-8\") as data:\n-            try:\n-                tags_data = json.load(data)\n-            except json.decoder.JSONDecodeError as ex:\n-                # Sometimes (why?) the JSON file becomes corrupt. In which case\n-                # log it and carry on as if setting up for first time.\n-                logger.error(\"Could not parse %s\", BUNDLE_DATA)\n-                logger.exception(ex)\n-    except FileNotFoundError:\n-        pass\n-    if not isinstance(tags_data, dict):\n-        tags_data = {}\n-    return tags_data\n-\n-\n-def tags_data_save_tag(key, tag):\n-    \"\"\"\n-    Add or change the saved tag value for a bundle.\n-\n-    :param str key: The bundle's identifier/key.\n-    :param str tag: The new tag for the bundle.\n-    \"\"\"\n-    tags_data = tags_data_load()\n-    tags_data[key] = tag\n-    with open(BUNDLE_DATA, \"w\", encoding=\"utf-8\") as data:\n-        json.dump(tags_data, data)\n-\n-\n-def libraries_from_code_py(code_py, mod_names):\n-    \"\"\"\n-    Parse the given code.py file and return the imported libraries\n-\n-    :param str code_py: Full path of the code.py file\n-    :return: sequence of library names\n-    \"\"\"\n-    # pylint: disable=broad-except\n-    try:\n-        found_imports = findimports.find_imports(code_py)\n-    except Exception as ex:  # broad exception because anything could go wrong\n-        logger.exception(ex)\n-        click.secho('Unable to read the auto file: \"{}\"'.format(str(ex)), fg=\"red\")\n-        sys.exit(2)\n-    # pylint: enable=broad-except\n-    imports = [info.name.split(\".\", 1)[0] for info in found_imports]\n-    return [r for r in imports if r in mod_names]\n-\n-\n-# ----------- CLI command definitions  ----------- #\n-\n-# The following functions have IO side effects (for instance they emit to\n-# stdout). Ergo, these are not checked with unit tests. Most of the\n-# functionality they provide is provided by the functions above, which *are*\n-# tested. Most of the logic of the following functions is to prepare things for\n-# presentation to / interaction with the user.\n-\n-\n-@click.group()\n-@click.option(\n-    \"--verbose\", is_flag=True, help=\"Comprehensive logging is sent to stdout.\"\n-)\n-@click.option(\n-    \"--path\",\n-    type=click.Path(exists=True, file_okay=False),\n-    help=\"Path to CircuitPython directory. Overrides automatic path detection.\",\n-)\n-@click.option(\n-    \"--host\",\n-    help=\"Hostname or IP address of a device. Overrides automatic path detection.\",\n-)\n-@click.option(\n-    \"--password\", help=\"Password to use for authentication when --host is used.\"\n-)\n-@click.option(\n-    \"--timeout\",\n-    default=30,\n-    help=\"Specify the timeout in seconds for any network operations.\",\n-)\n-@click.option(\n-    \"--board-id\",\n-    default=None,\n-    help=\"Manual Board ID of the CircuitPython device. If provided in combination \"\n-    \"with --cpy-version, it overrides the detected board ID.\",\n-)\n-@click.option(\n-    \"--cpy-version\",\n-    default=None,\n-    help=\"Manual CircuitPython version. If provided in combination \"\n-    \"with --board-id, it overrides the detected CPy version.\",\n-)\n-@click.version_option(\n-    prog_name=\"CircUp\",\n-    message=\"%(prog)s, A CircuitPython module updater. Version %(version)s\",\n-)\n-@click.pass_context\n-def main(  # pylint: disable=too-many-locals\n-    ctx, verbose, path, host, password, timeout, board_id, cpy_version\n-):  # pragma: no cover\n-    \"\"\"\n-    A tool to manage and update libraries on a CircuitPython device.\n-    \"\"\"\n-    # pylint: disable=too-many-arguments,too-many-branches,too-many-statements,too-many-locals\n-    ctx.ensure_object(dict)\n-    global REQUESTS_TIMEOUT\n-    ctx.obj[\"TIMEOUT\"] = REQUESTS_TIMEOUT = timeout\n-    device_path = get_device_path(host, password, path)\n-\n-    using_webworkflow = \"host\" in ctx.params.keys() and ctx.params[\"host\"] is not None\n-\n-    if using_webworkflow:\n-        if host == \"circuitpython.local\":\n-            click.echo(\"Checking versions.json on circuitpython.local to find hostname\")\n-            versions_resp = requests.get(\n-                \"http://circuitpython.local/cp/version.json\", timeout=timeout\n-            )\n-            host = f'{versions_resp.json()[\"hostname\"]}.local'\n-            click.echo(f\"Using hostname: {host}\")\n-            device_path = device_path.replace(\"circuitpython.local\", host)\n-        try:\n-            ctx.obj[\"backend\"] = WebBackend(\n-                host=host, password=password, logger=logger, timeout=timeout\n-            )\n-        except ValueError as e:\n-            click.secho(e, fg=\"red\")\n-            time.sleep(0.3)\n-            sys.exit(1)\n-        except RuntimeError as e:\n-            click.secho(e, fg=\"red\")\n-            sys.exit(1)\n-    else:\n-        try:\n-            ctx.obj[\"backend\"] = DiskBackend(device_path, logger)\n-        except ValueError as e:\n-            print(e)\n-\n-    if verbose:\n-        # Configure additional logging to stdout.\n-        global VERBOSE\n-        VERBOSE = True\n-        verbose_handler = logging.StreamHandler(sys.stdout)\n-        verbose_handler.setLevel(logging.INFO)\n-        verbose_handler.setFormatter(log_formatter)\n-        logger.addHandler(verbose_handler)\n-        click.echo(\"Logging to {}\\n\".format(LOGFILE))\n-    logger.info(\"### Started Circup ###\")\n-\n-    # If a newer version of circup is available, print a message.\n-    logger.info(\"Checking for a newer version of circup\")\n-    version = get_circup_version()\n-    if version:\n-        update_checker.update_check(\"circup\", version)\n-\n-    # stop early if the command is boardless\n-    if ctx.invoked_subcommand in BOARDLESS_COMMANDS:\n-        return\n-\n-    ctx.obj[\"DEVICE_PATH\"] = device_path\n-    latest_version = get_latest_release_from_url(\n-        \"https://github.com/adafruit/circuitpython/releases/latest\"\n-    )\n-    global CPY_VERSION\n-    if device_path is None or not ctx.obj[\"backend\"].is_device_present():\n-        click.secho(\"Could not find a connected CircuitPython device.\", fg=\"red\")\n-        sys.exit(1)\n-    else:\n-        CPY_VERSION, board_id = (\n-            ctx.obj[\"backend\"].get_circuitpython_version()\n-            if board_id is None or cpy_version is None\n-            else (cpy_version, board_id)\n-        )\n-        click.echo(\n-            \"Found device at {}, running CircuitPython {}.\".format(\n-                device_path, CPY_VERSION\n-            )\n-        )\n-    try:\n-        if VersionInfo.parse(CPY_VERSION) < VersionInfo.parse(latest_version):\n-            click.secho(\n-                \"A newer version of CircuitPython ({}) is available.\".format(\n-                    latest_version\n-                ),\n-                fg=\"green\",\n-            )\n-            if board_id:\n-                url_download = f\"https://circuitpython.org/board/{board_id}\"\n-            else:\n-                url_download = \"https://circuitpython.org/downloads\"\n-            click.secho(\"Get it here: {}\".format(url_download), fg=\"green\")\n-    except ValueError as ex:\n-        logger.warning(\"CircuitPython has incorrect semver value.\")\n-        logger.warning(ex)\n-\n-\n-def get_device_path(host, password, path):\n-    \"\"\"\n-    :param host Hostname or IP address.\n-    :param password REST API password.\n-    :param path File system path.\n-    :return device URL or None if the device cannot be found.\n-    \"\"\"\n-    if path:\n-        device_path = path\n-    elif host:\n-        # pylint: enable=no-member\n-        device_path = f\"http://:{password}@\" + host\n-    else:\n-        device_path = find_device()\n-    return device_path\n-\n-\n-@main.command()\n-@click.option(\"-r\", \"--requirement\", is_flag=True)\n-@click.pass_context\n-def freeze(ctx, requirement):  # pragma: no cover\n-    \"\"\"\n-    Output details of all the modules found on the connected CIRCUITPYTHON\n-    device. Option -r saves output to requirements.txt file\n-    \"\"\"\n-    logger.info(\"Freeze\")\n-    modules = find_modules(ctx.obj[\"backend\"], get_bundles_list())\n-    if modules:\n-        output = []\n-        for module in modules:\n-            output.append(\"{}=={}\".format(module.name, module.device_version))\n-        for module in output:\n-            click.echo(module)\n-            logger.info(module)\n-        if requirement:\n-            cwd = os.path.abspath(os.getcwd())\n-            for i, module in enumerate(output):\n-                output[i] += \"\\n\"\n-            with open(\n-                cwd + \"/\" + \"requirements.txt\", \"w\", newline=\"\\n\", encoding=\"utf-8\"\n-            ) as file:\n-                file.truncate(0)\n-                file.writelines(output)\n-    else:\n-        click.echo(\"No modules found on the device.\")\n-\n-\n-@main.command(\"list\")\n-@click.pass_context\n-def list_cli(ctx):  # pragma: no cover\n-    \"\"\"\n-    Lists all out of date modules found on the connected CIRCUITPYTHON device.\n-    \"\"\"\n-    logger.info(\"List\")\n-    # Grab out of date modules.\n-    data = [(\"Module\", \"Version\", \"Latest\", \"Update Reason\")]\n-\n-    modules = [\n-        m.row\n-        for m in find_modules(ctx.obj[\"backend\"], get_bundles_list())\n-        if m.outofdate\n-    ]\n-    if modules:\n-        data += modules\n-        # Nice tabular display.\n-        col_width = [0, 0, 0, 0]\n-        for row in data:\n-            for i, word in enumerate(row):\n-                col_width[i] = max(len(word) + 2, col_width[i])\n-        dashes = tuple((\"-\" * (width - 1) for width in col_width))\n-        data.insert(1, dashes)\n-        click.echo(\n-            \"The following modules are out of date or probably need an update.\\n\"\n-            \"Major Updates may include breaking changes. Review before updating.\\n\"\n-            \"MPY Format changes from Circuitpython 6 to 7 require an update.\\n\"\n-        )\n-        for row in data:\n-            output = \"\"\n-            for index, cell in enumerate(row):\n-                output += cell.ljust(col_width[index])\n-            if not VERBOSE:\n-                click.echo(output)\n-            logger.info(output)\n-    else:\n-        click.echo(\"All modules found on the device are up to date.\")\n-\n-\n-# pylint: disable=too-many-arguments,too-many-locals\n-@main.command()\n-@click.argument(\n-    \"modules\", required=False, nargs=-1, shell_complete=completion_for_install\n-)\n-@click.option(\n-    \"pyext\",\n-    \"--py\",\n-    is_flag=True,\n-    help=\"Install the .py version of the module(s) instead of the mpy version.\",\n-)\n-@click.option(\n-    \"-r\",\n-    \"--requirement\",\n-    type=click.Path(exists=True, dir_okay=False),\n-    help=\"specify a text file to install all modules listed in the text file.\"\n-    \" Typically requirements.txt.\",\n-)\n-@click.option(\n-    \"--auto\", \"-a\", is_flag=True, help=\"Install the modules imported in code.py.\"\n-)\n-@click.option(\n-    \"--auto-file\",\n-    default=None,\n-    help=\"Specify the name of a file on the board to read for auto install.\"\n-    \" Also accepts an absolute path or a local ./ path.\",\n-)\n-@click.pass_context\n-def install(ctx, modules, pyext, requirement, auto, auto_file):  # pragma: no cover\n-    \"\"\"\n-    Install a named module(s) onto the device. Multiple modules\n-    can be installed at once by providing more than one module name, each\n-    separated by a space.\n-    \"\"\"\n-\n-    # TODO: Ensure there's enough space on the device\n-    available_modules = get_bundle_versions(get_bundles_list())\n-    mod_names = {}\n-    for module, metadata in available_modules.items():\n-        mod_names[module.replace(\".py\", \"\").lower()] = metadata\n-    if requirement:\n-        with open(requirement, \"r\", encoding=\"utf-8\") as rfile:\n-            requirements_txt = rfile.read()\n-        requested_installs = libraries_from_requirements(requirements_txt)\n-    elif auto or auto_file:\n-        if auto_file is None:\n-            auto_file = \"code.py\"\n-            print(f\"Auto file: {auto_file}\")\n-        # pass a local file with \"./\" or \"../\"\n-        is_relative = not isinstance(ctx.obj[\"backend\"], WebBackend) or auto_file.split(\n-            os.sep\n-        )[0] in [os.path.curdir, os.path.pardir]\n-        if not os.path.isabs(auto_file) and not is_relative:\n-            auto_file = ctx.obj[\"backend\"].get_file_path(auto_file or \"code.py\")\n-\n-        auto_file_path = ctx.obj[\"backend\"].get_auto_file_path(auto_file)\n-        print(f\"Auto file path: {auto_file_path}\")\n-        if not os.path.isfile(auto_file_path):\n-            # fell through to here when run from random folder on windows - ask backend.\n-            new_auto_file = ctx.obj[\"backend\"].get_file_path(auto_file)\n-            if os.path.isfile(new_auto_file):\n-                auto_file = new_auto_file\n-                auto_file_path = ctx.obj[\"backend\"].get_auto_file_path(auto_file)\n-                print(f\"Auto file path: {auto_file_path}\")\n-            else:\n-                click.secho(f\"Auto file not found: {auto_file}\", fg=\"red\")\n-                sys.exit(1)\n-\n-        requested_installs = libraries_from_code_py(auto_file_path, mod_names)\n-    else:\n-        requested_installs = modules\n-    requested_installs = sorted(set(requested_installs))\n-    click.echo(f\"Searching for dependencies for: {requested_installs}\")\n-    to_install = get_dependencies(requested_installs, mod_names=mod_names)\n-    device_modules = ctx.obj[\"backend\"].get_device_versions()\n-    if to_install is not None:\n-        to_install = sorted(to_install)\n-        click.echo(f\"Ready to install: {to_install}\\n\")\n-        for library in to_install:\n-            ctx.obj[\"backend\"].install_module(\n-                ctx.obj[\"DEVICE_PATH\"], device_modules, library, pyext, mod_names\n-            )\n-\n-\n-# pylint: enable=too-many-arguments,too-many-locals\n-\n-\n-@main.command()\n-@click.argument(\"match\", required=False, nargs=1)\n-def show(match):  # pragma: no cover\n-    \"\"\"\n-    Show a list of available modules in the bundle. These are modules which\n-    *could* be installed on the device.\n-\n-    If MATCH is specified only matching modules will be listed.\n-    \"\"\"\n-    available_modules = get_bundle_versions(get_bundles_list())\n-    module_names = sorted([m.replace(\".py\", \"\") for m in available_modules])\n-    if match is not None:\n-        match = match.lower()\n-        module_names = [m for m in module_names if match in m]\n-    click.echo(\"\\n\".join(module_names))\n-\n-    click.echo(\n-        \"{} shown of {} packages.\".format(len(module_names), len(available_modules))\n-    )\n-\n-\n-@main.command()\n-@click.argument(\"module\", nargs=-1)\n-@click.pass_context\n-def uninstall(ctx, module):  # pragma: no cover\n-    \"\"\"\n-    Uninstall a named module(s) from the connected device. Multiple modules\n-    can be uninstalled at once by providing more than one module name, each\n-    separated by a space.\n-    \"\"\"\n-    device_path = ctx.obj[\"DEVICE_PATH\"]\n-    print(f\"Uninstalling {module} from {device_path}\")\n-    for name in module:\n-        device_modules = ctx.obj[\"backend\"].get_device_versions()\n-        name = name.lower()\n-        mod_names = {}\n-        for module_item, metadata in device_modules.items():\n-            mod_names[module_item.replace(\".py\", \"\").lower()] = metadata\n-        if name in mod_names:\n-            metadata = mod_names[name]\n-            module_path = metadata[\"path\"]\n-            ctx.obj[\"backend\"].uninstall(device_path, module_path)\n-            click.echo(\"Uninstalled '{}'.\".format(name))\n-        else:\n-            click.echo(\"Module '{}' not found on device.\".format(name))\n-        continue\n-\n-\n-# pylint: disable=too-many-branches\n-\n-\n-@main.command(\n-    short_help=(\n-        \"Update modules on the device. \"\n-        \"Use --all to automatically update all modules without Major Version warnings.\"\n-    )\n-)\n-@click.option(\n-    \"update_all\",\n-    \"--all\",\n-    is_flag=True,\n-    help=\"Update all modules without Major Version warnings.\",\n-)\n-@click.pass_context\n-# pylint: disable=too-many-locals\n-def update(ctx, update_all):  # pragma: no cover\n-    \"\"\"\n-    Checks for out-of-date modules on the connected CIRCUITPYTHON device, and\n-    prompts the user to confirm updating such modules.\n-    \"\"\"\n-    logger.info(\"Update\")\n-    # Grab current modules.\n-    bundles_list = get_bundles_list()\n-    installed_modules = find_modules(ctx.obj[\"backend\"], bundles_list)\n-    modules_to_update = [m for m in installed_modules if m.outofdate]\n-\n-    if not modules_to_update:\n-        click.echo(\"None of the module[s] found on the device need an update.\")\n-        return\n-\n-    # Process out of date modules\n-    updated_modules = []\n-    click.echo(\"Found {} module[s] needing update.\".format(len(modules_to_update)))\n-    if not update_all:\n-        click.echo(\"Please indicate which module[s] you wish to update:\\n\")\n-    for module in modules_to_update:\n-        update_flag = update_all\n-        if VERBOSE:\n-            click.echo(\n-                \"Device version: {}, Bundle version: {}\".format(\n-                    module.device_version, module.bundle_version\n-                )\n-            )\n-        if isinstance(module.bundle_version, str) and not VersionInfo.is_valid(\n-            module.bundle_version\n-        ):\n-            click.secho(\n-                f\"WARNING: Library {module.name} repo has incorrect __version__\"\n-                \"\\n\\tmetadata. Circup will assume it needs updating.\"\n-                \"\\n\\tPlease file an issue in the library repo.\",\n-                fg=\"yellow\",\n-            )\n-            if module.repo:\n-                click.secho(f\"\\t{module.repo}\", fg=\"yellow\")\n-        if not update_flag:\n-            if module.bad_format:\n-                click.secho(\n-                    f\"WARNING: '{module.name}': module corrupted or in an\"\n-                    \" unknown mpy format. Updating is required.\",\n-                    fg=\"yellow\",\n-                )\n-                update_flag = click.confirm(\"Do you want to update?\")\n-            elif module.mpy_mismatch:\n-                click.secho(\n-                    f\"WARNING: '{module.name}': mpy format doesn't match the\"\n-                    \" device's Circuitpython version. Updating is required.\",\n-                    fg=\"yellow\",\n-                )\n-                update_flag = click.confirm(\"Do you want to update?\")\n-            elif module.major_update:\n-                update_flag = click.confirm(\n-                    (\n-                        \"'{}' is a Major Version update and may contain breaking \"\n-                        \"changes. Do you want to update?\".format(module.name)\n-                    )\n-                )\n-            else:\n-                update_flag = click.confirm(\"Update '{}'?\".format(module.name))\n-        if update_flag:\n-            # pylint: disable=broad-except\n-            try:\n-                ctx.obj[\"backend\"].update(module)\n-                updated_modules.append(module.name)\n-                click.echo(\"Updated {}\".format(module.name))\n-            except Exception as ex:\n-                logger.exception(ex)\n-                click.echo(\"Something went wrong, {} (check the logs)\".format(str(ex)))\n-            # pylint: enable=broad-except\n-\n-    if not updated_modules:\n-        return\n-\n-    # We updated modules, look to see if any requirements are missing\n-    click.echo(\n-        \"Checking {} updated module[s] for missing requirements.\".format(\n-            len(updated_modules)\n-        )\n-    )\n-    available_modules = get_bundle_versions(bundles_list)\n-    mod_names = {}\n-    for module, metadata in available_modules.items():\n-        mod_names[module.replace(\".py\", \"\").lower()] = metadata\n-    missing_modules = get_dependencies(updated_modules, mod_names=mod_names)\n-    device_modules = ctx.obj[\"backend\"].get_device_versions()\n-    # Process newly needed modules\n-    if missing_modules is not None:\n-        installed_module_names = [m.name for m in installed_modules]\n-        missing_modules = set(missing_modules) - set(installed_module_names)\n-        missing_modules = sorted(list(missing_modules))\n-        click.echo(f\"Ready to install: {missing_modules}\\n\")\n-        for library in missing_modules:\n-            ctx.obj[\"backend\"].install_module(\n-                ctx.obj[\"DEVICE_PATH\"], device_modules, library, False, mod_names\n-            )\n-\n-\n-# pylint: enable=too-many-branches\n-\n-\n-@main.command(\"bundle-show\")\n-@click.option(\"--modules\", is_flag=True, help=\"List all the modules per bundle.\")\n-def bundle_show(modules):\n-    \"\"\"\n-    Show the list of bundles, default and local, with URL, current version\n-    and latest version retrieved from the web.\n-    \"\"\"\n-    local_bundles = get_bundles_local_dict().values()\n-    bundles = get_bundles_list()\n-    available_modules = get_bundle_versions(bundles)\n-\n-    for bundle in bundles:\n-        if bundle.key in local_bundles:\n-            click.secho(bundle.key, fg=\"yellow\")\n-        else:\n-            click.secho(bundle.key, fg=\"green\")\n-        click.echo(\"    \" + bundle.url)\n-        click.echo(\"    version = \" + bundle.current_tag)\n-        if modules:\n-            click.echo(\"Modules:\")\n-            for name, mod in sorted(available_modules.items()):\n-                if mod[\"bundle\"] == bundle:\n-                    click.echo(f\"   {name} ({mod.get('__version__', '-')})\")\n-\n-\n-@main.command(\"bundle-add\")\n-@click.argument(\"bundle\", nargs=-1)\n-def bundle_add(bundle):\n-    \"\"\"\n-    Add bundles to the local bundles list, by \"user/repo\" github string.\n-    A series of tests to validate that the bundle exists and at least looks\n-    like a bundle are done before validating it. There might still be errors\n-    when the bundle is downloaded for the first time.\n-    \"\"\"\n-    bundles_dict = get_bundles_local_dict()\n-    modified = False\n-    for bundle_repo in bundle:\n-        # cleanup in case seombody pastes the URL to the repo/releases\n-        bundle_repo = re.sub(\n-            r\"https?://github.com/([^/]+/[^/]+)(/.*)?\", r\"\\1\", bundle_repo\n-        )\n-        if bundle_repo in bundles_dict.values():\n-            click.secho(\"Bundle already in list.\", fg=\"yellow\")\n-            click.secho(\"    \" + bundle_repo, fg=\"yellow\")\n-            continue\n-        try:\n-            bundle_added = Bundle(bundle_repo)\n-        except ValueError:\n-            click.secho(\n-                \"Bundle string invalid, expecting github URL or `user/repository` string.\",\n-                fg=\"red\",\n-            )\n-            click.secho(\"    \" + bundle_repo, fg=\"red\")\n-            continue\n-        result = requests.get(\n-            \"https://github.com/\" + bundle_repo, timeout=REQUESTS_TIMEOUT\n-        )\n-        # pylint: disable=no-member\n-        if result.status_code == requests.codes.NOT_FOUND:\n-            click.secho(\"Bundle invalid, the repository doesn't exist (404).\", fg=\"red\")\n-            click.secho(\"    \" + bundle_repo, fg=\"red\")\n-            continue\n-        # pylint: enable=no-member\n-        if not bundle_added.validate():\n-            click.secho(\n-                \"Bundle invalid, is the repository a valid circup bundle ?\", fg=\"red\"\n-            )\n-            click.secho(\"    \" + bundle_repo, fg=\"red\")\n-            continue\n-        # note: use bun as the dictionary key for uniqueness\n-        bundles_dict[bundle_repo] = bundle_repo\n-        modified = True\n-        click.echo(\"Added \" + bundle_repo)\n-        click.echo(\"    \" + bundle_added.url)\n-    if modified:\n-        # save the bundles list\n-        save_local_bundles(bundles_dict)\n-        # update and get the new bundles for the first time\n-        get_bundle_versions(get_bundles_list())\n-\n-\n-@main.command(\"bundle-remove\")\n-@click.argument(\"bundle\", nargs=-1)\n-@click.option(\"--reset\", is_flag=True, help=\"Remove all local bundles.\")\n-def bundle_remove(bundle, reset):\n-    \"\"\"\n-    Remove one or more bundles from the local bundles list.\n-    \"\"\"\n-    if reset:\n-        save_local_bundles({})\n-        return\n-    bundle_config = list(get_bundles_dict().values())\n-    bundles_local_dict = get_bundles_local_dict()\n-    modified = False\n-    for bun in bundle:\n-        # cleanup in case seombody pastes the URL to the repo/releases\n-        bun = re.sub(r\"https?://github.com/([^/]+/[^/]+)(/.*)?\", r\"\\1\", bun)\n-        found = False\n-        for name, repo in list(bundles_local_dict.items()):\n-            if bun in (name, repo):\n-                found = True\n-                click.secho(f\"Bundle {repo}\")\n-                do_it = click.confirm(\"Do you want to remove that bundle ?\")\n-                if do_it:\n-                    click.secho(\"Removing the bundle from the local list\", fg=\"yellow\")\n-                    click.secho(f\"    {bun}\", fg=\"yellow\")\n-                    modified = True\n-                    del bundles_local_dict[name]\n-        if not found:\n-            if bun in bundle_config:\n-                click.secho(\"Cannot remove built-in module:\" \"\\n    \" + bun, fg=\"red\")\n-            else:\n-                click.secho(\n-                    \"Bundle not found in the local list, nothing removed:\"\n-                    \"\\n    \" + bun,\n-                    fg=\"red\",\n-                )\n-    if modified:\n-        save_local_bundles(bundles_local_dict)\n-\n+from circup.commands import *\n \n # Allows execution via `python -m circup ...`\n # pylint: disable=no-value-for-parameter\ndiff --git a/circup/bundle.py b/circup/bundle.py\nnew file mode 100644\nindex 0000000..39532aa\n--- /dev/null\n+++ b/circup/bundle.py\n@@ -0,0 +1,156 @@\n+# SPDX-FileCopyrightText: 2019 Nicholas Tollervey, 2024 Tim Cocks, written for Adafruit Industries\n+#\n+# SPDX-License-Identifier: MIT\n+\"\"\"\n+Class that represents a specific release of a Bundle.\n+\"\"\"\n+import os\n+import sys\n+\n+import click\n+import requests\n+\n+from circup.shared import (\n+    DATA_DIR,\n+    PLATFORMS,\n+    REQUESTS_TIMEOUT,\n+    tags_data_load,\n+    get_latest_release_from_url,\n+)\n+\n+from circup.logging import logger\n+\n+\n+class Bundle:\n+    \"\"\"\n+    All the links and file names for a bundle\n+    \"\"\"\n+\n+    def __init__(self, repo):\n+        \"\"\"\n+        Initialise a Bundle created from its github info.\n+        Construct all the strings in one place.\n+\n+        :param str repo: Repository string for github: \"user/repository\"\n+        \"\"\"\n+        vendor, bundle_id = repo.split(\"/\")\n+        bundle_id = bundle_id.lower().replace(\"_\", \"-\")\n+        self.key = repo\n+        #\n+        self.url = \"https://github.com/\" + repo\n+        self.basename = bundle_id + \"-{platform}-{tag}\"\n+        self.urlzip = self.basename + \".zip\"\n+        self.dir = os.path.join(DATA_DIR, vendor, bundle_id + \"-{platform}\")\n+        self.zip = os.path.join(DATA_DIR, bundle_id + \"-{platform}.zip\")\n+        self.url_format = self.url + \"/releases/download/{tag}/\" + self.urlzip\n+        # tag\n+        self._current = None\n+        self._latest = None\n+\n+    def lib_dir(self, platform):\n+        \"\"\"\n+        This bundle's lib directory for the platform.\n+\n+        :param str platform: The platform identifier (py/6mpy/...).\n+        :return: The path to the lib directory for the platform.\n+        \"\"\"\n+        tag = self.current_tag\n+        return os.path.join(\n+            self.dir.format(platform=platform),\n+            self.basename.format(platform=PLATFORMS[platform], tag=tag),\n+            \"lib\",\n+        )\n+\n+    def requirements_for(self, library_name, toml_file=False):\n+        \"\"\"\n+        The requirements file for this library.\n+\n+        :param str library_name: The name of the library.\n+        :return: The path to the requirements.txt file.\n+        \"\"\"\n+        platform = \"py\"\n+        tag = self.current_tag\n+        found_file = os.path.join(\n+            self.dir.format(platform=platform),\n+            self.basename.format(platform=PLATFORMS[platform], tag=tag),\n+            \"requirements\",\n+            library_name,\n+            \"requirements.txt\" if not toml_file else \"pyproject.toml\",\n+        )\n+        if os.path.isfile(found_file):\n+            with open(found_file, \"r\", encoding=\"utf-8\") as read_this:\n+                return read_this.read()\n+        return None\n+\n+    @property\n+    def current_tag(self):\n+        \"\"\"\n+        Lazy load current cached tag from the BUNDLE_DATA json file.\n+\n+        :return: The current cached tag value for the project.\n+        \"\"\"\n+        if self._current is None:\n+            self._current = tags_data_load(logger).get(self.key, \"0\")\n+        return self._current\n+\n+    @current_tag.setter\n+    def current_tag(self, tag):\n+        \"\"\"\n+        Set the current cached tag (after updating).\n+\n+        :param str tag: The new value for the current tag.\n+        :return: The current cached tag value for the project.\n+        \"\"\"\n+        self._current = tag\n+\n+    @property\n+    def latest_tag(self):\n+        \"\"\"\n+        Lazy find the value of the latest tag for the bundle.\n+\n+        :return: The most recent tag value for the project.\n+        \"\"\"\n+        if self._latest is None:\n+            self._latest = get_latest_release_from_url(\n+                self.url + \"/releases/latest\", logger\n+            )\n+        return self._latest\n+\n+    def validate(self):\n+        \"\"\"\n+        Test the existence of the expected URLs (not their content)\n+        \"\"\"\n+        tag = self.latest_tag\n+        if not tag or tag == \"releases\":\n+            if \"--verbose\" in sys.argv:\n+                click.secho(f'  Invalid tag \"{tag}\"', fg=\"red\")\n+            return False\n+        for platform in PLATFORMS.values():\n+            url = self.url_format.format(platform=platform, tag=tag)\n+            r = requests.get(url, stream=True, timeout=REQUESTS_TIMEOUT)\n+            # pylint: disable=no-member\n+            if r.status_code != requests.codes.ok:\n+                if \"--verbose\" in sys.argv:\n+                    click.secho(f\"  Unable to find {os.path.split(url)[1]}\", fg=\"red\")\n+                return False\n+            # pylint: enable=no-member\n+        return True\n+\n+    def __repr__(self):\n+        \"\"\"\n+        Helps with log files.\n+\n+        :return: A repr of a dictionary containing the Bundles's metadata.\n+        \"\"\"\n+        return repr(\n+            {\n+                \"key\": self.key,\n+                \"url\": self.url,\n+                \"urlzip\": self.urlzip,\n+                \"dir\": self.dir,\n+                \"zip\": self.zip,\n+                \"url_format\": self.url_format,\n+                \"current\": self._current,\n+                \"latest\": self._latest,\n+            }\n+        )\ndiff --git a/circup/command_utils.py b/circup/command_utils.py\nnew file mode 100644\nindex 0000000..528ba99\n--- /dev/null\n+++ b/circup/command_utils.py\n@@ -0,0 +1,562 @@\n+# SPDX-FileCopyrightText: 2019 Nicholas Tollervey, 2024 Tim Cocks, written for Adafruit Industries\n+#\n+# SPDX-License-Identifier: MIT\n+\"\"\"\n+Functions called from commands in order to provide behaviors and return information.\n+\"\"\"\n+\n+import ctypes\n+import os\n+\n+from subprocess import check_output\n+import sys\n+import shutil\n+import zipfile\n+import json\n+import re\n+import toml\n+import findimports\n+import requests\n+import click\n+\n+from circup.shared import (\n+    PLATFORMS,\n+    REQUESTS_TIMEOUT,\n+    _get_modules_file,\n+    BUNDLE_CONFIG_OVERWRITE,\n+    BUNDLE_CONFIG_FILE,\n+    BUNDLE_CONFIG_LOCAL,\n+    BUNDLE_DATA,\n+    NOT_MCU_LIBRARIES,\n+    tags_data_load,\n+)\n+from circup.logging import logger\n+from circup.module import Module\n+from circup.bundle import Bundle\n+\n+WARNING_IGNORE_MODULES = (\n+    \"typing-extensions\",\n+    \"pyasn1\",\n+    \"circuitpython-typing\",\n+)\n+\n+\n+def clean_library_name(assumed_library_name):\n+    \"\"\"\n+    Most CP repos and library names are look like this:\n+\n+        repo: Adafruit_CircuitPython_LC709203F\n+        library: adafruit_lc709203f\n+\n+    But some do not and this handles cleaning that up.\n+    Also cleans up if the pypi or reponame is passed in instead of the\n+    CP library name.\n+\n+    :param str assumed_library_name: An assumed name of a library from user\n+        or requirements.txt entry\n+    :return: str proper library name\n+    \"\"\"\n+    not_standard_names = {\n+        # Assumed Name : Actual Name\n+        \"adafruit_adafruitio\": \"adafruit_io\",\n+        \"adafruit_asyncio\": \"asyncio\",\n+        \"adafruit_busdevice\": \"adafruit_bus_device\",\n+        \"adafruit_connectionmanager\": \"adafruit_connection_manager\",\n+        \"adafruit_display_button\": \"adafruit_button\",\n+        \"adafruit_neopixel\": \"neopixel\",\n+        \"adafruit_sd\": \"adafruit_sdcard\",\n+        \"adafruit_simpleio\": \"simpleio\",\n+        \"pimoroni_ltr559\": \"pimoroni_circuitpython_ltr559\",\n+    }\n+    if \"circuitpython\" in assumed_library_name:\n+        # convert repo or pypi name to common library name\n+        assumed_library_name = (\n+            assumed_library_name.replace(\"-circuitpython-\", \"_\")\n+            .replace(\"_circuitpython_\", \"_\")\n+            .replace(\"-\", \"_\")\n+        )\n+    if assumed_library_name in not_standard_names:\n+        return not_standard_names[assumed_library_name]\n+    return assumed_library_name\n+\n+\n+def completion_for_install(ctx, param, incomplete):\n+    \"\"\"\n+    Returns the list of available modules for the command line tab-completion\n+    with the ``circup install`` command.\n+    \"\"\"\n+    # pylint: disable=unused-argument\n+    available_modules = get_bundle_versions(get_bundles_list(), avoid_download=True)\n+    module_names = {m.replace(\".py\", \"\") for m in available_modules}\n+    if incomplete:\n+        module_names = [name for name in module_names if name.startswith(incomplete)]\n+    return sorted(module_names)\n+\n+\n+def ensure_latest_bundle(bundle):\n+    \"\"\"\n+    Ensure that there's a copy of the latest library bundle available so circup\n+    can check the metadata contained therein.\n+\n+    :param Bundle bundle: the target Bundle object.\n+    \"\"\"\n+    logger.info(\"Checking library updates for %s.\", bundle.key)\n+    tag = bundle.latest_tag\n+    do_update = False\n+    if tag == bundle.current_tag:\n+        for platform in PLATFORMS:\n+            # missing directories (new platform added on an existing install\n+            # or side effect of pytest or network errors)\n+            do_update = do_update or not os.path.isdir(bundle.lib_dir(platform))\n+    else:\n+        do_update = True\n+\n+    if do_update:\n+        logger.info(\"New version available (%s).\", tag)\n+        try:\n+            get_bundle(bundle, tag)\n+            tags_data_save_tag(bundle.key, tag)\n+        except requests.exceptions.HTTPError as ex:\n+            # See #20 for reason for this\n+            click.secho(\n+                (\n+                    \"There was a problem downloading that platform bundle. \"\n+                    \"Skipping and using existing download if available.\"\n+                ),\n+                fg=\"red\",\n+            )\n+            logger.exception(ex)\n+    else:\n+        logger.info(\"Current bundle up to date %s.\", tag)\n+\n+\n+def find_device():\n+    \"\"\"\n+    Return the location on the filesystem for the connected CircuitPython device.\n+    This is based upon how Mu discovers this information.\n+\n+    :return: The path to the device on the local filesystem.\n+    \"\"\"\n+    device_dir = None\n+    # Attempt to find the path on the filesystem that represents the plugged in\n+    # CIRCUITPY board.\n+    if os.name == \"posix\":\n+        # Linux / OSX\n+        for mount_command in [\"mount\", \"/sbin/mount\"]:\n+            try:\n+                mount_output = check_output(mount_command).splitlines()\n+                mounted_volumes = [x.split()[2] for x in mount_output]\n+                for volume in mounted_volumes:\n+                    if volume.endswith(b\"CIRCUITPY\"):\n+                        device_dir = volume.decode(\"utf-8\")\n+            except FileNotFoundError:\n+                continue\n+    elif os.name == \"nt\":\n+        # Windows\n+\n+        def get_volume_name(disk_name):\n+            \"\"\"\n+            Each disk or external device connected to windows has an attribute\n+            called \"volume name\". This function returns the volume name for the\n+            given disk/device.\n+\n+            Based upon answer given here: http://stackoverflow.com/a/12056414\n+            \"\"\"\n+            vol_name_buf = ctypes.create_unicode_buffer(1024)\n+            ctypes.windll.kernel32.GetVolumeInformationW(\n+                ctypes.c_wchar_p(disk_name),\n+                vol_name_buf,\n+                ctypes.sizeof(vol_name_buf),\n+                None,\n+                None,\n+                None,\n+                None,\n+                0,\n+            )\n+            return vol_name_buf.value\n+\n+        #\n+        # In certain circumstances, volumes are allocated to USB\n+        # storage devices which cause a Windows popup to raise if their\n+        # volume contains no media. Wrapping the check in SetErrorMode\n+        # with SEM_FAILCRITICALERRORS (1) prevents this popup.\n+        #\n+        old_mode = ctypes.windll.kernel32.SetErrorMode(1)\n+        try:\n+            for disk in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n+                path = \"{}:\\\\\".format(disk)\n+                if os.path.exists(path) and get_volume_name(path) == \"CIRCUITPY\":\n+                    device_dir = path\n+                    # Report only the FIRST device found.\n+                    break\n+        finally:\n+            ctypes.windll.kernel32.SetErrorMode(old_mode)\n+    else:\n+        # No support for unknown operating systems.\n+        raise NotImplementedError('OS \"{}\" not supported.'.format(os.name))\n+    logger.info(\"Found device: %s\", device_dir)\n+    return device_dir\n+\n+\n+def find_modules(backend, bundles_list):\n+    \"\"\"\n+    Extracts metadata from the connected device and available bundles and\n+    returns this as a list of Module instances representing the modules on the\n+    device.\n+\n+    :param Backend backend: Backend with the device connection.\n+    :param List[Bundle] bundles_list: List of supported bundles as Bundle objects.\n+    :return: A list of Module instances describing the current state of the\n+             modules on the connected device.\n+    \"\"\"\n+    # pylint: disable=broad-except,too-many-locals\n+    try:\n+        device_modules = backend.get_device_versions()\n+        bundle_modules = get_bundle_versions(bundles_list)\n+        result = []\n+        for key, device_metadata in device_modules.items():\n+\n+            if key in bundle_modules:\n+                path = device_metadata[\"path\"]\n+                bundle_metadata = bundle_modules[key]\n+                repo = bundle_metadata.get(\"__repo__\")\n+                bundle = bundle_metadata.get(\"bundle\")\n+                device_version = device_metadata.get(\"__version__\")\n+                bundle_version = bundle_metadata.get(\"__version__\")\n+                mpy = device_metadata[\"mpy\"]\n+                compatibility = device_metadata.get(\"compatibility\", (None, None))\n+                module_name = (\n+                    path.split(os.sep)[-1]\n+                    if not path.endswith(os.sep)\n+                    else path[:-1].split(os.sep)[-1] + os.sep\n+                )\n+\n+                m = Module(\n+                    module_name,\n+                    backend,\n+                    repo,\n+                    device_version,\n+                    bundle_version,\n+                    mpy,\n+                    bundle,\n+                    compatibility,\n+                )\n+                result.append(m)\n+        return result\n+    except Exception as ex:\n+        # If it's not possible to get the device and bundle metadata, bail out\n+        # with a friendly message and indication of what's gone wrong.\n+        logger.exception(ex)\n+        click.echo(\"There was a problem: {}\".format(ex))\n+        sys.exit(1)\n+    # pylint: enable=broad-except,too-many-locals\n+\n+\n+def get_bundle(bundle, tag):\n+    \"\"\"\n+    Downloads and extracts the version of the bundle with the referenced tag.\n+    The resulting zip file is saved on the local filesystem.\n+\n+    :param Bundle bundle: the target Bundle object.\n+    :param str tag: The GIT tag to use to download the bundle.\n+    \"\"\"\n+    click.echo(f\"Downloading latest bundles for {bundle.key} ({tag}).\")\n+    for platform, github_string in PLATFORMS.items():\n+        # Report the platform: \"8.x-mpy\", etc.\n+        click.echo(f\"{github_string}:\")\n+        url = bundle.url_format.format(platform=github_string, tag=tag)\n+        logger.info(\"Downloading bundle: %s\", url)\n+        r = requests.get(url, stream=True, timeout=REQUESTS_TIMEOUT)\n+        # pylint: disable=no-member\n+        if r.status_code != requests.codes.ok:\n+            logger.warning(\"Unable to connect to %s\", url)\n+            r.raise_for_status()\n+        # pylint: enable=no-member\n+        total_size = int(r.headers.get(\"Content-Length\"))\n+        temp_zip = bundle.zip.format(platform=platform)\n+        with click.progressbar(\n+            r.iter_content(1024), label=\"Extracting:\", length=total_size\n+        ) as pbar, open(temp_zip, \"wb\") as zip_fp:\n+            for chunk in pbar:\n+                zip_fp.write(chunk)\n+                pbar.update(len(chunk))\n+        logger.info(\"Saved to %s\", temp_zip)\n+        temp_dir = bundle.dir.format(platform=platform)\n+        if os.path.isdir(temp_dir):\n+            shutil.rmtree(temp_dir)\n+        with zipfile.ZipFile(temp_zip, \"r\") as zfile:\n+            zfile.extractall(temp_dir)\n+    bundle.current_tag = tag\n+    click.echo(\"\\nOK\\n\")\n+\n+\n+def get_bundle_versions(bundles_list, avoid_download=False):\n+    \"\"\"\n+    Returns a dictionary of metadata from modules in the latest known release\n+    of the library bundle. Uses the Python version (rather than the compiled\n+    version) of the library modules.\n+\n+    :param List[Bundle] bundles_list: List of supported bundles as Bundle objects.\n+    :param bool avoid_download: if True, download the bundle only if missing.\n+    :return: A dictionary of metadata about the modules available in the\n+             library bundle.\n+    \"\"\"\n+    all_the_modules = dict()\n+    for bundle in bundles_list:\n+        if not avoid_download or not os.path.isdir(bundle.lib_dir(\"py\")):\n+            ensure_latest_bundle(bundle)\n+        path = bundle.lib_dir(\"py\")\n+        path_modules = _get_modules_file(path, logger)\n+        for name, module in path_modules.items():\n+            module[\"bundle\"] = bundle\n+            if name not in all_the_modules:  # here we decide the order of priority\n+                all_the_modules[name] = module\n+    return all_the_modules\n+\n+\n+def get_bundles_dict():\n+    \"\"\"\n+    Retrieve the dictionary from BUNDLE_CONFIG_FILE (JSON).\n+    Put the local dictionary in front, so it gets priority.\n+    It's a dictionary of bundle string identifiers.\n+\n+    :return: Combined dictionaries from the config files.\n+    \"\"\"\n+    bundle_dict = get_bundles_local_dict()\n+    try:\n+        with open(BUNDLE_CONFIG_OVERWRITE, \"rb\") as bundle_config_json:\n+            bundle_config = json.load(bundle_config_json)\n+    except (FileNotFoundError, json.decoder.JSONDecodeError):\n+        with open(BUNDLE_CONFIG_FILE, \"rb\") as bundle_config_json:\n+            bundle_config = json.load(bundle_config_json)\n+    for name, bundle in bundle_config.items():\n+        if bundle not in bundle_dict.values():\n+            bundle_dict[name] = bundle\n+    return bundle_dict\n+\n+\n+def get_bundles_local_dict():\n+    \"\"\"\n+    Retrieve the local bundles from BUNDLE_CONFIG_LOCAL (JSON).\n+\n+    :return: Raw dictionary from the config file(s).\n+    \"\"\"\n+    try:\n+        with open(BUNDLE_CONFIG_LOCAL, \"rb\") as bundle_config_json:\n+            bundle_config = json.load(bundle_config_json)\n+        if not isinstance(bundle_config, dict) or not bundle_config:\n+            logger.error(\"Local bundle list invalid. Skipped.\")\n+            raise FileNotFoundError(\"Bad local bundle list\")\n+        return bundle_config\n+    except (FileNotFoundError, json.decoder.JSONDecodeError):\n+        return dict()\n+\n+\n+def get_bundles_list():\n+    \"\"\"\n+    Retrieve the list of bundles from the config dictionary.\n+\n+    :return: List of supported bundles as Bundle objects.\n+    \"\"\"\n+    bundle_config = get_bundles_dict()\n+    bundles_list = [Bundle(bundle_config[b]) for b in bundle_config]\n+    logger.info(\"Using bundles: %s\", \", \".join(b.key for b in bundles_list))\n+    return bundles_list\n+\n+\n+def get_circup_version():\n+    \"\"\"Return the version of circup that is running. If not available, return None.\n+\n+    :return: Current version of circup, or None.\n+    \"\"\"\n+    try:\n+        from importlib import metadata  # pylint: disable=import-outside-toplevel\n+    except ImportError:\n+        try:\n+            import importlib_metadata as metadata  # pylint: disable=import-outside-toplevel\n+        except ImportError:\n+            return None\n+    try:\n+        return metadata.version(\"circup\")\n+    except metadata.PackageNotFoundError:\n+        return None\n+\n+\n+def get_dependencies(*requested_libraries, mod_names, to_install=()):\n+    \"\"\"\n+    Return a list of other CircuitPython libraries required by the given list\n+    of libraries\n+\n+    :param tuple requested_libraries: The libraries to search for dependencies\n+    :param object mod_names:  All the modules metadata from bundle\n+    :param list(str) to_install: Modules already selected for installation.\n+    :return: tuple of module names to install which we build\n+    \"\"\"\n+    # Internal variables\n+    _to_install = to_install\n+    _requested_libraries = []\n+    _rl = requested_libraries[0]\n+\n+    if not requested_libraries[0]:\n+        # If nothing is requested, we're done\n+        return _to_install\n+\n+    for lib_name in _rl:\n+        lower_lib_name = lib_name.lower()\n+        if lower_lib_name in NOT_MCU_LIBRARIES:\n+            logger.info(\n+                \"Skipping %s. It is not for microcontroller installs.\", lib_name\n+            )\n+        else:\n+            # Canonicalize, with some exceptions:\n+            # adafruit-circuitpython-something => adafruit_something\n+            canonical_lib_name = clean_library_name(lower_lib_name)\n+            try:\n+                # Don't process any names we can't find in mod_names\n+                mod_names[canonical_lib_name]  # pylint: disable=pointless-statement\n+                _requested_libraries.append(canonical_lib_name)\n+            except KeyError:\n+                if canonical_lib_name not in WARNING_IGNORE_MODULES:\n+                    click.secho(\n+                        f\"WARNING:\\n\\t{canonical_lib_name} is not a known CircuitPython library.\",\n+                        fg=\"yellow\",\n+                    )\n+\n+    if not _requested_libraries:\n+        # If nothing is requested, we're done\n+        return _to_install\n+\n+    for library in list(_requested_libraries):\n+        if library not in _to_install:\n+            _to_install = _to_install + (library,)\n+            # get the requirements.txt from bundle\n+            bundle = mod_names[library][\"bundle\"]\n+            requirements_txt = bundle.requirements_for(library)\n+            if requirements_txt:\n+                _requested_libraries.extend(\n+                    libraries_from_requirements(requirements_txt)\n+                )\n+\n+            circup_dependencies = get_circup_dependencies(bundle, library)\n+            for circup_dependency in circup_dependencies:\n+                _requested_libraries.append(circup_dependency)\n+\n+        # we've processed this library, remove it from the list\n+        _requested_libraries.remove(library)\n+\n+        return get_dependencies(\n+            tuple(_requested_libraries), mod_names=mod_names, to_install=_to_install\n+        )\n+\n+\n+def get_circup_dependencies(bundle, library):\n+    \"\"\"\n+    Get the list of circup dependencies from pyproject.toml\n+    e.g.\n+    [circup]\n+    circup_dependencies = [\"dependency_name_here\"]\n+\n+    :param bundle: The Bundle to look within\n+    :param library: The Library to find pyproject.toml for and get dependencies from\n+\n+    :return: The list of dependency libraries that were found\n+    \"\"\"\n+    try:\n+        pyproj_toml = bundle.requirements_for(library, toml_file=True)\n+        if pyproj_toml:\n+            pyproj_toml_data = toml.loads(pyproj_toml)\n+            dependencies = pyproj_toml_data[\"circup\"][\"circup_dependencies\"]\n+            if isinstance(dependencies, list):\n+                return dependencies\n+\n+            if isinstance(dependencies, str):\n+                return (dependencies,)\n+\n+        return tuple()\n+\n+    except KeyError:\n+        # no circup_dependencies in pyproject.toml\n+        return tuple()\n+\n+\n+def libraries_from_requirements(requirements):\n+    \"\"\"\n+    Clean up supplied requirements.txt and turn into tuple of CP libraries\n+\n+    :param str requirements: A string version of a requirements.txt\n+    :return: tuple of library names\n+    \"\"\"\n+    libraries = ()\n+    for line in requirements.split(\"\\n\"):\n+        line = line.lower().strip()\n+        if line.startswith(\"#\") or line == \"\":\n+            # skip comments\n+            pass\n+        else:\n+            # Remove everything after any pip style version specifiers\n+            line = re.split(\"[<>=~[;]\", line)[0].strip()\n+            libraries = libraries + (line,)\n+    return libraries\n+\n+\n+def save_local_bundles(bundles_data):\n+    \"\"\"\n+    Save the list of local bundles to the settings.\n+\n+    :param str key: The bundle's identifier/key.\n+    \"\"\"\n+    if len(bundles_data) > 0:\n+        with open(BUNDLE_CONFIG_LOCAL, \"w\", encoding=\"utf-8\") as data:\n+            json.dump(bundles_data, data)\n+    else:\n+        if os.path.isfile(BUNDLE_CONFIG_LOCAL):\n+            os.unlink(BUNDLE_CONFIG_LOCAL)\n+\n+\n+def tags_data_save_tag(key, tag):\n+    \"\"\"\n+    Add or change the saved tag value for a bundle.\n+\n+    :param str key: The bundle's identifier/key.\n+    :param str tag: The new tag for the bundle.\n+    \"\"\"\n+    tags_data = tags_data_load(logger)\n+    tags_data[key] = tag\n+    with open(BUNDLE_DATA, \"w\", encoding=\"utf-8\") as data:\n+        json.dump(tags_data, data)\n+\n+\n+def libraries_from_code_py(code_py, mod_names):\n+    \"\"\"\n+    Parse the given code.py file and return the imported libraries\n+\n+    :param str code_py: Full path of the code.py file\n+    :return: sequence of library names\n+    \"\"\"\n+    # pylint: disable=broad-except\n+    try:\n+        found_imports = findimports.find_imports(code_py)\n+    except Exception as ex:  # broad exception because anything could go wrong\n+        logger.exception(ex)\n+        click.secho('Unable to read the auto file: \"{}\"'.format(str(ex)), fg=\"red\")\n+        sys.exit(2)\n+    # pylint: enable=broad-except\n+    imports = [info.name.split(\".\", 1)[0] for info in found_imports]\n+    return [r for r in imports if r in mod_names]\n+\n+\n+def get_device_path(host, password, path):\n+    \"\"\"\n+    :param host Hostname or IP address.\n+    :param password REST API password.\n+    :param path File system path.\n+    :return device URL or None if the device cannot be found.\n+    \"\"\"\n+    if path:\n+        device_path = path\n+    elif host:\n+        # pylint: enable=no-member\n+        device_path = f\"http://:{password}@\" + host\n+    else:\n+        device_path = find_device()\n+    return device_path\ndiff --git a/circup/commands.py b/circup/commands.py\nnew file mode 100644\nindex 0000000..74117d9\n--- /dev/null\n+++ b/circup/commands.py\n@@ -0,0 +1,657 @@\n+# SPDX-FileCopyrightText: 2019 Nicholas Tollervey, 2024 Tim Cocks, written for Adafruit Industries\n+#\n+# SPDX-License-Identifier: MIT\n+\"\"\"\n+# ----------- CLI command definitions  ----------- #\n+\n+The following functions have IO side effects (for instance they emit to\n+stdout). Ergo, these are not checked with unit tests. Most of the\n+functionality they provide is provided by the functions from util_functions.py,\n+and the respective Backends which *are* tested. Most of the logic of the following\n+functions is to prepare things for presentation to / interaction with the user.\n+\"\"\"\n+import os\n+import time\n+import sys\n+import re\n+import logging\n+import update_checker\n+from semver import VersionInfo\n+import click\n+import requests\n+\n+\n+from circup.backends import WebBackend, DiskBackend\n+from circup.logging import logger, log_formatter, LOGFILE\n+from circup.shared import BOARDLESS_COMMANDS, get_latest_release_from_url\n+from circup.bundle import Bundle\n+from circup.command_utils import (\n+    get_device_path,\n+    get_circup_version,\n+    find_modules,\n+    get_bundles_list,\n+    completion_for_install,\n+    get_bundle_versions,\n+    libraries_from_requirements,\n+    libraries_from_code_py,\n+    get_dependencies,\n+    get_bundles_local_dict,\n+    save_local_bundles,\n+    get_bundles_dict,\n+)\n+\n+\n+@click.group()\n+@click.option(\n+    \"--verbose\", is_flag=True, help=\"Comprehensive logging is sent to stdout.\"\n+)\n+@click.option(\n+    \"--path\",\n+    type=click.Path(exists=True, file_okay=False),\n+    help=\"Path to CircuitPython directory. Overrides automatic path detection.\",\n+)\n+@click.option(\n+    \"--host\",\n+    help=\"Hostname or IP address of a device. Overrides automatic path detection.\",\n+)\n+@click.option(\n+    \"--password\", help=\"Password to use for authentication when --host is used.\"\n+)\n+@click.option(\n+    \"--timeout\",\n+    default=30,\n+    help=\"Specify the timeout in seconds for any network operations.\",\n+)\n+@click.option(\n+    \"--board-id\",\n+    default=None,\n+    help=\"Manual Board ID of the CircuitPython device. If provided in combination \"\n+    \"with --cpy-version, it overrides the detected board ID.\",\n+)\n+@click.option(\n+    \"--cpy-version\",\n+    default=None,\n+    help=\"Manual CircuitPython version. If provided in combination \"\n+    \"with --board-id, it overrides the detected CPy version.\",\n+)\n+@click.version_option(\n+    prog_name=\"CircUp\",\n+    message=\"%(prog)s, A CircuitPython module updater. Version %(version)s\",\n+)\n+@click.pass_context\n+def main(  # pylint: disable=too-many-locals\n+    ctx, verbose, path, host, password, timeout, board_id, cpy_version\n+):  # pragma: no cover\n+    \"\"\"\n+    A tool to manage and update libraries on a CircuitPython device.\n+    \"\"\"\n+    # pylint: disable=too-many-arguments,too-many-branches,too-many-statements,too-many-locals\n+    ctx.ensure_object(dict)\n+    ctx.obj[\"TIMEOUT\"] = timeout\n+    device_path = get_device_path(host, password, path)\n+\n+    using_webworkflow = \"host\" in ctx.params.keys() and ctx.params[\"host\"] is not None\n+\n+    if using_webworkflow:\n+        if host == \"circuitpython.local\":\n+            click.echo(\"Checking versions.json on circuitpython.local to find hostname\")\n+            versions_resp = requests.get(\n+                \"http://circuitpython.local/cp/version.json\", timeout=timeout\n+            )\n+            host = f'{versions_resp.json()[\"hostname\"]}.local'\n+            click.echo(f\"Using hostname: {host}\")\n+            device_path = device_path.replace(\"circuitpython.local\", host)\n+        try:\n+            ctx.obj[\"backend\"] = WebBackend(\n+                host=host, password=password, logger=logger, timeout=timeout\n+            )\n+        except ValueError as e:\n+            click.secho(e, fg=\"red\")\n+            time.sleep(0.3)\n+            sys.exit(1)\n+        except RuntimeError as e:\n+            click.secho(e, fg=\"red\")\n+            sys.exit(1)\n+    else:\n+        try:\n+            ctx.obj[\"backend\"] = DiskBackend(device_path, logger)\n+        except ValueError as e:\n+            print(e)\n+\n+    if verbose:\n+        # Configure additional logging to stdout.\n+        ctx.obj[\"verbose\"] = True\n+        verbose_handler = logging.StreamHandler(sys.stdout)\n+        verbose_handler.setLevel(logging.INFO)\n+        verbose_handler.setFormatter(log_formatter)\n+        logger.addHandler(verbose_handler)\n+        click.echo(\"Logging to {}\\n\".format(LOGFILE))\n+    else:\n+        ctx.obj[\"verbose\"] = False\n+\n+    logger.info(\"### Started Circup ###\")\n+\n+    # If a newer version of circup is available, print a message.\n+    logger.info(\"Checking for a newer version of circup\")\n+    version = get_circup_version()\n+    if version:\n+        update_checker.update_check(\"circup\", version)\n+\n+    # stop early if the command is boardless\n+    if ctx.invoked_subcommand in BOARDLESS_COMMANDS or \"--help\" in sys.argv:\n+        return\n+\n+    ctx.obj[\"DEVICE_PATH\"] = device_path\n+    latest_version = get_latest_release_from_url(\n+        \"https://github.com/adafruit/circuitpython/releases/latest\", logger\n+    )\n+\n+    if device_path is None or not ctx.obj[\"backend\"].is_device_present():\n+        click.secho(\"Could not find a connected CircuitPython device.\", fg=\"red\")\n+        sys.exit(1)\n+    else:\n+        cpy_version, board_id = (\n+            ctx.obj[\"backend\"].get_circuitpython_version()\n+            if board_id is None or cpy_version is None\n+            else (cpy_version, board_id)\n+        )\n+        click.echo(\n+            \"Found device at {}, running CircuitPython {}.\".format(\n+                device_path, cpy_version\n+            )\n+        )\n+    try:\n+        if VersionInfo.parse(cpy_version) < VersionInfo.parse(latest_version):\n+            click.secho(\n+                \"A newer version of CircuitPython ({}) is available.\".format(\n+                    latest_version\n+                ),\n+                fg=\"green\",\n+            )\n+            if board_id:\n+                url_download = f\"https://circuitpython.org/board/{board_id}\"\n+            else:\n+                url_download = \"https://circuitpython.org/downloads\"\n+            click.secho(\"Get it here: {}\".format(url_download), fg=\"green\")\n+    except ValueError as ex:\n+        logger.warning(\"CircuitPython has incorrect semver value.\")\n+        logger.warning(ex)\n+\n+\n+@main.command()\n+@click.option(\"-r\", \"--requirement\", is_flag=True)\n+@click.pass_context\n+def freeze(ctx, requirement):  # pragma: no cover\n+    \"\"\"\n+    Output details of all the modules found on the connected CIRCUITPYTHON\n+    device. Option -r saves output to requirements.txt file\n+    \"\"\"\n+    logger.info(\"Freeze\")\n+    modules = find_modules(ctx.obj[\"backend\"], get_bundles_list())\n+    if modules:\n+        output = []\n+        for module in modules:\n+            output.append(\"{}=={}\".format(module.name, module.device_version))\n+        for module in output:\n+            click.echo(module)\n+            logger.info(module)\n+        if requirement:\n+            cwd = os.path.abspath(os.getcwd())\n+            for i, module in enumerate(output):\n+                output[i] += \"\\n\"\n+\n+            overwrite = None\n+            if os.path.exists(os.path.join(cwd, \"requirements.txt\")):\n+                overwrite = click.confirm(\n+                    click.style(\n+                        \"\\nrequirements.txt file already exists in this location.\\n\"\n+                        \"Do you want to overwrite it?\",\n+                        fg=\"red\",\n+                    ),\n+                    default=False,\n+                )\n+            else:\n+                overwrite = True\n+\n+            if overwrite:\n+                with open(\n+                    cwd + \"/\" + \"requirements.txt\", \"w\", newline=\"\\n\", encoding=\"utf-8\"\n+                ) as file:\n+                    file.truncate(0)\n+                    file.writelines(output)\n+    else:\n+        click.echo(\"No modules found on the device.\")\n+\n+\n+@main.command(\"list\")\n+@click.pass_context\n+def list_cli(ctx):  # pragma: no cover\n+    \"\"\"\n+    Lists all out of date modules found on the connected CIRCUITPYTHON device.\n+    \"\"\"\n+    logger.info(\"List\")\n+    # Grab out of date modules.\n+    data = [(\"Module\", \"Version\", \"Latest\", \"Update Reason\")]\n+\n+    modules = [\n+        m.row\n+        for m in find_modules(ctx.obj[\"backend\"], get_bundles_list())\n+        if m.outofdate\n+    ]\n+    if modules:\n+        data += modules\n+        # Nice tabular display.\n+        col_width = [0, 0, 0, 0]\n+        for row in data:\n+            for i, word in enumerate(row):\n+                col_width[i] = max(len(word) + 2, col_width[i])\n+        dashes = tuple((\"-\" * (width - 1) for width in col_width))\n+        data.insert(1, dashes)\n+        click.echo(\n+            \"The following modules are out of date or probably need an update.\\n\"\n+            \"Major Updates may include breaking changes. Review before updating.\\n\"\n+            \"MPY Format changes from Circuitpython 8 to 9 require an update.\\n\"\n+        )\n+        for row in data:\n+            output = \"\"\n+            for index, cell in enumerate(row):\n+                output += cell.ljust(col_width[index])\n+            if \"--verbose\" not in sys.argv:\n+                click.echo(output)\n+            logger.info(output)\n+    else:\n+        click.echo(\"All modules found on the device are up to date.\")\n+\n+\n+# pylint: disable=too-many-arguments,too-many-locals\n+@main.command()\n+@click.argument(\n+    \"modules\", required=False, nargs=-1, shell_complete=completion_for_install\n+)\n+@click.option(\n+    \"pyext\",\n+    \"--py\",\n+    is_flag=True,\n+    help=\"Install the .py version of the module(s) instead of the mpy version.\",\n+)\n+@click.option(\n+    \"-r\",\n+    \"--requirement\",\n+    type=click.Path(exists=True, dir_okay=False),\n+    help=\"specify a text file to install all modules listed in the text file.\"\n+    \" Typically requirements.txt.\",\n+)\n+@click.option(\n+    \"--auto\", \"-a\", is_flag=True, help=\"Install the modules imported in code.py.\"\n+)\n+@click.option(\n+    \"--auto-file\",\n+    default=None,\n+    help=\"Specify the name of a file on the board to read for auto install.\"\n+    \" Also accepts an absolute path or a local ./ path.\",\n+)\n+@click.pass_context\n+def install(ctx, modules, pyext, requirement, auto, auto_file):  # pragma: no cover\n+    \"\"\"\n+    Install a named module(s) onto the device. Multiple modules\n+    can be installed at once by providing more than one module name, each\n+    separated by a space.\n+    \"\"\"\n+\n+    # TODO: Ensure there's enough space on the device\n+    available_modules = get_bundle_versions(get_bundles_list())\n+    mod_names = {}\n+    for module, metadata in available_modules.items():\n+        mod_names[module.replace(\".py\", \"\").lower()] = metadata\n+    if requirement:\n+        with open(requirement, \"r\", encoding=\"utf-8\") as rfile:\n+            requirements_txt = rfile.read()\n+        requested_installs = libraries_from_requirements(requirements_txt)\n+    elif auto or auto_file:\n+        if auto_file is None:\n+            auto_file = \"code.py\"\n+            print(f\"Auto file: {auto_file}\")\n+        # pass a local file with \"./\" or \"../\"\n+        is_relative = not isinstance(ctx.obj[\"backend\"], WebBackend) or auto_file.split(\n+            os.sep\n+        )[0] in [os.path.curdir, os.path.pardir]\n+        if not os.path.isabs(auto_file) and not is_relative:\n+            auto_file = ctx.obj[\"backend\"].get_file_path(auto_file or \"code.py\")\n+\n+        auto_file_path = ctx.obj[\"backend\"].get_auto_file_path(auto_file)\n+        print(f\"Auto file path: {auto_file_path}\")\n+        if not os.path.isfile(auto_file_path):\n+            # fell through to here when run from random folder on windows - ask backend.\n+            new_auto_file = ctx.obj[\"backend\"].get_file_path(auto_file)\n+            if os.path.isfile(new_auto_file):\n+                auto_file = new_auto_file\n+                auto_file_path = ctx.obj[\"backend\"].get_auto_file_path(auto_file)\n+                print(f\"Auto file path: {auto_file_path}\")\n+            else:\n+                click.secho(f\"Auto file not found: {auto_file}\", fg=\"red\")\n+                sys.exit(1)\n+\n+        requested_installs = libraries_from_code_py(auto_file_path, mod_names)\n+    else:\n+        requested_installs = modules\n+    requested_installs = sorted(set(requested_installs))\n+    click.echo(f\"Searching for dependencies for: {requested_installs}\")\n+    to_install = get_dependencies(requested_installs, mod_names=mod_names)\n+    device_modules = ctx.obj[\"backend\"].get_device_versions()\n+    if to_install is not None:\n+        to_install = sorted(to_install)\n+        click.echo(f\"Ready to install: {to_install}\\n\")\n+        for library in to_install:\n+            ctx.obj[\"backend\"].install_module(\n+                ctx.obj[\"DEVICE_PATH\"], device_modules, library, pyext, mod_names\n+            )\n+\n+\n+# pylint: enable=too-many-arguments,too-many-locals\n+\n+\n+@main.command()\n+@click.argument(\"match\", required=False, nargs=1)\n+def show(match):  # pragma: no cover\n+    \"\"\"\n+    Show a list of available modules in the bundle. These are modules which\n+    *could* be installed on the device.\n+\n+    If MATCH is specified only matching modules will be listed.\n+    \"\"\"\n+    available_modules = get_bundle_versions(get_bundles_list())\n+    module_names = sorted([m.replace(\".py\", \"\") for m in available_modules])\n+    if match is not None:\n+        match = match.lower()\n+        module_names = [m for m in module_names if match in m]\n+    click.echo(\"\\n\".join(module_names))\n+\n+    click.echo(\n+        \"{} shown of {} packages.\".format(len(module_names), len(available_modules))\n+    )\n+\n+\n+@main.command()\n+@click.argument(\"module\", nargs=-1)\n+@click.pass_context\n+def uninstall(ctx, module):  # pragma: no cover\n+    \"\"\"\n+    Uninstall a named module(s) from the connected device. Multiple modules\n+    can be uninstalled at once by providing more than one module name, each\n+    separated by a space.\n+    \"\"\"\n+    device_path = ctx.obj[\"DEVICE_PATH\"]\n+    print(f\"Uninstalling {module} from {device_path}\")\n+    for name in module:\n+        device_modules = ctx.obj[\"backend\"].get_device_versions()\n+        name = name.lower()\n+        mod_names = {}\n+        for module_item, metadata in device_modules.items():\n+            mod_names[module_item.replace(\".py\", \"\").lower()] = metadata\n+        if name in mod_names:\n+            metadata = mod_names[name]\n+            module_path = metadata[\"path\"]\n+            ctx.obj[\"backend\"].uninstall(device_path, module_path)\n+            click.echo(\"Uninstalled '{}'.\".format(name))\n+        else:\n+            click.echo(\"Module '{}' not found on device.\".format(name))\n+        continue\n+\n+\n+# pylint: disable=too-many-branches\n+\n+\n+@main.command(\n+    short_help=(\n+        \"Update modules on the device. \"\n+        \"Use --all to automatically update all modules without Major Version warnings.\"\n+    )\n+)\n+@click.option(\n+    \"update_all\",\n+    \"--all\",\n+    is_flag=True,\n+    help=\"Update all modules without Major Version warnings.\",\n+)\n+@click.pass_context\n+# pylint: disable=too-many-locals\n+def update(ctx, update_all):  # pragma: no cover\n+    \"\"\"\n+    Checks for out-of-date modules on the connected CIRCUITPYTHON device, and\n+    prompts the user to confirm updating such modules.\n+    \"\"\"\n+    logger.info(\"Update\")\n+    # Grab current modules.\n+    bundles_list = get_bundles_list()\n+    installed_modules = find_modules(ctx.obj[\"backend\"], bundles_list)\n+    modules_to_update = [m for m in installed_modules if m.outofdate]\n+\n+    if not modules_to_update:\n+        click.echo(\"None of the module[s] found on the device need an update.\")\n+        return\n+\n+    # Process out of date modules\n+    updated_modules = []\n+    click.echo(\"Found {} module[s] needing update.\".format(len(modules_to_update)))\n+    if not update_all:\n+        click.echo(\"Please indicate which module[s] you wish to update:\\n\")\n+    for module in modules_to_update:\n+        update_flag = update_all\n+        if \"--verbose\" in sys.argv:\n+            click.echo(\n+                \"Device version: {}, Bundle version: {}\".format(\n+                    module.device_version, module.bundle_version\n+                )\n+            )\n+        if isinstance(module.bundle_version, str) and not VersionInfo.is_valid(\n+            module.bundle_version\n+        ):\n+            click.secho(\n+                f\"WARNING: Library {module.name} repo has incorrect __version__\"\n+                \"\\n\\tmetadata. Circup will assume it needs updating.\"\n+                \"\\n\\tPlease file an issue in the library repo.\",\n+                fg=\"yellow\",\n+            )\n+            if module.repo:\n+                click.secho(f\"\\t{module.repo}\", fg=\"yellow\")\n+        if not update_flag:\n+            if module.bad_format:\n+                click.secho(\n+                    f\"WARNING: '{module.name}': module corrupted or in an\"\n+                    \" unknown mpy format. Updating is required.\",\n+                    fg=\"yellow\",\n+                )\n+                update_flag = click.confirm(\"Do you want to update?\")\n+            elif module.mpy_mismatch:\n+                click.secho(\n+                    f\"WARNING: '{module.name}': mpy format doesn't match the\"\n+                    \" device's Circuitpython version. Updating is required.\",\n+                    fg=\"yellow\",\n+                )\n+                update_flag = click.confirm(\"Do you want to update?\")\n+            elif module.major_update:\n+                update_flag = click.confirm(\n+                    (\n+                        \"'{}' is a Major Version update and may contain breaking \"\n+                        \"changes. Do you want to update?\".format(module.name)\n+                    )\n+                )\n+            else:\n+                update_flag = click.confirm(\"Update '{}'?\".format(module.name))\n+        if update_flag:\n+            # pylint: disable=broad-except\n+            try:\n+                ctx.obj[\"backend\"].update(module)\n+                updated_modules.append(module.name)\n+                click.echo(\"Updated {}\".format(module.name))\n+            except Exception as ex:\n+                logger.exception(ex)\n+                click.echo(\"Something went wrong, {} (check the logs)\".format(str(ex)))\n+            # pylint: enable=broad-except\n+\n+    if not updated_modules:\n+        return\n+\n+    # We updated modules, look to see if any requirements are missing\n+    click.echo(\n+        \"Checking {} updated module[s] for missing requirements.\".format(\n+            len(updated_modules)\n+        )\n+    )\n+    available_modules = get_bundle_versions(bundles_list)\n+    mod_names = {}\n+    for module, metadata in available_modules.items():\n+        mod_names[module.replace(\".py\", \"\").lower()] = metadata\n+    missing_modules = get_dependencies(updated_modules, mod_names=mod_names)\n+    device_modules = ctx.obj[\"backend\"].get_device_versions()\n+    # Process newly needed modules\n+    if missing_modules is not None:\n+        installed_module_names = [m.name for m in installed_modules]\n+        missing_modules = set(missing_modules) - set(installed_module_names)\n+        missing_modules = sorted(list(missing_modules))\n+        click.echo(f\"Ready to install: {missing_modules}\\n\")\n+        for library in missing_modules:\n+            ctx.obj[\"backend\"].install_module(\n+                ctx.obj[\"DEVICE_PATH\"], device_modules, library, False, mod_names\n+            )\n+\n+\n+# pylint: enable=too-many-branches\n+\n+\n+@main.command(\"bundle-show\")\n+@click.option(\"--modules\", is_flag=True, help=\"List all the modules per bundle.\")\n+def bundle_show(modules):\n+    \"\"\"\n+    Show the list of bundles, default and local, with URL, current version\n+    and latest version retrieved from the web.\n+    \"\"\"\n+    local_bundles = get_bundles_local_dict().values()\n+    bundles = get_bundles_list()\n+    available_modules = get_bundle_versions(bundles)\n+\n+    for bundle in bundles:\n+        if bundle.key in local_bundles:\n+            click.secho(bundle.key, fg=\"yellow\")\n+        else:\n+            click.secho(bundle.key, fg=\"green\")\n+        click.echo(\"    \" + bundle.url)\n+        click.echo(\"    version = \" + bundle.current_tag)\n+        if modules:\n+            click.echo(\"Modules:\")\n+            for name, mod in sorted(available_modules.items()):\n+                if mod[\"bundle\"] == bundle:\n+                    click.echo(f\"   {name} ({mod.get('__version__', '-')})\")\n+\n+\n+@main.command(\"bundle-add\")\n+@click.argument(\"bundle\", nargs=-1)\n+@click.pass_context\n+def bundle_add(ctx, bundle):\n+    \"\"\"\n+    Add bundles to the local bundles list, by \"user/repo\" github string.\n+    A series of tests to validate that the bundle exists and at least looks\n+    like a bundle are done before validating it. There might still be errors\n+    when the bundle is downloaded for the first time.\n+    \"\"\"\n+\n+    if len(bundle) == 0:\n+        click.secho(\n+            \"Must pass bundle argument, expecting github URL or `user/repository` string.\",\n+            fg=\"red\",\n+        )\n+        return\n+\n+    bundles_dict = get_bundles_local_dict()\n+    modified = False\n+    for bundle_repo in bundle:\n+        # cleanup in case seombody pastes the URL to the repo/releases\n+        bundle_repo = re.sub(\n+            r\"https?://github.com/([^/]+/[^/]+)(/.*)?\", r\"\\1\", bundle_repo\n+        )\n+        if bundle_repo in bundles_dict.values():\n+            click.secho(\"Bundle already in list.\", fg=\"yellow\")\n+            click.secho(\"    \" + bundle_repo, fg=\"yellow\")\n+            continue\n+        try:\n+            bundle_added = Bundle(bundle_repo)\n+        except ValueError:\n+            click.secho(\n+                \"Bundle string invalid, expecting github URL or `user/repository` string.\",\n+                fg=\"red\",\n+            )\n+            click.secho(\"    \" + bundle_repo, fg=\"red\")\n+            continue\n+        result = requests.get(\n+            \"https://github.com/\" + bundle_repo, timeout=ctx.obj[\"TIMEOUT\"]\n+        )\n+        # pylint: disable=no-member\n+        if result.status_code == requests.codes.NOT_FOUND:\n+            click.secho(\"Bundle invalid, the repository doesn't exist (404).\", fg=\"red\")\n+            click.secho(\"    \" + bundle_repo, fg=\"red\")\n+            continue\n+        # pylint: enable=no-member\n+        if not bundle_added.validate():\n+            click.secho(\n+                \"Bundle invalid, is the repository a valid circup bundle ?\", fg=\"red\"\n+            )\n+            click.secho(\"    \" + bundle_repo, fg=\"red\")\n+            continue\n+        # note: use bun as the dictionary key for uniqueness\n+        bundles_dict[bundle_repo] = bundle_repo\n+        modified = True\n+        click.echo(\"Added \" + bundle_repo)\n+        click.echo(\"    \" + bundle_added.url)\n+    if modified:\n+        # save the bundles list\n+        save_local_bundles(bundles_dict)\n+        # update and get the new bundles for the first time\n+        get_bundle_versions(get_bundles_list())\n+\n+\n+@main.command(\"bundle-remove\")\n+@click.argument(\"bundle\", nargs=-1)\n+@click.option(\"--reset\", is_flag=True, help=\"Remove all local bundles.\")\n+def bundle_remove(bundle, reset):\n+    \"\"\"\n+    Remove one or more bundles from the local bundles list.\n+    \"\"\"\n+    if reset:\n+        save_local_bundles({})\n+        return\n+\n+    if len(bundle) == 0:\n+        click.secho(\n+            \"Must pass bundle argument or --reset, expecting github URL or \"\n+            \"`user/repository` string. Run circup bundle-show to see a list of bundles.\",\n+            fg=\"red\",\n+        )\n+        return\n+    bundle_config = list(get_bundles_dict().values())\n+    bundles_local_dict = get_bundles_local_dict()\n+    modified = False\n+    for bun in bundle:\n+        # cleanup in case seombody pastes the URL to the repo/releases\n+        bun = re.sub(r\"https?://github.com/([^/]+/[^/]+)(/.*)?\", r\"\\1\", bun)\n+        found = False\n+        for name, repo in list(bundles_local_dict.items()):\n+            if bun in (name, repo):\n+                found = True\n+                click.secho(f\"Bundle {repo}\")\n+                do_it = click.confirm(\"Do you want to remove that bundle ?\")\n+                if do_it:\n+                    click.secho(\"Removing the bundle from the local list\", fg=\"yellow\")\n+                    click.secho(f\"    {bun}\", fg=\"yellow\")\n+                    modified = True\n+                    del bundles_local_dict[name]\n+        if not found:\n+            if bun in bundle_config:\n+                click.secho(\"Cannot remove built-in module:\" \"\\n    \" + bun, fg=\"red\")\n+            else:\n+                click.secho(\n+                    \"Bundle not found in the local list, nothing removed:\"\n+                    \"\\n    \" + bun,\n+                    fg=\"red\",\n+                )\n+    if modified:\n+        save_local_bundles(bundles_local_dict)\ndiff --git a/circup/logging.py b/circup/logging.py\nnew file mode 100644\nindex 0000000..8189126\n--- /dev/null\n+++ b/circup/logging.py\n@@ -0,0 +1,33 @@\n+# SPDX-FileCopyrightText: 2019 Nicholas Tollervey, 2024 Tim Cocks, written for Adafruit Industries\n+#\n+# SPDX-License-Identifier: MIT\n+\"\"\"\n+Logging utilities and configuration used by circup\n+\"\"\"\n+import os\n+import logging\n+from logging.handlers import RotatingFileHandler\n+import appdirs\n+\n+from circup.shared import DATA_DIR\n+\n+#: The directory containing the utility's log file.\n+LOG_DIR = appdirs.user_log_dir(appname=\"circup\", appauthor=\"adafruit\")\n+#: The location of the log file for the utility.\n+LOGFILE = os.path.join(LOG_DIR, \"circup.log\")\n+\n+# Ensure DATA_DIR / LOG_DIR related directories and files exist.\n+if not os.path.exists(DATA_DIR):  # pragma: no cover\n+    os.makedirs(DATA_DIR)\n+if not os.path.exists(LOG_DIR):  # pragma: no cover\n+    os.makedirs(LOG_DIR)\n+\n+# Setup logging.\n+logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+logfile_handler = RotatingFileHandler(LOGFILE, maxBytes=10_000_000, backupCount=0)\n+log_formatter = logging.Formatter(\n+    \"%(asctime)s %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\"\n+)\n+logfile_handler.setFormatter(log_formatter)\n+logger.addHandler(logfile_handler)\ndiff --git a/circup/module.py b/circup/module.py\nnew file mode 100644\nindex 0000000..9336a2d\n--- /dev/null\n+++ b/circup/module.py\n@@ -0,0 +1,209 @@\n+# SPDX-FileCopyrightText: 2019 Nicholas Tollervey, 2024 Tim Cocks, written for Adafruit Industries\n+#\n+# SPDX-License-Identifier: MIT\n+\"\"\"\n+Class that represents a specific CircuitPython module on a device or in a Bundle.\n+\"\"\"\n+import os\n+from urllib.parse import urljoin, urlparse\n+from semver import VersionInfo\n+\n+from circup.shared import BAD_FILE_FORMAT\n+from circup.backends import WebBackend\n+from circup.logging import logger\n+\n+\n+class Module:\n+    \"\"\"\n+    Represents a CircuitPython module.\n+    \"\"\"\n+\n+    # pylint: disable=too-many-arguments\n+\n+    def __init__(\n+        self,\n+        name,\n+        backend,\n+        repo,\n+        device_version,\n+        bundle_version,\n+        mpy,\n+        bundle,\n+        compatibility,\n+    ):\n+        \"\"\"\n+        The ``self.file`` and ``self.name`` attributes are constructed from\n+        the ``path`` value. If the path is to a directory based module, the\n+        resulting self.file value will be None, and the name will be the\n+        basename of the directory path.\n+\n+        :param str name: The file name of the module.\n+        :param Backend backend: The backend that the module is on.\n+        :param str repo: The URL of the Git repository for this module.\n+        :param str device_version: The semver value for the version on device.\n+        :param str bundle_version: The semver value for the version in bundle.\n+        :param bool mpy: Flag to indicate if the module is byte-code compiled.\n+        :param Bundle bundle: Bundle object where the module is located.\n+        :param (str,str) compatibility: Min and max versions of CP compatible with the mpy.\n+        \"\"\"\n+        self.name = name\n+        self.backend = backend\n+        self.path = (\n+            urljoin(backend.library_path, name, allow_fragments=False)\n+            if isinstance(backend, WebBackend)\n+            else os.path.join(backend.library_path, name)\n+        )\n+\n+        url = urlparse(self.path, allow_fragments=False)\n+\n+        if (\n+            url.path.endswith(\"/\")\n+            if isinstance(backend, WebBackend)\n+            else self.path.endswith(os.sep)\n+        ):\n+            self.file = None\n+            self.name = self.path.split(\n+                \"/\" if isinstance(backend, WebBackend) else os.sep\n+            )[-2]\n+        else:\n+            self.file = os.path.basename(url.path)\n+            self.name = (\n+                os.path.basename(url.path).replace(\".py\", \"\").replace(\".mpy\", \"\")\n+            )\n+\n+        self.repo = repo\n+        self.device_version = device_version\n+        self.bundle_version = bundle_version\n+        self.mpy = mpy\n+        self.min_version = compatibility[0]\n+        self.max_version = compatibility[1]\n+        # Figure out the bundle path.\n+        self.bundle_path = None\n+        if self.mpy:\n+            # Byte compiled, now check CircuitPython version.\n+\n+            major_version = self.backend.get_circuitpython_version()[0].split(\".\")[0]\n+            bundle_platform = \"{}mpy\".format(major_version)\n+        else:\n+            # Regular Python\n+            bundle_platform = \"py\"\n+        # module path in the bundle\n+        search_path = bundle.lib_dir(bundle_platform)\n+        if self.file:\n+            self.bundle_path = os.path.join(search_path, self.file)\n+        else:\n+            self.bundle_path = os.path.join(search_path, self.name)\n+        logger.info(self)\n+\n+    # pylint: enable=too-many-arguments\n+\n+    @property\n+    def outofdate(self):\n+        \"\"\"\n+        Returns a boolean to indicate if this module is out of date.\n+        Treat mismatched MPY versions as out of date.\n+\n+        :return: Truthy indication if the module is out of date.\n+        \"\"\"\n+        if self.mpy_mismatch:\n+            return True\n+        if self.device_version and self.bundle_version:\n+            try:\n+                return VersionInfo.parse(self.device_version) < VersionInfo.parse(\n+                    self.bundle_version\n+                )\n+            except ValueError as ex:\n+                logger.warning(\"Module '%s' has incorrect semver value.\", self.name)\n+                logger.warning(ex)\n+        return True  # Assume out of date to try to update.\n+\n+    @property\n+    def bad_format(self):\n+        \"\"\"A boolean indicating that the mpy file format could not be identified\"\"\"\n+        return self.mpy and self.device_version == BAD_FILE_FORMAT\n+\n+    @property\n+    def mpy_mismatch(self):\n+        \"\"\"\n+        Returns a boolean to indicate if this module's MPY version is compatible\n+        with the board's current version of Circuitpython. A min or max version\n+        that evals to False means no limit.\n+\n+        :return: Boolean indicating if the MPY versions don't match.\n+        \"\"\"\n+        if not self.mpy:\n+            return False\n+        try:\n+            cpv = VersionInfo.parse(self.backend.get_circuitpython_version()[0])\n+        except ValueError as ex:\n+            logger.warning(\"CircuitPython has incorrect semver value.\")\n+            logger.warning(ex)\n+        try:\n+            if self.min_version and cpv < VersionInfo.parse(self.min_version):\n+                return True  # CP version too old\n+            if self.max_version and cpv >= VersionInfo.parse(self.max_version):\n+                return True  # MPY version too old\n+        except (TypeError, ValueError) as ex:\n+            logger.warning(\n+                \"Module '%s' has incorrect MPY compatibility information.\", self.name\n+            )\n+            logger.warning(ex)\n+        return False\n+\n+    @property\n+    def major_update(self):\n+        \"\"\"\n+        Returns a boolean to indicate if this is a major version update.\n+\n+        :return: Boolean indicating if this is a major version upgrade\n+        \"\"\"\n+        try:\n+            if (\n+                VersionInfo.parse(self.device_version).major\n+                == VersionInfo.parse(self.bundle_version).major\n+            ):\n+                return False\n+        except (TypeError, ValueError) as ex:\n+            logger.warning(\"Module '%s' has incorrect semver value.\", self.name)\n+            logger.warning(ex)\n+        return True  # Assume Major Version udpate.\n+\n+    @property\n+    def row(self):\n+        \"\"\"\n+        Returns a tuple of items to display in a table row to show the module's\n+        name, local version and remote version, and reason to update.\n+\n+        :return: A tuple containing the module's name, version on the connected\n+                 device, version in the latest bundle and reason to update.\n+        \"\"\"\n+        loc = self.device_version if self.device_version else \"unknown\"\n+        rem = self.bundle_version if self.bundle_version else \"unknown\"\n+        if self.mpy_mismatch:\n+            update_reason = \"MPY Format\"\n+        elif self.major_update:\n+            update_reason = \"Major Version\"\n+        else:\n+            update_reason = \"Minor Version\"\n+        return (self.name, loc, rem, update_reason)\n+\n+    def __repr__(self):\n+        \"\"\"\n+        Helps with log files.\n+\n+        :return: A repr of a dictionary containing the module's metadata.\n+        \"\"\"\n+        return repr(\n+            {\n+                \"path\": self.path,\n+                \"file\": self.file,\n+                \"name\": self.name,\n+                \"repo\": self.repo,\n+                \"device_version\": self.device_version,\n+                \"bundle_version\": self.bundle_version,\n+                \"bundle_path\": self.bundle_path,\n+                \"mpy\": self.mpy,\n+                \"min_version\": self.min_version,\n+                \"max_version\": self.max_version,\n+            }\n+        )\ndiff --git a/circup/shared.py b/circup/shared.py\nindex eb45430..7ffbc08 100644\n--- a/circup/shared.py\n+++ b/circup/shared.py\n@@ -9,8 +9,10 @@\n import glob\n import os\n import re\n-\n+import json\n import appdirs\n+import pkg_resources\n+import requests\n \n #: Version identifier for a bad MPY file format\n BAD_FILE_FORMAT = \"Invalid\"\n@@ -18,6 +20,37 @@\n #: The location of data files used by circup (following OS conventions).\n DATA_DIR = appdirs.user_data_dir(appname=\"circup\", appauthor=\"adafruit\")\n \n+#: Module formats list (and the other form used in github files)\n+PLATFORMS = {\"py\": \"py\", \"8mpy\": \"8.x-mpy\", \"9mpy\": \"9.x-mpy\"}\n+\n+#: Timeout for requests calls like get()\n+REQUESTS_TIMEOUT = 30\n+\n+#: The path to the JSON file containing the metadata about the bundles.\n+BUNDLE_CONFIG_FILE = pkg_resources.resource_filename(\n+    \"circup\", \"config/bundle_config.json\"\n+)\n+#: Overwrite the bundles list with this file (only done manually)\n+BUNDLE_CONFIG_OVERWRITE = os.path.join(DATA_DIR, \"bundle_config.json\")\n+#: The path to the JSON file containing the local list of bundles.\n+BUNDLE_CONFIG_LOCAL = os.path.join(DATA_DIR, \"bundle_config_local.json\")\n+#: The path to the JSON file containing the metadata about the bundles.\n+BUNDLE_DATA = os.path.join(DATA_DIR, \"circup.json\")\n+\n+#:  The libraries (and blank lines) which don't go on devices\n+NOT_MCU_LIBRARIES = [\n+    \"\",\n+    \"adafruit-blinka\",\n+    \"adafruit-blinka-bleio\",\n+    \"adafruit-blinka-displayio\",\n+    \"adafruit-circuitpython-typing\",\n+    \"circuitpython_typing\",\n+    \"pyserial\",\n+]\n+\n+#: Commands that do not require an attached board\n+BOARDLESS_COMMANDS = [\"show\", \"bundle-add\", \"bundle-remove\", \"bundle-show\"]\n+\n \n def _get_modules_file(path, logger):\n     \"\"\"\n@@ -142,3 +175,44 @@ def extract_metadata(path, logger):\n             # not a valid MPY file\n             result[\"__version__\"] = BAD_FILE_FORMAT\n     return result\n+\n+\n+def tags_data_load(logger):\n+    \"\"\"\n+    Load the list of the version tags of the bundles on disk.\n+\n+    :return: a dict() of tags indexed by Bundle identifiers/keys.\n+    \"\"\"\n+    tags_data = None\n+    try:\n+        with open(BUNDLE_DATA, encoding=\"utf-8\") as data:\n+            try:\n+                tags_data = json.load(data)\n+            except json.decoder.JSONDecodeError as ex:\n+                # Sometimes (why?) the JSON file becomes corrupt. In which case\n+                # log it and carry on as if setting up for first time.\n+                logger.error(\"Could not parse %s\", BUNDLE_DATA)\n+                logger.exception(ex)\n+    except FileNotFoundError:\n+        pass\n+    if not isinstance(tags_data, dict):\n+        tags_data = {}\n+    return tags_data\n+\n+\n+def get_latest_release_from_url(url, logger):\n+    \"\"\"\n+    Find the tag name of the latest release by using HTTP HEAD and decoding the redirect.\n+\n+    :param str url: URL to the latest release page on a git repository.\n+    :return: The most recent tag value for the release.\n+    \"\"\"\n+\n+    logger.info(\"Requesting redirect information: %s\", url)\n+    response = requests.head(url, timeout=REQUESTS_TIMEOUT)\n+    responseurl = response.url\n+    if response.is_redirect:\n+        responseurl = response.headers[\"Location\"]\n+    tag = responseurl.rsplit(\"/\", 1)[-1]\n+    logger.info(\"Tag: '%s'\", tag)\n+    return tag\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a71107e..158bb5f 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -64,7 +64,7 @@\n #\n # This is also used if you do content translation via gettext catalogs.\n # Usually you set \"language\" from the command line for these cases.\n-language = None\n+language = \"en\"\n \n # List of patterns, relative to source directory, that match files and\n # directories to ignore when looking for source files.\ndiff --git a/optional_requirements.txt b/optional_requirements.txt\nnew file mode 100644\nindex 0000000..6c2456d\n--- /dev/null\n+++ b/optional_requirements.txt\n@@ -0,0 +1,4 @@\n+pytest\n+pytest-cov\n+pytest-faulthandler\n+pytest-random-order\ndiff --git a/optional_requirements.txt.license b/optional_requirements.txt.license\nnew file mode 100644\nindex 0000000..3cbd4cd\n--- /dev/null\n+++ b/optional_requirements.txt.license\n@@ -0,0 +1,3 @@\n+SPDX-FileCopyrightText: 2024 Autogenerated by 'pip freeze'\n+\n+SPDX-License-Identifier: MIT\ndiff --git a/requirements.txt b/requirements.txt\nindex e66f3a9..52b662f 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,58 +1,7 @@\n-alabaster==0.7.12\n-appdirs==1.4.3\n-astroid==2.6.6\n-atomicwrites==1.3.0\n-attrs==19.1.0\n-Babel==2.9.1\n-black==19.3b0\n-bleach==3.3.0\n-certifi==2023.7.22\n-chardet==3.0.4\n-charset-normalizer==2.0.4\n-click==8.0.1\n-coverage==4.5.4\n-docutils==0.15.2\n-findimports==2.1.0\n-idna==2.8\n-imagesize==1.1.0\n-importlib-metadata==4.12.0\n-isort==5.9.3\n-Jinja2==2.11.3\n-lazy-object-proxy==1.6.0\n-MarkupSafe==1.1.1\n-mccabe==0.6.1\n-more-itertools==7.2.0\n-packaging==19.1\n-pkginfo==1.5.0.1\n-pluggy==0.13.1\n-py==1.10.0\n-Pygments==2.15.0\n-pylint==2.9.6\n-pyparsing==2.4.2\n-pytest==5.1.2\n-pytest-cov==2.7.1\n-pytest-faulthandler==2.0.1\n-pytest-random-order==1.0.4\n-pytz==2019.2\n-readme-renderer==24.0\n-requests==2.31.0\n-requests-toolbelt==0.9.1\n-semver==3.0.1\n-six==1.12.0\n-snowballstemmer==1.9.0\n-Sphinx==2.2.0\n-sphinxcontrib-applehelp==1.0.1\n-sphinxcontrib-devhelp==1.0.1\n-sphinxcontrib-htmlhelp==1.0.2\n-sphinxcontrib-jsmath==1.0.1\n-sphinxcontrib-qthelp==1.0.2\n-sphinxcontrib-serializinghtml==1.1.3\n-toml==0.10.0\n-tqdm==4.35.0\n-twine==1.13.0\n-update-checker==0.18.0\n-urllib3==1.26.18\n-wcwidth==0.1.7\n-webencodings==0.5.1\n-wrapt==1.12.1\n-zipp==0.6.0\n+appdirs\n+Click\n+findimports\n+requests\n+semver\n+toml\n+update_checker\n", "instance_id": "adafruit__circup-208", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes an issue where running `circup install --help` results in an error message about not finding a connected CircuitPython device, and the desired behavior is to show the help information instead of the error. The goal is straightforward, and the input (command with `--help`) and expected output (help information) are implied. However, there are minor ambiguities and missing details. For instance, it does not explicitly specify whether this behavior should apply to all commands with `--help` or just `install --help`. Additionally, there are no mentions of constraints, edge cases, or specific requirements for how the help should be displayed when no device is detected. Despite these minor gaps, the problem is valid and understandable with minimal interpretation needed.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) due to the relatively straightforward nature of the required changes. The issue involves modifying the behavior of the `circup` tool to display help information instead of an error message when no CircuitPython device is detected. Based on the provided code changes, which involve significant refactoring (moving code into separate modules like `commands.py`, `bundle.py`, etc.), the actual fix for the specific issue seems to be a small part of a larger restructuring effort. The core change likely involves adjusting the logic in the command-line interface (CLI) handling to bypass the device detection check when the `--help` flag is present, which is a simple modification to the `main` function or related command processing logic in `commands.py`. \n\nThis task requires understanding some code logic, particularly around CLI argument parsing with the `click` library and the device detection mechanism. However, it does not appear to involve complex modifications across multiple files specifically for this issue, nor does it require handling intricate edge cases beyond ensuring the help output works as expected. The broader refactoring seen in the code changes (e.g., modularizing the codebase) adds some context but does not directly pertain to solving this specific problem, so it is not factored into the difficulty score for this issue. The problem does not impact the system's architecture significantly for this specific fix, and no advanced technical concepts or domain-specific knowledge beyond basic Python and CLI tool development are needed. Therefore, it falls into the easy category, requiring only a targeted and simple code adjustment.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "opening_hours.intervals(start, end) does not include intervals after last midnight in the specified range\nHello `opening-hours-rs` \ud83d\udc4b\ud83c\udffe \r\nI found an issue where `opening_hours.intervals(start, end)` does not include intervals after last midnight between `start` and `end`.\r\n\r\nCurrent behaviour is:\r\n```\r\n>>> list(OpeningHours('Mo-Su 00:00-06:00, 23:00-00:00').intervals(datetime(2024, 11, 11, 1, 0),datetime(2024, 11, 12, 1, 0)))\r\n[\r\n  (datetime.datetime(2024, 11, 11, 1, 0), datetime.datetime(2024, 11, 11, 6, 0), 'open', []),\r\n  (datetime.datetime(2024, 11, 11, 6, 0), datetime.datetime(2024, 11, 11, 23, 0), 'closed', []),\r\n  (datetime.datetime(2024, 11, 11, 23, 0), datetime.datetime(2024, 11, 12, 0, 0), 'open', [])\r\n]\r\n```\r\nWe would expect to see a fourth interval:\r\n```(datetime.datetime(2024, 11, 12, 0, 0), datetime.datetime(2024, 11, 12, 1, 0), 'open', [])```\r\nor maybe a longer third interval:\r\n```(datetime.datetime(2024, 11, 11, 23, 0), datetime.datetime(2024, 11, 12, 1, 0), 'open', [])```\r\n\r\nMore broadly `intervals` seems to run up to the last midnight included between `start` and `end`.\r\nThis example works fine because end is at a midnight:\r\n```\r\n>>> list(OpeningHours('Mo-Su 23:00-06:00').intervals(datetime(2024, 11, 11, 1, 0),datetime(2024, 11, 13, 0, 0)))\r\n[\r\n  (datetime.datetime(2024, 11, 11, 1, 0), datetime.datetime(2024, 11, 11, 6, 0), 'open', []),\r\n  (datetime.datetime(2024, 11, 11, 6, 0), datetime.datetime(2024, 11, 11, 23, 0), 'closed', []),\r\n  (datetime.datetime(2024, 11, 11, 23, 0), datetime.datetime(2024, 11, 12, 6, 0), 'open', []),\r\n  (datetime.datetime(2024, 11, 12, 6, 0), datetime.datetime(2024, 11, 12, 23, 0), 'closed', []),\r\n  (datetime.datetime(2024, 11, 12, 23, 0), datetime.datetime(2024, 11, 13, 0, 0), 'open', [])\r\n]\r\n```\r\nThis example again misses out the final interval because `end` runs into the next day:\r\n```\r\n>>> list(OpeningHours('Mo-Su 23:00-06:00').intervals(datetime(2024, 11, 11, 1, 0),datetime(2024, 11, 13, 5, 0)))\r\n[\r\n  (datetime.datetime(2024, 11, 11, 1, 0), datetime.datetime(2024, 11, 11, 6, 0), 'open', []),\r\n  (datetime.datetime(2024, 11, 11, 6, 0), datetime.datetime(2024, 11, 11, 23, 0), 'closed', []),\r\n  (datetime.datetime(2024, 11, 11, 23, 0), datetime.datetime(2024, 11, 12, 6, 0), 'open', []),\r\n  (datetime.datetime(2024, 11, 12, 6, 0), datetime.datetime(2024, 11, 12, 23, 0), 'closed', []),\r\n  (datetime.datetime(2024, 11, 12, 23, 0), datetime.datetime(2024, 11, 13, 0, 0), 'open', [])\r\n]\r\n```\r\n\r\n(Note for the above examples I've also tested with a couple different tariff formats like 'Mo-Su 00:00-06:00, 23:00-24:00' or 'Mo-Su 23:00-06:00' and got the same results)\r\n\r\nIt would be great to fix this so that `intervals` can return correct values\r\nThanks for your time\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 45e3b59..4a9a1c2 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,5 +1,12 @@\n # Changelog\n \n+## 0.8.3\n+\n+### Fixes\n+\n+- Fix [#52](https://github.com/remi-dupre/opening-hours-rs/pull/52): intervals\n+  were stopping at midnight before the last day.\n+\n \n ## 0.8.2\n \ndiff --git a/Cargo.lock b/Cargo.lock\nindex c5b1136..3f414a1 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -171,7 +171,7 @@ checksum = \"1462739cb27611015575c0c11df5df7601141071f07518d56fcc1be504cbec97\"\n \n [[package]]\n name = \"compact-calendar\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n dependencies = [\n  \"chrono\",\n ]\n@@ -499,7 +499,7 @@ checksum = \"b410bbe7e14ab526a0e86877eb47c6996a2bd7746f027ba551028c925390e4e9\"\n \n [[package]]\n name = \"opening-hours\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n dependencies = [\n  \"chrono\",\n  \"compact-calendar\",\n@@ -511,7 +511,7 @@ dependencies = [\n \n [[package]]\n name = \"opening-hours-py\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n dependencies = [\n  \"chrono\",\n  \"opening-hours\",\n@@ -522,7 +522,7 @@ dependencies = [\n \n [[package]]\n name = \"opening-hours-syntax\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n dependencies = [\n  \"chrono\",\n  \"log\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 4bbffab..0fbab8d 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"opening-hours\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n authors = [\"R\u00e9mi Dupr\u00e9 <remi@dupre.io>\"]\n license = \"MIT OR Apache-2.0\"\n readme = \"README.md\"\ndiff --git a/compact-calendar/Cargo.toml b/compact-calendar/Cargo.toml\nindex 2230951..1a9a7b2 100644\n--- a/compact-calendar/Cargo.toml\n+++ b/compact-calendar/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"compact-calendar\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n authors = [\"R\u00e9mi Dupr\u00e9 <remi@dupre.io>\"]\n license = \"MIT OR Apache-2.0\"\n readme = \"README.md\"\ndiff --git a/opening-hours-syntax/Cargo.toml b/opening-hours-syntax/Cargo.toml\nindex 5a49a7c..97028da 100644\n--- a/opening-hours-syntax/Cargo.toml\n+++ b/opening-hours-syntax/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"opening-hours-syntax\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n authors = [\"R\u00e9mi Dupr\u00e9 <remi@dupre.io>\"]\n license = \"MIT OR Apache-2.0\"\n readme = \"README.md\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex f40a8c0..d17a333 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"opening-hours-py\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n description = \"A parser for the opening_hours fields from OpenStreetMap.\"\n authors = [\"R\u00e9mi Dupr\u00e9 <remi@dupre.io>\"]\n \ndiff --git a/python/Cargo.toml b/python/Cargo.toml\nindex f60794b..68ff345 100644\n--- a/python/Cargo.toml\n+++ b/python/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"opening-hours-py\"\n-version = \"0.8.2\"\n+version = \"0.8.3\"\n authors = [\"R\u00e9mi Dupr\u00e9 <remi@dupre.io>\"]\n license = \"MIT OR Apache-2.0\"\n readme = \"README.md\"\ndiff --git a/src/opening_hours.rs b/src/opening_hours.rs\nindex f221c6e..0a220a5 100644\n--- a/src/opening_hours.rs\n+++ b/src/opening_hours.rs\n@@ -315,7 +315,8 @@ impl<'d> TimeDomainIterator<'d> {\n                 assert!(next_change_hint > self.curr_date);\n                 self.curr_date = next_change_hint;\n \n-                if self.curr_date < self.end_datetime.date() {\n+                if self.curr_date <= self.end_datetime.date() && self.curr_date < DATE_LIMIT.date()\n+                {\n                     self.curr_schedule = self\n                         .opening_hours\n                         .schedule_at(self.curr_date)\n", "instance_id": "remi-dupre__opening-hours-rs-53", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue with the `opening_hours.intervals(start, end)` method. It includes specific examples of current behavior versus expected behavior, which helps in understanding the bug. The goal is evident: to ensure that intervals after the last midnight within the specified range are included. However, there are minor ambiguities, such as the lack of explicit mention of edge cases (e.g., how to handle time zones, daylight saving time, or very long ranges) and no clear specification on whether the fix should merge intervals or create separate ones (as shown in the two possible expected outputs). Additionally, constraints on input values or performance expectations are not mentioned. Despite these minor gaps, the problem is well-articulated with reproducible examples, making it mostly clear.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows a minimal change in a single file (`src/opening_hours.rs`), specifically a one-line modification to a condition in the `TimeDomainIterator` implementation. The change adjusts the logic to include intervals up to and including the end date, rather than stopping at the last midnight. This is a localized fix with no apparent impact on the broader system architecture or multiple modules. The version bumps across various files (Cargo.toml, pyproject.toml, etc.) are routine and do not add to the complexity of the solution.\n\n2. **Number of Technical Concepts:** Solving this requires a basic understanding of Rust, date-time handling (likely using the `chrono` crate as seen in dependencies), and the internal logic of the `TimeDomainIterator` for iterating over time intervals. The concept of date comparisons and scheduling logic is straightforward for someone familiar with date-time libraries. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic time interval handling are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases like time zones, daylight saving transitions, or extremely long date ranges, but the fix might need to consider whether the modified condition (`self.curr_date <= self.end_datetime.date()`) behaves correctly under such scenarios. However, the provided code change does not introduce new error handling logic, and the complexity of potential edge cases appears manageable within the existing framework. The additional check for `DATE_LIMIT` suggests some boundary consideration, but it does not significantly elevate the difficulty.\n\n4. **Overall Complexity:** The bug fix involves a small logical tweak to extend the iteration boundary, which is a relatively simple modification. It requires understanding the existing code's intent (stopping at midnight before the last day) and adjusting it to meet the expected behavior. The impact is limited to the `intervals` method's output, and there are no indications of performance concerns or architectural changes.\n\nGiven these factors, a score of 0.35 reflects an \"Easy\" problem that requires understanding some code logic and making a simple modification. It is slightly above the lower end of the easy range due to the need to understand date-time iteration logic and ensure the fix does not introduce unintended side effects with date boundaries.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "snapcraft has more strict linting unexpectedly \n### Bug Description\n\nWhen building a snap using at least revision 12523 to build snaps which built successfully with revisions like 12503 (latest/edge versus 8.x/edge), snapcraft will now emit some error messages.\n\n### To Reproduce\n\nInstall snapcraft from latest/edge, build the snap with `snapcraft`.\n\n### Environment\n\nUbuntu 23.10, building with `snapcraft --use-lxd`.\n\n### snapcraft.yaml\n\n```yaml\nname: my-snap-name # you probably want to 'snapcraft register <name>'\r\nbase: core24 # the base snap is the execution environment for this snap\r\nversion: '0.1' # just for humans, typically '1.2+git' or '1.3.2'\r\nsummary: Single-line elevator pitch for your amazing snap # 79 char long summary\r\ndescription: |\r\n  This is my-snap's description. You have a paragraph or two to tell the\r\n  most important story about your snap. Keep it under 100 words though,\r\n  we live in tweetspace and your description wants to look good in the snap\r\n  store.\r\n\r\ngrade: devel # must be 'stable' to release into candidate/stable channels\r\nconfinement: devmode # use 'strict' once you have the right plugs and slots\r\n\r\nparts:\r\n  my-part:\r\n    # See 'snapcraft plugins'\r\n    plugin: nil\r\n\r\napps:\r\n  my-app:\r\n    command: bash\r\n    environment:\r\n      foo: 1\n```\n\n\n### Relevant log output\n\n```shell\ndilyn@Ares:~/test -> snapcraft \r\nStarting snapcraft, version 8.3.2.post71+git3508cf39\r\nLogging execution to '/home/dilyn/.local/state/snapcraft/log/snapcraft-20240905-191836.634265.log'\r\nBad snapcraft.yaml content:\r\n- input should be a valid string (in field 'apps.my-app.environment.foo')\n```\n\n\n### Additional context\n\nQuoting `1` results in successful builds (sans an error message unrelated to this issue).\n", "patch": "diff --git a/requirements-devel.txt b/requirements-devel.txt\nindex 3db5bdf4ef..01712df5c4 100644\n--- a/requirements-devel.txt\n+++ b/requirements-devel.txt\n@@ -27,11 +27,11 @@ coverage==7.6.1\n craft-application==4.2.3\n craft-archives==2.0.0\n craft-cli==2.7.0\n-craft-grammar==2.0.0\n+craft-grammar==2.0.1\n craft-parts==2.1.1\n craft-platforms==0.1.1\n craft-providers==2.0.1\n-craft-store==3.0.0\n+craft-store==3.0.1\n cryptography==43.0.1\n cssutils==2.11.1\n dict2css==0.3.0.post1\ndiff --git a/requirements-docs.txt b/requirements-docs.txt\nindex ae23b0c27f..7e58aec50a 100644\n--- a/requirements-docs.txt\n+++ b/requirements-docs.txt\n@@ -22,11 +22,11 @@ colorama==0.4.6\n craft-application==4.2.3\n craft-archives==2.0.0\n craft-cli==2.7.0\n-craft-grammar==2.0.0\n+craft-grammar==2.0.1\n craft-parts==2.1.1\n craft-platforms==0.1.1\n craft-providers==2.0.1\n-craft-store==3.0.0\n+craft-store==3.0.1\n cryptography==43.0.1\n cssutils==2.11.1\n dict2css==0.3.0.post1\ndiff --git a/requirements.txt b/requirements.txt\nindex fab86ab254..33eb42e01b 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -10,11 +10,11 @@ click==8.1.7\n craft-application==4.2.3\n craft-archives==2.0.0\n craft-cli==2.7.0\n-craft-grammar==2.0.0\n+craft-grammar==2.0.1\n craft-parts==2.1.1\n craft-platforms==0.1.1\n craft-providers==2.0.1\n-craft-store==3.0.0\n+craft-store==3.0.1\n cryptography==43.0.1\n distro==1.9.0\n docutils==0.19\ndiff --git a/setup.py b/setup.py\nindex 7b2a06c73e..c1ed03edc2 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -101,11 +101,11 @@ def recursive_data_files(directory, install_directory):\n     \"craft-application~=4.1\",\n     \"craft-archives~=2.0\",\n     \"craft-cli~=2.6\",\n-    \"craft-grammar~=2.0\",\n+    \"craft-grammar>=2.0.1,<3.0.0\",\n     \"craft-parts~=2.1\",\n     \"craft-platforms~=0.1\",\n     \"craft-providers~=2.0\",\n-    \"craft-store~=3.0\",\n+    \"craft-store>=3.0.1,<4.0.0\",\n     \"docutils<0.20\",  # Frozen until we can update sphinx dependencies.\n     \"gnupg\",\n     \"jsonschema==2.5.1\",\n", "instance_id": "canonical__snapcraft-5051", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: snapcraft has introduced stricter linting in a newer revision (12523) compared to an older one (12503), resulting in an error when building a snap due to an invalid environment variable type in the snapcraft.yaml file. The goal is implied (fix the linting error), and the input (snapcraft.yaml) and output (successful build) are indirectly specified through the log output and additional context. The reproduction steps and environment details are provided, which adds clarity. However, there are minor ambiguities: the problem statement does not explicitly state the expected fix or whether the issue lies in snapcraft's validation logic or the user's configuration. Additionally, edge cases or broader implications of the stricter linting are not mentioned. The provided log output and additional context (quoting the value resolves the issue) help, but a more explicit statement of the root cause or expected solution would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling in the easy range (0.2-0.4). The issue revolves around a change in linting behavior in snapcraft, and the code changes provided are minimal, involving only version updates to dependencies (craft-grammar and craft-store) in requirements files and setup.py. The scope of the changes is narrow, affecting only configuration files rather than core logic or multiple modules, and does not impact the system's architecture. The technical concepts required are basic: understanding dependency management and versioning in Python projects. No complex algorithms, design patterns, or domain-specific knowledge are needed beyond familiarity with Python packaging. Edge cases and error handling are not a significant concern here, as the change is straightforward and the problem statement implies a simple fix (quoting the value in snapcraft.yaml, though the code changes focus on dependency updates). The primary challenge might be understanding why the dependency update resolves the linting issue, but this is still within the realm of basic debugging and dependency management. Overall, this is a simple task requiring minimal code modification and understanding.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "End of Life for python 3.8 support\nPython 3.8 security [end of life](https://peps.python.org/pep-0569/#lifespan) is scheduled for October 2024. MontePy should move past 3.8 by that point.\n\nThis should have minimal impact on code, and should only required metadata updates, and removing old tests. \n\nThis does bring up a question of should we jump from almost dead version to almost dead version or keep in the middle of the pack for python support.\nDecide how to test against Numpy 1 and 2\nNumpy 2.0 came out this June and is the first major release in a long time, which means some backwards compatibility is lost. Numpy 2.0 requires python 3.9. Due to this I saw that for python 3.8 we are testing against Numpy 1.24, and for Python >=3.9 we are testing against numpy 2.0. So right now we are accidentally testing against both major versions. However, this isn't a good spot to be.\r\n\r\nSo actions IMO:\r\n1. Research what breaks in 2.0\r\n2. Determine if this matters to us.\r\n3. Decide if we need to peg to Numpy 1.xxx or Numpy 2.xxx\r\n4. Decide if we need to do matrix testing against both major versions of numpy.\r\n\r\nThoughts @tjlaboss?\n", "patch": "diff --git a/.github/workflows/main.yml b/.github/workflows/main.yml\nindex 821d4633..c636a23f 100644\n--- a/.github/workflows/main.yml\n+++ b/.github/workflows/main.yml\n@@ -10,7 +10,7 @@ jobs:\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n-        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n+        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n \n     steps:\n       - uses: actions/checkout@v4\n@@ -36,15 +36,13 @@ jobs:\n       - run: pip install --user dist/*.tar.gz\n       - run: pip install --user . montepy[test]\n       - run: pip install --user . montepy[doc]\n-        if: ${{ matrix.python-version != '3.8'}}\n       - run: pip install --user . montepy[format]\n       - run: pip install --user . montepy[build]\n       - run: pip install --user . montepy[develop]\n-        if: ${{ matrix.python-version != '3.8'}}\n       - run: pip freeze\n       - name: Upload build artifacts\n         uses: actions/upload-artifact@v4.3.1\n-        if: ${{ matrix.python-version == '3.9' }}\n+        if: ${{ matrix.python-version == '3.12'  && matrix.numpy-version == '2.0' }}\n         with: \n            name: build\n            path: dist/*\n@@ -54,7 +52,8 @@ jobs:\n     permissions: write-all\n     strategy:\n       matrix:\n-        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n+        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n+        numpy-version: [\"1.0\", \"2.0\"]\n     \n     steps:\n       - uses: actions/checkout@v4\n@@ -62,27 +61,33 @@ jobs:\n         uses: actions/setup-python@v5\n         with: \n           python-version: ${{ matrix.python-version }}\n+      - run: pip install numpy~=${{ matrix.numpy-version }}\n+        name: install specific numpy version\n       - run: pip install --user . montepy[test]\n       - run: pip install --user . montepy[build]\n+      - run: pip uninstall -y pytest-profiling\n+        name: Uninstall incompatible library\n+        if: ${{ matrix.python-version == '3.13' }}\n+      - run: pip freeze\n       - run: coverage run -m pytest --junitxml=test_report.xml\n       - run: coverage report\n         if:  ${{ success() || failure() }}\n       - run: coverage xml\n         if:  ${{ success() || failure() }}\n       - name: Upload test report\n-        if: ${{ matrix.python-version == '3.9' && (success() || failure() )}}\n+        if: ${{ matrix.python-version == '3.12' && matrix.numpy-version == '2.0' && (success() || failure() )}}\n         uses: actions/upload-artifact@v4\n         with:\n           name: test\n           path: test_report.xml\n       - name: Upload coverage report\n-        if: ${{ matrix.python-version == '3.9' && (success() || failure() )}}\n+        if: ${{ matrix.python-version == '3.12' && matrix.numpy-version == '2.0' && (success() || failure() )}}\n         uses: actions/upload-artifact@v4\n         with:\n           name: coverage\n           path: coverage.xml\n       - name: Coveralls GitHub Action\n-        if: ${{ matrix.python-version == '3.9' && (success() || failure() )}}\n+        if: ${{ matrix.python-version == '3.12' && matrix.numpy-version == '2.0' && (success() || failure() )}}\n         uses: coverallsapp/github-action@v2\n         with:\n           file: coverage.xml\ndiff --git a/pyproject.toml b/pyproject.toml\nindex d863fc63..07b2ea5c 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -5,7 +5,7 @@ description = \"A library for reading, editing, and writing MCNP input files\"\n \n # gitlab limits the readme to 4,000 chars.\n readme = \"README.md\"\n-requires-python = \">=3.8\"\n+requires-python = \">=3.9\"\n maintainers = [\n \t{name = \"Micah Gale\", email = \"mgale@montepy.org\"}\n ]\n@@ -18,7 +18,6 @@ keywords = [\"MCNP\", \"neutronics\", \"imcnp\", \"input file\", \"monte carlo\", \"radiati\n license = {file=\"LICENSE\"}\n classifiers = [\n        \"Development Status :: 4 - Beta\",\n-       \"Programming Language :: Python :: 3 :: Only\",\n        \"Intended Audience :: Science/Research\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n@@ -27,7 +26,12 @@ classifiers = [\n        \"Topic :: Scientific/Engineering :: Physics\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Human Machine Interfaces\",\n-\n+       \"Programming Language :: Python :: 3 :: Only\",\n+       \"Programming Language :: Python :: 3.9\",\n+       \"Programming Language :: Python :: 3.10\",\n+       \"Programming Language :: Python :: 3.11\",\n+       \"Programming Language :: Python :: 3.12\",\n+       \"Programming Language :: Python :: 3.13\"\n ]\n dependencies = [\n \t\"numpy>=1.18\",\n", "instance_id": "idaholab__MontePy-548", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to phase out Python 3.8 support due to its end-of-life status and to address compatibility issues with Numpy versions. It outlines the primary goal of updating metadata and removing old tests, and it raises relevant questions about Python version support strategy and Numpy compatibility. However, there are minor ambiguities and missing details. For instance, the statement Ascending and Descending (1.0, 2.0) steps for Numpy compatibility research are mentioned but not detailed, leaving uncertainty about the specific scope of investigation required. Additionally, the problem statement lacks explicit mention of edge cases or specific constraints related to the transition (e.g., potential user impact or backward compatibility concerns beyond Numpy). While the overall direction is clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" category (0.2-0.4) due to the relatively straightforward nature of the code changes required. The modifications are primarily confined to configuration files (e.g., `.github/workflows/main.yml` and `pyproject.toml`), involving updates to Python version support (removing 3.8, adding 3.9 to 3.13) and introducing matrix testing for Numpy versions (1.0 and 2.0). The scope of changes is limited to a few files and does not impact the core architecture or logic of the codebase, as the problem statement suggests minimal code impact beyond metadata and test updates. \n\nHowever, there are some elements that slightly elevate the difficulty beyond \"Very Easy.\" The task requires understanding CI/CD workflows (GitHub Actions), Python versioning, and dependency management (e.g., Numpy compatibility). Additionally, there is a need for research into Numpy 2.0 breaking changes and decision-making about version support strategy, which adds a small layer of complexity. The code changes themselves are not technically challenging, but handling potential edge cases (e.g., ensuring compatibility across Python and Numpy versions, managing test coverage) requires some attention. Overall, this task demands a basic-to-intermediate understanding of Python ecosystem tools and practices, but it does not involve deep architectural changes or advanced technical concepts, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "contourf fails for face variables with tri/quad mixed topologies\nSince xugrid 0.12.0 contourf raises `\"ValueError: triangles are indices into the points and must be in the range 0 <= i < 6906 but found value -1\"`. This cannot be reproduced with xugrid test datasets. I expect that it only happens for datsets containing both quads and triangles (so -1 values in the face_node_connectivity) and maybe the fact that this variable is defined on the faces is also relevant.\r\n\r\n```python\r\nimport xugrid as xu\r\nfile_nc = 'Grevelingen-FM_0000_map.nc'\r\nuds = xu.open_dataset(file_nc)\r\nuds['mesh2d_flowelem_bl'].ugrid.plot.contourf()\r\n```\r\n\r\nThis code works fine with xugrid 0.11.2, so I expect something was changed in the code and this usecase is also not covered by tests.\ncontourf fails for face variables with tri/quad mixed topologies\nSince xugrid 0.12.0 contourf raises `\"ValueError: triangles are indices into the points and must be in the range 0 <= i < 6906 but found value -1\"`. This cannot be reproduced with xugrid test datasets. I expect that it only happens for datsets containing both quads and triangles (so -1 values in the face_node_connectivity) and maybe the fact that this variable is defined on the faces is also relevant.\r\n\r\n```python\r\nimport xugrid as xu\r\nfile_nc = 'Grevelingen-FM_0000_map.nc'\r\nuds = xu.open_dataset(file_nc)\r\nuds['mesh2d_flowelem_bl'].ugrid.plot.contourf()\r\n```\r\n\r\nThis code works fine with xugrid 0.11.2, so I expect something was changed in the code and this usecase is also not covered by tests.\n", "patch": "diff --git a/docs/changelog.rst b/docs/changelog.rst\nindex c364491dd..62c793d71 100644\n--- a/docs/changelog.rst\n+++ b/docs/changelog.rst\n@@ -6,6 +6,27 @@ All notable changes to this project will be documented in this file.\n The format is based on `Keep a Changelog`_, and this project adheres to\n `Semantic Versioning`_.\n \n+Unreleased\n+----------\n+\n+Fixed\n+~~~~~\n+\n+- Release 0.12.0 changed the return type of the face node connectivity of\n+  :attr:`xugrid.Ugrid2d.voronoi_topology` from a `scipy.sparse.coo_matrix` to\n+  an ordinary `np.array` of integers (and similarly for internal voronoi\n+  tesselations); this dense array had fill (hard-coded) values of -1,\n+  potentially differing from the grid's fill value. This lead to a number of\n+  errors for methods relying on voronoi tesselations (such as contour plots)\n+  if the fill value of the grid was not -1. Internally, a ``FILL_VALUE = -1``\n+  is now used everywhere in connectivity arrays, and fill values are no longer\n+  passed for internal methods; a value of -1 is always assumed. When converting\n+  the grid (back) to a dataset with :meth:`xugrid.Ugrid1d.to_dataset` or\n+  :meth:`xugrid.Ugrid2d.to_dataset`, the fill value is set back to its original\n+  value; the fill value is also set when calling\n+  :meth:`xugrid.UgridDataArrayAccessor.to_netcdf` or\n+  :meth:`xugrid.UgridDatasetAccessor.to_netcdf`.\n+\n [0.12.0] 2024-09-03\n -------------------\n \ndiff --git a/examples-dev/voronoi.py b/examples-dev/voronoi.py\nindex 758ea5a70..cd5779e9e 100644\n--- a/examples-dev/voronoi.py\n+++ b/examples-dev/voronoi.py\n@@ -19,11 +19,14 @@\n modules, should you not want to rely on more complex dependencies such as\n ``xugrid`` and ``xarray``.\n \"\"\"\n+# %%\n import matplotlib.pyplot as plt\n import matplotlib.tri as mtri\n import numpy as np\n from matplotlib.collections import LineCollection, PolyCollection\n \n+from xugrid.constants import FILL_VALUE  # equals -1\n+\n # %%\n # From xugrid, we need only import the ``connectivity`` and ``voronoi``\n # modules. The functions in these modules depend only on ``numpy`` and\n@@ -63,12 +66,12 @@ def generate_disk(partitions: int, depth: int):\n     return np.column_stack((x, y)), triang.triangles\n \n \n-def edge_plot(vertices, edge_nodes, ax, fill_value=-1, **kwargs):\n+def edge_plot(vertices, edge_nodes, ax, **kwargs):\n     n_edge = len(edge_nodes)\n     edge_coords = np.empty((n_edge, 2, 2), dtype=float)\n     node_0 = edge_nodes[:, 0]\n     node_1 = edge_nodes[:, 1]\n-    valid = (node_0 != fill_value) & (node_1 != fill_value)\n+    valid = (node_0 != FILL_VALUE) & (node_1 != FILL_VALUE)\n     node_0 = node_0[valid]\n     node_1 = node_1[valid]\n     edge_coords[:, 0, 0] = vertices[node_0, 0]\n@@ -81,10 +84,10 @@ def edge_plot(vertices, edge_nodes, ax, fill_value=-1, **kwargs):\n     return primitive\n \n \n-def face_plot(vertices, face_nodes, ax, fill_value=-1, **kwargs):\n+def face_plot(vertices, face_nodes, ax, **kwargs):\n     vertices = vertices[face_nodes]\n     # Replace fill value; PolyCollection ignores NaN.\n-    vertices[face_nodes == fill_value] = np.nan\n+    vertices[face_nodes == FILL_VALUE] = np.nan\n     collection = PolyCollection(vertices, **kwargs)\n     primitive = ax.add_collection(collection)\n     ax.autoscale()\n@@ -107,12 +110,12 @@ def comparison_plot(\n         sharex=True,\n     )\n \n-    edges0, _ = connectivity.edge_connectivity(faces0, -1)\n+    edges0, _ = connectivity.edge_connectivity(faces0)\n     edge_plot(vertices0, edges0, ax0, colors=\"black\")\n     ax0.scatter(*centroids0.T, color=\"red\")\n     ax0.scatter(*vertices0.T, color=\"black\")\n \n-    edges1, _ = connectivity.edge_connectivity(faces1, -1)\n+    edges1, _ = connectivity.edge_connectivity(faces1)\n     edge_plot(vertices0, edges0, ax1, colors=\"black\")\n     edge_plot(vertices1, edges1, ax1, colors=\"red\")\n \n@@ -126,13 +129,11 @@ def comparison_plot(\n # %%\n # Let's start by generating a simple unstructured mesh and use only its\n # centroids to generate a voronoi tesselation.\n-#\n-# Note: ``-1`` functions as the fill value in this example.\n \n vertices, faces = generate_disk(5, 2)\n centroids = vertices[faces].mean(axis=1)\n \n-node_face_connectivity = connectivity.invert_dense_to_sparse(faces, -1)\n+node_face_connectivity = connectivity.invert_dense_to_sparse(faces)\n voronoi_vertices, voronoi_faces, face_index, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n     vertices,\n@@ -158,17 +159,14 @@ def comparison_plot(\n # The ``voronoi_topology`` is capable of preserving the exterior exactly, but\n # this requires more topological information.\n \n-edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(\n-    faces, -1\n-)\n-edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity, -1)\n+edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(faces)\n+edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity)\n voronoi_vertices, voronoi_faces, face_index, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n     vertices,\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=True,\n )\n@@ -190,18 +188,15 @@ def comparison_plot(\n faces = connectivity.renumber(new)\n centroids = vertices[faces].mean(axis=1)\n \n-node_face_connectivity = connectivity.invert_dense_to_sparse(faces, -1)\n-edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(\n-    faces, -1\n-)\n-edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity, -1)\n+node_face_connectivity = connectivity.invert_dense_to_sparse(faces)\n+edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(faces)\n+edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity)\n voronoi_vertices, voronoi_faces, face_index, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n     vertices,\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=True,\n )\n@@ -217,18 +212,15 @@ def comparison_plot(\n # of the original mesh altogether. We still add an orthogonal projection of\n # every centroid to exterior edges.\n \n-node_face_connectivity = connectivity.invert_dense_to_sparse(faces, -1)\n-edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(\n-    faces, -1\n-)\n-edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity, -1)\n+node_face_connectivity = connectivity.invert_dense_to_sparse(faces)\n+edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(faces)\n+edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity)\n voronoi_vertices, voronoi_faces, face_index, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n     vertices,\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=False,\n )\n@@ -240,18 +232,15 @@ def comparison_plot(\n # exactly. Alternatively, we can choose to skip the exterior vertex if it\n # creates a concave face:\n \n-node_face_connectivity = connectivity.invert_dense_to_sparse(faces, -1)\n-edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(\n-    faces, -1\n-)\n-edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity, -1)\n+node_face_connectivity = connectivity.invert_dense_to_sparse(faces)\n+edge_node_connectivity, face_edge_connectivity = connectivity.edge_connectivity(faces)\n+edge_face_connectivity = connectivity.invert_dense(face_edge_connectivity)\n voronoi_vertices, voronoi_faces, face_index, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n     vertices,\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=True,\n     skip_concave=True,\n@@ -267,7 +256,7 @@ def comparison_plot(\n     vertices,\n     centroids,\n )\n-edges0, _ = connectivity.edge_connectivity(faces0, -1)\n+edges0, _ = connectivity.edge_connectivity(faces0)\n \n nodes1, faces1, face_index1, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n@@ -275,11 +264,10 @@ def comparison_plot(\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=False,\n )\n-edges1, _ = connectivity.edge_connectivity(faces1, -1)\n+edges1, _ = connectivity.edge_connectivity(faces1)\n \n nodes2, faces2, _, _ = voronoi.voronoi_topology(\n     node_face_connectivity,\n@@ -287,11 +275,10 @@ def comparison_plot(\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=True,\n )\n-edges2, _ = connectivity.edge_connectivity(faces2, -1)\n+edges2, _ = connectivity.edge_connectivity(faces2)\n \n nodes3, faces3, face_index3, node_map3 = voronoi.voronoi_topology(\n     node_face_connectivity,\n@@ -299,12 +286,11 @@ def comparison_plot(\n     centroids,\n     edge_face_connectivity=edge_face_connectivity,\n     edge_node_connectivity=edge_node_connectivity,\n-    fill_value=-1,\n     add_exterior=True,\n     add_vertices=True,\n     skip_concave=True,\n )\n-edges3, _ = connectivity.edge_connectivity(faces3, -1)\n+edges3, _ = connectivity.edge_connectivity(faces3)\n \n fig, axes = plt.subplots(\n     nrows=1,\n@@ -342,10 +328,10 @@ def comparison_plot(\n # fourth option, since it includes some vertices of the original mesh, which\n # are connected to multiple faces.\n \n-triangles0, face_triangles0 = connectivity.triangulate(faces0, -1)\n+triangles0, face_triangles0 = connectivity.triangulate(faces0)\n triangulation0 = mtri.Triangulation(nodes0[:, 0], nodes0[:, 1], triangles0)\n \n-triangles1, face_triangles1 = connectivity.triangulate(faces1, -1)\n+triangles1, face_triangles1 = connectivity.triangulate(faces1)\n triangulation1 = mtri.Triangulation(nodes1[:, 0], nodes1[:, 1], triangles1)\n \n \n@@ -422,3 +408,5 @@ def comparison_plot(\n \n ax0.set_xlim(-1.5, 1.5)\n ax0.set_ylim(-1.5, 1.5)\n+\n+# %%\ndiff --git a/xugrid/constants.py b/xugrid/constants.py\nindex befb2a387..4df367236 100644\n--- a/xugrid/constants.py\n+++ b/xugrid/constants.py\n@@ -22,6 +22,10 @@\n PolygonArray = np.ndarray\n SparseMatrix = Union[coo_matrix, csr_matrix]\n \n+# Internally we always use a fill value of -1. This ensures we can always index\n+# with the fill value as well, since any array will have at least size 1.\n+FILL_VALUE = -1\n+\n \n class Point(NamedTuple):\n     x: float\ndiff --git a/xugrid/conversion.py b/xugrid/conversion.py\nindex 465fcc46c..9b1e76ec5 100644\n--- a/xugrid/conversion.py\n+++ b/xugrid/conversion.py\n@@ -11,6 +11,7 @@\n import xarray as xr\n \n from xugrid.constants import (\n+    FILL_VALUE,\n     FloatArray,\n     IntArray,\n     IntDType,\n@@ -48,9 +49,9 @@ def edges_to_linestrings(\n \n \n def faces_to_polygons(\n-    x: FloatArray, y: FloatArray, face_node_connectivity: IntArray, fill_value: int\n+    x: FloatArray, y: FloatArray, face_node_connectivity: IntArray\n ) -> PolygonArray:\n-    is_data = face_node_connectivity != fill_value\n+    is_data = face_node_connectivity != FILL_VALUE\n     m_per_row = is_data.sum(axis=1)\n     i = np.repeat(np.arange(len(face_node_connectivity)), m_per_row)\n     c = face_node_connectivity.ravel()[is_data.ravel()]\n@@ -98,7 +99,6 @@ def polygons_to_faces(\n     n = len(polygons)\n     m_per_row = np.bincount(indices)\n     m = m_per_row.max()\n-    fill_value = -1\n     # Allocate 2D array and create a flat view of the dense connectivity\n     conn = np.empty((n, m), dtype=IntDType)\n     flat_conn = conn.ravel()\n@@ -108,10 +108,10 @@ def polygons_to_faces(\n         valid = slice(None)  # a[:] equals a[slice(None)]\n     else:\n         valid = ragged_index(n, m, m_per_row).ravel()\n-        flat_conn[~valid] = fill_value\n+        flat_conn[~valid] = FILL_VALUE\n     flat_conn[valid] = inverse\n     x, y = contiguous_xy(unique)\n-    return x, y, conn, fill_value\n+    return x, y, conn\n \n \n def _scalar_spacing(coords, spacing):\ndiff --git a/xugrid/plot/plot.py b/xugrid/plot/plot.py\nindex cc6a425ba..7f573b2b7 100644\n--- a/xugrid/plot/plot.py\n+++ b/xugrid/plot/plot.py\n@@ -511,7 +511,7 @@ def pcolormesh(grid, da, ax, **kwargs):\n         raise ValueError(\"pcolormesh only supports data on faces\")\n \n     nodes = grid.node_coordinates\n-    faces, _ = close_polygons(grid.face_node_connectivity, grid.fill_value)\n+    faces, _ = close_polygons(grid.face_node_connectivity)\n     vertices = nodes[faces]\n \n     # PolyCollection takes a norm, but not vmin, vmax.\ndiff --git a/xugrid/regrid/unstructured.py b/xugrid/regrid/unstructured.py\nindex ab8a6a3e6..b823af4ad 100644\n--- a/xugrid/regrid/unstructured.py\n+++ b/xugrid/regrid/unstructured.py\n@@ -152,7 +152,6 @@ def barycentric(self, other):\n             grid.centroids,\n             edge_face_connectivity=grid.edge_face_connectivity,\n             edge_node_connectivity=grid.edge_node_connectivity,\n-            fill_value=grid.fill_value,\n             add_exterior=True,\n             add_vertices=True,\n             skip_concave=True,\ndiff --git a/xugrid/ugrid/connectivity.py b/xugrid/ugrid/connectivity.py\nindex 82d66c38b..d5088554b 100644\n--- a/xugrid/ugrid/connectivity.py\n+++ b/xugrid/ugrid/connectivity.py\n@@ -7,7 +7,14 @@\n import pandas as pd\n from scipy import sparse\n \n-from xugrid.constants import BoolArray, FloatArray, IntArray, IntDType, SparseMatrix\n+from xugrid.constants import (\n+    FILL_VALUE,\n+    BoolArray,\n+    FloatArray,\n+    IntArray,\n+    IntDType,\n+    SparseMatrix,\n+)\n \n \n def argsort_rows(array: np.ndarray) -> IntArray:\n@@ -215,10 +222,10 @@ def contract_vertices(A: sparse.csr_matrix, indices: IntArray) -> IntArray:\n \n # Conversion between dense and sparse\n # -----------------------------------\n-def _to_ij(conn: IntArray, fill_value: int, invert: bool) -> Tuple[IntArray, IntArray]:\n+def _to_ij(conn: IntArray, invert: bool) -> Tuple[IntArray, IntArray]:\n     n, m = conn.shape\n     j = conn.ravel()\n-    valid = j != fill_value\n+    valid = j != FILL_VALUE\n     i = np.repeat(np.arange(n), m)[valid]\n     j = j[valid]\n     if invert:\n@@ -227,10 +234,8 @@ def _to_ij(conn: IntArray, fill_value: int, invert: bool) -> Tuple[IntArray, Int\n         return i, j\n \n \n-def _to_sparse(\n-    conn: IntArray, fill_value: int, invert: bool, sort_indices: bool\n-) -> sparse.csr_matrix:\n-    i, j = _to_ij(conn, fill_value, invert)\n+def _to_sparse(conn: IntArray, invert: bool, sort_indices: bool) -> sparse.csr_matrix:\n+    i, j = _to_ij(conn, invert)\n     coo_content = (j, (i, j))\n     coo_matrix = sparse.coo_matrix(coo_content)\n     csr_matrix = coo_matrix.tocsr()\n@@ -270,13 +275,11 @@ def ragged_index(n: int, m: int, m_per_row: IntArray) -> BoolArray:\n     return (column_number.T < m_per_row).T\n \n \n-def to_sparse(\n-    conn: IntArray, fill_value: int, sort_indices: bool = True\n-) -> sparse.csr_matrix:\n-    return _to_sparse(conn, fill_value, invert=False, sort_indices=sort_indices)\n+def to_sparse(conn: IntArray, sort_indices: bool = True) -> sparse.csr_matrix:\n+    return _to_sparse(conn, invert=False, sort_indices=sort_indices)\n \n \n-def to_dense(conn: SparseMatrix, fill_value: int, n_columns: int = None) -> IntArray:\n+def to_dense(conn: SparseMatrix, n_columns: int = None) -> IntArray:\n     n, _ = conn.shape\n     m_per_row = conn.getnnz(axis=1)\n     m = m_per_row.max()\n@@ -296,7 +299,7 @@ def to_dense(conn: SparseMatrix, fill_value: int, n_columns: int = None) -> IntA\n         valid = slice(None)  # a[:] equals a[slice(None)]\n     else:\n         valid = ragged_index(n, m, m_per_row).ravel()\n-        flat_conn[~valid] = fill_value\n+        flat_conn[~valid] = FILL_VALUE\n \n     if isinstance(conn, sparse.csr_matrix):\n         flat_conn[valid] = conn.indices\n@@ -310,18 +313,14 @@ def to_dense(conn: SparseMatrix, fill_value: int, n_columns: int = None) -> IntA\n # Inverting connectivities\n # ------------------------\n def invert_dense_to_sparse(\n-    conn: IntArray, fill_value: int, sort_indices: bool = True\n+    conn: IntArray, sort_indices: bool = True\n ) -> sparse.csr_matrix:\n-    return _to_sparse(conn, fill_value, invert=True, sort_indices=sort_indices)\n+    return _to_sparse(conn, invert=True, sort_indices=sort_indices)\n \n \n-def invert_dense(\n-    conn: IntArray, fill_value: int, sort_indices: bool = True\n-) -> IntArray:\n-    sparse_inverted = _to_sparse(\n-        conn, fill_value, invert=True, sort_indices=sort_indices\n-    )\n-    return to_dense(sparse_inverted, fill_value)\n+def invert_dense(conn: IntArray, sort_indices: bool = True) -> IntArray:\n+    sparse_inverted = _to_sparse(conn, invert=True, sort_indices=sort_indices)\n+    return to_dense(sparse_inverted)\n \n \n def invert_sparse(conn: sparse.csr_matrix) -> sparse.csr_matrix:\n@@ -333,9 +332,9 @@ def invert_sparse(conn: sparse.csr_matrix) -> sparse.csr_matrix:\n     return inverted.tocsr()\n \n \n-def invert_sparse_to_dense(conn: sparse.csr_matrix, fill_value: int) -> IntArray:\n+def invert_sparse_to_dense(conn: sparse.csr_matrix) -> IntArray:\n     inverted = invert_sparse(conn)\n-    return to_dense(inverted, fill_value)\n+    return to_dense(inverted)\n \n \n # Renumbering\n@@ -353,51 +352,46 @@ def _renumber(a: IntArray) -> IntArray:\n     return dense.reshape(a.shape)\n \n \n-def renumber(a: IntArray, fill_value: int = None):\n-    if fill_value is None:\n-        return _renumber(a)\n-\n-    valid = a != fill_value\n-    renumbered = np.full_like(a, fill_value)\n+def renumber(a: IntArray):\n+    valid = a != FILL_VALUE\n+    renumbered = np.full_like(a, FILL_VALUE)\n     renumbered[valid] = _renumber(a[valid])\n     return renumbered\n \n \n-def close_polygons(face_node_connectivity: IntArray, fill_value: int) -> IntArray:\n+def close_polygons(face_node_connectivity: IntArray) -> IntArray:\n     # Wrap around and create closed polygon: put the first node at the end of the row\n     # In case of fill values, replace all fill values\n     n, m = face_node_connectivity.shape\n-    closed = np.full((n, m + 1), fill_value, dtype=IntDType)\n+    closed = np.full((n, m + 1), FILL_VALUE, dtype=IntDType)\n     closed[:, :-1] = face_node_connectivity\n     first_node = face_node_connectivity[:, 0]\n     # Identify fill value, and replace by first node also\n-    isfill = closed == fill_value\n+    isfill = closed == FILL_VALUE\n     closed.ravel()[isfill.ravel()] = np.repeat(first_node, isfill.sum(axis=1))\n     return closed, isfill\n \n \n-def reverse_orientation(face_node_connectivity: IntArray, fill_value: int):\n+def reverse_orientation(face_node_connectivity: IntArray):\n     # We cannot simply reverse the rows with [:, ::-1], since there may be fill\n     # values present.\n     reversed_orientation = face_node_connectivity.copy()\n     in_reverse = face_node_connectivity[:, ::-1]\n-    in_reverse = in_reverse[in_reverse != fill_value]\n-    replace = face_node_connectivity != fill_value\n+    in_reverse = in_reverse[in_reverse != FILL_VALUE]\n+    replace = face_node_connectivity != FILL_VALUE\n     reversed_orientation[replace] = in_reverse\n     return reversed_orientation\n \n \n-def counterclockwise(\n-    face_node_connectivity: IntArray, fill_value: int, nodes: FloatArray\n-) -> IntArray:\n+def counterclockwise(face_node_connectivity: IntArray, nodes: FloatArray) -> IntArray:\n     # TODO: in case of \"periodic grids\", we need to wrap around the grid.\n-    closed, _ = close_polygons(face_node_connectivity, fill_value)\n+    closed, _ = close_polygons(face_node_connectivity)\n     p = nodes[closed]\n     dxy = np.diff(p, axis=1)\n     reverse = (np.cross(dxy[:, :-1], dxy[:, 1:])).sum(axis=1) < 0\n     ccw = face_node_connectivity.copy()\n     if reverse.any():\n-        ccw[reverse] = reverse_orientation(face_node_connectivity[reverse], fill_value)\n+        ccw[reverse] = reverse_orientation(face_node_connectivity[reverse])\n     return ccw\n \n \n@@ -405,24 +399,22 @@ def counterclockwise(\n # ----------------------\n def boundary_node_connectivity(\n     edge_face_connectivity: IntArray,\n-    fill_value: int,\n     edge_node_connectivity: IntArray,\n ) -> IntArray:\n     \"\"\"Is a subset of the edge_node_connectivity\"\"\"\n-    is_boundary = (edge_face_connectivity == fill_value).any(axis=1)\n+    is_boundary = (edge_face_connectivity == FILL_VALUE).any(axis=1)\n     return edge_node_connectivity[is_boundary]\n \n \n def edge_connectivity(\n     face_node_connectivity: IntArray,\n-    fill_value: int,\n     edge_node_connectivity=None,\n ) -> Tuple[IntArray, IntArray]:\n     \"\"\"Derive new edge_node_connectivity and face_edge_connectivity.\"\"\"\n     prior = edge_node_connectivity\n     n, m = face_node_connectivity.shape\n     # Close the polygons: [0 1 2 3] -> [0 1 2 3 0]\n-    closed, isfill = close_polygons(face_node_connectivity, fill_value)\n+    closed, isfill = close_polygons(face_node_connectivity)\n     # Allocate array for edge_node_connectivity: includes duplicate edges\n     edge_node_connectivity = np.empty((n * m, 2), dtype=IntDType)\n     edge_node_connectivity[:, 0] = closed[:, :-1].ravel()\n@@ -449,7 +441,7 @@ def edge_connectivity(\n         edge_node_connectivity = prior\n \n     # Create face_edge_connectivity\n-    face_edge_connectivity = np.full((n, m), fill_value, dtype=np.int64)\n+    face_edge_connectivity = np.full((n, m), FILL_VALUE, dtype=np.int64)\n     isnode = ~isfill[:, :-1]\n     face_edge_connectivity[isnode] = inverse_indices\n     return edge_node_connectivity, face_edge_connectivity\n@@ -457,14 +449,13 @@ def edge_connectivity(\n \n def validate_edge_node_connectivity(\n     face_node_connectivity: IntArray,\n-    fill_value: int,\n     edge_node_connectivity: IntArray,\n ) -> BoolArray:\n     \"\"\"\n     * Is the edge defined by the face_node_connectivity?\n     * Are the edges unique?\n     \"\"\"\n-    new, _ = edge_connectivity(face_node_connectivity, fill_value)\n+    new, _ = edge_connectivity(face_node_connectivity)\n     old = np.sort(edge_node_connectivity, axis=1)\n \n     new1d = new.astype(np.int32).view(np.int64).ravel()\n@@ -485,7 +476,6 @@ def validate_edge_node_connectivity(\n \n def face_face_connectivity(\n     edge_face_connectivity: IntArray,\n-    fill_value: int,\n ) -> sparse.csr_matrix:\n     \"\"\"\n     Derive face to face connectivity.\n@@ -495,7 +485,7 @@ def face_face_connectivity(\n     \"\"\"\n     i = edge_face_connectivity[:, 0]\n     j = edge_face_connectivity[:, 1]\n-    is_connection = j != fill_value\n+    is_connection = j != FILL_VALUE\n     i = i[is_connection]\n     j = j[is_connection]\n     edge_index = np.arange(len(edge_face_connectivity))[is_connection]\n@@ -557,12 +547,11 @@ def structured_connectivity(active: IntArray) -> AdjacencyMatrix:\n \n def perimeter(\n     face_node_connectivity: IntArray,\n-    fill_value: int,\n     node_x: FloatArray,\n     node_y: FloatArray,\n ):\n     nodes = np.column_stack([node_x, node_y])\n-    closed, _ = close_polygons(face_node_connectivity, fill_value)\n+    closed, _ = close_polygons(face_node_connectivity)\n     coordinates = nodes[closed]\n     # Shift coordinates to avoid precision loss\n     xy0 = coordinates[:, 0]\n@@ -583,19 +572,17 @@ def area_from_coordinates(\n \n def area(\n     face_node_connectivity: IntArray,\n-    fill_value: int,\n     node_x: FloatArray,\n     node_y: FloatArray,\n ) -> FloatArray:\n     nodes = np.column_stack([node_x, node_y])\n-    closed, _ = close_polygons(face_node_connectivity, fill_value)\n+    closed, _ = close_polygons(face_node_connectivity)\n     coordinates = nodes[closed]\n     return area_from_coordinates(coordinates)\n \n \n def centroids(\n     face_node_connectivity: IntArray,\n-    fill_value: int,\n     node_x: FloatArray,\n     node_y: FloatArray,\n ) -> FloatArray:\n@@ -610,7 +597,7 @@ def centroids(\n         # This is mathematically equivalent to triangulating, computing triangle centroids\n         # and computing the area weighted average of those centroids\n         centroid_coordinates = np.empty((n_face, 2), dtype=np.float64)\n-        closed, _ = close_polygons(face_node_connectivity, fill_value)\n+        closed, _ = close_polygons(face_node_connectivity)\n         coordinates = nodes[closed]\n         # Express coordinates relative to first node\n         xy0 = coordinates[:, 0]\n@@ -644,7 +631,6 @@ def _circumcenters_triangle(xxx: FloatArray, yyy: FloatArray):\n \n def circumcenters(\n     face_node_connectivity: IntArray,\n-    fill_value: int,\n     node_x: FloatArray,\n     node_y: FloatArray,\n ) -> FloatArray:\n@@ -684,16 +670,14 @@ def _triangulate(i: IntArray, j: IntArray, n_triangle_per_row: IntArray) -> IntA\n     return triangles\n \n \n-def triangulate_dense(\n-    face_node_connectivity: IntArray, fill_value: int\n-) -> Tuple[IntArray, IntArray]:\n+def triangulate_dense(face_node_connectivity: IntArray) -> Tuple[IntArray, IntArray]:\n     n_face, n_max = face_node_connectivity.shape\n \n     if n_max == 3:\n         triangles = face_node_connectivity.copy()\n         return triangles, np.arange(n_face)\n \n-    valid = face_node_connectivity != fill_value\n+    valid = face_node_connectivity != FILL_VALUE\n     n_per_row = valid.sum(axis=1)\n     n_triangle_per_row = n_per_row - 2\n     i = np.repeat(np.arange(n_face), n_per_row)\n@@ -727,9 +711,7 @@ def triangulate_coo(\n     return triangles, triangle_face_connectivity\n \n \n-def triangulate(\n-    face_node_connectivity, fill_value: int | None = None\n-) -> Tuple[IntArray, IntArray]:\n+def triangulate(face_node_connectivity) -> Tuple[IntArray, IntArray]:\n     \"\"\"\n     Convert polygons into its constituent triangles.\n \n@@ -747,9 +729,7 @@ def triangulate(\n     triangle_face_connectivity: ndarray of integers with shape ``(n_triangle,)``\n     \"\"\"\n     if isinstance(face_node_connectivity, IntArray):\n-        if fill_value is None:\n-            raise ValueError(\"fill_value is required for dense connectivity\")\n-        return triangulate_dense(face_node_connectivity, fill_value)\n+        return triangulate_dense(face_node_connectivity)\n     elif isinstance(face_node_connectivity, sparse.coo_matrix):\n         return triangulate_coo(face_node_connectivity)\n     else:\ndiff --git a/xugrid/ugrid/partitioning.py b/xugrid/ugrid/partitioning.py\nindex e2a64e038..de6e253ae 100644\n--- a/xugrid/ugrid/partitioning.py\n+++ b/xugrid/ugrid/partitioning.py\n@@ -6,7 +6,7 @@\n import numpy as np\n import xarray as xr\n \n-from xugrid.constants import IntArray, IntDType\n+from xugrid.constants import FILL_VALUE, IntArray, IntDType\n from xugrid.core.wrap import UgridDataArray, UgridDataset\n from xugrid.ugrid.connectivity import renumber\n from xugrid.ugrid.ugridbase import UgridType\n@@ -115,17 +115,17 @@ def _merge_connectivity(gathered, slices):\n     return merged, indexes\n \n \n-def merge_faces(grids, node_inverse, fill_value: int = -1):\n+def merge_faces(grids, node_inverse):\n     node_offsets = tuple(accumulate([0] + [grid.n_node for grid in grids]))\n     n_face = [grid.n_face for grid in grids]\n     n_max_node = max(grid.n_max_node_per_face for grid in grids)\n     slices = (0,) + tuple(accumulate(n_face))\n \n-    all_faces = np.full((sum(n_face), n_max_node), fill_value, dtype=IntDType)\n+    all_faces = np.full((sum(n_face), n_max_node), FILL_VALUE, dtype=IntDType)\n     for grid, face_offset, node_offset in zip(grids, slices, node_offsets):\n         faces = grid.face_node_connectivity\n         n_face, n_node_per_face = faces.shape\n-        valid = faces != grid.fill_value\n+        valid = faces != FILL_VALUE\n         all_faces[face_offset : face_offset + n_face, :n_node_per_face][\n             valid\n         ] = node_inverse[faces[valid] + node_offset]\ndiff --git a/xugrid/ugrid/polygonize.py b/xugrid/ugrid/polygonize.py\nindex 8a88b5a69..6e19c74a2 100644\n--- a/xugrid/ugrid/polygonize.py\n+++ b/xugrid/ugrid/polygonize.py\n@@ -3,7 +3,7 @@\n import numpy as np\n from scipy import sparse\n \n-from xugrid.constants import IntArray\n+from xugrid.constants import FILL_VALUE, IntArray\n \n \n def _bbox_area(bounds):\n@@ -11,7 +11,7 @@ def _bbox_area(bounds):\n \n \n def _classify(\n-    fill_value: int, i: IntArray, j: IntArray, face_values: np.ndarray\n+    i: IntArray, j: IntArray, face_values: np.ndarray\n ) -> Tuple[int, IntArray]:\n     \"\"\"\n     Find out how many discrete polygons are created. Identify the connectivity,\n@@ -19,8 +19,6 @@ def _classify(\n \n     Parameters\n     ----------\n-    fill_value: int\n-        Fill value in j: marks exterior edges.\n     i: np.ndarray of int\n         First face of the edge.\n     j: np.ndarray of int\n@@ -40,7 +38,7 @@ def _classify(\n     # For labelling, only those parts of the mesh that have the same value\n     # should be connected with each other.\n     # Since we dropped NaN values before, we needn't worry about those.\n-    is_connection = (i != fill_value) & (j != fill_value) & (vi == vj)\n+    is_connection = (i != FILL_VALUE) & (j != FILL_VALUE) & (vi == vj)\n     i = i[is_connection]\n     j = j[is_connection]\n     ij = np.concatenate([i, j])\n@@ -94,8 +92,7 @@ def polygonize(uda: \"UgridDataArray\") -> \"gpd.GeoDataFrame\":  # type: ignore # n\n     face_values = dropped.to_numpy()\n     grid = dropped.ugrid.grid\n     i, j = grid.edge_face_connectivity.T\n-    fill_value = grid.fill_value\n-    n_polygon, polygon_id = _classify(fill_value, i, j, face_values)\n+    n_polygon, polygon_id = _classify(i, j, face_values)\n \n     # Now we identify for each label the subset of edges. These are the\n     # \"exterior\" edges: either the exterior edge of the mesh identified by a\n@@ -106,8 +103,8 @@ def polygonize(uda: \"UgridDataArray\") -> \"gpd.GeoDataFrame\":  # type: ignore # n\n     vj = polygon_id[j]\n     # Ensure that no result thas has been created by indexing with the\n     # fill_value remains. Since polygon_id starts counting a 0, we may use -1.\n-    vi[i == fill_value] = -1\n-    vj[j == fill_value] = -1\n+    vi[i == FILL_VALUE] = FILL_VALUE\n+    vj[j == FILL_VALUE] = FILL_VALUE\n     boundary = vi != vj\n \n     polygons = []\ndiff --git a/xugrid/ugrid/snapping.py b/xugrid/ugrid/snapping.py\nindex 53b93e29e..b8a842144 100644\n--- a/xugrid/ugrid/snapping.py\n+++ b/xugrid/ugrid/snapping.py\n@@ -402,7 +402,7 @@ def create_snap_to_grid_dataframe(\n     vertices = topology.node_coordinates\n     edge_centroids = topology.edge_coordinates\n     face_edge_connectivity = topology.face_edge_connectivity\n-    A = connectivity.to_sparse(face_edge_connectivity, fill_value=-1)\n+    A = connectivity.to_sparse(face_edge_connectivity)\n     n, m = A.shape\n     face_edge_connectivity = AdjacencyMatrix(A.indices, A.indptr, A.nnz, n, m)\n \ndiff --git a/xugrid/ugrid/ugrid1d.py b/xugrid/ugrid/ugrid1d.py\nindex 16ab0dcd6..4d8f74bf9 100644\n--- a/xugrid/ugrid/ugrid1d.py\n+++ b/xugrid/ugrid/ugrid1d.py\n@@ -9,6 +9,7 @@\n import xugrid\n from xugrid import conversion\n from xugrid.constants import (\n+    FILL_VALUE,\n     BoolArray,\n     FloatArray,\n     FloatDType,\n@@ -133,7 +134,6 @@ def from_dataset(cls, dataset: xr.Dataset, topology: str = None):\n             + list(connectivity.values())\n             + list(chain.from_iterable(chain.from_iterable(coordinates.values())))\n         )\n-        fill_value = -1\n \n         # Take the first coordinates by default.\n         # They can be reset with .set_node_coords()\n@@ -144,7 +144,7 @@ def from_dataset(cls, dataset: xr.Dataset, topology: str = None):\n \n         edge_nodes = connectivity[\"edge_node_connectivity\"]\n         edge_node_connectivity = cls._prepare_connectivity(\n-            ds[edge_nodes], fill_value, dtype=IntDType\n+            ds[edge_nodes], FILL_VALUE, dtype=IntDType\n         ).to_numpy()\n \n         indexes[\"node_x\"] = x_index\n@@ -154,7 +154,7 @@ def from_dataset(cls, dataset: xr.Dataset, topology: str = None):\n         return cls(\n             node_x_coordinates,\n             node_y_coordinates,\n-            fill_value,\n+            FILL_VALUE,\n             edge_node_connectivity,\n             name=topology,\n             dataset=dataset[ugrid_vars],\n@@ -211,7 +211,7 @@ def from_meshkernel(\n         return cls(\n             mesh.node_x,\n             mesh.node_y,\n-            fill_value=-1,\n+            fill_value=FILL_VALUE,\n             edge_node_connectivity=mesh.edge_nodes.reshape((-1, 2)),\n             name=name,\n             projected=projected,\n@@ -389,8 +389,7 @@ def from_shapely(geometry: LineArray, crs=None) -> \"Ugrid1d\":\n             )\n \n         x, y, edge_node_connectivity = conversion.linestrings_to_edges(geometry)\n-        fill_value = -1\n-        return Ugrid1d(x, y, fill_value, edge_node_connectivity, crs=crs)\n+        return Ugrid1d(x, y, FILL_VALUE, edge_node_connectivity, crs=crs)\n \n     def to_pygeos(self, dim):\n         from warnings import warn\ndiff --git a/xugrid/ugrid/ugrid2d.py b/xugrid/ugrid/ugrid2d.py\nindex 1ea8c936c..377c1b3e9 100644\n--- a/xugrid/ugrid/ugrid2d.py\n+++ b/xugrid/ugrid/ugrid2d.py\n@@ -16,6 +16,7 @@\n from xugrid import conversion\n from xugrid import meshkernel_utils as mku\n from xugrid.constants import (\n+    FILL_VALUE,\n     BoolArray,\n     FloatArray,\n     FloatDType,\n@@ -104,17 +105,18 @@ def __init__(\n         self.projected = projected\n \n         if isinstance(face_node_connectivity, np.ndarray):\n-            face_node_connectivity = face_node_connectivity\n+            self.face_node_connectivity = face_node_connectivity.copy()\n         elif isinstance(face_node_connectivity, (coo_matrix, csr_matrix)):\n-            face_node_connectivity = connectivity.to_dense(\n-                face_node_connectivity, fill_value\n-            )\n+            self.face_node_connectivity = connectivity.to_dense(face_node_connectivity)\n         else:\n             raise TypeError(\n                 \"face_node_connectivity should be an array of integers or a sparse matrix\"\n             )\n \n-        self.face_node_connectivity = face_node_connectivity\n+        # Ensure the fill value is FILL_VALUE (-1)\n+        self.face_node_connectivity[\n+            self.face_node_connectivity == self.fill_value\n+        ] = FILL_VALUE\n \n         # TODO: do this in validation instead. While UGRID conventions demand it,\n         # where does it go wrong?\n@@ -225,15 +227,14 @@ def from_meshkernel(\n         \"\"\"\n         n_face = len(mesh.nodes_per_face)\n         n_max_node = mesh.nodes_per_face.max()\n-        fill_value = -1\n-        face_node_connectivity = np.full((n_face, n_max_node), fill_value)\n+        face_node_connectivity = np.full((n_face, n_max_node), FILL_VALUE)\n         isnode = connectivity.ragged_index(n_face, n_max_node, mesh.nodes_per_face)\n         face_node_connectivity[isnode] = mesh.face_nodes\n         edge_node_connectivity = np.reshape(mesh.edge_nodes, (-1, 2))\n         return cls(\n             node_x=mesh.node_x,\n             node_y=mesh.node_y,\n-            fill_value=fill_value,\n+            fill_value=FILL_VALUE,\n             face_node_connectivity=face_node_connectivity,\n             edge_node_connectivity=edge_node_connectivity,\n             name=name,\n@@ -330,14 +331,14 @@ def to_dataset(\n         data_vars = {\n             self.name: 0,\n             face_nodes: xr.DataArray(\n-                data=self.face_node_connectivity,\n+                data=self._set_fillvalue(self.face_node_connectivity),\n                 attrs=face_nodes_attrs,\n                 dims=(self.face_dimension, nmax_node_dim),\n             ),\n         }\n         if self.edge_node_connectivity is not None or optional_attributes:\n             data_vars[edge_nodes] = xr.DataArray(\n-                data=self.edge_node_connectivity,\n+                data=self.edge_node_connectivity,  # has no fill values\n                 attrs=edge_nodes_attrs,\n                 dims=(self.edge_dimension, \"two\"),\n             )\n@@ -346,28 +347,29 @@ def to_dataset(\n             face_faces, face_faces_attrs = self._get_name_and_attrs(\"face_face\")\n             edge_faces, edge_faces_attrs = self._get_name_and_attrs(\"edge_face\")\n             bound_nodes, bound_nodes_attrs = self._get_name_and_attrs(\"boundary_node\")\n-            fill_value = self.fill_value\n             boundary_edge_dim = self._attrs[\"boundary_edge_dimension\"]\n \n             data_vars[face_edges] = xr.DataArray(\n-                data=self.face_edge_connectivity,\n+                data=self._set_fillvalue(self.face_edge_connectivity),\n                 attrs=face_edges_attrs,\n                 dims=(self.face_dimension, nmax_node_dim),\n             )\n             data_vars[face_faces] = xr.DataArray(\n-                data=connectivity.to_dense(\n-                    self.face_face_connectivity, fill_value, self.n_max_node_per_face\n+                data=self._set_fillvalue(\n+                    connectivity.to_dense(\n+                        self.face_face_connectivity, self.n_max_node_per_face\n+                    )\n                 ),\n                 attrs=face_faces_attrs,\n                 dims=(self.face_dimension, nmax_node_dim),\n             )\n             data_vars[edge_faces] = xr.DataArray(\n-                data=self.edge_face_connectivity,\n+                data=self._set_fillvalue(self.edge_face_connectivity),\n                 attrs=edge_faces_attrs,\n                 dims=(self.edge_dimension, \"two\"),\n             )\n             data_vars[bound_nodes] = xr.DataArray(\n-                data=self.boundary_node_connectivity,\n+                data=self._set_fillvalue(self.boundary_node_connectivity),\n                 attrs=bound_nodes_attrs,\n                 dims=(boundary_edge_dim, \"two\"),\n             )\n@@ -407,7 +409,7 @@ def n_max_node_per_face(self) -> int:\n \n     @property\n     def n_node_per_face(self) -> IntArray:\n-        return (self.face_node_connectivity != self.fill_value).sum(axis=1)\n+        return (self.face_node_connectivity != FILL_VALUE).sum(axis=1)\n \n     @property\n     def core_dimension(self):\n@@ -460,7 +462,6 @@ def _edge_connectivity(self):\n             self._face_edge_connectivity,\n         ) = connectivity.edge_connectivity(\n             self.face_node_connectivity,\n-            self.fill_value,\n             self._edge_node_connectivity,\n         )\n \n@@ -507,7 +508,6 @@ def boundary_node_connectivity(self) -> IntArray:\n         if self._boundary_node_connectivity is None:\n             self._boundary_node_connectivity = connectivity.boundary_node_connectivity(\n                 self.edge_face_connectivity,\n-                self.fill_value,\n                 self.edge_node_connectivity,\n             )\n         return self._boundary_node_connectivity\n@@ -524,7 +524,6 @@ def centroids(self) -> FloatArray:\n         if self._centroids is None:\n             self._centroids = connectivity.centroids(\n                 self.face_node_connectivity,\n-                self.fill_value,\n                 self.node_x,\n                 self.node_y,\n             )\n@@ -539,7 +538,6 @@ def circumcenters(self):\n         if self._circumcenters is None:\n             self._circumcenters = connectivity.circumcenters(\n                 self.face_node_connectivity,\n-                self.fill_value,\n                 self.node_x,\n                 self.node_y,\n             )\n@@ -551,7 +549,6 @@ def area(self) -> FloatArray:\n         if self._area is None:\n             self._area = connectivity.area(\n                 self.face_node_connectivity,\n-                self.fill_value,\n                 self.node_x,\n                 self.node_y,\n             )\n@@ -563,7 +560,6 @@ def perimeter(self) -> FloatArray:\n         if self._perimeter is None:\n             self._perimeter = connectivity.perimeter(\n                 self.face_node_connectivity,\n-                self.fill_value,\n                 self.node_x,\n                 self.node_y,\n             )\n@@ -581,7 +577,7 @@ def face_bounds(self):\n         \"\"\"\n         x = self.node_x[self.face_node_connectivity]\n         y = self.node_y[self.face_node_connectivity]\n-        isfill = self.face_node_connectivity == self.fill_value\n+        isfill = self.face_node_connectivity == FILL_VALUE\n         x[isfill] = np.nan\n         y[isfill] = np.nan\n         return np.column_stack(\n@@ -628,7 +624,7 @@ def face_node_coordinates(self) -> FloatArray:\n         coords = np.full(\n             (self.n_face, self.n_max_node_per_face, 2), np.nan, dtype=FloatDType\n         )\n-        is_node = self.face_node_connectivity != self.fill_value\n+        is_node = self.face_node_connectivity != FILL_VALUE\n         index = self.face_node_connectivity[is_node]\n         coords[is_node, :] = self.node_coordinates[index]\n         return coords\n@@ -639,7 +635,7 @@ def edge_face_connectivity(self) -> IntArray:\n         Edge to face connectivity. An edge may belong to a single face\n         (exterior edge), or it may be shared by two faces (interior edge).\n \n-        An exterior edge will contain a ``fill_value`` for the second column.\n+        An exterior edge will contain a FILL_VALUE of -1 for the second column.\n \n         Returns\n         -------\n@@ -647,7 +643,7 @@ def edge_face_connectivity(self) -> IntArray:\n         \"\"\"\n         if self._edge_face_connectivity is None:\n             self._edge_face_connectivity = connectivity.invert_dense(\n-                self.face_edge_connectivity, self.fill_value\n+                self.face_edge_connectivity\n             )\n         return self._edge_face_connectivity\n \n@@ -667,7 +663,7 @@ def face_face_connectivity(self) -> csr_matrix:\n         \"\"\"\n         if self._face_face_connectivity is None:\n             self._face_face_connectivity = connectivity.face_face_connectivity(\n-                self.edge_face_connectivity, self.fill_value\n+                self.edge_face_connectivity\n             )\n         return self._face_face_connectivity\n \n@@ -682,7 +678,7 @@ def node_face_connectivity(self):\n         \"\"\"\n         if self._node_face_connectivity is None:\n             self._node_face_connectivity = connectivity.invert_dense_to_sparse(\n-                self.face_node_connectivity, self.fill_value\n+                self.face_node_connectivity\n             )\n         return self._node_face_connectivity\n \n@@ -739,7 +735,7 @@ def mesh(self) -> \"mk.Mesh2d\":  # type: ignore # noqa\n         import meshkernel as mk\n \n         edge_nodes = self.edge_node_connectivity.ravel().astype(np.int32)\n-        is_node = self.face_node_connectivity != self.fill_value\n+        is_node = self.face_node_connectivity != FILL_VALUE\n         nodes_per_face = is_node.sum(axis=1).astype(np.int32)\n         face_nodes = self.face_node_connectivity[is_node].ravel().astype(np.int32)\n \n@@ -792,7 +788,6 @@ def voronoi_topology(self):\n                 self.centroids,\n                 self.edge_face_connectivity,\n                 self.edge_node_connectivity,\n-                self.fill_value,\n                 add_exterior=True,\n                 add_vertices=False,\n             )\n@@ -816,7 +811,7 @@ def centroid_triangulation(self):\n         \"\"\"\n         if self._centroid_triangulation is None:\n             nodes, faces, face_index = self.voronoi_topology\n-            triangles, _ = connectivity.triangulate(faces, self.fill_value)\n+            triangles, _ = connectivity.triangulate(faces)\n             triangulation = (nodes[:, 0].copy(), nodes[:, 1].copy(), triangles)\n             self._centroid_triangulation = (triangulation, face_index)\n         return self._centroid_triangulation\n@@ -835,7 +830,7 @@ def triangulation(self):\n         \"\"\"\n         if self._triangulation is None:\n             triangles, triangle_face_connectivity = connectivity.triangulate(\n-                self.face_node_connectivity, self.fill_value\n+                self.face_node_connectivity\n             )\n             triangulation = (self.node_x, self.node_y, triangles)\n             self._triangulation = (triangulation, triangle_face_connectivity)\n@@ -851,7 +846,7 @@ def exterior_edges(self) -> IntArray:\n         edge_index: 1d array of integers\n         \"\"\"\n         # Numpy argwhere doesn't return a 1D array\n-        return np.nonzero(self.edge_face_connectivity[:, 1] == self.fill_value)[0]\n+        return np.nonzero(self.edge_face_connectivity[:, 1] == FILL_VALUE)[0]\n \n     @property\n     def exterior_faces(self) -> IntArray:\n@@ -864,7 +859,7 @@ def exterior_faces(self) -> IntArray:\n         \"\"\"\n         exterior_edges = self.exterior_edges\n         exterior_faces = self.edge_face_connectivity[exterior_edges].ravel()\n-        return np.unique(exterior_faces[exterior_faces != self.fill_value])\n+        return np.unique(exterior_faces[exterior_faces != FILL_VALUE])\n \n     @property\n     def celltree(self):\n@@ -875,7 +870,7 @@ def celltree(self):\n         \"\"\"\n         if self._celltree is None:\n             self._celltree = CellTree2d(\n-                self.node_coordinates, self.face_node_connectivity, self.fill_value\n+                self.node_coordinates, self.face_node_connectivity, FILL_VALUE\n             )\n         return self._celltree\n \n@@ -904,7 +899,6 @@ def validate_edge_node_connectivity(self):\n         \"\"\"\n         return connectivity.validate_edge_node_connectivity(\n             self.face_node_connectivity,\n-            self.fill_value,\n             self.edge_node_connectivity,\n         )\n \n@@ -1121,8 +1115,8 @@ def topology_subset(\n         index = face_index.to_numpy()\n         face_subset = self.face_node_connectivity[index]\n         node_index = np.unique(face_subset.ravel())\n-        node_index = node_index[node_index != self.fill_value]\n-        new_faces = connectivity.renumber(face_subset, self.fill_value)\n+        node_index = node_index[node_index != FILL_VALUE]\n+        new_faces = connectivity.renumber(face_subset)\n         node_x = self.node_x[node_index]\n         node_y = self.node_y[node_index]\n \n@@ -1130,7 +1124,7 @@ def topology_subset(\n         new_edges = None\n         if self.edge_node_connectivity is not None:\n             edge_index = np.unique(self.face_edge_connectivity[index].ravel())\n-            edge_index = edge_index[edge_index != self.fill_value]\n+            edge_index = edge_index[edge_index != FILL_VALUE]\n             edge_subset = self.edge_node_connectivity[edge_index]\n             new_edges = connectivity.renumber(edge_subset)\n \n@@ -1214,7 +1208,7 @@ def isel(self, indexers=None, return_index=False, **indexers_kwargs):\n         if edgedim in indexers:\n             edge_index = indexers[edgedim]\n             index = np.unique(self.edge_face_connectivity[edge_index])\n-            face_index[edgedim] = index[index != self.fill_value]\n+            face_index[edgedim] = index[index != FILL_VALUE]\n         if facedim in indexers:\n             face_index[facedim] = indexers[facedim]\n \n@@ -1604,11 +1598,8 @@ def merge_partitions(\n \n         # Grab a sample grid\n         grid = next(iter(grids))\n-        fill_value = grid.fill_value\n         node_coordinates, node_indexes, node_inverse = partitioning.merge_nodes(grids)\n-        new_faces, face_indexes = partitioning.merge_faces(\n-            grids, node_inverse, fill_value\n-        )\n+        new_faces, face_indexes = partitioning.merge_faces(grids, node_inverse)\n         indexes = {\n             grid.node_dimension: node_indexes,\n             grid.face_dimension: face_indexes,\n@@ -1622,7 +1613,7 @@ def merge_partitions(\n \n         merged_grid = Ugrid2d(\n             *node_coordinates.T,\n-            fill_value,\n+            FILL_VALUE,\n             new_faces,\n             name=grid.name,\n             edge_node_connectivity=new_edges,\n@@ -1862,10 +1853,8 @@ def triangulate(self):\n         -------\n         triangles: Ugrid2d\n         \"\"\"\n-        triangles, _ = connectivity.triangulate(\n-            self.face_node_connectivity, self.fill_value\n-        )\n-        return Ugrid2d(self.node_x, self.node_y, self.fill_value, triangles)\n+        triangles, _ = connectivity.triangulate(self.face_node_connectivity)\n+        return Ugrid2d(self.node_x, self.node_y, FILL_VALUE, triangles)\n \n     def _tesselate_voronoi(self, centroids, add_exterior, add_vertices, skip_concave):\n         if add_exterior:\n@@ -1881,12 +1870,11 @@ def _tesselate_voronoi(self, centroids, add_exterior, add_vertices, skip_concave\n             centroids,\n             edge_face_connectivity,\n             edge_node_connectivity,\n-            self.fill_value,\n             add_exterior,\n             add_vertices,\n             skip_concave,\n         )\n-        return Ugrid2d(vertices[:, 0], vertices[:, 1], self.fill_value, faces)\n+        return Ugrid2d(vertices[:, 0], vertices[:, 1], FILL_VALUE, faces)\n \n     def tesselate_centroidal_voronoi(\n         self, add_exterior=True, add_vertices=True, skip_concave=False\n@@ -2092,10 +2080,8 @@ def from_shapely(geometry: PolygonArray, crs=None) -> \"Ugrid2d\":\n                 \"geometry contains other types of geometries.\"\n             )\n \n-        x, y, face_node_connectivity, fill_value = conversion.polygons_to_faces(\n-            geometry\n-        )\n-        return Ugrid2d(x, y, fill_value, face_node_connectivity, crs=crs)\n+        x, y, face_node_connectivity = conversion.polygons_to_faces(geometry)\n+        return Ugrid2d(x, y, FILL_VALUE, face_node_connectivity, crs=crs)\n \n     @staticmethod\n     def _from_intervals_helper(\n@@ -2295,7 +2281,6 @@ def to_shapely(self, dim):\n                 self.node_x,\n                 self.node_y,\n                 self.face_node_connectivity,\n-                self.fill_value,\n             )\n         elif dim == self.node_dimension:\n             return conversion.nodes_to_points(\ndiff --git a/xugrid/ugrid/ugridbase.py b/xugrid/ugrid/ugridbase.py\nindex 8a7295e6a..77c940f8f 100644\n--- a/xugrid/ugrid/ugridbase.py\n+++ b/xugrid/ugrid/ugridbase.py\n@@ -10,7 +10,7 @@\n from numpy.typing import ArrayLike\n from scipy.sparse import csr_matrix\n \n-from xugrid.constants import BoolArray, FloatArray, IntArray\n+from xugrid.constants import FILL_VALUE, BoolArray, FloatArray, IntArray\n from xugrid.ugrid import connectivity, conventions\n \n \n@@ -483,6 +483,12 @@ def _prepare_connectivity(\n             raise ValueError(\"connectivity contains negative values\")\n         return da.copy(data=cast)\n \n+    def _set_fillvalue(self, connectivity: IntArray) -> IntArray:\n+        c = connectivity.copy()\n+        if self.fill_value != FILL_VALUE:\n+            c[c == FILL_VALUE] = self.fill_value\n+        return c\n+\n     def _precheck(self, multi_index):\n         dim, index = multi_index.popitem()\n         for check_dim, check_index in multi_index.items():\n@@ -646,7 +652,7 @@ def node_edge_connectivity(self) -> csr_matrix:\n         \"\"\"\n         if self._node_edge_connectivity is None:\n             self._node_edge_connectivity = connectivity.invert_dense_to_sparse(\n-                self.edge_node_connectivity, self.fill_value\n+                self.edge_node_connectivity\n             )\n         return self._node_edge_connectivity\n \ndiff --git a/xugrid/ugrid/voronoi.py b/xugrid/ugrid/voronoi.py\nindex 84dee3d7d..7edb057b9 100644\n--- a/xugrid/ugrid/voronoi.py\n+++ b/xugrid/ugrid/voronoi.py\n@@ -20,7 +20,7 @@\n import pandas as pd\n from scipy import sparse\n \n-from xugrid.constants import X_EPSILON, FloatArray, IntArray\n+from xugrid.constants import FILL_VALUE, X_EPSILON, FloatArray, IntArray\n from xugrid.ugrid.connectivity import (\n     area_from_coordinates,\n     close_polygons,\n@@ -61,7 +61,7 @@ def _create_face_node_connectivity(i: IntArray, j: IntArray) -> IntArray:\n     n = len(n_vertex)\n     m = n_vertex.max()\n     index = ragged_index(n, m, n_vertex)\n-    face_node_connectivity = np.full((n, m), -1)\n+    face_node_connectivity = np.full((n, m), FILL_VALUE)\n     face_node_connectivity[index] = j\n     return face_node_connectivity\n \n@@ -79,10 +79,9 @@ def interior_centroids(\n     node_face_connectivity: sparse.csr_matrix,\n     edge_face_connectivity: IntArray,\n     edge_node_connectivity: IntArray,\n-    fill_value: int,\n ):\n     # Find exterior nodes associated with interior edges\n-    is_exterior = edge_face_connectivity[:, 1] == fill_value\n+    is_exterior = edge_face_connectivity[:, 1] == FILL_VALUE\n     exterior_nodes = np.unique(edge_node_connectivity[is_exterior].ravel())\n     m_per_node = node_face_connectivity.getnnz(axis=1)\n     is_interior_only = m_per_node > 1\n@@ -146,12 +145,11 @@ def _interpolate_between_projections(\n def exterior_vertices(\n     edge_face_connectivity: IntArray,\n     edge_node_connectivity: IntArray,\n-    fill_value: int,\n     vertices: FloatArray,\n     centroids: FloatArray,\n     add_vertices: bool,\n ):\n-    is_exterior = edge_face_connectivity[:, 1] == fill_value\n+    is_exterior = edge_face_connectivity[:, 1] == FILL_VALUE\n     exterior_nodes = edge_node_connectivity[is_exterior]\n     # For every exterior node, project the centroids to exterior edges\n     edge_vertices = vertices[exterior_nodes]\n@@ -211,7 +209,7 @@ def choose_convex(\n     faces = np.full((n, m), -1)\n     faces[index] = j\n     # Close the polygons so we can easily compute areas.\n-    closed, _ = close_polygons(faces, -1)\n+    closed, _ = close_polygons(faces)\n     # Make a copy and insert the original vertices.\n     modified_nodes = nodes.copy()\n     modified_nodes[-n_interpolated:] = original_vertices\n@@ -232,7 +230,6 @@ def exterior_topology(\n     edge_face_connectivity: IntArray,\n     edge_node_connectivity: IntArray,\n     node_face_connectivity: sparse.csr_matrix,\n-    fill_value: int,\n     vertices: FloatArray,\n     centroids: FloatArray,\n     add_vertices: bool,\n@@ -278,7 +275,6 @@ def exterior_topology(\n         node_face_connectivity,\n         edge_face_connectivity,\n         edge_node_connectivity,\n-        fill_value,\n     )\n     i1, j1 = exterior_centroids(node_face_connectivity)\n     (\n@@ -291,7 +287,6 @@ def exterior_topology(\n     ) = exterior_vertices(\n         edge_face_connectivity,\n         edge_node_connectivity,\n-        fill_value,\n         vertices,\n         centroids,\n         add_vertices,\n@@ -337,7 +332,6 @@ def voronoi_topology(\n     centroids: FloatArray,\n     edge_face_connectivity: IntArray = None,\n     edge_node_connectivity: IntArray = None,\n-    fill_value: int = None,\n     add_exterior: bool = False,\n     add_vertices: bool = False,\n     skip_concave: bool = False,\n@@ -377,8 +371,6 @@ def voronoi_topology(\n     centroids: ndarray of floats with shape ``(n_centroid, 2)``\n     edge_face_connectivity: ndarray of integers with shape ``(n_edge, 2)``, optional\n     edge_node_connectivity: ndarray of integers with shape ``(n_edge, 2)``, optional\n-    fill_value: int, optional\n-        Fill value for edge_face_connectivity.\n     add_exterior: bool, optional\n         Whether to consider exterior edges of the original mesh, or to consider\n         exclusively centroids.\n@@ -401,12 +393,9 @@ def voronoi_topology(\n         interpolated.\n     \"\"\"\n     if add_exterior:\n-        if any(\n-            arg is None\n-            for arg in [edge_face_connectivity, edge_node_connectivity, fill_value]\n-        ):\n+        if any(arg is None for arg in [edge_face_connectivity, edge_node_connectivity]):\n             raise ValueError(\n-                \"edge_face_connectivity, edge_node_connectivity, fill_value \"\n+                \"edge_face_connectivity, edge_node_connectivity \"\n                 \"must be provided if add_exterior is True.\"\n             )\n \n@@ -415,7 +404,7 @@ def voronoi_topology(\n     # take any valid internal polygon we can construct: at least a triangle.\n     ncol_per_row = node_face_connectivity.getnnz(axis=1)\n     if add_exterior:\n-        is_exterior = edge_face_connectivity[:, 1] == fill_value\n+        is_exterior = edge_face_connectivity[:, 1] == FILL_VALUE\n         exterior_nodes = edge_node_connectivity[is_exterior]\n         valid = np.full(len(vertices), True)\n         valid[exterior_nodes.ravel()] = False\n@@ -449,7 +438,6 @@ def voronoi_topology(\n             edge_face_connectivity,\n             edge_node_connectivity,\n             node_face_connectivity,\n-            fill_value,\n             vertices,\n             centroids,\n             add_vertices,\n", "instance_id": "Deltares__xugrid-299", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `contourf` method in the `xugrid` library fails with a specific error (`ValueError` related to invalid triangle indices) for datasets with mixed triangle and quadrilateral topologies since version 0.12.0. It provides a reproducible code snippet and specifies that the issue does not occur with test datasets, hinting at a specific use case involving mixed topologies and face-defined variables. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or output for the `contourf` method in this context, nor does it provide detailed constraints or edge cases beyond the mixed topology. Additionally, the lack of a direct reference to the specific dataset (`Grevelingen-FM_0000_map.nc`) or its structure limits full reproducibility for someone unfamiliar with the data. Overall, while the issue is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and modules in the `xugrid` library, including core connectivity logic, Voronoi tessellation, and plotting functionalities. The changes involve replacing a hardcoded fill value (`-1`) with a constant (`FILL_VALUE`) across the codebase, ensuring consistency in handling connectivity arrays, and updating methods to respect the original grid fill values during dataset conversion. This requires a deep understanding of the library's architecture and the interplay between sparse and dense data representations, as well as Voronoi tessellation algorithms.\n\nSecond, the number of technical concepts involved is substantial. Solving this requires knowledge of Python, NumPy, SciPy (sparse matrices), and domain-specific concepts related to unstructured grid topologies (e.g., face-node connectivity, edge-face connectivity). Familiarity with plotting libraries like Matplotlib (for `contourf`) and the internal handling of mixed topologies (triangles and quads) is also necessary. Additionally, understanding the implications of fill values on indexing and visualization adds to the complexity.\n\nThird, while the problem statement does not explicitly mention edge cases beyond mixed topologies, the code changes address potential issues with fill value mismatches and ensure proper handling during dataset export. This implies a need to consider edge cases like grids with different fill values or invalid connectivity data, which adds to the problem's depth.\n\nOverall, the task demands a solid grasp of the codebase's internals and careful modifications to avoid introducing new bugs, especially in a library dealing with complex geometric data structures. I rate it at 0.65, reflecting a hard problem that requires significant expertise but does not reach the extreme complexity of system-level or highly domain-specific challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add an AnemoiMlflowClient with authentication support\n### Is your feature request related to a problem? Please describe.\r\n\r\nIf you want to use the mlflow python API on a server with token authentication, you currently have to manually insert the authentication calls before every mlflow call, like this:\r\n\r\n```python \r\nimport mlflow\r\nfrom anemoi.training.diagnostics.mlflow.auth import TokenAuth\r\n\r\ntracking_uri = \"https://mlflow.url\"\r\n\r\nauth = TokenAuth(tracking_uri)\r\nclient = mlflow.tracking.MlflowClient(tracking_uri)\r\n\r\n# check if we already have a login token on disk\r\n# this can also be done from the command line with `anemoi-training mlflow login`\r\nauth.login()\r\n\r\n# get an access token\r\nauth.authenticate()\r\n# do mlflow stuff\r\nclient.search_experiments()\r\n\r\n# authenticate again to be sure the token didn't expire since last time\r\nauth.authenticate()\r\n# do more mlflow stuff\r\nclient.log_metric(...)\r\n```\r\n\r\n\r\n### Describe the solution you'd like\r\n\r\nA custom `AnemoiMlflowClient` that inherits from `mlflow.tracking.MlflowClient` , that injects authentication calls before every mlflow call.\r\n\r\n```python\r\nfrom anemoi.training.somewhere import AnemoiMlflowClient\r\n\r\nclient = AnemoiMlflowClient(tracking_uri, authentication=True)\r\n\r\n# do mlflow stuff with login + authentication built in\r\nclient.search_experiments()\r\nclient.log_metric(...)\r\n```\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 8826b89a..3ca30a81 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -26,6 +26,7 @@ Keep it human-readable, your future self will thank you!\n - Feature: Add configurable models [#50](https://github.com/ecmwf/anemoi-training/pulls/50)\n - Feature: Authentication support for mlflow sync - [#51](https://github.com/ecmwf/anemoi-training/pull/51)\n - Feature: Support training for datasets with missing time steps [#48](https://github.com/ecmwf/anemoi-training/pulls/48)\n+- Feature: `AnemoiMlflowClient`, an mlflow client with authentication support [#86](https://github.com/ecmwf/anemoi-training/pull/86)\n - Long Rollout Plots\n \n ### Fixed\ndiff --git a/src/anemoi/training/diagnostics/mlflow/client.py b/src/anemoi/training/diagnostics/mlflow/client.py\nnew file mode 100644\nindex 00000000..5c49f929\n--- /dev/null\n+++ b/src/anemoi/training/diagnostics/mlflow/client.py\n@@ -0,0 +1,56 @@\n+# (C) Copyright 2024 European Centre for Medium-Range Weather Forecasts.\n+# This software is licensed under the terms of the Apache Licence Version 2.0\n+# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n+# In applying this licence, ECMWF does not waive the privileges and immunities\n+# granted to it by virtue of its status as an intergovernmental organisation\n+# nor does it submit to any jurisdiction.\n+\n+from __future__ import annotations\n+\n+from typing import Any\n+\n+from mlflow import MlflowClient\n+\n+from anemoi.training.diagnostics.mlflow.auth import TokenAuth\n+from anemoi.training.diagnostics.mlflow.utils import health_check\n+\n+\n+class AnemoiMlflowClient(MlflowClient):\n+    \"\"\"Anemoi extension of the MLflow client with token authentication support.\"\"\"\n+\n+    def __init__(\n+        self,\n+        tracking_uri: str,\n+        *args,\n+        authentication: bool = False,\n+        check_health: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        \"\"\"Behaves like a normal `mlflow.MlflowClient` but with token authentication injected on every call.\n+\n+        Parameters\n+        ----------\n+        tracking_uri : str\n+            The URI of the MLflow tracking server.\n+        authentication : bool, optional\n+            Enable token authentication, by default False\n+        check_health : bool, optional\n+            Check the health of the MLflow server on init, by default True\n+        *args : Any\n+            Additional arguments to pass to the MLflow client.\n+        **kwargs : Any\n+            Additional keyword arguments to pass to the MLflow client.\n+\n+        \"\"\"\n+        self.anemoi_auth = TokenAuth(tracking_uri, enabled=authentication)\n+        if check_health:\n+            super().__getattribute__(\"anemoi_auth\").authenticate()\n+            health_check(tracking_uri)\n+        super().__init__(tracking_uri, *args, **kwargs)\n+\n+    def __getattribute__(self, name: str) -> Any:\n+        \"\"\"Intercept attribute access and inject authentication.\"\"\"\n+        attr = super().__getattribute__(name)\n+        if callable(attr) and name != \"anemoi_auth\":\n+            super().__getattribute__(\"anemoi_auth\").authenticate()\n+        return attr\n", "instance_id": "ecmwf__anemoi-training-86", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding an `AnemoiMlflowClient` that extends the `mlflow.tracking.MlflowClient` to handle token authentication automatically before every MLflow API call. It provides a practical example of the current manual authentication process and contrasts it with the desired solution, which helps in understanding the intent. The input (tracking URI, authentication flag) and expected behavior (automatic authentication) are implied through the code snippets. However, there are minor ambiguities: the problem statement does not explicitly mention edge cases like token expiration during a call, failure of authentication, or how to handle authentication errors. Additionally, constraints or requirements for performance (e.g., minimizing authentication calls) are not specified. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are confined to a single new file (`client.py`) with a moderate amount of code (56 lines). It does not impact other parts of the codebase or require architectural changes. The implementation involves subclassing `MlflowClient` and overriding `__getattribute__` to inject authentication, which is a straightforward modification.\n\n2. **Technical Concepts Involved**: The solution requires understanding of Python's object-oriented programming (inheritance and method overriding via `__getattribute__`), basic interaction with the MLflow library, and integration with an existing `TokenAuth` class for authentication. These concepts are not particularly complex for a developer familiar with Python and third-party libraries like MLflow. No advanced algorithms, design patterns, or domain-specific knowledge beyond MLflow usage are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases like authentication failures or token expiration mid-operation, and the provided code does not implement specific error handling beyond invoking `authenticate()` before each call. The `health_check` functionality is included but is optional and simple. Handling more complex edge cases (e.g., retry logic or token refresh failures) is not required based on the current scope, though it could be a future consideration.\n\n4. **Overall Complexity**: The task requires understanding a specific library (MLflow) and implementing a wrapper around it, which is a common pattern in Python. The logic to inject authentication is simple, relying on Python's dynamic attribute access. There are no significant performance or scalability concerns evident in the problem or solution.\n\nGiven these points, the task is relatively straightforward, involving a focused change with moderate library-specific knowledge but no deep architectural or complex logical challenges. A score of 0.35 reflects an \"Easy\" problem that requires some understanding of the MLflow library and Python's dynamic features but is not overly demanding.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Explicitly reexport package names\nWhen in strict mode, mypy objects to imports from transitions with errors like\r\n```\r\nerror: Module \"transitions.extensions\" does not explicitly export attribute \"LockedMachine\"; implicit reexport disabled  [attr-defined]\r\n```\r\nThis occurs because the ``__init__.py`` files import names into the package namespace, but do not explicitly reexport them.\r\n\r\nThe fix is for ``__init__.py`` files to provide an explicit ``__all__``.\r\n\r\nMeanwhile, downstream packages can workaround this by adding to ``mypy.ini``\r\n```\r\n[mypy-transitions]\r\nimplicit_reexport = True\r\n```\n", "patch": "diff --git a/Changelog.md b/Changelog.md\nindex 348fe5a6..a9414b0e 100644\n--- a/Changelog.md\n+++ b/Changelog.md\n@@ -13,8 +13,11 @@\n - Bug #626: Process exceptions with `Machine.on_exception` in may_<trigger> as well (thanks @match1)\n - PR #666: Improved handling of removing transitions for `GraphMachine` and `HiearachicalMachine` (thanks @drpjm)\n - Typing:\n+  + Added `--strict` mypy flag for `transitions` type checks (not `tests` though)\n   + Made state, event and machine property mandatory in (Nested)EventData\n   + Transition.dest may be None\n+  + Introduced (Async)CallbackFunc to declutter callback-related signatures\n+  + Add stub file for extension package for explicite reexport (thanks @DrewDevereux)\n \n ## 0.9.0 (September 2022)\n \ndiff --git a/transitions/__init__.py b/transitions/__init__.py\nindex 763a364c..28e11b9e 100644\n--- a/transitions/__init__.py\n+++ b/transitions/__init__.py\n@@ -9,7 +9,7 @@\n from .version import __version__\n from .core import (State, Transition, Event, EventData, Machine, MachineError)\n \n-__copyright__ = \"Copyright (c) 2021 Tal Yarkoni, Alexander Neumann\"\n+__copyright__ = \"Copyright (c) 2024 Tal Yarkoni, Alexander Neumann\"\n __license__ = \"MIT\"\n-__summary__ = \"A lightweight, object-oriented finite state machine in Python\"\n-__uri__ = \"https://github.com/tyarkoni/transitions\"\n+__summary__ = \"A lightweight, object-oriented finite state machine implementation in Python with many extensions\"\n+__uri__ = \"https://github.com/pytransitions/transitions\"\ndiff --git a/transitions/core.pyi b/transitions/core.pyi\nindex db9b75ed..b216bfa3 100644\n--- a/transitions/core.pyi\n+++ b/transitions/core.pyi\n@@ -10,13 +10,14 @@ from enum import Enum, EnumMeta\n \n _LOGGER: Logger\n \n-Callback = Union[str, Callable]\n+CallbackFunc = Callable[..., Optional[bool]]\n+Callback = Union[str, CallbackFunc]\n CallbackList = List[Callback]\n CallbacksArg = Optional[Union[Callback, CallbackList]]\n-ModelState = Union[str, Enum, List]\n+ModelState = Union[str, Enum, List[\"ModelState\"]]\n ModelParameter = Union[Union[Literal['self'], Any], List[Union[Literal['self'], Any]]]\n \n-def listify(obj: Union[None, list, tuple, EnumMeta, Any]) -> Union[list, tuple, EnumMeta]: ...\n+def listify(obj: Union[None, List[Any], Tuple[Any], EnumMeta, Any]) -> Union[List[Any], Tuple[Any], EnumMeta]: ...\n \n def _prep_ordered_arg(desired_length: int, arguments: CallbacksArg) -> CallbackList: ...\n \n@@ -89,7 +90,7 @@ class Event:\n     transitions: DefaultDict[str, List[Transition]]\n     def __init__(self, name: str, machine: Machine) -> None: ...\n     def add_transition(self, transition: Transition) -> None: ...\n-    def trigger(self, model: object, *args: List, **kwargs: Dict) -> bool: ...\n+    def trigger(self, model: object, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n     def _trigger(self, event_data: EventData) -> bool: ...\n     def _process(self, event_data: EventData) -> bool: ...\n     def _is_valid_source(self, state: State) -> bool: ...\n@@ -105,7 +106,7 @@ class Machine:\n     event_cls: Type[Event]\n     self_literal: Literal['self']\n     _queued: bool\n-    _transition_queue: Deque[partial]\n+    _transition_queue: Deque[CallbackFunc]\n     _before_state_change: CallbackList\n     _after_state_change: CallbackList\n     _prepare_event: CallbackList\n@@ -178,9 +179,9 @@ class Machine:\n                    on_enter: CallbacksArg = ..., on_exit: CallbacksArg = ...,\n                    ignore_invalid_triggers: Optional[bool] = ..., **kwargs: Dict[str, Any]) -> None: ...\n     def _add_model_to_state(self, state: State, model: object) -> None: ...\n-    def _checked_assignment(self, model: object, name: str, func: Callable) -> None: ...\n+    def _checked_assignment(self, model: object, name: str, func: CallbackFunc) -> None: ...\n     def _add_trigger_to_model(self, trigger: str, model: object) -> None: ...\n-    def _get_trigger(self, model: object, trigger_name: str, *args: List, **kwargs: Dict[str, Any]) -> bool: ...\n+    def _get_trigger(self, model: object, trigger_name: str, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n     def get_triggers(self, *args: Union[str, Enum, State]) -> List[str]: ...\n     def add_transition(self, trigger: str,\n                        source: Union[StateIdentifier, List[StateIdentifier]],\n@@ -201,13 +202,13 @@ class Machine:\n     def get_transitions(self, trigger: str = ...,\n                         source: StateIdentifier = ..., dest: StateIdentifier = ...) -> List[Transition]: ...\n     def remove_transition(self, trigger: str, source: str = ..., dest: str = ...) -> None: ...\n-    def dispatch(self, trigger: str, *args: List, **kwargs: Dict[str, Any]) -> bool: ...\n-    def callbacks(self, funcs: Iterable[Union[str, Callable]], event_data: EventData) -> None: ...\n-    def callback(self, func: Union[str, Callable], event_data: EventData) -> None: ...\n+    def dispatch(self, trigger: str, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n+    def callbacks(self, funcs: Iterable[Callback], event_data: EventData) -> None: ...\n+    def callback(self, func: Callback, event_data: EventData) -> None: ...\n     @staticmethod\n-    def resolve_callable(func: Union[str, Callable], event_data: EventData) -> Callable:  ...\n+    def resolve_callable(func: Callback, event_data: EventData) -> CallbackFunc:  ...\n     def _has_state(self, state: StateIdentifier, raise_error: bool = ...) -> bool: ...\n-    def _process(self, trigger: partial) -> bool: ...\n+    def _process(self, trigger: Callable[[], bool]) -> bool: ...\n     def _identify_callback(self, name: str) -> Tuple[Optional[str], Optional[str]]: ...\n     def __getattr__(self, name: str) -> Any: ...\n \ndiff --git a/transitions/extensions/__init__.pyi b/transitions/extensions/__init__.pyi\nnew file mode 100644\nindex 00000000..bdb28374\n--- /dev/null\n+++ b/transitions/extensions/__init__.pyi\n@@ -0,0 +1,9 @@\n+from .asyncio import AsyncMachine as AsyncMachine, HierarchicalAsyncMachine as HierarchicalAsyncMachine\n+from .diagrams import GraphMachine as GraphMachine, HierarchicalGraphMachine as HierarchicalGraphMachine\n+from .factory import AsyncGraphMachine as AsyncGraphMachine, HierarchicalAsyncGraphMachine as HierarchicalAsyncGraphMachine\n+from .factory import MachineFactory as MachineFactory, LockedHierarchicalGraphMachine as LockedHierarchicalGraphMachine\n+from .factory import LockedHierarchicalMachine as LockedHierarchicalMachine, LockedGraphMachine as LockedGraphMachine\n+from .locking import LockedMachine as LockedMachine\n+from .nesting import HierarchicalMachine as HierarchicalMachine\n+\n+\ndiff --git a/transitions/extensions/asyncio.pyi b/transitions/extensions/asyncio.pyi\nindex 592a08f1..45bcea77 100644\n--- a/transitions/extensions/asyncio.pyi\n+++ b/transitions/extensions/asyncio.pyi\n@@ -1,6 +1,6 @@\n-from ..core import Condition, Event, EventData, Machine, State, Transition, StateConfig, ModelParameter, TransitionConfig\n+from ..core import Callback, Condition, Event, EventData, Machine, State, Transition, StateConfig, ModelParameter, TransitionConfig\n from .nesting import HierarchicalMachine, NestedEvent, NestedState, NestedTransition\n-from typing import Any, Optional, List, Type, Dict, Deque, Callable, Union, Iterable, DefaultDict, Literal, Sequence\n+from typing import Any, Awaitable, Optional, List, Type, Dict, Deque, Callable, Union, Iterable, DefaultDict, Literal, Sequence\n from asyncio import Task\n from functools import partial\n from logging import Logger\n@@ -11,6 +11,9 @@ from ..core import StateIdentifier, CallbacksArg, CallbackList\n \n _LOGGER: Logger\n \n+AsyncCallbackFunc = Callable[..., Awaitable[Optional[bool]]]\n+AsyncCallback = Union[str, AsyncCallbackFunc]\n+\n class AsyncState(State):\n     async def enter(self, event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n     async def exit(self, event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n@@ -42,7 +45,7 @@ class AsyncEvent(Event):\n     machine: AsyncMachine\n     transitions: DefaultDict[str, List[AsyncTransition]]  # type: ignore\n \n-    async def trigger(self, model: object, *args: List, **kwargs: Dict[str, Any]) -> bool: ...  # type: ignore[override]\n+    async def trigger(self, model: object, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...  # type: ignore[override]\n     async def _trigger(self, event_data: AsyncEventData) -> bool: ...  # type: ignore[override]\n     async def _process(self, event_data: AsyncEventData) -> bool: ...  # type: ignore[override]\n \n@@ -56,12 +59,12 @@ class AsyncMachine(Machine):\n     state_cls: Type[NestedAsyncState]\n     transition_cls: Type[AsyncTransition]\n     event_cls: Type[AsyncEvent]\n-    async_tasks: Dict[int, List[Task]]\n+    async_tasks: Dict[int, List[Task[Any]]]\n     events: Dict[str, AsyncEvent]  # type: ignore\n     queued: Union[bool, Literal[\"model\"]]\n-    protected_tasks: List[Task]\n-    current_context: ContextVar\n-    _transition_queue_dict: Dict[int, Deque[Callable]]\n+    protected_tasks: List[Task[Any]]\n+    current_context: ContextVar[Optional[Task[Any]]]\n+    _transition_queue_dict: Dict[int, Deque[AsyncCallbackFunc]]\n     def __init__(self, model: Optional[ModelParameter] = ...,\n                  states: Optional[Union[Sequence[StateConfig], Type[Enum]]] = ...,\n                  initial: Optional[StateIdentifier] = ...,\n@@ -75,17 +78,16 @@ class AsyncMachine(Machine):\n                  **kwargs: Dict[str, Any]) -> None: ...\n     def add_model(self, model: Union[Union[Literal[\"self\"], object], Sequence[Union[Literal[\"self\"], object]]],\n                   initial: Optional[StateIdentifier] = ...) -> None: ...\n-    async def dispatch(self, trigger: str, *args: List, **kwargs: Dict[str, Any]) -> bool: ...  # type: ignore[override]\n-    async def callbacks(self, funcs: Iterable[Union[str, Callable]], event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n-    async def callback(self, func: Union[str, Callable], event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n+    async def dispatch(self, trigger: str, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...  # type: ignore[override]\n+    async def callbacks(self, funcs: Iterable[Callback], event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n+    async def callback(self, func: AsyncCallback, event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n     @staticmethod\n-    async def await_all(callables: List[Callable]) -> List: ...\n+    async def await_all(callables: List[AsyncCallbackFunc]) -> Awaitable[List[Any]]: ...\n     async def switch_model_context(self, model: object) -> None: ...\n     def get_state(self, state: Union[str, Enum]) -> AsyncState: ...\n-    async def process_context(self, func: partial, model: object) -> bool: ...\n+    async def process_context(self, func: Callable[[], Awaitable[None]], model: object) -> bool: ...\n     def remove_model(self, model: object) -> None: ...\n-    def _process(self, trigger: partial) -> bool: ...\n-    async def _process_async(self, trigger: partial, model: object) -> bool: ...\n+    async def _process_async(self, trigger: Callable[[], Awaitable[None]], model: object) -> bool: ...\n \n \n class HierarchicalAsyncMachine(HierarchicalMachine, AsyncMachine):  # type: ignore\n@@ -93,7 +95,7 @@ class HierarchicalAsyncMachine(HierarchicalMachine, AsyncMachine):  # type: igno\n     transition_cls: Type[NestedAsyncTransition]\n     event_cls: Type[NestedAsyncEvent]  # type: ignore\n     async def trigger_event(self, model: object, trigger: str, # type: ignore[override]\n-                            *args: List, **kwargs: Dict[str, Any]) -> bool: ...\n+                            *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n     async def _trigger_event(self, event_data: AsyncEventData, trigger: str) -> bool: ...  # type: ignore[override]\n \n \n@@ -101,18 +103,18 @@ class AsyncTimeout(AsyncState):\n     dynamic_methods: List[str]\n     timeout: float\n     _on_timeout: CallbacksArg\n-    runner: Dict[int, Task]\n-    def __init__(self, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n+    runner: Dict[int, Task[Any]]\n+    def __init__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n     async def enter(self, event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n     async def exit(self, event_data: AsyncEventData) -> None: ...  # type: ignore[override]\n-    def create_timer(self, event_data: AsyncEventData) -> Task: ...\n+    def create_timer(self, event_data: AsyncEventData) -> Task[Any]: ...\n     async def _process_timeout(self, event_data: AsyncEventData) -> None: ...\n     @property\n     def on_timeout(self) -> CallbackList: ...\n     @on_timeout.setter\n     def on_timeout(self, value: CallbacksArg) -> None: ...\n \n-class _DictionaryMock(dict):\n+class _DictionaryMock(Dict[Any, Any]):\n     _value: Any\n     def __init__(self, item: Any) -> None: ...\n     def __setitem__(self, key: Any, item: Any) -> None: ...\ndiff --git a/transitions/extensions/diagrams.pyi b/transitions/extensions/diagrams.pyi\nindex b42cdcfe..2fd98c23 100644\n--- a/transitions/extensions/diagrams.pyi\n+++ b/transitions/extensions/diagrams.pyi\n@@ -17,7 +17,7 @@ GraphvizParameters = Dict[str, Union[str, Dict[str, Any]]]\n \n class TransitionGraphSupport(Transition):\n     label: str\n-    def __init__(self, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n+    def __init__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n     def _change_state(self, event_data: EventData) -> None: ...\n \n \ndiff --git a/transitions/extensions/diagrams_base.pyi b/transitions/extensions/diagrams_base.pyi\nindex 97df1f60..f63b3ebc 100644\n--- a/transitions/extensions/diagrams_base.pyi\n+++ b/transitions/extensions/diagrams_base.pyi\n@@ -1,5 +1,5 @@\n import abc\n-from typing import Protocol, Optional, Union, List, Dict, IO, Tuple, Generator\n+from typing import BinaryIO, Protocol, Optional, Union, List, Dict, Tuple, Generator\n \n from .diagrams import GraphMachine, HierarchicalGraphMachine\n from ..core import ModelState\n@@ -7,7 +7,7 @@ from ..core import ModelState\n \n class GraphProtocol(Protocol):\n \n-    def draw(self, filename: Optional[Union[str, IO]], format:Optional[str] = ...,\n+    def draw(self, filename: Optional[Union[str, BinaryIO]], format:Optional[str] = ...,\n              prog: Optional[str] = ..., args:str = ...) -> Optional[str]: ...\n \n class GraphModelProtocol(Protocol):\ndiff --git a/transitions/extensions/diagrams_graphviz.pyi b/transitions/extensions/diagrams_graphviz.pyi\nindex ddd76ab3..b81f7f43 100644\n--- a/transitions/extensions/diagrams_graphviz.pyi\n+++ b/transitions/extensions/diagrams_graphviz.pyi\n@@ -2,7 +2,7 @@ from ..core import State, ModelState\n from .diagrams import GraphMachine\n from .diagrams_base import BaseGraph\n from logging import Logger\n-from typing import Type, Optional, Dict, List, Union, IO, DefaultDict, Any\n+from typing import BinaryIO, Type, Optional, Dict, List, Union, DefaultDict, Any\n try:\n     from graphviz import Digraph\n     from graphviz.dot import SubgraphContext\n@@ -16,7 +16,7 @@ except ImportError:\n _LOGGER: Logger\n \n class Graph(BaseGraph):\n-    custom_styles: Dict[str, DefaultDict]\n+    custom_styles: Dict[str, DefaultDict[str, Union[str, DefaultDict[str, str]]]]\n     def __init__(self, machine: Type[GraphMachine]) -> None: ...\n     def set_previous_transition(self, src: str, dst: str) -> None: ...\n     def set_node_style(self, state: ModelState, style: str) -> None: ...\n@@ -28,12 +28,12 @@ class Graph(BaseGraph):\n     def generate(self) -> None: ...\n     def get_graph(self, title: Optional[str] = ...,  # type: ignore[no-any-unimported]\n                   roi_state: Optional[str] = ...) -> Digraph: ...\n-    def draw(self, filename: Optional[Union[str, IO]], format:Optional[str] = ...,\n+    def draw(self, filename: Optional[Union[str, BinaryIO]], format:Optional[str] = ...,\n              prog: Optional[str] = ..., args:str = ...) -> Optional[str]: ...\n \n class NestedGraph(Graph):\n     _cluster_states: List[str]\n-    def __init__(self, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n+    def __init__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n     def set_previous_transition(self, src: str, dst: str) -> None: ...\n     def _add_nodes(self, states: List[Dict[str, str]],  # type: ignore[no-any-unimported]\n                    container: Union[Digraph, SubgraphContext]) -> None: ...\ndiff --git a/transitions/extensions/diagrams_pygraphviz.pyi b/transitions/extensions/diagrams_pygraphviz.pyi\nindex 56cdb1ba..2a82a657 100644\n--- a/transitions/extensions/diagrams_pygraphviz.pyi\n+++ b/transitions/extensions/diagrams_pygraphviz.pyi\n@@ -30,7 +30,7 @@ class Graph(BaseGraph):\n \n class NestedGraph(Graph):\n     seen_transitions: Any\n-    def __init__(self, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n+    def __init__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n     def _add_nodes(self,  # type: ignore[override, no-any-unimported]\n                    states: List[Dict[str, Union[str, List[Dict[str, str]]]]],\n                    container: AGraph, prefix: str = ..., default_style: str = ...) -> None: ...\ndiff --git a/transitions/extensions/factory.pyi b/transitions/extensions/factory.pyi\nindex f7e49163..f329b4fc 100644\n--- a/transitions/extensions/factory.pyi\n+++ b/transitions/extensions/factory.pyi\n@@ -1,8 +1,8 @@\n-from ..core import Machine, State\n+from ..core import CallbackFunc, Machine, State\n from .diagrams import GraphMachine, NestedGraphTransition, HierarchicalGraphMachine\n from .locking import LockedMachine\n from .nesting import HierarchicalMachine, NestedEvent\n-from typing import Type, Dict, Tuple, Callable, Union\n+from typing import Any, Type, Dict, Tuple, Callable, Union\n \n try:\n     from transitions.extensions.asyncio import AsyncMachine, AsyncTransition\n@@ -39,13 +39,13 @@ class LockedHierarchicalMachine(LockedMachine, HierarchicalMachine):  # type: ig\n \n class LockedGraphMachine(GraphMachine, LockedMachine):  # type: ignore\n     @staticmethod\n-    def format_references(func: Callable) -> str: ...\n+    def format_references(func: CallbackFunc) -> str: ...\n \n class LockedHierarchicalGraphMachine(GraphMachine, LockedHierarchicalMachine):  # type: ignore\n     transition_cls: Type[NestedGraphTransition]\n     event_cls: Type[NestedEvent]\n     @staticmethod\n-    def format_references(func: Callable) -> str: ...\n+    def format_references(func: CallbackFunc) -> str: ...\n \n class AsyncGraphMachine(GraphMachine, AsyncMachine):\n     # AsyncTransition already considers graph models when necessary\ndiff --git a/transitions/extensions/locking.pyi b/transitions/extensions/locking.pyi\nindex 36cf68ff..f201c14e 100644\n--- a/transitions/extensions/locking.pyi\n+++ b/transitions/extensions/locking.pyi\n@@ -1,17 +1,18 @@\n+from contextlib import AbstractContextManager\n from transitions.core import Event, Machine, ModelParameter, TransitionConfig, CallbacksArg, StateConfig\n-from typing import Any, Dict, ContextManager, Literal, Optional, Type, List, DefaultDict, Union, Callable, Sequence\n+from typing import Any, Dict, Literal, Optional, Type, List, DefaultDict, Union, Callable, Sequence\n from types import TracebackType\n from logging import Logger\n from threading import Lock\n+from enum import Enum\n \n from ..core import StateIdentifier, State\n \n _LOGGER: Logger\n \n-from enum import Enum\n-\n+LockContext = AbstractContextManager[None]\n \n-class PicklableLock(ContextManager):\n+class PicklableLock(LockContext):\n     lock: Lock\n     def __init__(self) -> None: ...\n     def __getstate__(self) -> Dict[str, Any]: ...\n@@ -20,7 +21,7 @@ class PicklableLock(ContextManager):\n     def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException],\n                  exc_tb: Optional[TracebackType]) -> None: ...\n \n-class IdentManager(ContextManager):\n+class IdentManager(LockContext):\n     current: int\n     def __init__(self) -> None: ...\n     def __enter__(self) -> None: ...\n@@ -29,14 +30,14 @@ class IdentManager(ContextManager):\n \n class LockedEvent(Event):\n     machine: LockedMachine\n-    def trigger(self, model: object, *args: List, **kwargs: Dict[str, Any]) -> bool: ...\n+    def trigger(self, model: object, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n \n \n class LockedMachine(Machine):\n     event_cls: Type[LockedEvent]\n     _ident: IdentManager\n-    machine_context: List[ContextManager]\n-    model_context_map: DefaultDict[int, List[ContextManager]]\n+    machine_context: List[LockContext]\n+    model_context_map: DefaultDict[int, List[LockContext]]\n     def __init__(self, model: Optional[ModelParameter] = ...,\n                  states: Optional[Union[Sequence[StateConfig], Type[Enum]]] = ...,\n                  initial: Optional[StateIdentifier] = ...,\n@@ -47,17 +48,17 @@ class LockedMachine(Machine):\n                  name: str = ..., queued: bool = ...,\n                  prepare_event: CallbacksArg = ..., finalize_event: CallbacksArg = ...,\n                  model_attribute: str = ..., on_exception: CallbacksArg = ...,\n-                 machine_context: Optional[Union[List[ContextManager], ContextManager]] = ...,\n+                 machine_context: Optional[Union[List[LockContext], LockContext]] = ...,\n                  **kwargs: Dict[str, Any]) -> None: ...\n     def __getstate__(self) -> Dict[str, Any]: ...\n     def __setstate__(self, state: Dict[str, Any]) -> None: ...\n     def add_model(self, model:  Union[Union[Literal['self'], object], List[Union[Literal['self'], object]]],\n                   initial: Optional[StateIdentifier] = ...,\n-                  model_context: Optional[Union[ContextManager, List[ContextManager]]] = ...) -> None: ...\n+                  model_context: Optional[Union[LockContext, List[LockContext]]] = ...) -> None: ...\n     def remove_model(self, model: Union[Union[Literal['self'], object],\n                                         List[Union[Literal['self'], object]]]) -> None: ...\n     def __getattribute__(self, item: str) -> Any: ...\n     def __getattr__(self, item: str) -> Any: ...\n     def _add_model_to_state(self, state: State, model: object) -> None: ...\n     def _get_qualified_state_name(self, state: State) -> str: ...\n-    def _locked_method(self, func: Callable, *args: List, **kwargs: Dict[str, Any]) -> Any: ...\n+    def _locked_method(self, func: Callable[..., Any], *args: List[Any], **kwargs: Dict[str, Any]) -> Any: ...\ndiff --git a/transitions/extensions/markup.pyi b/transitions/extensions/markup.pyi\nindex c4ea6efa..aa580a70 100644\n--- a/transitions/extensions/markup.pyi\n+++ b/transitions/extensions/markup.pyi\n@@ -1,6 +1,6 @@\n import numbers\n \n-from ..core import Machine, StateIdentifier, CallbacksArg, StateConfig, Event, TransitionConfig, ModelParameter\n+from ..core import CallbackFunc, Machine, StateIdentifier, CallbacksArg, StateConfig, Event, TransitionConfig, ModelParameter\n from .nesting import HierarchicalMachine\n from typing import  List, Dict, Union, Optional, Callable, Tuple, Any, Type, Sequence, TypedDict\n \n@@ -46,7 +46,7 @@ class MarkupMachine(Machine):\n                    on_enter: CallbacksArg = ..., on_exit: CallbacksArg = ...,\n                    ignore_invalid_triggers: Optional[bool] = ..., **kwargs: Dict[str, Any]) -> None: ...\n     @staticmethod\n-    def format_references(func: Callable) -> str: ...\n+    def format_references(func: CallbackFunc) -> str: ...\n     def _convert_states_and_transitions(self, root: MarkupConfig) -> None: ...\n     def _convert_states(self, root: MarkupConfig) -> None: ...\n     def _convert_transitions(self, root: MarkupConfig) -> None: ...\n@@ -61,7 +61,7 @@ class HierarchicalMarkupMachine(MarkupMachine, HierarchicalMachine):  # type: ig\n     pass\n \n \n-def rep(func: Union[Callable, str, numbers.Number],\n-        format_references: Optional[Callable] = ...) -> str: ...\n-def _convert(obj: object, attributes: List[str], format_references: Optional[Callable]) -> MarkupConfig: ...\n+def rep(func: Union[CallbackFunc, str, Enum],\n+        format_references: Optional[Callable[[CallbackFunc], str]] = ...) -> str: ...\n+def _convert(obj: object, attributes: List[str], format_references: Optional[Callable[[CallbackFunc], str]]) -> MarkupConfig: ...\n \ndiff --git a/transitions/extensions/nesting.pyi b/transitions/extensions/nesting.pyi\nindex 0f8b7f72..b01e3668 100644\n--- a/transitions/extensions/nesting.pyi\n+++ b/transitions/extensions/nesting.pyi\n@@ -1,4 +1,4 @@\n-from ..core import Event, EventData, Machine, State, Transition, CallbacksArg, Callback, ModelParameter, TransitionConfig\n+from ..core import CallbackFunc, Event, EventData, Machine, State, Transition, CallbacksArg, Callback, ModelParameter, TransitionConfig\n from collections import defaultdict as defaultdict\n from typing import OrderedDict, Sequence, Union, List, Dict, Optional, Type, Tuple, Callable, Any, Collection\n from types import TracebackType\n@@ -9,10 +9,10 @@ from functools import partial\n _LOGGER: Logger\n \n class FunctionWrapper:\n-    _func: Optional[Callable]\n-    def __init__(self, func: Callable, path: List[str]) -> None: ...\n-    def add(self, func: Callable, path: List[str]) -> None: ...\n-    def __call__(self, *args: List, **kwargs: Dict[str, Any]) -> Any: ...\n+    _func: Optional[CallbackFunc]\n+    def __init__(self, func: CallbackFunc, path: List[str]) -> None: ...\n+    def add(self, func: CallbackFunc, path: List[str]) -> None: ...\n+    def __call__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> Any: ...\n \n \n class NestedEvent(Event):\n@@ -55,13 +55,13 @@ def _build_state_list(state_tree: StateTree, separator: str,\n def resolve_order(state_tree: Dict[str, str]) -> List[List[str]]: ...\n \n class NestedTransition(Transition):\n-    def _resolve_transition(self, event_data: NestedEventData) -> Tuple[StateTree, List[partial], List[partial]]: ...\n+    def _resolve_transition(self, event_data: NestedEventData) -> Tuple[StateTree, List[Callable[[], None]], List[Callable[[], Any]]]: ...\n     def _change_state(self, event_data: NestedEventData) -> None: ...  # type: ignore[override]\n     def _enter_nested(self, root: List[str], dest: List[str], prefix_path: List[str],\n-                      event_data: NestedEventData) -> Tuple[StateTree, List[partial]]: ...\n+                      event_data: NestedEventData) -> Tuple[StateTree, List[Callable[[], None]]]: ...\n     @staticmethod\n     def _update_model(event_data: NestedEventData, tree: StateTree) -> None: ...\n-    def __deepcopy__(self, memo: Dict) -> NestedTransition: ...\n+    def __deepcopy__(self, memo: Dict[str, Any]) -> NestedTransition: ...\n \n ScopeTuple = Tuple[Union[NestedState, 'HierarchicalMachine'], OrderedDict[str, NestedState],\n                    Dict[str, NestedEvent], List[str]]\n@@ -126,8 +126,8 @@ class HierarchicalMachine(Machine):\n     def on_exit(self, state_name: str, callback: Callback) -> None: ...\n     def set_state(self, state: Union[NestedStateIdentifier, List[NestedStateIdentifier]],  # type: ignore[override]\n                   model: Optional[object] = ...) -> None: ...\n-    def to_state(self, model: object, state_name: str, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n-    def trigger_event(self, model: object, trigger: str, *args: List, **kwargs: Dict[str, Any]) -> bool: ...\n+    def to_state(self, model: object, state_name: str, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n+    def trigger_event(self, model: object, trigger: str, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n     def _add_model_to_state(self, state: NestedState, model: object) -> None: ...  # type: ignore[override]\n     def _add_dict_state(self, state: Dict[str, Any], ignore_invalid_triggers: bool, remap: Optional[Dict[str, str]],\n                         **kwargs: Dict[str, Any]) -> None: ...\n@@ -141,15 +141,15 @@ class HierarchicalMachine(Machine):\n     def build_state_tree(self, model_states: Union[str, Enum, Sequence[Union[str, Enum, Sequence[Any]]]],\n                          separator: str, tree: Optional[StateTree] = ...) -> StateTree: ...\n     @classmethod\n-    def _create_transition(cls, *args: List, **kwargs: Dict[str, Any]) -> NestedTransition: ...\n+    def _create_transition(cls, *args: List[Any], **kwargs: Dict[str, Any]) -> NestedTransition: ...\n     @classmethod\n-    def _create_event(cls, *args: List, **kwargs: Dict[str, Any]) -> NestedEvent: ...\n+    def _create_event(cls, *args: List[Any], **kwargs: Dict[str, Any]) -> NestedEvent: ...\n     @classmethod\n-    def _create_state(cls, *args: List, **kwargs: Dict[str, Any]) -> NestedState: ...\n+    def _create_state(cls, *args: List[Any], **kwargs: Dict[str, Any]) -> NestedState: ...\n     def _get_enum_path(self, enum_state: Enum, prefix: Optional[List[str]] =...) -> List[str]: ...\n     def _get_state_path(self, state: NestedState, prefix: Optional[List[str]] = ...) -> List[str]: ...\n     def _check_event_result(self, res: bool, model: object, trigger: str) -> bool: ...\n-    def _get_trigger(self, model: object, trigger_name: str, *args: List, **kwargs: Dict[str, Any]) -> bool: ...\n+    def _get_trigger(self, model: object, trigger_name: str, *args: List[Any], **kwargs: Dict[str, Any]) -> bool: ...\n     def _has_state(self, state: NestedState, raise_error: bool = ...) -> bool: ...  # type: ignore[override]\n     def _init_state(self, state: NestedState) -> None: ...\n     def _recursive_initial(self, value: NestedStateIdentifier) -> Union[str, List[str]]: ...\ndiff --git a/transitions/extensions/states.pyi b/transitions/extensions/states.pyi\nindex c7354d03..882204a2 100644\n--- a/transitions/extensions/states.pyi\n+++ b/transitions/extensions/states.pyi\n@@ -19,7 +19,7 @@ class Timeout(State):\n     timeout: float\n     _on_timeout: Optional[List[Callback]]\n     runner: Dict[int, Timer]\n-    def __init__(self, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n+    def __init__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n     def enter(self, event_data: EventData) -> None: ...\n     def exit(self, event_data: EventData) -> None: ...\n     def _process_timeout(self, event_data: EventData) -> None: ...\n@@ -32,10 +32,10 @@ class Volatile(State):\n     volatile_cls: Any\n     volatile_hook: str\n     initialized: bool\n-    def __init__(self, *args: List, **kwargs: Dict[str, Any]) -> None: ...\n+    def __init__(self, *args: List[Any], **kwargs: Dict[str, Any]) -> None: ...\n     def enter(self, event_data: EventData) -> None: ...\n     def exit(self, event_data: EventData) -> None: ...\n \n-def add_state_features(*args: Type) -> Any: ...\n+def add_state_features(*args: Union[Type[State], List[Type[State]]]) -> Any: ...\n \n class VolatileObject: ...\n", "instance_id": "pytransitions__transitions-667", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: mypy in strict mode raises errors due to implicit reexports in the `transitions` package, and the solution involves adding explicit `__all__` definitions in `__init__.py` files. It also provides a workaround for downstream users via `mypy.ini` configuration. However, the statement lacks specific details about which modules or attributes need to be explicitly reexported, and it does not mention potential edge cases or constraints (e.g., compatibility with older versions of mypy or Python). Additionally, there are no examples of the expected `__all__` definitions or the exact scope of changes. While the intent and general approach are clear, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is significant, as it involves multiple files across the codebase (e.g., type stub files like `transitions/extensions/__init__.pyi` and updates to core type definitions in `core.pyi` and other extension files). The changes require understanding Python's type hinting system (specifically mypy's strict mode and reexport rules) and modifying type annotations to introduce new types like `CallbackFunc` and `AsyncCallbackFunc`, which declutter callback signatures. This involves a moderate number of technical concepts, including Python's typing system, stub files (`.pyi`), and package initialization mechanics (`__all__`). While the changes do not appear to impact the core architecture of the system (the runtime behavior remains unchanged, focusing only on type hints), they do require careful attention to ensure consistency across the codebase. Edge cases and error handling are not explicitly mentioned in the problem statement, and the code changes do not introduce significant new error handling logic, though ensuring all relevant attributes are reexported correctly could be a minor concern. Overall, this task requires a solid understanding of Python's type system and codebase structure, placing it at the lower end of medium difficulty (0.45), as it is not inherently complex in logic or architecture but does span multiple files and concepts.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Json Encoder: dataclass classes are encoded to empty dict\n### Question\r\n\r\nI've noticed that when encoding a dataclass class (as opposed to an instance), the class is encoded to an empty dict.\r\n\r\nI was wondering if this was intentional behaviour? I would have expected an error as if I had passed in any other class (rather than instance).\r\n\r\n```python\r\n>>> import msgspec.json\r\n>>> e = msgspec.json.Encoder()\r\n>>> from dataclasses import dataclass\r\n>>> @dataclass\r\n... class SomeDataclass:\r\n...   foo: int\r\n... \r\n>>> e.encode(SomeDataclass)\r\nb'{}'\r\n>>> class SomeClass:\r\n...  pass\r\n... \r\n>>> e.encode(SomeClass)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: Encoding objects of type type is unsupported\r\n```\n", "patch": "diff --git a/msgspec/_core.c b/msgspec/_core.c\nindex d2087b85..949f8a8f 100644\n--- a/msgspec/_core.c\n+++ b/msgspec/_core.c\n@@ -13137,7 +13137,7 @@ mpack_encode_uncommon(EncoderState *self, PyTypeObject *type, PyObject *obj)\n     else if (PyAnySet_Check(obj)) {\n         return mpack_encode_set(self, obj);\n     }\n-    else if (type->tp_dict != NULL) {\n+    else if (!PyType_Check(obj) && type->tp_dict != NULL) {\n         PyObject *fields = PyObject_GetAttr(obj, self->mod->str___dataclass_fields__);\n         if (fields != NULL) {\n             int status = mpack_encode_dataclass(self, obj, fields);\n@@ -14231,7 +14231,7 @@ json_encode_uncommon(EncoderState *self, PyTypeObject *type, PyObject *obj) {\n     else if (PyAnySet_Check(obj)) {\n         return json_encode_set(self, obj);\n     }\n-    else if (type->tp_dict != NULL) {\n+    else if (!PyType_Check(obj) && type->tp_dict != NULL) {\n         PyObject *fields = PyObject_GetAttr(obj, self->mod->str___dataclass_fields__);\n         if (fields != NULL) {\n             int status = json_encode_dataclass(self, obj, fields);\n@@ -19924,7 +19924,7 @@ to_builtins(ToBuiltinsState *self, PyObject *obj, bool is_key) {\n     else if (PyAnySet_Check(obj)) {\n         return to_builtins_set(self, obj, is_key);\n     }\n-    else if (type->tp_dict != NULL) {\n+    else if (!PyType_Check(obj) && type->tp_dict != NULL) {\n         PyObject *fields = PyObject_GetAttr(obj, self->mod->str___dataclass_fields__);\n         if (fields != NULL) {\n             PyObject *out = to_builtins_dataclass(self, obj, fields);\n", "instance_id": "jcrist__msgspec-755", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: encoding a dataclass class (not an instance) results in an empty dictionary, which the user questions as potentially unintended behavior compared to encoding a regular class, which raises an error. The goal is implied\u2014to modify the behavior to raise an error for dataclass classes as well. The provided example code effectively illustrates the discrepancy. However, the statement lacks explicit confirmation of the desired outcome (e.g., should it always raise an error, or is there another expected behavior?). Additionally, edge cases or constraints (e.g., specific dataclass configurations or inheritance scenarios) are not mentioned, which could impact the solution. Overall, while the problem is valid and mostly clear, these minor ambiguities prevent a perfect score.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows modifications in three specific locations within a single file (`msgspec/_core.c`), focusing on the encoding logic for JSON, MessagePack, and a built-in conversion function. The change is minimal\u2014adding a condition `!PyType_Check(obj)` to prevent encoding dataclass classes (as opposed to instances). This does not impact the broader system architecture or require changes across multiple modules, limiting the scope and complexity.\n\n2. **Technical Concepts Involved:** Solving this requires a basic understanding of Python's C API (e.g., `PyType_Check`, `tp_dict`), as the codebase is written in C for performance (likely part of a library like `msgspec`). Additionally, familiarity with Python's dataclass internals (e.g., `__dataclass_fields__`) is necessary. While these concepts are not trivial for a beginner, they are straightforward for someone with moderate experience in Python internals or C extensions, and no advanced algorithms or design patterns are involved.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code change implicitly addresses the core issue (distinguishing between class and instance). No additional error handling logic is introduced beyond the conditional check, and the modification aligns with existing behavior for non-dataclass classes (raising a `TypeError`). Potential edge cases, such as custom dataclass implementations or inheritance, are not addressed in the diff or problem statement, suggesting limited complexity in this area.\n\n4. **Overall Complexity:** The fix is a small, targeted change that requires understanding a specific part of the encoding logic but does not demand deep knowledge of the entire codebase or complex refactoring. The primary challenge lies in navigating the C-based implementation of a Python library, which adds a slight learning curve but remains manageable for someone with intermediate skills.\n\nThus, a score of 0.30 reflects an \"Easy\" problem that involves simple modifications with moderate technical understanding, fitting well within the 0.2-0.4 range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Issue when unsubscribing/subscribing again a subscriber_name\nHi,\r\n\r\nwe would like to seek to a different timestamp while a subscription is already running.\r\nI tried to simplify our current approach to this script:\r\n\r\n```python\r\nimport asyncio\r\nimport datetime\r\n\r\nimport rstream as rs\r\n\r\nSTREAM = \"hello-stream\"\r\nAPP_ID = \"abcdef123\"\r\n\r\n\r\nasync def seek_somewhere(consumer: rs.Consumer):\r\n    await asyncio.sleep(1)\r\n    print(\"SEEKING NOW\")\r\n    await consumer.unsubscribe(APP_ID)\r\n\r\n    # await self._setup()\r\n    timestamp = int(datetime.datetime(2023, 7, 18,\r\n                                      tzinfo=datetime.timezone.utc).timestamp())\r\n    offset = rs.ConsumerOffsetSpecification(rs.OffsetType.TIMESTAMP, timestamp)\r\n    await consumer.subscribe(\r\n        stream=STREAM,\r\n        subscriber_name=APP_ID,\r\n        callback=print,\r\n        decoder=rs.amqp_decoder,\r\n        offset_specification=offset\r\n    )\r\n    await consumer.start()\r\n    print(\"finished seeking\")\r\n\r\n\r\nasync def main():\r\n    consumer = rs.Consumer(\r\n        host=\"localhost\",\r\n        username=\"guest\",\r\n        password=\"guest\",\r\n        connection_name=APP_ID\r\n    )\r\n    await consumer.subscribe(\r\n        stream=STREAM,\r\n        subscriber_name=APP_ID,\r\n        callback=print,\r\n        decoder=rs.amqp_decoder,\r\n    )\r\n    task = asyncio.create_task(seek_somewhere(consumer))\r\n    await consumer.run()\r\n    await task\r\n\r\n\r\nif __name__ == '__main__':\r\n    asyncio.run(main())\r\n\r\n```\r\nHowever, this doesn't work. Messages are received, until the seeking is attempted, after which no more messages are received (but the program keeps running).\r\n\r\nWhat is the best way to do this? Do we have to create a new consumer?\r\n\r\nThanks for any help in advance.\n", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex d65f62e..8d98774 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"rstream\"\n-version = \"0.20.1\"\n+version = \"0.20.2\"\n description = \"A python client for RabbitMQ Streams\"\n authors = [\"George Fortunatov <qweeeze@gmail.com>\", \"Daniele Palaia <dpalaia@vmware.com>\"]\n readme = \"README.md\"\ndiff --git a/rstream/consumer.py b/rstream/consumer.py\nindex 3786601..32ade3f 100644\n--- a/rstream/consumer.py\n+++ b/rstream/consumer.py\n@@ -319,6 +319,12 @@ async def unsubscribe(self, subscriber_name: str) -> None:\n         except BaseException as exc:\n             logger.warning(\"exception in unsubscribe of Consumer:\" + str(exc))\n \n+        stream = subscriber.stream\n+\n+        if stream in self._clients:\n+            await self._clients[stream].remove_stream(stream)\n+            await self._clients[stream].free_available_id(subscriber.subscription_id)\n+\n         del self._subscribers[subscriber_name]\n \n     async def query_offset(self, stream: str, subscriber_name: str) -> int:\n", "instance_id": "rabbitmq-community__rstream-199", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the goal is to seek to a different timestamp in a running subscription using the `rstream` library for RabbitMQ Streams in Python. The provided script illustrates the user's attempt to unsubscribe and resubscribe with a new timestamp offset, and the issue (no messages received after seeking) is explicitly stated. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether creating a new consumer is a requirement or just a question, nor does it mention any specific constraints or edge cases (e.g., behavior during high load or connection interruptions). Additionally, the expected behavior after seeking (e.g., should old messages be replayed?) is not fully clarified. Despite these minor gaps, the intent and issue are understandable, supported by a reproducible code snippet, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively small, confined to a single file (`consumer.py`) and a specific method (`unsubscribe`) in the `rstream` library, with only a few lines added to handle stream cleanup and ID freeing. This limits the impact on the broader codebase and does not suggest architectural changes. Second, the technical concepts involved include understanding asynchronous programming in Python (via `asyncio`), the `rstream` library's internal handling of subscriptions and streams, and RabbitMQ Streams' offset management. These concepts are moderately complex but not overly advanced for someone familiar with messaging systems. Third, the problem requires addressing potential edge cases, such as ensuring proper cleanup of resources during unsubscription to avoid leaks or stale states, though these are not explicitly mentioned in the problem statement. Finally, the solution involves modifying existing logic to ensure that unsubscribing and resubscribing work seamlessly, which requires a moderate understanding of the library's internals. Overall, this problem is not trivial but does not demand deep architectural changes or advanced domain knowledge, justifying a difficulty score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
