{"problem_statement": "Backward compatibility for configs of pools/providers\n\r\n## Rationale\r\n\r\nUMF should provide backward compatible interfaces.\r\n\r\n## Description\r\n\r\n### Background\r\n\r\nMPI team experienced the issue after the PR #692 extends `level_zero_memory_provider_params_t` config structure. The root cause of the issue relates to how MPI instantiate/initialize the L0 provider config. They did the following:\r\n```c\r\nlevel_zero_memory_provider_params_t l0_config = {\r\n    .level_zero_context_handle = hContext;\r\n    .level_zero_device_handle = hDevice;\r\n    .memory_type = UMF_MEMORY_TYPE_DEVICE;\r\n};\r\n\r\numf_memory_provider_handle_t hProvider = NULL;\r\numf_memory_provider_ops_t *l0_ops = umfLevelZeroMemoryProviderOps();\r\numfMemoryProviderCreate(l0_ops, &l0_config, &hProvider);\r\n```\r\n\r\nAfter PR #692 the code above start crashing because two new fields were added to the `level_zero_memory_provider_params_t` data structure and the example above does not initialize these new fields:\r\n```c\r\ntypedef struct level_zero_memory_provider_params_t {\r\n    ze_context_handle_t level_zero_context_handle;\r\n    ze_device_handle_t level_zero_device_handle;\r\n    umf_usm_memory_type_t memory_type;\r\n\r\n    // New fields\r\n    ze_device_handle_t *resident_device_handles;\r\n    uint32_t resident_device_count;\r\n} level_zero_memory_provider_params_t;\r\n```\r\n\r\nA quick fix for the issue was to init the config data structure with `0` and than assign the required fields:\r\n```c\r\nlevel_zero_memory_provider_params_t l0_config = { 0 }; // zero-initialize\r\nl0_config.level_zero_context_handle = hContext;\r\nl0_config.level_zero_device_handle = hDevice;\r\nl0_config.memory_type = UMF_MEMORY_TYPE_DEVICE;\r\n```\r\n\r\n### Open Questions\r\n\r\nThe quick fix above works only because it is OK to init fields of the `level_zero_memory_provider_params_t` structure with zeroes. But there are 2 related major question we should address:\r\n\r\n#### 1. How to initialize the configs with default values?\r\n\r\nIn general case, not every field of the config data structure could/should be initialized with zeroes.\r\n\r\n#### 2. How to support backward compatibility?\r\n\r\nConfig data structures are defined in the headers. If application was built with old version of UMF but on the system there is a newer version than even the application initialize all config's fields properly it initializes only fields that exists in the old version of UMF.\r\nConsider an example. There is a `provider_foo` and corresponding `foo_config_t` structure that contains an int field in the 1st version:\r\n\r\n```c\r\n// provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n} foo_config_t;\r\n\r\n// provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, void **provider) {\r\n    foo_params_t *foo_params = (foo_params_t *)params;\r\n    // expected sizeof(foo_params_t) is equal sizeof(int) that is 4 bytes\r\n\r\n    assert(foo_params->field1 == 0);\r\n}\r\n\r\n// application code:\r\nint main() {\r\n    foo_params_t foo_params = {0};\r\n\r\n    umf_memory_provider_handle_t hProvider = NULL;\r\n    umf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\n    umfMemoryProviderCreate(foo_ops, &foo_params, &hProvider);\r\n}\r\n```\r\n\r\nNow in the 2nd version of UMF we extend the `foo_params_t` structure with additional field:\r\n\r\n```c\r\n// provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n    int field2;\r\n} foo_config_t;\r\n\r\n// provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, void **provider) {\r\n    foo_params_t *foo_params = (foo_params_t *)params;\r\n    // expected sizeof(foo_params_t) is equal 2*sizeof(int) that is 8 bytes\r\n\r\n    assert(foo_params->field1 == 0);\r\n    assert(foo_params->field2 == 0); // ERROR: if application was compiled with 1st version of the UMF lib then the size of memory pointed by `params` is 4 bytes\r\n}\r\n```\r\n\r\n## Possible API Changes\r\n\r\n### Option 1: handle-based approach\r\nDo not expose the config structure in interfaces. The config object is allocated inside libumf.so and handle to the config is returned to the client code. Setter/getter APIs are used to setup config parameters. For example:\r\n\r\n```c\r\nfoo_params_handle_t foo_params = umfCreateFooParams();\r\numfFooParamsSetField1(foo_params, 7);\r\n\r\numf_memory_provider_handle_t hProvider = NULL;\r\numf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\numfMemoryProviderCreate(foo_ops, foo_params, &hProvider);\r\n```\r\n\r\nClient code does not depend on the layout of the `foo_params_t` structure only UMF knows it and can change in different versions.\r\n\r\n### Option 2: explicitly pass the size of params \r\n\r\nData structures that define configs for the pools/providers remains in headers (the same as today). The new fields can be added only to the end. Application explicitly passes the size of the config data structure. Provider/pool implementation determine the version of the config based on the size. To init the config data structure we need to introduce a special macros or header-based inline functions. For example:\r\n\r\n```c\r\n// Version 1 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n     params->field1 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n    int field2;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n     params->field1 = 0;\r\n     params->field2 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, size_t params_size, void **provider) {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(& foo_params);\r\n\r\n    if(sizeof(foo_params_t) == params_size) {\r\n         // current version, just copy input params as is.\r\n        foo_params = *(foo_params_t*)params;\r\n    } else {\r\n        // old version, copy only first `params_size` bytes and keep the rest default initialized\r\n        memcpy(& foo_params, params, params_size);\r\n    }\r\n\r\n    assert(foo_params->field1 == 0);\r\n    assert(foo_params->field2 == 0);\r\n}\r\n\r\n// application code:\r\nint main() {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(&params);\r\n\r\n    umf_memory_provider_handle_t hProvider = NULL;\r\n    umf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\n    umfMemoryProviderCreate(foo_ops, &foo_params, sizeof(foo_params_t), &hProvider);\r\n}\r\n```\r\n\r\n### Option 3: store version in the params data structure\r\n\r\nSimilar to Option 2, but instead of using the size of params data structure to determine the version store it explicitly as a first field in the params data structure.\r\n\r\n```c\r\n// Version 1 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int version;\r\n    int field 1;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n    version = 1;\r\n    params->field1 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.h:\r\ntypedef struct foo_params_t {\r\n    int field 1;\r\n    int field2;\r\n} foo_config_t;\r\n\r\ninline void umfFooParamsInit(foo_params_t *params) {\r\n    version = 2;\r\n    params->field1 = 0;\r\n    params->field2 = 0;\r\n}\r\n\r\n// Version 2 provider_foo.c:\r\numf_result_t foo_provider_initialize(void *params, size_t params_size, void **provider) {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(& foo_params);\r\n\r\n    int params_version = *(int *)params;\r\n\r\n    switch(params_version) {\r\n        case 1:\r\n            ...\r\n            break;\r\n        case 2:\r\n            ...\r\n            break;\r\n        default:\r\n            LOG_ERR(\"Wrong version\");\r\n    }\r\n\r\n    assert(foo_params->field1 == 0);\r\n    assert(foo_params->field2 == 0);\r\n}\r\n\r\n// application code:\r\nint main() {\r\n    foo_params_t foo_params;\r\n    umfFooParamsInit(&params);\r\n\r\n    umf_memory_provider_handle_t hProvider = NULL;\r\n    umf_memory_provider_ops_t *foo_ops = umfFooMemoryProviderOps();\r\n    umfMemoryProviderCreate(foo_ops, &foo_params, &hProvider);\r\n}\r\n```\r\n\r\n### Design Considerations\r\n\r\nI prefer Option 1 over Option 2 and Option 3. I think it provides best ABI compatibility. Another advantage is that the config data structure always initialized when user calls `umfCreateFooParams()`. There is no way to get non-initialized fields in the config, while in case of Option 2 or 3 user might forget to call `umfFooParamsInit(&params)` function.\r\n\r\nBetween Options 2 and 3, I prefer Option 3 because of two things:\r\n\r\n- it stores version explicitly.\r\n- the `umfMemoryProviderCreate` API remains unchanged.\r\n\r\n## Implementation details\r\n\r\nTBD\r\n\r\n\n", "patch": "diff --git a/benchmark/ubench.c b/benchmark/ubench.c\nindex 0ac174de6..645ddb743 100644\n--- a/benchmark/ubench.c\n+++ b/benchmark/ubench.c\n@@ -421,11 +421,10 @@ static void do_ipc_get_put_benchmark(alloc_t *allocs, size_t num_allocs,\n     }\n }\n \n-int create_level_zero_params(level_zero_memory_provider_params_t *params) {\n+int create_level_zero_params(ze_context_handle_t *context,\n+                             ze_device_handle_t *device) {\n     uint32_t driver_idx = 0;\n     ze_driver_handle_t driver = NULL;\n-    ze_context_handle_t context = NULL;\n-    ze_device_handle_t device = NULL;\n \n     int ret = init_level_zero();\n     if (ret != 0) {\n@@ -439,36 +438,68 @@ int create_level_zero_params(level_zero_memory_provider_params_t *params) {\n         return ret;\n     }\n \n-    ret = create_context(driver, &context);\n+    ret = create_context(driver, context);\n     if (ret != 0) {\n         fprintf(stderr, \"Failed to create L0 context!\\n\");\n         return ret;\n     }\n \n-    ret = find_gpu_device(driver, &device);\n-    if (ret || device == NULL) {\n+    ret = find_gpu_device(driver, device);\n+    if (ret) {\n         fprintf(stderr, \"Cannot find GPU device!\\n\");\n-        destroy_context(context);\n+        destroy_context(*context);\n         return ret;\n     }\n \n-    params->level_zero_context_handle = context;\n-    params->level_zero_device_handle = device;\n-    params->memory_type = UMF_MEMORY_TYPE_DEVICE;\n-\n     return ret;\n }\n \n UBENCH_EX(ipc, disjoint_pool_with_level_zero_provider) {\n     const size_t BUFFER_SIZE = 100;\n     const size_t N_BUFFERS = 1000;\n-    level_zero_memory_provider_params_t level_zero_params = {0};\n+    umf_result_t umf_result;\n+    ze_context_handle_t context = NULL;\n+    ze_device_handle_t device = NULL;\n+    umf_level_zero_memory_provider_params_handle_t level_zero_params = NULL;\n \n-    int ret = create_level_zero_params(&level_zero_params);\n+    int ret = create_level_zero_params(&context, &device);\n     if (ret != 0) {\n+        fprintf(stderr, \"error: create_level_zero_params() failed\\n\");\n         exit(-1);\n     }\n \n+    umf_result = umfLevelZeroMemoryProviderParamsCreate(&level_zero_params);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr,\n+                \"error: umfLevelZeroMemoryProviderParamsCreate() failed\\n\");\n+        goto err_destroy_context;\n+    }\n+\n+    umf_result =\n+        umfLevelZeroMemoryProviderParamsSetContext(level_zero_params, context);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr,\n+                \"error: umfLevelZeroMemoryProviderParamsSetContext() failed\\n\");\n+        goto err_destroy_params;\n+    }\n+\n+    umf_result =\n+        umfLevelZeroMemoryProviderParamsSetDevice(level_zero_params, device);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr,\n+                \"error: umfLevelZeroMemoryProviderParamsSetDevice() failed\\n\");\n+        goto err_destroy_params;\n+    }\n+\n+    umf_result = umfLevelZeroMemoryProviderParamsSetMemoryType(\n+        level_zero_params, UMF_MEMORY_TYPE_DEVICE);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(\n+            stderr,\n+            \"error: umfLevelZeroMemoryProviderParamsSetMemoryType() failed\\n\");\n+        goto err_destroy_params;\n+    }\n+\n     alloc_t *allocs = alloc_array(N_BUFFERS);\n     if (allocs == NULL) {\n         fprintf(stderr, \"error: alloc_array() failed\\n\");\n@@ -481,10 +512,9 @@ UBENCH_EX(ipc, disjoint_pool_with_level_zero_provider) {\n         goto err_free_allocs;\n     }\n \n-    umf_result_t umf_result;\n     umf_memory_provider_handle_t provider = NULL;\n     umf_result = umfMemoryProviderCreate(umfLevelZeroMemoryProviderOps(),\n-                                         &level_zero_params, &provider);\n+                                         level_zero_params, &provider);\n     if (umf_result != UMF_RESULT_SUCCESS) {\n         fprintf(stderr, \"error: umfMemoryProviderCreate() failed\\n\");\n         goto err_free_ipc_handles;\n@@ -570,8 +600,11 @@ UBENCH_EX(ipc, disjoint_pool_with_level_zero_provider) {\n err_free_allocs:\n     free(allocs);\n \n+err_destroy_params:\n+    umfLevelZeroMemoryProviderParamsDestroy(level_zero_params);\n+\n err_destroy_context:\n-    destroy_context(level_zero_params.level_zero_context_handle);\n+    destroy_context(context);\n }\n #endif /* (defined UMF_BUILD_LIBUMF_POOL_DISJOINT && defined UMF_BUILD_LEVEL_ZERO_PROVIDER && defined UMF_BUILD_GPU_TESTS) */\n \ndiff --git a/examples/ipc_level_zero/ipc_level_zero.c b/examples/ipc_level_zero/ipc_level_zero.c\nindex cfab57b0d..fc93eb930 100644\n--- a/examples/ipc_level_zero/ipc_level_zero.c\n+++ b/examples/ipc_level_zero/ipc_level_zero.c\n@@ -20,15 +20,48 @@ int create_level_zero_pool(ze_context_handle_t context,\n                            ze_device_handle_t device,\n                            umf_memory_pool_handle_t *pool) {\n     // setup params\n-    level_zero_memory_provider_params_t params = {0};\n-    params.level_zero_context_handle = context;\n-    params.level_zero_device_handle = device;\n-    params.memory_type = UMF_MEMORY_TYPE_DEVICE;\n+    umf_level_zero_memory_provider_params_handle_t provider_params = NULL;\n+\n+    umf_result_t umf_result =\n+        umfLevelZeroMemoryProviderParamsCreate(&provider_params);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr,\n+                \"ERROR: Failed to create Level Zero memory provider params!\\n\");\n+        return -1;\n+    }\n+\n+    umf_result =\n+        umfLevelZeroMemoryProviderParamsSetContext(provider_params, context);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"ERROR: Failed to set context in Level Zero memory \"\n+                        \"provider params!\\n\");\n+        umfLevelZeroMemoryProviderParamsDestroy(provider_params);\n+        return -1;\n+    }\n+\n+    umf_result =\n+        umfLevelZeroMemoryProviderParamsSetDevice(provider_params, device);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"ERROR: Failed to set device in Level Zero memory \"\n+                        \"provider params!\\n\");\n+        umfLevelZeroMemoryProviderParamsDestroy(provider_params);\n+        return -1;\n+    }\n+\n+    umf_result = umfLevelZeroMemoryProviderParamsSetMemoryType(\n+        provider_params, UMF_MEMORY_TYPE_DEVICE);\n+    if (umf_result != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"ERROR: Failed to set memory type in Level Zero memory \"\n+                        \"provider params!\\n\");\n+        umfLevelZeroMemoryProviderParamsDestroy(provider_params);\n+        return -1;\n+    }\n \n     // create Level Zero provider\n     umf_memory_provider_handle_t provider = 0;\n-    umf_result_t umf_result = umfMemoryProviderCreate(\n-        umfLevelZeroMemoryProviderOps(), &params, &provider);\n+    umf_result = umfMemoryProviderCreate(umfLevelZeroMemoryProviderOps(),\n+                                         provider_params, &provider);\n+    umfLevelZeroMemoryProviderParamsDestroy(provider_params);\n     if (umf_result != UMF_RESULT_SUCCESS) {\n         fprintf(stderr,\n                 \"ERROR: Failed to create Level Zero memory provider!\\n\");\ndiff --git a/examples/level_zero_shared_memory/level_zero_shared_memory.c b/examples/level_zero_shared_memory/level_zero_shared_memory.c\nindex 1a38beceb..d7f68168d 100644\n--- a/examples/level_zero_shared_memory/level_zero_shared_memory.c\n+++ b/examples/level_zero_shared_memory/level_zero_shared_memory.c\n@@ -49,22 +49,51 @@ int main(void) {\n \n     // Setup parameters for the Level Zero memory provider. It will be used for\n     // allocating memory from Level Zero devices.\n-    level_zero_memory_provider_params_t ze_memory_provider_params = {0};\n-    ze_memory_provider_params.level_zero_context_handle = hContext;\n-    ze_memory_provider_params.level_zero_device_handle = hDevice;\n+    umf_level_zero_memory_provider_params_handle_t ze_memory_provider_params =\n+        NULL;\n+    res = umfLevelZeroMemoryProviderParamsCreate(&ze_memory_provider_params);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"Failed to create memory provider params!\\n\");\n+        ret = -1;\n+        goto level_zero_destroy;\n+    }\n+\n+    res = umfLevelZeroMemoryProviderParamsSetContext(ze_memory_provider_params,\n+                                                     hContext);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"Failed to set context in memory provider params!\\n\");\n+        ret = -1;\n+        goto provider_params_destroy;\n+    }\n+\n+    res = umfLevelZeroMemoryProviderParamsSetDevice(ze_memory_provider_params,\n+                                                    hDevice);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr, \"Failed to set device in memory provider params!\\n\");\n+        ret = -1;\n+        goto provider_params_destroy;\n+    }\n+\n     // Set the memory type to shared to allow the memory to be accessed on both\n     // CPU and GPU.\n-    ze_memory_provider_params.memory_type = UMF_MEMORY_TYPE_SHARED;\n+    res = umfLevelZeroMemoryProviderParamsSetMemoryType(\n+        ze_memory_provider_params, UMF_MEMORY_TYPE_SHARED);\n+    if (res != UMF_RESULT_SUCCESS) {\n+        fprintf(stderr,\n+                \"Failed to set memory type in memory provider params!\\n\");\n+        ret = -1;\n+        goto provider_params_destroy;\n+    }\n \n     // Create Level Zero memory provider\n     umf_memory_provider_handle_t ze_memory_provider;\n-    res = umfMemoryProviderCreate(umfLevelZeroMemoryProviderOps(),\n-                                  &ze_memory_provider_params,\n-                                  &ze_memory_provider);\n+    res =\n+        umfMemoryProviderCreate(umfLevelZeroMemoryProviderOps(),\n+                                ze_memory_provider_params, &ze_memory_provider);\n     if (res != UMF_RESULT_SUCCESS) {\n         fprintf(stderr, \"Failed to create a memory provider!\\n\");\n         ret = -1;\n-        goto level_zero_destroy;\n+        goto provider_params_destroy;\n     }\n \n     printf(\"Level Zero memory provider created at %p\\n\",\n@@ -154,6 +183,9 @@ int main(void) {\n memory_provider_destroy:\n     umfMemoryProviderDestroy(ze_memory_provider);\n \n+provider_params_destroy:\n+    umfLevelZeroMemoryProviderParamsDestroy(ze_memory_provider_params);\n+\n level_zero_destroy:\n     ret = destroy_context(hContext);\n     return ret;\ndiff --git a/include/umf/providers/provider_level_zero.h b/include/umf/providers/provider_level_zero.h\nindex b3cc02851..f760c5724 100644\n--- a/include/umf/providers/provider_level_zero.h\n+++ b/include/umf/providers/provider_level_zero.h\n@@ -17,20 +17,56 @@ extern \"C\" {\n typedef struct _ze_device_handle_t *ze_device_handle_t;\n typedef struct _ze_context_handle_t *ze_context_handle_t;\n \n-/// @brief Level Zero Memory Provider settings struct\n-typedef struct level_zero_memory_provider_params_t {\n-    ze_context_handle_t\n-        level_zero_context_handle; ///< Handle to the Level Zero context\n-    ze_device_handle_t\n-        level_zero_device_handle; ///< Handle to the Level Zero device\n-\n-    umf_usm_memory_type_t memory_type; ///< Allocation memory type\n-\n-    ze_device_handle_t *\n-        resident_device_handles; ///< Array of devices for which the memory should be made resident\n-    uint32_t\n-        resident_device_count; ///< Number of devices for which the memory should be made resident\n-} level_zero_memory_provider_params_t;\n+struct umf_level_zero_memory_provider_params_t;\n+\n+/// @brief handle to the parameters of the Level Zero Memory Provider.\n+typedef struct umf_level_zero_memory_provider_params_t\n+    *umf_level_zero_memory_provider_params_handle_t;\n+\n+/// @brief  Create a struct to store parameters of the Level Zero Memory Provider.\n+/// @param  hParams [out] handle to the newly created parameters struct.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfLevelZeroMemoryProviderParamsCreate(\n+    umf_level_zero_memory_provider_params_handle_t *hParams);\n+\n+/// @brief  Destroy parameters struct.\n+/// @param  hParams handle to the parameters of the Level Zero Memory Provider.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfLevelZeroMemoryProviderParamsDestroy(\n+    umf_level_zero_memory_provider_params_handle_t hParams);\n+\n+/// @brief  Set the Level Zero context handle in the parameters struct.\n+/// @param  hParams handle to the parameters of the Level Zero Memory Provider.\n+/// @param  hContext handle to the Level Zero context. Cannot be \\p NULL.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfLevelZeroMemoryProviderParamsSetContext(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_context_handle_t hContext);\n+\n+/// @brief  Set the Level Zero device handle in the parameters struct.\n+/// @param  hParams handle to the parameters of the Level Zero Memory Provider.\n+/// @param  hDevice handle to the Level Zero device. Can be \\p NULL if memory type is \\p UMF_MEMORY_TYPE_HOST.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfLevelZeroMemoryProviderParamsSetDevice(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_device_handle_t hDevice);\n+\n+/// @brief  Set the memory type in the parameters struct.\n+/// @param  hParams handle to the parameters of the Level Zero Memory Provider.\n+/// @param  memoryType memory type.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfLevelZeroMemoryProviderParamsSetMemoryType(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    umf_usm_memory_type_t memoryType);\n+\n+/// @brief  Set the resident devices in the parameters struct.\n+/// @param  hParams handle to the parameters of the Level Zero Memory Provider.\n+/// @param  hDevices array of devices for which the memory should be made resident.\n+/// @param  deviceCount number of devices for which the memory should be made resident.\n+/// @return UMF_RESULT_SUCCESS on success or appropriate error code on failure.\n+umf_result_t umfLevelZeroMemoryProviderParamsSetResidentDevices(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_device_handle_t *hDevices, uint32_t deviceCount);\n \n umf_memory_provider_ops_t *umfLevelZeroMemoryProviderOps(void);\n \ndiff --git a/src/libumf.def b/src/libumf.def\nindex 8f29f4579..42a924bd7 100644\n--- a/src/libumf.def\n+++ b/src/libumf.def\n@@ -28,6 +28,12 @@ EXPORTS\n     umfGetIPCHandle\n     umfGetLastFailedMemoryProvider\n     umfLevelZeroMemoryProviderOps\n+    umfLevelZeroMemoryProviderParamsCreate\n+    umfLevelZeroMemoryProviderParamsDestroy\n+    umfLevelZeroMemoryProviderParamsSetContext\n+    umfLevelZeroMemoryProviderParamsSetDevice\n+    umfLevelZeroMemoryProviderParamsSetMemoryType\n+    umfLevelZeroMemoryProviderParamsSetResidentDevices\n     umfMemoryProviderAlloc\n     umfMemoryProviderAllocationMerge\n     umfMemoryProviderAllocationSplit\ndiff --git a/src/libumf.map b/src/libumf.map\nindex f70d247b5..443194e55 100644\n--- a/src/libumf.map\n+++ b/src/libumf.map\n@@ -22,6 +22,12 @@ UMF_1.0 {\n         umfGetIPCHandle;\n         umfGetLastFailedMemoryProvider;\n         umfLevelZeroMemoryProviderOps;\n+        umfLevelZeroMemoryProviderParamsCreate;\n+        umfLevelZeroMemoryProviderParamsDestroy;\n+        umfLevelZeroMemoryProviderParamsSetContext;\n+        umfLevelZeroMemoryProviderParamsSetDevice;\n+        umfLevelZeroMemoryProviderParamsSetMemoryType;\n+        umfLevelZeroMemoryProviderParamsSetResidentDevices;\n         umfMemoryProviderAlloc;\n         umfMemoryProviderAllocationMerge;\n         umfMemoryProviderAllocationSplit;\ndiff --git a/src/provider/provider_level_zero.c b/src/provider/provider_level_zero.c\nindex 6b4468da6..f4a3e97c2 100644\n--- a/src/provider/provider_level_zero.c\n+++ b/src/provider/provider_level_zero.c\n@@ -16,6 +16,51 @@\n \n #if defined(UMF_NO_LEVEL_ZERO_PROVIDER)\n \n+umf_result_t umfLevelZeroMemoryProviderParamsCreate(\n+    umf_level_zero_memory_provider_params_handle_t *hParams) {\n+    (void)hParams;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsDestroy(\n+    umf_level_zero_memory_provider_params_handle_t hParams) {\n+    (void)hParams;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetContext(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_context_handle_t hContext) {\n+    (void)hParams;\n+    (void)hContext;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetDevice(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_device_handle_t hDevice) {\n+    (void)hParams;\n+    (void)hDevice;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetMemoryType(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    umf_usm_memory_type_t memoryType) {\n+    (void)hParams;\n+    (void)memoryType;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetResidentDevices(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_device_handle_t *hDevices, uint32_t deviceCount) {\n+    (void)hParams;\n+    (void)hDevices;\n+    (void)deviceCount;\n+    return UMF_RESULT_ERROR_NOT_SUPPORTED;\n+}\n+\n umf_memory_provider_ops_t *umfLevelZeroMemoryProviderOps(void) {\n     // not supported\n     return NULL;\n@@ -24,6 +69,7 @@ umf_memory_provider_ops_t *umfLevelZeroMemoryProviderOps(void) {\n #else // !defined(UMF_NO_LEVEL_ZERO_PROVIDER)\n \n #include \"base_alloc_global.h\"\n+#include \"libumf.h\"\n #include \"utils_assert.h\"\n #include \"utils_common.h\"\n #include \"utils_concurrency.h\"\n@@ -32,6 +78,21 @@ umf_memory_provider_ops_t *umfLevelZeroMemoryProviderOps(void) {\n #include \"utils_sanitizers.h\"\n #include \"ze_api.h\"\n \n+// Level Zero Memory Provider settings struct\n+typedef struct umf_level_zero_memory_provider_params_t {\n+    ze_context_handle_t\n+        level_zero_context_handle; ///< Handle to the Level Zero context\n+    ze_device_handle_t\n+        level_zero_device_handle; ///< Handle to the Level Zero device\n+\n+    umf_usm_memory_type_t memory_type; ///< Allocation memory type\n+\n+    ze_device_handle_t *\n+        resident_device_handles; ///< Array of devices for which the memory should be made resident\n+    uint32_t\n+        resident_device_count; ///< Number of devices for which the memory should be made resident\n+} umf_level_zero_memory_provider_params_t;\n+\n typedef struct ze_memory_provider_t {\n     ze_context_handle_t context;\n     ze_device_handle_t device;\n@@ -134,26 +195,127 @@ static void init_ze_global_state(void) {\n     }\n }\n \n+umf_result_t umfLevelZeroMemoryProviderParamsCreate(\n+    umf_level_zero_memory_provider_params_handle_t *hParams) {\n+    libumfInit();\n+    if (!hParams) {\n+        LOG_ERR(\"Level zero memory provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    umf_level_zero_memory_provider_params_t *params =\n+        umf_ba_global_alloc(sizeof(umf_level_zero_memory_provider_params_t));\n+    if (!params) {\n+        LOG_ERR(\"Cannot allocate memory for Level Zero memory provider params\");\n+        return UMF_RESULT_ERROR_OUT_OF_HOST_MEMORY;\n+    }\n+\n+    // Assign default values\n+    params->level_zero_context_handle = NULL;\n+    params->level_zero_device_handle = NULL;\n+    params->memory_type = UMF_MEMORY_TYPE_UNKNOWN;\n+    params->resident_device_handles = NULL;\n+    params->resident_device_count = 0;\n+\n+    *hParams = params;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsDestroy(\n+    umf_level_zero_memory_provider_params_handle_t hParams) {\n+    umf_ba_global_free(hParams);\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetContext(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_context_handle_t hContext) {\n+    if (!hParams) {\n+        LOG_ERR(\"Level zero memory provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    if (!hContext) {\n+        LOG_ERR(\"Level zero context handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->level_zero_context_handle = hContext;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetDevice(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_device_handle_t hDevice) {\n+    if (!hParams) {\n+        LOG_ERR(\"Level zero memory provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->level_zero_device_handle = hDevice;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetMemoryType(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    umf_usm_memory_type_t memoryType) {\n+    if (!hParams) {\n+        LOG_ERR(\"Level zero memory provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->memory_type = memoryType;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n+umf_result_t umfLevelZeroMemoryProviderParamsSetResidentDevices(\n+    umf_level_zero_memory_provider_params_handle_t hParams,\n+    ze_device_handle_t *hDevices, uint32_t deviceCount) {\n+    if (!hParams) {\n+        LOG_ERR(\"Level zero memory provider params handle is NULL\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    if (deviceCount && !hDevices) {\n+        LOG_ERR(\"Resident devices array is NULL, but deviceCount is not zero\");\n+        return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n+    }\n+\n+    hParams->resident_device_handles = hDevices;\n+    hParams->resident_device_count = deviceCount;\n+\n+    return UMF_RESULT_SUCCESS;\n+}\n+\n static umf_result_t ze_memory_provider_initialize(void *params,\n                                                   void **provider) {\n     if (params == NULL) {\n         return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n     }\n \n-    level_zero_memory_provider_params_t *ze_params =\n-        (level_zero_memory_provider_params_t *)params;\n+    umf_level_zero_memory_provider_params_handle_t ze_params =\n+        (umf_level_zero_memory_provider_params_handle_t)params;\n \n     if (!ze_params->level_zero_context_handle) {\n+        LOG_ERR(\"Level Zero context handle is NULL\");\n         return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n     }\n \n     if ((ze_params->memory_type == UMF_MEMORY_TYPE_HOST) ==\n         (ze_params->level_zero_device_handle != NULL)) {\n+        LOG_ERR(\"Level Zero device handle is NULL\");\n         return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n     }\n \n-    if ((bool)ze_params->resident_device_count !=\n-        (ze_params->resident_device_handles != NULL)) {\n+    if ((bool)ze_params->resident_device_count &&\n+        (ze_params->resident_device_handles == NULL)) {\n+        LOG_ERR(\"Resident devices handles array is NULL, but device_count is \"\n+                \"not zero\");\n         return UMF_RESULT_ERROR_INVALID_ARGUMENT;\n     }\n \n@@ -166,6 +328,7 @@ static umf_result_t ze_memory_provider_initialize(void *params,\n     ze_memory_provider_t *ze_provider =\n         umf_ba_global_alloc(sizeof(ze_memory_provider_t));\n     if (!ze_provider) {\n+        LOG_ERR(\"Cannot allocate memory for Level Zero Memory Provider\");\n         return UMF_RESULT_ERROR_OUT_OF_HOST_MEMORY;\n     }\n \n@@ -178,6 +341,7 @@ static umf_result_t ze_memory_provider_initialize(void *params,\n             ze_provider->device, &ze_provider->device_properties));\n \n         if (ret != UMF_RESULT_SUCCESS) {\n+            LOG_ERR(\"Cannot get device properties\");\n             umf_ba_global_free(ze_provider);\n             return ret;\n         }\n@@ -190,6 +354,7 @@ static umf_result_t ze_memory_provider_initialize(void *params,\n         ze_provider->resident_device_handles = umf_ba_global_alloc(\n             sizeof(ze_device_handle_t) * ze_params->resident_device_count);\n         if (!ze_provider->resident_device_handles) {\n+            LOG_ERR(\"Cannot allocate memory for resident devices\");\n             umf_ba_global_free(ze_provider);\n             return UMF_RESULT_ERROR_OUT_OF_HOST_MEMORY;\n         }\n", "instance_id": "oneapi-src__unified-memory-framework-920", "clarity": 3, "difficulty": 0.75, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly outlines the issue of backward compatibility in configuration structures for memory providers in the UMF library, specifically focusing on the `level_zero_memory_provider_params_t` structure. The goal is explicitly defined: to ensure backward compatibility when the library evolves and new fields are added to configuration structures. The statement provides detailed background information, including a real-world example of a crash caused by uninitialized new fields after a library update. It also includes multiple potential solutions (three options) with detailed explanations, code snippets, and design considerations, which help in understanding the problem and evaluating trade-offs. Inputs, outputs, and constraints are implicitly clear through the context of API usage and structure definitions. There are no significant ambiguities, and the problem is supported by examples of both the issue and proposed fixes. The only minor gap is the lack of explicit mention of specific edge cases in the implementation details (marked as TBD), but this does not detract from the overall clarity due to the depth of discussion provided.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, impacting multiple files (e.g., benchmarks, examples, headers, and core provider logic) and requiring a shift from direct structure access to a handle-based API approach for configuration management. This involves not just adding new functions but also refactoring existing code to use the new API, as seen in the changes to `umfMemoryProviderCreate` calls and the introduction of setter/getter functions for configuration parameters. Second, the problem demands a deep understanding of several technical concepts, including ABI compatibility, dynamic library versioning, memory management in C, and the specifics of the Level Zero API for GPU memory handling. Third, the architectural impact is notable since the solution (Option 1: handle-based approach) changes how configuration data is managed, moving from exposed structures to opaque handles, which affects the library's public interface and client code. Fourth, while edge cases are not exhaustively detailed in the statement, the code changes and problem description imply the need to handle scenarios like invalid handles, null pointers, and version mismatches, requiring robust error handling. The combination of these factors\u2014cross-file modifications, conceptual depth, architectural changes, and error handling\u2014makes this a challenging problem that requires significant experience in systems programming and library design. It does not reach the \"Very Hard\" category (0.8-1.0) as it does not involve extremely complex domain-specific knowledge or system-level redesign beyond the library's configuration management.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc1b [Bug] BF16 causing unspported numpy dtype error in create_constant\n##  Bug Description\r\n\r\n```bash\r\nWARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\r\nINFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)\r\nINFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.bf16: 10>}, debug=True, workspace_size=0, min_block_size=1, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)\r\n\r\nDEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:\r\ngraph():\r\n    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\r\n    %l__self___linear : [num_users=1] = call_module[target=L__self___linear](args = (%l_x_,), kwargs = {})\r\n    return (l__self___linear,)\r\nDEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:\r\ngraph():\r\n    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\r\n    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_x_,), kwargs = {})\r\n    %l__self___linear : [num_users=1] = call_module[target=L__self___linear](args = (%clone_default,), kwargs = {})\r\n    return (l__self___linear,)\r\nDEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:\r\ngraph():\r\n    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\r\n    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_x_,), kwargs = {})\r\n    %l__self___linear : [num_users=1] = call_module[target=L__self___linear](args = (%clone_default,), kwargs = {})\r\n    return (l__self___linear,)\r\nDEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:\r\ngraph():\r\n    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\r\n    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_x_,), kwargs = {})\r\n    %l__self___linear : [num_users=1] = call_module[target=L__self___linear](args = (%clone_default,), kwargs = {})\r\n    return (l__self___linear,)\r\nDEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:\r\ngraph():\r\n    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\r\n    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})\r\n    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]\r\n    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant0, [1, 0]), kwargs = {})\r\n    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]\r\n    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %clone, %permute), kwargs = {})\r\n    return (addmm,)\r\nDEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.\r\nDEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:\r\ngraph():\r\n    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\r\n    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]\r\n    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant0, [1, 0]), kwargs = {})\r\n    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]\r\n    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0_1, %permute), kwargs = {})\r\n    return (addmm,)\r\nDEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:\r\ngraph():\r\n    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\r\n    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]\r\n    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]\r\n    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0_1, %_frozen_param0), kwargs = {})\r\n    return (addmm,)\r\nDEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:\r\n graph():\r\n    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\r\n    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]\r\n    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]\r\n    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0_1, %_frozen_param0), kwargs = {})\r\n    return (addmm,)\r\nDEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:\r\nSupported Nodes:\r\n- torch.ops.aten.addmm.default + Operator Count: 1\r\n\r\nDEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:\r\nAll Nodes Supported\r\n\r\nDEBUG:torch_tensorrt.dynamo._compiler:Detected support for 1 operators out of 1 in subgraph.\r\nWARNING:torch_tensorrt.dynamo._compiler:Node _param_constant1 of op type get_attr does not have metadata. This could sometimes lead to undefined behavior.\r\nWARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.\r\nDEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\r\nNumber of TensorRT-Accelerated Engines Generated: 1\r\nDEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\r\nSupported Nodes:\r\n- torch.ops.aten.addmm.default + Operator Count: 1\r\n\r\nDEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:\r\nAll Nodes Supported\r\n\r\nDEBUG:torch_tensorrt.dynamo._compiler:Submodule name: _run_on_acc_0\r\n Input shapes: [(128, 20)]\r\n graph():\r\n    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]\r\n    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]\r\n    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]\r\n    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0_1, %_frozen_param0), kwargs = {})\r\n    return addmm\r\nINFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +3, GPU +0, now: CPU 12984, GPU 1045 (MiB)\r\nINFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +2657, GPU +308, now: CPU 15907, GPU 1353 (MiB)\r\nDEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[128, 20], dtype=DataType.BF16]\r\nDEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node addmm (kind: aten.addmm.default, args: ('<torch.Tensor as np.ndarray [shape=(30,), dtype=float32]>', 'arg0_1 <tensorrt.ITensor [shape=(128, 20), dtype=DataType.BF16]>', '<torch.Tensor as np.ndarray [shape=(20, 30), dtype=float32]>'))\r\nDEBUG:torch_tensorrt.dynamo.conversion.converter_utils:Freezing tensor addmm_constant_0 to TRT IConstantLayer\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HolyWu\\Downloads\\test.py\", line 29, in <module>\r\n    optimized_model(*inputs)\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1552, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1561, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 432, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1552, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1561, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1115, in __call__\r\n    return self._torchdynamo_orig_callable(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 947, in __call__\r\n    result = self._inner_convert(\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 471, in __call__\r\n    return _compile(\r\n           ^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_utils_internal.py\", line 83, in wrapper_function\r\n    return StrobelightCompileTimeProfiler.profile_compile_time(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_strobelight\\compile_time_profiler.py\", line 129, in profile_compile_time\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\contextlib.py\", line 81, in inner\r\n    return func(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 816, in _compile\r\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 232, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 635, in compile_inner\r\n    out_code = transform_code_object(code, transform)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1184, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 177, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 581, in transform\r\n    tracer.run()\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2455, in run\r\n    super().run()\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 897, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 809, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2646, in RETURN_VALUE\r\n    self._return(inst)\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2631, in _return\r\n    self.output.compile_subgraph(\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1097, in compile_subgraph\r\n    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\r\n  File \"C:\\Python312\\Lib\\contextlib.py\", line 81, in inner\r\n    return func(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1314, in compile_and_call_fx_graph\r\n    compiled_fn = self.call_user_compiler(gm)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 232, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1405, in call_user_compiler\r\n    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1386, in call_user_compiler\r\n    compiled_fn = compiler_fn(gm, self.example_inputs())\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 128, in __call__\r\n    compiled_gm = compiler_fn(gm, example_inputs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\__init__.py\", line 1989, in __call__\r\n    return self.compiler_fn(model_, inputs_, **self.kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\backend\\backends.py\", line 44, in torch_tensorrt_backend\r\n    return DEFAULT_BACKEND(gm, sample_inputs, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\backend\\backends.py\", line 52, in aot_torch_tensorrt_aten_backend\r\n    return _pretraced_backend(gm, sample_inputs, settings)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\backend\\backends.py\", line 108, in _pretraced_backend\r\n    trt_compiled = compile_module(\r\n                   ^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\_compiler.py\", line 412, in compile_module\r\n    trt_module = convert_module(\r\n                 ^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\_conversion.py\", line 106, in convert_module\r\n    interpreter_result = interpret_module_to_result(module, inputs, settings)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\_conversion.py\", line 87, in interpret_module_to_result\r\n    interpreter_result = interpreter.run()\r\n                         ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\_TRTInterpreter.py\", line 310, in run\r\n    super().run()\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\fx\\interpreter.py\", line 145, in run\r\n    self.env[node] = self.run_node(node)\r\n                     ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\_TRTInterpreter.py\", line 349, in run_node\r\n    trt_node: torch.fx.Node = super().run_node(n)\r\n                              ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch\\fx\\interpreter.py\", line 202, in run_node\r\n    return getattr(self, n.op)(n.target, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\_TRTInterpreter.py\", line 457, in call_function\r\n    return converter(self.ctx, target, args, kwargs, self._cur_node_name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\converter_utils.py\", line 469, in convert_with_type_enforcement\r\n    return func(ctx, target, new_args, new_kwargs, name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\aten_ops_converters.py\", line 2714, in aten_ops_addmm\r\n    return impl.addmm.addmm(\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\impl\\addmm.py\", line 24, in addmm\r\n    mm = impl.matmul.matrix_multiply(ctx, target, source_ir, f\"{name}_mm\", mat1, mat2)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\impl\\matmul.py\", line 28, in matrix_multiply\r\n    other = get_trt_tensor(\r\n            ^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\converter_utils.py\", line 328, in get_trt_tensor\r\n    return create_constant(ctx, input_val, name, dtype, min_rank)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\dynamo\\conversion\\converter_utils.py\", line 287, in create_constant\r\n    value, _enums.dtype._from(dtype).to(np.dtype) if dtype is not None else None\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\site-packages\\torch_tensorrt\\_enums.py\", line 279, in to\r\n    raise TypeError(\"Unspported numpy dtype\")\r\ntorch._dynamo.exc.BackendCompilerFailed: backend='torch_tensorrt_backend' raised:\r\nTypeError: Unspported numpy dtype\r\n\r\nWhile executing %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0_1, %_frozen_param0), kwargs = {_itensor_to_tensor_meta: {<tensorrt_bindings.tensorrt.ITensor object at 0x000001E4A3D7F930>: ((128, 20), torch.bfloat16, False, (20, 1), torch.contiguous_format, False, {})}})\r\nOriginal traceback:\r\nNone\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n```\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch_tensorrt\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = torch.nn.Linear(20, 30)\r\n\r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n\r\ndevice = torch.device(\"cuda\", 0)\r\nmodel = MyModule().eval().to(device).bfloat16()\r\ninputs = [torch.randn((128, 20), dtype=torch.bfloat16, device=device)]\r\n\r\nwith torch.inference_mode():\r\n    optimized_model = torch_tensorrt.compile(\r\n        model,\r\n        ir=\"torch_compile\",\r\n        inputs=inputs,\r\n        enabled_precisions={torch.bfloat16},\r\n        debug=True,\r\n        min_block_size=1,\r\n        device=device,\r\n    )\r\n\r\n    optimized_model(*inputs)\r\n```\r\n\r\n## Environment\r\n\r\n - Torch-TensorRT Version (e.g. 1.0.0): 2.4.0.dev20240607+cu124\r\n - PyTorch Version (e.g. 1.0): 2.4.0.dev20240607+cu124\r\n - CPU Architecture: x64\r\n - OS (e.g., Linux): Windows 11\r\n - How you installed PyTorch (`conda`, `pip`, `libtorch`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Are you using local sources or building from archives:\r\n - Python version: 3.12.3\r\n - CUDA version: 12.4\r\n - GPU models and configuration: RTX 3050\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nAdding `use_default=True` argument to `to(np.dtype)` at https://github.com/pytorch/TensorRT/blob/4a0184a2758b2e77c92f683a6b707b6afb73e349/py/torch_tensorrt/dynamo/conversion/converter_utils.py#L287 can make the compilation succeed. But I'm not sure if you'd like to solve it in the other way.\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 4d6b9eacc7..0605f7706e 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -51,6 +51,10 @@ repos:\n     hooks:\n       - id: black\n         exclude: ^examples/custom_converters/elu_converter/setup.py|^docs\n+  - repo: https://github.com/crate-ci/typos\n+    rev: v1.22.9\n+    hooks:\n+      - id: typos\n   - repo: local\n     hooks:\n     -   id: dont-commit-upstream\ndiff --git a/CODE_OF_CONDUCT.md b/CODE_OF_CONDUCT.md\nindex 08b500a221..fe49a253d0 100644\n--- a/CODE_OF_CONDUCT.md\n+++ b/CODE_OF_CONDUCT.md\n@@ -6,7 +6,7 @@ In the interest of fostering an open and welcoming environment, we as\n contributors and maintainers pledge to make participation in our project and\n our community a harassment-free experience for everyone, regardless of age, body\n size, disability, ethnicity, sex characteristics, gender identity and expression,\n-level of experience, education, socio-economic status, nationality, personal\n+level of experience, education, socioeconomic status, nationality, personal\n appearance, race, religion, or sexual identity and orientation.\n \n ## Our Standards\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex ba9ab32cf6..930321614b 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -4,9 +4,9 @@\n \n Do try to fill an issue with your feature or bug before filling a PR (op support is generally an exception as long as you provide tests to prove functionality). There is also a backlog (https://github.com/pytorch/TensorRT/issues) of issues which are tagged with the area of focus, a coarse priority level and whether the issue may be accessible to new contributors. Let us know if you are interested in working on a issue. We are happy to provide guidance and mentorship for new contributors. Though note, there is no claiming of issues, we prefer getting working code quickly vs. addressing concerns about \"wasted work\".\n \n-#### Development enviornment\n+#### Development environment\n \n-Our build system relies on `bazel` (https://bazel.build/). Though there are many ways to install `bazel`, the prefered method is to use `bazelisk` (https://github.com/bazelbuild/bazelisk) which makes it simple to set up the correct version of bazel on the fly. Additional developement dependencies can be installed via the `requirements-dev.txt` file.\n+Our build system relies on `bazel` (https://bazel.build/). Though there are many ways to install `bazel`, the preferred method is to use `bazelisk` (https://github.com/bazelbuild/bazelisk) which makes it simple to set up the correct version of bazel on the fly. Additional development dependencies can be installed via the `requirements-dev.txt` file.\n \n #### Communication\n \n@@ -27,7 +27,7 @@ We use the PyTorch Slack for communication about core development, integration w\n \n - Avoid introducing unnecessary complexity into existing code so that maintainability and readability are preserved\n \n-- Try to avoid commiting commented out code\n+- Try to avoid committing commented out code\n \n - Minimize warnings (and no errors) from the compiler\n \ndiff --git a/cmake/paths.cmake b/cmake/paths.cmake\nindex 3822c698ff..b80b18a6d3 100644\n--- a/cmake/paths.cmake\n+++ b/cmake/paths.cmake\n@@ -6,7 +6,7 @@ set(ARCHIVE_OUTPUT_DIRECTORY \"lib\")\n set(RUNTIME_OUTPUT_DIRECTORY \"bin\")\n set(HEADERS_OUTPUT_DIRECTORY \"include\")\n \n-#Set target ouput directory in the build directory\n+#Set target output directory in the build directory\n set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/${ARCHIVE_OUTPUT_DIRECTORY}\")\n set(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/${LIBRARY_OUTPUT_DIRECTORY}\")\n set(CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/${RUNTIME_OUTPUT_DIRECTORY}\")\ndiff --git a/dev_dep_versions.yml b/dev_dep_versions.yml\nindex 5c560c41f9..a8da87116c 100644\n--- a/dev_dep_versions.yml\n+++ b/dev_dep_versions.yml\n@@ -1,3 +1,2 @@\n-__version__: \"2.5.0.dev0\"\n __cuda_version__: \"12.4\"\n __tensorrt_version__: \"10.0.1\"\ndiff --git a/docker/Dockerfile b/docker/Dockerfile\nindex 79c55a2e93..9d20c2f6e8 100644\n--- a/docker/Dockerfile\n+++ b/docker/Dockerfile\n@@ -50,7 +50,7 @@ RUN TENSORRT_MAJOR_VERSION=`echo ${TENSORRT_VERSION} | cut -d '.' -f 1` && \\\n RUN wget -q https://github.com/bazelbuild/bazelisk/releases/download/v1.17.0/bazelisk-linux-amd64 -O /usr/bin/bazel &&\\\n     chmod a+x /usr/bin/bazel\n \n-# Build Torch-TensorRT in an auxillary container\n+# Build Torch-TensorRT in an auxiliary container\n FROM base as torch-tensorrt-builder-base\n \n ARG ARCH=\"x86_64\"\ndiff --git a/docker/README.md b/docker/README.md\nindex 824dae4d54..bddaaab0ba 100644\n--- a/docker/README.md\n+++ b/docker/README.md\n@@ -35,7 +35,7 @@ nvidia-docker run --gpus all -it --shm-size=8gb --env=\"DISPLAY\" --volume=\"/tmp/.\n Test:\n \n \n-You can run any converter test to verify if Torch-TRT built sucessfully inside the container. Once you launch the container, you can run\n+You can run any converter test to verify if Torch-TRT built successfully inside the container. Once you launch the container, you can run\n ```\n bazel test //tests/core/conversion/converters:test_activation --compilation_mode=opt --test_output=summary --config use_precompiled_torchtrt --config pre_cxx11_abi\n ```\ndiff --git a/docsrc/RELEASE_CHECKLIST.md b/docsrc/RELEASE_CHECKLIST.md\nindex 77bb973487..0900d0b2bd 100644\n--- a/docsrc/RELEASE_CHECKLIST.md\n+++ b/docsrc/RELEASE_CHECKLIST.md\n@@ -9,7 +9,7 @@ While Torch-TensorRT is in alpha, patch versions are bumped sequentially on brea\n In beta Torch-TensorRT will get a minor version bump on breaking changes, or upgrade to the next version of PyTorch, patch version will be incremented based on significant bug fixes, or siginficant new functionality in the compiler.\n \n Once Torch-TensorRT hits version 1.0.0, major versions are bumped on breaking API changes, breaking changes or significant new functionality in the compiler\n-will result in a minor version bump and sigificant bug fixes will result in a patch version change.\n+will result in a minor version bump and significant bug fixes will result in a patch version change.\n \n ## Steps to Packaging a Release\n \n@@ -50,7 +50,7 @@ will result in a minor version bump and sigificant bug fixes will result in a pa\n             - `[3, 1920, 1080]` (P2)\n     - Batch Sizes: 1, 4, 8, 16, 32\n     - Frameworks: PyTorch, Torch-TensorRT, ONNX + TRT\n-        - If any models do not convert to ONNX / TRT, that is fine. Mark them as failling / no result\n+        - If any models do not convert to ONNX / TRT, that is fine. Mark them as failing / no result\n     - Devices:\n         - A100 (P0)\n         - A30 / A30 MIG (P1) (same batches as T4\ndiff --git a/docsrc/conf.py b/docsrc/conf.py\nindex d1cae714cc..2e782358cb 100644\n--- a/docsrc/conf.py\n+++ b/docsrc/conf.py\n@@ -25,7 +25,7 @@\n # -- Project information -----------------------------------------------------\n \n project = \"Torch-TensorRT\"\n-copyright = \"2022, NVIDIA Corporation\"\n+copyright = \"2024, NVIDIA Corporation\"\n author = \"NVIDIA Corporation\"\n \n version = f\"v{torch_tensorrt.__version__}\"\n@@ -151,6 +151,9 @@\n     \"master_doc\": True,\n     \"version_info\": {\n         \"main\": \"https://pytorch.org/TensorRT/\",\n+        \"v2.3.0\": \"https://pytorch.org/TensorRT/v2.3.0\",\n+        \"v2.2.0\": \"https://pytorch.org/TensorRT/v2.2.0\",\n+        \"v2.1.0\": \"https://pytorch.org/TensorRT/v2.1.0\",\n         \"v1.4.0\": \"https://pytorch.org/TensorRT/v1.4.0\",\n         \"v1.3.0\": \"https://pytorch.org/TensorRT/v1.3.0\",\n         \"v1.2.0\": \"https://pytorch.org/TensorRT/v1.2.0\",\n@@ -186,6 +189,8 @@\n \n nbsphinx_execute = \"never\"\n \n+autodoc_member_order = \"groupwise\"\n+\n # -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n # See http://stackoverflow.com/a/41184353/3343043\n \ndiff --git a/docsrc/contributors/conversion.rst b/docsrc/contributors/conversion.rst\nindex f19fc5eba8..fdb477bc67 100644\n--- a/docsrc/contributors/conversion.rst\n+++ b/docsrc/contributors/conversion.rst\n@@ -3,7 +3,7 @@\n Conversion Phase\n ==================\n \n-Once the graph has be simplified to a form thats easy to convert, we then set up a conversion context\n+Once the graph has be simplified to a form that's easy to convert, we then set up a conversion context\n to manage the construction of a TensorRT ``INetworkDefinition`` from the blocks nodes. The conversion context\n records the set of converted nodes, block inputs and outputs and other information about the conversion\n of the graph. This data is then used to help converters link together layers and also hold build time\ndiff --git a/docsrc/contributors/dynamo_converters.rst b/docsrc/contributors/dynamo_converters.rst\nindex 3238d609f3..7cc85f5bea 100644\n--- a/docsrc/contributors/dynamo_converters.rst\n+++ b/docsrc/contributors/dynamo_converters.rst\n@@ -36,7 +36,7 @@ The decorator takes a number of arguments:\n All that is required for a converter is the key.\n \n The function body is responsible for taking the current state of the network and adding the next subgraph to perform the op specified in the decorator with TensorRT operations.\n-The function is provided arguments as the native PyTorch op would be provided with the added case of numpy arrays for frozen Tensor attributes or TensorRT ITensors which are ouput Tensors of previous nodes, correspoding to edges/output Tensors of intermediate operations in the graph.\n+The function is provided arguments as the native PyTorch op would be provided with the added case of numpy arrays for frozen Tensor attributes or TensorRT ITensors which are output Tensors of previous nodes, corresponding to edges/output Tensors of intermediate operations in the graph.\n To determine the types expected as well as the return type of the converter, look at the definition of the op being converted. In the case of ``aten`` operations, this file will be the source of truth: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml\n Since many converters a developer may write are a composition of lower level operators, instead of needing to implement the converter in raw TensorRT, the ``torch_tensorrt.dynamo.conversion.impl`` subpackage contains many implementations of operations that can be chained to create a TensorRT subgraph.\n \n@@ -53,14 +53,14 @@ Capability Validation\n \n There are some converters which have special cases to be accounted for. In those cases, one should use ``capability_validators`` to register the converter using ``@dynamo_tensorrt_converter``\n We illustrate this through ``torch.ops.aten.embedding.default``. It has parameters - ``scale_grad_by_freq`` and ``sparse`` which are not currently supported by the implementation.\n-In such cases we can write validator ``embedding_param_validator`` which implements that given those paramters the converter is not supported and register the converter by\n+In such cases we can write validator ``embedding_param_validator`` which implements that given those parameters the converter is not supported and register the converter by\n \n \n Type Contract\n ^^^^^^^^^^^^^^^\n \n The function is expected to follow the type contract established by the signature. This includes accepting the union of valid PyTorch types + numpy arrays for constant tensors and TensorRT ITensors.\n-In the case that only a subset of types is supported in the converter, you can also add the ``torch_tensorrt.dynamo.conversion.converter_utils.enforce_tensor_types``, which allows you to specify a dictionary mapping between input positions and types that those inputs can take. Where possible the decorator will convert inputs to match these types prefering the order provided.\n+In the case that only a subset of types is supported in the converter, you can also add the ``torch_tensorrt.dynamo.conversion.converter_utils.enforce_tensor_types``, which allows you to specify a dictionary mapping between input positions and types that those inputs can take. Where possible the decorator will convert inputs to match these types preferring the order provided.\n ``int`` keys in the dictionary will refer to positional arguments in ``args``. ``str`` keys will refer to keyword arguments in ``kwargs``.\n \n \n@@ -105,7 +105,7 @@ Some operations do not produce TensorRT subgraphs as a side-effect. These are te\n Operator Decomposition\n -----------------------\n \n-There are some converters which can be decomposed into suboperations in PyTorch and need not have seperate converter registration.\n+There are some converters which can be decomposed into suboperations in PyTorch and need not have separate converter registration.\n Such converters can be implemented via a decomposition\n \n Example: ``addmm``\ndiff --git a/docsrc/contributors/lowering.rst b/docsrc/contributors/lowering.rst\nindex a82f497ed2..69fa587988 100644\n--- a/docsrc/contributors/lowering.rst\n+++ b/docsrc/contributors/lowering.rst\n@@ -30,12 +30,12 @@ Eliminate Dead Code\n \n Dead code elimination will check if a node has side effects and not delete it if it does.\n \n-Eliminate Exeception Or Pass Pattern\n+Eliminate Exception Or Pass Pattern\n ***************************************\n \n     `Torch-TensorRT/core/lowering/passes/exception_elimination.cpp <https://github.com/pytorch/TensorRT/blob/master/core/lowering/passes/exception_elimination.cpp>`_\n \n-A common pattern in scripted modules are dimension gaurds which will throw execptions if\n+A common pattern in scripted modules are dimension guards which will throw exceptions if\n the input dimension is not what was expected.\n \n .. code-block:: none\n@@ -48,9 +48,9 @@ the input dimension is not what was expected.\n         block1():\n         -> ()\n \n-Since we are resolving all of this at compile time and there are no execptions in the TensorRT graph, we just remove it.\n+Since we are resolving all of this at compile time and there are no exceptions in the TensorRT graph, we just remove it.\n \n-Eliminate Redundant Gaurds\n+Eliminate Redundant Guards\n ***************************************\n \n     `torch/csrc/jit/passes/guard_elimination.h <https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/guard_elimination.h>`_\n@@ -63,7 +63,7 @@ Freeze Module\n \n     `torch/csrc/jit/passes/freeze_module.h <https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/freeze_module.h>`_\n \n-Freeze attributes and inline constants and modules. Propogates constants in the graph.\n+Freeze attributes and inline constants and modules. Propagates constants in the graph.\n \n Fuse AddMM Branches\n ***************************************\n@@ -71,7 +71,7 @@ Fuse AddMM Branches\n     `Torch-TensorRT/core/lowering/passes/fuse_addmm_branches.cpp <https://github.com/pytorch/TensorRT/blob/master/core/lowering/passes/fuse_addmm_branches.cpp>`_\n \n A common pattern in scripted modules is tensors of different dimensions use different constructions for implementing linear layers. We fuse these\n-different varients into a single one that will get caught by the Unpack AddMM pass.\n+different variants into a single one that will get caught by the Unpack AddMM pass.\n \n .. code-block:: none\n \n@@ -103,7 +103,7 @@ Fuse Flatten Linear\n \n     `Torch-TensorRT/core/lowering/passes/fuse_flatten_linear.cpp <https://github.com/pytorch/TensorRT/blob/master/core/lowering/passes/fuse_flatten_linear.cpp>`_\n \n-TensorRT implicity flattens input layers into fully connected layers when they are higher than 1D. So when there is a\n+TensorRT implicitly flattens input layers into fully connected layers when they are higher than 1D. So when there is a\n ``aten::flatten`` -> ``aten::linear`` pattern we remove the ``aten::flatten``.\n \n Lower Graph\n@@ -147,7 +147,7 @@ Places delimiting nodes around module calls pre freezing to signify where in the\n \n Looks for delimiters then marks all nodes between the delimiters to tell partitioning to run them in PyTorch\n \n-Peephole Optimze\n+Peephole Optimize\n ***************************************\n \n     `torch/csrc/jit/passes/peephole_optimze.h <https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/ppeephole_optimze.h>`_\n@@ -179,7 +179,7 @@ Remove To\n \n     `Torch-TensorRT/core/lowering/passes/remove_to.cpp <https://github.com/pytorch/TensorRT/blob/master/core/lowering/passes/remove_to.cpp>`_\n \n-Removes ``aten::to`` operators that do casting, since TensorRT mangages it itself. It is important that this is one of the last passes run so that\n+Removes ``aten::to`` operators that do casting, since TensorRT manages it itself. It is important that this is one of the last passes run so that\n other passes have a change to move required cast operators out of the main namespace.\n \n Unpack AddMM\n@@ -204,7 +204,7 @@ Unroll Loops\n \n     `torch/csrc/jit/passes/loop_unrolling.h <https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/loop_unrolling.h>`_\n \n-Unrolls the operations of compatable loops (e.g. sufficently short) so that you only have to go through the loop once.\n+Unrolls the operations of compatible loops (e.g. sufficiently short) so that you only have to go through the loop once.\n \n Replace Tile with Repeat\n ***************************************\ndiff --git a/docsrc/contributors/runtime.rst b/docsrc/contributors/runtime.rst\nindex 23d83b6db2..94021f986c 100644\n--- a/docsrc/contributors/runtime.rst\n+++ b/docsrc/contributors/runtime.rst\n@@ -6,9 +6,9 @@ Runtime Phase\n The Runtime phase is responsible for constructing self standing TorchScript graphs with embedded TensorRT engines and serving as the runtime\n when these engines are called. The main interface accepts a serialized TensorRT engine. The execution phase\n will deserialize and wrap this engine in a class which maintains a execution context for each engine\n-and some metadata about its inputs and outputs and is compatable with the TorchScript interpreter so that\n+and some metadata about its inputs and outputs and is compatible with the TorchScript interpreter so that\n it can be moved around and used like other TorchScript IValues. The engine is run by providing it and inputs\n-to the ``tensorrt::execute_engine`` operator which will take the engine and its inputs and return the results of engine exeuction.\n+to the ``tensorrt::execute_engine`` operator which will take the engine and its inputs and return the results of engine execution.\n \n \n Background\ndiff --git a/docsrc/dynamo/dynamo_export.rst b/docsrc/dynamo/dynamo_export.rst\nindex 7a17cd5df2..f23dce679e 100644\n--- a/docsrc/dynamo/dynamo_export.rst\n+++ b/docsrc/dynamo/dynamo_export.rst\n@@ -27,7 +27,7 @@ usage of the dynamo frontend\n \n .. note::  ``torch_tensorrt.dynamo.compile`` is the main API for users to interact with Torch-TensorRT dynamo frontend. The input type of the model should be ``ExportedProgram`` (ideally the output of ``torch.export.export`` or ``torch_tensorrt.dynamo.trace`` (discussed in the section below)) and output type is a ``torch.fx.GraphModule`` object.\n \n-Customizeable Settings\n+Customizable Settings\n ----------------------\n \n There are lot of options for users to customize their settings for optimizing with TensorRT.\ndiff --git a/docsrc/dynamo/torch_compile.rst b/docsrc/dynamo/torch_compile.rst\nindex 955f64d64c..14498fc0c3 100644\n--- a/docsrc/dynamo/torch_compile.rst\n+++ b/docsrc/dynamo/torch_compile.rst\n@@ -26,7 +26,7 @@ The primary goal of the Torch-TensorRT `torch.compile` backend is to enable Just\n \n The backend can handle a variety of challenging model structures and offers a simple-to-use interface for effective acceleration of models. Additionally, it has many customization options to ensure the compilation process is fitting to the specific use case.\n \n-Customizeable Settings\n+Customizable Settings\n -----------------\n .. autoclass:: CompilationSettings\n \n@@ -87,7 +87,7 @@ If key operators for your model are unsupported, see :ref:`dynamo_conversion` to\n \n Feasibility of Serialization\n ^^^^^^^^^^^^^^^^^\n-Compilation can also be helpful in demonstrating graph breaks and the feasibility of serialization of a particular model. For instance, if a model has no graph breaks and compiles successfully with the Torch-TensorRT backend, then that model should be compileable and serializeable via the `torch_tensorrt` Dynamo IR, as discussed in :ref:`dynamic_shapes`. To determine the number of graph breaks in a model, the `torch._dynamo.explain` function is very useful:\n+Compilation can also be helpful in demonstrating graph breaks and the feasibility of serialization of a particular model. For instance, if a model has no graph breaks and compiles successfully with the Torch-TensorRT backend, then that model should be compilable and serializeable via the `torch_tensorrt` Dynamo IR, as discussed in :ref:`dynamic_shapes`. To determine the number of graph breaks in a model, the `torch._dynamo.explain` function is very useful:\n \n .. code-block:: python\n \ndiff --git a/docsrc/getting_started/getting_started_with_windows.rst b/docsrc/getting_started/getting_started_with_windows.rst\ndeleted file mode 100644\nindex a90221b532..0000000000\n--- a/docsrc/getting_started/getting_started_with_windows.rst\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-.. _getting_started_windows:\n-\n-Building Torch-TensorRT on Windows\n-====================================\n-\n-Torch-TensorRT has community support for Windows platform using CMake\n-\n-Prerequisite:\n-\n-* Microsoft VS 2022 Tools\n-* Bazelisk\n-* CUDA\n-\n-\n-Build steps\n--------------------\n-\n-* Open the app \"x64 Native Tools Command Prompt for VS 2022\" - note that Admin priveleges may be necessary\n-* Ensure Bazelisk (Bazel launcher) is installed on your machine and available from the command line. Package installers such as Chocolatey can be used to install Bazelisk\n-* Install latest version of Torch (i.e. with `pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu124`)\n-* Clone the Torch-TensorRT repository and navigate to its root directory\n-* Run `pip install ninja wheel setuptools`\n-* Run `pip install --pre -r py/requirements.txt`\n-* Run `set DISTUTILS_USE_SDK=1`\n-* Run `python setup.py bdist_wheel`\n-* Run `pip install dist/*.whl`\n-\n-Advanced setup and Troubleshooting\n--------------------\n-In the `WORKSPACE` file, the `cuda_win`, `libtorch_win`, and `tensorrt_win` are Windows-specific modules which can be customized. For instance, if you would like to build with a different version of CUDA, or your CUDA installation is in a non-standard location, update the `path` in the `cuda_win` module.\n-\n-Similarly, if you would like to use a different version of pytorch or tensorrt, customize the `urls` in the `libtorch_win` and `tensorrt_win` modules, respectively.\n-\n-Local versions of these packages can also be used on Windows. See `toolchains\\ci_workspaces\\WORKSPACE.win.release.tmpl` for an example of using a local version of TensorRT on Windows.\ndiff --git a/docsrc/getting_started/installation.rst b/docsrc/getting_started/installation.rst\nindex 379756c347..26f13c0aca 100644\n--- a/docsrc/getting_started/installation.rst\n+++ b/docsrc/getting_started/installation.rst\n@@ -1,15 +1,15 @@\n .. _installation:\n \n Installation\n-=============\n+##################\n \n Precompiled Binaries\n-*********************\n+---------------------\n \n-Torch-TensorRT 2.x is centered primarily around Python. As such, precompiled releases can be found on pypi.org\n+Torch-TensorRT 2.x is centered primarily around Python. As such, precompiled releases can be found on `pypi.org <https://pypi.org/project/torch-tensorrt/>`_\n \n Dependencies\n----------------\n+~~~~~~~~~~~~~~\n \n You need to have CUDA, PyTorch, and TensorRT (python package is sufficient) installed to use Torch-TensorRT\n \n@@ -18,7 +18,7 @@ You need to have CUDA, PyTorch, and TensorRT (python package is sufficient) inst\n \n \n Installing Torch-TensorRT\n----------------------------\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n You can install the python package using\n \n@@ -26,8 +26,10 @@ You can install the python package using\n \n     python -m pip install torch torch-tensorrt tensorrt\n \n+Packages are uploaded for Linux on x86 and Windows\n+\n Installing Torch-TensorRT for a specific CUDA version\n---------------------------------------------------------\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n Similar to PyTorch, Torch-TensorRT has builds compiled for different versions of CUDA. These are distributed on PyTorch's package index\n \n@@ -38,7 +40,7 @@ For example CUDA 11.8\n     python -m pip install torch torch-tensorrt tensorrt --extra-index-url https://download.pytorch.org/whl/cu118\n \n Installing Nightly Builds\n----------------------------\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n Torch-TensorRT distributed nightlies targeting the PyTorch nightly. These can be installed from the PyTorch nightly package index (separated by CUDA version)\n \n@@ -51,19 +53,22 @@ Torch-TensorRT distributed nightlies targeting the PyTorch nightly. These can be\n .. _bin-dist:\n \n C++ Precompiled Binaries (TorchScript Only)\n---------------------------------------------------\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n Precompiled tarballs for releases are provided here: https://github.com/pytorch/TensorRT/releases\n \n .. _compile-from-source:\n \n Compiling From Source\n-******************************************\n+------------------------\n+\n+Building on Linux\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n .. _installing-deps:\n \n-Dependencies for Compilation\n--------------------------------\n+Dependencies\n+^^^^^^^^^^^^^^\n \n * Torch-TensorRT is built with **Bazel**, so begin by installing it.\n \n@@ -95,7 +100,7 @@ Dependencies for Compilation\n     * https://github.com/pytorch/TensorRT/blob/4e5b0f6e860910eb510fa70a76ee3eb9825e7a4d/WORKSPACE#L53C1-L53C1\n \n \n-* **TensorRT** is not required to be installed on the system to build Torch-TensorRT, in fact this is preferable to ensure reproducable builds. If versions other than the default are needed\n+* **TensorRT** is not required to be installed on the system to build Torch-TensorRT, in fact this is preferable to ensure reproducible builds. If versions other than the default are needed\n   point the WORKSPACE file to the URL of the tarball or download the tarball for TensorRT from https://developer.nvidia.com and update the paths in the WORKSPACE file here https://github.com/pytorch/TensorRT/blob/4e5b0f6e860910eb510fa70a76ee3eb9825e7a4d/WORKSPACE#L71\n \n     For example:\n@@ -114,13 +119,13 @@ Dependencies for Compilation\n             ],\n         )\n \n-    Remember at runtime, these libraries must be added to your ``LD_LIBRARY_PATH`` explicity\n+    Remember at runtime, these libraries must be added to your ``LD_LIBRARY_PATH`` explicitly\n \n If you have a local version of TensorRT installed, this can be used as well by commenting out the above lines and uncommenting the following lines https://github.com/pytorch/TensorRT/blob/4e5b0f6e860910eb510fa70a76ee3eb9825e7a4d/WORKSPACE#L114C1-L124C3\n \n \n Building the Package\n----------------------\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n Once the WORKSPACE has been configured properly, all that is required to build torch-tensorrt is the following command\n \n@@ -135,12 +140,41 @@ To build the wheel file\n \n         python -m pip wheel --no-deps --pre . --extra-index-url https://download.pytorch.org/whl/nightly/cu124 -w dist\n \n+Additional Build Options\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+Some features in the library are optional and allow builds to be lighter or more portable.\n+\n+Python Only Distribution\n+............................\n+\n+There are multiple features of the library which require C++ components to be enabled. This includes both the TorchScript frontend which accepts TorchScript modules for compilation\n+and the Torch-TensorRT runtime, the default executor for modules compiled with Torch-TensorRT, be it with the TorchScript or Dynamo frontend.\n+\n+In the case you may want a build which does not require C++ you can disable these features and avoid building these components. As a result, the only available runtime will be the Python based on\n+which has implications for features like serialization.\n+\n+.. code-block:: sh\n+\n+    PYTHON_ONLY=1 python -m pip install --pre . --extra-index-url https://download.pytorch.org/whl/nightly/cu124\n+\n+\n+No TorchScript Frontend\n+............................\n+\n+The TorchScript frontend is a legacy feature of Torch-TensorRT which is now in maintenance as TorchDynamo has become the preferred compiler technology for this project. It contains quite a bit\n+of C++ code that is no longer necessary for most users. Therefore you can exclude this component from your build to speed up build times. The C++ based runtime will still be available to use.\n+\n+.. code-block:: sh\n+\n+    NO_TORCHSCRIPT=1 python -m pip install --pre . --extra-index-url https://download.pytorch.org/whl/nightly/cu124\n+\n \n-Building the C++ Library (TorchScript Only)\n-------------------------------\n+Building the C++ Library Standalone (TorchScript Only)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n Release Build\n-^^^^^^^^^^^^^^^^^^^^^^^^\n+............................\n \n .. code-block:: shell\n \n@@ -151,7 +185,7 @@ A tarball with the include files and library can then be found in ``bazel-bin``\n .. _build-from-archive-debug:\n \n Debug Build\n-^^^^^^^^^^^^^^^^^^^^^^^^\n+............................\n \n To build with debug symbols use the following command\n \n@@ -162,7 +196,7 @@ To build with debug symbols use the following command\n A tarball with the include files and library can then be found in ``bazel-bin``\n \n Pre CXX11 ABI Build\n-^^^^^^^^^^^^^^^^^^^^^^^^\n+............................\n \n To build using the pre-CXX11 ABI use the ``pre_cxx11_abi`` config\n \n@@ -204,8 +238,45 @@ recommended commands:\n \n     NOTE: For all of the above cases you must correctly declare the source of PyTorch you intend to use in your WORKSPACE file for both Python and C++ builds. See below for more information\n \n-**Building with CMake** (TorchScript Only)\n--------------------------------------------\n+\n+\n+Building on Windows\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+\n+* Microsoft VS 2022 Tools\n+* Bazelisk\n+* CUDA\n+\n+\n+Build steps\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+* Open the app \"x64 Native Tools Command Prompt for VS 2022\" - note that Admin privileges may be necessary\n+* Ensure Bazelisk (Bazel launcher) is installed on your machine and available from the command line. Package installers such as Chocolatey can be used to install Bazelisk\n+* Install latest version of Torch (i.e. with ``pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu124``)\n+* Clone the Torch-TensorRT repository and navigate to its root directory\n+* Run ``pip install ninja wheel setuptools``\n+* Run ``pip install --pre -r py/requirements.txt``\n+* Run ``set DISTUTILS_USE_SDK=1``\n+* Run ``python setup.py bdist_wheel``\n+* Run ``pip install dist/*.whl``\n+\n+Advanced setup and Troubleshooting\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+In the ``WORKSPACE`` file, the ``cuda_win``, ``libtorch_win``, and ``tensorrt_win`` are Windows-specific modules which can be customized. For instance, if you would like to build with a different version of CUDA, or your CUDA installation is in a non-standard location, update the `path` in the `cuda_win` module.\n+\n+Similarly, if you would like to use a different version of pytorch or tensorrt, customize the `urls` in the ``libtorch_win`` and ``tensorrt_win`` modules, respectively.\n+\n+Local versions of these packages can also be used on Windows. See ``toolchains\\\\ci_workspaces\\\\WORKSPACE.win.release.tmpl`` for an example of using a local version of TensorRT on Windows.\n+\n+\n+Alternative Build Systems\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Building with CMake (TorchScript Only)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n It is possible to build the API libraries (in cpp/) and the torchtrtc executable using CMake instead of Bazel.\n Currently, the python API and the tests cannot be built with CMake.\n@@ -233,11 +304,12 @@ A few useful CMake options include:\n             [-DCMAKE_BUILD_TYPE=Debug|Release]\n         cmake --build <build directory>\n \n-**Building Natively on aarch64 (Jetson)**\n--------------------------------------------\n+\n+Building Natively on aarch64 (Jetson)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n Prerequisites\n-^^^^^^^^^^^^^^\n+............................\n \n Install or compile a build of PyTorch/LibTorch for aarch64\n \n@@ -246,8 +318,8 @@ NVIDIA hosts builds the latest release branch for Jetson here:\n     https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048\n \n \n-Enviorment Setup\n-^^^^^^^^^^^^^^^^^\n+Environment Setup\n+............................\n \n To build natively on aarch64-linux-gnu platform, configure the ``WORKSPACE`` with local available dependencies.\n \n@@ -279,7 +351,7 @@ use that library, set the paths to the same path but when you compile make sure\n \n \n Compile C++ Library and Compiler CLI\n-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+........................................................\n \n     NOTE: Due to shifting dependency locations between Jetpack 4.5 and 4.6 there is a now a flag to inform bazel of the Jetpack version\n \n@@ -295,9 +367,9 @@ Compile Torch-TensorRT library using bazel command:\n    bazel build //:libtorchtrt --platforms //toolchains:jetpack_5.0\n \n Compile Python API\n-^^^^^^^^^^^^^^^^^^^^\n+............................\n \n-    NOTE: Due to shifting dependencies locations between Jetpack 4.5 and newer Jetpack verisons there is now a flag for ``setup.py`` which sets the jetpack version (default: 5.0)\n+    NOTE: Due to shifting dependencies locations between Jetpack 4.5 and newer Jetpack versions there is now a flag for ``setup.py`` which sets the jetpack version (default: 5.0)\n \n Compile the Python API using the following command from the ``//py`` directory:\n \n@@ -307,4 +379,4 @@ Compile the Python API using the following command from the ``//py`` directory:\n \n If you have a build of PyTorch that uses Pre-CXX11 ABI drop the ``--use-cxx11-abi`` flag\n \n-If you are building for Jetpack 4.5 add the ``--jetpack-version 5.0`` flag\n+If you are building for Jetpack 4.5 add the ``--jetpack-version 5.0`` flag\n\\ No newline at end of file\ndiff --git a/docsrc/getting_started/quick_start.rst b/docsrc/getting_started/quick_start.rst\nnew file mode 100644\nindex 0000000000..e3e3c371cd\n--- /dev/null\n+++ b/docsrc/getting_started/quick_start.rst\n@@ -0,0 +1,74 @@\n+.. _quick_start:\n+\n+Quick Start\n+##################\n+\n+Option 1: torch.compile\n+-------------------------\n+\n+You can use Torch-TensorRT anywhere you use torch.compile:\n+\n+.. code-block:: py\n+\n+    import torch\n+    import torch_tensorrt\n+\n+    model = MyModel().eval().cuda() # define your model here\n+    x = torch.randn((1, 3, 224, 224)).cuda() # define what the inputs to the model will look like\n+\n+    optimized_model = torch.compile(model, backend=\"tensorrt\")\n+    optimized_model(x) # compiled on first run\n+\n+    optimized_model(x) # this will be fast!\n+\n+\n+Option 2: Export\n+-------------------------\n+\n+If you want to optimize your model ahead-of-time and/or deploy in a C++ environment, Torch-TensorRT provides an export-style workflow that serializes an optimized module. This module can be deployed in PyTorch or with libtorch (i.e. without a Python dependency).\n+\n+Step 1: Optimize + serialize\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. code-block:: py\n+\n+    import torch\n+    import torch_tensorrt\n+\n+    model = MyModel().eval().cuda() # define your model here\n+    inputs = [torch.randn((1, 3, 224, 224)).cuda()] # define a list of representative inputs here\n+\n+    trt_gm = torch_tensorrt.compile(model, ir=\"dynamo\", inputs)\n+    torch_tensorrt.save(trt_gm, \"trt.ep\", inputs=inputs) # PyTorch only supports Python runtime for an ExportedProgram. For C++ deployment, use a TorchScript file\n+    torch_tensorrt.save(trt_gm, \"trt.ts\", output_format=\"torchscript\", inputs=inputs)\n+\n+Step 2: Deploy\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Deployment in Python:\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+\n+.. code-block:: py\n+\n+    import torch\n+    import torch_tensorrt\n+\n+    inputs = [torch.randn((1, 3, 224, 224)).cuda()] # your inputs go here\n+\n+    # You can run this in a new python session!\n+    model = torch.export.load(\"trt.ep\").module()\n+    # model = torch_tensorrt.load(\"trt.ep\").module() # this also works\n+    model(*inputs)\n+\n+Deployment in C++:\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+.. code-block:: c++\n+\n+    #include \"torch/script.h\"\n+    #include \"torch_tensorrt/torch_tensorrt.h\"\n+\n+    auto trt_mod = torch::jit::load(\"trt.ts\");\n+    auto input_tensor = [...]; // fill this with your inputs\n+    auto results = trt_mod.forward({input_tensor});\n\\ No newline at end of file\ndiff --git a/docsrc/index.rst b/docsrc/index.rst\nindex df3f297162..0eadef1d14 100644\n--- a/docsrc/index.rst\n+++ b/docsrc/index.rst\n@@ -26,8 +26,31 @@ Getting Started\n    :hidden:\n \n    getting_started/installation\n-   getting_started/getting_started_with_windows\n+   getting_started/quick_start\n \n+User Guide\n+------------\n+\n+* :ref:`torch_tensorrt_explained`\n+* :ref:`dynamic_shapes`\n+* :ref:`ptq`\n+* :ref:`saving_models`\n+* :ref:`runtime`\n+* :ref:`using_dla`\n+\n+.. toctree::\n+   :caption: User Guide\n+   :maxdepth: 1\n+   :hidden:\n+\n+   user_guide/torch_tensorrt_explained\n+   user_guide/getting_started\n+   user_guide/dynamic_shapes\n+   user_guide/saving_models\n+   user_guide/runtime\n+   user_guide/using_dla\n+   tutorials/_rendered_examples/dynamo/torch_compile_advanced_usage\n+   tutorials/_rendered_examples/dynamo/vgg16_fp8_ptq\n \n Dynamo Frontend\n ----------------\n@@ -59,6 +82,7 @@ TorchScript Frontend\n    ts/getting_started_with_python_api\n    ts/getting_started_with_cpp_api\n    ts/use_from_pytorch\n+   ts/ptq\n \n FX Frontend\n ------------\n@@ -72,28 +96,6 @@ FX Frontend\n \n    fx/getting_started_with_fx_path\n \n-\n-User Guide\n-------------\n-\n-* :ref:`dynamic_shapes`\n-* :ref:`ptq`\n-* :ref:`saving_models`\n-* :ref:`runtime`\n-* :ref:`using_dla`\n-\n-.. toctree::\n-   :caption: User Guide\n-   :maxdepth: 1\n-   :hidden:\n-\n-\n-   user_guide/dynamic_shapes\n-   user_guide/ptq\n-   user_guide/saving_models\n-   user_guide/runtime\n-   user_guide/using_dla\n-\n Tutorials\n ------------\n * :ref:`torch_tensorrt_tutorials`\n@@ -116,28 +118,28 @@ Tutorials\n    tutorials/_rendered_examples/distributed_inference/data_parallel_stable_diffusion\n    tutorials/_rendered_examples/dynamo/vgg16_fp8_ptq\n \n-Python API Documenation\n+Python API Documentation\n ------------------------\n * :ref:`torch_tensorrt_py`\n-* :ref:`torch_tensorrt_logging_py`\n-* :ref:`torch_tensorrt_ptq_py`\n * :ref:`torch_tensorrt_dynamo_py`\n-* :ref:`torch_tensorrt_ts_py`\n+* :ref:`torch_tensorrt_logging_py`\n * :ref:`torch_tensorrt_fx_py`\n+* :ref:`torch_tensorrt_ts_py`\n+* :ref:`torch_tensorrt_ptq_py`\n \n .. toctree::\n-   :caption: Python API Documenation\n+   :caption: Python API Documentation\n    :maxdepth: 0\n    :hidden:\n \n    py_api/torch_tensorrt\n-   py_api/logging\n-   py_api/ptq\n    py_api/dynamo\n-   py_api/ts\n+   py_api/logging\n    py_api/fx\n+   py_api/ts\n+   py_api/ptq\n \n-C++ API Documenation\n+C++ API Documentation\n ----------------------\n * :ref:`namespace_torch_tensorrt`\n * :ref:`namespace_torch_tensorrt__logging`\n@@ -146,7 +148,7 @@ C++ API Documenation\n \n \n .. toctree::\n-   :caption: C++ API Documenation\n+   :caption: C++ API Documentation\n    :maxdepth: 1\n    :hidden:\n \n@@ -161,7 +163,7 @@ CLI Documentation\n * :ref:`torchtrtc`\n \n .. toctree::\n-   :caption: CLI Documenation\n+   :caption: CLI Documentation\n    :maxdepth: 0\n    :hidden:\n \ndiff --git a/docsrc/py_api/dynamo.rst b/docsrc/py_api/dynamo.rst\nindex 12fa5e76c1..0f3e32f2f7 100644\n--- a/docsrc/py_api/dynamo.rst\n+++ b/docsrc/py_api/dynamo.rst\n@@ -26,13 +26,8 @@ Functions\n \n .. autofunction:: refit_module_weights\n \n+\n Classes\n --------\n \n .. autoclass:: CompilationSettings\n-\n-.. autoclass:: SourceIR\n-\n-.. autoclass:: runtime.TorchTensorRTModule\n-\n-.. autoclass:: runtime.PythonTorchTensorRTModule\n\\ No newline at end of file\ndiff --git a/docsrc/py_api/logging.rst b/docsrc/py_api/logging.rst\nindex 7918fe7f86..0c0ecb7bf6 100644\n--- a/docsrc/py_api/logging.rst\n+++ b/docsrc/py_api/logging.rst\n@@ -7,7 +7,5 @@ torch_tensorrt.logging\n \n .. automodule:: torch_tensorrt.logging\n    :members:\n-   :undoc-members:\n-   :show-inheritance:\n \n .. autoclass:: py torch_tensorrt.logging.Level\ndiff --git a/docsrc/py_api/ptq.rst b/docsrc/py_api/ptq.rst\nindex ec83662efb..81925d2b09 100644\n--- a/docsrc/py_api/ptq.rst\n+++ b/docsrc/py_api/ptq.rst\n@@ -1,27 +1,27 @@\n .. _torch_tensorrt_ptq_py:\n \n-torch_tensorrt.ptq\n-===================\n+torch_tensorrt.ts.ptq\n+======================\n \n-.. currentmodule:: torch_tensorrt.ptq\n+These components are legacy quantization utilities designed to work with the TorchScript Frontend. They have been replaced by the `TensorRT Model Optimizer <https://nvidia.github.io/TensorRT-Model-Optimizer/>`_ toolkit\n+which can be used with the dynamo frontend:\n \n-.. automodule:: torch_tensorrt.ptq\n-   :members:\n-   :undoc-members:\n-   :show-inheritance:\n+.. currentmodule:: torch_tensorrt.ts.ptq\n+\n+.. automodule:: torch_tensorrt.ts.ptq\n \n Classes\n ---------\n \n .. autoclass:: DataLoaderCalibrator\n    :members:\n-   :special-members: __init__\n \n .. autoclass:: CacheCalibrator\n    :members:\n-   :special-members: __init__\n \n Enums\n -------\n \n .. autoclass:: CalibrationAlgo\n+   :members:\n+   :undoc-members:\ndiff --git a/docsrc/py_api/runtime.rst b/docsrc/py_api/runtime.rst\nnew file mode 100644\nindex 0000000000..4e6721c7a6\n--- /dev/null\n+++ b/docsrc/py_api/runtime.rst\n@@ -0,0 +1,26 @@\n+.. _torch_tensorrt_py:\n+\n+torch_tensorrt.runtime\n+==============================\n+\n+.. automodule:: torch_tensorrt.runtime\n+   :members:\n+   :undoc-members:\n+   :show-inheritance:\n+\n+Functions\n+------------\n+\n+.. autofunction:: set_multi_device_safe_mode\n+\n+\n+Classes\n+---------\n+\n+.. autoclass:: TorchTensorRTModule\n+   :members:\n+   :special-members: __init__\n+\n+.. autoclass:: PythonTorchTensorRTModule\n+   :members:\n+   :special-members: __init__\n\\ No newline at end of file\ndiff --git a/docsrc/py_api/torch_tensorrt.rst b/docsrc/py_api/torch_tensorrt.rst\nindex eb8285e103..c2ddc9c701 100644\n--- a/docsrc/py_api/torch_tensorrt.rst\n+++ b/docsrc/py_api/torch_tensorrt.rst\n@@ -26,6 +26,10 @@ Functions\n \n .. autofunction:: dump_build_info\n \n+.. autofunction:: save\n+\n+.. autofunction:: load\n+\n Classes\n ---------\n \n@@ -41,12 +45,20 @@ Enums\n -------\n \n .. autoclass:: dtype\n+   :members:\n+   :member-order:\n \n .. autoclass:: DeviceType\n+   :members:\n+   :member-order:\n \n .. autoclass:: EngineCapability\n+   :members:\n+   :member-order:\n \n .. autoclass:: memory_format\n+   :members:\n+   :member-order:\n \n Submodules\n ----------\n@@ -59,3 +71,4 @@ Submodules\n    ts\n    fx\n    dynamo\n+   runtime\ndiff --git a/docsrc/ts/creating_torchscript_module_in_python.rst b/docsrc/ts/creating_torchscript_module_in_python.rst\nindex 1d1cdba574..6c63302c76 100644\n--- a/docsrc/ts/creating_torchscript_module_in_python.rst\n+++ b/docsrc/ts/creating_torchscript_module_in_python.rst\n@@ -49,11 +49,11 @@ For example, we can define a LeNet module like this:\n         def __init__(self):\n             super(LeNet, self).__init__()\n             self.feat = LeNetFeatExtractor()\n-            self.classifer = LeNetClassifier()\n+            self.classifier = LeNetClassifier()\n \n         def forward(self, x):\n             x = self.feat(x)\n-            x = self.classifer(x)\n+            x = self.classifier(x)\n             return x\n \n .\n@@ -84,7 +84,7 @@ include these components. We can run the script compiler on our LeNet module by\n     model = LeNet()\n     script_model = torch.jit.script(model)\n \n-There are reasons to use one path or another, the PyTorch documentation has information on how to choose. From a Torch-TensorRT prespective, there is\n+There are reasons to use one path or another, the PyTorch documentation has information on how to choose. From a Torch-TensorRT perspective, there is\n better support (i.e your module is more likely to compile) for traced modules because it doesn't include all the complexities of a complete\n programming language, though both paths supported.\n \n@@ -97,7 +97,7 @@ Here is what the LeNet traced module IR looks like:\n \n     graph(%self.1 : __torch__.___torch_mangle_10.LeNet,\n         %input.1 : Float(1, 1, 32, 32)):\n-        %129 : __torch__.___torch_mangle_9.LeNetClassifier = prim::GetAttr[name=\"classifer\"](%self.1)\n+        %129 : __torch__.___torch_mangle_9.LeNetClassifier = prim::GetAttr[name=\"classifier\"](%self.1)\n         %119 : __torch__.___torch_mangle_5.LeNetFeatExtractor = prim::GetAttr[name=\"feat\"](%self.1)\n         %137 : Tensor = prim::CallMethod[name=\"forward\"](%119, %input.1)\n         %138 : Tensor = prim::CallMethod[name=\"forward\"](%129, %137)\n@@ -111,7 +111,7 @@ and the LeNet scripted module IR:\n         %x.1 : Tensor):\n         %2 : __torch__.LeNetFeatExtractor = prim::GetAttr[name=\"feat\"](%self)\n         %x.3 : Tensor = prim::CallMethod[name=\"forward\"](%2, %x.1) # x.py:38:12\n-        %5 : __torch__.LeNetClassifier = prim::GetAttr[name=\"classifer\"](%self)\n+        %5 : __torch__.LeNetClassifier = prim::GetAttr[name=\"classifier\"](%self)\n         %x.5 : Tensor = prim::CallMethod[name=\"forward\"](%5, %x.3) # x.py:39:12\n         return (%x.5)\n \n@@ -123,7 +123,7 @@ Working with TorchScript in Python\n -----------------------------------\n \n TorchScript Modules are run the same way you run normal PyTorch modules. You can run the forward pass using the\n-``forward`` method or just calling the module ``torch_scirpt_module(in_tensor)`` The JIT compiler will compile\n+``forward`` method or just calling the module ``torch_script_module(in_tensor)`` The JIT compiler will compile\n and optimize the module on the fly and then returns the results.\n \n Saving TorchScript Module to Disk\ndiff --git a/docsrc/ts/getting_started_with_cpp_api.rst b/docsrc/ts/getting_started_with_cpp_api.rst\nindex 70f439ea6d..4e22b7f938 100644\n--- a/docsrc/ts/getting_started_with_cpp_api.rst\n+++ b/docsrc/ts/getting_started_with_cpp_api.rst\n@@ -118,7 +118,7 @@ With our module loaded, we can feed it into the Torch-TensorRT compiler. When we\n         auto trt_mod = torch_tensorrt::CompileGraph(mod, std::vector<torch_tensorrt::CompileSpec::InputRange>{{in.sizes()}});\n         auto out = trt_mod.forward({in});\n \n-Thats it! Now the graph runs primarily not with the JIT compiler but using TensorRT (though we execute the graph using the JIT runtime).\n+That's it! Now the graph runs primarily not with the JIT compiler but using TensorRT (though we execute the graph using the JIT runtime).\n \n We can also set settings like operating precision to run in FP16.\n \n@@ -209,9 +209,9 @@ When a module is provided to Torch-TensorRT, the compiler starts by mapping a gr\n         %10 : bool = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\n         %11 : int = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n         %12 : bool = prim::Constant[value=0]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n-        %self.classifer.fc3.bias : Float(10) = prim::Constant[value= 0.0464  0.0383  0.0678  0.0932  0.1045 -0.0805 -0.0435 -0.0818  0.0208 -0.0358 [ CUDAFloatType{10} ]]()\n-        %self.classifer.fc2.bias : Float(84) = prim::Constant[value=<Tensor>]()\n-        %self.classifer.fc1.bias : Float(120) = prim::Constant[value=<Tensor>]()\n+        %self.classifier.fc3.bias : Float(10) = prim::Constant[value= 0.0464  0.0383  0.0678  0.0932  0.1045 -0.0805 -0.0435 -0.0818  0.0208 -0.0358 [ CUDAFloatType{10} ]]()\n+        %self.classifier.fc2.bias : Float(84) = prim::Constant[value=<Tensor>]()\n+        %self.classifier.fc1.bias : Float(120) = prim::Constant[value=<Tensor>]()\n         %self.feat.conv2.weight : Float(16, 6, 3, 3) = prim::Constant[value=<Tensor>]()\n         %self.feat.conv2.bias : Float(16) = prim::Constant[value=<Tensor>]()\n         %self.feat.conv1.weight : Float(6, 1, 3, 3) = prim::Constant[value=<Tensor>]()\n@@ -224,15 +224,15 @@ When a module is provided to Torch-TensorRT, the compiler starts by mapping a gr\n         %x.1 : Tensor = aten::max_pool2d(%input2.1, %7, %6, %8, %9, %12) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n         %input.1 : Tensor = aten::flatten(%x.1, %11, %5) # x.py:25:0\n         %27 : Tensor = aten::matmul(%input.1, %4)\n-        %28 : Tensor = trt::const(%self.classifer.fc1.bias)\n+        %28 : Tensor = trt::const(%self.classifier.fc1.bias)\n         %29 : Tensor = aten::add_(%28, %27, %11)\n         %input0.2 : Tensor = aten::relu(%29) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n         %31 : Tensor = aten::matmul(%input0.2, %3)\n-        %32 : Tensor = trt::const(%self.classifer.fc2.bias)\n+        %32 : Tensor = trt::const(%self.classifier.fc2.bias)\n         %33 : Tensor = aten::add_(%32, %31, %11)\n         %input1.1 : Tensor = aten::relu(%33) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n         %35 : Tensor = aten::matmul(%input1.1, %2)\n-        %36 : Tensor = trt::const(%self.classifer.fc3.bias)\n+        %36 : Tensor = trt::const(%self.classifier.fc3.bias)\n         %37 : Tensor = aten::add_(%36, %35, %11)\n         return (%37)\n     (CompileGraph)\n@@ -264,10 +264,10 @@ You can see the call where the engine is executed, after extracting the attribut\n Working with Unsupported Operators\n -----------------------------------\n \n-Torch-TensorRT is a new library and the PyTorch operator library is quite large, so there will be ops that aren't supported natively by the compiler. You can either use the composition techinques\n+Torch-TensorRT is a new library and the PyTorch operator library is quite large, so there will be ops that aren't supported natively by the compiler. You can either use the composition techniques\n shown above to make modules are fully Torch-TensorRT supported and ones that are not and stitch the modules together in the deployment application or you can register converters for missing ops.\n \n-    You can check support without going through the full compilation pipleine using the ``torch_tensorrt::CheckMethodOperatorSupport(const torch::jit::Module& module, std::string method_name)`` api\n+    You can check support without going through the full compilation pipeline using the ``torch_tensorrt::CheckMethodOperatorSupport(const torch::jit::Module& module, std::string method_name)`` api\n     to see what operators are not supported. ``torchtrtc`` automatically checks modules with this method before starting compilation and will print out a list of operators that are not supported.\n \n .. _custom_converters:\n@@ -333,7 +333,7 @@ for example we can quickly get the output size by just running the operation in\n     int main() {\n         ...\n \n-To use this converter in Python, it is recommended to use PyTorch's `C++ / CUDA Extention <https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions>`_\n+To use this converter in Python, it is recommended to use PyTorch's `C++ / CUDA Extension <https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions>`_\n template to wrap your library of converters into a ``.so`` that you can load with ``ctypes.CDLL()`` in your Python application.\n \n You can find more information on all the details of writing converters in the contributors documentation (:ref:`writing_converters`).\ndiff --git a/docsrc/user_guide/ptq.rst b/docsrc/ts/ptq.rst\nsimilarity index 100%\nrename from docsrc/user_guide/ptq.rst\nrename to docsrc/ts/ptq.rst\ndiff --git a/docsrc/user_guide/torch_tensorrt_explained.rst b/docsrc/user_guide/torch_tensorrt_explained.rst\nnew file mode 100644\nindex 0000000000..53216b2806\n--- /dev/null\n+++ b/docsrc/user_guide/torch_tensorrt_explained.rst\n@@ -0,0 +1,107 @@\n+.. _torch_tensorrt_explained:\n+\n+Torch-TensorRT Explained\n+=================================\n+\n+Torch-TensorRT is a compiler for PyTorch models targeting NVIDIA GPUs\n+via the TensorRT Model Optimization SDK. It aims to provide better\n+inference performance for PyTorch models while still maintaining the\n+great ergonomics of PyTorch.\n+\n+Dynamo Frontend\n+-----------------\n+\n+The Dynamo frontend is the default frontend for Torch-TensorRT. It utilizes the `dynamo compiler stack <https://pytorch.org/docs/stable/torch.compiler_deepdive.html>`_ from PyTorch.\n+\n+\n+``torch.compile`` (Just-in-time)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+``torch.compile`` is a JIT compiler stack, as such, compilation is deferred until first use. This means that as conditions change in the graph, the graph will automatically recompile.\n+This provides users the most runtime flexibility, however limits options regarding serialization.\n+\n+Under the hood, `torch.compile <https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile>`_ delegates subgraphs it believes can be lowered to Torch-TensorRT. Torch-TensorRT further lowers these graphs into ops consisting of solely `Core ATen Operators <https://pytorch.org/executorch/stable/ir-ops-set-definition.html>`_\n+or select \"High-level Ops\" amenable to TensorRT acceleration. Subgraphs are further partitioned into components that will run in PyTorch and ones to be further compiled to TensorRT based\n+on support for operators. TensorRT engines then replace supported blocks and a hybrid subgraph is returned to ``torch.compile`` to be run on call.\n+\n+Accepted Formats\n+...................\n+- torch.fx GraphModule (``torch.fx.GraphModule``)\n+- PyTorch Module (``torch.nn.Module``)\n+\n+Returns\n+...................\n+- Boxed-function that triggers compilation on first call\n+\n+\n+``torch_tensorrt.dynamo.compile`` (Ahead-of-time)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+``torch_tensorrt.dynamo.compile`` is an AOT compiler, models are compiled in an explicit compilation phase. These compilation artifacts can then be serialized and reloaded at a later date.\n+Graphs go through the ``torch.export.trace`` system to be lowered into a graph consisting of `Core ATen Operators <https://pytorch.org/executorch/stable/ir-ops-set-definition.html>`_ or select \"High-level Ops\" amenable to TensoRT acceleration.\n+Subgraphs are further partitioned into components that will run in PyTorch and ones to be further compiled to TensorRT based on support for operators. TensorRT engines then replace supported blocks\n+and a hybrid subgraph is packed into an `ExportedProgram <https://pytorch.org/docs/stable/export.ir_spec.html#exportedprogram>`_ which can be serialized and reloaded.\n+\n+Accepted Formats\n+...................\n+- torch.export.ExportedProgram (``torch.export.ExportedProgram``)\n+- torch.fx GraphModule (``torch.fx.GraphModule``) (via ``torch.export.export``)\n+- PyTorch Module (``torch.nn.Module``) (via ``torch.export.export``)\n+\n+Returns\n+...................\n+- torch.fx.GraphModule (serializable with ``torch.export.ExportedProgram``)\n+\n+Legacy Frontends\n+------------------\n+\n+As there has been a number of compiler technologies in the PyTorch ecosystem over the years\n+Torch-TensorRT has some legacy features targeting them.\n+\n+\n+TorchScript (`torch_tensorrt.ts.compile`)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+The TorchScript frontend was the original default frontend for Torch-TensorRT and targets models in the TorchScript format. The graph provided will be partitioned into supported and unsupported\n+blocks. Supported blocks will be lowered to TensorRT and unsupported blocks will remain to run with LibTorch. The resultant graph is returned back to the user as a ``ScriptModule`` that can be loaded and saved\n+with the Torch-TensorRT PyTorch runtime extension.\n+\n+Accepted Formats\n+...................\n+- TorchScript Module (``torch.jit.ScriptModule``)\n+- PyTorch Module (``torch.nn.Module``) (via ``torch.jit.script`` or ``torch.jit.trace``)\n+\n+Returns\n+...................\n+- TorchScript Module (``torch.jit.ScriptModule``)\n+\n+\n+FX Graph Modules (`torch_tensorrt.fx.compile`)\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+This frontend has almost entirely been replaced by the Dynamo frontend which is a superset of the\n+features available though the FX frontend. The original FX frontend remains in the codebase for\n+backwards compatibility reasons.\n+\n+Accepted Formats\n+...................\n+- torch.fx GraphModule (``torch.fx.GraphModule``)\n+- PyTorch Module (``torch.nn.Module``) (via ``torch.fx.trace``)\n+\n+Returns\n+...................\n+- torch.fx GraphModule (``torch.fx.GraphModule``)\n+\n+``torch_tensorrt.compile``\n+----------------------------------\n+\n+As there are many different frontends and supported formats, we provide a convenience layer called ``torch_tensorrt.compile`` which lets users access\n+all the different compiler options. You can specify to ``torch_tensorrt.compile`` what compiler path to use by setting the ``ir`` option, telling\n+Torch-TensorRT to try to lower the provided model through a specific intermediate representation.\n+\n+``ir`` Options\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+- ``torch_compile``: Use the ``torch.compile`` system. Immediately returns a boxed-function that will compile on first call\n+- ``dynamo``: Run the graph through the ``torch.export``/ torchdynamo stack. If the input module is a ``torch.nn.Module``, it must be \"export-traceable\" as the module will be traced with ``torch.export.export``. Returns a ``torch.fx.GraphModule`` which can be run immediately or saved via ``torch.export.export`` or ``torch_tensorrt.save``\n+- ``torchscript`` or ``ts``: Run graph through the TorchScript stack. If the input module is a ``torch.nn.Module``, it must be \"scriptable\" as the module will be compiled with ``torch.jit.script``. Returns a ``torch.jit.ScriptModule`` which can be run immediately or saved via ``torch.save`` or ``torch_tensorrt.save``\n+- ``fx``: Run graph through the ``torch.fx`` stack. If the input module is a ``torch.nn.Module``, it will be traced with ``torch.fx.trace`` and subject to its limitations.\n\\ No newline at end of file\ndiff --git a/examples/custom_converters/README.md b/examples/custom_converters/README.md\nindex 322a867194..70fad0ae27 100644\n--- a/examples/custom_converters/README.md\n+++ b/examples/custom_converters/README.md\n@@ -23,7 +23,7 @@ it doesn't support elu operator in default. (Torch-TensorRT <= v0.1.0)\n We can register a converter for this operator in our application. You can find more\n information on all the details of writing converters in the contributors documentation\n ([Writing Converters](https://nvidia.github.io/Torch-TensorRT/contributors/writing_converters.html)).\n-Once we are clear about these rules and writing patterns, we can create a seperate new C++ source file as:\n+Once we are clear about these rules and writing patterns, we can create a separate new C++ source file as:\n \n ```c++\n #include \"core/conversion/converters/converters.h\"\n@@ -58,7 +58,7 @@ auto actelu = torch_tensorrt::core::conversion::converters::RegisterNodeConversi\n To use this converter in Python, it is recommended to use PyTorch's\n [C++/CUDA Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions).\n We give an example here about how to wrap the converter into a `.so`\n-library so that you can load it to use in Python applicaton.\n+library so that you can load it to use in Python application.\n ```python\n import os\n from setuptools import setup, Extension\n@@ -124,7 +124,7 @@ def cal_max_diff(pytorch_out, torch_tensorrt_out):\n     diff = torch.sub(pytorch_out, torch_tensorrt_out)\n     abs_diff = torch.abs(diff)\n     max_diff = torch.max(abs_diff)\n-    print(\"Maximum differnce between Torch-TensorRT and PyTorch: \\n\", max_diff)\n+    print(\"Maximum difference between Torch-TensorRT and PyTorch: \\n\", max_diff)\n \n \n def main():\ndiff --git a/examples/custom_converters/elu_model.py b/examples/custom_converters/elu_model.py\nindex 01cfdd1250..e4edfa39f5 100644\n--- a/examples/custom_converters/elu_model.py\n+++ b/examples/custom_converters/elu_model.py\n@@ -20,7 +20,7 @@ def cal_max_diff(pytorch_out, torch_tensorrt_out):\n     diff = torch.sub(pytorch_out, torch_tensorrt_out)\n     abs_diff = torch.abs(diff)\n     max_diff = torch.max(abs_diff)\n-    print(\"Maximum differnce between Torch-TensorRT and PyTorch: \\n\", max_diff)\n+    print(\"Maximum difference between Torch-TensorRT and PyTorch: \\n\", max_diff)\n \n \n def main():\ndiff --git a/examples/distributed_inference/README.md b/examples/distributed_inference/README.md\nindex f9608e8950..c164e9581f 100644\n--- a/examples/distributed_inference/README.md\n+++ b/examples/distributed_inference/README.md\n@@ -2,7 +2,7 @@\n \n Examples in this folder demonstrates doing distributed inference on multiple devices with Torch-TensorRT backend.\n \n-1. Data parallel distributed inference based on [Acclerate](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference)\n+1. Data parallel distributed inference based on [Accelerate](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference)\n \n Using Accelerate users can achieve data parallel distributed inference with Torch-TensorRt backend. In this case, the entire model\n will be loaded onto each GPU and different chunks of batch input is processed on each device.\ndiff --git a/examples/dynamo/custom_kernel_plugins.py b/examples/dynamo/custom_kernel_plugins.py\nindex 4165c54105..73b06119ae 100644\n--- a/examples/dynamo/custom_kernel_plugins.py\n+++ b/examples/dynamo/custom_kernel_plugins.py\n@@ -13,7 +13,7 @@\n in terms of PyTorch ops that are supported in Torch-TensorRT or a converter (see:\n `Writing converters for the Dynamo frontend <https://pytorch.org/TensorRT/contributors/dynamo_converters.html>`_) - which defines the operator in terms of TensorRT operators.\n \n-In some cases there isnt a great way to do either of these, perhaps because the operator is a custom kernel that is not part of standard PyTorch or\n+In some cases there isn't a great way to do either of these, perhaps because the operator is a custom kernel that is not part of standard PyTorch or\n TensorRT cannot support it natively.\n \n For these cases, it is possible to use a TensorRT plugin to replace the operator **inside** the TensorRT engine, thereby avoiding\n@@ -147,7 +147,7 @@ def triton_circular_pad(x: torch.Tensor, padding: Sequence[int]) -> torch.Tensor\n \n # %%\n # Testing our custom op\n-# ^^^^^^^^^^^^^^^^^^^^^^^\n+# -----------------------------------------\n \n # %%\n # The native PyTorch implementation\n@@ -190,7 +190,7 @@ def _(x: torch.Tensor, padding: Sequence[int]) -> torch.Tensor:\n     return torch.nn.functional.pad(x, padding, \"circular\")\n \n \n-# Additionally one may want to define an autograd implementation for the backwards pass to round out the custom op implmentation but that is beyond the scope of this tutorial (see https://pytorch.org/docs/main/library.html#torch.library.register_autograd for more)\n+# Additionally one may want to define an autograd implementation for the backwards pass to round out the custom op implementation but that is beyond the scope of this tutorial (see https://pytorch.org/docs/main/library.html#torch.library.register_autograd for more)\n \n \n # %%\n@@ -304,7 +304,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n \n # %%\n # Wrapping Custom Kernels to use in TensorRT\n-# =============================================\n+# --------------------------------------------\n #\n # To address this graph break, the first step is to make our kernel implementation available in TensorRT. Again this can be done in either C++ or Python. For the actual details on how to implement\n # TensorRT plugins refer `here <https://github.com/NVIDIA/TensorRT/tree/release/10.0/samples/python/python_plugin>`_. From a high level, similar to PyTorch you will need to\n@@ -501,7 +501,7 @@ def deserialize_plugin(self, name: str, data: bytes) -> CircularPaddingPlugin:\n \n # %%\n # Using Torch-TensorRT to Insert the Kernel\n-# =============================================\n+# -------------------------------------------\n # Now with our TensorRT plugin, we can create a converter so that Torch-TensorRT knows to insert our plugin in place of our custom circular padding operator.\n # More information on writing converters can be found `here <https://pytorch.org/TensorRT/contributors/dynamo_converters.html>`_\n \n@@ -534,7 +534,7 @@ def circular_padding_converter(\n     plugin_creator = plugin_registry.get_plugin_creator(\n         type=\"CircularPaddingPlugin\", version=\"1\", plugin_namespace=\"\"\n     )\n-    assert plugin_creator, f\"Unabled to find CircularPaddingPlugin creator\"\n+    assert plugin_creator, f\"Unable to find CircularPaddingPlugin creator\"\n \n     # Pass configurations to the plugin implementation\n     field_configs = trt.PluginFieldCollection(\ndiff --git a/examples/dynamo/refit_engine_example.py b/examples/dynamo/refit_engine_example.py\nindex c841c5f57a..167344977e 100644\n--- a/examples/dynamo/refit_engine_example.py\n+++ b/examples/dynamo/refit_engine_example.py\n@@ -91,7 +91,7 @@\n print(\"Refit successfully!\")\n \n # %%\n-# Alterative Workflow using Python Runtime\n+# Alternative Workflow using Python Runtime\n # -----------------------------\n \n # Currently python runtime does not support engine serialization. So the refitting will be done in the same runtime.\ndiff --git a/examples/int8/ptq/README.md b/examples/int8/ptq/README.md\nindex 329d4d021d..246ef6e1d4 100644\n--- a/examples/int8/ptq/README.md\n+++ b/examples/int8/ptq/README.md\n@@ -6,7 +6,7 @@ Post Training Quantization (PTQ) is a technique to reduce the required computati\n \n Users writing TensorRT applications are required to setup a calibrator class which will provide sample data to the TensorRT calibrator. With Torch-TensorRT we look to leverage existing infrastructure in PyTorch to make implementing calibrators easier.\n \n-LibTorch provides a `Dataloader` and `Dataset` API which steamlines preprocessing and batching input data. Torch-TensorRT uses Dataloaders as the base of a generic calibrator implementation. So you will be able to reuse or quickly implement a `torch::Dataset` for your target domain, place it in a Dataloader and create a INT8 Calibrator from it which you can provide to Torch-TensorRT to run INT8 Calibration during compliation of your module.\n+LibTorch provides a `Dataloader` and `Dataset` API which steamlines preprocessing and batching input data. Torch-TensorRT uses Dataloaders as the base of a generic calibrator implementation. So you will be able to reuse or quickly implement a `torch::Dataset` for your target domain, place it in a Dataloader and create a INT8 Calibrator from it which you can provide to Torch-TensorRT to run INT8 Calibration during compilation of your module.\n \n ### Code\n \n@@ -92,7 +92,7 @@ The calibrator factories create a calibrator that inherits from a `nvinfer1::IIn\n auto calibrator = torch_tensorrt::ptq::make_int8_calibrator<nvinfer1::IInt8MinMaxCalibrator>(std::move(calibration_dataloader), calibration_cache_file, true);\n ```\n \n-Then all thats required to setup the module for INT8 calibration is to set the following compile settings in the `torch_tensorrt::CompileSpec` struct and compiling the module:\n+Then all that's required to setup the module for INT8 calibration is to set the following compile settings in the `torch_tensorrt::CompileSpec` struct and compiling the module:\n \n ```C++\n     std::vector<std::vector<int64_t>> input_shape = {{32, 3, 32, 32}};\n@@ -102,7 +102,7 @@ Then all thats required to setup the module for INT8 calibration is to set the f\n     compile_spec.enabled_precisions.insert(torch::kI8);\n     /// Use the TensorRT Entropy Calibrator\n     compile_spec.ptq_calibrator = calibrator;\n-    /// Set a larger workspace (you may get better performace from doing so)\n+    /// Set a larger workspace (you may get better performance from doing so)\n     compile_spec.workspace_size = 1 << 28;\n \n     auto trt_mod = torch_tensorrt::CompileGraph(mod, compile_spec);\ndiff --git a/examples/int8/training/vgg16/README.md b/examples/int8/training/vgg16/README.md\nindex 5aff4ca116..05404494be 100644\n--- a/examples/int8/training/vgg16/README.md\n+++ b/examples/int8/training/vgg16/README.md\n@@ -2,7 +2,7 @@\n \n This is a recipe to train a VGG network on CIFAR10 to use with the Torch-TensorRT PTQ example.\n \n-## Prequisites\n+## Prerequisites\n \n ```\n pip3 install -r requirements.txt --user\ndiff --git a/examples/int8/training/vgg16/finetune_qat.py b/examples/int8/training/vgg16/finetune_qat.py\nindex 0414af00de..60850bf6a3 100644\n--- a/examples/int8/training/vgg16/finetune_qat.py\n+++ b/examples/int8/training/vgg16/finetune_qat.py\n@@ -8,17 +8,14 @@\n import torch.nn.functional as F\n import torch.optim as optim\n import torch.utils.data as data\n-import torchvision.transforms as transforms\n import torchvision.datasets as datasets\n-\n-from torch.utils.tensorboard import SummaryWriter\n-\n+import torchvision.transforms as transforms\n+from pytorch_quantization import calib\n from pytorch_quantization import nn as quant_nn\n from pytorch_quantization import quant_modules\n from pytorch_quantization.tensor_quant import QuantDescriptor\n-from pytorch_quantization import calib\n+from torch.utils.tensorboard import SummaryWriter\n from tqdm import tqdm\n-\n from vgg16 import vgg16\n \n PARSER = argparse.ArgumentParser(\n@@ -49,7 +46,7 @@\n     \"--start-from\",\n     default=0,\n     type=int,\n-    help=\"Epoch to resume from (requires a checkpoin in the providied checkpoi\",\n+    help=\"Epoch to resume from (requires a checkpoint in the providied checkpoi\",\n )\n PARSER.add_argument(\"--seed\", type=int, help=\"Seed value for rng\")\n PARSER.add_argument(\n@@ -147,7 +144,7 @@ def calibrate_model(\n         data_loader: calibration data set\n         num_calib_batch: amount of calibration passes to perform\n         calibrator: type of calibration to use (max/histogram)\n-        hist_percentile: percentiles to be used for historgram calibration\n+        hist_percentile: percentiles to be used for histogram calibration\n         out_dir: dir to save state files in\n     \"\"\"\n \ndiff --git a/examples/int8/training/vgg16/main.py b/examples/int8/training/vgg16/main.py\nindex 3db8e9d4dd..aed7f68565 100644\n--- a/examples/int8/training/vgg16/main.py\n+++ b/examples/int8/training/vgg16/main.py\n@@ -35,7 +35,7 @@\n     \"--start-from\",\n     default=0,\n     type=int,\n-    help=\"Epoch to resume from (requires a checkpoin in the providied checkpoi\",\n+    help=\"Epoch to resume from (requires a checkpoint in the providied checkpoi\",\n )\n PARSER.add_argument(\"--seed\", type=int, help=\"Seed value for rng\")\n PARSER.add_argument(\ndiff --git a/py/README.md b/py/README.md\nindex 234c86fa05..45c68ff98f 100644\n--- a/py/README.md\n+++ b/py/README.md\n@@ -2,7 +2,7 @@\n \n > Ahead of Time (AOT) compiling for PyTorch JIT\n \n-Torch-TensorRT is a compiler for PyTorch/TorchScript, targeting NVIDIA GPUs via NVIDIA's TensorRT Deep Learning Optimizer and Runtime. Unlike PyTorch's Just-In-Time (JIT) compiler, Torch-TensorRT is an Ahead-of-Time (AOT) compiler, meaning that before you deploy your TorchScript code, you go through an explicit compile step to convert a standard TorchScript program into an module targeting a TensorRT engine. Torch-TensorRT operates as a PyTorch extention and compiles modules that integrate into the JIT runtime seamlessly. After compilation using the optimized graph should feel no different than running a TorchScript module. You also have access to TensorRT's suite of configurations at compile time, so you are able to specify operating precision (FP32/FP16/INT8) and other settings for your module.\n+Torch-TensorRT is a compiler for PyTorch/TorchScript, targeting NVIDIA GPUs via NVIDIA's TensorRT Deep Learning Optimizer and Runtime. Unlike PyTorch's Just-In-Time (JIT) compiler, Torch-TensorRT is an Ahead-of-Time (AOT) compiler, meaning that before you deploy your TorchScript code, you go through an explicit compile step to convert a standard TorchScript program into an module targeting a TensorRT engine. Torch-TensorRT operates as a PyTorch extension and compiles modules that integrate into the JIT runtime seamlessly. After compilation using the optimized graph should feel no different than running a TorchScript module. You also have access to TensorRT's suite of configurations at compile time, so you are able to specify operating precision (FP32/FP16/INT8) and other settings for your module.\n \n ## Example Usage\n \n@@ -57,9 +57,9 @@ graph(%input.2 : Tensor):\n     %10 : bool = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\n     %11 : int = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n     %12 : bool = prim::Constant[value=0]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n-    %self.classifer.fc3.bias : Float(10) = prim::Constant[value= 0.0464  0.0383  0.0678  0.0932  0.1045 -0.0805 -0.0435 -0.0818  0.0208 -0.0358 [ CUDAFloatType{10} ]]()\n-    %self.classifer.fc2.bias : Float(84) = prim::Constant[value=<Tensor>]()\n-    %self.classifer.fc1.bias : Float(120) = prim::Constant[value=<Tensor>]()\n+    %self.classifier.fc3.bias : Float(10) = prim::Constant[value= 0.0464  0.0383  0.0678  0.0932  0.1045 -0.0805 -0.0435 -0.0818  0.0208 -0.0358 [ CUDAFloatType{10} ]]()\n+    %self.classifier.fc2.bias : Float(84) = prim::Constant[value=<Tensor>]()\n+    %self.classifier.fc1.bias : Float(120) = prim::Constant[value=<Tensor>]()\n     %self.feat.conv2.weight : Float(16, 6, 3, 3) = prim::Constant[value=<Tensor>]()\n     %self.feat.conv2.bias : Float(16) = prim::Constant[value=<Tensor>]()\n     %self.feat.conv1.weight : Float(6, 1, 3, 3) = prim::Constant[value=<Tensor>]()\n@@ -72,15 +72,15 @@ graph(%input.2 : Tensor):\n     %x.1 : Tensor = aten::max_pool2d(%input2.1, %7, %6, %8, %9, %12) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\n     %input.1 : Tensor = aten::flatten(%x.1, %11, %5) # x.py:25:0\n     %27 : Tensor = aten::matmul(%input.1, %4)\n-    %28 : Tensor = trt::const(%self.classifer.fc1.bias)\n+    %28 : Tensor = trt::const(%self.classifier.fc1.bias)\n     %29 : Tensor = aten::add_(%28, %27, %11)\n     %input0.2 : Tensor = aten::relu(%29) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n     %31 : Tensor = aten::matmul(%input0.2, %3)\n-    %32 : Tensor = trt::const(%self.classifer.fc2.bias)\n+    %32 : Tensor = trt::const(%self.classifier.fc2.bias)\n     %33 : Tensor = aten::add_(%32, %31, %11)\n     %input1.1 : Tensor = aten::relu(%33) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\n     %35 : Tensor = aten::matmul(%input1.1, %2)\n-    %36 : Tensor = trt::const(%self.classifer.fc3.bias)\n+    %36 : Tensor = trt::const(%self.classifier.fc3.bias)\n     %37 : Tensor = aten::add_(%36, %35, %11)\n     return (%37)\n (CompileGraph)\n@@ -107,7 +107,7 @@ graph(%self.1 : __torch__.___torch_mangle_10.LeNet_trt,\n You can see the call where the engine is executed, based on a constant which is the ID of the engine, telling JIT how to find the engine and the input tensor which will be fed to TensorRT.\n The engine represents the exact same calculations as what is done by running a normal PyTorch module but optimized to run on your GPU.\n \n-Torch-TensorRT converts from TorchScript by generating layers or subgraphs in correspondance with instructions seen in the graph. Converters are small modules of code used to map one specific\n+Torch-TensorRT converts from TorchScript by generating layers or subgraphs in correspondence with instructions seen in the graph. Converters are small modules of code used to map one specific\n operation to a layer or subgraph in TensorRT. Not all operations are support, but if you need to implement one, you can in C++.\n \n ## Registering Custom Converters\n@@ -166,7 +166,7 @@ static auto flatten_converter = torch_tensorrt::core::conversion::converters::Re\n     });\n ```\n \n-To use this converter in Python, it is recommended to use PyTorch\u2019s [C++ / CUDA Extention](https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions) template to wrap\n+To use this converter in Python, it is recommended to use PyTorch\u2019s [C++ / CUDA Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions) template to wrap\n your library of converters into a `.so` that you can load with `ctypes.CDLL()` in your Python application.\n \n You can find more information on all the details of writing converters in the contributors documentation ([Writing Converters](https://nvidia.github.io/Torch-TensorRT/contributors/writing_converters.html#writing-converters)). If you\ndiff --git a/py/torch_tensorrt/_Device.py b/py/torch_tensorrt/_Device.py\nindex 4c8c855943..e425c89be5 100644\n--- a/py/torch_tensorrt/_Device.py\n+++ b/py/torch_tensorrt/_Device.py\n@@ -45,7 +45,7 @@ def __init__(self, *args: Any, **kwargs: Any):\n             spec (str): String with device spec e.g. \"dla:0\" for dla, core_id 0\n \n         Keyword Arguments:\n-            gpu_id (int): ID of target GPU (will get overrided if dla_core is specified to the GPU managing DLA). If specified, no positional arguments should be provided\n+            gpu_id (int): ID of target GPU (will get overridden if dla_core is specified to the GPU managing DLA). If specified, no positional arguments should be provided\n             dla_core (int): ID of target DLA core. If specified, no positional arguments should be provided.\n             allow_gpu_fallback (bool): Allow TensorRT to schedule operations on GPU if they are not supported on DLA (ignored if device type is not DLA)\n \ndiff --git a/py/torch_tensorrt/_Input.py b/py/torch_tensorrt/_Input.py\nindex 18636f8114..72775944cb 100644\n--- a/py/torch_tensorrt/_Input.py\n+++ b/py/torch_tensorrt/_Input.py\n@@ -15,11 +15,11 @@ class Input(object):\n         shape_mode (torch_tensorrt.Input._ShapeMode): Is input statically or dynamically shaped\n         shape (Tuple or Dict): Either a single Tuple or a dict of tuples defining the input shape.\n             Static shaped inputs will have a single tuple. Dynamic inputs will have a dict of the form\n-            ``{\n-                \"min_shape\": Tuple,\n-                \"opt_shape\": Tuple,\n-                \"max_shape\": Tuple\n-            }``\n+\n+            .. code-block:: py\n+\n+                {\"min_shape\": Tuple, \"opt_shape\": Tuple, \"max_shape\": Tuple}\n+\n         dtype (torch_tensorrt.dtype): The expected data type of the input tensor (default: torch_tensorrt.dtype.float32)\n         format (torch_tensorrt.TensorFormat): The expected format of the input tensor (default: torch_tensorrt.TensorFormat.NCHW)\n     \"\"\"\n@@ -60,11 +60,11 @@ def __init__(self, *args: Any, **kwargs: Any) -> None:\n         Keyword Arguments:\n             shape (Tuple or List, optional): Static shape of input tensor\n             min_shape (Tuple or List, optional): Min size of input tensor's shape range\n-                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implictly this sets Input's shape_mode to DYNAMIC\n+                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implicitly this sets Input's shape_mode to DYNAMIC\n             opt_shape (Tuple or List, optional): Opt size of input tensor's shape range\n-                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implictly this sets Input's shape_mode to DYNAMIC\n+                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implicitly this sets Input's shape_mode to DYNAMIC\n             max_shape (Tuple or List, optional): Max size of input tensor's shape range\n-                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implictly this sets Input's shape_mode to DYNAMIC\n+                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implicitly this sets Input's shape_mode to DYNAMIC\n             dtype (torch.dtype or torch_tensorrt.dtype): Expected data type for input tensor (default: torch_tensorrt.dtype.float32)\n             format (torch.memory_format or torch_tensorrt.TensorFormat): The expected format of the input tensor (default: torch_tensorrt.TensorFormat.NCHW)\n             tensor_domain (Tuple(float, float), optional): The domain of allowed values for the tensor, as interval notation: [tensor_domain[0], tensor_domain[1]).\ndiff --git a/py/torch_tensorrt/_compile.py b/py/torch_tensorrt/_compile.py\nindex ce966a2609..e59d6a6f3e 100644\n--- a/py/torch_tensorrt/_compile.py\n+++ b/py/torch_tensorrt/_compile.py\n@@ -169,7 +169,7 @@ def compile(\n             torch datatypes or torch_tensorrt datatypes and you can use either torch devices or the torch_tensorrt device type enum\n             to select device type. ::\n \n-                input=[\n+                inputs=[\n                     torch_tensorrt.Input((1, 3, 224, 224)), # Static NCHW input shape for input #1\n                     torch_tensorrt.Input(\n                         min_shape=(1, 224, 224, 3),\n@@ -367,8 +367,15 @@ def convert_method_to_trt_engine(\n \n def load(file_path: str = \"\") -> Any:\n     \"\"\"\n-    Load either a Torchscript model or ExportedProgram. Autodetect the type using\n-    try, except\n+    Load either a Torchscript model or ExportedProgram.\n+\n+    Loads a TorchScript or ExportedProgram file from disk. File type will be detect the type using try, except.\n+\n+    Arguments:\n+        file_path (str): Path to file on the disk\n+\n+    Raises:\n+        ValueError: If there is no file or the file is not either a TorchScript file or ExportedProgram file\n     \"\"\"\n     try:\n         logger.debug(f\"Loading the provided file {file_path} using torch.jit.load()\")\n@@ -405,11 +412,12 @@ def save(\n ) -> None:\n     \"\"\"\n     Save the model to disk in the specified output format.\n+\n     Arguments:\n-        module : Compiled Torch-TensorRT module (Options include torch.jit.ScriptModule | torch.export.ExportedProgram | torch.fx.GraphModule)\n+        module (Optional(torch.jit.ScriptModule | torch.export.ExportedProgram | torch.fx.GraphModule)): Compiled Torch-TensorRT module\n         inputs (torch.Tensor): Torch input tensors\n-        output_format: Format to save the model. Options include exported_program | torchscript.\n-        retrace: When the module type is a fx.GraphModule, this option re-exports the graph using torch.export.export(strict=False) to save it.\n+        output_format (str): Format to save the model. Options include exported_program | torchscript.\n+        retrace (bool): When the module type is a fx.GraphModule, this option re-exports the graph using torch.export.export(strict=False) to save it.\n                 This flag is experimental for now.\n     \"\"\"\n     module_type = _parse_module_type(module)\ndiff --git a/py/torch_tensorrt/_enums.py b/py/torch_tensorrt/_enums.py\nindex befc22d474..7d261a88bf 100644\n--- a/py/torch_tensorrt/_enums.py\n+++ b/py/torch_tensorrt/_enums.py\n@@ -5,27 +5,81 @@\n from typing import Any, Optional, Type, Union\n \n import numpy as np\n-import tensorrt as trt\n import torch\n from torch_tensorrt._features import ENABLED_FEATURES\n \n+import tensorrt as trt\n+\n \n class dtype(Enum):\n-    \"\"\"Enum to set supported dtypes in the compiler\"\"\"\n+    \"\"\"Enum to describe data types to Torch-TensorRT, has compatibility with torch, tensorrt and numpy dtypes\"\"\"\n \n     # Supported types in Torch-TensorRT\n     unknown = auto()\n+    \"\"\"Sentinel value\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     u8 = auto()\n+    \"\"\"Unsigned 8 bit integer, equivalent to ``dtype.uint8``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     i8 = auto()\n+    \"\"\"Signed 8 bit integer, equivalent to ``dtype.int8``, when enabled as a kernel precision typically requires the model to support quantization\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     i32 = auto()\n+    \"\"\"Signed 32 bit integer, equivalent to ``dtype.int32`` and ``dtype.int``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     i64 = auto()\n+    \"\"\"Signed 64 bit integer, equivalent to ``dtype.int64`` and ``dtype.long``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     f16 = auto()\n+    \"\"\"16 bit floating-point number, equivalent to ``dtype.half``, ``dtype.fp16`` and ``dtype.float16``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     f32 = auto()\n+    \"\"\"32 bit floating-point number, equivalent to ``dtype.float``, ``dtype.fp32`` and ``dtype.float32``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     f64 = auto()\n+    \"\"\"64 bit floating-point number, equivalent to ``dtype.double``, ``dtype.fp64`` and ``dtype.float64``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     b = auto()\n+    \"\"\"Boolean value, equivalent to ``dtype.bool``\n+\n+    :meta hide-value:\n+    \"\"\"\n \n     bf16 = auto()\n+    \"\"\"16 bit \"Brain\" floating-point number, equivalent to ``dtype.bfloat16``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     f8 = auto()\n+    \"\"\"8 bit floating-point number, equivalent to ``dtype.fp8`` and ``dtype.float8``\n+\n+    :meta hide-value:\n+    \"\"\"\n \n     uint8 = u8\n     int8 = i8\n@@ -67,6 +121,36 @@ def _from(\n         t: Union[torch.dtype, trt.DataType, np.dtype, dtype, type],\n         use_default: bool = False,\n     ) -> dtype:\n+        \"\"\"Create a Torch-TensorRT dtype from another library's dtype system.\n+\n+        Takes a dtype enum from one of numpy, torch, and tensorrt and create a ``torch_tensorrt.dtype``.\n+        If the source dtype system is not supported or the type is not supported in Torch-TensorRT,\n+        then an exception will be raised. As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.dtype.try_from()``\n+\n+        Arguments:\n+            t (Union(torch.dtype, tensorrt.DataType, numpy.dtype, dtype)): Data type enum from another library\n+            use_default (bool): In some cases a catch all type (such as ``torch_tensorrt.dtype.f32``) is sufficient, so instead of throwing an exception, return default value.\n+\n+        Returns:\n+            dtype: Equivalent ``torch_tensorrt.dtype`` to ``t``\n+\n+        Raises:\n+            TypeError: Unsupported data type or unknown source\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                float_dtype = torch_tensorrt.dtype._from(torch.float) # Returns torch_tensorrt.dtype.f32\n+\n+                # Throws exception\n+                float_dtype = torch_tensorrt.dtype._from(torch.complex128)\n+\n+        \"\"\"\n+\n         # TODO: Ideally implemented with match statement but need to wait for Py39 EoL\n         if isinstance(t, torch.dtype):\n             if t == torch.uint8:\n@@ -139,6 +223,10 @@ def _from(\n                 return dtype.f64\n             elif t == np.bool:\n                 return dtype.b\n+            # TODO: Consider using ml_dtypes when issues like this are resolved:\n+            # https://github.com/pytorch/pytorch/issues/109873\n+            # elif t == ml_dtypes.bfloat16:\n+            #    return dtype.bf16\n             elif use_default:\n                 logging.warning(\n                     f\"Given dtype that does not have direct mapping to Torch-TensorRT supported types ({t}), defaulting to torch_tensorrt.dtype.float\"\n@@ -188,6 +276,32 @@ def try_from(\n         t: Union[torch.dtype, trt.DataType, np.dtype, dtype],\n         use_default: bool = False,\n     ) -> Optional[dtype]:\n+        \"\"\"Create a Torch-TensorRT dtype from another library's dtype system.\n+\n+        Takes a dtype enum from one of numpy, torch, and tensorrt and create a ``torch_tensorrt.dtype``.\n+        If the source dtype system is not supported or the type is not supported in Torch-TensorRT,\n+        then returns ``None``.\n+\n+\n+        Arguments:\n+            t (Union(torch.dtype, tensorrt.DataType, numpy.dtype, dtype)): Data type enum from another library\n+            use_default (bool): In some cases a catch all type (such as ``torch_tensorrt.dtype.f32``) is sufficient, so instead of throwing an exception, return default value.\n+\n+        Returns:\n+            Optional(dtype): Equivalent ``torch_tensorrt.dtype`` to ``t`` or ``None``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                float_dtype = torch_tensorrt.dtype.try_from(torch.float) # Returns torch_tensorrt.dtype.f32\n+\n+                # Unsupported type\n+                float_dtype = torch_tensorrt.dtype.try_from(torch.complex128) # Returns None\n+\n+        \"\"\"\n+\n         try:\n             casted_format = dtype._from(t, use_default=use_default)\n             return casted_format\n@@ -202,6 +316,36 @@ def to(\n         t: Union[Type[torch.dtype], Type[trt.DataType], Type[np.dtype], Type[dtype]],\n         use_default: bool = False,\n     ) -> Union[torch.dtype, trt.DataType, np.dtype, dtype]:\n+        \"\"\"Convert dtype into the equivalent type in [torch, numpy, tensorrt]\n+\n+        Converts ``self`` into one of numpy, torch, and tensorrt equivalent dtypes.\n+        If  ``self`` is not supported in the target library, then an exception will be raised.\n+        As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.dtype.try_to()``\n+\n+        Arguments:\n+            t (Union(Type(torch.dtype), Type(tensorrt.DataType), Type(numpy.dtype), Type(dtype))): Data type enum from another library to convert to\n+            use_default (bool): In some cases a catch all type (such as ``torch.float``) is sufficient, so instead of throwing an exception, return default value.\n+\n+        Returns:\n+            Union(torch.dtype, tensorrt.DataType, numpy.dtype, dtype): dtype equivalent ``torch_tensorrt.dtype`` from library enum ``t``\n+\n+        Raises:\n+            TypeError: Unsupported data type or unknown target\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                float_dtype = torch_tensorrt.dtype.f32.to(torch.dtype) # Returns torch.float\n+\n+                # Failure\n+                float_dtype = torch_tensorrt.dtype.bf16.to(numpy.dtype) # Throws exception\n+\n+        \"\"\"\n+\n         # TODO: Ideally implemented with match statement but need to wait for Py39 EoL\n         if t == torch.dtype:\n             if self == dtype.u8:\n@@ -273,10 +417,14 @@ def to(\n                 return np.float64\n             elif self == dtype.b:\n                 return np.bool_\n+            # TODO: Consider using ml_dtypes when issues like this are resolved:\n+            # https://github.com/pytorch/pytorch/issues/109873\n+            # elif self == dtype.bf16:\n+            #    return ml_dtypes.bfloat16\n             elif use_default:\n                 return np.float32\n             else:\n-                raise TypeError(\"Unspported numpy dtype\")\n+                raise TypeError(\"Unsupported numpy dtype\")\n \n         elif t == dtype:\n             return self\n@@ -315,6 +463,30 @@ def try_to(\n         t: Union[Type[torch.dtype], Type[trt.DataType], Type[np.dtype], Type[dtype]],\n         use_default: bool,\n     ) -> Optional[Union[torch.dtype, trt.DataType, np.dtype, dtype]]:\n+        \"\"\"Convert dtype into the equivalent type in [torch, numpy, tensorrt]\n+\n+        Converts ``self`` into one of numpy, torch, and tensorrt equivalent dtypes.\n+        If  ``self`` is not supported in the target library, then returns ``None``.\n+\n+        Arguments:\n+            t (Union(Type(torch.dtype), Type(tensorrt.DataType), Type(numpy.dtype), Type(dtype))): Data type enum from another library to convert to\n+            use_default (bool): In some cases a catch all type (such as ``torch.float``) is sufficient, so instead of throwing an exception, return default value.\n+\n+        Returns:\n+            Optional(Union(torch.dtype, tensorrt.DataType, numpy.dtype, dtype)): dtype equivalent ``torch_tensorrt.dtype`` from library enum ``t``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                float_dtype = torch_tensorrt.dtype.f32.to(torch.dtype) # Returns torch.float\n+\n+                # Failure\n+                float_dtype = torch_tensorrt.dtype.bf16.to(numpy.dtype) # Returns None\n+\n+        \"\"\"\n+\n         try:\n             casted_format = self.to(t, use_default)\n             return casted_format\n@@ -338,21 +510,130 @@ def __hash__(self) -> int:\n \n \n class memory_format(Enum):\n+    \"\"\"\"\"\"\n \n     # TensorRT supported memory layouts\n     linear = auto()\n+    \"\"\"Row major linear format.\n+\n+    For a tensor with dimensions {N, C, H, W}, the W axis always has unit stride, and the stride of every other axis is at least the product of the next dimension times the next stride. the strides are the same as for a C array with dimensions [N][C][H][W].\n+\n+    Equivient to ``memory_format.contiguous``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     chw2 = auto()\n+    \"\"\"Two wide channel vectorized row major format.\n+\n+    This format is bound to FP16 in TensorRT. It is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to a C array with dimensions [N][(C+1)/2][H][W][2], with the tensor coordinates (n, c, h, w) mapping to array subscript [n][c/2][h][w][c%2].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     hwc8 = auto()\n+    \"\"\"Eight channel format where C is padded to a multiple of 8.\n+\n+    This format is bound to FP16. It is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to the array with dimensions [N][H][W][(C+7)/8*8], with the tensor coordinates (n, c, h, w) mapping to array subscript [n][h][w][c].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     chw4 = auto()\n+    \"\"\"Four wide channel vectorized row major format. This format is bound to INT8. It is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to a C array with dimensions [N][(C+3)/4][H][W][4], with the tensor coordinates (n, c, h, w) mapping to array subscript [n][c/4][h][w][c%4].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     chw16 = auto()\n+    \"\"\"Sixteen wide channel vectorized row major format.\n+\n+    This format is bound to FP16. It is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to a C array with dimensions [N][(C+15)/16][H][W][16], with the tensor coordinates (n, c, h, w) mapping to array subscript [n][c/16][h][w][c%16].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     chw32 = auto()\n+    \"\"\"Thirty-two wide channel vectorized row major format.\n+\n+    This format is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to a C array with dimensions [N][(C+31)/32][H][W][32], with the tensor coordinates (n, c, h, w) mapping to array subscript [n][c/32][h][w][c%32].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     dhwc8 = auto()\n+    \"\"\"Eight channel format where C is padded to a multiple of 8.\n+\n+    This format is bound to FP16, and it is only available for dimensions >= 4.\n+\n+    For a tensor with dimensions {N, C, D, H, W}, the memory layout is equivalent to an array with dimensions [N][D][H][W][(C+7)/8*8], with the tensor coordinates (n, c, d, h, w) mapping to array subscript [n][d][h][w][c].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     cdhw32 = auto()\n+    \"\"\"Thirty-two wide channel vectorized row major format with 3 spatial dimensions.\n+\n+    This format is bound to FP16 and INT8. It is only available for dimensions >= 4.\n+\n+    For a tensor with dimensions {N, C, D, H, W}, the memory layout is equivalent to a C array with dimensions [N][(C+31)/32][D][H][W][32], with the tensor coordinates (n, d, c, h, w) mapping to array subscript [n][c/32][d][h][w][c%32].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     hwc = auto()\n+    \"\"\"Non-vectorized channel-last format. This format is bound to FP32 and is only available for dimensions >= 3.\n+\n+    Equivient to ``memory_format.channels_last``\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     dla_linear = auto()\n+    \"\"\" DLA planar format. Row major format. The stride for stepping along the H axis is rounded up to 64 bytes.\n+\n+    This format is bound to FP16/Int8 and is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to a C array with dimensions [N][C][H][roundUp(W, 64/elementSize)] where elementSize is 2 for FP16 and 1 for Int8, with the tensor coordinates (n, c, h, w) mapping to array subscript [n][c][h][w].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     dla_hwc4 = auto()\n+    \"\"\"DLA image format. channel-last format. C can only be 1, 3, 4. If C == 3 it will be rounded to 4. The stride for stepping along the H axis is rounded up to 32 bytes.\n+\n+    This format is bound to FP16/Int8 and is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, with C\u2019 is 1, 4, 4 when C is 1, 3, 4 respectively, the memory layout is equivalent to a C array with dimensions [N][H][roundUp(W, 32/C\u2019/elementSize)][C\u2019] where elementSize is 2 for FP16 and 1 for Int8, C\u2019 is the rounded C. The tensor coordinates (n, c, h, w) maps to array subscript [n][h][w][c].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     hwc16 = auto()\n+    \"\"\"Sixteen channel format where C is padded to a multiple of 16. This format is bound to FP16. It is only available for dimensions >= 3.\n+\n+    For a tensor with dimensions {N, C, H, W}, the memory layout is equivalent to the array with dimensions [N][H][W][(C+15)/16*16], with the tensor coordinates (n, c, h, w) mapping to array subscript [n][h][w][c].\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     dhwc = auto()\n+    \"\"\"Non-vectorized channel-last format. This format is bound to FP32. It is only available for dimensions >= 4.\n+\n+    Equivient to ``memory_format.channels_last_3d``\n+\n+    :meta hide-value:\n+    \"\"\"\n \n     # PyTorch aliases for TRT layouts\n     contiguous = linear\n@@ -363,6 +644,30 @@ class memory_format(Enum):\n     def _from(\n         cls, f: Union[torch.memory_format, trt.TensorFormat, memory_format]\n     ) -> memory_format:\n+        \"\"\"Create a Torch-TensorRT memory format enum from another library memory format enum.\n+\n+        Takes a memory format enum from one of torch, and tensorrt and create a ``torch_tensorrt.memory_format``.\n+        If the source is not supported or the memory format is not supported in Torch-TensorRT,\n+        then an exception will be raised. As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.memory_format.try_from()``\n+\n+        Arguments:\n+            f (Union(torch.memory_format, tensorrt.TensorFormat, memory_format)): Memory format enum from another library\n+\n+        Returns:\n+            memory_format: Equivalent ``torch_tensorrt.memory_format`` to ``f``\n+\n+        Raises:\n+            TypeError: Unsupported memory format or unknown source\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                torchtrt_linear = torch_tensorrt.memory_format._from(torch.contiguous)\n+\n+        \"\"\"\n         # TODO: Ideally implemented with match statement but need to wait for Py39 EoL\n         if isinstance(f, torch.memory_format):\n             if f == torch.contiguous_format:\n@@ -430,6 +735,26 @@ def _from(\n     def try_from(\n         cls, f: Union[torch.memory_format, trt.TensorFormat, memory_format]\n     ) -> Optional[memory_format]:\n+        \"\"\"Create a Torch-TensorRT memory format enum from another library memory format enum.\n+\n+        Takes a memory format enum from one of torch, and tensorrt and create a ``torch_tensorrt.memory_format``.\n+        If the source is not supported or the memory format is not supported in Torch-TensorRT,\n+        then ``None`` will be returned.\n+\n+\n+        Arguments:\n+            f (Union(torch.memory_format, tensorrt.TensorFormat, memory_format)): Memory format enum from another library\n+\n+        Returns:\n+            Optional(memory_format): Equivalent ``torch_tensorrt.memory_format`` to ``f``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                torchtrt_linear = torch_tensorrt.memory_format.try_from(torch.contiguous)\n+\n+        \"\"\"\n         try:\n             casted_format = memory_format._from(f)\n             return casted_format\n@@ -446,6 +771,31 @@ def to(\n             Type[torch.memory_format], Type[trt.TensorFormat], Type[memory_format]\n         ],\n     ) -> Union[torch.memory_format, trt.TensorFormat, memory_format]:\n+        \"\"\"Convert ``memory_format`` into the equivalent type in torch or tensorrt\n+\n+        Converts ``self`` into one of torch or tensorrt equivalent memory format.\n+        If  ``self`` is not supported in the target library, then an exception will be raised.\n+        As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.memory_format.try_to()``\n+\n+        Arguments:\n+            t (Union(Type(torch.memory_format), Type(tensorrt.TensorFormat), Type(memory_format))): Memory format type enum from another library to convert to\n+\n+        Returns:\n+            Union(torch.memory_format, tensorrt.TensorFormat, memory_format): Memory format equivalent ``torch_tensorrt.memory_format`` in enum ``t``\n+\n+        Raises:\n+            TypeError: Unknown target type or unsupported memory format\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                tf = torch_tensorrt.memory_format.linear.to(torch.dtype) # Returns torch.contiguous\n+        \"\"\"\n+\n         if t == torch.memory_format:\n             if self == memory_format.contiguous:\n                 return torch.contiguous_format\n@@ -512,6 +862,25 @@ def try_to(\n             Type[torch.memory_format], Type[trt.TensorFormat], Type[memory_format]\n         ],\n     ) -> Optional[Union[torch.memory_format, trt.TensorFormat, memory_format]]:\n+        \"\"\"Convert ``memory_format`` into the equivalent type in torch or tensorrt\n+\n+        Converts ``self`` into one of torch or tensorrt equivalent memory format.\n+        If  ``self`` is not supported in the target library, then ``None`` will be returned\n+\n+        Arguments:\n+            t (Union(Type(torch.memory_format), Type(tensorrt.TensorFormat), Type(memory_format))): Memory format type enum from another library to convert to\n+\n+        Returns:\n+            Optional(Union(torch.memory_format, tensorrt.TensorFormat, memory_format)): Memory format equivalent ``torch_tensorrt.memory_format`` in enum ``t``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                tf = torch_tensorrt.memory_format.linear.to(torch.dtype) # Returns torch.contiguous\n+        \"\"\"\n+\n         try:\n             casted_format = self.to(t)\n             return casted_format\n@@ -533,12 +902,55 @@ def __hash__(self) -> int:\n \n \n class DeviceType(Enum):\n+    \"\"\"Type of device TensorRT will target\"\"\"\n+\n     UNKNOWN = auto()\n+    \"\"\"\n+    Sentinel value\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     GPU = auto()\n+    \"\"\"\n+    Target is a GPU\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     DLA = auto()\n+    \"\"\"\n+    Target is a DLA core\n+\n+    :meta hide-value:\n+    \"\"\"\n \n     @classmethod\n     def _from(cls, d: Union[trt.DeviceType, DeviceType]) -> DeviceType:\n+        \"\"\"Create a Torch-TensorRT device type enum from a TensorRT device type enum.\n+\n+        Takes a device type enum from tensorrt and create a ``torch_tensorrt.DeviceType``.\n+        If the source is not supported or the device type is not supported in Torch-TensorRT,\n+        then an exception will be raised. As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.DeviceType.try_from()``\n+\n+        Arguments:\n+            d (Union(tensorrt.DeviceType, DeviceType)): Device type enum from another library\n+\n+        Returns:\n+            DeviceType: Equivalent ``torch_tensorrt.DeviceType`` to ``d``\n+\n+        Raises:\n+            TypeError: Unknown source type or unsupported device type\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                torchtrt_dla = torch_tensorrt.DeviceType._from(tensorrt.DeviceType.DLA)\n+\n+        \"\"\"\n         if isinstance(d, trt.DeviceType):\n             if d == trt.DeviceType.GPU:\n                 return DeviceType.GPU\n@@ -569,6 +981,27 @@ def _from(cls, d: Union[trt.DeviceType, DeviceType]) -> DeviceType:\n \n     @classmethod\n     def try_from(cls, d: Union[trt.DeviceType, DeviceType]) -> Optional[DeviceType]:\n+        \"\"\"Create a Torch-TensorRT device type enum from a TensorRT device type enum.\n+\n+        Takes a device type enum from tensorrt and create a ``torch_tensorrt.DeviceType``.\n+        If the source is not supported or the device type is not supported in Torch-TensorRT,\n+        then an exception will be raised. As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.DeviceType.try_from()``\n+\n+        Arguments:\n+            d (Union(tensorrt.DeviceType, DeviceType)): Device type enum from another library\n+\n+        Returns:\n+            DeviceType: Equivalent ``torch_tensorrt.DeviceType`` to ``d``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                torchtrt_dla = torch_tensorrt.DeviceType._from(tensorrt.DeviceType.DLA)\n+\n+        \"\"\"\n         try:\n             casted_format = DeviceType._from(d)\n             return casted_format\n@@ -584,6 +1017,31 @@ def to(\n         t: Union[Type[trt.DeviceType], Type[DeviceType]],\n         use_default: bool = False,\n     ) -> Union[trt.DeviceType, DeviceType]:\n+        \"\"\"Convert ``DeviceType`` into the equivalent type in tensorrt\n+\n+        Converts ``self`` into one of torch or tensorrt equivalent device type.\n+        If  ``self`` is not supported in the target library, then an exception will be raised.\n+        As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.DeviceType.try_to()``\n+\n+        Arguments:\n+            t (Union(Type(tensorrt.DeviceType), Type(DeviceType))): Device type enum from another library to convert to\n+\n+        Returns:\n+            Union(tensorrt.DeviceType, DeviceType): Device type equivalent ``torch_tensorrt.DeviceType`` in enum ``t``\n+\n+        Raises:\n+            TypeError: Unknown target type or unsupported device type\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                trt_dla = torch_tensorrt.DeviceType.DLA.to(tensorrt.DeviceType) # Returns tensorrt.DeviceType.DLA\n+        \"\"\"\n+\n         if t == trt.DeviceType:\n             if self == DeviceType.GPU:\n                 return trt.DeviceType.GPU\n@@ -621,6 +1079,24 @@ def try_to(\n         t: Union[Type[trt.DeviceType], Type[DeviceType]],\n         use_default: bool = False,\n     ) -> Optional[Union[trt.DeviceType, DeviceType]]:\n+        \"\"\"Convert ``DeviceType`` into the equivalent type in tensorrt\n+\n+        Converts ``self`` into one of torch or tensorrt equivalent memory format.\n+        If  ``self`` is not supported in the target library, then ``None`` will be returned.\n+\n+        Arguments:\n+            t (Union(Type(tensorrt.DeviceType), Type(DeviceType))): Device type enum from another library to convert to\n+\n+        Returns:\n+            Optional(Union(tensorrt.DeviceType, DeviceType)): Device type equivalent ``torch_tensorrt.DeviceType`` in enum ``t``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                trt_dla = torch_tensorrt.DeviceType.DLA.to(tensorrt.DeviceType) # Returns tensorrt.DeviceType.DLA\n+        \"\"\"\n         try:\n             casted_format = self.to(t, use_default=use_default)\n             return casted_format\n@@ -640,14 +1116,59 @@ def __hash__(self) -> int:\n \n \n class EngineCapability(Enum):\n+    \"\"\"\n+    EngineCapability determines the restrictions of a network during build time and what runtime it targets.\n+    \"\"\"\n+\n     STANDARD = auto()\n+    \"\"\"\n+    EngineCapability.STANDARD does not provide any restrictions on functionality and the resulting serialized engine can be executed with TensorRT\u2019s standard runtime APIs.\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     SAFETY = auto()\n+    \"\"\"\n+    EngineCapability.SAFETY provides a restricted subset of network operations that are safety certified and the resulting serialized engine can be executed with TensorRT\u2019s safe runtime APIs in the tensorrt.safe namespace.\n+\n+    :meta hide-value:\n+    \"\"\"\n+\n     DLA_STANDALONE = auto()\n+    \"\"\"\n+    ``EngineCapability.DLA_STANDALONE`` provides a restricted subset of network operations that are DLA compatible and the resulting serialized engine can be executed using standalone DLA runtime APIs.\n+\n+    :meta hide-value:\n+    \"\"\"\n \n     @classmethod\n     def _from(\n         cls, c: Union[trt.EngineCapability, EngineCapability]\n     ) -> EngineCapability:\n+        \"\"\"Create a Torch-TensorRT Engine capability enum from a TensorRT Engine capability enum.\n+\n+        Takes a device type enum from tensorrt and create a ``torch_tensorrt.EngineCapability``.\n+        If the source is not supported or the engine capability is not supported in Torch-TensorRT,\n+        then an exception will be raised. As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.EngineCapability.try_from()``\n+\n+        Arguments:\n+            c (Union(tensorrt.EngineCapability, EngineCapability)): Engine capability enum from another library\n+\n+        Returns:\n+            EngineCapability: Equivalent ``torch_tensorrt.EngineCapability`` to ``c``\n+\n+        Raises:\n+            TypeError: Unknown source type or unsupported engine capability\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                torchtrt_ec = torch_tensorrt.EngineCapability._from(tensorrt.EngineCapability.SAFETY)\n+\n+        \"\"\"\n         if isinstance(c, trt.EngineCapability):\n             if c == trt.EngineCapability.STANDARD:\n                 return EngineCapability.STANDARD\n@@ -682,6 +1203,27 @@ def _from(\n     def try_from(\n         c: Union[trt.EngineCapability, EngineCapability]\n     ) -> Optional[EngineCapability]:\n+        \"\"\"Create a Torch-TensorRT engine capability enum from a TensorRT engine capability enum.\n+\n+        Takes a device type enum from tensorrt and create a ``torch_tensorrt.EngineCapability``.\n+        If the source is not supported or the engine capability level is not supported in Torch-TensorRT,\n+        then an exception will be raised. As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.EngineCapability.try_from()``\n+\n+        Arguments:\n+            c (Union(tensorrt.EngineCapability, EngineCapability)): Engine capability enum from another library\n+\n+        Returns:\n+            EngineCapability: Equivalent ``torch_tensorrt.EngineCapability`` to ``c``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                torchtrt_safety_ec = torch_tensorrt.EngineCapability._from(tensorrt.EngineCapability.SAEFTY)\n+\n+        \"\"\"\n         try:\n             casted_format = EngineCapability._from(c)\n             return casted_format\n@@ -695,6 +1237,30 @@ def try_from(\n     def to(\n         self, t: Union[Type[trt.EngineCapability], Type[EngineCapability]]\n     ) -> Union[trt.EngineCapability, EngineCapability]:\n+        \"\"\"Convert ``EngineCapability`` into the equivalent type in tensorrt\n+\n+        Converts ``self`` into one of torch or tensorrt equivalent engine capability.\n+        If  ``self`` is not supported in the target library, then an exception will be raised.\n+        As such it is not recommended to use this method directly.\n+\n+        Alternatively use ``torch_tensorrt.EngineCapability.try_to()``\n+\n+        Arguments:\n+            t (Union(Type(tensorrt.EngineCapability), Type(EngineCapability))): Engine capability enum from another library to convert to\n+\n+        Returns:\n+            Union(tensorrt.EngineCapability, EngineCapability): Engine capability equivalent ``torch_tensorrt.EngineCapability`` in enum ``t``\n+\n+        Raises:\n+            TypeError: Unknown target type or unsupported engine capability\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                torchtrt_dla_ec = torch_tensorrt.EngineCapability.DLA_STANDALONE.to(tensorrt.EngineCapability) # Returns tensorrt.EngineCapability.DLA\n+        \"\"\"\n         if t == trt.EngineCapability:\n             if self == EngineCapability.STANDARD:\n                 return trt.EngineCapability.STANDARD\n@@ -722,12 +1288,30 @@ def to(\n                     raise ValueError(\"Provided an unsupported engine capability\")\n         # else: # commented out for mypy\n         raise TypeError(\n-            \"Provided unsupported destination type for engine capablity type conversion\"\n+            \"Provided unsupported destination type for engine capability type conversion\"\n         )\n \n     def try_to(\n         self, t: Union[Type[trt.EngineCapability], Type[EngineCapability]]\n     ) -> Optional[Union[trt.EngineCapability, EngineCapability]]:\n+        \"\"\"Convert ``EngineCapability`` into the equivalent type in tensorrt\n+\n+        Converts ``self`` into one of torch or tensorrt equivalent engine capability.\n+        If  ``self`` is not supported in the target library, then ``None`` will be returned.\n+\n+        Arguments:\n+            t (Union(Type(tensorrt.EngineCapability), Type(EngineCapability))): Engine capability enum from another library to convert to\n+\n+        Returns:\n+            Optional(Union(tensorrt.EngineCapability, EngineCapability)): Engine capability equivalent ``torch_tensorrt.EngineCapability`` in enum ``t``\n+\n+        Examples:\n+\n+            .. code:: py\n+\n+                # Succeeds\n+                trt_dla_ec = torch_tensorrt.EngineCapability.DLA.to(tensorrt.EngineCapability) # Returns tensorrt.EngineCapability.DLA_STANDALONE\n+        \"\"\"\n         try:\n             casted_format = self.to(t)\n             return casted_format\ndiff --git a/py/torch_tensorrt/csrc/torch_tensorrt_py.cpp b/py/torch_tensorrt/csrc/torch_tensorrt_py.cpp\nindex e9ad8b159c..e32d102f8b 100644\n--- a/py/torch_tensorrt/csrc/torch_tensorrt_py.cpp\n+++ b/py/torch_tensorrt/csrc/torch_tensorrt_py.cpp\n@@ -242,7 +242,7 @@ PYBIND11_MODULE(_C, m) {\n       .def(\"__str__\", &InputSignature::to_str)\n       .def_readwrite(\"_signature_ivalue\", &InputSignature::signature_ivalue);\n \n-  py::enum_<DataType>(m, \"dtype\", \"Enum to specifiy operating precision for engine execution\")\n+  py::enum_<DataType>(m, \"dtype\", \"Enum to specify operating precision for engine execution\")\n       .value(\"float\", DataType::kFloat, \"32 bit floating point number\")\n       .value(\"float32\", DataType::kFloat, \"32 bit floating point number\")\n       .value(\"half\", DataType::kHalf, \"16 bit floating point number\")\n@@ -270,7 +270,7 @@ PYBIND11_MODULE(_C, m) {\n       .value(\"DLA_STANDALONE\", EngineCapability::kDLA_STANDALONE, \"Use DLA kernels only\")\n       .value(\"STANDARD\", EngineCapability::kSTANDARD, \"Use default behavior\");\n \n-  py::enum_<TensorFormat>(m, \"TensorFormat\", \"Enum to specifiy the memory layout of tensors\")\n+  py::enum_<TensorFormat>(m, \"TensorFormat\", \"Enum to specify the memory layout of tensors\")\n       .value(\"contiguous\", TensorFormat::kContiguous, \"Contiguous memory layout (NCHW / Linear)\")\n       .value(\"channels_last\", TensorFormat::kChannelsLast, \"Channels last memory layout (NHWC)\")\n       .export_values();\n@@ -401,7 +401,7 @@ PYBIND11_MODULE(_C, m) {\n   ts_sub_mod.def(\n       \"check_method_op_support\",\n       &torch_tensorrt::pyapi::CheckMethodOperatorSupport,\n-      \"Takes a module and a method name and checks if the method graph contains purely convertable operators\");\n+      \"Takes a module and a method name and checks if the method graph contains purely convertible operators\");\n   ts_sub_mod.def(\n       \"embed_engine_in_new_module\",\n       &torch_tensorrt::pyapi::EmbedEngineInNewModule,\ndiff --git a/py/torch_tensorrt/csrc/util.h b/py/torch_tensorrt/csrc/util.h\nindex 10f07e7f82..01a47564d3 100644\n--- a/py/torch_tensorrt/csrc/util.h\n+++ b/py/torch_tensorrt/csrc/util.h\n@@ -13,14 +13,14 @@ namespace util {\n namespace py = pybind11;\n \n // Method for calling the python function and returning the value (returned from python) used in cpp trampoline\n-// classes. Prints an error if no such method is overriden in python.\n+// classes. Prints an error if no such method is overridden in python.\n // T* must NOT be a trampoline class!\n template <typename T>\n py::function getOverload(const T* self, const std::string& overloadName) {\n   py::function overload = py::get_override(self, overloadName.c_str());\n   if (!overload) {\n     std::string msg{\n-        \"Method: \" + overloadName + \" was not overriden. Please provide an implementation for this method.\"};\n+        \"Method: \" + overloadName + \" was not overridden. Please provide an implementation for this method.\"};\n     LOG_ERROR(msg);\n   }\n   return overload;\ndiff --git a/py/torch_tensorrt/dynamo/_compiler.py b/py/torch_tensorrt/dynamo/_compiler.py\nindex e854b04d42..0c29bd378e 100644\n--- a/py/torch_tensorrt/dynamo/_compiler.py\n+++ b/py/torch_tensorrt/dynamo/_compiler.py\n@@ -92,19 +92,21 @@ def compile(\n         exported_program (torch.export.ExportedProgram): Source module, running torch.export on a ``torch.nn.Module``\n         inputs (Tuple[Any, ...]): List of specifications of input shape, dtype and memory layout for inputs to the module. This argument is required. Input Sizes can be specified as torch sizes, tuples or lists. dtypes can be specified using\n             torch datatypes or torch_tensorrt datatypes and you can use either torch devices or the torch_tensorrt device type enum\n-            to select device type. ::\n-\n-                input=[\n-                    torch_tensorrt.Input((1, 3, 224, 224)), # Static NCHW input shape for input #1\n-                    torch_tensorrt.Input(\n-                        min_shape=(1, 224, 224, 3),\n-                        opt_shape=(1, 512, 512, 3),\n-                        max_shape=(1, 1024, 1024, 3),\n-                        dtype=torch.int32\n-                        format=torch.channel_last\n-                    ), # Dynamic input shape for input #2\n-                    torch.randn((1, 3, 224, 244)) # Use an example tensor and let torch_tensorrt infer settings\n-                ]\n+            to select device type.\n+\n+                .. code-block:: py\n+\n+                    inputs=[\n+                        torch_tensorrt.Input((1, 3, 224, 224)), # Static NCHW input shape for input #1\n+                        torch_tensorrt.Input(\n+                            min_shape=(1, 224, 224, 3),\n+                            opt_shape=(1, 512, 512, 3),\n+                            max_shape=(1, 1024, 1024, 3),\n+                            dtype=torch.int32\n+                            format=torch.channel_last\n+                        ), # Dynamic input shape for input #2\n+                        torch.randn((1, 3, 224, 244)) # Use an example tensor and let torch_tensorrt infer settings\n+                    ]\n \n     Keyword Arguments:\n         device (Union(torch_tensorrt.Device, torch.device, dict)): Target device for TensorRT engines to run on ::\n@@ -126,7 +128,7 @@ def compile(\n         truncate_double (bool): Truncate weights provided in double (float64) to float32\n         calibrator (Union(torch_tensorrt._C.IInt8Calibrator, tensorrt.IInt8Calibrator)): Calibrator object which will provide data to the PTQ system for INT8 Calibration\n         require_full_compilation (bool): Require modules to be compiled end to end or return an error as opposed to returning a hybrid graph where operations that cannot be run in TensorRT are run in PyTorch\n-        min_block_size (int): The minimum number of contiguous TensorRT convertable operations in order to run a set of operations in TensorRT\n+        min_block_size (int): The minimum number of contiguous TensorRT convertible operations in order to run a set of operations in TensorRT\n         torch_executed_ops (Collection[Target]): Set of aten operators that must be run in PyTorch. An error will be thrown if this set is not empty but ``require_full_compilation`` is True\n         torch_executed_modules (List[str]): List of modules that must be run in PyTorch. An error will be thrown if this list is not empty but ``require_full_compilation`` is True\n         pass_through_build_failures (bool): Error out if there are issues during compilation (only applicable to torch.compile workflows)\n@@ -134,8 +136,8 @@ def compile(\n         version_compatible (bool): Build the TensorRT engines compatible with future versions of TensorRT (Restrict to lean runtime operators to provide version forward compatibility for the engines)\n         optimization_level: (Optional[int]): Setting a higher optimization level allows TensorRT to spend longer engine building time searching for more optimization options. The resulting engine may have better performance compared to an engine built with a lower optimization level. The default optimization level is 3. Valid values include integers from 0 to the maximum optimization level, which is currently 5. Setting it to be greater than the maximum level results in identical behavior to the maximum level.\n         use_python_runtime: (bool): Return a graph using a pure Python runtime, reduces options for serialization\n-        use_fast_partitioner: (bool): Use the adjacency based partitioning scheme instead of the global partitioner. Adjacency partitioning is faster but may not be optiminal. Use the global paritioner (``False``) if looking for best performance\n-        enable_experimental_decompositions (bool): Use the full set of operator decompositions. These decompositions may not be tested but serve to make the grap easier to covert to TensorRT, potentially increasing the amount of graphs run in TensorRT.\n+        use_fast_partitioner: (bool): Use the adjacency based partitioning scheme instead of the global partitioner. Adjacency partitioning is faster but may not be optimal. Use the global paritioner (``False``) if looking for best performance\n+        enable_experimental_decompositions (bool): Use the full set of operator decompositions. These decompositions may not be tested but serve to make the graph easier to convert to TensorRT, potentially increasing the amount of graphs run in TensorRT.\n         dryrun (bool): Toggle for \"Dryrun\" mode, running everything except conversion to TRT and logging outputs\n         hardware_compatible (bool): Build the TensorRT engines compatible with GPU architectures other than that of the GPU on which the engine was built (currently works for NVIDIA Ampere and newer)\n         timing_cache_path (str): Path to the timing cache if it exists (or) where it will be saved after compilation\n@@ -464,7 +466,7 @@ def contains_metadata(gm: torch.fx.GraphModule) -> bool:\n \n def convert_module_to_trt_engine(\n     exported_program: ExportedProgram,\n-    inputs: Tuple[Any, ...],\n+    inputs: Sequence[Any],\n     *,\n     enabled_precisions: (\n         Set[torch.dtype | dtype] | Tuple[torch.dtype | dtype]\n@@ -507,19 +509,21 @@ def convert_module_to_trt_engine(\n     Keyword Args:\n         inputs (Optional[Sequence[torch_tensorrt.Input | torch.Tensor]]): **Required** List of specifications of input shape, dtype and memory layout for inputs to the module. This argument is required. Input Sizes can be specified as torch sizes, tuples or lists. dtypes can be specified using\n             torch datatypes or torch_tensorrt datatypes and you can use either torch devices or the torch_tensorrt device type enum\n-            to select device type. ::\n-\n-                input=[\n-                    torch_tensorrt.Input((1, 3, 224, 224)), # Static NCHW input shape for input #1\n-                    torch_tensorrt.Input(\n-                        min_shape=(1, 224, 224, 3),\n-                        opt_shape=(1, 512, 512, 3),\n-                        max_shape=(1, 1024, 1024, 3),\n-                        dtype=torch.int32\n-                        format=torch.channel_last\n-                    ), # Dynamic input shape for input #2\n-                    torch.randn((1, 3, 224, 244)) # Use an example tensor and let torch_tensorrt infer settings\n-                ]\n+            to select device type.\n+\n+                .. code-block:: py\n+\n+                  inputs=[\n+                        torch_tensorrt.Input((1, 3, 224, 224)), # Static NCHW input shape for input #1\n+                        torch_tensorrt.Input(\n+                            min_shape=(1, 224, 224, 3),\n+                            opt_shape=(1, 512, 512, 3),\n+                            max_shape=(1, 1024, 1024, 3),\n+                            dtype=torch.int32\n+                            format=torch.channel_last\n+                        ), # Dynamic input shape for input #2\n+                        torch.randn((1, 3, 224, 244)) # Use an example tensor and let torch_tensorrt infer settings\n+                    ]\n         enabled_precisions (Optional[Set[torch.dtype | _enums.dtype]]): The set of datatypes that TensorRT can use\n         debug (bool): Whether to print out verbose debugging information\n         workspace_size (int): Workspace TRT is allowed to use for the module (0 is default)\ndiff --git a/py/torch_tensorrt/dynamo/_refit.py b/py/torch_tensorrt/dynamo/_refit.py\nindex 38810e59b3..569fc2db55 100644\n--- a/py/torch_tensorrt/dynamo/_refit.py\n+++ b/py/torch_tensorrt/dynamo/_refit.py\n@@ -6,7 +6,6 @@\n from typing import Any, Sequence, Tuple\n \n import numpy as np\n-import tensorrt as trt\n import torch\n from torch.export import ExportedProgram\n from torch_tensorrt._enums import dtype\n@@ -43,6 +42,8 @@\n )\n from torch_tensorrt.logging import TRT_LOGGER\n \n+import tensorrt as trt\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -95,12 +96,16 @@ def construct_refit_mapping(\n         layer_type: str = layer.type.name\n         if layer_type in MODULE_MAP:\n             # Cast the parent class to child class to access attributes\n-            # For example: ILayer does not have ILayer.kernal/ILayer.bias\n+            # For example: ILayer does not have ILayer.kernel/ILayer.bias\n             # So we cast it to IConvolutionLayer and access the attributes\n             layer.__class__ = MODULE_MAP[layer_type][0]\n             for weight_type, weight_name in MODULE_MAP[layer_type][1]:\n                 weight = layer.__getattribute__(weight_type).copy()\n-                weight_dtype = dtype.try_from(weight.dtype).to(trt.DataType)\n+                weight_dtype_opt = dtype.try_from(weight.dtype)\n+                assert (\n+                    weight_dtype_opt is not None\n+                ), f\"Weights {weight_name} has unsupported dtype {weight.dtype}\"\n+                weight_dtype = weight_dtype_opt.to(trt.DataType)\n                 weight_map[f\"{layer.name} {weight_name}\"] = (\n                     weight,\n                     weight_dtype,\ndiff --git a/py/torch_tensorrt/dynamo/_settings.py b/py/torch_tensorrt/dynamo/_settings.py\nindex 57b7d5dc69..13c786b858 100644\n--- a/py/torch_tensorrt/dynamo/_settings.py\n+++ b/py/torch_tensorrt/dynamo/_settings.py\n@@ -70,7 +70,7 @@ class CompilationSettings:\n         dla_global_dram_size (int): Host RAM used by DLA to store weights and metadata for execution\n         dryrun (Union[bool, str]): Toggle \"Dryrun\" mode, which runs everything through partitioning, short of conversion to\n             TRT Engines. Prints detailed logs of the graph structure and nature of partitioning. Optionally saves the\n-            ouptut to a file if a string path is specified\n+            output to a file if a string path is specified\n         hardware_compatible (bool): Build the TensorRT engines compatible with GPU architectures other than that of the GPU on which the engine was built (currently works for NVIDIA Ampere and newer)\n         timing_cache_path (str): Path to the timing cache if it exists (or) where it will be saved after compilation\n     \"\"\"\ndiff --git a/py/torch_tensorrt/dynamo/_tracer.py b/py/torch_tensorrt/dynamo/_tracer.py\nindex 6bc334f427..e1b89886ca 100644\n--- a/py/torch_tensorrt/dynamo/_tracer.py\n+++ b/py/torch_tensorrt/dynamo/_tracer.py\n@@ -45,7 +45,7 @@ def trace(\n             device=torch.device(\"cuda:0\")\n \n         debug (bool): Enable debuggable engine\n-        enable_experimental_decompositions (bool): Use the full set of operator decompositions. These decompositions may not be tested but serve to make the grap easier to covert to TensorRT, potentially increasing the amount of graphs run in TensorRT.\n+        enable_experimental_decompositions (bool): Use the full set of operator decompositions. These decompositions may not be tested but serve to make the graph easier to convert to TensorRT, potentially increasing the amount of graphs run in TensorRT.\n         **kwargs: Any,\n     Returns:\n         torch.fx.GraphModule: Compiled FX Module, when run it will execute via TensorRT\ndiff --git a/py/torch_tensorrt/dynamo/conversion/_conversion.py b/py/torch_tensorrt/dynamo/conversion/_conversion.py\nindex 1142559838..ea3034cb8c 100644\n--- a/py/torch_tensorrt/dynamo/conversion/_conversion.py\n+++ b/py/torch_tensorrt/dynamo/conversion/_conversion.py\n@@ -4,7 +4,6 @@\n import logging\n from typing import List, Sequence\n \n-import tensorrt as trt\n import torch\n from torch.fx.experimental.proxy_tensor import maybe_disable_fake_tensor_mode\n from torch_tensorrt._Device import Device\n@@ -19,6 +18,8 @@\n from torch_tensorrt.dynamo.runtime import PythonTorchTensorRTModule, TorchTensorRTModule\n from torch_tensorrt.dynamo.utils import get_torch_inputs\n \n+import tensorrt as trt\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -44,7 +45,7 @@ def infer_module_output_dtypes(\n         if not isinstance(output, torch.Tensor):\n             if isinstance(output, str):\n                 raise ValueError(\n-                    f\"Receieved an output type {type(output)} that's not in the acceptable datatypes (https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\"\n+                    f\"Received an output type {type(output)} that's not in the acceptable datatypes (https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\"\n                 )\n             else:\n                 output_ = torch.tensor(output)\ndiff --git a/py/torch_tensorrt/dynamo/conversion/converter_utils.py b/py/torch_tensorrt/dynamo/conversion/converter_utils.py\nindex 4bff27fd26..f847091800 100644\n--- a/py/torch_tensorrt/dynamo/conversion/converter_utils.py\n+++ b/py/torch_tensorrt/dynamo/conversion/converter_utils.py\n@@ -5,7 +5,6 @@\n from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union, overload\n \n import numpy as np\n-import tensorrt as trt\n import torch\n import torch_tensorrt.dynamo.conversion.impl as impl\n from torch.fx.node import Argument, Target\n@@ -18,6 +17,8 @@\n     DynamoConverterImplSignature,\n )\n \n+import tensorrt as trt\n+\n from ..types import Shape, TRTDataType, TRTLayer, TRTTensor\n \n _LOGGER: logging.Logger = logging.getLogger(__name__)\n@@ -70,7 +71,7 @@ def format_tensor_metadata(metadata: Union[Any, Sequence[Any]]) -> str:\n             return formatted_str[:-2] + \")\"\n         else:\n             _LOGGER.warning(\n-                f\"Detected unparseable type in node formatting: {type(metadata)}\"\n+                f\"Detected unparsable type in node formatting: {type(metadata)}\"\n             )\n             return \"\"\n \n@@ -182,7 +183,7 @@ def cast_int_int_div_trt_tensor(\n         rhs_val (TRTTensor): A TRT Tensor numerator\n         name (str): Name of calling layer\n     Returns:\n-        A list of lhs_val and rhs_val casted to the approriate datatype\n+        A list of lhs_val and rhs_val casted to the appropriate datatype\n     \"\"\"\n     if lhs_val.dtype == trt.int32 and rhs_val.dtype == trt.int32:\n         lhs_val = cast_trt_tensor(ctx, lhs_val, trt.float32, name)\n@@ -344,9 +345,7 @@ def create_constant(\n     # Rank 0 constant is required in IFillLayer inputs.\n     if min_rank == 0:\n         shape = trt.Dims()\n-    numpy_value = to_numpy(\n-        value, _enums.dtype._from(dtype).to(np.dtype) if dtype is not None else None\n-    )\n+    numpy_value = to_numpy(value, dtype)\n     constant = ctx.net.add_constant(\n         shape if isinstance(value, (int, float, bool)) else value.shape,\n         numpy_value.copy() if isinstance(numpy_value, np.ndarray) else numpy_value,\n@@ -576,7 +575,7 @@ def to_numpy(\n         return (\n             output\n             if (dtype is None or output is None)\n-            else output.astype(_enums.dtype._from(dtype).to(np.dtype))\n+            else output.astype(_enums.dtype._from(dtype).to(np.dtype, use_default=True))\n         )\n     else:\n         raise AssertionError(\n@@ -637,7 +636,7 @@ def append(\n         name (str): Name of the calling layer\n         original_tensor (TRTTensor): A TRTTensor to append the new value to\n         new_value (Union[TRTTensor, int, float, torch.Tensor, np.ndarray]): A new value to append\n-        dim (int, optional): Dimention to append the new value. Defaults to 0.\n+        dim (int, optional): Dimension to append the new value. Defaults to 0.\n \n     Returns:\n         TRTTensor: A new TRTTensor that is the result of appending the new value to the original tensor\ndiff --git a/py/torch_tensorrt/dynamo/conversion/impl/shape.py b/py/torch_tensorrt/dynamo/conversion/impl/shape.py\nindex 8c5ee6a26a..c2dfac802b 100644\n--- a/py/torch_tensorrt/dynamo/conversion/impl/shape.py\n+++ b/py/torch_tensorrt/dynamo/conversion/impl/shape.py\n@@ -3,7 +3,6 @@\n from typing import List, Optional, Tuple\n \n import numpy as np\n-import tensorrt as trt\n import torch\n from torch.fx.node import Target\n from torch_tensorrt.dynamo._SourceIR import SourceIR\n@@ -23,6 +22,8 @@\n )\n from torch_tensorrt.fx.types import TRTTensor\n \n+import tensorrt as trt\n+\n \n def shape(\n     ctx: ConversionContext,\n@@ -73,7 +74,7 @@ def get_shape_with_dynamic_shape(\n     reduce operation output shape. Steps of calculations are:\n         1. get the actual tensor shape of input_val via add_shape layer;\n         2. create a all 0 tensor [0, 0, 0];\n-        3. run elementwise comparision the [0, 0, 0] and [-1, 128, 256] tensor, get a condition tensor [True, False, False];\n+        3. run elementwise comparison the [0, 0, 0] and [-1, 128, 256] tensor, get a condition tensor [True, False, False];\n         4. use the condition tensor [True, False, False] to do selection between [2048, 256, 512] and [-1, 128, 256], replace\n            all -1 dynamic shape dimensions with actual batch_size value;\n         5. output shape with actual batch_size as [2048, 128, 256]\ndiff --git a/py/torch_tensorrt/dynamo/conversion/impl/shuffle.py b/py/torch_tensorrt/dynamo/conversion/impl/shuffle.py\nindex 45927e7709..86030ea4df 100644\n--- a/py/torch_tensorrt/dynamo/conversion/impl/shuffle.py\n+++ b/py/torch_tensorrt/dynamo/conversion/impl/shuffle.py\n@@ -1,7 +1,6 @@\n from typing import Optional, Sequence, Union\n \n import numpy as np\n-import tensorrt as trt\n import torch_tensorrt.dynamo.conversion.impl as impl\n from torch.fx.node import Target\n from torch_tensorrt import _enums\n@@ -13,9 +12,10 @@\n     get_trt_tensor,\n     set_layer_name,\n )\n-from torch_tensorrt.dynamo.utils import Frameworks, unified_dtype_converter\n from torch_tensorrt.fx.types import TRTTensor\n \n+import tensorrt as trt\n+\n \n def reshape(\n     ctx: ConversionContext,\n@@ -145,7 +145,7 @@ def resize(\n     input: TRTTensor,\n     sizes: Sequence[int],\n ) -> TRTTensor:\n-    input_np_dtype = unified_dtype_converter(input.dtype, Frameworks.NUMPY)\n+    input_np_dtype = _enums.dtype._from(input.dtype).to(np.dtype)\n     input_val = get_trt_tensor(ctx, input, f\"{name}_input\")\n \n     # Calculate the total number of elements for new and current shape\ndiff --git a/py/torch_tensorrt/dynamo/conversion/truncate_double.py b/py/torch_tensorrt/dynamo/conversion/truncate_double.py\nindex e05cd30961..b14ee95dab 100644\n--- a/py/torch_tensorrt/dynamo/conversion/truncate_double.py\n+++ b/py/torch_tensorrt/dynamo/conversion/truncate_double.py\n@@ -118,7 +118,7 @@ def _repair_64bit_input(\n     # Only enter this code block if there exists a 64-bit output\n     # This implies a cast is needed, since TRT cannot output 64-bit tensors\n     if output_positions_64bit:\n-        # Determine whther the outputs of the module are tuple-type or not\n+        # Determine whether the outputs of the module are tuple-type or not\n         is_collection_output = False\n         if isinstance(submodule_outputs, tuple):\n             is_collection_output = True\ndiff --git a/py/torch_tensorrt/dynamo/lowering/passes/remove_input_alias_fixing_clones.py b/py/torch_tensorrt/dynamo/lowering/passes/remove_input_alias_fixing_clones.py\nindex 7630f3c1a5..87bd518555 100644\n--- a/py/torch_tensorrt/dynamo/lowering/passes/remove_input_alias_fixing_clones.py\n+++ b/py/torch_tensorrt/dynamo/lowering/passes/remove_input_alias_fixing_clones.py\n@@ -30,7 +30,7 @@ def remove_input_alias_fixing_clones(\n         ):\n             modified_graph = True\n \n-            # Replace all uses of the clone with the placholder, delete the clone\n+            # Replace all uses of the clone with the placeholder, delete the clone\n             clone_node = list(node.users)[0]\n             logger.debug(\n                 f\"Removing node {clone_node} from graph, since it is a clone node which \"\ndiff --git a/py/torch_tensorrt/dynamo/lowering/passes/replace_max_pool_with_indices.py b/py/torch_tensorrt/dynamo/lowering/passes/replace_max_pool_with_indices.py\nindex 29d9dcd3cc..6e3762e73c 100644\n--- a/py/torch_tensorrt/dynamo/lowering/passes/replace_max_pool_with_indices.py\n+++ b/py/torch_tensorrt/dynamo/lowering/passes/replace_max_pool_with_indices.py\n@@ -34,7 +34,7 @@ def replace_max_pool_with_indices(\n         ):\n             modified_graph = True\n \n-            # Replace all uses of the clone with the placholder, delete the clone\n+            # Replace all uses of the clone with the placeholder, delete the clone\n             getitem_node = list(node.users)[0]\n \n             with gm.graph.inserting_after(getitem_node):\ndiff --git a/py/torch_tensorrt/dynamo/partitioning/_adjacency_partitioner.py b/py/torch_tensorrt/dynamo/partitioning/_adjacency_partitioner.py\nindex c00d92577c..0e9077cdcb 100644\n--- a/py/torch_tensorrt/dynamo/partitioning/_adjacency_partitioner.py\n+++ b/py/torch_tensorrt/dynamo/partitioning/_adjacency_partitioner.py\n@@ -95,7 +95,7 @@ class TRTPartitioner(_SplitterBase):  # type: ignore\n     Args:\n         module: FX GraphModule to partition\n         operator_support: OperatorSupport class describing allowed operators\n-        allowed_single_node_partition_ops: Nodes which can be included in single-node partitons.\n+        allowed_single_node_partition_ops: Nodes which can be included in single-node partitions.\n             Generally useful for module-level exclusion ops which are intensive despite being single functions\n         min_block_size: Minimum number of computational operators per block\n         require_full_compilation: Require that all computational operators be run in TRT\ndiff --git a/py/torch_tensorrt/dynamo/partitioning/_global_partitioner.py b/py/torch_tensorrt/dynamo/partitioning/_global_partitioner.py\nindex 5982cc95ba..823a43beb8 100644\n--- a/py/torch_tensorrt/dynamo/partitioning/_global_partitioner.py\n+++ b/py/torch_tensorrt/dynamo/partitioning/_global_partitioner.py\n@@ -26,7 +26,7 @@ class TRTPartitioner(CapabilityBasedPartitioner):  # type: ignore[misc]\n         graph_module: FX GraphModule to partition\n         operator_support: OperatorSupport class describing allowed operators\n         non_compute_ops: Operators which are not considered computational (e.g. getattr)\n-        allowed_single_node_partition_ops: Nodes which can be included in single-node partitons.\n+        allowed_single_node_partition_ops: Nodes which can be included in single-node partitions.\n             Generally useful for module-level exclusion ops which are intensive despite being single functions\n         min_block_size: Minimum number of computational operators per block\n         require_full_compilation: Require that all computational operators be run in TRT\ndiff --git a/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py b/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py\nindex b5365bf208..78395b8943 100644\n--- a/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py\n+++ b/py/torch_tensorrt/dynamo/runtime/_PythonTorchTensorRTModule.py\n@@ -4,7 +4,6 @@\n from contextlib import nullcontext\n from typing import Any, Dict, List, Optional, Tuple\n \n-import tensorrt as trt\n import torch\n import torch_tensorrt\n from torch.nn import Module\n@@ -19,13 +18,15 @@\n from torch_tensorrt.dynamo.utils import DYNAMIC_DIM\n from torch_tensorrt.logging import TRT_LOGGER\n \n+import tensorrt as trt\n+\n logger = logging.getLogger(__name__)\n \n \n class PythonTorchTensorRTModule(Module):  # type: ignore[misc]\n     \"\"\"PythonTorchTensorRTModule is a PyTorch module which encompasses an arbitrary TensorRT Engine.\n \n-    This module is backed by the Torch-TensorRT runtime and is only compatibile with\n+    This module is backed by the Torch-TensorRT runtime and is only compatible with\n     FX / Dynamo / Python deployments. This module cannot be serialized to torchscript via torch.jit.trace for C++ deployment.\n     \"\"\"\n \ndiff --git a/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py b/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py\nindex 1449d4ae36..601147279a 100644\n--- a/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py\n+++ b/py/torch_tensorrt/dynamo/runtime/_TorchTensorRTModule.py\n@@ -33,7 +33,7 @@\n class TorchTensorRTModule(torch.nn.Module):  # type: ignore[misc]\n     \"\"\"TorchTensorRTModule is a PyTorch module which encompasses an arbitrary TensorRT Engine.\n \n-    This module is backed by the Torch-TensorRT runtime and is fully compatibile with both\n+    This module is backed by the Torch-TensorRT runtime and is fully compatible with both\n     FX / Python deployments (just ``import torch_tensorrt`` as part of the application) as\n     well as TorchScript / C++ deployments since TorchTensorRTModule can be passed to ``torch.jit.trace``\n     and then saved.\n@@ -41,11 +41,11 @@ class TorchTensorRTModule(torch.nn.Module):  # type: ignore[misc]\n     The forward function is simpily forward(*args: torch.Tensor) -> Tuple[torch.Tensor] where\n     the internal implementation is ``return Tuple(torch.ops.tensorrt.execute_engine(list(inputs), self.engine))``\n \n-    > Note: TorchTensorRTModule only supports engines built with explict batch\n+    > Note: TorchTensorRTModule only supports engines built with explicit batch\n \n     Attributes:\n         name (str): Name of module (for easier debugging)\n-        engine (torch.classess.tensorrt.Engine): Torch-TensorRT TensorRT Engine instance, manages [de]serialization, device configuration, profiling\n+        engine (torch.classes.tensorrt.Engine): Torch-TensorRT TensorRT Engine instance, manages [de]serialization, device configuration, profiling\n         input_binding_names (List[str]): List of input TensorRT engine binding names in the order they would be passed to the TRT modules\n         output_binding_names (List[str]): List of output TensorRT engine binding names in the order they should be returned\n     \"\"\"\n@@ -58,9 +58,7 @@ def __init__(\n         output_binding_names: Optional[List[str]] = None,\n         settings: CompilationSettings = CompilationSettings(),\n     ):\n-        \"\"\"__init__ method for torch_tensorrt.dynamo.runtime._TorchTensorRTModule.TorchTensorRTModule\n-\n-        Takes a name, target device, serialized TensorRT engine, and binding names / order and constructs\n+        \"\"\"Takes a name, target device, serialized TensorRT engine, and binding names / order and constructs\n         a PyTorch ``torch.nn.Module`` around it.\n \n         If binding names are not provided, it is assumed that the engine binding names follow the following convention:\n@@ -68,16 +66,17 @@ def __init__(\n             - [symbol].[index in input / output array]\n                 - ex. [x.0, x.1, x.2] -> [y.0]\n \n-        Args:\n+        Arguments:\n             name (str): Name for module\n             serialized_engine (bytearray): Serialized TensorRT engine in the form of a bytearray\n             input_binding_names (List[str]): List of input TensorRT engine binding names in the order they would be passed to the TRT modules\n             output_binding_names (List[str]): List of output TensorRT engine binding names in the order they should be returned\n-            target_device: (torch_tensorrt.Device): Device to instantiate TensorRT engine on. Must be a compatible device i.e. same GPU model / compute capability as was used to build the engine\n+            target_device (torch_tensorrt.Device): Device to instantiate TensorRT engine on. Must be a compatible device i.e. same GPU model / compute capability as was used to build the engine\n+            hardware_compatible (bool): If the engine has be built with the hardware compatibility feature enabled\n \n         Example:\n \n-            ..code-block:: py\n+            .. code-block:: py\n \n                 with io.BytesIO() as engine_bytes:\n                     engine_bytes.write(trt_engine.serialize())\n@@ -186,7 +185,7 @@ def forward(self, *inputs: Any) -> torch.Tensor | Tuple[torch.Tensor, ...]:\n             torch.Tensor or Tuple(torch.Tensor): Result of the engine computation\n         \"\"\"\n         if self.engine is None:\n-            raise RuntimeError(\"Engine has not been initalized yet.\")\n+            raise RuntimeError(\"Engine has not been initialized yet.\")\n \n         assert len(inputs) == len(\n             self.input_binding_names\n@@ -220,7 +219,7 @@ def enable_profiling(self, profiling_results_dir: Optional[str] = None) -> None:\n             profiling_results_dir (str): Absolute path to the directory to sort results of profiling.\n         \"\"\"\n         if self.engine is None:\n-            raise RuntimeError(\"Engine has not been initalized yet.\")\n+            raise RuntimeError(\"Engine has not been initialized yet.\")\n \n         if profiling_results_dir is not None:\n             self.engine.profile_path_prefix = profiling_results_dir\n@@ -229,7 +228,7 @@ def enable_profiling(self, profiling_results_dir: Optional[str] = None) -> None:\n     def disable_profiling(self) -> None:\n         \"\"\"Disable the profiler\"\"\"\n         if self.engine is None:\n-            raise RuntimeError(\"Engine has not been initalized yet.\")\n+            raise RuntimeError(\"Engine has not been initialized yet.\")\n \n         self.engine.disable_profiling()\n \n@@ -241,7 +240,7 @@ def get_layer_info(self) -> str:\n             str: A JSON string which contains the layer information of the engine incapsulated in this module\n         \"\"\"\n         if self.engine is None:\n-            raise RuntimeError(\"Engine has not been initalized yet.\")\n+            raise RuntimeError(\"Engine has not been initialized yet.\")\n \n         layer_info: str = self.engine.get_engine_layer_info()\n         return layer_info\n@@ -249,7 +248,7 @@ def get_layer_info(self) -> str:\n     def dump_layer_info(self) -> None:\n         \"\"\"Dump layer information encoded by the TensorRT engine in this module to STDOUT\"\"\"\n         if self.engine is None:\n-            raise RuntimeError(\"Engine has not been initalized yet.\")\n+            raise RuntimeError(\"Engine has not been initialized yet.\")\n \n         self.engine.dump_engine_layer_info()\n \ndiff --git a/py/torch_tensorrt/dynamo/utils.py b/py/torch_tensorrt/dynamo/utils.py\nindex 7a65e27bfb..acfb2b0094 100644\n--- a/py/torch_tensorrt/dynamo/utils.py\n+++ b/py/torch_tensorrt/dynamo/utils.py\n@@ -6,7 +6,6 @@\n from typing import Any, Callable, Dict, Optional, Sequence, Union\n \n import numpy as np\n-import tensorrt as trt\n import torch\n from torch_tensorrt._Device import Device\n from torch_tensorrt._enums import dtype\n@@ -14,6 +13,7 @@\n from torch_tensorrt.dynamo import _defaults\n from torch_tensorrt.dynamo._settings import CompilationSettings\n \n+import tensorrt as trt\n from packaging import version\n \n from .types import TRTDataType\n@@ -387,34 +387,3 @@ def check_output(\n                 return False\n \n     return True\n-\n-\n-def unified_dtype_converter(\n-    dtype: Union[TRTDataType, torch.dtype, np.dtype], to: Frameworks\n-) -> Union[np.dtype, torch.dtype, TRTDataType]:\n-    \"\"\"\n-    Convert TensorRT, Numpy, or Torch data types to any other of those data types.\n-\n-    Args:\n-        dtype (TRTDataType, torch.dtype, np.dtype): A TensorRT, Numpy, or Torch data type.\n-        to (Frameworks): The framework to convert the data type to.\n-\n-    Returns:\n-        The equivalent data type in the requested framework.\n-    \"\"\"\n-    assert to in Frameworks, f\"Expected valid Framework for translation, got {to}\"\n-    trt_major_version = int(trt.__version__.split(\".\")[0])\n-    if dtype in (np.int8, torch.int8, trt.int8):\n-        return DataTypeEquivalence[trt.int8][to]\n-    elif trt_major_version >= 7 and dtype in (np.bool_, torch.bool, trt.bool):\n-        return DataTypeEquivalence[trt.bool][to]\n-    elif dtype in (np.int32, torch.int32, trt.int32):\n-        return DataTypeEquivalence[trt.int32][to]\n-    elif dtype in (np.int64, torch.int64, trt.int64):\n-        return DataTypeEquivalence[trt.int64][to]\n-    elif dtype in (np.float16, torch.float16, trt.float16):\n-        return DataTypeEquivalence[trt.float16][to]\n-    elif dtype in (np.float32, torch.float32, trt.float32):\n-        return DataTypeEquivalence[trt.float32][to]\n-    else:\n-        raise TypeError(\"%s is not a supported dtype\" % dtype)\ndiff --git a/py/torch_tensorrt/logging.py b/py/torch_tensorrt/logging.py\nindex e75998b870..8447169cc2 100644\n--- a/py/torch_tensorrt/logging.py\n+++ b/py/torch_tensorrt/logging.py\n@@ -36,10 +36,12 @@ def log(self, severity: trt.ILogger.Severity, msg: str) -> None:\n class internal_errors:\n     \"\"\"Context-manager to limit displayed log messages to just internal errors\n \n-    Example::\n+    Example:\n \n-    with torch_tensorrt.logging.internal_errors():\n-        outputs = model_torchtrt(inputs)\n+        .. code-block:: py\n+\n+            with torch_tensorrt.logging.internal_errors():\n+                outputs = model_torchtrt(inputs)\n     \"\"\"\n \n     def __enter__(self) -> None:\n@@ -73,10 +75,12 @@ def __exit__(self, exc_type: Any, exc_value: Any, exc_tb: Any) -> None:\n class errors:\n     \"\"\"Context-manager to limit displayed log messages to just errors and above\n \n-    Example::\n+    Example:\n+\n+        .. code-block:: py\n \n-    with torch_tensorrt.logging.errors():\n-        outputs = model_torchtrt(inputs)\n+            with torch_tensorrt.logging.errors():\n+                outputs = model_torchtrt(inputs)\n     \"\"\"\n \n     def __enter__(self) -> None:\n@@ -108,10 +112,12 @@ def __exit__(self, exc_type: Any, exc_value: Any, exc_tb: Any) -> None:\n class warnings:\n     \"\"\"Context-manager to limit displayed log messages to just warnings and above\n \n-    Example::\n+    Example:\n+\n+        .. code-block:: py\n \n-    with torch_tensorrt.logging.warnings():\n-        model_trt = torch_tensorrt.compile(model, **spec)\n+            with torch_tensorrt.logging.warnings():\n+                model_trt = torch_tensorrt.compile(model, **spec)\n     \"\"\"\n \n     def __enter__(self) -> None:\n@@ -143,10 +149,12 @@ def __exit__(self, exc_type: Any, exc_value: Any, exc_tb: Any) -> None:\n class info:\n     \"\"\"Context-manager to display all info and greater severity messages\n \n-    Example::\n+    Example:\n \n-    with torch_tensorrt.logging.info():\n-        model_trt = torch_tensorrt.compile(model, **spec)\n+        .. code-block:: py\n+\n+            with torch_tensorrt.logging.info():\n+                model_trt = torch_tensorrt.compile(model, **spec)\n     \"\"\"\n \n     def __enter__(self) -> None:\n@@ -178,10 +186,12 @@ def __exit__(self, exc_type: Any, exc_value: Any, exc_tb: Any) -> None:\n class debug:\n     \"\"\"Context-manager to display full debug information through the logger\n \n-    Example::\n+    Example:\n+\n+        .. code-block:: py\n \n-    with torch_tensorrt.logging.debug():\n-        model_trt = torch_tensorrt.compile(model, **spec)\n+            with torch_tensorrt.logging.debug():\n+                model_trt = torch_tensorrt.compile(model, **spec)\n     \"\"\"\n \n     def __enter__(self) -> None:\n@@ -214,10 +224,12 @@ class graphs:\n     \"\"\"Context-manager to display the results of intermediate lowering passes\n     as well as full debug information through the logger\n \n-    Example::\n+    Example:\n+\n+        .. code-block:: py\n \n-    with torch_tensorrt.logging.graphs():\n-        model_trt = torch_tensorrt.compile(model, **spec)\n+            with torch_tensorrt.logging.graphs():\n+                model_trt = torch_tensorrt.compile(model, **spec)\n     \"\"\"\n \n     def __enter__(self) -> None:\ndiff --git a/py/torch_tensorrt/runtime/__init__.py b/py/torch_tensorrt/runtime/__init__.py\nindex 29895c83d5..d202c897f6 100644\n--- a/py/torch_tensorrt/runtime/__init__.py\n+++ b/py/torch_tensorrt/runtime/__init__.py\n@@ -1,1 +1,6 @@\n+from torch_tensorrt.dynamo.runtime import (  # noqa: F401\n+    PythonTorchTensorRTModule,\n+    TorchTensorRTModule,\n+)\n+\n from .multi_device_safe_mode import set_multi_device_safe_mode\ndiff --git a/py/torch_tensorrt/runtime/multi_device_safe_mode.py b/py/torch_tensorrt/runtime/multi_device_safe_mode.py\nindex 0ddd900ab6..547868edf6 100644\n--- a/py/torch_tensorrt/runtime/multi_device_safe_mode.py\n+++ b/py/torch_tensorrt/runtime/multi_device_safe_mode.py\n@@ -36,6 +36,24 @@ def __exit__(self, *args: Any) -> None:\n \n \n def set_multi_device_safe_mode(mode: bool) -> _MultiDeviceSafeModeContextManager:\n+    \"\"\"Sets the runtime (Python-only and default) into multi-device safe mode\n+\n+    In the case that multiple devices are available on the system, in order for the\n+    runtime to execute safely, additional device checks are necessary. These checks\n+    can have a performance impact so they are therefore opt-in. Used to suppress\n+    the warning about running unsafely in a multi-device context.\n+\n+    Arguments:\n+        mode (bool): Enable (``True``) or disable (``False``) multi-device checks\n+\n+    Example:\n+\n+        .. code-block:: py\n+\n+            with torch_tensorrt.runtime.set_multi_device_safe_mode(True):\n+                results = trt_compiled_module(*inputs)\n+\n+    \"\"\"\n     # Fetch existing safe mode and set new mode for Python\n     global _PY_RT_MULTI_DEVICE_SAFE_MODE\n     old_mode = _PY_RT_MULTI_DEVICE_SAFE_MODE\ndiff --git a/py/torch_tensorrt/ts/_Device.py b/py/torch_tensorrt/ts/_Device.py\nindex 3ae10a9c4d..6e6b39dc2d 100644\n--- a/py/torch_tensorrt/ts/_Device.py\n+++ b/py/torch_tensorrt/ts/_Device.py\n@@ -38,7 +38,7 @@ def __init__(self, *args: Any, **kwargs: Any):\n             spec (str): String with device spec e.g. \"dla:0\" for dla, core_id 0\n \n         Keyword Arguments:\n-            gpu_id (int): ID of target GPU (will get overrided if dla_core is specified to the GPU managing DLA). If specified, no positional arguments should be provided\n+            gpu_id (int): ID of target GPU (will get overridden if dla_core is specified to the GPU managing DLA). If specified, no positional arguments should be provided\n             dla_core (int): ID of target DLA core. If specified, no positional arguments should be provided.\n             allow_gpu_fallback (bool): Allow TensorRT to schedule operations on GPU if they are not supported on DLA (ignored if device type is not DLA)\n \ndiff --git a/py/torch_tensorrt/ts/_Input.py b/py/torch_tensorrt/ts/_Input.py\nindex 6099efbcd2..348cdcf56d 100644\n--- a/py/torch_tensorrt/ts/_Input.py\n+++ b/py/torch_tensorrt/ts/_Input.py\n@@ -33,11 +33,11 @@ def __init__(self, *args: Any, **kwargs: Any) -> None:\n         Keyword Arguments:\n             shape (Tuple or List, optional): Static shape of input tensor\n             min_shape (Tuple or List, optional): Min size of input tensor's shape range\n-                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implictly this sets Input's shape_mode to DYNAMIC\n+                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implicitly this sets Input's shape_mode to DYNAMIC\n             opt_shape (Tuple or List, optional): Opt size of input tensor's shape range\n-                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implictly this sets Input's shape_mode to DYNAMIC\n+                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implicitly this sets Input's shape_mode to DYNAMIC\n             max_shape (Tuple or List, optional): Max size of input tensor's shape range\n-                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implictly this sets Input's shape_mode to DYNAMIC\n+                Note: All three of min_shape, opt_shape, max_shape must be provided, there must be no positional arguments, shape must not be defined and implicitly this sets Input's shape_mode to DYNAMIC\n             dtype (torch.dtype or torch_tensorrt.dtype): Expected data type for input tensor (default: torch_tensorrt.dtype.float32)\n             format (torch.memory_format or torch_tensorrt.TensorFormat): The expected format of the input tensor (default: torch_tensorrt.TensorFormat.NCHW)\n             tensor_domain (Tuple(float, float), optional): The domain of allowed values for the tensor, as interval notation: [tensor_domain[0], tensor_domain[1]).\ndiff --git a/py/torch_tensorrt/ts/_compile_spec.py b/py/torch_tensorrt/ts/_compile_spec.py\nindex 1574de02f3..4843ec0145 100644\n--- a/py/torch_tensorrt/ts/_compile_spec.py\n+++ b/py/torch_tensorrt/ts/_compile_spec.py\n@@ -232,7 +232,7 @@ def _parse_compile_spec(compile_spec_: Dict[str, Any]) -> _ts_C.CompileSpec:\n \n     else:\n         raise KeyError(\n-            'Module input definitions are requried to compile module. Provide a list of torch_tensorrt.Input keyed to \"inputs\" in the compile spec'\n+            'Module input definitions are required to compile module. Provide a list of torch_tensorrt.Input keyed to \"inputs\" in the compile spec'\n         )\n \n     if \"enabled_precisions\" in compile_spec:\n@@ -323,7 +323,7 @@ def TensorRTCompileSpec(\n     calibrator: object = None,\n     allow_shape_tensors: bool = False,\n ) -> torch.classes.tensorrt.CompileSpec:\n-    \"\"\"Utility to create a formated spec dictionary for using the PyTorch TensorRT backend\n+    \"\"\"Utility to create a formatted spec dictionary for using the PyTorch TensorRT backend\n \n     Keyword Args:\n         inputs (List[Union(torch_tensorrt.Input, torch.Tensor)]): **Required** List of specifications of input shape, dtype and memory layout for inputs to the module. This argument is required. Input Sizes can be specified as torch sizes, tuples or lists. dtypes can be specified using\n@@ -359,7 +359,7 @@ def TensorRTCompileSpec(\n         allow_shape_tensors: (Experimental) Allow aten::size to output shape tensors using IShapeLayer in TensorRT\n \n       Returns:\n-        torch.classes.tensorrt.CompileSpec: List of methods and formated spec objects to be provided to ``torch._C._jit_to_tensorrt``\n+        torch.classes.tensorrt.CompileSpec: List of methods and formatted spec objects to be provided to ``torch._C._jit_to_tensorrt``\n     \"\"\"\n \n     compile_spec = {\ndiff --git a/py/torch_tensorrt/ts/_compiler.py b/py/torch_tensorrt/ts/_compiler.py\nindex 3be9b7a4c2..675c245ac8 100644\n--- a/py/torch_tensorrt/ts/_compiler.py\n+++ b/py/torch_tensorrt/ts/_compiler.py\n@@ -9,8 +9,6 @@\n from torch_tensorrt._Input import Input\n from torch_tensorrt.ts._compile_spec import _parse_compile_spec, _parse_device\n \n-from torch_tensorrt import _enums\n-\n \n def compile(\n     module: torch.jit.ScriptModule,\n@@ -95,7 +93,7 @@ def compile(\n         truncate_long_and_double (bool): Truncate weights provided in int64 or double (float64) to int32 and float32\n         calibrator (Union(torch_tensorrt._C.IInt8Calibrator, tensorrt.IInt8Calibrator)): Calibrator object which will provide data to the PTQ system for INT8 Calibration\n         require_full_compilation (bool): Require modules to be compiled end to end or return an error as opposed to returning a hybrid graph where operations that cannot be run in TensorRT are run in PyTorch\n-        min_block_size (int): The minimum number of contiguous TensorRT convertable operations in order to run a set of operations in TensorRT\n+        min_block_size (int): The minimum number of contiguous TensorRT convertible operations in order to run a set of operations in TensorRT\n         torch_executed_ops (List[str]): List of aten operators that must be run in PyTorch. An error will be thrown if this list is not empty but ``require_full_compilation`` is True\n         torch_executed_modules (List[str]): List of modules that must be run in PyTorch. An error will be thrown if this list is not empty but ``require_full_compilation`` is True\n         allow_shape_tensors: (Experimental) Allow aten::size to output shape tensors using IShapeLayer in TensorRT\ndiff --git a/py/torch_tensorrt/ts/ptq.py b/py/torch_tensorrt/ts/ptq.py\nindex d129ea2824..6545de9674 100644\n--- a/py/torch_tensorrt/ts/ptq.py\n+++ b/py/torch_tensorrt/ts/ptq.py\n@@ -73,14 +73,15 @@ def __reduce__(self: object) -> str:\n \n class DataLoaderCalibrator(object):\n     \"\"\"\n-    Constructs a calibrator class in TensorRT and uses pytorch dataloader to load/preproces\n+    Constructs a calibrator class in TensorRT and uses pytorch dataloader to load/preprocess\n     data which is passed during calibration.\n-    Args:\n-        dataloader: an instance of pytorch dataloader which iterates through a given dataset.\n-        algo_type: choice of calibration algorithm.\n-        cache_file: path to cache file.\n-        use_cache: flag which enables usage of pre-existing cache.\n-        device: device on which calibration data is copied to.\n+\n+    Arguments:\n+        dataloader (torch.utils.data.DataLoader): an instance of pytorch dataloader which iterates through a given dataset.\n+        algo_type (CalibrationAlgo): choice of calibration algorithm.\n+        cache_file (str): path to cache file.\n+        use_cache (bool): flag which enables usage of pre-existing cache.\n+        device (Device): device on which calibration data is copied to.\n     \"\"\"\n \n     def __init__(self, **kwargs: Any):\n@@ -164,9 +165,10 @@ def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n class CacheCalibrator(object):\n     \"\"\"\n     Constructs a calibrator class in TensorRT which directly uses pre-existing cache file for calibration.\n-    Args:\n-        cache_file: path to cache file.\n-        algo_type: choice of calibration algorithm.\n+\n+    Arguments:\n+        cache_file (str): path to cache file.\n+        algo_type (CalibrationAlgo): choice of calibration algorithm.\n     \"\"\"\n \n     def __init__(self, **kwargs: Any):\ndiff --git a/pyproject.toml b/pyproject.toml\nindex f9e1f27690..d44baa3c07 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -37,7 +37,7 @@ classifiers = [\n     \"Topic :: Software Development\",\n     \"Topic :: Software Development :: Libraries\",\n ]\n-readme = {file = \"py/README.md\", content-type = \"text/markdown\"}\n+readme = {file = \"README.md\", content-type = \"text/markdown\"}\n requires-python = \">=3.8\"\n keywords = [\"pytorch\", \"torch\", \"tensorrt\", \"trt\", \"ai\", \"artificial intelligence\", \"ml\", \"machine learning\", \"dl\", \"deep learning\", \"compiler\", \"dynamo\", \"torchscript\", \"inference\"]\n dependencies = [\n@@ -218,3 +218,27 @@ disallow_untyped_calls = false\n module = \"torch_tensorrt.fx.*\"\n ignore_errors = true\n follow_imports = \"skip\"\n+\n+[tool.typos]\n+files.extend-exclude = [\n+    \"docs/**/*\",\n+    \"*/fx/*\",\n+    \"docsrc/_rendered_examples/\",\n+    \"core/*\",\n+    \"!core/runtime/\",\n+    \"third_party/\",\n+    \"CHANGELOG.md\",\n+    \"*.ipynb\",\n+    \"cpp/\",\n+    \"py/torch_tensorrt/fx/\"\n+]\n+\n+[tool.typos.default]\n+extend-ignore-identifiers-re = [\n+    \"^([A-z]|[a-z])*Nd*\",\n+    \"^([A-z]|[a-z])*nd*\",\n+    \"activ*([A-z]|[a-z]|[0-9])*,\"\n+]\n+\n+[tool.typos.default.extend-words]\n+arange = \"arange\"\ndiff --git a/setup.py b/setup.py\nindex c74fb9e130..5e3b8d9dfa 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -140,6 +140,10 @@ def load_dep_info():\n     sys.argv.remove(\"--use-cxx11-abi\")\n     CXX11_ABI = True\n \n+if (cxx11_abi_env_var := os.environ.get(\"USE_CXX11_ABI\")) is not None:\n+    if cxx11_abi_env_var == \"1\":\n+        CXX11_ABI = True\n+\n if platform.uname().processor == \"aarch64\":\n     if \"--jetpack-version\" in sys.argv:\n         version_idx = sys.argv.index(\"--jetpack-version\") + 1\n@@ -617,7 +621,7 @@ def run(self):\n         }\n     )\n \n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n+with open(os.path.join(get_root_dir(), \"README.md\"), \"r\", encoding=\"utf-8\") as fh:\n     long_description = fh.read()\n \n setup(\ndiff --git a/tools/cpp_benchmark/README.md b/tools/cpp_benchmark/README.md\nindex f7c08636ab..3acc3e7441 100644\n--- a/tools/cpp_benchmark/README.md\n+++ b/tools/cpp_benchmark/README.md\n@@ -30,4 +30,4 @@ You can run a module with JIT or TRT via Torch-TensorRT in either FP32 or FP16.\n \n - To also save the TRT engine, add the argument `--cxxopt=\"-DSAVE_ENGINE\"`\n \n-> It's suggested to also define `--cxxopt=\"-DNDEBUG\"` to supress debug information\n+> It's suggested to also define `--cxxopt=\"-DNDEBUG\"` to suppress debug information\ndiff --git a/tools/opset_coverage.ipynb b/tools/opset_coverage.ipynb\nindex 5cbc802138..667cf51dad 100644\n--- a/tools/opset_coverage.ipynb\n+++ b/tools/opset_coverage.ipynb\n@@ -2,9 +2,352 @@\n  \"cells\": [\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": null,\n+   \"execution_count\": 1,\n    \"metadata\": {},\n-   \"outputs\": [],\n+   \"outputs\": [\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::relu(Tensor input) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::relu_(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sigmoid(Tensor input) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sigmoid_(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::tanh(Tensor input) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::tanh_(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::hardtanh(Tensor self, Scalar min_val, Scalar max_val) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::hardtanh_(Tensor self, Scalar min_val, Scalar max_val) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::prelu(Tensor self, Tensor weight) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::leaky_relu(Tensor self, Scalar negative_slope) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::leaky_relu_(Tensor self, Scalar negative_slope) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::elu(Tensor self, Scalar alpha, Scalar scale, Scalar input_scale) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::batch_norm(Tensor input, Tensor? gamma, Tensor? beta, Tensor? mean, Tensor? var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::bitwise_not(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::to(Tensor self, int dtype, bool non_blocking, bool copy, int? memory_format) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::to(Tensor self, Device device, int dtype, bool non_blocking, bool copy, int? memory_format) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::to(Tensor self, Tensor other, bool non_blocking, bool copy, int? memory_format) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::to(Tensor self, Device? device, int? dtype, bool non_blocking, bool copy) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::chunk(Tensor self, int chunks, int dim) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::cat(Tensor[] tensors, int dim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for trt::const(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::constant_pad_nd(Tensor self, int[] pad, Scalar value) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::cumsum(Tensor self, int dim, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::einsum(str equation, Tensor[] tensors, *, int[]? path) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::add(Tensor self, Tensor other, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::add_(Tensor self, Tensor other, *, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::add(Tensor self, Scalar other, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::clamp(Tensor self, Scalar? min, Scalar? max) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::clamp_min(Tensor self, Scalar min) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::clamp_max(Tensor self, Scalar max) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sub(Tensor self, Tensor other, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sub(Tensor self, Scalar other, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sub_(Tensor self, Tensor other, *, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::rsub(Tensor self, Scalar other, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::rsub(Tensor self, Tensor other, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::div(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::div(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::div(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::div_(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::div_(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::square(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::mul(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::mul(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::mul_(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::ne(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::ne(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::pow(Tensor self, Tensor exponent) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::pow(Tensor self, Scalar exponent) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::floor_divide(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::floor_divide(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::max(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::min(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::gt(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::gt(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::lt(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::lt(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::eq(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::eq(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::ge(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::ge(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::le(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::le(Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::logical_and(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::atan2(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::expand(Tensor self, int[] size, *, bool implicit) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::expand_as(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::repeat(Tensor self, int[] repeats) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::repeat_interleave(Tensor self, int repeats, int? dim, *, int? output_size) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::meshgrid(Tensor[] tensors) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for trt::attn_bias_from_attn_mask(Tensor attn_mask) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_nearest1d(Tensor self, int[] output_size, float? scales) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_nearest1d(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_nearest2d(Tensor self, int[] output_size, float? scales_h, float? scales_w) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_nearest2d(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_nearest3d(Tensor self, int[] output_size, float? scales_d, float? scales_h, float? scales_w) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_nearest3d(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_linear1d(Tensor self, int[] output_size, bool align_corners, float? scales) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_linear1d(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_bilinear2d(Tensor self, int[] output_size, bool align_corners, float? scales_h, float? scales_w) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_bilinear2d(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_trilinear3d(Tensor self, int[] output_size, bool align_corners, float? scales_d, float? scales_h, float? scales_w) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::upsample_trilinear3d(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? gamma, Tensor? beta, float eps, bool cudnn_enabled) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::linear(Tensor input, Tensor weight, Tensor? bias) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih, Tensor? b_hh) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih, Tensor? b_hh) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::matmul(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::bmm(Tensor self, Tensor mat2) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta, Scalar alpha) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::max(Tensor self, int dim, bool keepdim) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::min(Tensor self, int dim, bool keepdim) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::argmax(Tensor self, int dim, bool keepdim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::argmin(Tensor self, int dim, bool keepdim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::norm(Tensor self, Scalar? p, int[] dim, bool keepdim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::frobenius_norm(Tensor self, int[] dim, bool keepdim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::linalg_norm(Tensor self, Scalar? ord, int[]? dim, bool keepdim, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::max_pool1d(Tensor self, int[] kernel_size, int[] stride, int[] padding, int[] dilation, bool ceil_mode) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::avg_pool1d(Tensor self, int[] kernel_size, int[] stride, int[] padding, bool ceil_mode, bool count_include_pad) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::max_pool2d(Tensor self, int[] kernel_size, int[] stride, int[] padding, int[] dilation, bool ceil_mode) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::avg_pool2d(Tensor self, int[] kernel_size, int[] stride, int[] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::max_pool3d(Tensor self, int[] kernel_size, int[] stride, int[] padding, int[] dilation, bool ceil_mode) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::avg_pool3d(Tensor self, int[] kernel_size, int[] stride, int[] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::adaptive_avg_pool1d(Tensor self, int[] output_size) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::adaptive_max_pool1d(Tensor self, int[] output_size) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::adaptive_avg_pool2d(Tensor self, int[] output_size) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::adaptive_max_pool2d(Tensor self, int[] output_size) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::adaptive_avg_pool3d(Tensor self, int[] output_size) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::adaptive_max_pool3d(Tensor self, int[] output_size) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::fake_quantize_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::mean(Tensor self, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::mean(Tensor self, int[] dim, bool keepdim, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sum(Tensor self, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sum(Tensor self, int[] dim, bool keepdim, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::prod(Tensor self, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::prod(Tensor self, int dim, bool keepdim, *, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::max(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::min(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::any(Tensor self, int dim, bool keepdim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::all(Tensor self, int dim, bool keepdim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::reflection_pad2d(Tensor self, int[] padding) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::reflection_pad1d(Tensor self, int[] padding) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::replication_pad1d(Tensor self, int[] padding) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::replication_pad2d(Tensor self, int[] padding) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::replication_pad3d(Tensor self, int[] padding) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::select(Tensor self, int dim, int index) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::index_select(Tensor self, int dim, Tensor index) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::narrow(Tensor self, int dim, int start, int length) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::narrow(Tensor self, int dim, Tensor start, int length) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::embedding(Tensor weight, Tensor indices, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::roll(Tensor self, int[] shifts, int[] dims) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::index(Tensor self, Tensor?[] indices) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::slice(Tensor self, int dim, int? start, int? end, int step) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::split(Tensor self, int[] split_sizes, int dim) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::split(Tensor self, int[] split_size, int dim) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::split(Tensor self, int split_size, int dim) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::split_with_sizes(Tensor self, int[] split_sizes, int dim) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::unbind(Tensor self, int dim) -> Tensor[]\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::masked_fill(Tensor self, Tensor mask, Scalar value) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::scatter(Tensor self, int dim, Tensor index, Scalar value) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::scatter(Tensor self, int dim, Tensor index, Tensor src) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::where(Tensor condition, Tensor self, Tensor other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::where(Tensor condition, Tensor self, Scalar other) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::flip(Tensor self, int[] dims) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::flatten(Tensor self, int start_dim, int end_dim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::unflatten(Tensor self, int dim, int[] sizes) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::reshape(Tensor self, int[] shape) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::view(Tensor self, int[] size) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::permute(Tensor self, int[] dims) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::transpose(Tensor self, int dim0, int dim1) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::t(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::softmax(Tensor self, int dim, int? dtype) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::squeeze(Tensor self, int dim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::squeeze(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::stack(Tensor[] tensors, int dim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::topk(Tensor self, int k, int dim, bool largest, bool sorted) -> (Tensor, Tensor)\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::abs(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::reciprocal(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::log2(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::logical_not(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sqrt(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::isfinite(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::cos(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::acos(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::cosh(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sin(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::asin(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sinh(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::tan(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::atan(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::floor(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::log(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::ceil(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::exp(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::neg(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::erf(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::sign(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::asinh(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::acosh(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::atanh(Tensor self) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering converter for aten::unsqueeze(Tensor self, int dim) -> Tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::eq\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::ne\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::lt\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::gt\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::le\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::ge\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::pow\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__and__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__or__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__xor__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__round_to_zero_floordiv\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::zeros\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::ones\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::new_zeros\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::new_ones\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::zeros_like\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::ones_like\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::fill_\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::full\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::full_like\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::slice\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::len\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::size\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__getitem__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::append\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::extend\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::neg\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::add\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::add_\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::mul\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::sub\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::Bool\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::Float\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::Int\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__not__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__is__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__isnot__\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::numel\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::dim\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::div\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::floordiv\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::floor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::sqrt\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::warn\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::is_floating_point\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::tensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::arange\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::clone\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::copy_\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::format\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__range_length\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::__derive_index\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for aten::list\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::Constant\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::NumToTensor\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::ListUnpack\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::ListConstruct\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::dtype\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::min\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::max\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::shape\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::TupleConstruct\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::TupleIndex\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::TupleUnpack\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::unchecked_cast\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::Uninitialized\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Registering evaluator for prim::RaiseException\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT - Debug Build] - Runtime:\\n\",\n+      \" Available CUDA Devices: \\n\",\n+      \"    Device(ID: 0, Name: NVIDIA TITAN V, SM Capability: 7.0, Type: GPU)\\n\",\n+      \"\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CaskDeconvV2RunnerWeightsTransformerPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CaskDeconvV1RunnerWeightsTransformerPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CaskConvolutionRunnerWeightsTransformerPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CaskFlattenConvolutionRunnerWeightsTransformerPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CaskConvActPoolWeightsTransformerPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CaskDepSepConvWeightsTransformerPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - MyelinWeightsTransformPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - DisentangledAttention_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomEmbLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomEmbLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomEmbLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomFCPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomGeluPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - GroupNormalizationPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomSkipLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomSkipLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomSkipLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomSkipLayerNormPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RnRes2Br1Br2c_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RnRes2Br1Br2c_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RnRes2Br2bBr2c_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RnRes2Br2bBr2c_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RnRes2FullFusion_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - SingleStepLSTMPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomQKVToContextPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomQKVToContextPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CustomQKVToContextPluginDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - DLRM_BOTTOM_MLP_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - SmallTileGEMM_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RNNTEncoderPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Interpolate, Namespace: torch_tensorrt\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - NormalizePlugin, Namespace: torch_tensorrt\\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - BatchedNMSDynamic_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - BatchedNMS_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - BatchTilePlugin_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Clip_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CoordConvAC, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CropAndResizeDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - CropAndResize, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - DecodeBbox3DPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - DetectionLayer_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - EfficientNMS_Explicit_TF_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - EfficientNMS_Implicit_TF_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - EfficientNMS_ONNX_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - EfficientNMS_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - FlattenConcat_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - GenerateDetection_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - GridAnchor_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - GridAnchorRect_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - InstanceNormalization_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - InstanceNormalization_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - LReLU_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ModulatedDeformConv2d, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - MultilevelCropAndResize_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - MultilevelProposeROI_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - MultiscaleDeformableAttnPlugin_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - NMSDynamic_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - NMS_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Normalize_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - PillarScatterPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - PriorBox_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ProposalDynamic, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ProposalLayer_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Proposal, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - PyramidROIAlign_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Region_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Reorg_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Reorg_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ResizeNearest_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ROIAlign_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - RPROI_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ScatterElements, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - ScatterND, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - SpecialSlice_TRT, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - Split, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Registered plugin creator - VoxelGeneratorPlugin, Namespace: \\n\",\n+      \"\\u001b[1;35mDEBUG: \\u001b[0m[Torch-TensorRT Plugins Context] - Total number of plugins registered: 76\\n\",\n+      \"WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\\n\"\n+     ]\n+    }\n+   ],\n    \"source\": [\n     \"import torch_tensorrt\\n\",\n     \"from torch_tensorrt.dynamo.tools.opset_coverage import ATEN_COVERAGE, PRIMS_COVERAGE, PY_OVERLOAD_COVERAGE, SupportStatus, OpsetCoverage\"\n@@ -12,9 +355,37 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": null,\n+   \"execution_count\": 2,\n    \"metadata\": {},\n-   \"outputs\": [],\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Unsupported Ops:\\n\",\n+      \"aten.nonzero(Tensor self) -> Tensor\\n\",\n+      \"aten.resize_(Tensor(a!) self, SymInt[] size, \\\\*, MemoryFormat? memory_format=None) -> Tensor(a!)\\n\",\n+      \"aten.empty_strided(SymInt[] size, SymInt[] stride, \\\\*, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\\n\",\n+      \"aten.index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor\\n\",\n+      \"aten.max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)\\n\",\n+      \"aten.gather(Tensor self, int dim, Tensor index, \\\\*, bool sparse_grad=False) -> Tensor\\n\",\n+      \"aten.max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)\\n\",\n+      \"aten.sym_stride.int(Tensor self, int dim) -> SymInt\\n\",\n+      \"aten.scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, \\\\*, bool include_self=True) -> Tensor\\n\",\n+      \"aten.scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor\\n\",\n+      \"aten.empty.memory_format(SymInt[] size, \\\\*, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\\n\",\n+      \"aten.native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)\\n\",\n+      \"aten.sym_storage_offset(Tensor self) -> SymInt\\n\",\n+      \"aten._local_scalar_dense(Tensor self) -> Scalar\\n\",\n+      \"\\n\",\n+      \"Backwards Ops:\\n\",\n+      \"aten.avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor\\n\",\n+      \"aten._adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor\\n\",\n+      \"aten.convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\\n\",\n+      \"aten.max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor\\n\"\n+     ]\n+    }\n+   ],\n    \"source\": [\n     \"unsupported_ops = {}\\n\",\n     \"backwards_ops = {}\\n\",\n@@ -37,9 +408,140 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": null,\n+   \"execution_count\": 3,\n    \"metadata\": {},\n-   \"outputs\": [],\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Unsupported Ops:\\n\",\n+      \"prims.(Tensor a, int[] dims) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor a, SymInt[] stride) -> Tensor\\n\",\n+      \"prims.(Tensor(a!) a, SymInt[] shape) -> Tensor(a!)\\n\",\n+      \"prims.(Tensor[] tensors, int dim) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor(a) self) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, \\\\*, int[] dim, bool forward) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a, ScalarType dtype) -> Tensor(a)\\n\",\n+      \"prims.(Tensor(a) a, int dim, SymInt outer_length) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor a, Device device) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Scalar s, \\\\*, ScalarType? dtype=None, Device? device=None) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(SymInt[] shape, \\\\*, Scalar low, Scalar high, ScalarType dtype, Device device, Generator? generator=None) -> Tensor\\n\",\n+      \"prims.(Tensor A, \\\\*, bool full_matrices) -> (Tensor U, Tensor S, Tensor Vh)\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, \\\\*, int[] dim, SymInt last_dim_size) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor a, ScalarType dtype) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> (Tensor mantissa, Tensor exponent)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, Scalar value) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, \\\\*, int[] dim, bool onesided) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(ScalarType dtype) -> Scalar\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, \\\\*, MemoryFormat? memory_format=None) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(SymInt[] shape, int[] physical_layout, \\\\*, ScalarType dtype, Device device, bool requires_grad) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a, int start, int end) -> Tensor(a)\\n\",\n+      \"prims.(Tensor(a) a, int[] dimensions) -> Tensor(a)\\n\",\n+      \"prims.(ScalarType dtype) -> Scalar\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a, SymInt[] start_indices, SymInt[] limit_indices, SymInt[]? strides=None) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a, int[] permutation) -> Tensor(a)\\n\",\n+      \"prims.(SymInt[] shape, \\\\*, Scalar mean, Scalar std, ScalarType dtype, Device device, bool requires_grad, Generator? generator=None) -> Tensor\\n\",\n+      \"prims.(Tensor a, SymInt[] shape) -> Tensor\\n\",\n+      \"prims.(SymInt length, \\\\*, SymInt start, SymInt step, ScalarType dtype, Device device, bool requires_grad) -> Tensor\\n\",\n+      \"prims.(Tensor(a!) a, Tensor b) -> Tensor(a!)\\n\",\n+      \"prims.() -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor pred, Tensor a, Tensor b) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor inp, int[]? dims, \\\\*, ScalarType? output_dtype=None) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(SymInt[] shape, SymInt[] strides, \\\\*, ScalarType dtype, Device device, bool requires_grad) -> Tensor\\n\",\n+      \"prims.(Tensor a) -> Scalar\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor a, int start, int end) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor[] tokens) -> ()\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor inp, int[]? dims, \\\\*, ScalarType? output_dtype=None) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor(a) self) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor inp, int[]? dims, \\\\*, ScalarType? output_dtype=None) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor(a!) a, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor(a!)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor inp, int[]? dims, \\\\*, ScalarType? output_dtype=None) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a, SymInt start_index, SymInt limit_index, int stride=1, int axis=0) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor(a) a) -> Tensor(a)\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"prims.(Tensor self, Tensor other) -> Tensor\\n\",\n+      \"prims.(Tensor self) -> Tensor\\n\",\n+      \"\\n\",\n+      \"Backwards Ops:\\n\"\n+     ]\n+    }\n+   ],\n    \"source\": [\n     \"unsupported_ops = {}\\n\",\n     \"backwards_ops = {}\\n\",\n@@ -84,7 +586,7 @@\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n-   \"version\": \"3.11.7\"\n+   \"version\": \"3.12.3\"\n   }\n  },\n  \"nbformat\": 4,\n", "instance_id": "pytorch__TensorRT-2974", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear, providing a detailed bug description related to BF16 (bfloat16) data type causing an unsupported numpy dtype error during the compilation process in Torch-TensorRT. It includes a comprehensive log output that shows the error traceback, a reproducible code snippet to demonstrate the issue, and relevant environment details (e.g., versions of Torch-TensorRT, PyTorch, CUDA, and hardware). The goal is evident: to resolve the error encountered when compiling a model with BF16 precision. However, there are minor ambiguities, such as the lack of explicit discussion on expected behavior or desired output beyond \"making the compilation succeed.\" Additionally, while the user suggests a potential fix (adding `use_default=True` to a specific line), there is no detailed explanation of why this works or if there are alternative approaches, leaving some room for interpretation. Edge cases or constraints related to BF16 usage across different hardware or configurations are also not specified.", "difficulty_explanation": "The difficulty of this problem is rated as hard (0.65) due to several factors. First, it requires a deep understanding of the Torch-TensorRT compilation pipeline, specifically how data types like BF16 are handled during conversion to TensorRT engines. The issue involves interaction between PyTorch's Dynamo frontend and TensorRT's backend, necessitating familiarity with both frameworks' internals, including type conversion mechanisms and potential limitations in numpy dtype mappings. The suggested fix (adding `use_default=True` to a type conversion call) indicates a need to modify a specific part of the codebase, but validating this fix or exploring alternatives requires understanding the implications on other data types and ensuring compatibility across different hardware (e.g., RTX 3050 as mentioned). The problem also touches on potential edge cases, such as BF16 support varying by GPU architecture or TensorRT version, which adds complexity to a robust solution. While the code changes provided in the diff are unrelated to the core issue (mostly documentation and typo fixes), the actual fix would likely involve a targeted change in the type conversion logic, as hinted by the user. However, the impact of such a change could be significant, potentially affecting other precision modes or compilation paths, and requires careful testing. This places the difficulty in the hard range, as it demands advanced knowledge of deep learning frameworks and GPU-specific optimizations, though it does not reach the very hard level since the scope appears confined to a specific error in the conversion process rather than a complete architectural overhaul.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "list_functions() returns an empty result with any input dimension filter.\nConsider the following:\r\n\r\n```python\r\n>>> import uqtestfuns as uqtf\r\n>>> uqtf.list_functions()\r\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\r\n|  No.  |          Constructor          |  # Input  |  # Output  |  Param.  |  Application  | Description                    |\r\n+=======+===============================+===========+============+==========+===============+================================+\r\n|   1   |           Ackley()            |     M     |     1      |   True   | optimization, | Optimization test function     |\r\n|       |                               |           |            |          | metamodeling  | from Ackley (1987)             |\r\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\r\n|   2   |        Alemazkoor20D()        |    20     |     1      |  False   | metamodeling  | High-dimensional low-degree    |\r\n|       |                               |           |            |          |               | polynomial from Alemazkoor &   |\r\n|       |                               |           |            |          |               | Meidani (2018)                 |\r\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\r\n...\r\n```\r\n\r\nClearly there is at least one 20 dimensional function.\r\nBut when I type\r\n\r\n```python\r\n>>> uqtf.list_functions(input_dimension=20)\r\n```\r\n\r\nnothing is returned. Something is clearly amiss: input dimension is an int but internally everything is a string (to handle \"m\") so the comparison used in the filtering does not work.\r\n\r\n\r\n\n", "patch": "diff --git a/.github/workflows/main.yml b/.github/workflows/main.yml\nindex 409480f..146c4dc 100644\n--- a/.github/workflows/main.yml\n+++ b/.github/workflows/main.yml\n@@ -42,7 +42,7 @@ jobs:\n         run: tox -e typecheck\n   test:\n     name: Test\n-    runs-on: ubuntu-latest\n+    runs-on: ubuntu-20.04\n     strategy:\n       matrix:\n         python:\n@@ -54,6 +54,10 @@ jobs:\n             toxenv: \"py39\"\n           - version: \"3.11\"\n             toxenv: \"py311\"\n+          - version: \"3.12\"\n+            toxenv: \"py312\"\n+          - version: \"3.13\"\n+            toxenv: \"py313\"\n     steps:\n       - uses: actions/checkout@v3\n       - uses: actions/setup-python@v4.0.0\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a4122ff..776aa49 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -52,6 +52,11 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n   for metamodeling exercise.\n - The 8-dimensional robot arm function for metamodeling exercises.\n \n+## Fixed\n+\n+- The argument `input_dimension` to filter the output of `list_functions()`\n+  is now effective and does not always return zero result.\n+\n ## [0.5.0] - 2024-11-18\n \n ### Added\n", "instance_id": "damar-wicaksono__uqtestfuns-437", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `list_functions()` method in the `uqtestfuns` library fails to return results when filtering by `input_dimension` due to a type mismatch (integer input vs. string internal representation). The goal is evident\u2014fix the filtering logic to ensure correct comparison. The provided example output and user input help illustrate the problem effectively. However, there are minor ambiguities: the statement does not explicitly specify the expected output format or behavior for edge cases (e.g., invalid input dimensions or handling of \"M\" as a variable dimension). Additionally, constraints or requirements for the fix (e.g., maintaining backward compatibility) are not mentioned. Thus, while the core issue is clear, some details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes are in unrelated files (GitHub workflow and changelog), which do not directly address the core issue. However, based on the problem statement, the fix likely involves a single file or function within the `uqtestfuns` library, specifically the `list_functions()` method or its filtering logic. The change is expected to be localized, involving a small amount of code (likely a type conversion or comparison adjustment). There is no indication of architectural impact or cross-module dependencies.\n\n2. **Number of Technical Concepts:** Solving this requires basic knowledge of Python, specifically type handling (converting between strings and integers) and conditional logic for filtering. No advanced language features, libraries, algorithms, or design patterns are needed. The concept of handling a special case like \"M\" (variable dimension) might add minor complexity but remains straightforward.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but a reasonable fix would need to consider scenarios like invalid input dimensions (e.g., negative numbers) or non-numeric internal representations beyond \"M\". However, these are simple to handle with basic validation or conditional checks, and no complex error handling logic is implied.\n\n4. **Overall Complexity:** The issue is a straightforward bug fix involving a type mismatch in a filtering condition. It requires minimal understanding of the broader codebase beyond the specific function and does not involve performance optimization, system design, or domain-specific knowledge.\n\nGiven these factors, I assign a difficulty score of 0.25, as the problem requires understanding some code logic and making a simple modification, fitting within the easy category. It is slightly above the very easy range (0.0-0.2) due to the need to handle the special case of \"M\" and potential minor edge cases, but it remains a relatively simple task for a developer with basic Python skills.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "States checking for model type `sbm_gwf` is missing required river and land state `:h_av`\n### Wflow version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have checked that this bug exists on the latest version of Wflow.\n\n\n### Reproducible Example\n\nWhen running wflow with model type `sbm_gwf` it is skipping the required state `:h_av` for river and land domain. These states are required for the vertical `SBM` concept.\n\n### Current behaviour\n\nSee Reproducible Example.\n\n### Desired behaviour\n\n`:h_av` should be part of the required states for model type `sbm_gwf`.\n\n### Additional Context\n\n_No response_\n", "patch": "diff --git a/docs/src/changelog.md b/docs/src/changelog.md\nindex 399c288c5..c37e167a6 100644\n--- a/docs/src/changelog.md\n+++ b/docs/src/changelog.md\n@@ -35,6 +35,9 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n   zero when `LoopVectorization` is applied (`@tturbo`) to the for loop of these functions.\n   This issue occured on a virtual machine, Windows 10 Enterprise, with Intel(R) Xeon(R) Gold\n   6144 CPU (2 processors). This has been fixed by replacing `@tturbo` with `reduction` of\n+  `Polyester.jl`.\n+- Fixed required states of the model type `sbm_gwf`: added `h_av` for the river and land\n+  domain.\n \n ### Changed\n - Stop exposing scalar variables through BMI. The `BMI.get_value_ptr` function was not\ndiff --git a/src/states.jl b/src/states.jl\nindex 777754c3a..9af01cc0e 100644\n--- a/src/states.jl\n+++ b/src/states.jl\n@@ -122,7 +122,7 @@ function extract_required_states(config::Config)\n         if land_routing == \"local-inertial\"\n             land_states = (:qx, :qy, :h, :h_av)\n         elseif land_routing == \"kinematic-wave\"\n-            if model_type == \"sbm\"\n+            if model_type == \"sbm\" || model_type == \"sbm_gwf\"\n                 land_states = (:q, :h, :h_av)\n             else\n                 land_states = (:q, :h)\n@@ -152,7 +152,7 @@ function extract_required_states(config::Config)\n             :outlagg,\n             :outgrav,\n         )\n-    elseif model_type == \"sbm\"\n+    elseif model_type == \"sbm\" || model_type == \"sbm_gwf\"\n         river_states = (:q, :h, :h_av)\n     else\n         river_states = (:q, :h)\n", "instance_id": "Deltares__Wflow.jl-424", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the model type `sbm_gwf` is missing the required state `:h_av` for river and land domains in the Wflow system. The goal (adding `:h_av` to the required states) and the context (specific model type and domains) are specified. However, there are minor ambiguities and missing details. For instance, the \"Reproducible Example\" section does not provide a concrete code snippet or steps to reproduce the issue, which would have made the problem more actionable. Additionally, there is no mention of potential edge cases, performance implications, or specific constraints related to adding this state. While the desired behavior is stated, the lack of detailed examples or test cases slightly reduces the clarity. Overall, the problem is understandable but could benefit from additional specifics.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is minimal and localized to a single file (`states.jl`) with straightforward modifications\u2014adding conditions to include `:h_av` for the `sbm_gwf` model type in two specific places. The changes involve simple conditional logic and do not impact the broader system architecture or require deep understanding of complex interactions across the codebase. Second, the technical concepts required are basic: familiarity with conditional statements and understanding the domain-specific meaning of model types and states in the Wflow system. No advanced algorithms, design patterns, or language-specific features are needed. Third, there are no explicit edge cases or error handling requirements mentioned in the problem statement, and the code changes do not introduce new error handling logic. Finally, the amount of code change is small, and the impact is limited to updating the state definitions. While some domain knowledge of the Wflow system is necessary, the problem does not pose significant technical challenges, making it relatively easy for a developer with basic familiarity with the codebase to resolve.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update dependency setuptools to v75\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [setuptools](https://redirect.github.com/pypa/setuptools) ([changelog](https://setuptools.pypa.io/en/stable/history.html)) | `~=74.0` -> `~=75.1` | [![age](https://developer.mend.io/api/mc/badges/age/pypi/setuptools/75.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/pypi/setuptools/75.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/pypi/setuptools/74.1.3/75.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/pypi/setuptools/74.1.3/75.1.0?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n### Release Notes\n\n<details>\n<summary>pypa/setuptools (setuptools)</summary>\n\n### [`v75.1.0`](https://redirect.github.com/pypa/setuptools/compare/v75.0.0...v75.1.0)\n\n[Compare Source](https://redirect.github.com/pypa/setuptools/compare/v75.0.0...v75.1.0)\n\n### [`v75.0.0`](https://redirect.github.com/pypa/setuptools/compare/v74.1.3...v75.0.0)\n\n[Compare Source](https://redirect.github.com/pypa/setuptools/compare/v74.1.3...v75.0.0)\n\n</details>\n\n---\n\n### Configuration\n\n\ud83d\udcc5 **Schedule**: Branch creation - \"before 4am\" in timezone Europe/Amsterdam, Automerge - At any time (no schedule defined).\n\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\n\u267b **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/plugwise/python-plugwise).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOC43NC4xIiwidXBkYXRlZEluVmVyIjoiMzguODAuMCIsInRhcmdldEJyYW5jaCI6Im1haW4iLCJsYWJlbHMiOltdfQ==-->\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex bbed6202d..97ddcd023 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,5 +1,9 @@\n # Changelog\n \n+## v0.37.3\n+\n+- Fix for [plugwise-beta #620](https://github.com/plugwise/plugwise-beta/issues/620)\n+\n ## v0.37.2\n \n - Code improvements\ndiff --git a/plugwise/legacy/smile.py b/plugwise/legacy/smile.py\nindex 116c15571..42c35ad5a 100644\n--- a/plugwise/legacy/smile.py\n+++ b/plugwise/legacy/smile.py\n@@ -249,8 +249,12 @@ async def _set_groupswitch_member_state(\n \n             await self._request(uri, method=\"put\", data=data)\n \n-    async def set_temperature(self, setpoint: str, _: dict[str, float]) -> None:\n+    async def set_temperature(self, _: str, items: dict[str, float]) -> None:\n         \"\"\"Set the given Temperature on the relevant Thermostat.\"\"\"\n+        setpoint: float | None = None\n+        if \"setpoint\" in items:\n+            setpoint = items[\"setpoint\"]\n+\n         if setpoint is None:\n             raise PlugwiseError(\n                 \"Plugwise: failed setting temperature: no valid input provided\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex e79fbdb91..2f86b9be1 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -4,7 +4,7 @@ build-backend = \"setuptools.build_meta\"\n \n [project]\n name            = \"plugwise\"\n-version         = \"0.37.2\"\n+version         = \"0.37.3\"\n license         = {file = \"LICENSE\"}\n description     = \"Plugwise Smile (Adam/Anna/P1) and Stretch module for Python 3.\"\n readme          = \"README.md\"\n", "instance_id": "plugwise__python-plugwise-547", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to update the `setuptools` dependency from version 74.0 to 75.1. It provides relevant metadata such as age, adoption, passing, and confidence metrics, which are useful for assessing the update's reliability. Additionally, it includes release notes and configuration details about scheduling and automerge settings. However, the statement lacks specific details about potential breaking changes or compatibility issues introduced by the update, which could be critical for understanding the full scope of the task. There are no explicit mentions of required testing or validation steps post-update, nor are there details on how this update might interact with other dependencies or parts of the codebase. These minor ambiguities prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The primary task is updating a dependency version in the `pyproject.toml` file, which is a straightforward modification requiring minimal code changes. The provided code changes also include a minor version bump for the project (from 0.37.2 to 0.37.3) and a small fix in the `smile.py` file unrelated to the dependency update itself, but these are also simple and localized. The scope of changes is limited to a few lines across three files (`pyproject.toml`, `CHANGELOG.md`, and `smile.py`), with no significant impact on the system's architecture or interactions between modules. The technical concepts involved are basic\u2014understanding dependency management in Python and version updates. There are no complex edge cases or error handling requirements mentioned or implied by the dependency update itself. The main potential challenge could be verifying compatibility post-update, but this is not explicitly required by the problem statement and is generally a routine task. Overall, this task requires minimal effort and expertise, suitable for a junior developer or even an automated tool like Renovate.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`q**2` is always positive for vector.MomentumObject4D\n### Vector Version\n\n1.3.1\n\n### Python Version\n\n3.11.4\n\n### OS / Environment\n\nKubuntu 22.04\r\nvector is installed via pip inside a conda environment\n\n### Describe the bug\n\nWhen calculating `q**2` of a 4-vector that is created from `vector.obj`, the value is always positive, even when it should not be:\r\n```python\r\n>>> vector.obj(E=1, px=1, py=1, pz=1)**2\r\n2.0000000000000004\r\n```\r\nFor numpy vector arrays, the behavior is correct and as expected:\r\n```python\r\n>>> vector.array({\"E\": [1], \"px\": [1], \"py\": [1], \"pz\": [1]})**2\r\narray([-2.])\r\n```\r\nI have not tested other backends (e.g. awkward).\n\n### Any additional but relevant log output\n\n_No response_\n", "patch": "diff --git a/src/vector/backends/object.py b/src/vector/backends/object.py\nindex ddc44cad..18a202b5 100644\n--- a/src/vector/backends/object.py\n+++ b/src/vector/backends/object.py\n@@ -387,7 +387,7 @@ def __itruediv__(self: SameVectorType, other: float) -> VectorProtocol:\n         return _replace_data(self, numpy.true_divide(self, other))  # type: ignore[call-overload]\n \n     def __pow__(self, other: float) -> float:\n-        return numpy.power(self, other)  # type: ignore[call-overload]\n+        return numpy.square(self) if other == 2 else numpy.power(self, other)  # type: ignore[call-overload]\n \n     def __matmul__(self, other: VectorProtocol) -> float:\n         return numpy.matmul(self, other)  # type: ignore[call-overload]\n", "instance_id": "scikit-hep__vector-444", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the calculation of `q**2` for a 4-vector using `vector.obj` always returns a positive value, which is incorrect compared to the expected behavior shown with `vector.array`. The statement provides specific examples with code snippets to illustrate the bug, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, it does not explicitly define what the correct behavior of `q**2` should be (e.g., under what conditions it should be negative or positive), nor does it mention potential edge cases or constraints related to the input values. Additionally, the statement lacks clarity on whether this issue affects other operations or backends beyond the ones tested. Despite these minor gaps, the problem is valid and comprehensible with the provided context.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is minimal, confined to a single line in a single file (`object.py`), where the fix involves conditionally using `numpy.square` for the specific case of exponentiation with 2, instead of the generic `numpy.power`. This indicates a straightforward bug fix with no impact on the broader system architecture or interactions between modules. Second, the technical concepts required are basic: familiarity with Python, the `numpy` library, and operator overloading (`__pow__`). There is no need for advanced algorithms, design patterns, or domain-specific knowledge beyond understanding the intent of the operation (likely related to physics or vector mathematics, though not explicitly required to solve the issue). Third, the problem does not mention specific edge cases or complex error handling requirements, and the code change does not introduce new error handling logic. Overall, this is a simple bug fix that requires minimal effort and understanding of the surrounding codebase, justifying a difficulty score of 0.25.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Usage examples incompatible with neovim >=0.8 (using deprecated $NVIM_LISTEN_ADDRESS)\nPer https://neovim.io/doc/user/deprecated.html#%24NVIM_LISTEN_ADDRESS, `$NVIM_LISTEN_ADDRESS` no longer works in neovim 0.8 and newer, so the usage examples here that rely on it are broken.\n", "patch": "diff --git a/README.md b/README.md\nindex bd91c8fa..a573a858 100644\n--- a/README.md\n+++ b/README.md\n@@ -72,21 +72,22 @@ documentation.\n ### Usage from the Python REPL\n \n A number of different transports are supported, but the simplest way to get\n-started is with the python REPL. First, start Nvim with a known address (or use\n-the `$NVIM_LISTEN_ADDRESS` of a running instance):\n+started is with the python REPL. First, start Nvim with a known address:\n \n ```sh\n-$ NVIM_LISTEN_ADDRESS=/tmp/nvim nvim\n+$ nvim --listen /tmp/nvim.sock\n ```\n \n+Or alternatively, note the `v:servername` address of a running Nvim instance.\n+\n In another terminal, connect a python REPL to Nvim (note that the API is similar\n to the one exposed by the [python-vim\n bridge](http://vimdoc.sourceforge.net/htmldoc/if_pyth.html#python-vim)):\n \n ```python\n >>> import pynvim\n-# Create a python API session attached to unix domain socket created above:\n->>> nvim = pynvim.attach('socket', path='/tmp/nvim')\n+# Create a session attached to Nvim's address (`v:servername`).\n+>>> nvim = pynvim.attach('socket', path='/tmp/nvim.sock')\n # Now do some work.\n >>> buffer = nvim.current.buffer # Get the current buffer\n >>> buffer[0] = 'replace first line'\ndiff --git a/docs/development.rst b/docs/development.rst\nindex 43138d07..de08e769 100644\n--- a/docs/development.rst\n+++ b/docs/development.rst\n@@ -7,7 +7,7 @@ If you change the code, you need to run::\n \n for the changes to have effect.\n \n-Alternatively you could execute Neovim with the ``$PYTHONPATH`` environment variable::\n+Alternatively you could execute Nvim with the ``$PYTHONPATH`` environment variable::\n \n     PYTHONPATH=/path/to/pynvim nvim\n \n@@ -15,13 +15,13 @@ But note this is not completely reliable,\n as installed packages can appear before ``$PYTHONPATH`` in the python search path.\n \n You need to rerun this command if you have changed the code,\n-in order for Neovim to use it for the plugin host.\n+in order for Nvim to use it for the plugin host.\n \n To run the tests execute::\n \n     python -m pytest\n \n-This will run the tests in an embedded instance of Neovim, with the current\n+This will run the tests in an embedded instance of Nvim, with the current\n directory added to ``sys.path``.\n \n If you want to test a different version than ``nvim`` in ``$PATH`` use::\n@@ -30,11 +30,11 @@ If you want to test a different version than ``nvim`` in ``$PATH`` use::\n \n Alternatively, if you want to see the state of nvim, you could use::\n \n-    export NVIM_LISTEN_ADDRESS=/tmp/nvimtest\n-    xterm -e \"nvim -u NONE\"&\n+    export NVIM=/tmp/nvimtest\n+    xterm -e \"nvim --listen $NVIM -u NONE\" &\n     python -m pytest\n \n-But note you need to restart Neovim every time you run the tests!\n+But note you need to restart Nvim every time you run the tests!\n Substitute your favorite terminal emulator for ``xterm``.\n \n Contributing\n@@ -58,7 +58,7 @@ If you have `tox`_, you can test with multiple python versions locally:\n Troubleshooting\n ---------------\n \n-You can run the plugin host in Neovim with logging enabled to debug errors::\n+You can run the plugin host in Nvim with logging enabled to debug errors::\n \n     NVIM_PYTHON_LOG_FILE=logfile NVIM_PYTHON_LOG_LEVEL=DEBUG nvim\n \n@@ -75,18 +75,18 @@ Usage through the Python REPL\n \n A number of different transports are supported,\n but the simplest way to get started is with the python REPL.\n-First, start Neovim with a known address (or use the ``$NVIM_LISTEN_ADDRESS`` of a running instance)::\n+First, start Nvim with a known address (or use the ``v:servername`` of a running instance)::\n \n-    NVIM_LISTEN_ADDRESS=/tmp/nvim nvim\n+    nvim --listen /tmp/nvim.sock\n \n In another terminal,\n-connect a python REPL to Neovim (note that the API is similar to the one exposed by the `python-vim bridge`_):\n+connect a python REPL to Nvim (note that the API is similar to the one exposed by the `python-vim bridge`_):\n \n .. code-block:: python\n \n     >>> from pynvim import attach\n-    # Create a python API session attached to unix domain socket created above:\n-    >>> nvim = attach('socket', path='/tmp/nvim')\n+    # Create a session attached to Nvim's address (`v:servername`).\n+    >>> nvim = attach('socket', path='/tmp/nvim.sock')\n     # Now do some work. \n     >>> buffer = nvim.current.buffer # Get the current buffer\n     >>> buffer[0] = 'replace first line'\n@@ -99,7 +99,7 @@ connect a python REPL to Neovim (note that the API is similar to the one exposed\n \n .. _`python-vim bridge`: http://vimdoc.sourceforge.net/htmldoc/if_pyth.html#python-vim\n \n-You can embed Neovim into your python application instead of binding to a running neovim instance:\n+You can embed Nvim into your python application instead of binding to a running neovim instance:\n \n .. code-block:: python\n \n", "instance_id": "neovim__pynvim-575", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the usage examples in the documentation are incompatible with Neovim versions 0.8 and newer due to the deprecation of `$NVIM_LISTEN_ADDRESS`. It provides a reference to the deprecation notice and indicates that the examples relying on this variable are broken. However, it lacks explicit mention of the expected solution or detailed requirements for updating the documentation (e.g., whether to include alternative methods or additional context for users). Additionally, edge cases or potential compatibility issues with older versions of Neovim are not addressed. Despite these minor omissions, the goal of updating the documentation to reflect the current Neovim API is understandable, and the provided code changes align with the stated problem.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The task involves updating documentation in two files (README.md and docs/development.rst) to replace references to the deprecated `$NVIM_LISTEN_ADDRESS` with the current method of using the `--listen` flag or `v:servername`. The scope of code changes is minimal, confined to textual updates in documentation rather than functional code modifications, and does not impact the system's architecture or require understanding complex interactions within the codebase. No advanced programming concepts, algorithms, or domain-specific knowledge beyond basic familiarity with Neovim's command-line options are needed. There are no significant edge cases or error handling requirements mentioned or implied in the problem or code changes. This task is straightforward and can be completed quickly by someone with basic knowledge of the tool and documentation practices.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[ENH]: Improve API Error Handling for Wrong API Endpoints\n### Contact Details\n\n_No response_\n\n### Feature Description\n\nCurrently, when a user hits an incorrect API endpoint, the error response is not as informative as it could be. To improve the developer experience and make debugging easier, we propose enhancing the response for wrong API endpoints as below.\r\n\r\nHere's an example of a well-structured response:\r\n\r\nExample - GET 'http://localhost:9090/api/group'\r\nResponse -\r\n`{\r\n    \"timestamp\": \"2024-09-16T11:21:21.470+00:00\",\r\n    \"status\": 404,\r\n    \"error\": \"Not Found\",\r\n    \"path\": \"/api/group\"\r\n}`\r\n\r\nKey components -\r\n1. HTTP Status Code: 404 Not Found \u2013 clearly indicating the issue.\r\n2. Path: Showing the invalid endpoint requested.\r\n\r\nThis helps the client understand the problem and provides them with the information they need to fix the issue.\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct\n", "patch": "diff --git a/paig-server/backend/paig/server.py b/paig-server/backend/paig/server.py\nindex bb915e4b..c2d9700d 100644\n--- a/paig-server/backend/paig/server.py\n+++ b/paig-server/backend/paig/server.py\n@@ -23,6 +23,7 @@\n import logging\n from fastapi.exceptions import RequestValidationError\n from fastapi.encoders import jsonable_encoder\n+from starlette.exceptions import HTTPException as StarletteHTTPException\n \n logger = logging.getLogger(__name__)\n \n@@ -102,6 +103,16 @@ async def unhandled_exception_handler(request: Request, exc: Exception):\n                      \"message\": \"An unexpected error occurred. Please try again later.\"}\n         )\n \n+    @app_.exception_handler(StarletteHTTPException)\n+    async def path_not_found_exception_handler(request: Request, exc: StarletteHTTPException):\n+        if exc.status_code == 404:\n+            return JSONResponse(content=jsonable_encoder({\n+                    \"error_code\": 404,\n+                    \"success\": False,\n+                    \"message\": \"Path Not Found\",\n+                    \"path\": request.url.path\n+                }), status_code=status.HTTP_404_NOT_FOUND)\n+\n \n def init_cache() -> None:\n     pass\n", "instance_id": "privacera__paig-121", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear and provides a valid description of the enhancement needed. The goal of improving API error handling for incorrect endpoints is well-defined, and an example of the desired response format is provided, which includes key components like HTTP status code and the requested path. However, there are minor details missing, such as explicit mention of edge cases (e.g., handling different types of invalid requests beyond just 404 errors) or constraints on the response format (e.g., timezone format for timestamp, additional fields that might be required). Additionally, while the example response includes a timestamp, the code changes do not implement it, creating a slight discrepancy between the problem description and the solution provided. Overall, the statement is clear enough to understand the intent but lacks some precision and completeness in requirements.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. The code changes are confined to a single file (`server.py`) and involve adding a straightforward exception handler for 404 errors using FastAPI/Starlette's built-in mechanisms. The scope of the change is minimal, requiring only the addition of a new handler function with a simple JSON response structure. The technical concepts involved are basic: familiarity with FastAPI exception handling, HTTP status codes, and JSON response formatting, which are standard for web development in Python. There is no significant impact on the system's architecture, nor does it require understanding complex interactions across multiple modules. While the problem statement implies potential edge cases (e.g., different types of HTTP errors beyond 404), the provided code changes do not address them, and no complex error handling logic is required beyond returning a static response for 404 errors. Overall, this task requires minimal effort and understanding, suitable for a developer with basic to intermediate experience in web frameworks like FastAPI.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Supporting for Windows\nI have tested it on win11, please check it, hope it helpful.\r\n\r\nscrapy_playwright/handler.py :\r\n\r\n```python\r\nimport asyncio\r\nimport logging\r\nfrom contextlib import suppress\r\nfrom dataclasses import dataclass\r\nfrom ipaddress import ip_address\r\nfrom tempfile import NamedTemporaryFile\r\nfrom time import time\r\nfrom typing import Awaitable, Callable, Dict, Optional, Tuple, Type, TypeVar, Union\r\n\r\nfrom playwright.async_api import (\r\n    BrowserContext,\r\n    BrowserType,\r\n    Download,\r\n    Error as PlaywrightError,\r\n    Page,\r\n    PlaywrightContextManager,\r\n    Request as PlaywrightRequest,\r\n    Response as PlaywrightResponse,\r\n    Route,\r\n)\r\nfrom scrapy import Spider, signals\r\nfrom scrapy.core.downloader.handlers.http import HTTPDownloadHandler\r\nfrom scrapy.crawler import Crawler\r\nfrom scrapy.http import Request, Response\r\nfrom scrapy.http.headers import Headers\r\nfrom scrapy.responsetypes import responsetypes\r\nfrom scrapy.utils.defer import deferred_from_coro\r\nfrom scrapy.utils.misc import load_object\r\nfrom scrapy.utils.reactor import verify_installed_reactor\r\nfrom twisted.internet.defer import Deferred, inlineCallbacks\r\n\r\nfrom scrapy_playwright.headers import use_scrapy_headers\r\nfrom scrapy_playwright.page import PageMethod\r\nfrom scrapy_playwright._utils import (\r\n    _encode_body,\r\n    _get_header_value,\r\n    _get_page_content,\r\n    _is_safe_close_error,\r\n    _maybe_await,\r\n)\r\n\r\n## working on Windows ######################################\r\nimport sys\r\n\r\nif sys.platform == 'win32' and sys.version_info >= (3, 8):\r\n    import threading\r\n\r\n    _loop = None\r\n    _thread = None\r\n\r\n\r\n    def get_default_event_loop():\r\n        global _loop, _thread\r\n        if _thread is None:\r\n            if _loop is None:\r\n                _loop = asyncio.WindowsProactorEventLoopPolicy().new_event_loop()\r\n                asyncio.set_event_loop(_loop)\r\n            if not _loop.is_running():\r\n                _thread = threading.Thread(\r\n                    target=_loop.run_forever,\r\n                    daemon=True)\r\n                _thread.start()\r\n        return _loop\r\n\r\n\r\n    deferred_from_coro__old = deferred_from_coro\r\n\r\n\r\n    def deferred_from_coro(o):\r\n        async def get_result():\r\n            future = asyncio.run_coroutine_threadsafe(o, get_default_event_loop())\r\n            result = future.result()\r\n            return result\r\n\r\n        if isinstance(o, Deferred):\r\n            return o\r\n        return deferred_from_coro__old(get_result())\r\n## working on Windows ######################################\r\n\r\n\r\n__all__ = [\"ScrapyPlaywrightDownloadHandler\"]\r\n\r\nPlaywrightHandler = TypeVar(\"PlaywrightHandler\", bound=\"ScrapyPlaywrightDownloadHandler\")\r\n\r\nlogger = logging.getLogger(\"scrapy-playwright\")\r\n\r\nDEFAULT_BROWSER_TYPE = \"chromium\"\r\nDEFAULT_CONTEXT_NAME = \"default\"\r\nPERSISTENT_CONTEXT_PATH_KEY = \"user_data_dir\"\r\n\r\n\r\n@dataclass\r\nclass BrowserContextWrapper:\r\n    context: BrowserContext\r\n    semaphore: asyncio.Semaphore\r\n    persistent: bool\r\n\r\n\r\nclass ScrapyPlaywrightDownloadHandler(HTTPDownloadHandler):\r\n    def __init__(self, crawler: Crawler) -> None:\r\n        settings = crawler.settings\r\n        super().__init__(settings=settings, crawler=crawler)\r\n        verify_installed_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\r\n        crawler.signals.connect(self._engine_started, signals.engine_started)\r\n        self.stats = crawler.stats\r\n\r\n        # browser\r\n        self.browser_cdp_url = settings.get(\"PLAYWRIGHT_CDP_URL\")\r\n        self.browser_cdp_kwargs = settings.get(\"PLAYWRIGHT_CDP_KWARGS\") or {}\r\n        self.browser_cdp_kwargs.pop(\"endpoint_url\", None)\r\n        self.browser_type_name = settings.get(\"PLAYWRIGHT_BROWSER_TYPE\") or DEFAULT_BROWSER_TYPE\r\n        self.browser_launch_lock = asyncio.Lock()\r\n        self.launch_options: dict = settings.getdict(\"PLAYWRIGHT_LAUNCH_OPTIONS\") or {}\r\n        if self.browser_cdp_url and self.launch_options:\r\n            logger.warning(\"PLAYWRIGHT_CDP_URL is set, ignoring PLAYWRIGHT_LAUNCH_OPTIONS\")\r\n\r\n        # contexts\r\n        self.max_pages_per_context: int = settings.getint(\r\n            \"PLAYWRIGHT_MAX_PAGES_PER_CONTEXT\"\r\n        ) or settings.getint(\"CONCURRENT_REQUESTS\")\r\n        self.context_launch_lock = asyncio.Lock()\r\n        self.context_wrappers: Dict[str, BrowserContextWrapper] = {}\r\n        self.startup_context_kwargs: dict = settings.getdict(\"PLAYWRIGHT_CONTEXTS\")\r\n        if settings.getint(\"PLAYWRIGHT_MAX_CONTEXTS\"):\r\n            self.context_semaphore = asyncio.Semaphore(\r\n                value=settings.getint(\"PLAYWRIGHT_MAX_CONTEXTS\")\r\n            )\r\n\r\n        self.default_navigation_timeout: Optional[float] = None\r\n        if \"PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT\" in settings:\r\n            with suppress(TypeError, ValueError):\r\n                self.default_navigation_timeout = float(\r\n                    settings.get(\"PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT\")\r\n                )\r\n\r\n        # headers\r\n        if \"PLAYWRIGHT_PROCESS_REQUEST_HEADERS\" in settings:\r\n            if settings[\"PLAYWRIGHT_PROCESS_REQUEST_HEADERS\"] is None:\r\n                self.process_request_headers = None\r\n            else:\r\n                self.process_request_headers = load_object(\r\n                    settings[\"PLAYWRIGHT_PROCESS_REQUEST_HEADERS\"]\r\n                )\r\n        else:\r\n            self.process_request_headers = use_scrapy_headers\r\n\r\n        self.abort_request: Optional[Callable[[PlaywrightRequest], Union[Awaitable, bool]]] = None\r\n        if settings.get(\"PLAYWRIGHT_ABORT_REQUEST\"):\r\n            self.abort_request = load_object(settings[\"PLAYWRIGHT_ABORT_REQUEST\"])\r\n\r\n    @classmethod\r\n    def from_crawler(cls: Type[PlaywrightHandler], crawler: Crawler) -> PlaywrightHandler:\r\n        return cls(crawler)\r\n\r\n    def _engine_started(self) -> Deferred:\r\n        \"\"\"Launch the browser. Use the engine_started signal as it supports returning deferreds.\"\"\"\r\n        return deferred_from_coro(self._launch())\r\n\r\n    async def _launch(self) -> None:\r\n        \"\"\"Launch Playwright manager and configured startup context(s).\"\"\"\r\n        logger.info(\"Starting download handler\")\r\n        self.playwright_context_manager = PlaywrightContextManager()\r\n        self.playwright = await self.playwright_context_manager.start()\r\n        self.browser_type: BrowserType = getattr(self.playwright, self.browser_type_name)\r\n        if self.startup_context_kwargs:\r\n            logger.info(\"Launching %i startup context(s)\", len(self.startup_context_kwargs))\r\n            await asyncio.gather(\r\n                *[\r\n                    self._create_browser_context(name=name, context_kwargs=kwargs)\r\n                    for name, kwargs in self.startup_context_kwargs.items()\r\n                ]\r\n            )\r\n            self._set_max_concurrent_context_count()\r\n            logger.info(\"Startup context(s) launched\")\r\n            self.stats.set_value(\"playwright/page_count\", self._get_total_page_count())\r\n        del self.startup_context_kwargs\r\n\r\n    async def _maybe_launch_browser(self) -> None:\r\n        async with self.browser_launch_lock:\r\n            if not hasattr(self, \"browser\"):\r\n                logger.info(\"Launching browser %s\", self.browser_type.name)\r\n                self.browser = await self.browser_type.launch(**self.launch_options)\r\n                logger.info(\"Browser %s launched\", self.browser_type.name)\r\n\r\n    async def _maybe_connect_devtools(self) -> None:\r\n        async with self.browser_launch_lock:\r\n            if not hasattr(self, \"browser\"):\r\n                logger.info(\"Connecting using CDP: %s\", self.browser_cdp_url)\r\n                self.browser = await self.browser_type.connect_over_cdp(\r\n                    self.browser_cdp_url, **self.browser_cdp_kwargs\r\n                )\r\n                logger.info(\"Connected using CDP: %s\", self.browser_cdp_url)\r\n\r\n    async def _create_browser_context(\r\n            self,\r\n            name: str,\r\n            context_kwargs: Optional[dict],\r\n            spider: Optional[Spider] = None,\r\n    ) -> BrowserContextWrapper:\r\n        \"\"\"Create a new context, also launching a local browser or connecting\r\n        to a remote one if necessary.\r\n        \"\"\"\r\n        if hasattr(self, \"context_semaphore\"):\r\n            await self.context_semaphore.acquire()\r\n        context_kwargs = context_kwargs or {}\r\n        if context_kwargs.get(PERSISTENT_CONTEXT_PATH_KEY):\r\n            context = await self.browser_type.launch_persistent_context(**context_kwargs)\r\n            persistent = True\r\n            remote = False\r\n        elif self.browser_cdp_url:\r\n            await self._maybe_connect_devtools()\r\n            context = await self.browser.new_context(**context_kwargs)\r\n            persistent = False\r\n            remote = True\r\n        else:\r\n            await self._maybe_launch_browser()\r\n            context = await self.browser.new_context(**context_kwargs)\r\n            persistent = False\r\n            remote = False\r\n\r\n        context.on(\r\n            \"close\", self._make_close_browser_context_callback(name, persistent, remote, spider)\r\n        )\r\n        self.stats.inc_value(\"playwright/context_count\")\r\n        self.stats.inc_value(f\"playwright/context_count/persistent/{persistent}\")\r\n        self.stats.inc_value(f\"playwright/context_count/remote/{remote}\")\r\n        logger.debug(\r\n            \"Browser context started: '%s' (persistent=%s, remote=%s)\",\r\n            name,\r\n            persistent,\r\n            remote,\r\n            extra={\r\n                \"spider\": spider,\r\n                \"context_name\": name,\r\n                \"persistent\": persistent,\r\n                \"remote\": remote,\r\n            },\r\n        )\r\n        if self.default_navigation_timeout is not None:\r\n            context.set_default_navigation_timeout(self.default_navigation_timeout)\r\n        self.context_wrappers[name] = BrowserContextWrapper(\r\n            context=context,\r\n            semaphore=asyncio.Semaphore(value=self.max_pages_per_context),\r\n            persistent=persistent,\r\n        )\r\n        self._set_max_concurrent_context_count()\r\n        return self.context_wrappers[name]\r\n\r\n    async def _create_page(self, request: Request, spider: Spider) -> Page:\r\n        \"\"\"Create a new page in a context, also creating a new context if necessary.\"\"\"\r\n        context_name = request.meta.setdefault(\"playwright_context\", DEFAULT_CONTEXT_NAME)\r\n        # this block needs to be locked because several attempts to launch a context\r\n        # with the same name could happen at the same time from different requests\r\n        async with self.context_launch_lock:\r\n            ctx_wrapper = self.context_wrappers.get(context_name)\r\n            if ctx_wrapper is None:\r\n                ctx_wrapper = await self._create_browser_context(\r\n                    name=context_name,\r\n                    context_kwargs=request.meta.get(\"playwright_context_kwargs\"),\r\n                    spider=spider,\r\n                )\r\n\r\n        await ctx_wrapper.semaphore.acquire()\r\n        page = await ctx_wrapper.context.new_page()\r\n        self.stats.inc_value(\"playwright/page_count\")\r\n        total_page_count = self._get_total_page_count()\r\n        logger.debug(\r\n            \"[Context=%s] New page created, page count is %i (%i for all contexts)\",\r\n            context_name,\r\n            len(ctx_wrapper.context.pages),\r\n            total_page_count,\r\n            extra={\r\n                \"spider\": spider,\r\n                \"context_name\": context_name,\r\n                \"context_page_count\": len(ctx_wrapper.context.pages),\r\n                \"total_page_count\": total_page_count,\r\n                \"scrapy_request_url\": request.url,\r\n                \"scrapy_request_method\": request.method,\r\n            },\r\n        )\r\n        self._set_max_concurrent_page_count()\r\n        if self.default_navigation_timeout is not None:\r\n            page.set_default_navigation_timeout(self.default_navigation_timeout)\r\n\r\n        page.on(\"close\", self._make_close_page_callback(context_name))\r\n        page.on(\"crash\", self._make_close_page_callback(context_name))\r\n        page.on(\"request\", _make_request_logger(context_name, spider))\r\n        page.on(\"response\", _make_response_logger(context_name, spider))\r\n        page.on(\"request\", self._increment_request_stats)\r\n        page.on(\"response\", self._increment_response_stats)\r\n\r\n        return page\r\n\r\n    def _get_total_page_count(self):\r\n        return sum(len(ctx.context.pages) for ctx in self.context_wrappers.values())\r\n\r\n    def _set_max_concurrent_page_count(self):\r\n        count = self._get_total_page_count()\r\n        current_max_count = self.stats.get_value(\"playwright/page_count/max_concurrent\")\r\n        if current_max_count is None or count > current_max_count:\r\n            self.stats.set_value(\"playwright/page_count/max_concurrent\", count)\r\n\r\n    def _set_max_concurrent_context_count(self):\r\n        current_max_count = self.stats.get_value(\"playwright/context_count/max_concurrent\")\r\n        if current_max_count is None or len(self.context_wrappers) > current_max_count:\r\n            self.stats.set_value(\r\n                \"playwright/context_count/max_concurrent\", len(self.context_wrappers)\r\n            )\r\n\r\n    @inlineCallbacks\r\n    def close(self) -> Deferred:\r\n        logger.info(\"Closing download handler\")\r\n        yield super().close()\r\n        yield deferred_from_coro(self._close())\r\n\r\n    async def _close(self) -> None:\r\n        await asyncio.gather(*[ctx.context.close() for ctx in self.context_wrappers.values()])\r\n        self.context_wrappers.clear()\r\n        if hasattr(self, \"browser\"):\r\n            logger.info(\"Closing browser\")\r\n            await self.browser.close()\r\n        await self.playwright_context_manager.__aexit__()\r\n        await self.playwright.stop()\r\n\r\n    def download_request(self, request: Request, spider: Spider) -> Deferred:\r\n        if request.meta.get(\"playwright\"):\r\n            return deferred_from_coro(self._download_request(request, spider))\r\n        return super().download_request(request, spider)\r\n\r\n    async def _download_request(self, request: Request, spider: Spider) -> Response:\r\n        page = request.meta.get(\"playwright_page\")\r\n        if not isinstance(page, Page):\r\n            page = await self._create_page(request=request, spider=spider)\r\n        context_name = request.meta.setdefault(\"playwright_context\", DEFAULT_CONTEXT_NAME)\r\n\r\n        _attach_page_event_handlers(\r\n            page=page, request=request, spider=spider, context_name=context_name\r\n        )\r\n\r\n        await page.unroute(\"**\")\r\n        await page.route(\r\n            \"**\",\r\n            self._make_request_handler(\r\n                context_name=context_name,\r\n                method=request.method,\r\n                url=request.url,\r\n                headers=request.headers,\r\n                body=request.body,\r\n                encoding=request.encoding,\r\n                spider=spider,\r\n            ),\r\n        )\r\n\r\n        await _maybe_execute_page_init_callback(\r\n            page=page, request=request, context_name=context_name, spider=spider\r\n        )\r\n\r\n        try:\r\n            return await self._download_request_with_page(request, page, spider)\r\n        except Exception as ex:\r\n            if not request.meta.get(\"playwright_include_page\") and not page.is_closed():\r\n                logger.warning(\r\n                    \"Closing page due to failed request: %s exc_type=%s exc_msg=%s\",\r\n                    request,\r\n                    type(ex),\r\n                    str(ex),\r\n                    extra={\r\n                        \"spider\": spider,\r\n                        \"context_name\": context_name,\r\n                        \"scrapy_request_url\": request.url,\r\n                        \"scrapy_request_method\": request.method,\r\n                        \"exception\": ex,\r\n                    },\r\n                    exc_info=True,\r\n                )\r\n                await page.close()\r\n                self.stats.inc_value(\"playwright/page_count/closed\")\r\n            raise\r\n\r\n    async def _download_request_with_page(\r\n            self, request: Request, page: Page, spider: Spider\r\n    ) -> Response:\r\n        # set this early to make it available in errbacks even if something fails\r\n        if request.meta.get(\"playwright_include_page\"):\r\n            request.meta[\"playwright_page\"] = page\r\n\r\n        start_time = time()\r\n        response, download = await self._get_response_and_download(request=request, page=page)\r\n        if isinstance(response, PlaywrightResponse):\r\n            await _set_redirect_meta(request=request, response=response)\r\n            headers = Headers(await response.all_headers())\r\n            headers.pop(\"Content-Encoding\", None)\r\n        else:\r\n            logger.warning(\r\n                \"Navigating to %s returned None, the response\"\r\n                \" will have empty headers and status 200\",\r\n                request,\r\n                extra={\r\n                    \"spider\": spider,\r\n                    \"context_name\": request.meta.get(\"playwright_context\"),\r\n                    \"scrapy_request_url\": request.url,\r\n                    \"scrapy_request_method\": request.method,\r\n                },\r\n            )\r\n            headers = Headers()\r\n\r\n        await self._apply_page_methods(page, request, spider)\r\n        body_str = await _get_page_content(\r\n            page=page,\r\n            spider=spider,\r\n            context_name=request.meta.get(\"playwright_context\"),\r\n            scrapy_request_url=request.url,\r\n            scrapy_request_method=request.method,\r\n        )\r\n        request.meta[\"download_latency\"] = time() - start_time\r\n\r\n        server_ip_address = None\r\n        if response is not None:\r\n            request.meta[\"playwright_security_details\"] = await response.security_details()\r\n            with suppress(KeyError, TypeError, ValueError):\r\n                server_addr = await response.server_addr()\r\n                server_ip_address = ip_address(server_addr[\"ipAddress\"])\r\n\r\n        if download.get(\"exception\"):\r\n            raise download[\"exception\"]\r\n\r\n        if not request.meta.get(\"playwright_include_page\"):\r\n            await page.close()\r\n            self.stats.inc_value(\"playwright/page_count/closed\")\r\n\r\n        if download:\r\n            request.meta[\"playwright_suggested_filename\"] = download.get(\"suggested_filename\")\r\n            respcls = responsetypes.from_args(url=download[\"url\"], body=download[\"bytes\"])\r\n            return respcls(\r\n                url=download[\"url\"],\r\n                status=200,\r\n                body=download[\"bytes\"],\r\n                request=request,\r\n                flags=[\"playwright\"],\r\n            )\r\n\r\n        body, encoding = _encode_body(headers=headers, text=body_str)\r\n        respcls = responsetypes.from_args(headers=headers, url=page.url, body=body)\r\n        return respcls(\r\n            url=page.url,\r\n            status=response.status if response is not None else 200,\r\n            headers=headers,\r\n            body=body,\r\n            request=request,\r\n            flags=[\"playwright\"],\r\n            encoding=encoding,\r\n            ip_address=server_ip_address,\r\n        )\r\n\r\n    async def _get_response_and_download(\r\n            self, request: Request, page: Page\r\n    ) -> Tuple[Optional[PlaywrightResponse], dict]:\r\n        response: Optional[PlaywrightResponse] = None\r\n        download: dict = {}  # updated in-place in _handle_download\r\n        download_ready = asyncio.Event()\r\n\r\n        async def _handle_download(dwnld: Download) -> None:\r\n            self.stats.inc_value(\"playwright/download_count\")\r\n            try:\r\n                if failure := await dwnld.failure():\r\n                    raise RuntimeError(f\"Failed to download {dwnld.url}: {failure}\")\r\n                with NamedTemporaryFile() as temp_file:\r\n                    await dwnld.save_as(temp_file.name)\r\n                    temp_file.seek(0)\r\n                    download[\"bytes\"] = temp_file.read()\r\n                download[\"url\"] = dwnld.url\r\n                download[\"suggested_filename\"] = dwnld.suggested_filename\r\n            except Exception as ex:\r\n                download[\"exception\"] = ex\r\n            finally:\r\n                download_ready.set()\r\n\r\n        page_goto_kwargs = request.meta.get(\"playwright_page_goto_kwargs\") or {}\r\n        page_goto_kwargs.pop(\"url\", None)\r\n        page.on(\"download\", _handle_download)\r\n        try:\r\n            response = await page.goto(url=request.url, **page_goto_kwargs)\r\n        except PlaywrightError as err:\r\n            if not (\r\n                    self.browser_type_name in (\"firefox\", \"webkit\")\r\n                    and \"Download is starting\" in err.message\r\n                    or self.browser_type_name == \"chromium\"\r\n                    and \"net::ERR_ABORTED\" in err.message\r\n            ):\r\n                raise\r\n            await download_ready.wait()\r\n        finally:\r\n            page.remove_listener(\"download\", _handle_download)\r\n\r\n        return response, download\r\n\r\n    async def _apply_page_methods(self, page: Page, request: Request, spider: Spider) -> None:\r\n        context_name = request.meta.get(\"playwright_context\")\r\n        page_methods = request.meta.get(\"playwright_page_methods\") or ()\r\n        if isinstance(page_methods, dict):\r\n            page_methods = page_methods.values()\r\n        for pm in page_methods:\r\n            if isinstance(pm, PageMethod):\r\n                try:\r\n                    method = getattr(page, pm.method)\r\n                except AttributeError as ex:\r\n                    logger.warning(\r\n                        \"Ignoring %r: could not find method\",\r\n                        pm,\r\n                        extra={\r\n                            \"spider\": spider,\r\n                            \"context_name\": context_name,\r\n                            \"scrapy_request_url\": request.url,\r\n                            \"scrapy_request_method\": request.method,\r\n                            \"exception\": ex,\r\n                        },\r\n                        exc_info=True,\r\n                    )\r\n                else:\r\n                    pm.result = await _maybe_await(method(*pm.args, **pm.kwargs))\r\n                    await page.wait_for_load_state(timeout=self.default_navigation_timeout)\r\n            else:\r\n                logger.warning(\r\n                    \"Ignoring %r: expected PageMethod, got %r\",\r\n                    pm,\r\n                    type(pm),\r\n                    extra={\r\n                        \"spider\": spider,\r\n                        \"context_name\": context_name,\r\n                        \"scrapy_request_url\": request.url,\r\n                        \"scrapy_request_method\": request.method,\r\n                    },\r\n                )\r\n\r\n    def _increment_request_stats(self, request: PlaywrightRequest) -> None:\r\n        stats_prefix = \"playwright/request_count\"\r\n        self.stats.inc_value(stats_prefix)\r\n        self.stats.inc_value(f\"{stats_prefix}/resource_type/{request.resource_type}\")\r\n        self.stats.inc_value(f\"{stats_prefix}/method/{request.method}\")\r\n        if request.is_navigation_request():\r\n            self.stats.inc_value(f\"{stats_prefix}/navigation\")\r\n\r\n    def _increment_response_stats(self, response: PlaywrightResponse) -> None:\r\n        stats_prefix = \"playwright/response_count\"\r\n        self.stats.inc_value(stats_prefix)\r\n        self.stats.inc_value(f\"{stats_prefix}/resource_type/{response.request.resource_type}\")\r\n        self.stats.inc_value(f\"{stats_prefix}/method/{response.request.method}\")\r\n\r\n    def _make_close_page_callback(self, context_name: str) -> Callable:\r\n        def close_page_callback() -> None:\r\n            if context_name in self.context_wrappers:\r\n                self.context_wrappers[context_name].semaphore.release()\r\n\r\n        return close_page_callback\r\n\r\n    def _make_close_browser_context_callback(\r\n            self, name: str, persistent: bool, remote: bool, spider: Optional[Spider] = None\r\n    ) -> Callable:\r\n        def close_browser_context_callback() -> None:\r\n            self.context_wrappers.pop(name, None)\r\n            if hasattr(self, \"context_semaphore\"):\r\n                self.context_semaphore.release()\r\n            logger.debug(\r\n                \"Browser context closed: '%s' (persistent=%s, remote=%s)\",\r\n                name,\r\n                persistent,\r\n                remote,\r\n                extra={\r\n                    \"spider\": spider,\r\n                    \"context_name\": name,\r\n                    \"persistent\": persistent,\r\n                    \"remote\": remote,\r\n                },\r\n            )\r\n\r\n        return close_browser_context_callback\r\n\r\n    def _make_request_handler(\r\n            self,\r\n            context_name: str,\r\n            method: str,\r\n            url: str,\r\n            headers: Headers,\r\n            body: Optional[bytes],\r\n            encoding: str,\r\n            spider: Spider,\r\n    ) -> Callable:\r\n        async def _request_handler(route: Route, playwright_request: PlaywrightRequest) -> None:\r\n            \"\"\"Override request headers, method and body.\"\"\"\r\n            if self.abort_request:\r\n                should_abort = await _maybe_await(self.abort_request(playwright_request))\r\n                if should_abort:\r\n                    await route.abort()\r\n                    logger.debug(\r\n                        \"[Context=%s] Aborted Playwright request <%s %s>\",\r\n                        context_name,\r\n                        playwright_request.method.upper(),\r\n                        playwright_request.url,\r\n                        extra={\r\n                            \"spider\": spider,\r\n                            \"context_name\": context_name,\r\n                            \"scrapy_request_url\": url,\r\n                            \"scrapy_request_method\": method,\r\n                            \"playwright_request_url\": playwright_request.url,\r\n                            \"playwright_request_method\": playwright_request.method,\r\n                        },\r\n                    )\r\n                    self.stats.inc_value(\"playwright/request_count/aborted\")\r\n                    return None\r\n\r\n            overrides: dict = {}\r\n\r\n            if self.process_request_headers is None:\r\n                final_headers = await playwright_request.all_headers()\r\n            else:\r\n                overrides[\"headers\"] = final_headers = await _maybe_await(\r\n                    self.process_request_headers(\r\n                        self.browser_type_name, playwright_request, headers\r\n                    )\r\n                )\r\n\r\n            # if the current request corresponds to the original scrapy one\r\n            if (\r\n                    playwright_request.url.rstrip(\"/\") == url.rstrip(\"/\")\r\n                    and playwright_request.is_navigation_request()\r\n            ):\r\n                if method.upper() != playwright_request.method.upper():\r\n                    overrides[\"method\"] = method\r\n                if body:\r\n                    overrides[\"post_data\"] = body.decode(encoding)\r\n                # the request that reaches the callback should contain the final headers\r\n                headers.clear()\r\n                headers.update(final_headers)\r\n\r\n            del final_headers\r\n\r\n            original_playwright_method: str = playwright_request.method\r\n            try:\r\n                await route.continue_(**overrides)\r\n                if overrides.get(\"method\"):\r\n                    logger.debug(\r\n                        \"[Context=%s] Overridden method for Playwright request\"\r\n                        \" to %s: original=%s new=%s\",\r\n                        context_name,\r\n                        playwright_request.url,\r\n                        original_playwright_method,\r\n                        overrides[\"method\"],\r\n                        extra={\r\n                            \"spider\": spider,\r\n                            \"context_name\": context_name,\r\n                            \"scrapy_request_url\": url,\r\n                            \"scrapy_request_method\": method,\r\n                            \"playwright_request_url\": playwright_request.url,\r\n                            \"playwright_request_method_original\": original_playwright_method,\r\n                            \"playwright_request_method_new\": overrides[\"method\"],\r\n                        },\r\n                    )\r\n            except PlaywrightError as ex:\r\n                if _is_safe_close_error(ex):\r\n                    logger.warning(\r\n                        \"Failed processing Playwright request: <%s %s> exc_type=%s exc_msg=%s\",\r\n                        playwright_request.method,\r\n                        playwright_request.url,\r\n                        type(ex),\r\n                        str(ex),\r\n                        extra={\r\n                            \"spider\": spider,\r\n                            \"context_name\": context_name,\r\n                            \"scrapy_request_url\": url,\r\n                            \"scrapy_request_method\": method,\r\n                            \"playwright_request_url\": playwright_request.url,\r\n                            \"playwright_request_method\": playwright_request.method,\r\n                            \"exception\": ex,\r\n                        },\r\n                        exc_info=True,\r\n                    )\r\n                else:\r\n                    raise\r\n\r\n        return _request_handler\r\n\r\n\r\ndef _attach_page_event_handlers(\r\n        page: Page, request: Request, spider: Spider, context_name: str\r\n) -> None:\r\n    event_handlers = request.meta.get(\"playwright_page_event_handlers\") or {}\r\n    for event, handler in event_handlers.items():\r\n        if callable(handler):\r\n            page.on(event, handler)\r\n        elif isinstance(handler, str):\r\n            try:\r\n                page.on(event, getattr(spider, handler))\r\n            except AttributeError as ex:\r\n                logger.warning(\r\n                    \"Spider '%s' does not have a '%s' attribute,\"\r\n                    \" ignoring handler for event '%s'\",\r\n                    spider.name,\r\n                    handler,\r\n                    event,\r\n                    extra={\r\n                        \"spider\": spider,\r\n                        \"context_name\": context_name,\r\n                        \"scrapy_request_url\": request.url,\r\n                        \"scrapy_request_method\": request.method,\r\n                        \"exception\": ex,\r\n                    },\r\n                    exc_info=True,\r\n                )\r\n\r\n\r\nasync def _set_redirect_meta(request: Request, response: PlaywrightResponse) -> None:\r\n    \"\"\"Update a Scrapy request with metadata about redirects.\"\"\"\r\n    redirect_times: int = 0\r\n    redirect_urls: list = []\r\n    redirect_reasons: list = []\r\n    redirected = response.request.redirected_from\r\n    while redirected is not None:\r\n        redirect_times += 1\r\n        redirect_urls.append(redirected.url)\r\n        redirected_response = await redirected.response()\r\n        reason = None if redirected_response is None else redirected_response.status\r\n        redirect_reasons.append(reason)\r\n        redirected = redirected.redirected_from\r\n    if redirect_times:\r\n        request.meta[\"redirect_times\"] = redirect_times\r\n        request.meta[\"redirect_urls\"] = list(reversed(redirect_urls))\r\n        request.meta[\"redirect_reasons\"] = list(reversed(redirect_reasons))\r\n\r\n\r\nasync def _maybe_execute_page_init_callback(\r\n        page: Page,\r\n        request: Request,\r\n        context_name: str,\r\n        spider: Spider,\r\n) -> None:\r\n    page_init_callback = request.meta.get(\"playwright_page_init_callback\")\r\n    if page_init_callback:\r\n        try:\r\n            page_init_callback = load_object(page_init_callback)\r\n            await page_init_callback(page, request)\r\n        except Exception as ex:\r\n            logger.warning(\r\n                \"[Context=%s] Page init callback exception for %s exc_type=%s exc_msg=%s\",\r\n                context_name,\r\n                repr(request),\r\n                type(ex),\r\n                str(ex),\r\n                extra={\r\n                    \"spider\": spider,\r\n                    \"context_name\": context_name,\r\n                    \"scrapy_request_url\": request.url,\r\n                    \"scrapy_request_method\": request.method,\r\n                    \"exception\": ex,\r\n                },\r\n                exc_info=True,\r\n            )\r\n\r\n\r\ndef _make_request_logger(context_name: str, spider: Spider) -> Callable:\r\n    async def _log_request(request: PlaywrightRequest) -> None:\r\n        log_args = [context_name, request.method.upper(), request.url, request.resource_type]\r\n        referrer = await _get_header_value(request, \"referer\")\r\n        if referrer:\r\n            log_args.append(referrer)\r\n            log_msg = \"[Context=%s] Request: <%s %s> (resource type: %s, referrer: %s)\"\r\n        else:\r\n            log_msg = \"[Context=%s] Request: <%s %s> (resource type: %s)\"\r\n        logger.debug(\r\n            log_msg,\r\n            *log_args,\r\n            extra={\r\n                \"spider\": spider,\r\n                \"context_name\": context_name,\r\n                \"playwright_request_url\": request.url,\r\n                \"playwright_request_method\": request.method,\r\n                \"playwright_resource_type\": request.resource_type,\r\n            },\r\n        )\r\n\r\n    return _log_request\r\n\r\n\r\ndef _make_response_logger(context_name: str, spider: Spider) -> Callable:\r\n    async def _log_response(response: PlaywrightResponse) -> None:\r\n        log_args = [context_name, response.status, response.url]\r\n        location = await _get_header_value(response, \"location\")\r\n        if location:\r\n            log_args.append(location)\r\n            log_msg = \"[Context=%s] Response: <%i %s> (location: %s)\"\r\n        else:\r\n            log_msg = \"[Context=%s] Response: <%i %s>\"\r\n        logger.debug(\r\n            log_msg,\r\n            *log_args,\r\n            extra={\r\n                \"spider\": spider,\r\n                \"context_name\": context_name,\r\n                \"playwright_response_url\": response.url,\r\n                \"playwright_response_status\": response.status,\r\n            },\r\n        )\r\n\r\n    return _log_response\r\n\r\n```\nSupporting for windows\n\n", "patch": "diff --git a/README.md b/README.md\nindex 13230d2..6727a3b 100644\n--- a/README.md\n+++ b/README.md\n@@ -56,10 +56,13 @@ See the [changelog](docs/changelog.md) document.\n \n ## Activation\n \n+### Download handler\n+\n Replace the default `http` and/or `https` Download Handlers through\n [`DOWNLOAD_HANDLERS`](https://docs.scrapy.org/en/latest/topics/settings.html):\n \n ```python\n+# settings.py\n DOWNLOAD_HANDLERS = {\n     \"http\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n     \"https\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n@@ -70,12 +73,19 @@ Note that the `ScrapyPlaywrightDownloadHandler` class inherits from the default\n `http/https` handler. Unless explicitly marked (see [Basic usage](#basic-usage)),\n requests will be processed by the regular Scrapy download handler.\n \n-Also, be sure to [install the `asyncio`-based Twisted reactor](https://docs.scrapy.org/en/latest/topics/asyncio.html#installing-the-asyncio-reactor):\n+\n+### Twisted reactor\n+\n+When running on GNU/Linux or macOS you'll need to\n+[install the `asyncio`-based Twisted reactor](https://docs.scrapy.org/en/latest/topics/asyncio.html#installing-the-asyncio-reactor):\n \n ```python\n+# settings.py\n TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n ```\n \n+This is not a requirement on Windows (see [Windows support](#windows-support))\n+\n \n ## Basic usage\n \n@@ -112,6 +122,20 @@ does not match the running Browser. If you prefer the `User-Agent` sent by\n default by the specific browser you're using, set the Scrapy user agent to `None`.\n \n \n+## Windows support\n+\n+Windows support is possible by running Playwright in a `ProactorEventLoop` in a separate thread.\n+This is necessary because it's not possible to run Playwright in the same\n+asyncio event loop as the Scrapy crawler:\n+* Playwright runs the driver in a subprocess. Source:\n+  [Playwright repository](https://github.com/microsoft/playwright-python/blob/v1.44.0/playwright/_impl/_transport.py#L120-L130).\n+* \"On Windows, the default event loop `ProactorEventLoop` supports subprocesses,\n+  whereas `SelectorEventLoop` does not\". Source:\n+  [Python docs](https://docs.python.org/3/library/asyncio-platforms.html#asyncio-windows-subprocess).\n+* Twisted's `asyncio` reactor requires the `SelectorEventLoop`. Source:\n+  [Twisted repository](https://github.com/twisted/twisted/blob/twisted-24.3.0/src/twisted/internet/asyncioreactor.py#L31)\n+\n+\n ## Supported [settings](https://docs.scrapy.org/en/latest/topics/settings.html)\n \n ### `PLAYWRIGHT_BROWSER_TYPE`\n@@ -851,6 +875,12 @@ Refer to the\n [upstream docs](https://docs.scrapy.org/en/latest/topics/extensions.html#module-scrapy.extensions.memusage)\n for more information about supported settings.\n \n+### Windows support\n+\n+Just like the [upstream Scrapy extension](https://docs.scrapy.org/en/latest/topics/extensions.html#module-scrapy.extensions.memusage), this custom memory extension does not work\n+on Windows. This is because the stdlib [`resource`](https://docs.python.org/3/library/resource.html)\n+module is not available.\n+\n \n ## Examples\n \n@@ -912,23 +942,6 @@ See the [examples](examples) directory for more.\n \n ## Known issues\n \n-### Lack of native support for Windows\n-\n-This package does not work natively on Windows. This is because:\n-\n-* Playwright runs the driver in a subprocess. Source:\n-[Playwright repository](https://github.com/microsoft/playwright-python/blob/v1.28.0/playwright/_impl/_transport.py#L120-L129).\n-* \"On Windows, the default event loop `ProactorEventLoop` supports subprocesses,\n-whereas `SelectorEventLoop` does not\". Source:\n-[Python docs](https://docs.python.org/3/library/asyncio-platforms.html#asyncio-windows-subprocess).\n-* Twisted's `asyncio` reactor requires the `SelectorEventLoop`. Source:\n-[Twisted repository](https://github.com/twisted/twisted/blob/twisted-22.4.0/src/twisted/internet/asyncioreactor.py#L31).\n-\n-Some users have reported having success\n-[running under WSL](https://github.com/scrapy-plugins/scrapy-playwright/issues/7#issuecomment-817394494).\n-See also [#78](https://github.com/scrapy-plugins/scrapy-playwright/issues/78)\n-for information about working in headful mode under WSL.\n-\n ### No per-request proxy support\n Specifying a proxy via the `proxy` Request meta key is not supported.\n Refer to the [Proxy support](#proxy-support) section for more information.\ndiff --git a/docs/changelog.md b/docs/changelog.md\nindex f3ba5af..1cd4e6f 100644\n--- a/docs/changelog.md\n+++ b/docs/changelog.md\n@@ -1,5 +1,10 @@\n # scrapy-playwright changelog\n \n+### [v0.0.36](https://github.com/scrapy-plugins/scrapy-playwright/releases/tag/v0.0.36) (2024-MM-DD)\n+\n+* Windows support (#276)\n+\n+\n ### [v0.0.35](https://github.com/scrapy-plugins/scrapy-playwright/releases/tag/v0.0.35) (2024-06-01)\n \n * Update exception message check\ndiff --git a/scrapy_playwright/_utils.py b/scrapy_playwright/_utils.py\nindex d6eee04..bc93f14 100644\n--- a/scrapy_playwright/_utils.py\n+++ b/scrapy_playwright/_utils.py\n@@ -1,10 +1,15 @@\n+import asyncio\n+import concurrent\n import logging\n+import platform\n+import threading\n from typing import Awaitable, Iterator, Optional, Tuple, Union\n \n+import scrapy\n from playwright.async_api import Error, Page, Request, Response\n-from scrapy import Spider\n from scrapy.http.headers import Headers\n from scrapy.utils.python import to_unicode\n+from twisted.internet.defer import Deferred\n from w3lib.encoding import html_body_declared_encoding, http_content_type_encoding\n \n \n@@ -53,7 +58,7 @@ def _is_safe_close_error(error: Error) -> bool:\n \n async def _get_page_content(\n     page: Page,\n-    spider: Spider,\n+    spider: scrapy.Spider,\n     context_name: str,\n     scrapy_request_url: str,\n     scrapy_request_method: str,\n@@ -89,3 +94,38 @@ async def _get_header_value(\n         return await resource.header_value(header_name)\n     except Exception:\n         return None\n+\n+\n+if platform.system() == \"Windows\":\n+\n+    class _WindowsAdapter:\n+        \"\"\"Utility class to redirect coroutines to an asyncio event loop running\n+        in a different thread. This allows to use a ProactorEventLoop, which is\n+        supported by Playwright on Windows.\n+        \"\"\"\n+\n+        loop = None\n+        thread = None\n+\n+        @classmethod\n+        def get_event_loop(cls) -> asyncio.AbstractEventLoop:\n+            if cls.thread is None:\n+                if cls.loop is None:\n+                    policy = asyncio.WindowsProactorEventLoopPolicy()  # type: ignore\n+                    cls.loop = policy.new_event_loop()\n+                    asyncio.set_event_loop(cls.loop)\n+                if not cls.loop.is_running():\n+                    cls.thread = threading.Thread(target=cls.loop.run_forever, daemon=True)\n+                    cls.thread.start()\n+                    logger.info(\"Started loop on separate thread: %s\", cls.loop)\n+            return cls.loop\n+\n+        @classmethod\n+        async def get_result(cls, coro) -> concurrent.futures.Future:\n+            return asyncio.run_coroutine_threadsafe(coro=coro, loop=cls.get_event_loop()).result()\n+\n+    def _deferred_from_coro(coro) -> Deferred:\n+        return scrapy.utils.defer.deferred_from_coro(_WindowsAdapter.get_result(coro))\n+\n+else:\n+    _deferred_from_coro = scrapy.utils.defer.deferred_from_coro\ndiff --git a/scrapy_playwright/handler.py b/scrapy_playwright/handler.py\nindex a2ae3f2..9096cbb 100644\n--- a/scrapy_playwright/handler.py\n+++ b/scrapy_playwright/handler.py\n@@ -1,5 +1,6 @@\n import asyncio\n import logging\n+import platform\n from contextlib import suppress\n from dataclasses import dataclass\n from ipaddress import ip_address\n@@ -25,7 +26,6 @@\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.settings import Settings\n-from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import verify_installed_reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -38,6 +38,7 @@\n     _get_page_content,\n     _is_safe_close_error,\n     _maybe_await,\n+    _deferred_from_coro,\n )\n \n \n@@ -101,7 +102,8 @@ class ScrapyPlaywrightDownloadHandler(HTTPDownloadHandler):\n \n     def __init__(self, crawler: Crawler) -> None:\n         super().__init__(settings=crawler.settings, crawler=crawler)\n-        verify_installed_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+        if platform.system() != \"Windows\":\n+            verify_installed_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n         crawler.signals.connect(self._engine_started, signals.engine_started)\n         self.stats = crawler.stats\n \n@@ -134,7 +136,7 @@ def from_crawler(cls: Type[PlaywrightHandler], crawler: Crawler) -> PlaywrightHa\n \n     def _engine_started(self) -> Deferred:\n         \"\"\"Launch the browser. Use the engine_started signal as it supports returning deferreds.\"\"\"\n-        return deferred_from_coro(self._launch())\n+        return _deferred_from_coro(self._launch())\n \n     async def _launch(self) -> None:\n         \"\"\"Launch Playwright manager and configured startup context(s).\"\"\"\n@@ -290,7 +292,7 @@ def _set_max_concurrent_context_count(self):\n     def close(self) -> Deferred:\n         logger.info(\"Closing download handler\")\n         yield super().close()\n-        yield deferred_from_coro(self._close())\n+        yield _deferred_from_coro(self._close())\n \n     async def _close(self) -> None:\n         await asyncio.gather(*[ctx.context.close() for ctx in self.context_wrappers.values()])\n@@ -305,7 +307,7 @@ async def _close(self) -> None:\n \n     def download_request(self, request: Request, spider: Spider) -> Deferred:\n         if request.meta.get(\"playwright\"):\n-            return deferred_from_coro(self._download_request(request, spider))\n+            return _deferred_from_coro(self._download_request(request, spider))\n         return super().download_request(request, spider)\n \n     async def _download_request(self, request: Request, spider: Spider) -> Response:\n", "instance_id": "scrapy-plugins__scrapy-playwright-276", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to add Windows support for the `scrapy-playwright` library by addressing event loop compatibility issues between Scrapy and Playwright on Windows. It provides context about the need for a separate `ProactorEventLoop` running in a different thread due to platform-specific limitations. However, the statement lacks critical details such as specific constraints, potential risks, or edge cases related to running the event loop in a separate thread (e.g., thread safety, resource management, or shutdown behavior). Additionally, there are no examples or test cases provided to validate the solution's correctness across different Windows environments or Python versions. While the intent and high-level approach are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, it requires a deep understanding of asynchronous programming in Python, specifically the differences between `ProactorEventLoop` and `SelectorEventLoop` on Windows, as well as their compatibility with Twisted's asyncio reactor and Playwright's subprocess requirements. Second, the solution involves modifying core functionality in the download handler to conditionally bypass reactor verification on Windows and redirect coroutines to a separate event loop thread, which impacts the interaction between Scrapy and Playwright. This necessitates knowledge of threading, event loop management, and Scrapy's deferred system. Third, the code changes span multiple files (`handler.py`, `_utils.py`, and documentation), requiring a holistic understanding of the codebase. While the changes are not architecturally transformative, they introduce complexity in managing a separate thread for the event loop, which could lead to subtle bugs (e.g., thread lifecycle issues or coroutine scheduling problems). Finally, potential edge cases such as thread shutdown, resource exhaustion, or compatibility with different Windows versions and Python configurations add to the challenge, though these are not explicitly addressed in the problem statement. A score of 0.65 reflects the need for advanced knowledge and careful handling of platform-specific behavior, placing it on the lower end of the \"Hard\" range due to the focused scope of changes.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "State in the table for running kernels does not get updated\nThe second table in the dialog does not get updated when the kernel state changes\r\n\r\nhttps://github.com/nebari-dev/jupyterlab-launchpad/blob/5ba3fee41f7cf757d565d5b1266c546087abf0aa/src/dialogs.tsx#L292-L313\r\n\r\nBecause it includes things like \"state: starting\" it needs to be dynamically updated. We need to trigger `forceUpdate` when item metadata changes, and for that we need to pass through another signal, like with `lastUsedChanged`:\r\n\r\nhttps://github.com/nebari-dev/jupyterlab-launchpad/blob/5ba3fee41f7cf757d565d5b1266c546087abf0aa/src/components/table.tsx#L103-L117\r\n\r\nWe derive the items from the `sessions` object passed in options:\r\n\r\nhttps://github.com/nebari-dev/jupyterlab-launchpad/blob/5ba3fee41f7cf757d565d5b1266c546087abf0aa/src/dialogs.tsx#L239\r\n\r\nand we fetch the `sessions` when creating the dialog:\r\n\r\nhttps://github.com/nebari-dev/jupyterlab-launchpad/blob/5ba3fee41f7cf757d565d5b1266c546087abf0aa/src/dialogs.tsx#L75-L80\r\n\r\nThe `SessionManager` emits [`runningChanged`](https://jupyterlab.readthedocs.io/en/stable/api/classes/services.SessionManager-1.html#runningChanged) signal which we could use.\n", "patch": "diff --git a/package.json b/package.json\nindex 37da91c..e62712c 100644\n--- a/package.json\n+++ b/package.json\n@@ -1,6 +1,6 @@\n {\n     \"name\": \"jupyterlab-launchpad\",\n-    \"version\": \"1.0.2\",\n+    \"version\": \"1.0.3\",\n     \"description\": \"A redesigned JupyterLab launcher\",\n     \"keywords\": [\n         \"jupyter\",\ndiff --git a/src/dialogs.tsx b/src/dialogs.tsx\nindex 4f882c3..402d351 100644\n--- a/src/dialogs.tsx\n+++ b/src/dialogs.tsx\n@@ -4,6 +4,7 @@ import {\n } from '@jupyterlab/application';\n import type { CommandRegistry } from '@lumino/commands';\n import type { Message } from '@lumino/messaging';\n+import { Signal, ISignal } from '@lumino/signaling';\n import {\n   SessionContextDialogs,\n   ISessionContextDialogs,\n@@ -70,14 +71,24 @@ class CustomSessionContextDialogs extends SessionContextDialogs {\n     const hasCheckbox = typeof autoStartDefault === 'boolean';\n     const settings = await this.options.settingRegistry.load(MAIN_PLUGIN_ID);\n \n+    const dataChanged = new Signal<CustomSessionContextDialogs, void>(this);\n+    sessionContext.sessionManager.runningChanged.connect(() => {\n+      dataChanged.emit();\n+    });\n+    this.options.kernelManager.runningChanged.connect(() => {\n+      dataChanged.emit();\n+    });\n+\n     const dialog = new Dialog<Partial<Kernel.IModel> | null>({\n       title: trans.__('Select Kernel'),\n       body: new KernelSelector({\n-        data: {\n+        data: () => ({\n           specs: sessionContext.specsManager.specs,\n           sessions: sessionContext.sessionManager.running(),\n+          kernels: this.options.kernelManager.running(),\n           preference: sessionContext.kernelPreference\n-        },\n+        }),\n+        dataChanged,\n         name: sessionContext.name,\n         commands: this.options.commands,\n         favoritesDatabase: this.options.database.favorites,\n@@ -131,6 +142,7 @@ export namespace CustomSessionContextDialogs {\n     database: ILauncherDatabase;\n     commands: CommandRegistry;\n     settingRegistry: ISettingRegistry;\n+    kernelManager: Kernel.IManager;\n   }\n }\n \n@@ -154,7 +166,8 @@ export const sessionDialogsPlugin: JupyterFrontEndPlugin<ISessionContextDialogs>\n         translator: translator,\n         database: database,\n         commands: app.commands,\n-        settingRegistry: settingRegistry\n+        settingRegistry: settingRegistry,\n+        kernelManager: app.serviceManager.kernels\n       });\n     }\n   };\n@@ -190,6 +203,7 @@ export class KernelSelector extends ReactWidget {\n \n   onAfterAttach(msg: Message) {\n     super.onAfterAttach(msg);\n+    this.options.dataChanged.connect(this.update.bind(this));\n     requestAnimationFrame(() => {\n       // Set minimum dimensions so that when user starts typing to filter\n       // the kernels the dialog does not start jumping around.\n@@ -199,12 +213,18 @@ export class KernelSelector extends ReactWidget {\n     });\n   }\n \n+  onAfterDetach(msg: Message) {\n+    super.onAfterDetach(msg);\n+    this.options.dataChanged.disconnect(this.update);\n+  }\n+\n   /**\n    * Render the launcher to virtual DOM nodes.\n    */\n   protected render(): React.ReactElement<any> | null {\n     const items: ILauncher.IItemOptions[] = [];\n-    const specs = this.options.data.specs!.kernelspecs!;\n+    const data = this.options.data();\n+    const specs = data.specs!.kernelspecs!;\n     // Note: this command is not executed, but it is only used to match favourite/last used metadata\n     const command =\n       this.options.type === 'console'\n@@ -236,8 +256,14 @@ export class KernelSelector extends ReactWidget {\n       });\n     }\n     const runningItems: ILauncher.IItemOptions[] = [];\n-    for (const model of this.options.data.sessions!) {\n-      const kernel = model.kernel;\n+    const kernels = new Map([...data.kernels].map(k => [k.id, k]));\n+    for (const model of data.sessions!) {\n+      // session models may be outdated, use the more frequently\n+      // updated instance from the kernels manager\n+      if (!model.kernel) {\n+        continue;\n+      }\n+      const kernel = kernels.get(model.kernel?.id);\n       if (!kernel) {\n         continue;\n       }\n@@ -344,7 +370,10 @@ export namespace KernelSelector {\n     settings: ISettingRegistry.ISettings;\n     commands: CommandRegistry;\n     trans: TranslationBundle;\n-    data: SessionContext.IKernelSearch;\n+    data: () => SessionContext.IKernelSearch & {\n+      kernels: IterableIterator<Kernel.IModel>;\n+    };\n+    dataChanged: ISignal<CustomSessionContextDialogs, void>;\n     acceptDialog: () => void;\n     name: string;\n     // known values are \"notebook\" and \"console\"\n", "instance_id": "nebari-dev__jupyterlab-launchpad-70", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the state of running kernels in a table within a dialog does not update dynamically when the kernel state changes. It provides specific references to the codebase (via GitHub links) and suggests a solution involving the use of a signal (`runningChanged`) to trigger updates. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior of the update (e.g., how frequently or under what exact conditions the table should refresh). Additionally, it lacks mention of potential edge cases, such as what happens if the `SessionManager` or `KernelManager` signals fail to emit or if there are discrepancies between session and kernel data. While the intent and general approach are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving modifications to a few key areas in the `dialogs.tsx` file, such as adding signal handling for dynamic updates and adjusting data retrieval logic. It does not appear to impact the broader system architecture but requires understanding the interaction between components like `SessionManager`, `KernelManager`, and the dialog rendering logic. Second, the technical concepts involved include familiarity with TypeScript/React (given the `.tsx` extension), the JupyterLab API (specifically signals like `runningChanged`), and event-driven programming with Lumino's signaling system. These concepts are moderately complex for someone unfamiliar with the JupyterLab ecosystem but manageable for an experienced developer. Third, the code changes are non-trivial, as they involve connecting and disconnecting signals, updating data structures, and ensuring the UI refreshes correctly, which requires careful handling to avoid issues like unnecessary re-renders or memory leaks. Finally, while the problem statement does not explicitly mention edge cases, the code changes suggest potential challenges, such as ensuring consistency between session and kernel data or handling cases where kernel models are missing or outdated. Overall, this problem requires a solid understanding of the codebase and moderate effort to implement correctly, justifying a score of 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Common errors intended for the form, rather than the fields, are not displayed.\nThe form's template output does not include display of errors intended for the form, rather than the fields. This can be done roughly like this:\r\n\r\n```jinja2\r\n{% extends \"admin/admin_action_wizard.html\" %}\r\n\r\n{% block field_sets %}\r\n    {{ block.super }}\r\n    {% if adminform.form.errors %}\r\n        {{ adminform.form.non_field_errors }}\r\n    {% endif %}\r\n{% endblock %}\r\n\r\n```\n", "patch": "diff --git a/src/admin_wizard/templates/admin/admin_action_wizard.html b/src/admin_wizard/templates/admin/admin_action_wizard.html\nindex 01e3366..1ab5bcb 100644\n--- a/src/admin_wizard/templates/admin/admin_action_wizard.html\n+++ b/src/admin_wizard/templates/admin/admin_action_wizard.html\n@@ -40,6 +40,9 @@\n                 {% for fieldset in adminform %}\n                     {% include \"admin/includes/fieldset.html\" %}\n                 {% endfor %}\n+                {% if adminform.form.errors %}\n+                    {{ adminform.form.non_field_errors }}\n+                {% endif %}\n             {% endblock %}\n             <input type=\"hidden\" name=\"action\" value=\"{{ action }}\"/>\n             <input type=\"hidden\" name=\"_redirect\" value=\"{{ redirect }}\"/>\ndiff --git a/src/admin_wizard/templates/admin/admin_update_wizard.html b/src/admin_wizard/templates/admin/admin_update_wizard.html\nindex 2814925..279b881 100644\n--- a/src/admin_wizard/templates/admin/admin_update_wizard.html\n+++ b/src/admin_wizard/templates/admin/admin_update_wizard.html\n@@ -23,9 +23,12 @@\n         <form action=\"\" method=\"post\">\n             {% csrf_token %}\n             {% block field_sets %}\n-            {% for fieldset in adminform %}\n-              {% include \"admin/includes/fieldset.html\" %}\n-            {% endfor %}\n+                {% for fieldset in adminform %}\n+                    {% include \"admin/includes/fieldset.html\" %}\n+                {% endfor %}\n+                {% if adminform.form.errors %}\n+                    {{ adminform.form.non_field_errors }}\n+                {% endif %}\n             {% endblock %}\n             {{ redirect_form }}\n             <input type=\"hidden\" name=\"action\" value=\"{{ action }}\"/>\n", "instance_id": "just-work__django-admin-wizard-56", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: form-level errors are not being displayed in the template output, and a solution is proposed using a Jinja2 template snippet to render non-field errors. The goal (displaying form-level errors) and the general approach are evident. However, there are minor ambiguities and missing details. For instance, the statement does not specify under what conditions or contexts these errors should be displayed, nor does it mention any potential side effects or constraints (e.g., styling, localization, or interaction with other template elements). Additionally, there are no examples of what constitutes a \"form-level error\" versus a \"field-level error,\" which could lead to minor confusion for someone unfamiliar with the framework or codebase. Overall, the problem is valid and mostly clear, but it lacks comprehensive details or edge case specifications, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are localized to two template files (`admin_action_wizard.html` and `admin_update_wizard.html`) and involve adding a small conditional block to render form-level errors. The modifications are straightforward, requiring only a few lines of code per file, and do not impact the broader system architecture or require understanding complex interactions between modules. The changes are purely presentational and confined to the template layer.\n\n2. **Number of Technical Concepts:** The solution requires basic familiarity with Jinja2 templating syntax (e.g., `{% if %}` conditionals and rendering variables like `adminform.form.non_field_errors`). It also assumes a rudimentary understanding of Django's form error handling (distinguishing between field and non-field errors). These concepts are not particularly complex for anyone with even introductory experience in Django or templating engines, making this a simple task.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention any edge cases or specific error conditions to handle beyond the general display of non-field errors. The code changes do not introduce new error handling logic; they simply render existing error data. There might be implicit considerations (e.g., ensuring the error display aligns with the UI design or handles empty error lists gracefully), but these are minor and do not significantly increase the difficulty.\n\n4. **Overall Complexity:** The task is a simple bug fix or feature addition that does not require deep knowledge of the codebase or advanced programming skills. It can be completed quickly by someone with basic knowledge of the tools involved.\n\nGiven these factors, I assign a difficulty score of 0.25, as the problem requires understanding some code logic (template rendering and form error structure) and making simple modifications, but it remains a straightforward task with minimal complexity or risk of unintended consequences.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": " New \"airflow assets\" CLI subcommand\n### Description\n\n[User Presentation Considerations](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-UserPresentationConsiderations)\n\n* [x] `airflow assets list` https://github.com/apache/airflow/pull/44387\n* [x] `airflow assets detail` https://github.com/apache/airflow/pull/44445\n* [x] `airflow assets --alias` to list asset aliases https://github.com/apache/airflow/pull/44595\n* [x] `airflow assets detail --alias` to view an asset alias https://github.com/apache/airflow/pull/44595\n* [x] `airflow assets materialize` https://github.com/apache/airflow/pull/44558\n    * something like `airflow dags trigger`\n    * steps\n        1. get an asset\n        2. find the tasks reference this asset as an outlet\n            1. raise an error if there's not\n            2. raise an error if there're more than one (as of now)\n        3. trigger that DAG\n\n### Use case/motivation\n\n[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "patch": "diff --git a/airflow/api_fastapi/core_api/datamodels/assets.py b/airflow/api_fastapi/core_api/datamodels/assets.py\nindex adc32c2e4808f..638ee1cba6e29 100644\n--- a/airflow/api_fastapi/core_api/datamodels/assets.py\n+++ b/airflow/api_fastapi/core_api/datamodels/assets.py\n@@ -53,7 +53,9 @@ class AssetResponse(BaseModel):\n     \"\"\"Asset serializer for responses.\"\"\"\n \n     id: int\n+    name: str\n     uri: str\n+    group: str\n     extra: dict | None = None\n     created_at: datetime\n     updated_at: datetime\ndiff --git a/airflow/api_fastapi/core_api/openapi/v1-generated.yaml b/airflow/api_fastapi/core_api/openapi/v1-generated.yaml\nindex 46fd0382eb982..25ade1d917195 100644\n--- a/airflow/api_fastapi/core_api/openapi/v1-generated.yaml\n+++ b/airflow/api_fastapi/core_api/openapi/v1-generated.yaml\n@@ -5737,9 +5737,15 @@ components:\n         id:\n           type: integer\n           title: Id\n+        name:\n+          type: string\n+          title: Name\n         uri:\n           type: string\n           title: Uri\n+        group:\n+          type: string\n+          title: Group\n         extra:\n           anyOf:\n           - type: object\n@@ -5771,7 +5777,9 @@ components:\n       type: object\n       required:\n       - id\n+      - name\n       - uri\n+      - group\n       - created_at\n       - updated_at\n       - consuming_dags\ndiff --git a/airflow/cli/cli_config.py b/airflow/cli/cli_config.py\nindex 5611004f83dff..d03ebd312600e 100644\n--- a/airflow/cli/cli_config.py\n+++ b/airflow/cli/cli_config.py\n@@ -974,6 +974,13 @@ def string_lower_type(val):\n     default=(\"dag_id\", \"fileloc\", \"owners\", \"is_paused\"),\n )\n \n+ARG_ASSET_LIST_COLUMNS = Arg(\n+    (\"--columns\",),\n+    type=string_list_type,\n+    help=\"List of columns to render. (default: ['name', 'uri', 'group', 'extra'])\",\n+    default=(\"name\", \"uri\", \"group\", \"extra\"),\n+)\n+\n ALTERNATIVE_CONN_SPECS_ARGS = [\n     ARG_CONN_TYPE,\n     ARG_CONN_DESCRIPTION,\n@@ -1009,6 +1016,14 @@ class GroupCommand(NamedTuple):\n \n CLICommand = Union[ActionCommand, GroupCommand]\n \n+ASSETS_COMMANDS = (\n+    ActionCommand(\n+        name=\"list\",\n+        help=\"List assets\",\n+        func=lazy_load_command(\"airflow.cli.commands.asset_command.asset_list\"),\n+        args=(ARG_OUTPUT, ARG_VERBOSE, ARG_ASSET_LIST_COLUMNS),\n+    ),\n+)\n BACKFILL_COMMANDS = (\n     ActionCommand(\n         name=\"create\",\n@@ -1856,6 +1871,11 @@ class GroupCommand(NamedTuple):\n         help=\"Manage tasks\",\n         subcommands=TASKS_COMMANDS,\n     ),\n+    GroupCommand(\n+        name=\"assets\",\n+        help=\"Manage assets\",\n+        subcommands=ASSETS_COMMANDS,\n+    ),\n     GroupCommand(\n         name=\"pools\",\n         help=\"Manage pools\",\ndiff --git a/airflow/cli/commands/asset_command.py b/airflow/cli/commands/asset_command.py\nnew file mode 100644\nindex 0000000000000..a43fe409021ba\n--- /dev/null\n+++ b/airflow/cli/commands/asset_command.py\n@@ -0,0 +1,53 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import logging\n+import typing\n+\n+from sqlalchemy import select\n+\n+from airflow.api_fastapi.core_api.datamodels.assets import AssetResponse\n+from airflow.cli.simple_table import AirflowConsole\n+from airflow.models.asset import AssetModel\n+from airflow.utils import cli as cli_utils\n+from airflow.utils.session import NEW_SESSION, provide_session\n+\n+if typing.TYPE_CHECKING:\n+    from typing import Any\n+\n+    from sqlalchemy.orm import Session\n+\n+log = logging.getLogger(__name__)\n+\n+\n+@cli_utils.action_cli\n+@provide_session\n+def asset_list(args, *, session: Session = NEW_SESSION) -> None:\n+    \"\"\"Display assets in the command line.\"\"\"\n+    assets = session.scalars(select(AssetModel).order_by(AssetModel.name))\n+\n+    def detail_mapper(asset: AssetModel) -> dict[str, Any]:\n+        model = AssetResponse.model_validate(asset)\n+        return model.model_dump(include=args.columns)\n+\n+    AirflowConsole().print_as(\n+        data=assets,\n+        output=args.output,\n+        mapper=detail_mapper,\n+    )\ndiff --git a/airflow/ui/openapi-gen/requests/schemas.gen.ts b/airflow/ui/openapi-gen/requests/schemas.gen.ts\nindex f657ba8d4e2d8..84c63a7d7bc34 100644\n--- a/airflow/ui/openapi-gen/requests/schemas.gen.ts\n+++ b/airflow/ui/openapi-gen/requests/schemas.gen.ts\n@@ -240,10 +240,18 @@ export const $AssetResponse = {\n       type: \"integer\",\n       title: \"Id\",\n     },\n+    name: {\n+      type: \"string\",\n+      title: \"Name\",\n+    },\n     uri: {\n       type: \"string\",\n       title: \"Uri\",\n     },\n+    group: {\n+      type: \"string\",\n+      title: \"Group\",\n+    },\n     extra: {\n       anyOf: [\n         {\n@@ -290,7 +298,9 @@ export const $AssetResponse = {\n   type: \"object\",\n   required: [\n     \"id\",\n+    \"name\",\n     \"uri\",\n+    \"group\",\n     \"created_at\",\n     \"updated_at\",\n     \"consuming_dags\",\ndiff --git a/airflow/ui/openapi-gen/requests/types.gen.ts b/airflow/ui/openapi-gen/requests/types.gen.ts\nindex e15de90441bb0..1adcdc8eba8f4 100644\n--- a/airflow/ui/openapi-gen/requests/types.gen.ts\n+++ b/airflow/ui/openapi-gen/requests/types.gen.ts\n@@ -68,7 +68,9 @@ export type AssetEventResponse = {\n  */\n export type AssetResponse = {\n   id: number;\n+  name: string;\n   uri: string;\n+  group: string;\n   extra?: {\n     [key: string]: unknown;\n   } | null;\n", "instance_id": "apache__airflow-44387", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement for the new \"airflow assets\" CLI subcommand is mostly clear, as it outlines the goal of adding a set of subcommands (`list`, `detail`, `materialize`, etc.) to manage assets in Apache Airflow. It provides a high-level description of the intended functionality, such as listing assets or triggering DAGs based on asset references, and includes links to related documentation and pull requests for context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input formats, detailed output formats, or specific constraints for each subcommand (e.g., what happens if an asset URI is invalid?). Additionally, while the `materialize` subcommand steps are outlined, they lack depth regarding error handling or edge cases (e.g., what if the DAG trigger fails?). These gaps prevent it from being fully comprehensive, but the overall intent and scope are understandable, especially with the provided code changes as context.", "difficulty_explanation": "The difficulty of this problem is rated as medium (0.50) due to several factors. First, the scope of code changes spans multiple files, including CLI configuration, API models, OpenAPI specifications, and UI type definitions, indicating a need to understand interactions across different parts of the Apache Airflow codebase. However, the changes themselves are relatively straightforward, primarily involving adding new fields (`name` and `group`) to asset models, updating schemas, and implementing a basic CLI command (`asset_list`) to display assets. The technical concepts required are moderate, including familiarity with Python, SQLAlchemy for database queries, CLI development using Airflow's utilities, and OpenAPI schema updates. While the problem does not appear to impact core system architecture significantly, it does require careful integration into the existing CLI framework and API structure. Edge cases and error handling are mentioned in the problem statement for the `materialize` subcommand (e.g., raising errors for no or multiple referencing tasks), but the provided code changes (focused on `list`) do not address complex error handling or performance considerations. Overall, this task requires a solid understanding of multiple components and moderate effort to implement and test, but it does not involve highly complex logic or deep architectural changes, placing it in the medium difficulty range.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ocs container builds are failing due to bug in setuptools\nBuilds currently fail with the following error:\r\n```\r\n$ docker build --no-cache -t ocs .\r\n\r\n...snip....\r\n\r\nStep 14/16 : RUN pip3 install .\r\n ---> Running in 41f49f20402f\r\nProcessing /app/ocs\r\n  Installing build dependencies: started\r\n  Installing build dependencies: finished with status 'done'\r\n  Getting requirements to build wheel: started\r\n  Getting requirements to build wheel: finished with status 'error'\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 /tmp/tmpx1r3y0c0 get_requires_for_build_wheel /tmp/tmpya52bdh4\r\n       cwd: /tmp/pip-req-build-8uhb4nj9\r\n  Complete output (49 lines):\r\n  Warning: build in /tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages is using versioneer.py from /tmp/pip-req-build-8uhb4nj9/versioneer.py\r\n  running egg_info\r\n  writing ocs.egg-info/PKG-INFO\r\n  writing dependency_links to ocs.egg-info/dependency_links.txt\r\n  writing entry points to ocs.egg-info/entry_points.txt\r\n  writing requirements to ocs.egg-info/requires.txt\r\n  writing top-level names to ocs.egg-info/top_level.txt\r\n  reading manifest file 'ocs.egg-info/SOURCES.txt'\r\n  reading manifest template 'MANIFEST.in'\r\n  adding license file 'LICENSE.txt'\r\n  Traceback (most recent call last):\r\n    File \"/tmp/tmpx1r3y0c0\", line 280, in <module>\r\n      main()\r\n    File \"/tmp/tmpx1r3y0c0\", line 263, in main\r\n      json_out['return_val'] = hook(**hook_input['kwargs'])\r\n    File \"/tmp/tmpx1r3y0c0\", line 114, in get_requires_for_build_wheel\r\n      return hook(config_settings)\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 327, in get_requires_for_build_wheel\r\n      return self._get_build_requires(config_settings, requirements=[])\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 297, in _get_build_requires\r\n      self.run_setup()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 313, in run_setup\r\n      exec(code, locals())\r\n    File \"<string>\", line 12, in <module>\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/__init__.py\", line 108, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 184, in setup\r\n      return run_commands(dist)\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\r\n      dist.run_commands()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\r\n      self.run_command(cmd)\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 945, in run_command\r\n      super().run_command(command)\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n      cmd_obj.run()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/command/egg_info.py\", line 310, in run\r\n      self.find_sources()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/command/egg_info.py\", line 318, in find_sources\r\n      mm.run()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/command/egg_info.py\", line 544, in run\r\n      self.prune_file_list()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/command/egg_info.py\", line 610, in prune_file_list\r\n      base_dir = self.distribution.get_fullname()\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/_core_metadata.py\", line 266, in get_fullname\r\n      return _distribution_fullname(self.get_name(), self.get_version())\r\n    File \"/tmp/pip-build-env-a5hyznc_/overlay/lib/python3.8/site-packages/setuptools/_core_metadata.py\", line 284, in _distribution_fullname\r\n      canonicalize_version(version, strip_trailing_zero=False),\r\n  TypeError: canonicalize_version() got an unexpected keyword argument 'strip_trailing_zero'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/bin/python3 /tmp/tmpx1r3y0c0 get_requires_for_build_wheel /tmp/tmpya52bdh4 Check the logs for full command output.\r\nThe command '/bin/sh -c pip3 install .' returned a non-zero code: 1\r\n```\r\n\r\nThis was reported upstream here: https://github.com/pypa/setuptools/issues/4483/\r\n\r\nPinning setuptools below 72.0.0 resolves the issue for now, but we should keep an eye on this getting fixed and unpin when possible.\n", "patch": "diff --git a/.flake8 b/.flake8\nindex 75a32ade..42c79fcc 100644\n--- a/.flake8\n+++ b/.flake8\n@@ -11,6 +11,4 @@ in-place = true\n recursive = true\n aggressive = 2\n exclude =\n-    ocs/_version.py,\n-    versioneer.py,\n     docs/conf.py,\ndiff --git a/.gitattributes b/.gitattributes\ndeleted file mode 100644\nindex 85cf7c7f..00000000\n--- a/.gitattributes\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-ocs/_version.py export-subst\ndiff --git a/.gitignore b/.gitignore\nindex cd8f9301..222cf16e 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -1,3 +1,6 @@\n+# dynamic package version\n+ocs/_version.py\n+\n # local sqlite db's\n *.db\n \ndiff --git a/MANIFEST.in b/MANIFEST.in\ndeleted file mode 100644\nindex 5586378b..00000000\n--- a/MANIFEST.in\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-include versioneer.py\n-include ocs/_version.py\ndiff --git a/docs/conf.py b/docs/conf.py\nindex de4b1eb7..4ee7d37e 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -46,6 +46,9 @@\n ]\n extensions += ['sphinxarg.ext']\n \n+# support inline tabs\n+extensions += ['sphinx_inline_tabs']\n+\n # Present auto-documented members in source order (rather than alphabetical).\n autodoc_default_options = {\n     'member-order': 'bysource',\ndiff --git a/docs/developer/writing-plugins.rst b/docs/developer/writing-plugins.rst\nindex eda53103..3d0207ff 100644\n--- a/docs/developer/writing-plugins.rst\n+++ b/docs/developer/writing-plugins.rst\n@@ -37,29 +37,47 @@ within the ``__main__`` block.\n Package Entry Point\n -------------------\n After you have created the plugin file, you need to add an entry point to\n-``setup.py``. This will register your package for discovery by ocs. This looks\n-like (assuming the plugin file is called ``plugin.py`` and lives at the top\n-level of the package):\n+``pyproject.toml`` or ``setup.py``. This will register your package for\n+discovery by ocs. This looks like (assuming the plugin file is called\n+``plugin.py`` and lives at the top level of the package):\n \n-.. code-block:: python\n+.. tab:: pyproject.toml\n+\n+    .. code-block::\n+\n+        [project.entry-points.'ocs.plugins']\n+        <plugin name> = '<package name>.plugin'\n+\n+.. tab:: setup.py\n+\n+    .. code-block:: python\n \n-       entry_points={\n-           'ocs.plugins': [\n-               '<plugin name> = <package name>.plugin',\n-               ],\n-       },\n+           entry_points={\n+               'ocs.plugins': [\n+                   '<plugin name> = <package name>.plugin',\n+                   ],\n+           },\n \n-Plugin name should just match the package name, however the group name must\n-always be ``ocs.plugins`` in order for OCS to recognize the plugin. For\n+Plugin name should typically match the package name, however the group name\n+must always be ``ocs.plugins`` in order for OCS to recognize the plugin. For\n example, if your package was called ``my_ocs_pkg`` this would be:\n \n-.. code-block:: python\n+.. tab:: pyproject.toml\n+\n+    .. code-block::\n+\n+        [project.entry-points.'ocs.plugins']\n+        my_ocs_pkg = 'my_ocs_pkg.plugin'\n+\n+.. tab:: setup.py\n+\n+    .. code-block:: python\n \n-       entry_points={\n-           'ocs.plugins': [\n-               'my_ocs_pkg = my_ocs_pkg.plugin',\n-               ],\n-       },\n+           entry_points={\n+               'ocs.plugins': [\n+                   'my_ocs_pkg = my_ocs_pkg.plugin',\n+                   ],\n+           },\n \n Agent Entry Point\n -----------------\ndiff --git a/docs/requirements.txt b/docs/requirements.txt\nindex 4609b7f8..e525c1a8 100644\n--- a/docs/requirements.txt\n+++ b/docs/requirements.txt\n@@ -1,3 +1,4 @@\n sphinx>=5.0.0\n sphinx_rtd_theme>=2.0.0\n sphinx-argparse>=0.4.0\n+sphinx-inline-tabs\ndiff --git a/ocs/__init__.py b/ocs/__init__.py\nindex aaadc156..2cdec324 100644\n--- a/ocs/__init__.py\n+++ b/ocs/__init__.py\n@@ -2,5 +2,25 @@\n \n from . import site_config  # noqa: F401\n \n-from . import _version\n-__version__ = _version.get_versions()['version']\n+# Define the variable '__version__':\n+# This has the closest behavior to versioneer that I could find\n+# https://github.com/maresb/hatch-vcs-footgun-example\n+try:\n+    # If setuptools_scm is installed (e.g. in a development environment with\n+    # an editable install), then use it to determine the version dynamically.\n+    from setuptools_scm import get_version\n+\n+    # This will fail with LookupError if the package is not installed in\n+    # editable mode or if Git is not installed.\n+    __version__ = get_version(root=\"..\", relative_to=__file__, version_scheme=\"no-guess-dev\")\n+except (ImportError, LookupError):\n+    # As a fallback, use the version that is hard-coded in the file.\n+    try:\n+        from ocs._version import __version__  # noqa: F401\n+    except ModuleNotFoundError:\n+        # The user is probably trying to run this without having installed\n+        # the package, so complain.\n+        raise RuntimeError(\n+            \"ocs is not correctly installed. \"\n+            \"Please install it with pip.\"\n+        )\ndiff --git a/ocs/_version.py b/ocs/_version.py\ndeleted file mode 100644\nindex 1fa73019..00000000\n--- a/ocs/_version.py\n+++ /dev/null\n@@ -1,683 +0,0 @@\n-\n-# This file helps to compute a version number in source trees obtained from\n-# git-archive tarball (such as those provided by githubs download-from-tag\n-# feature). Distribution tarballs (built by setup.py sdist) and build\n-# directories (produced by setup.py build) will contain a much shorter file\n-# that just contains the computed version number.\n-\n-# This file is released into the public domain.\n-# Generated by versioneer-0.29\n-# https://github.com/python-versioneer/python-versioneer\n-\n-\"\"\"Git implementation of _version.py.\"\"\"\n-\n-import errno\n-import os\n-import re\n-import subprocess\n-import sys\n-from typing import Any, Callable, Dict, List, Optional, Tuple\n-import functools\n-\n-\n-def get_keywords() -> Dict[str, str]:\n-    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n-    # these strings will be replaced by git during git-archive.\n-    # setup.py/versioneer.py will grep for the variable names, so they must\n-    # each be defined on a line of their own. _version.py will just call\n-    # get_keywords().\n-    git_refnames = \"$Format:%d$\"\n-    git_full = \"$Format:%H$\"\n-    git_date = \"$Format:%ci$\"\n-    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n-    return keywords\n-\n-\n-class VersioneerConfig:\n-    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n-\n-    VCS: str\n-    style: str\n-    tag_prefix: str\n-    parentdir_prefix: str\n-    versionfile_source: str\n-    verbose: bool\n-\n-\n-def get_config() -> VersioneerConfig:\n-    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n-    # these strings are filled in when 'setup.py versioneer' creates\n-    # _version.py\n-    cfg = VersioneerConfig()\n-    cfg.VCS = \"git\"\n-    cfg.style = \"pep440\"\n-    cfg.tag_prefix = \"v\"\n-    cfg.parentdir_prefix = \"ocs-\"\n-    cfg.versionfile_source = \"ocs/_version.py\"\n-    cfg.verbose = False\n-    return cfg\n-\n-\n-class NotThisMethod(Exception):\n-    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n-\n-\n-LONG_VERSION_PY: Dict[str, str] = {}\n-HANDLERS: Dict[str, Dict[str, Callable]] = {}\n-\n-\n-def register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n-    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n-    def decorate(f: Callable) -> Callable:\n-        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        if vcs not in HANDLERS:\n-            HANDLERS[vcs] = {}\n-        HANDLERS[vcs][method] = f\n-        return f\n-    return decorate\n-\n-\n-def run_command(\n-    commands: List[str],\n-    args: List[str],\n-    cwd: Optional[str] = None,\n-    verbose: bool = False,\n-    hide_stderr: bool = False,\n-    env: Optional[Dict[str, str]] = None,\n-) -> Tuple[Optional[str], Optional[int]]:\n-    \"\"\"Call the given command(s).\"\"\"\n-    assert isinstance(commands, list)\n-    process = None\n-\n-    popen_kwargs: Dict[str, Any] = {}\n-    if sys.platform == \"win32\":\n-        # This hides the console window if pythonw.exe is used\n-        startupinfo = subprocess.STARTUPINFO()\n-        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n-        popen_kwargs[\"startupinfo\"] = startupinfo\n-\n-    for command in commands:\n-        try:\n-            dispcmd = str([command] + args)\n-            # remember shell=False, so use git.cmd on windows, not just git\n-            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n-                                       stdout=subprocess.PIPE,\n-                                       stderr=(subprocess.PIPE if hide_stderr\n-                                               else None), **popen_kwargs)\n-            break\n-        except OSError as e:\n-            if e.errno == errno.ENOENT:\n-                continue\n-            if verbose:\n-                print(\"unable to run %s\" % dispcmd)\n-                print(e)\n-            return None, None\n-    else:\n-        if verbose:\n-            print(\"unable to find command, tried %s\" % (commands,))\n-        return None, None\n-    stdout = process.communicate()[0].strip().decode()\n-    if process.returncode != 0:\n-        if verbose:\n-            print(\"unable to run %s (error)\" % dispcmd)\n-            print(\"stdout was %s\" % stdout)\n-        return None, process.returncode\n-    return stdout, process.returncode\n-\n-\n-def versions_from_parentdir(\n-    parentdir_prefix: str,\n-    root: str,\n-    verbose: bool,\n-) -> Dict[str, Any]:\n-    \"\"\"Try to determine the version from the parent directory name.\n-\n-    Source tarballs conventionally unpack into a directory that includes both\n-    the project name and a version string. We will also support searching up\n-    two directory levels for an appropriately named parent directory\n-    \"\"\"\n-    rootdirs = []\n-\n-    for _ in range(3):\n-        dirname = os.path.basename(root)\n-        if dirname.startswith(parentdir_prefix):\n-            return {\"version\": dirname[len(parentdir_prefix):],\n-                    \"full-revisionid\": None,\n-                    \"dirty\": False, \"error\": None, \"date\": None}\n-        rootdirs.append(root)\n-        root = os.path.dirname(root)  # up a level\n-\n-    if verbose:\n-        print(\"Tried directories %s but none started with prefix %s\" %\n-              (str(rootdirs), parentdir_prefix))\n-    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n-\n-\n-@register_vcs_handler(\"git\", \"get_keywords\")\n-def git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n-    \"\"\"Extract version information from the given file.\"\"\"\n-    # the code embedded in _version.py can just fetch the value of these\n-    # keywords. When used from setup.py, we don't want to import _version.py,\n-    # so we do it with a regexp instead. This function is not used from\n-    # _version.py.\n-    keywords: Dict[str, str] = {}\n-    try:\n-        with open(versionfile_abs, \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(\"git_refnames =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"refnames\"] = mo.group(1)\n-                if line.strip().startswith(\"git_full =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"full\"] = mo.group(1)\n-                if line.strip().startswith(\"git_date =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"date\"] = mo.group(1)\n-    except OSError:\n-        pass\n-    return keywords\n-\n-\n-@register_vcs_handler(\"git\", \"keywords\")\n-def git_versions_from_keywords(\n-    keywords: Dict[str, str],\n-    tag_prefix: str,\n-    verbose: bool,\n-) -> Dict[str, Any]:\n-    \"\"\"Get version information from git keywords.\"\"\"\n-    if \"refnames\" not in keywords:\n-        raise NotThisMethod(\"Short version file found\")\n-    date = keywords.get(\"date\")\n-    if date is not None:\n-        # Use only the last line.  Previous lines may contain GPG signature\n-        # information.\n-        date = date.splitlines()[-1]\n-\n-        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n-        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n-        # -like\" string, which we must then edit to make compliant), because\n-        # it's been around since git-1.5.3, and it's too difficult to\n-        # discover which version we're using, or to work around using an\n-        # older one.\n-        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-    refnames = keywords[\"refnames\"].strip()\n-    if refnames.startswith(\"$Format\"):\n-        if verbose:\n-            print(\"keywords are unexpanded, not using\")\n-        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n-    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n-    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n-    TAG = \"tag: \"\n-    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n-    if not tags:\n-        # Either we're using git < 1.8.3, or there really are no tags. We use\n-        # a heuristic: assume all version tags have a digit. The old git %d\n-        # expansion behaves like git log --decorate=short and strips out the\n-        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n-        # between branches and tags. By ignoring refnames without digits, we\n-        # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = {r for r in refs if re.search(r'\\d', r)}\n-        if verbose:\n-            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n-    if verbose:\n-        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n-    for ref in sorted(tags):\n-        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n-        if ref.startswith(tag_prefix):\n-            r = ref[len(tag_prefix):]\n-            # Filter out refs that exactly match prefix or that don't start\n-            # with a number once the prefix is stripped (mostly a concern\n-            # when prefix is '')\n-            if not re.match(r'\\d', r):\n-                continue\n-            if verbose:\n-                print(\"picking %s\" % r)\n-            return {\"version\": r,\n-                    \"full-revisionid\": keywords[\"full\"].strip(),\n-                    \"dirty\": False, \"error\": None,\n-                    \"date\": date}\n-    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n-    if verbose:\n-        print(\"no suitable tags, using unknown + full revision id\")\n-    return {\"version\": \"0+unknown\",\n-            \"full-revisionid\": keywords[\"full\"].strip(),\n-            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n-\n-\n-@register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(\n-    tag_prefix: str,\n-    root: str,\n-    verbose: bool,\n-    runner: Callable = run_command\n-) -> Dict[str, Any]:\n-    \"\"\"Get version from 'git describe' in the root of the source tree.\n-\n-    This only gets called if the git-archive 'subst' keywords were *not*\n-    expanded, and _version.py hasn't already been rewritten with a short\n-    version string, meaning we're inside a checked out source tree.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-\n-    # GIT_DIR can interfere with correct operation of Versioneer.\n-    # It may be intended to be passed to the Versioneer-versioned project,\n-    # but that should not change where we get our version from.\n-    env = os.environ.copy()\n-    env.pop(\"GIT_DIR\", None)\n-    runner = functools.partial(runner, env=env)\n-\n-    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                   hide_stderr=not verbose)\n-    if rc != 0:\n-        if verbose:\n-            print(\"Directory %s not under git control\" % root)\n-        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n-\n-    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n-    # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n-        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n-    ], cwd=root)\n-    # --long was added in git-1.5.5\n-    if describe_out is None:\n-        raise NotThisMethod(\"'git describe' failed\")\n-    describe_out = describe_out.strip()\n-    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n-    if full_out is None:\n-        raise NotThisMethod(\"'git rev-parse' failed\")\n-    full_out = full_out.strip()\n-\n-    pieces: Dict[str, Any] = {}\n-    pieces[\"long\"] = full_out\n-    pieces[\"short\"] = full_out[:7]  # maybe improved later\n-    pieces[\"error\"] = None\n-\n-    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n-                             cwd=root)\n-    # --abbrev-ref was added in git-1.6.3\n-    if rc != 0 or branch_name is None:\n-        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n-    branch_name = branch_name.strip()\n-\n-    if branch_name == \"HEAD\":\n-        # If we aren't exactly on a branch, pick a branch which represents\n-        # the current commit. If all else fails, we are on a branchless\n-        # commit.\n-        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n-        # --contains was added in git-1.5.4\n-        if rc != 0 or branches is None:\n-            raise NotThisMethod(\"'git branch --contains' returned error\")\n-        branches = branches.split(\"\\n\")\n-\n-        # Remove the first line if we're running detached\n-        if \"(\" in branches[0]:\n-            branches.pop(0)\n-\n-        # Strip off the leading \"* \" from the list of branches.\n-        branches = [branch[2:] for branch in branches]\n-        if \"master\" in branches:\n-            branch_name = \"master\"\n-        elif not branches:\n-            branch_name = None\n-        else:\n-            # Pick the first branch that is returned. Good or bad.\n-            branch_name = branches[0]\n-\n-    pieces[\"branch\"] = branch_name\n-\n-    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n-    # TAG might have hyphens.\n-    git_describe = describe_out\n-\n-    # look for -dirty suffix\n-    dirty = git_describe.endswith(\"-dirty\")\n-    pieces[\"dirty\"] = dirty\n-    if dirty:\n-        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n-\n-    # now we have TAG-NUM-gHEX or HEX\n-\n-    if \"-\" in git_describe:\n-        # TAG-NUM-gHEX\n-        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n-        if not mo:\n-            # unparsable. Maybe git-describe is misbehaving?\n-            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n-                               % describe_out)\n-            return pieces\n-\n-        # tag\n-        full_tag = mo.group(1)\n-        if not full_tag.startswith(tag_prefix):\n-            if verbose:\n-                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n-                print(fmt % (full_tag, tag_prefix))\n-            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n-                               % (full_tag, tag_prefix))\n-            return pieces\n-        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n-\n-        # distance: number of commits since tag\n-        pieces[\"distance\"] = int(mo.group(2))\n-\n-        # commit: short hex revision ID\n-        pieces[\"short\"] = mo.group(3)\n-\n-    else:\n-        # HEX: no tags\n-        pieces[\"closest-tag\"] = None\n-        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n-        pieces[\"distance\"] = len(out.split())  # total number of commits\n-\n-    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n-    # Use only the last line.  Previous lines may contain GPG signature\n-    # information.\n-    date = date.splitlines()[-1]\n-    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-\n-    return pieces\n-\n-\n-def plus_or_dot(pieces: Dict[str, Any]) -> str:\n-    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n-    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n-        return \".\"\n-    return \"+\"\n-\n-\n-def render_pep440(pieces: Dict[str, Any]) -> str:\n-    \"\"\"Build up version string, with post-release \"local version identifier\".\n-\n-    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n-    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n-\n-    Exceptions:\n-    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_branch(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n-    (a feature branch will appear \"older\" than the master branch).\n-\n-    Exceptions:\n-    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0\"\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n-    \"\"\"Split pep440 version string at the post-release segment.\n-\n-    Returns the release segments before the post-release and the\n-    post-release version number (or -1 if no post-release segment is present).\n-    \"\"\"\n-    vc = str.split(ver, \".post\")\n-    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n-\n-\n-def render_pep440_pre(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.post0.devDISTANCE\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        if pieces[\"distance\"]:\n-            # update the post release segment\n-            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n-            rendered = tag_version\n-            if post_version is not None:\n-                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n-            else:\n-                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n-        else:\n-            # no commits, use the tag as the version\n-            rendered = pieces[\"closest-tag\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n-    return rendered\n-\n-\n-def render_pep440_post(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n-\n-    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n-    (a dirty tree will appear \"older\" than the corresponding clean one),\n-    but you shouldn't be releasing software with -dirty anyways.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-    return rendered\n-\n-\n-def render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_old(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]] .\n-\n-    The \".dev0\" means dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-    return rendered\n-\n-\n-def render_git_describe(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n-\n-    Like 'git describe --tags --dirty --always'.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"]:\n-            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render_git_describe_long(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n-\n-    Like 'git describe --tags --dirty --always -long'.\n-    The distance/hash is unconditional.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n-    \"\"\"Render the given version pieces into the requested style.\"\"\"\n-    if pieces[\"error\"]:\n-        return {\"version\": \"unknown\",\n-                \"full-revisionid\": pieces.get(\"long\"),\n-                \"dirty\": None,\n-                \"error\": pieces[\"error\"],\n-                \"date\": None}\n-\n-    if not style or style == \"default\":\n-        style = \"pep440\"  # the default\n-\n-    if style == \"pep440\":\n-        rendered = render_pep440(pieces)\n-    elif style == \"pep440-branch\":\n-        rendered = render_pep440_branch(pieces)\n-    elif style == \"pep440-pre\":\n-        rendered = render_pep440_pre(pieces)\n-    elif style == \"pep440-post\":\n-        rendered = render_pep440_post(pieces)\n-    elif style == \"pep440-post-branch\":\n-        rendered = render_pep440_post_branch(pieces)\n-    elif style == \"pep440-old\":\n-        rendered = render_pep440_old(pieces)\n-    elif style == \"git-describe\":\n-        rendered = render_git_describe(pieces)\n-    elif style == \"git-describe-long\":\n-        rendered = render_git_describe_long(pieces)\n-    else:\n-        raise ValueError(\"unknown style '%s'\" % style)\n-\n-    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n-            \"dirty\": pieces[\"dirty\"], \"error\": None,\n-            \"date\": pieces.get(\"date\")}\n-\n-\n-def get_versions() -> Dict[str, Any]:\n-    \"\"\"Get version information or return default if unable to do so.\"\"\"\n-    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n-    # __file__, we can work backwards from there to the root. Some\n-    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n-    # case we can only use expanded keywords.\n-\n-    cfg = get_config()\n-    verbose = cfg.verbose\n-\n-    try:\n-        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n-                                          verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        root = os.path.realpath(__file__)\n-        # versionfile_source is the relative path from the top of the source\n-        # tree (where the .git directory might live) to this file. Invert\n-        # this to find the root from __file__.\n-        for _ in cfg.versionfile_source.split('/'):\n-            root = os.path.dirname(root)\n-    except NameError:\n-        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-                \"dirty\": None,\n-                \"error\": \"unable to find root of source tree\",\n-                \"date\": None}\n-\n-    try:\n-        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n-        return render(pieces, cfg.style)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        if cfg.parentdir_prefix:\n-            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-            \"dirty\": None,\n-            \"error\": \"unable to compute version\", \"date\": None}\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 1dc0ea7a..ef6fd36d 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,11 +1,83 @@\n [build-system]\n requires = [\n-    # Upper bound needed until https://github.com/pypa/setuptools/issues/4483/ is fixed\n-    \"setuptools>=42,<71.0.0\",\n-    \"wheel\",\n-    \"versioneer-518\",\n+    \"hatch-vcs\",\n+    \"hatchling\",\n ]\n-build-backend = \"setuptools.build_meta\"\n+build-backend = \"hatchling.build\"\n \n-[tool.isort]\n-skip = [\"versioneer.py\", \"socs/_version.py\"]\n+[project]\n+name = \"ocs\"\n+dynamic = [\"version\"]\n+description = \"Observatory Control System\"\n+readme = \"README.rst\"\n+license = {file = \"LICENSE.txt\"}\n+requires-python = \">=3.7\"\n+classifiers = [\n+    \"Framework :: Twisted\",\n+    \"Intended Audience :: Science/Research\",\n+    \"License :: OSI Approved :: BSD License\",\n+    \"Programming Language :: Python :: 3\",\n+    \"Programming Language :: Python :: 3.7\",\n+    \"Programming Language :: Python :: 3.8\",\n+    \"Programming Language :: Python :: 3.9\",\n+    \"Programming Language :: Python :: 3.10\",\n+    \"Programming Language :: Python :: 3.11\",\n+    \"Topic :: Scientific/Engineering :: Astronomy\",\n+]\n+dependencies = [\n+    \"autobahn\",\n+    \"deprecation\",\n+    \"importlib_metadata;python_version<\\\"3.10\\\"\",\n+    \"influxdb\",\n+    \"numpy<2.0\",  # pin until 2.0 is supported in so3g\n+    \"PyYAML\",\n+    \"setproctitle\",\n+    \"twisted\",\n+]\n+\n+[project.entry-points.\"ocs.plugins\"]\n+ocs = \"ocs.plugin\"\n+\n+[project.optional-dependencies]\n+dev = [\n+    \"coverage\",\n+    \"docker\",\n+    \"pytest\",\n+    \"pytest-cov\",\n+    \"pytest-docker\",\n+    \"pytest-twisted\",\n+    \"setuptools-scm\",\n+    \"so3g\",\n+]\n+so3g = [\n+    \"so3g\",\n+]\n+\n+[project.scripts]\n+ocs-agent-cli = \"ocs.agent_cli:main\"\n+ocs-client-cli = \"ocs.client_cli:main\"\n+ocs-install-systemd = \"ocs.ocs_systemd:main\"\n+ocs-local-support = \"ocs.ocsbow:main_local\"\n+ocsbow = \"ocs.ocsbow:main\"\n+\n+[project.urls]\n+\"Bug Tracker\" = \"https://github.com/simonsobs/ocs/issues\"\n+Documentation = \"https://ocs.readthedocs.io/\"\n+Homepage = \"https://github.com/simonsobs/ocs\"\n+\"Source Code\" = \"https://github.com/simonsobs/ocs\"\n+\n+[tool.hatch.version]\n+source = \"vcs\"\n+\n+# closest scheme to versioneer behavior, avoids auto-incremented version number\n+# https://setuptools-scm.readthedocs.io/en/latest/usage/#default-versioning-scheme\n+[tool.hatch.version.raw-options]\n+version_scheme = \"no-guess-dev\"\n+\n+[tool.hatch.build.hooks.vcs]\n+version-file = \"ocs/_version.py\"\n+\n+[tool.hatch.build.targets.sdist]\n+include = [\n+    \"/ocs\",\n+]\ndiff --git a/setup.cfg b/setup.cfg\ndeleted file mode 100644\nindex e2ebcc81..00000000\n--- a/setup.cfg\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-[versioneer]\n-VCS = git\n-style = pep440\n-versionfile_source = ocs/_version.py\n-versionfile_build = ocs/_version.py\n-tag_prefix = v\n-parentdir_prefix = ocs-\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex d5172f90..00000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,67 +0,0 @@\n-from setuptools import setup, find_packages\n-\n-import versioneer\n-\n-with open(\"README.rst\", \"r\", encoding=\"utf-8\") as fh:\n-    long_description = fh.read()\n-\n-so3g_etxras = [\"so3g\"]\n-dev_extras = [\"pytest\", \"pytest-twisted\", \"pytest-docker\", \"pytest-cov\", \"coverage\", \"docker\"]\n-dev_extras.extend(so3g_etxras)\n-\n-setup(name='ocs',\n-      description='Observatory Control System',\n-      long_description=long_description,\n-      long_description_content_type=\"text/x-rst\",\n-      package_dir={'ocs': 'ocs'},\n-      packages=find_packages(include=['ocs', 'ocs.*']),\n-      scripts=[],\n-      entry_points={\n-           'console_scripts': [\n-               'ocsbow=ocs.ocsbow:main',\n-               'ocs-local-support=ocs.ocsbow:main_local',\n-               'ocs-client-cli=ocs.client_cli:main',\n-               'ocs-agent-cli=ocs.agent_cli:main',\n-               'ocs-install-systemd=ocs.ocs_systemd:main',\n-           ],\n-          'ocs.plugins': [\n-               'ocs = ocs.plugin',\n-           ],\n-      },\n-      package_data={'': ['support/*json']},\n-      version=versioneer.get_version(),\n-      cmdclass=versioneer.get_cmdclass(),\n-      url=\"https://github.com/simonsobs/ocs\",\n-      project_urls={\n-          \"Source Code\": \"https://github.com/simonsobs/ocs\",\n-          \"Documentation\": \"https://ocs.readthedocs.io/\",\n-          \"Bug Tracker\": \"https://github.com/simonsobs/ocs/issues\",\n-      },\n-      classifiers=[\n-          \"Programming Language :: Python :: 3\",\n-          \"Programming Language :: Python :: 3.7\",\n-          \"Programming Language :: Python :: 3.8\",\n-          \"Programming Language :: Python :: 3.9\",\n-          \"Programming Language :: Python :: 3.10\",\n-          \"Programming Language :: Python :: 3.11\",\n-          \"License :: OSI Approved :: BSD License\",\n-          \"Intended Audience :: Science/Research\",\n-          \"Topic :: Scientific/Engineering :: Astronomy\",\n-          \"Framework :: Twisted\",\n-      ],\n-      python_requires=\">=3.7\",\n-      install_requires=[\n-          'autobahn',\n-          'twisted',\n-          'deprecation',\n-          'PyYAML',\n-          'influxdb',\n-          'numpy<2.0',  # pin until 2.0 is supported in so3g\n-          'importlib_metadata;python_version<\"3.10\"',\n-          'setproctitle',\n-      ],\n-      extras_require={\n-          \"so3g\": so3g_etxras,\n-          \"dev\": dev_extras,\n-      },\n-      )\ndiff --git a/versioneer.py b/versioneer.py\ndeleted file mode 100644\nindex 1e3753e6..00000000\n--- a/versioneer.py\n+++ /dev/null\n@@ -1,2277 +0,0 @@\n-\n-# Version: 0.29\n-\n-\"\"\"The Versioneer - like a rocketeer, but for versions.\n-\n-The Versioneer\n-==============\n-\n-* like a rocketeer, but for versions!\n-* https://github.com/python-versioneer/python-versioneer\n-* Brian Warner\n-* License: Public Domain (Unlicense)\n-* Compatible with: Python 3.7, 3.8, 3.9, 3.10, 3.11 and pypy3\n-* [![Latest Version][pypi-image]][pypi-url]\n-* [![Build Status][travis-image]][travis-url]\n-\n-This is a tool for managing a recorded version number in setuptools-based\n-python projects. The goal is to remove the tedious and error-prone \"update\n-the embedded version string\" step from your release process. Making a new\n-release should be as easy as recording a new tag in your version-control\n-system, and maybe making new tarballs.\n-\n-\n-## Quick Install\n-\n-Versioneer provides two installation modes. The \"classic\" vendored mode installs\n-a copy of versioneer into your repository. The experimental build-time dependency mode\n-is intended to allow you to skip this step and simplify the process of upgrading.\n-\n-### Vendored mode\n-\n-* `pip install versioneer` to somewhere in your $PATH\n-   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n-     available, so you can also use `conda install -c conda-forge versioneer`\n-* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n-  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n-   * Note that you will need to add `tomli; python_version < \"3.11\"` to your\n-     build-time dependencies if you use `pyproject.toml`\n-* run `versioneer install --vendor` in your source tree, commit the results\n-* verify version information with `python setup.py version`\n-\n-### Build-time dependency mode\n-\n-* `pip install versioneer` to somewhere in your $PATH\n-   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n-     available, so you can also use `conda install -c conda-forge versioneer`\n-* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n-  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n-* add `versioneer` (with `[toml]` extra, if configuring in `pyproject.toml`)\n-  to the `requires` key of the `build-system` table in `pyproject.toml`:\n-  ```toml\n-  [build-system]\n-  requires = [\"setuptools\", \"versioneer[toml]\"]\n-  build-backend = \"setuptools.build_meta\"\n-  ```\n-* run `versioneer install --no-vendor` in your source tree, commit the results\n-* verify version information with `python setup.py version`\n-\n-## Version Identifiers\n-\n-Source trees come from a variety of places:\n-\n-* a version-control system checkout (mostly used by developers)\n-* a nightly tarball, produced by build automation\n-* a snapshot tarball, produced by a web-based VCS browser, like github's\n-  \"tarball from tag\" feature\n-* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n-\n-Within each source tree, the version identifier (either a string or a number,\n-this tool is format-agnostic) can come from a variety of places:\n-\n-* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n-  about recent \"tags\" and an absolute revision-id\n-* the name of the directory into which the tarball was unpacked\n-* an expanded VCS keyword ($Id$, etc)\n-* a `_version.py` created by some earlier build step\n-\n-For released software, the version identifier is closely related to a VCS\n-tag. Some projects use tag names that include more than just the version\n-string (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\n-needs to strip the tag prefix to extract the version identifier. For\n-unreleased software (between tags), the version identifier should provide\n-enough information to help developers recreate the same tree, while also\n-giving them an idea of roughly how old the tree is (after version 1.2, before\n-version 1.3). Many VCS systems can report a description that captures this,\n-for example `git describe --tags --dirty --always` reports things like\n-\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n-0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\n-uncommitted changes).\n-\n-The version identifier is used for multiple purposes:\n-\n-* to allow the module to self-identify its version: `myproject.__version__`\n-* to choose a name and prefix for a 'setup.py sdist' tarball\n-\n-## Theory of Operation\n-\n-Versioneer works by adding a special `_version.py` file into your source\n-tree, where your `__init__.py` can import it. This `_version.py` knows how to\n-dynamically ask the VCS tool for version information at import time.\n-\n-`_version.py` also contains `$Revision$` markers, and the installation\n-process marks `_version.py` to have this marker rewritten with a tag name\n-during the `git archive` command. As a result, generated tarballs will\n-contain enough information to get the proper version.\n-\n-To allow `setup.py` to compute a version too, a `versioneer.py` is added to\n-the top level of your source tree, next to `setup.py` and the `setup.cfg`\n-that configures it. This overrides several distutils/setuptools commands to\n-compute the version when invoked, and changes `setup.py build` and `setup.py\n-sdist` to replace `_version.py` with a small static file that contains just\n-the generated version data.\n-\n-## Installation\n-\n-See [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n-\n-## Version-String Flavors\n-\n-Code which uses Versioneer can learn about its version string at runtime by\n-importing `_version` from your main `__init__.py` file and running the\n-`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\n-import the top-level `versioneer.py` and run `get_versions()`.\n-\n-Both functions return a dictionary with different flavors of version\n-information:\n-\n-* `['version']`: A condensed version string, rendered using the selected\n-  style. This is the most commonly used value for the project's version\n-  string. The default \"pep440\" style yields strings like `0.11`,\n-  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n-  below for alternative styles.\n-\n-* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n-  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n-\n-* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n-  commit date in ISO 8601 format. This will be None if the date is not\n-  available.\n-\n-* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n-  this is only accurate if run in a VCS checkout, otherwise it is likely to\n-  be False or None\n-\n-* `['error']`: if the version string could not be computed, this will be set\n-  to a string describing the problem, otherwise it will be None. It may be\n-  useful to throw an exception in setup.py if this is set, to avoid e.g.\n-  creating tarballs with a version string of \"unknown\".\n-\n-Some variants are more useful than others. Including `full-revisionid` in a\n-bug report should allow developers to reconstruct the exact code being tested\n-(or indicate the presence of local changes that should be shared with the\n-developers). `version` is suitable for display in an \"about\" box or a CLI\n-`--version` output: it can be easily compared against release notes and lists\n-of bugs fixed in various releases.\n-\n-The installer adds the following text to your `__init__.py` to place a basic\n-version in `YOURPROJECT.__version__`:\n-\n-    from ._version import get_versions\n-    __version__ = get_versions()['version']\n-    del get_versions\n-\n-## Styles\n-\n-The setup.cfg `style=` configuration controls how the VCS information is\n-rendered into a version string.\n-\n-The default style, \"pep440\", produces a PEP440-compliant string, equal to the\n-un-prefixed tag name for actual releases, and containing an additional \"local\n-version\" section with more detail for in-between builds. For Git, this is\n-TAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n---dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\n-tree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\n-that this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\n-software (exactly equal to a known tag), the identifier will only contain the\n-stripped tag, e.g. \"0.11\".\n-\n-Other styles are available. See [details.md](details.md) in the Versioneer\n-source tree for descriptions.\n-\n-## Debugging\n-\n-Versioneer tries to avoid fatal errors: if something goes wrong, it will tend\n-to return a version of \"0+unknown\". To investigate the problem, run `setup.py\n-version`, which will run the version-lookup code in a verbose mode, and will\n-display the full contents of `get_versions()` (including the `error` string,\n-which may help identify what went wrong).\n-\n-## Known Limitations\n-\n-Some situations are known to cause problems for Versioneer. This details the\n-most significant ones. More can be found on Github\n-[issues page](https://github.com/python-versioneer/python-versioneer/issues).\n-\n-### Subprojects\n-\n-Versioneer has limited support for source trees in which `setup.py` is not in\n-the root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\n-two common reasons why `setup.py` might not be in the root:\n-\n-* Source trees which contain multiple subprojects, such as\n-  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n-  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n-  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n-  distributions (and upload multiple independently-installable tarballs).\n-* Source trees whose main purpose is to contain a C library, but which also\n-  provide bindings to Python (and perhaps other languages) in subdirectories.\n-\n-Versioneer will look for `.git` in parent directories, and most operations\n-should get the right version string. However `pip` and `setuptools` have bugs\n-and implementation details which frequently cause `pip install .` from a\n-subproject directory to fail to find a correct version string (so it usually\n-defaults to `0+unknown`).\n-\n-`pip install --editable .` should work correctly. `setup.py install` might\n-work too.\n-\n-Pip-8.1.1 is known to have this problem, but hopefully it will get fixed in\n-some later version.\n-\n-[Bug #38](https://github.com/python-versioneer/python-versioneer/issues/38) is tracking\n-this issue. The discussion in\n-[PR #61](https://github.com/python-versioneer/python-versioneer/pull/61) describes the\n-issue from the Versioneer side in more detail.\n-[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n-[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\n-pip to let Versioneer work correctly.\n-\n-Versioneer-0.16 and earlier only looked for a `.git` directory next to the\n-`setup.cfg`, so subprojects were completely unsupported with those releases.\n-\n-### Editable installs with setuptools <= 18.5\n-\n-`setup.py develop` and `pip install --editable .` allow you to install a\n-project into a virtualenv once, then continue editing the source code (and\n-test) without re-installing after every change.\n-\n-\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\n-convenient way to specify executable scripts that should be installed along\n-with the python package.\n-\n-These both work as expected when using modern setuptools. When using\n-setuptools-18.5 or earlier, however, certain operations will cause\n-`pkg_resources.DistributionNotFound` errors when running the entrypoint\n-script, which must be resolved by re-installing the package. This happens\n-when the install happens with one version, then the egg_info data is\n-regenerated while a different version is checked out. Many setup.py commands\n-cause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\n-a different virtualenv), so this can be surprising.\n-\n-[Bug #83](https://github.com/python-versioneer/python-versioneer/issues/83) describes\n-this one, but upgrading to a newer version of setuptools should probably\n-resolve it.\n-\n-\n-## Updating Versioneer\n-\n-To upgrade your project to a new release of Versioneer, do the following:\n-\n-* install the new Versioneer (`pip install -U versioneer` or equivalent)\n-* edit `setup.cfg` and `pyproject.toml`, if necessary,\n-  to include any new configuration settings indicated by the release notes.\n-  See [UPGRADING](./UPGRADING.md) for details.\n-* re-run `versioneer install --[no-]vendor` in your source tree, to replace\n-  `SRC/_version.py`\n-* commit any changed files\n-\n-## Future Directions\n-\n-This tool is designed to make it easily extended to other version-control\n-systems: all VCS-specific components are in separate directories like\n-src/git/ . The top-level `versioneer.py` script is assembled from these\n-components by running make-versioneer.py . In the future, make-versioneer.py\n-will take a VCS name as an argument, and will construct a version of\n-`versioneer.py` that is specific to the given VCS. It might also take the\n-configuration arguments that are currently provided manually during\n-installation by editing setup.py . Alternatively, it might go the other\n-direction and include code from all supported VCS systems, reducing the\n-number of intermediate scripts.\n-\n-## Similar projects\n-\n-* [setuptools_scm](https://github.com/pypa/setuptools_scm/) - a non-vendored build-time\n-  dependency\n-* [minver](https://github.com/jbweston/miniver) - a lightweight reimplementation of\n-  versioneer\n-* [versioningit](https://github.com/jwodder/versioningit) - a PEP 518-based setuptools\n-  plugin\n-\n-## License\n-\n-To make Versioneer easier to embed, all its code is dedicated to the public\n-domain. The `_version.py` that it creates is also in the public domain.\n-Specifically, both are released under the \"Unlicense\", as described in\n-https://unlicense.org/.\n-\n-[pypi-image]: https://img.shields.io/pypi/v/versioneer.svg\n-[pypi-url]: https://pypi.python.org/pypi/versioneer/\n-[travis-image]:\n-https://img.shields.io/travis/com/python-versioneer/python-versioneer.svg\n-[travis-url]: https://travis-ci.com/github/python-versioneer/python-versioneer\n-\n-\"\"\"\n-# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring\n-# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements\n-# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error\n-# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with\n-# pylint:disable=attribute-defined-outside-init,too-many-arguments\n-\n-import configparser\n-import errno\n-import json\n-import os\n-import re\n-import subprocess\n-import sys\n-from pathlib import Path\n-from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n-from typing import NoReturn\n-import functools\n-\n-have_tomllib = True\n-if sys.version_info >= (3, 11):\n-    import tomllib\n-else:\n-    try:\n-        import tomli as tomllib\n-    except ImportError:\n-        have_tomllib = False\n-\n-\n-class VersioneerConfig:\n-    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n-\n-    VCS: str\n-    style: str\n-    tag_prefix: str\n-    versionfile_source: str\n-    versionfile_build: Optional[str]\n-    parentdir_prefix: Optional[str]\n-    verbose: Optional[bool]\n-\n-\n-def get_root() -> str:\n-    \"\"\"Get the project root directory.\n-\n-    We require that all commands are run from the project root, i.e. the\n-    directory that contains setup.py, setup.cfg, and versioneer.py .\n-    \"\"\"\n-    root = os.path.realpath(os.path.abspath(os.getcwd()))\n-    setup_py = os.path.join(root, \"setup.py\")\n-    pyproject_toml = os.path.join(root, \"pyproject.toml\")\n-    versioneer_py = os.path.join(root, \"versioneer.py\")\n-    if not (\n-        os.path.exists(setup_py)\n-        or os.path.exists(pyproject_toml)\n-        or os.path.exists(versioneer_py)\n-    ):\n-        # allow 'python path/to/setup.py COMMAND'\n-        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n-        setup_py = os.path.join(root, \"setup.py\")\n-        pyproject_toml = os.path.join(root, \"pyproject.toml\")\n-        versioneer_py = os.path.join(root, \"versioneer.py\")\n-    if not (\n-        os.path.exists(setup_py)\n-        or os.path.exists(pyproject_toml)\n-        or os.path.exists(versioneer_py)\n-    ):\n-        err = (\"Versioneer was unable to run the project root directory. \"\n-               \"Versioneer requires setup.py to be executed from \"\n-               \"its immediate directory (like 'python setup.py COMMAND'), \"\n-               \"or in a way that lets it use sys.argv[0] to find the root \"\n-               \"(like 'python path/to/setup.py COMMAND').\")\n-        raise VersioneerBadRootError(err)\n-    try:\n-        # Certain runtime workflows (setup.py install/develop in a setuptools\n-        # tree) execute all dependencies in a single python process, so\n-        # \"versioneer\" may be imported multiple times, and python's shared\n-        # module-import table will cache the first one. So we can't use\n-        # os.path.dirname(__file__), as that will find whichever\n-        # versioneer.py was first imported, even in later projects.\n-        my_path = os.path.realpath(os.path.abspath(__file__))\n-        me_dir = os.path.normcase(os.path.splitext(my_path)[0])\n-        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n-        if me_dir != vsr_dir and \"VERSIONEER_PEP518\" not in globals():\n-            print(\"Warning: build in %s is using versioneer.py from %s\"\n-                  % (os.path.dirname(my_path), versioneer_py))\n-    except NameError:\n-        pass\n-    return root\n-\n-\n-def get_config_from_root(root: str) -> VersioneerConfig:\n-    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n-    # This might raise OSError (if setup.cfg is missing), or\n-    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n-    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n-    # the top of versioneer.py for instructions on writing your setup.cfg .\n-    root_pth = Path(root)\n-    pyproject_toml = root_pth / \"pyproject.toml\"\n-    setup_cfg = root_pth / \"setup.cfg\"\n-    section: Union[Dict[str, Any], configparser.SectionProxy, None] = None\n-    if pyproject_toml.exists() and have_tomllib:\n-        try:\n-            with open(pyproject_toml, 'rb') as fobj:\n-                pp = tomllib.load(fobj)\n-            section = pp['tool']['versioneer']\n-        except (tomllib.TOMLDecodeError, KeyError) as e:\n-            print(f\"Failed to load config from {pyproject_toml}: {e}\")\n-            print(\"Try to load it from setup.cfg\")\n-    if not section:\n-        parser = configparser.ConfigParser()\n-        with open(setup_cfg) as cfg_file:\n-            parser.read_file(cfg_file)\n-        parser.get(\"versioneer\", \"VCS\")  # raise error if missing\n-\n-        section = parser[\"versioneer\"]\n-\n-    # `cast`` really shouldn't be used, but its simplest for the\n-    # common VersioneerConfig users at the moment. We verify against\n-    # `None` values elsewhere where it matters\n-\n-    cfg = VersioneerConfig()\n-    cfg.VCS = section['VCS']\n-    cfg.style = section.get(\"style\", \"\")\n-    cfg.versionfile_source = cast(str, section.get(\"versionfile_source\"))\n-    cfg.versionfile_build = section.get(\"versionfile_build\")\n-    cfg.tag_prefix = cast(str, section.get(\"tag_prefix\"))\n-    if cfg.tag_prefix in (\"''\", '\"\"', None):\n-        cfg.tag_prefix = \"\"\n-    cfg.parentdir_prefix = section.get(\"parentdir_prefix\")\n-    if isinstance(section, configparser.SectionProxy):\n-        # Make sure configparser translates to bool\n-        cfg.verbose = section.getboolean(\"verbose\")\n-    else:\n-        cfg.verbose = section.get(\"verbose\")\n-\n-    return cfg\n-\n-\n-class NotThisMethod(Exception):\n-    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n-\n-\n-# these dictionaries contain VCS-specific tools\n-LONG_VERSION_PY: Dict[str, str] = {}\n-HANDLERS: Dict[str, Dict[str, Callable]] = {}\n-\n-\n-def register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n-    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n-    def decorate(f: Callable) -> Callable:\n-        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        HANDLERS.setdefault(vcs, {})[method] = f\n-        return f\n-    return decorate\n-\n-\n-def run_command(\n-    commands: List[str],\n-    args: List[str],\n-    cwd: Optional[str] = None,\n-    verbose: bool = False,\n-    hide_stderr: bool = False,\n-    env: Optional[Dict[str, str]] = None,\n-) -> Tuple[Optional[str], Optional[int]]:\n-    \"\"\"Call the given command(s).\"\"\"\n-    assert isinstance(commands, list)\n-    process = None\n-\n-    popen_kwargs: Dict[str, Any] = {}\n-    if sys.platform == \"win32\":\n-        # This hides the console window if pythonw.exe is used\n-        startupinfo = subprocess.STARTUPINFO()\n-        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n-        popen_kwargs[\"startupinfo\"] = startupinfo\n-\n-    for command in commands:\n-        try:\n-            dispcmd = str([command] + args)\n-            # remember shell=False, so use git.cmd on windows, not just git\n-            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n-                                       stdout=subprocess.PIPE,\n-                                       stderr=(subprocess.PIPE if hide_stderr\n-                                               else None), **popen_kwargs)\n-            break\n-        except OSError as e:\n-            if e.errno == errno.ENOENT:\n-                continue\n-            if verbose:\n-                print(\"unable to run %s\" % dispcmd)\n-                print(e)\n-            return None, None\n-    else:\n-        if verbose:\n-            print(\"unable to find command, tried %s\" % (commands,))\n-        return None, None\n-    stdout = process.communicate()[0].strip().decode()\n-    if process.returncode != 0:\n-        if verbose:\n-            print(\"unable to run %s (error)\" % dispcmd)\n-            print(\"stdout was %s\" % stdout)\n-        return None, process.returncode\n-    return stdout, process.returncode\n-\n-\n-LONG_VERSION_PY['git'] = r'''\n-# This file helps to compute a version number in source trees obtained from\n-# git-archive tarball (such as those provided by githubs download-from-tag\n-# feature). Distribution tarballs (built by setup.py sdist) and build\n-# directories (produced by setup.py build) will contain a much shorter file\n-# that just contains the computed version number.\n-\n-# This file is released into the public domain.\n-# Generated by versioneer-0.29\n-# https://github.com/python-versioneer/python-versioneer\n-\n-\"\"\"Git implementation of _version.py.\"\"\"\n-\n-import errno\n-import os\n-import re\n-import subprocess\n-import sys\n-from typing import Any, Callable, Dict, List, Optional, Tuple\n-import functools\n-\n-\n-def get_keywords() -> Dict[str, str]:\n-    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n-    # these strings will be replaced by git during git-archive.\n-    # setup.py/versioneer.py will grep for the variable names, so they must\n-    # each be defined on a line of their own. _version.py will just call\n-    # get_keywords().\n-    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n-    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n-    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n-    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n-    return keywords\n-\n-\n-class VersioneerConfig:\n-    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n-\n-    VCS: str\n-    style: str\n-    tag_prefix: str\n-    parentdir_prefix: str\n-    versionfile_source: str\n-    verbose: bool\n-\n-\n-def get_config() -> VersioneerConfig:\n-    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n-    # these strings are filled in when 'setup.py versioneer' creates\n-    # _version.py\n-    cfg = VersioneerConfig()\n-    cfg.VCS = \"git\"\n-    cfg.style = \"%(STYLE)s\"\n-    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n-    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n-    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n-    cfg.verbose = False\n-    return cfg\n-\n-\n-class NotThisMethod(Exception):\n-    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n-\n-\n-LONG_VERSION_PY: Dict[str, str] = {}\n-HANDLERS: Dict[str, Dict[str, Callable]] = {}\n-\n-\n-def register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n-    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n-    def decorate(f: Callable) -> Callable:\n-        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        if vcs not in HANDLERS:\n-            HANDLERS[vcs] = {}\n-        HANDLERS[vcs][method] = f\n-        return f\n-    return decorate\n-\n-\n-def run_command(\n-    commands: List[str],\n-    args: List[str],\n-    cwd: Optional[str] = None,\n-    verbose: bool = False,\n-    hide_stderr: bool = False,\n-    env: Optional[Dict[str, str]] = None,\n-) -> Tuple[Optional[str], Optional[int]]:\n-    \"\"\"Call the given command(s).\"\"\"\n-    assert isinstance(commands, list)\n-    process = None\n-\n-    popen_kwargs: Dict[str, Any] = {}\n-    if sys.platform == \"win32\":\n-        # This hides the console window if pythonw.exe is used\n-        startupinfo = subprocess.STARTUPINFO()\n-        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n-        popen_kwargs[\"startupinfo\"] = startupinfo\n-\n-    for command in commands:\n-        try:\n-            dispcmd = str([command] + args)\n-            # remember shell=False, so use git.cmd on windows, not just git\n-            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n-                                       stdout=subprocess.PIPE,\n-                                       stderr=(subprocess.PIPE if hide_stderr\n-                                               else None), **popen_kwargs)\n-            break\n-        except OSError as e:\n-            if e.errno == errno.ENOENT:\n-                continue\n-            if verbose:\n-                print(\"unable to run %%s\" %% dispcmd)\n-                print(e)\n-            return None, None\n-    else:\n-        if verbose:\n-            print(\"unable to find command, tried %%s\" %% (commands,))\n-        return None, None\n-    stdout = process.communicate()[0].strip().decode()\n-    if process.returncode != 0:\n-        if verbose:\n-            print(\"unable to run %%s (error)\" %% dispcmd)\n-            print(\"stdout was %%s\" %% stdout)\n-        return None, process.returncode\n-    return stdout, process.returncode\n-\n-\n-def versions_from_parentdir(\n-    parentdir_prefix: str,\n-    root: str,\n-    verbose: bool,\n-) -> Dict[str, Any]:\n-    \"\"\"Try to determine the version from the parent directory name.\n-\n-    Source tarballs conventionally unpack into a directory that includes both\n-    the project name and a version string. We will also support searching up\n-    two directory levels for an appropriately named parent directory\n-    \"\"\"\n-    rootdirs = []\n-\n-    for _ in range(3):\n-        dirname = os.path.basename(root)\n-        if dirname.startswith(parentdir_prefix):\n-            return {\"version\": dirname[len(parentdir_prefix):],\n-                    \"full-revisionid\": None,\n-                    \"dirty\": False, \"error\": None, \"date\": None}\n-        rootdirs.append(root)\n-        root = os.path.dirname(root)  # up a level\n-\n-    if verbose:\n-        print(\"Tried directories %%s but none started with prefix %%s\" %%\n-              (str(rootdirs), parentdir_prefix))\n-    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n-\n-\n-@register_vcs_handler(\"git\", \"get_keywords\")\n-def git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n-    \"\"\"Extract version information from the given file.\"\"\"\n-    # the code embedded in _version.py can just fetch the value of these\n-    # keywords. When used from setup.py, we don't want to import _version.py,\n-    # so we do it with a regexp instead. This function is not used from\n-    # _version.py.\n-    keywords: Dict[str, str] = {}\n-    try:\n-        with open(versionfile_abs, \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(\"git_refnames =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"refnames\"] = mo.group(1)\n-                if line.strip().startswith(\"git_full =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"full\"] = mo.group(1)\n-                if line.strip().startswith(\"git_date =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"date\"] = mo.group(1)\n-    except OSError:\n-        pass\n-    return keywords\n-\n-\n-@register_vcs_handler(\"git\", \"keywords\")\n-def git_versions_from_keywords(\n-    keywords: Dict[str, str],\n-    tag_prefix: str,\n-    verbose: bool,\n-) -> Dict[str, Any]:\n-    \"\"\"Get version information from git keywords.\"\"\"\n-    if \"refnames\" not in keywords:\n-        raise NotThisMethod(\"Short version file found\")\n-    date = keywords.get(\"date\")\n-    if date is not None:\n-        # Use only the last line.  Previous lines may contain GPG signature\n-        # information.\n-        date = date.splitlines()[-1]\n-\n-        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n-        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n-        # -like\" string, which we must then edit to make compliant), because\n-        # it's been around since git-1.5.3, and it's too difficult to\n-        # discover which version we're using, or to work around using an\n-        # older one.\n-        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-    refnames = keywords[\"refnames\"].strip()\n-    if refnames.startswith(\"$Format\"):\n-        if verbose:\n-            print(\"keywords are unexpanded, not using\")\n-        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n-    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n-    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n-    TAG = \"tag: \"\n-    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n-    if not tags:\n-        # Either we're using git < 1.8.3, or there really are no tags. We use\n-        # a heuristic: assume all version tags have a digit. The old git %%d\n-        # expansion behaves like git log --decorate=short and strips out the\n-        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n-        # between branches and tags. By ignoring refnames without digits, we\n-        # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = {r for r in refs if re.search(r'\\d', r)}\n-        if verbose:\n-            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n-    if verbose:\n-        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n-    for ref in sorted(tags):\n-        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n-        if ref.startswith(tag_prefix):\n-            r = ref[len(tag_prefix):]\n-            # Filter out refs that exactly match prefix or that don't start\n-            # with a number once the prefix is stripped (mostly a concern\n-            # when prefix is '')\n-            if not re.match(r'\\d', r):\n-                continue\n-            if verbose:\n-                print(\"picking %%s\" %% r)\n-            return {\"version\": r,\n-                    \"full-revisionid\": keywords[\"full\"].strip(),\n-                    \"dirty\": False, \"error\": None,\n-                    \"date\": date}\n-    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n-    if verbose:\n-        print(\"no suitable tags, using unknown + full revision id\")\n-    return {\"version\": \"0+unknown\",\n-            \"full-revisionid\": keywords[\"full\"].strip(),\n-            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n-\n-\n-@register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(\n-    tag_prefix: str,\n-    root: str,\n-    verbose: bool,\n-    runner: Callable = run_command\n-) -> Dict[str, Any]:\n-    \"\"\"Get version from 'git describe' in the root of the source tree.\n-\n-    This only gets called if the git-archive 'subst' keywords were *not*\n-    expanded, and _version.py hasn't already been rewritten with a short\n-    version string, meaning we're inside a checked out source tree.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-\n-    # GIT_DIR can interfere with correct operation of Versioneer.\n-    # It may be intended to be passed to the Versioneer-versioned project,\n-    # but that should not change where we get our version from.\n-    env = os.environ.copy()\n-    env.pop(\"GIT_DIR\", None)\n-    runner = functools.partial(runner, env=env)\n-\n-    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                   hide_stderr=not verbose)\n-    if rc != 0:\n-        if verbose:\n-            print(\"Directory %%s not under git control\" %% root)\n-        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n-\n-    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n-    # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n-        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n-    ], cwd=root)\n-    # --long was added in git-1.5.5\n-    if describe_out is None:\n-        raise NotThisMethod(\"'git describe' failed\")\n-    describe_out = describe_out.strip()\n-    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n-    if full_out is None:\n-        raise NotThisMethod(\"'git rev-parse' failed\")\n-    full_out = full_out.strip()\n-\n-    pieces: Dict[str, Any] = {}\n-    pieces[\"long\"] = full_out\n-    pieces[\"short\"] = full_out[:7]  # maybe improved later\n-    pieces[\"error\"] = None\n-\n-    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n-                             cwd=root)\n-    # --abbrev-ref was added in git-1.6.3\n-    if rc != 0 or branch_name is None:\n-        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n-    branch_name = branch_name.strip()\n-\n-    if branch_name == \"HEAD\":\n-        # If we aren't exactly on a branch, pick a branch which represents\n-        # the current commit. If all else fails, we are on a branchless\n-        # commit.\n-        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n-        # --contains was added in git-1.5.4\n-        if rc != 0 or branches is None:\n-            raise NotThisMethod(\"'git branch --contains' returned error\")\n-        branches = branches.split(\"\\n\")\n-\n-        # Remove the first line if we're running detached\n-        if \"(\" in branches[0]:\n-            branches.pop(0)\n-\n-        # Strip off the leading \"* \" from the list of branches.\n-        branches = [branch[2:] for branch in branches]\n-        if \"master\" in branches:\n-            branch_name = \"master\"\n-        elif not branches:\n-            branch_name = None\n-        else:\n-            # Pick the first branch that is returned. Good or bad.\n-            branch_name = branches[0]\n-\n-    pieces[\"branch\"] = branch_name\n-\n-    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n-    # TAG might have hyphens.\n-    git_describe = describe_out\n-\n-    # look for -dirty suffix\n-    dirty = git_describe.endswith(\"-dirty\")\n-    pieces[\"dirty\"] = dirty\n-    if dirty:\n-        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n-\n-    # now we have TAG-NUM-gHEX or HEX\n-\n-    if \"-\" in git_describe:\n-        # TAG-NUM-gHEX\n-        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n-        if not mo:\n-            # unparsable. Maybe git-describe is misbehaving?\n-            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n-                               %% describe_out)\n-            return pieces\n-\n-        # tag\n-        full_tag = mo.group(1)\n-        if not full_tag.startswith(tag_prefix):\n-            if verbose:\n-                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n-                print(fmt %% (full_tag, tag_prefix))\n-            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n-                               %% (full_tag, tag_prefix))\n-            return pieces\n-        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n-\n-        # distance: number of commits since tag\n-        pieces[\"distance\"] = int(mo.group(2))\n-\n-        # commit: short hex revision ID\n-        pieces[\"short\"] = mo.group(3)\n-\n-    else:\n-        # HEX: no tags\n-        pieces[\"closest-tag\"] = None\n-        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n-        pieces[\"distance\"] = len(out.split())  # total number of commits\n-\n-    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = runner(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"], cwd=root)[0].strip()\n-    # Use only the last line.  Previous lines may contain GPG signature\n-    # information.\n-    date = date.splitlines()[-1]\n-    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-\n-    return pieces\n-\n-\n-def plus_or_dot(pieces: Dict[str, Any]) -> str:\n-    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n-    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n-        return \".\"\n-    return \"+\"\n-\n-\n-def render_pep440(pieces: Dict[str, Any]) -> str:\n-    \"\"\"Build up version string, with post-release \"local version identifier\".\n-\n-    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n-    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n-\n-    Exceptions:\n-    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_branch(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n-    (a feature branch will appear \"older\" than the master branch).\n-\n-    Exceptions:\n-    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0\"\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n-    \"\"\"Split pep440 version string at the post-release segment.\n-\n-    Returns the release segments before the post-release and the\n-    post-release version number (or -1 if no post-release segment is present).\n-    \"\"\"\n-    vc = str.split(ver, \".post\")\n-    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n-\n-\n-def render_pep440_pre(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.post0.devDISTANCE\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        if pieces[\"distance\"]:\n-            # update the post release segment\n-            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n-            rendered = tag_version\n-            if post_version is not None:\n-                rendered += \".post%%d.dev%%d\" %% (post_version + 1, pieces[\"distance\"])\n-            else:\n-                rendered += \".post0.dev%%d\" %% (pieces[\"distance\"])\n-        else:\n-            # no commits, use the tag as the version\n-            rendered = pieces[\"closest-tag\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post0.dev%%d\" %% pieces[\"distance\"]\n-    return rendered\n-\n-\n-def render_pep440_post(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n-\n-    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n-    (a dirty tree will appear \"older\" than the corresponding clean one),\n-    but you shouldn't be releasing software with -dirty anyways.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%%d\" %% pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%%s\" %% pieces[\"short\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-        rendered += \"+g%%s\" %% pieces[\"short\"]\n-    return rendered\n-\n-\n-def render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%%d\" %% pieces[\"distance\"]\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%%s\" %% pieces[\"short\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+g%%s\" %% pieces[\"short\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_old(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]] .\n-\n-    The \".dev0\" means dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%%d\" %% pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-    return rendered\n-\n-\n-def render_git_describe(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n-\n-    Like 'git describe --tags --dirty --always'.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"]:\n-            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render_git_describe_long(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n-\n-    Like 'git describe --tags --dirty --always -long'.\n-    The distance/hash is unconditional.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n-    \"\"\"Render the given version pieces into the requested style.\"\"\"\n-    if pieces[\"error\"]:\n-        return {\"version\": \"unknown\",\n-                \"full-revisionid\": pieces.get(\"long\"),\n-                \"dirty\": None,\n-                \"error\": pieces[\"error\"],\n-                \"date\": None}\n-\n-    if not style or style == \"default\":\n-        style = \"pep440\"  # the default\n-\n-    if style == \"pep440\":\n-        rendered = render_pep440(pieces)\n-    elif style == \"pep440-branch\":\n-        rendered = render_pep440_branch(pieces)\n-    elif style == \"pep440-pre\":\n-        rendered = render_pep440_pre(pieces)\n-    elif style == \"pep440-post\":\n-        rendered = render_pep440_post(pieces)\n-    elif style == \"pep440-post-branch\":\n-        rendered = render_pep440_post_branch(pieces)\n-    elif style == \"pep440-old\":\n-        rendered = render_pep440_old(pieces)\n-    elif style == \"git-describe\":\n-        rendered = render_git_describe(pieces)\n-    elif style == \"git-describe-long\":\n-        rendered = render_git_describe_long(pieces)\n-    else:\n-        raise ValueError(\"unknown style '%%s'\" %% style)\n-\n-    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n-            \"dirty\": pieces[\"dirty\"], \"error\": None,\n-            \"date\": pieces.get(\"date\")}\n-\n-\n-def get_versions() -> Dict[str, Any]:\n-    \"\"\"Get version information or return default if unable to do so.\"\"\"\n-    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n-    # __file__, we can work backwards from there to the root. Some\n-    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n-    # case we can only use expanded keywords.\n-\n-    cfg = get_config()\n-    verbose = cfg.verbose\n-\n-    try:\n-        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n-                                          verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        root = os.path.realpath(__file__)\n-        # versionfile_source is the relative path from the top of the source\n-        # tree (where the .git directory might live) to this file. Invert\n-        # this to find the root from __file__.\n-        for _ in cfg.versionfile_source.split('/'):\n-            root = os.path.dirname(root)\n-    except NameError:\n-        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-                \"dirty\": None,\n-                \"error\": \"unable to find root of source tree\",\n-                \"date\": None}\n-\n-    try:\n-        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n-        return render(pieces, cfg.style)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        if cfg.parentdir_prefix:\n-            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-            \"dirty\": None,\n-            \"error\": \"unable to compute version\", \"date\": None}\n-'''\n-\n-\n-@register_vcs_handler(\"git\", \"get_keywords\")\n-def git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n-    \"\"\"Extract version information from the given file.\"\"\"\n-    # the code embedded in _version.py can just fetch the value of these\n-    # keywords. When used from setup.py, we don't want to import _version.py,\n-    # so we do it with a regexp instead. This function is not used from\n-    # _version.py.\n-    keywords: Dict[str, str] = {}\n-    try:\n-        with open(versionfile_abs, \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(\"git_refnames =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"refnames\"] = mo.group(1)\n-                if line.strip().startswith(\"git_full =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"full\"] = mo.group(1)\n-                if line.strip().startswith(\"git_date =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"date\"] = mo.group(1)\n-    except OSError:\n-        pass\n-    return keywords\n-\n-\n-@register_vcs_handler(\"git\", \"keywords\")\n-def git_versions_from_keywords(\n-    keywords: Dict[str, str],\n-    tag_prefix: str,\n-    verbose: bool,\n-) -> Dict[str, Any]:\n-    \"\"\"Get version information from git keywords.\"\"\"\n-    if \"refnames\" not in keywords:\n-        raise NotThisMethod(\"Short version file found\")\n-    date = keywords.get(\"date\")\n-    if date is not None:\n-        # Use only the last line.  Previous lines may contain GPG signature\n-        # information.\n-        date = date.splitlines()[-1]\n-\n-        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n-        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n-        # -like\" string, which we must then edit to make compliant), because\n-        # it's been around since git-1.5.3, and it's too difficult to\n-        # discover which version we're using, or to work around using an\n-        # older one.\n-        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-    refnames = keywords[\"refnames\"].strip()\n-    if refnames.startswith(\"$Format\"):\n-        if verbose:\n-            print(\"keywords are unexpanded, not using\")\n-        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n-    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n-    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n-    TAG = \"tag: \"\n-    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n-    if not tags:\n-        # Either we're using git < 1.8.3, or there really are no tags. We use\n-        # a heuristic: assume all version tags have a digit. The old git %d\n-        # expansion behaves like git log --decorate=short and strips out the\n-        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n-        # between branches and tags. By ignoring refnames without digits, we\n-        # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = {r for r in refs if re.search(r'\\d', r)}\n-        if verbose:\n-            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n-    if verbose:\n-        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n-    for ref in sorted(tags):\n-        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n-        if ref.startswith(tag_prefix):\n-            r = ref[len(tag_prefix):]\n-            # Filter out refs that exactly match prefix or that don't start\n-            # with a number once the prefix is stripped (mostly a concern\n-            # when prefix is '')\n-            if not re.match(r'\\d', r):\n-                continue\n-            if verbose:\n-                print(\"picking %s\" % r)\n-            return {\"version\": r,\n-                    \"full-revisionid\": keywords[\"full\"].strip(),\n-                    \"dirty\": False, \"error\": None,\n-                    \"date\": date}\n-    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n-    if verbose:\n-        print(\"no suitable tags, using unknown + full revision id\")\n-    return {\"version\": \"0+unknown\",\n-            \"full-revisionid\": keywords[\"full\"].strip(),\n-            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n-\n-\n-@register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(\n-    tag_prefix: str,\n-    root: str,\n-    verbose: bool,\n-    runner: Callable = run_command\n-) -> Dict[str, Any]:\n-    \"\"\"Get version from 'git describe' in the root of the source tree.\n-\n-    This only gets called if the git-archive 'subst' keywords were *not*\n-    expanded, and _version.py hasn't already been rewritten with a short\n-    version string, meaning we're inside a checked out source tree.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-\n-    # GIT_DIR can interfere with correct operation of Versioneer.\n-    # It may be intended to be passed to the Versioneer-versioned project,\n-    # but that should not change where we get our version from.\n-    env = os.environ.copy()\n-    env.pop(\"GIT_DIR\", None)\n-    runner = functools.partial(runner, env=env)\n-\n-    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                   hide_stderr=not verbose)\n-    if rc != 0:\n-        if verbose:\n-            print(\"Directory %s not under git control\" % root)\n-        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n-\n-    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n-    # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n-        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n-    ], cwd=root)\n-    # --long was added in git-1.5.5\n-    if describe_out is None:\n-        raise NotThisMethod(\"'git describe' failed\")\n-    describe_out = describe_out.strip()\n-    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n-    if full_out is None:\n-        raise NotThisMethod(\"'git rev-parse' failed\")\n-    full_out = full_out.strip()\n-\n-    pieces: Dict[str, Any] = {}\n-    pieces[\"long\"] = full_out\n-    pieces[\"short\"] = full_out[:7]  # maybe improved later\n-    pieces[\"error\"] = None\n-\n-    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n-                             cwd=root)\n-    # --abbrev-ref was added in git-1.6.3\n-    if rc != 0 or branch_name is None:\n-        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n-    branch_name = branch_name.strip()\n-\n-    if branch_name == \"HEAD\":\n-        # If we aren't exactly on a branch, pick a branch which represents\n-        # the current commit. If all else fails, we are on a branchless\n-        # commit.\n-        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n-        # --contains was added in git-1.5.4\n-        if rc != 0 or branches is None:\n-            raise NotThisMethod(\"'git branch --contains' returned error\")\n-        branches = branches.split(\"\\n\")\n-\n-        # Remove the first line if we're running detached\n-        if \"(\" in branches[0]:\n-            branches.pop(0)\n-\n-        # Strip off the leading \"* \" from the list of branches.\n-        branches = [branch[2:] for branch in branches]\n-        if \"master\" in branches:\n-            branch_name = \"master\"\n-        elif not branches:\n-            branch_name = None\n-        else:\n-            # Pick the first branch that is returned. Good or bad.\n-            branch_name = branches[0]\n-\n-    pieces[\"branch\"] = branch_name\n-\n-    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n-    # TAG might have hyphens.\n-    git_describe = describe_out\n-\n-    # look for -dirty suffix\n-    dirty = git_describe.endswith(\"-dirty\")\n-    pieces[\"dirty\"] = dirty\n-    if dirty:\n-        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n-\n-    # now we have TAG-NUM-gHEX or HEX\n-\n-    if \"-\" in git_describe:\n-        # TAG-NUM-gHEX\n-        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n-        if not mo:\n-            # unparsable. Maybe git-describe is misbehaving?\n-            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n-                               % describe_out)\n-            return pieces\n-\n-        # tag\n-        full_tag = mo.group(1)\n-        if not full_tag.startswith(tag_prefix):\n-            if verbose:\n-                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n-                print(fmt % (full_tag, tag_prefix))\n-            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n-                               % (full_tag, tag_prefix))\n-            return pieces\n-        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n-\n-        # distance: number of commits since tag\n-        pieces[\"distance\"] = int(mo.group(2))\n-\n-        # commit: short hex revision ID\n-        pieces[\"short\"] = mo.group(3)\n-\n-    else:\n-        # HEX: no tags\n-        pieces[\"closest-tag\"] = None\n-        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n-        pieces[\"distance\"] = len(out.split())  # total number of commits\n-\n-    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n-    # Use only the last line.  Previous lines may contain GPG signature\n-    # information.\n-    date = date.splitlines()[-1]\n-    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-\n-    return pieces\n-\n-\n-def do_vcs_install(versionfile_source: str, ipy: Optional[str]) -> None:\n-    \"\"\"Git-specific installation logic for Versioneer.\n-\n-    For Git, this means creating/changing .gitattributes to mark _version.py\n-    for export-subst keyword substitution.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-    files = [versionfile_source]\n-    if ipy:\n-        files.append(ipy)\n-    if \"VERSIONEER_PEP518\" not in globals():\n-        try:\n-            my_path = __file__\n-            if my_path.endswith((\".pyc\", \".pyo\")):\n-                my_path = os.path.splitext(my_path)[0] + \".py\"\n-            versioneer_file = os.path.relpath(my_path)\n-        except NameError:\n-            versioneer_file = \"versioneer.py\"\n-        files.append(versioneer_file)\n-    present = False\n-    try:\n-        with open(\".gitattributes\", \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(versionfile_source):\n-                    if \"export-subst\" in line.strip().split()[1:]:\n-                        present = True\n-                        break\n-    except OSError:\n-        pass\n-    if not present:\n-        with open(\".gitattributes\", \"a+\") as fobj:\n-            fobj.write(f\"{versionfile_source} export-subst\\n\")\n-        files.append(\".gitattributes\")\n-    run_command(GITS, [\"add\", \"--\"] + files)\n-\n-\n-def versions_from_parentdir(\n-    parentdir_prefix: str,\n-    root: str,\n-    verbose: bool,\n-) -> Dict[str, Any]:\n-    \"\"\"Try to determine the version from the parent directory name.\n-\n-    Source tarballs conventionally unpack into a directory that includes both\n-    the project name and a version string. We will also support searching up\n-    two directory levels for an appropriately named parent directory\n-    \"\"\"\n-    rootdirs = []\n-\n-    for _ in range(3):\n-        dirname = os.path.basename(root)\n-        if dirname.startswith(parentdir_prefix):\n-            return {\"version\": dirname[len(parentdir_prefix):],\n-                    \"full-revisionid\": None,\n-                    \"dirty\": False, \"error\": None, \"date\": None}\n-        rootdirs.append(root)\n-        root = os.path.dirname(root)  # up a level\n-\n-    if verbose:\n-        print(\"Tried directories %s but none started with prefix %s\" %\n-              (str(rootdirs), parentdir_prefix))\n-    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n-\n-\n-SHORT_VERSION_PY = \"\"\"\n-# This file was generated by 'versioneer.py' (0.29) from\n-# revision-control system data, or from the parent directory name of an\n-# unpacked source archive. Distribution tarballs contain a pre-generated copy\n-# of this file.\n-\n-import json\n-\n-version_json = '''\n-%s\n-'''  # END VERSION_JSON\n-\n-\n-def get_versions():\n-    return json.loads(version_json)\n-\"\"\"\n-\n-\n-def versions_from_file(filename: str) -> Dict[str, Any]:\n-    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n-    try:\n-        with open(filename) as f:\n-            contents = f.read()\n-    except OSError:\n-        raise NotThisMethod(\"unable to read _version.py\")\n-    mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",\n-                   contents, re.M | re.S)\n-    if not mo:\n-        mo = re.search(r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\",\n-                       contents, re.M | re.S)\n-    if not mo:\n-        raise NotThisMethod(\"no version_json in _version.py\")\n-    return json.loads(mo.group(1))\n-\n-\n-def write_to_version_file(filename: str, versions: Dict[str, Any]) -> None:\n-    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n-    contents = json.dumps(versions, sort_keys=True,\n-                          indent=1, separators=(\",\", \": \"))\n-    with open(filename, \"w\") as f:\n-        f.write(SHORT_VERSION_PY % contents)\n-\n-    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n-\n-\n-def plus_or_dot(pieces: Dict[str, Any]) -> str:\n-    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n-    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n-        return \".\"\n-    return \"+\"\n-\n-\n-def render_pep440(pieces: Dict[str, Any]) -> str:\n-    \"\"\"Build up version string, with post-release \"local version identifier\".\n-\n-    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n-    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n-\n-    Exceptions:\n-    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_branch(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n-    (a feature branch will appear \"older\" than the master branch).\n-\n-    Exceptions:\n-    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0\"\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n-    \"\"\"Split pep440 version string at the post-release segment.\n-\n-    Returns the release segments before the post-release and the\n-    post-release version number (or -1 if no post-release segment is present).\n-    \"\"\"\n-    vc = str.split(ver, \".post\")\n-    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n-\n-\n-def render_pep440_pre(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.post0.devDISTANCE\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        if pieces[\"distance\"]:\n-            # update the post release segment\n-            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n-            rendered = tag_version\n-            if post_version is not None:\n-                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n-            else:\n-                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n-        else:\n-            # no commits, use the tag as the version\n-            rendered = pieces[\"closest-tag\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n-    return rendered\n-\n-\n-def render_pep440_post(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n-\n-    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n-    (a dirty tree will appear \"older\" than the corresponding clean one),\n-    but you shouldn't be releasing software with -dirty anyways.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-    return rendered\n-\n-\n-def render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_old(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[.postDISTANCE[.dev0]] .\n-\n-    The \".dev0\" means dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-    return rendered\n-\n-\n-def render_git_describe(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n-\n-    Like 'git describe --tags --dirty --always'.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"]:\n-            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render_git_describe_long(pieces: Dict[str, Any]) -> str:\n-    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n-\n-    Like 'git describe --tags --dirty --always -long'.\n-    The distance/hash is unconditional.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n-    \"\"\"Render the given version pieces into the requested style.\"\"\"\n-    if pieces[\"error\"]:\n-        return {\"version\": \"unknown\",\n-                \"full-revisionid\": pieces.get(\"long\"),\n-                \"dirty\": None,\n-                \"error\": pieces[\"error\"],\n-                \"date\": None}\n-\n-    if not style or style == \"default\":\n-        style = \"pep440\"  # the default\n-\n-    if style == \"pep440\":\n-        rendered = render_pep440(pieces)\n-    elif style == \"pep440-branch\":\n-        rendered = render_pep440_branch(pieces)\n-    elif style == \"pep440-pre\":\n-        rendered = render_pep440_pre(pieces)\n-    elif style == \"pep440-post\":\n-        rendered = render_pep440_post(pieces)\n-    elif style == \"pep440-post-branch\":\n-        rendered = render_pep440_post_branch(pieces)\n-    elif style == \"pep440-old\":\n-        rendered = render_pep440_old(pieces)\n-    elif style == \"git-describe\":\n-        rendered = render_git_describe(pieces)\n-    elif style == \"git-describe-long\":\n-        rendered = render_git_describe_long(pieces)\n-    else:\n-        raise ValueError(\"unknown style '%s'\" % style)\n-\n-    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n-            \"dirty\": pieces[\"dirty\"], \"error\": None,\n-            \"date\": pieces.get(\"date\")}\n-\n-\n-class VersioneerBadRootError(Exception):\n-    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n-\n-\n-def get_versions(verbose: bool = False) -> Dict[str, Any]:\n-    \"\"\"Get the project version from whatever source is available.\n-\n-    Returns dict with two keys: 'version' and 'full'.\n-    \"\"\"\n-    if \"versioneer\" in sys.modules:\n-        # see the discussion in cmdclass.py:get_cmdclass()\n-        del sys.modules[\"versioneer\"]\n-\n-    root = get_root()\n-    cfg = get_config_from_root(root)\n-\n-    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n-    handlers = HANDLERS.get(cfg.VCS)\n-    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n-    verbose = verbose or bool(cfg.verbose)  # `bool()` used to avoid `None`\n-    assert cfg.versionfile_source is not None, \\\n-        \"please set versioneer.versionfile_source\"\n-    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n-\n-    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n-\n-    # extract version from first of: _version.py, VCS command (e.g. 'git\n-    # describe'), parentdir. This is meant to work for developers using a\n-    # source checkout, for users of a tarball created by 'setup.py sdist',\n-    # and for users of a tarball/zipball created by 'git archive' or github's\n-    # download-from-tag feature or the equivalent in other VCSes.\n-\n-    get_keywords_f = handlers.get(\"get_keywords\")\n-    from_keywords_f = handlers.get(\"keywords\")\n-    if get_keywords_f and from_keywords_f:\n-        try:\n-            keywords = get_keywords_f(versionfile_abs)\n-            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n-            if verbose:\n-                print(\"got version from expanded keyword %s\" % ver)\n-            return ver\n-        except NotThisMethod:\n-            pass\n-\n-    try:\n-        ver = versions_from_file(versionfile_abs)\n-        if verbose:\n-            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n-        return ver\n-    except NotThisMethod:\n-        pass\n-\n-    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n-    if from_vcs_f:\n-        try:\n-            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n-            ver = render(pieces, cfg.style)\n-            if verbose:\n-                print(\"got version from VCS %s\" % ver)\n-            return ver\n-        except NotThisMethod:\n-            pass\n-\n-    try:\n-        if cfg.parentdir_prefix:\n-            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n-            if verbose:\n-                print(\"got version from parentdir %s\" % ver)\n-            return ver\n-    except NotThisMethod:\n-        pass\n-\n-    if verbose:\n-        print(\"unable to compute version\")\n-\n-    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-            \"dirty\": None, \"error\": \"unable to compute version\",\n-            \"date\": None}\n-\n-\n-def get_version() -> str:\n-    \"\"\"Get the short version string for this project.\"\"\"\n-    return get_versions()[\"version\"]\n-\n-\n-def get_cmdclass(cmdclass: Optional[Dict[str, Any]] = None):\n-    \"\"\"Get the custom setuptools subclasses used by Versioneer.\n-\n-    If the package uses a different cmdclass (e.g. one from numpy), it\n-    should be provide as an argument.\n-    \"\"\"\n-    if \"versioneer\" in sys.modules:\n-        del sys.modules[\"versioneer\"]\n-        # this fixes the \"python setup.py develop\" case (also 'install' and\n-        # 'easy_install .'), in which subdependencies of the main project are\n-        # built (using setup.py bdist_egg) in the same python process. Assume\n-        # a main project A and a dependency B, which use different versions\n-        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n-        # sys.modules by the time B's setup.py is executed, causing B to run\n-        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n-        # sandbox that restores sys.modules to it's pre-build state, so the\n-        # parent is protected against the child's \"import versioneer\". By\n-        # removing ourselves from sys.modules here, before the child build\n-        # happens, we protect the child from the parent's versioneer too.\n-        # Also see https://github.com/python-versioneer/python-versioneer/issues/52\n-\n-    cmds = {} if cmdclass is None else cmdclass.copy()\n-\n-    # we add \"version\" to setuptools\n-    from setuptools import Command\n-\n-    class cmd_version(Command):\n-        description = \"report generated version string\"\n-        user_options: List[Tuple[str, str, str]] = []\n-        boolean_options: List[str] = []\n-\n-        def initialize_options(self) -> None:\n-            pass\n-\n-        def finalize_options(self) -> None:\n-            pass\n-\n-        def run(self) -> None:\n-            vers = get_versions(verbose=True)\n-            print(\"Version: %s\" % vers[\"version\"])\n-            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n-            print(\" dirty: %s\" % vers.get(\"dirty\"))\n-            print(\" date: %s\" % vers.get(\"date\"))\n-            if vers[\"error\"]:\n-                print(\" error: %s\" % vers[\"error\"])\n-    cmds[\"version\"] = cmd_version\n-\n-    # we override \"build_py\" in setuptools\n-    #\n-    # most invocation pathways end up running build_py:\n-    #  distutils/build -> build_py\n-    #  distutils/install -> distutils/build ->..\n-    #  setuptools/bdist_wheel -> distutils/install ->..\n-    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n-    #  setuptools/install -> bdist_egg ->..\n-    #  setuptools/develop -> ?\n-    #  pip install:\n-    #   copies source tree to a tempdir before running egg_info/etc\n-    #   if .git isn't copied too, 'git describe' will fail\n-    #   then does setup.py bdist_wheel, or sometimes setup.py install\n-    #  setup.py egg_info -> ?\n-\n-    # pip install -e . and setuptool/editable_wheel will invoke build_py\n-    # but the build_py command is not expected to copy any files.\n-\n-    # we override different \"build_py\" commands for both environments\n-    if 'build_py' in cmds:\n-        _build_py: Any = cmds['build_py']\n-    else:\n-        from setuptools.command.build_py import build_py as _build_py\n-\n-    class cmd_build_py(_build_py):\n-        def run(self) -> None:\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            versions = get_versions()\n-            _build_py.run(self)\n-            if getattr(self, \"editable_mode\", False):\n-                # During editable installs `.py` and data files are\n-                # not copied to build_lib\n-                return\n-            # now locate _version.py in the new build/ directory and replace\n-            # it with an updated value\n-            if cfg.versionfile_build:\n-                target_versionfile = os.path.join(self.build_lib,\n-                                                  cfg.versionfile_build)\n-                print(\"UPDATING %s\" % target_versionfile)\n-                write_to_version_file(target_versionfile, versions)\n-    cmds[\"build_py\"] = cmd_build_py\n-\n-    if 'build_ext' in cmds:\n-        _build_ext: Any = cmds['build_ext']\n-    else:\n-        from setuptools.command.build_ext import build_ext as _build_ext\n-\n-    class cmd_build_ext(_build_ext):\n-        def run(self) -> None:\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            versions = get_versions()\n-            _build_ext.run(self)\n-            if self.inplace:\n-                # build_ext --inplace will only build extensions in\n-                # build/lib<..> dir with no _version.py to write to.\n-                # As in place builds will already have a _version.py\n-                # in the module dir, we do not need to write one.\n-                return\n-            # now locate _version.py in the new build/ directory and replace\n-            # it with an updated value\n-            if not cfg.versionfile_build:\n-                return\n-            target_versionfile = os.path.join(self.build_lib,\n-                                              cfg.versionfile_build)\n-            if not os.path.exists(target_versionfile):\n-                print(f\"Warning: {target_versionfile} does not exist, skipping \"\n-                      \"version update. This can happen if you are running build_ext \"\n-                      \"without first running build_py.\")\n-                return\n-            print(\"UPDATING %s\" % target_versionfile)\n-            write_to_version_file(target_versionfile, versions)\n-    cmds[\"build_ext\"] = cmd_build_ext\n-\n-    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n-        from cx_Freeze.dist import build_exe as _build_exe  # type: ignore\n-        # nczeczulin reports that py2exe won't like the pep440-style string\n-        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n-        # setup(console=[{\n-        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n-        #   \"product_version\": versioneer.get_version(),\n-        #   ...\n-\n-        class cmd_build_exe(_build_exe):\n-            def run(self) -> None:\n-                root = get_root()\n-                cfg = get_config_from_root(root)\n-                versions = get_versions()\n-                target_versionfile = cfg.versionfile_source\n-                print(\"UPDATING %s\" % target_versionfile)\n-                write_to_version_file(target_versionfile, versions)\n-\n-                _build_exe.run(self)\n-                os.unlink(target_versionfile)\n-                with open(cfg.versionfile_source, \"w\") as f:\n-                    LONG = LONG_VERSION_PY[cfg.VCS]\n-                    f.write(LONG %\n-                            {\"DOLLAR\": \"$\",\n-                             \"STYLE\": cfg.style,\n-                             \"TAG_PREFIX\": cfg.tag_prefix,\n-                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n-                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n-                             })\n-        cmds[\"build_exe\"] = cmd_build_exe\n-        del cmds[\"build_py\"]\n-\n-    if 'py2exe' in sys.modules:  # py2exe enabled?\n-        try:\n-            from py2exe.setuptools_buildexe import py2exe as _py2exe  # type: ignore\n-        except ImportError:\n-            from py2exe.distutils_buildexe import py2exe as _py2exe  # type: ignore\n-\n-        class cmd_py2exe(_py2exe):\n-            def run(self) -> None:\n-                root = get_root()\n-                cfg = get_config_from_root(root)\n-                versions = get_versions()\n-                target_versionfile = cfg.versionfile_source\n-                print(\"UPDATING %s\" % target_versionfile)\n-                write_to_version_file(target_versionfile, versions)\n-\n-                _py2exe.run(self)\n-                os.unlink(target_versionfile)\n-                with open(cfg.versionfile_source, \"w\") as f:\n-                    LONG = LONG_VERSION_PY[cfg.VCS]\n-                    f.write(LONG %\n-                            {\"DOLLAR\": \"$\",\n-                             \"STYLE\": cfg.style,\n-                             \"TAG_PREFIX\": cfg.tag_prefix,\n-                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n-                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n-                             })\n-        cmds[\"py2exe\"] = cmd_py2exe\n-\n-    # sdist farms its file list building out to egg_info\n-    if 'egg_info' in cmds:\n-        _egg_info: Any = cmds['egg_info']\n-    else:\n-        from setuptools.command.egg_info import egg_info as _egg_info\n-\n-    class cmd_egg_info(_egg_info):\n-        def find_sources(self) -> None:\n-            # egg_info.find_sources builds the manifest list and writes it\n-            # in one shot\n-            super().find_sources()\n-\n-            # Modify the filelist and normalize it\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            self.filelist.append('versioneer.py')\n-            if cfg.versionfile_source:\n-                # There are rare cases where versionfile_source might not be\n-                # included by default, so we must be explicit\n-                self.filelist.append(cfg.versionfile_source)\n-            self.filelist.sort()\n-            self.filelist.remove_duplicates()\n-\n-            # The write method is hidden in the manifest_maker instance that\n-            # generated the filelist and was thrown away\n-            # We will instead replicate their final normalization (to unicode,\n-            # and POSIX-style paths)\n-            from setuptools import unicode_utils\n-            normalized = [unicode_utils.filesys_decode(f).replace(os.sep, '/')\n-                          for f in self.filelist.files]\n-\n-            manifest_filename = os.path.join(self.egg_info, 'SOURCES.txt')\n-            with open(manifest_filename, 'w') as fobj:\n-                fobj.write('\\n'.join(normalized))\n-\n-    cmds['egg_info'] = cmd_egg_info\n-\n-    # we override different \"sdist\" commands for both environments\n-    if 'sdist' in cmds:\n-        _sdist: Any = cmds['sdist']\n-    else:\n-        from setuptools.command.sdist import sdist as _sdist\n-\n-    class cmd_sdist(_sdist):\n-        def run(self) -> None:\n-            versions = get_versions()\n-            self._versioneer_generated_versions = versions\n-            # unless we update this, the command will keep using the old\n-            # version\n-            self.distribution.metadata.version = versions[\"version\"]\n-            return _sdist.run(self)\n-\n-        def make_release_tree(self, base_dir: str, files: List[str]) -> None:\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            _sdist.make_release_tree(self, base_dir, files)\n-            # now locate _version.py in the new base_dir directory\n-            # (remembering that it may be a hardlink) and replace it with an\n-            # updated value\n-            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n-            print(\"UPDATING %s\" % target_versionfile)\n-            write_to_version_file(target_versionfile,\n-                                  self._versioneer_generated_versions)\n-    cmds[\"sdist\"] = cmd_sdist\n-\n-    return cmds\n-\n-\n-CONFIG_ERROR = \"\"\"\n-setup.cfg is missing the necessary Versioneer configuration. You need\n-a section like:\n-\n- [versioneer]\n- VCS = git\n- style = pep440\n- versionfile_source = src/myproject/_version.py\n- versionfile_build = myproject/_version.py\n- tag_prefix =\n- parentdir_prefix = myproject-\n-\n-You will also need to edit your setup.py to use the results:\n-\n- import versioneer\n- setup(version=versioneer.get_version(),\n-       cmdclass=versioneer.get_cmdclass(), ...)\n-\n-Please read the docstring in ./versioneer.py for configuration instructions,\n-edit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n-\"\"\"\n-\n-SAMPLE_CONFIG = \"\"\"\n-# See the docstring in versioneer.py for instructions. Note that you must\n-# re-run 'versioneer.py setup' after changing this section, and commit the\n-# resulting files.\n-\n-[versioneer]\n-#VCS = git\n-#style = pep440\n-#versionfile_source =\n-#versionfile_build =\n-#tag_prefix =\n-#parentdir_prefix =\n-\n-\"\"\"\n-\n-OLD_SNIPPET = \"\"\"\n-from ._version import get_versions\n-__version__ = get_versions()['version']\n-del get_versions\n-\"\"\"\n-\n-INIT_PY_SNIPPET = \"\"\"\n-from . import {0}\n-__version__ = {0}.get_versions()['version']\n-\"\"\"\n-\n-\n-def do_setup() -> int:\n-    \"\"\"Do main VCS-independent setup function for installing Versioneer.\"\"\"\n-    root = get_root()\n-    try:\n-        cfg = get_config_from_root(root)\n-    except (OSError, configparser.NoSectionError,\n-            configparser.NoOptionError) as e:\n-        if isinstance(e, (OSError, configparser.NoSectionError)):\n-            print(\"Adding sample versioneer config to setup.cfg\",\n-                  file=sys.stderr)\n-            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n-                f.write(SAMPLE_CONFIG)\n-        print(CONFIG_ERROR, file=sys.stderr)\n-        return 1\n-\n-    print(\" creating %s\" % cfg.versionfile_source)\n-    with open(cfg.versionfile_source, \"w\") as f:\n-        LONG = LONG_VERSION_PY[cfg.VCS]\n-        f.write(LONG % {\"DOLLAR\": \"$\",\n-                        \"STYLE\": cfg.style,\n-                        \"TAG_PREFIX\": cfg.tag_prefix,\n-                        \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n-                        \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n-                        })\n-\n-    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n-                       \"__init__.py\")\n-    maybe_ipy: Optional[str] = ipy\n-    if os.path.exists(ipy):\n-        try:\n-            with open(ipy, \"r\") as f:\n-                old = f.read()\n-        except OSError:\n-            old = \"\"\n-        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]\n-        snippet = INIT_PY_SNIPPET.format(module)\n-        if OLD_SNIPPET in old:\n-            print(\" replacing boilerplate in %s\" % ipy)\n-            with open(ipy, \"w\") as f:\n-                f.write(old.replace(OLD_SNIPPET, snippet))\n-        elif snippet not in old:\n-            print(\" appending to %s\" % ipy)\n-            with open(ipy, \"a\") as f:\n-                f.write(snippet)\n-        else:\n-            print(\" %s unmodified\" % ipy)\n-    else:\n-        print(\" %s doesn't exist, ok\" % ipy)\n-        maybe_ipy = None\n-\n-    # Make VCS-specific changes. For git, this means creating/changing\n-    # .gitattributes to mark _version.py for export-subst keyword\n-    # substitution.\n-    do_vcs_install(cfg.versionfile_source, maybe_ipy)\n-    return 0\n-\n-\n-def scan_setup_py() -> int:\n-    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n-    found = set()\n-    setters = False\n-    errors = 0\n-    with open(\"setup.py\", \"r\") as f:\n-        for line in f.readlines():\n-            if \"import versioneer\" in line:\n-                found.add(\"import\")\n-            if \"versioneer.get_cmdclass()\" in line:\n-                found.add(\"cmdclass\")\n-            if \"versioneer.get_version()\" in line:\n-                found.add(\"get_version\")\n-            if \"versioneer.VCS\" in line:\n-                setters = True\n-            if \"versioneer.versionfile_source\" in line:\n-                setters = True\n-    if len(found) != 3:\n-        print(\"\")\n-        print(\"Your setup.py appears to be missing some important items\")\n-        print(\"(but I might be wrong). Please make sure it has something\")\n-        print(\"roughly like the following:\")\n-        print(\"\")\n-        print(\" import versioneer\")\n-        print(\" setup( version=versioneer.get_version(),\")\n-        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n-        print(\"\")\n-        errors += 1\n-    if setters:\n-        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n-        print(\"'versioneer.versionfile_source = ' . This configuration\")\n-        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n-        print(\"\")\n-        errors += 1\n-    return errors\n-\n-\n-def setup_command() -> NoReturn:\n-    \"\"\"Set up Versioneer and exit with appropriate error code.\"\"\"\n-    errors = do_setup()\n-    errors += scan_setup_py()\n-    sys.exit(1 if errors else 0)\n-\n-\n-if __name__ == \"__main__\":\n-    cmd = sys.argv[1]\n-    if cmd == \"setup\":\n-        setup_command()\n", "instance_id": "simonsobs__ocs-403", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a bug in setuptools version 72.0.0 or higher causes OCS container builds to fail, with a detailed error log provided to illustrate the failure during the `pip3 install .` step. The goal of resolving the build failure by pinning setuptools to a version below 72.0.0 is explicit, and a reference to an upstream issue on GitHub adds credibility and context. However, there are minor ambiguities and missing details. For instance, the statement does not specify the exact environment or dependencies beyond setuptools that might contribute to the issue (e.g., Python version, Docker image details). Additionally, while the error log is helpful, there is no mention of specific constraints or requirements for the solution beyond pinning the version, such as compatibility with other tools or long-term resolution strategies. Edge cases or potential side effects of pinning setuptools are also not addressed. Overall, the problem is valid and clear but lacks some minor contextual details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the easy range (0.2-0.4) due to the nature of the solution and the scope of changes required. The primary issue is a compatibility bug in setuptools, and the proposed solution\u2014pinning setuptools to a version below 72.0.0\u2014is straightforward and involves minimal code changes. However, the code changes provided go beyond just pinning setuptools; they include a significant overhaul of the project's build system, migrating from a `setup.py` and `versioneer`-based setup to a modern `pyproject.toml` and `hatchling`/`hatch-vcs` configuration. This migration introduces additional complexity, requiring an understanding of Python packaging standards, build tools, and version control integration for dynamic versioning. The changes span multiple files (e.g., `pyproject.toml`, `setup.py` deletion, updates to `__init__.py`), but they do not significantly impact the core architecture of the OCS system itself, as they are confined to build and configuration aspects. The number of technical concepts involved is moderate, including familiarity with Python packaging, build systems (setuptools, hatchling), and version management. There are no complex edge cases or error handling requirements explicitly mentioned in the problem statement, though the migration to a new build system could introduce subtle compatibility issues with existing workflows or environments. Overall, while the core fix (pinning setuptools) is very easy, the broader context of the build system migration pushes the difficulty slightly higher within the easy range, to 0.35, as it requires some understanding of multiple tools and careful validation to ensure the new setup works as expected.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "recv_timeout should be used as a receive timeout in _sock_exact_recv()\n`socket_timeout` is period which the underlying implementation will break out from a \"select\" loop, \"recv_timeout\" should be the maximum period for which to wait for data from the server. `_sock_exact_recv()` however uses `keep_alive` as a receive timeout:\r\nhttps://github.com/adafruit/Adafruit_CircuitPython_MiniMQTT/blob/1c25441a34f4e8d841bdf554ca772e773247eec5/adafruit_minimqtt/adafruit_minimqtt.py#L1118-L1127\r\n\r\nBy default `socket_timeout` is 1 second, `recv_timeout` is 10 seconds and `keep_alive` is 60 seconds.\r\nAs a result, when a server disconnects the client and the client pings it, it will take 60 seconds for the client to discover  that the connection is unresponsive. Instead, `recv_timeout` should be used. The way I understand it is that keep alive is server side feature and should not trump receive timeout on the client. \r\n\r\n~The fix would simplify the various `No data ...` `MMQTTException`s used throughout the code.~\r\n\n", "patch": "diff --git a/adafruit_minimqtt/adafruit_minimqtt.py b/adafruit_minimqtt/adafruit_minimqtt.py\nindex 7ede955..2ca47ae 100644\n--- a/adafruit_minimqtt/adafruit_minimqtt.py\n+++ b/adafruit_minimqtt/adafruit_minimqtt.py\n@@ -593,7 +593,7 @@ def ping(self) -> list[int]:\n         self._connected()\n         self.logger.debug(\"Sending PINGREQ\")\n         self._sock.send(MQTT_PINGREQ)\n-        ping_timeout = self.keep_alive\n+        ping_timeout = self._recv_timeout\n         stamp = self.get_monotonic_time()\n         self._last_msg_sent_timestamp = stamp\n         rc, rcs = None, []\n@@ -602,7 +602,9 @@ def ping(self) -> list[int]:\n             if rc:\n                 rcs.append(rc)\n             if self.get_monotonic_time() - stamp > ping_timeout:\n-                raise MMQTTException(\"PINGRESP not returned from broker.\")\n+                raise MMQTTException(\n+                    f\"PINGRESP not returned from broker within {ping_timeout} seconds.\"\n+                )\n         return rcs\n \n     # pylint: disable=too-many-branches, too-many-statements\n@@ -1068,7 +1070,7 @@ def _sock_exact_recv(\n             to_read = bufsize - recv_len\n             if to_read < 0:\n                 raise MMQTTException(f\"negative number of bytes to read: {to_read}\")\n-            read_timeout = timeout if timeout is not None else self.keep_alive\n+            read_timeout = timeout if timeout is not None else self._recv_timeout\n             mv = mv[recv_len:]\n             while to_read > 0:\n                 recv_len = self._sock.recv_into(mv, to_read)\n@@ -1079,7 +1081,7 @@ def _sock_exact_recv(\n                         f\"Unable to receive {to_read} bytes within {read_timeout} seconds.\"\n                     )\n         else:  # ESP32SPI Impl.\n-            # This will timeout with socket timeout (not keepalive timeout)\n+            # This will time out with socket timeout (not receive timeout).\n             rc = self._sock.recv(bufsize)\n             if not rc:\n                 self.logger.debug(\"_sock_exact_recv timeout\")\n@@ -1089,7 +1091,7 @@ def _sock_exact_recv(\n             # or raise exception if wait longer than read_timeout\n             to_read = bufsize - len(rc)\n             assert to_read >= 0\n-            read_timeout = self.keep_alive\n+            read_timeout = self._recv_timeout\n             while to_read > 0:\n                 recv = self._sock.recv(to_read)\n                 to_read -= len(recv)\n", "instance_id": "adafruit__Adafruit_CircuitPython_MiniMQTT-190", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the function `_sock_exact_recv()` in the Adafruit_CircuitPython_MiniMQTT library incorrectly uses `keep_alive` as a timeout value instead of `recv_timeout`, leading to a longer-than-expected delay (60 seconds instead of 10 seconds) when a server disconnects. The goal of using `recv_timeout` as the correct timeout period is evident, and the provided link to the specific code section helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly clarify the expected behavior in all scenarios (e.g., how `socket_timeout` interacts with `recv_timeout` in edge cases) or provide examples of failure cases. Additionally, the crossed-out sentence about simplifying `MMQTTException`s is confusing and irrelevant to the provided code changes, as no such simplification is reflected in the diff. Overall, while the core issue is understandable, the lack of detailed edge case descriptions or comprehensive examples prevents it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are localized to a single file (`adafruit_minimqtt.py`) and involve straightforward modifications in a few specific functions (`ping()` and `_sock_exact_recv()`). The changes primarily replace `keep_alive` with `self._recv_timeout` as the timeout value and update related error messages or comments. The diff shows a small amount of code change (less than 20 lines), with no impact on the broader system architecture or interactions between multiple modules.\n\n2. **Number of Technical Concepts:** Solving this problem requires basic familiarity with Python, object-oriented programming (understanding class attributes like `self._recv_timeout`), and a rudimentary understanding of socket programming concepts (e.g., timeouts in network communication). No advanced algorithms, design patterns, or domain-specific knowledge beyond basic MQTT client behavior are needed. The concepts involved are relatively simple for anyone with intermediate programming experience.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases beyond the general issue of server disconnection. The code changes include a minor improvement in error messaging (adding the timeout value to the exception text), but no complex error handling logic is introduced or required. Potential edge cases, such as interactions between `socket_timeout` and `recv_timeout` or behavior under network interruptions, are not addressed in the problem statement or code changes, and solving the core issue does not necessitate handling them.\n\n4. **Overall Complexity:** The task is essentially a bug fix that involves understanding a small part of the codebase and making targeted replacements. It does not require deep knowledge of the entire library or complex debugging. The fix is mechanical\u2014replacing one timeout variable with another\u2014and the provided diff already demonstrates the solution, further reducing the cognitive load.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task. It requires minimal effort beyond identifying the correct variable to use for the timeout and updating a few lines of code, making it suitable for a junior or intermediate developer with basic Python and networking knowledge.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Configure Renovate\n[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nWelcome to [Renovate](https://togithub.com/renovatebot/renovate)! This is an onboarding PR to help you understand and configure settings before regular Pull Requests begin.\n\n\ud83d\udea6 To activate Renovate, merge this Pull Request. To disable Renovate, simply close this Pull Request unmerged.\n\n\n\n---\n### Detected Package Files\n\n * `.github/workflows/build.yml` (github-actions)\n * `.github/workflows/lint-pr.yml` (github-actions)\n * `hooks/openfeature-hooks-opentelemetry/pyproject.toml` (pep621)\n * `providers/openfeature-provider-flagd/pyproject.toml` (pep621)\n\n### Configuration Summary\n\nBased on the default config's presets, Renovate will:\n\n  - Start dependency updates only once this onboarding PR is merged\n  - Show all Merge Confidence badges for pull requests.\n  - Enable Renovate Dependency Dashboard creation.\n  - Use semantic commit type `fix` for dependencies and `chore` for all others if semantic commits are in use.\n  - Ignore `node_modules`, `bower_components`, `vendor` and various test/tests directories.\n  - Group known monorepo packages together.\n  - Use curated list of recommended non-monorepo package groupings.\n  - Apply crowd-sourced package replacement rules.\n  - Apply crowd-sourced workarounds for known problems with packages.\n\n\ud83d\udd21 Do you want to change how Renovate upgrades your dependencies? Add your custom config to `renovate.json` in this branch. Renovate will update the Pull Request description the next time it runs.\n\n---\n\n### What to Expect\n\nWith your current configuration, Renovate will create 1 Pull Request:\n\n<details>\n<summary>chore(deps): update bufbuild/buf-setup-action action to v1.29.0</summary>\n\n  - Schedule: [\"at any time\"]\n  - Branch name: `renovate/bufbuild-buf-setup-action-1.x`\n  - Merge into: `main`\n  - Upgrade [bufbuild/buf-setup-action](https://togithub.com/bufbuild/buf-setup-action) to `v1.29.0`\n\n\n</details>\n\n\n---\n\n\u2753 Got questions? Check out Renovate's [Docs](https://docs.renovatebot.com/), particularly the Getting Started section.\nIf you need any further assistance then you can also [request help here](https://togithub.com/renovatebot/renovate/discussions).\n\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/open-feature/python-sdk-contrib).\n\n<!--renovate-config-hash:e80b4e42a3043bc12fa0640db4bac392d2bf770acf841360d7c8ceeeac2ec1a9-->\n\n", "patch": "diff --git a/.gitmodules b/.gitmodules\nindex 0a552e2f..a8bef85f 100644\n--- a/.gitmodules\n+++ b/.gitmodules\n@@ -1,3 +1,6 @@\n [submodule \"schemas\"]\n \tpath = providers/openfeature-provider-flagd/schemas\n \turl = https://github.com/open-feature/schemas\n+[submodule \"providers/openfeature-provider-flagd/test-harness\"]\n+\tpath = providers/openfeature-provider-flagd/test-harness\n+\turl = git@github.com:open-feature/flagd-testbed.git\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 508d68c5..ac517000 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -19,8 +19,13 @@ repos:\n     rev: v1.9.0\n     hooks:\n       - id: mypy\n+        args: [--python-version=3.8]\n         additional_dependencies:\n           - openfeature-sdk>=0.4.0\n           - opentelemetry-api\n           - types-protobuf\n+          - types-PyYAML\n+          - mmh3\n+          - semver\n+          - panzi-json-logic\n         exclude: proto|tests\ndiff --git a/providers/openfeature-provider-flagd/README.md b/providers/openfeature-provider-flagd/README.md\nindex ea2a533c..aa63c55c 100644\n--- a/providers/openfeature-provider-flagd/README.md\n+++ b/providers/openfeature-provider-flagd/README.md\n@@ -19,6 +19,19 @@ from openfeature.contrib.provider.flagd import FlagdProvider\n api.set_provider(FlagdProvider())\n ```\n \n+To use in-process evaluation in offline mode with a file as source:\n+\n+```python\n+from openfeature import api\n+from openfeature.contrib.provider.flagd import FlagdProvider\n+from openfeature.contrib.provider.flagd.config import ResolverType\n+\n+api.set_provider(FlagdProvider(\n+    resolver_type=ResolverType.IN_PROCESS,\n+    offline_flag_source_path=\"my-flag.json\",\n+))\n+```\n+\n ### Configuration options\n \n The default options can be defined in the FlagdProvider constructor.\ndiff --git a/providers/openfeature-provider-flagd/pyproject.toml b/providers/openfeature-provider-flagd/pyproject.toml\nindex cd435bc7..6fe406b6 100644\n--- a/providers/openfeature-provider-flagd/pyproject.toml\n+++ b/providers/openfeature-provider-flagd/pyproject.toml\n@@ -20,6 +20,10 @@ dependencies = [\n   \"openfeature-sdk>=0.4.0\",\n   \"grpcio>=1.60.0\",\n   \"protobuf>=4.25.2\",\n+  \"mmh3>=4.1.0\",\n+  \"panzi-json-logic>=1.0.1\",\n+  \"semver>=3,<4\",\n+  \"pyyaml>=6.0.1\",\n ]\n requires-python = \">=3.8\"\n \n@@ -32,6 +36,7 @@ Homepage = \"https://github.com/open-feature/python-sdk-contrib\"\n dependencies = [\n   \"coverage[toml]>=6.5\",\n   \"pytest\",\n+  \"pytest-bdd\",\n ]\n post-install-commands = [\n   \"./scripts/gen_protos.sh\"\n@@ -42,6 +47,7 @@ test = \"pytest {args:tests}\"\n test-cov = \"coverage run -m pytest {args:tests}\"\n cov-report = [\n   \"coverage xml\",\n+  \"coverage html\",\n ]\n cov = [\n   \"test-cov\",\n@@ -61,4 +67,5 @@ packages = [\"src/openfeature\"]\n omit = [\n   # exclude generated files\n   \"src/openfeature/contrib/provider/flagd/proto/*\",\n+  \"tests/**\",\n ]\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/config.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/config.py\nindex e2db98a5..a95c3153 100644\n--- a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/config.py\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/config.py\n@@ -1,5 +1,6 @@\n import os\n import typing\n+from enum import Enum\n \n T = typing.TypeVar(\"T\")\n \n@@ -17,13 +18,21 @@ def env_or_default(\n     return val if cast is None else cast(val)\n \n \n+class ResolverType(Enum):\n+    GRPC = \"grpc\"\n+    IN_PROCESS = \"in-process\"\n+\n+\n class Config:\n-    def __init__(\n+    def __init__(  # noqa: PLR0913\n         self,\n         host: typing.Optional[str] = None,\n         port: typing.Optional[int] = None,\n         tls: typing.Optional[bool] = None,\n         timeout: typing.Optional[int] = None,\n+        resolver_type: typing.Optional[ResolverType] = None,\n+        offline_flag_source_path: typing.Optional[str] = None,\n+        offline_poll_interval_seconds: typing.Optional[float] = None,\n     ):\n         self.host = env_or_default(\"FLAGD_HOST\", \"localhost\") if host is None else host\n         self.port = (\n@@ -33,3 +42,18 @@ def __init__(\n             env_or_default(\"FLAGD_TLS\", False, cast=str_to_bool) if tls is None else tls\n         )\n         self.timeout = 5 if timeout is None else timeout\n+        self.resolver_type = (\n+            ResolverType(env_or_default(\"FLAGD_RESOLVER_TYPE\", \"grpc\"))\n+            if resolver_type is None\n+            else resolver_type\n+        )\n+        self.offline_flag_source_path = (\n+            env_or_default(\"FLAGD_OFFLINE_FLAG_SOURCE_PATH\", None)\n+            if offline_flag_source_path is None\n+            else offline_flag_source_path\n+        )\n+        self.offline_poll_interval_seconds = (\n+            float(env_or_default(\"FLAGD_OFFLINE_POLL_INTERVAL_SECONDS\", 1.0))\n+            if offline_poll_interval_seconds is None\n+            else offline_poll_interval_seconds\n+        )\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/provider.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/provider.py\nindex ea91a1d1..76307475 100644\n--- a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/provider.py\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/provider.py\n@@ -23,24 +23,13 @@\n \n import typing\n \n-import grpc\n-from google.protobuf.struct_pb2 import Struct\n-\n from openfeature.evaluation_context import EvaluationContext\n-from openfeature.exception import (\n-    FlagNotFoundError,\n-    GeneralError,\n-    InvalidContextError,\n-    ParseError,\n-    TypeMismatchError,\n-)\n from openfeature.flag_evaluation import FlagResolutionDetails\n from openfeature.provider.metadata import Metadata\n from openfeature.provider.provider import AbstractProvider\n \n-from .config import Config\n-from .flag_type import FlagType\n-from .proto.schema.v1 import schema_pb2, schema_pb2_grpc\n+from .config import Config, ResolverType\n+from .resolvers import AbstractResolver, GrpcResolver, InProcessResolver\n \n T = typing.TypeVar(\"T\")\n \n@@ -48,12 +37,15 @@\n class FlagdProvider(AbstractProvider):\n     \"\"\"Flagd OpenFeature Provider\"\"\"\n \n-    def __init__(\n+    def __init__(  # noqa: PLR0913\n         self,\n         host: typing.Optional[str] = None,\n         port: typing.Optional[int] = None,\n         tls: typing.Optional[bool] = None,\n         timeout: typing.Optional[int] = None,\n+        resolver_type: typing.Optional[ResolverType] = None,\n+        offline_flag_source_path: typing.Optional[str] = None,\n+        offline_poll_interval_seconds: typing.Optional[float] = None,\n     ):\n         \"\"\"\n         Create an instance of the FlagdProvider\n@@ -68,14 +60,26 @@ def __init__(\n             port=port,\n             tls=tls,\n             timeout=timeout,\n+            resolver_type=resolver_type,\n+            offline_flag_source_path=offline_flag_source_path,\n+            offline_poll_interval_seconds=offline_poll_interval_seconds,\n         )\n \n-        channel_factory = grpc.secure_channel if tls else grpc.insecure_channel\n-        self.channel = channel_factory(f\"{self.config.host}:{self.config.port}\")\n-        self.stub = schema_pb2_grpc.ServiceStub(self.channel)\n+        self.resolver = self.setup_resolver()\n+\n+    def setup_resolver(self) -> AbstractResolver:\n+        if self.config.resolver_type == ResolverType.GRPC:\n+            return GrpcResolver(self.config)\n+        elif self.config.resolver_type == ResolverType.IN_PROCESS:\n+            return InProcessResolver(self.config, self)\n+        else:\n+            raise ValueError(\n+                f\"`resolver_type` parameter invalid: {self.config.resolver_type}\"\n+            )\n \n     def shutdown(self) -> None:\n-        self.channel.close()\n+        if self.resolver:\n+            self.resolver.shutdown()\n \n     def get_metadata(self) -> Metadata:\n         \"\"\"Returns provider metadata\"\"\"\n@@ -87,7 +91,9 @@ def resolve_boolean_details(\n         default_value: bool,\n         evaluation_context: typing.Optional[EvaluationContext] = None,\n     ) -> FlagResolutionDetails[bool]:\n-        return self._resolve(key, FlagType.BOOLEAN, default_value, evaluation_context)\n+        return self.resolver.resolve_boolean_details(\n+            key, default_value, evaluation_context\n+        )\n \n     def resolve_string_details(\n         self,\n@@ -95,7 +101,9 @@ def resolve_string_details(\n         default_value: str,\n         evaluation_context: typing.Optional[EvaluationContext] = None,\n     ) -> FlagResolutionDetails[str]:\n-        return self._resolve(key, FlagType.STRING, default_value, evaluation_context)\n+        return self.resolver.resolve_string_details(\n+            key, default_value, evaluation_context\n+        )\n \n     def resolve_float_details(\n         self,\n@@ -103,7 +111,9 @@ def resolve_float_details(\n         default_value: float,\n         evaluation_context: typing.Optional[EvaluationContext] = None,\n     ) -> FlagResolutionDetails[float]:\n-        return self._resolve(key, FlagType.FLOAT, default_value, evaluation_context)\n+        return self.resolver.resolve_float_details(\n+            key, default_value, evaluation_context\n+        )\n \n     def resolve_integer_details(\n         self,\n@@ -111,7 +121,9 @@ def resolve_integer_details(\n         default_value: int,\n         evaluation_context: typing.Optional[EvaluationContext] = None,\n     ) -> FlagResolutionDetails[int]:\n-        return self._resolve(key, FlagType.INTEGER, default_value, evaluation_context)\n+        return self.resolver.resolve_integer_details(\n+            key, default_value, evaluation_context\n+        )\n \n     def resolve_object_details(\n         self,\n@@ -119,76 +131,6 @@ def resolve_object_details(\n         default_value: typing.Union[dict, list],\n         evaluation_context: typing.Optional[EvaluationContext] = None,\n     ) -> FlagResolutionDetails[typing.Union[dict, list]]:\n-        return self._resolve(key, FlagType.OBJECT, default_value, evaluation_context)\n-\n-    def _resolve(\n-        self,\n-        flag_key: str,\n-        flag_type: FlagType,\n-        default_value: T,\n-        evaluation_context: typing.Optional[EvaluationContext],\n-    ) -> FlagResolutionDetails[T]:\n-        context = self._convert_context(evaluation_context)\n-        call_args = {\"timeout\": self.config.timeout}\n-        try:\n-            if flag_type == FlagType.BOOLEAN:\n-                request = schema_pb2.ResolveBooleanRequest(  # type:ignore[attr-defined]\n-                    flag_key=flag_key, context=context\n-                )\n-                response = self.stub.ResolveBoolean(request, **call_args)\n-            elif flag_type == FlagType.STRING:\n-                request = schema_pb2.ResolveStringRequest(  # type:ignore[attr-defined]\n-                    flag_key=flag_key, context=context\n-                )\n-                response = self.stub.ResolveString(request, **call_args)\n-            elif flag_type == FlagType.OBJECT:\n-                request = schema_pb2.ResolveObjectRequest(  # type:ignore[attr-defined]\n-                    flag_key=flag_key, context=context\n-                )\n-                response = self.stub.ResolveObject(request, **call_args)\n-            elif flag_type == FlagType.FLOAT:\n-                request = schema_pb2.ResolveFloatRequest(  # type:ignore[attr-defined]\n-                    flag_key=flag_key, context=context\n-                )\n-                response = self.stub.ResolveFloat(request, **call_args)\n-            elif flag_type == FlagType.INTEGER:\n-                request = schema_pb2.ResolveIntRequest(  # type:ignore[attr-defined]\n-                    flag_key=flag_key, context=context\n-                )\n-                response = self.stub.ResolveInt(request, **call_args)\n-            else:\n-                raise ValueError(f\"Unknown flag type: {flag_type}\")\n-\n-        except grpc.RpcError as e:\n-            code = e.code()\n-            message = f\"received grpc status code {code}\"\n-\n-            if code == grpc.StatusCode.NOT_FOUND:\n-                raise FlagNotFoundError(message) from e\n-            elif code == grpc.StatusCode.INVALID_ARGUMENT:\n-                raise TypeMismatchError(message) from e\n-            elif code == grpc.StatusCode.DATA_LOSS:\n-                raise ParseError(message) from e\n-            raise GeneralError(message) from e\n-\n-        # Got a valid flag and valid type. Return it.\n-        return FlagResolutionDetails(\n-            value=response.value,\n-            reason=response.reason,\n-            variant=response.variant,\n+        return self.resolver.resolve_object_details(\n+            key, default_value, evaluation_context\n         )\n-\n-    def _convert_context(\n-        self, evaluation_context: typing.Optional[EvaluationContext]\n-    ) -> Struct:\n-        s = Struct()\n-        if evaluation_context:\n-            try:\n-                s[\"targetingKey\"] = evaluation_context.targeting_key\n-                s.update(evaluation_context.attributes)\n-            except ValueError as exc:\n-                message = (\n-                    \"could not serialize evaluation context to google.protobuf.Struct\"\n-                )\n-                raise InvalidContextError(message) from exc\n-        return s\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/__init__.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/__init__.py\nnew file mode 100644\nindex 00000000..53e17938\n--- /dev/null\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/__init__.py\n@@ -0,0 +1,51 @@\n+import typing\n+\n+from typing_extensions import Protocol\n+\n+from openfeature.evaluation_context import EvaluationContext\n+from openfeature.flag_evaluation import FlagResolutionDetails\n+\n+from .grpc import GrpcResolver\n+from .in_process import InProcessResolver\n+\n+\n+class AbstractResolver(Protocol):\n+    def shutdown(self) -> None: ...\n+\n+    def resolve_boolean_details(\n+        self,\n+        key: str,\n+        default_value: bool,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[bool]: ...\n+\n+    def resolve_string_details(\n+        self,\n+        key: str,\n+        default_value: str,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[str]: ...\n+\n+    def resolve_float_details(\n+        self,\n+        key: str,\n+        default_value: float,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[float]: ...\n+\n+    def resolve_integer_details(\n+        self,\n+        key: str,\n+        default_value: int,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[int]: ...\n+\n+    def resolve_object_details(\n+        self,\n+        key: str,\n+        default_value: typing.Union[dict, list],\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[typing.Union[dict, list]]: ...\n+\n+\n+__all__ = [\"AbstractResolver\", \"GrpcResolver\", \"InProcessResolver\"]\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/grpc.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/grpc.py\nnew file mode 100644\nindex 00000000..caab101a\n--- /dev/null\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/grpc.py\n@@ -0,0 +1,145 @@\n+import typing\n+\n+import grpc\n+from google.protobuf.struct_pb2 import Struct\n+\n+from openfeature.evaluation_context import EvaluationContext\n+from openfeature.exception import (\n+    FlagNotFoundError,\n+    GeneralError,\n+    InvalidContextError,\n+    ParseError,\n+    TypeMismatchError,\n+)\n+from openfeature.flag_evaluation import FlagResolutionDetails\n+\n+from ..config import Config\n+from ..flag_type import FlagType\n+from ..proto.schema.v1 import schema_pb2, schema_pb2_grpc\n+\n+T = typing.TypeVar(\"T\")\n+\n+\n+class GrpcResolver:\n+    def __init__(self, config: Config):\n+        self.config = config\n+        channel_factory = (\n+            grpc.secure_channel if self.config.tls else grpc.insecure_channel\n+        )\n+        self.channel = channel_factory(f\"{self.config.host}:{self.config.port}\")\n+        self.stub = schema_pb2_grpc.ServiceStub(self.channel)\n+\n+    def shutdown(self) -> None:\n+        self.channel.close()\n+\n+    def resolve_boolean_details(\n+        self,\n+        key: str,\n+        default_value: bool,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[bool]:\n+        return self._resolve(key, FlagType.BOOLEAN, default_value, evaluation_context)\n+\n+    def resolve_string_details(\n+        self,\n+        key: str,\n+        default_value: str,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[str]:\n+        return self._resolve(key, FlagType.STRING, default_value, evaluation_context)\n+\n+    def resolve_float_details(\n+        self,\n+        key: str,\n+        default_value: float,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[float]:\n+        return self._resolve(key, FlagType.FLOAT, default_value, evaluation_context)\n+\n+    def resolve_integer_details(\n+        self,\n+        key: str,\n+        default_value: int,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[int]:\n+        return self._resolve(key, FlagType.INTEGER, default_value, evaluation_context)\n+\n+    def resolve_object_details(\n+        self,\n+        key: str,\n+        default_value: typing.Union[dict, list],\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[typing.Union[dict, list]]:\n+        return self._resolve(key, FlagType.OBJECT, default_value, evaluation_context)\n+\n+    def _resolve(\n+        self,\n+        flag_key: str,\n+        flag_type: FlagType,\n+        default_value: T,\n+        evaluation_context: typing.Optional[EvaluationContext],\n+    ) -> FlagResolutionDetails[T]:\n+        context = self._convert_context(evaluation_context)\n+        call_args = {\"timeout\": self.config.timeout}\n+        try:\n+            if flag_type == FlagType.BOOLEAN:\n+                request = schema_pb2.ResolveBooleanRequest(  # type:ignore[attr-defined]\n+                    flag_key=flag_key, context=context\n+                )\n+                response = self.stub.ResolveBoolean(request, **call_args)\n+            elif flag_type == FlagType.STRING:\n+                request = schema_pb2.ResolveStringRequest(  # type:ignore[attr-defined]\n+                    flag_key=flag_key, context=context\n+                )\n+                response = self.stub.ResolveString(request, **call_args)\n+            elif flag_type == FlagType.OBJECT:\n+                request = schema_pb2.ResolveObjectRequest(  # type:ignore[attr-defined]\n+                    flag_key=flag_key, context=context\n+                )\n+                response = self.stub.ResolveObject(request, **call_args)\n+            elif flag_type == FlagType.FLOAT:\n+                request = schema_pb2.ResolveFloatRequest(  # type:ignore[attr-defined]\n+                    flag_key=flag_key, context=context\n+                )\n+                response = self.stub.ResolveFloat(request, **call_args)\n+            elif flag_type == FlagType.INTEGER:\n+                request = schema_pb2.ResolveIntRequest(  # type:ignore[attr-defined]\n+                    flag_key=flag_key, context=context\n+                )\n+                response = self.stub.ResolveInt(request, **call_args)\n+            else:\n+                raise ValueError(f\"Unknown flag type: {flag_type}\")\n+\n+        except grpc.RpcError as e:\n+            code = e.code()\n+            message = f\"received grpc status code {code}\"\n+\n+            if code == grpc.StatusCode.NOT_FOUND:\n+                raise FlagNotFoundError(message) from e\n+            elif code == grpc.StatusCode.INVALID_ARGUMENT:\n+                raise TypeMismatchError(message) from e\n+            elif code == grpc.StatusCode.DATA_LOSS:\n+                raise ParseError(message) from e\n+            raise GeneralError(message) from e\n+\n+        # Got a valid flag and valid type. Return it.\n+        return FlagResolutionDetails(\n+            value=response.value,\n+            reason=response.reason,\n+            variant=response.variant,\n+        )\n+\n+    def _convert_context(\n+        self, evaluation_context: typing.Optional[EvaluationContext]\n+    ) -> Struct:\n+        s = Struct()\n+        if evaluation_context:\n+            try:\n+                s[\"targetingKey\"] = evaluation_context.targeting_key\n+                s.update(evaluation_context.attributes)\n+            except ValueError as exc:\n+                message = (\n+                    \"could not serialize evaluation context to google.protobuf.Struct\"\n+                )\n+                raise InvalidContextError(message) from exc\n+        return s\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/in_process.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/in_process.py\nnew file mode 100644\nindex 00000000..907a62d6\n--- /dev/null\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/in_process.py\n@@ -0,0 +1,122 @@\n+import time\n+import typing\n+\n+from json_logic import builtins, jsonLogic  # type: ignore[import-untyped]\n+\n+from openfeature.evaluation_context import EvaluationContext\n+from openfeature.exception import FlagNotFoundError, ParseError\n+from openfeature.flag_evaluation import FlagResolutionDetails, Reason\n+from openfeature.provider.provider import AbstractProvider\n+\n+from ..config import Config\n+from .process.custom_ops import ends_with, fractional, sem_ver, starts_with\n+from .process.file_watcher import FileWatcherFlagStore\n+\n+T = typing.TypeVar(\"T\")\n+\n+\n+class InProcessResolver:\n+    OPERATORS: typing.ClassVar[dict] = {\n+        **builtins.BUILTINS,\n+        \"fractional\": fractional,\n+        \"starts_with\": starts_with,\n+        \"ends_with\": ends_with,\n+        \"sem_ver\": sem_ver,\n+    }\n+\n+    def __init__(self, config: Config, provider: AbstractProvider):\n+        self.config = config\n+        self.provider = provider\n+        if not self.config.offline_flag_source_path:\n+            raise ValueError(\n+                \"offline_flag_source_path must be provided when using in-process resolver\"\n+            )\n+        self.flag_store = FileWatcherFlagStore(\n+            self.config.offline_flag_source_path,\n+            self.provider,\n+            self.config.offline_poll_interval_seconds,\n+        )\n+\n+    def shutdown(self) -> None:\n+        self.flag_store.shutdown()\n+\n+    def resolve_boolean_details(\n+        self,\n+        key: str,\n+        default_value: bool,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[bool]:\n+        return self._resolve(key, default_value, evaluation_context)\n+\n+    def resolve_string_details(\n+        self,\n+        key: str,\n+        default_value: str,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[str]:\n+        return self._resolve(key, default_value, evaluation_context)\n+\n+    def resolve_float_details(\n+        self,\n+        key: str,\n+        default_value: float,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[float]:\n+        return self._resolve(key, default_value, evaluation_context)\n+\n+    def resolve_integer_details(\n+        self,\n+        key: str,\n+        default_value: int,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[int]:\n+        return self._resolve(key, default_value, evaluation_context)\n+\n+    def resolve_object_details(\n+        self,\n+        key: str,\n+        default_value: typing.Union[dict, list],\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[typing.Union[dict, list]]:\n+        return self._resolve(key, default_value, evaluation_context)\n+\n+    def _resolve(\n+        self,\n+        key: str,\n+        default_value: T,\n+        evaluation_context: typing.Optional[EvaluationContext] = None,\n+    ) -> FlagResolutionDetails[T]:\n+        flag = self.flag_store.get_flag(key)\n+        if not flag:\n+            raise FlagNotFoundError(f\"Flag with key {key} not present in flag store.\")\n+\n+        if flag.state == \"DISABLED\":\n+            return FlagResolutionDetails(default_value, reason=Reason.DISABLED)\n+\n+        if not flag.targeting:\n+            variant, value = flag.default\n+            return FlagResolutionDetails(value, variant=variant, reason=Reason.STATIC)\n+\n+        json_logic_context = evaluation_context.attributes if evaluation_context else {}\n+        json_logic_context[\"$flagd\"] = {\"flagKey\": key, \"timestamp\": int(time.time())}\n+        json_logic_context[\"targetingKey\"] = (\n+            evaluation_context.targeting_key if evaluation_context else None\n+        )\n+        variant = jsonLogic(flag.targeting, json_logic_context, self.OPERATORS)\n+        if variant is None:\n+            variant, value = flag.default\n+            return FlagResolutionDetails(value, variant=variant, reason=Reason.DEFAULT)\n+        if not isinstance(variant, (str, bool)):\n+            raise ParseError(\n+                \"Parsed JSONLogic targeting did not return a string or bool\"\n+            )\n+\n+        variant, value = flag.get_variant(variant)\n+        if not value:\n+            raise ParseError(f\"Resolved variant {variant} not in variants config.\")\n+\n+        return FlagResolutionDetails(\n+            value,\n+            variant=variant,\n+            reason=Reason.TARGETING_MATCH,\n+        )\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/custom_ops.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/custom_ops.py\nnew file mode 100644\nindex 00000000..17763615\n--- /dev/null\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/custom_ops.py\n@@ -0,0 +1,126 @@\n+import logging\n+import typing\n+\n+import mmh3\n+import semver\n+\n+JsonPrimitive = typing.Union[str, bool, float, int]\n+JsonLogicArg = typing.Union[JsonPrimitive, typing.Sequence[JsonPrimitive]]\n+\n+logger = logging.getLogger(\"openfeature.contrib\")\n+\n+\n+def fractional(data: dict, *args: JsonLogicArg) -> typing.Optional[str]:\n+    if not args:\n+        logger.error(\"No arguments provided to fractional operator.\")\n+        return None\n+\n+    bucket_by = None\n+    if isinstance(args[0], str):\n+        bucket_by = args[0]\n+        args = args[1:]\n+    else:\n+        seed = data.get(\"$flagd\", {}).get(\"flagKey\", \"\")\n+        targeting_key = data.get(\"targetingKey\")\n+        if not targeting_key:\n+            logger.error(\"No targetingKey provided for fractional shorthand syntax.\")\n+            return None\n+        bucket_by = seed + targeting_key\n+\n+    if not bucket_by:\n+        logger.error(\"No hashKey value resolved\")\n+        return None\n+\n+    hash_ratio = abs(mmh3.hash(bucket_by)) / (2**31 - 1)\n+    bucket = int(hash_ratio * 100)\n+\n+    for arg in args:\n+        if (\n+            not isinstance(arg, (tuple, list))\n+            or len(arg) != 2\n+            or not isinstance(arg[0], str)\n+            or not isinstance(arg[1], int)\n+        ):\n+            logger.error(\"Fractional variant weights must be (str, int) tuple\")\n+            return None\n+    variant_weights: typing.Tuple[typing.Tuple[str, int]] = args  # type: ignore[assignment]\n+\n+    range_end = 0\n+    for variant, weight in variant_weights:\n+        range_end += weight\n+        if bucket < range_end:\n+            return variant\n+\n+    return None\n+\n+\n+def starts_with(data: dict, *args: JsonLogicArg) -> typing.Optional[bool]:\n+    def f(s1: str, s2: str) -> bool:\n+        return s1.startswith(s2)\n+\n+    return string_comp(f, data, *args)\n+\n+\n+def ends_with(data: dict, *args: JsonLogicArg) -> typing.Optional[bool]:\n+    def f(s1: str, s2: str) -> bool:\n+        return s1.endswith(s2)\n+\n+    return string_comp(f, data, *args)\n+\n+\n+def string_comp(\n+    comparator: typing.Callable[[str, str], bool], data: dict, *args: JsonLogicArg\n+) -> typing.Optional[bool]:\n+    if not args:\n+        logger.error(\"No arguments provided to string_comp operator.\")\n+        return None\n+    if len(args) != 2:\n+        logger.error(\"Exactly 2 args expected for string_comp operator.\")\n+        return None\n+    arg1, arg2 = args\n+    if not isinstance(arg1, str):\n+        logger.debug(f\"incorrect argument for first argument, expected string: {arg1}\")\n+        return False\n+    if not isinstance(arg2, str):\n+        logger.debug(f\"incorrect argument for second argument, expected string: {arg2}\")\n+        return False\n+\n+    return comparator(arg1, arg2)\n+\n+\n+def sem_ver(data: dict, *args: JsonLogicArg) -> typing.Optional[bool]:  # noqa: C901\n+    if not args:\n+        logger.error(\"No arguments provided to sem_ver operator.\")\n+        return None\n+    if len(args) != 3:\n+        logger.error(\"Exactly 3 args expected for sem_ver operator.\")\n+        return None\n+\n+    arg1, op, arg2 = args\n+\n+    try:\n+        v1 = semver.Version.parse(str(arg1))\n+        v2 = semver.Version.parse(str(arg2))\n+    except ValueError as e:\n+        logger.exception(e)\n+        return None\n+\n+    if op == \"=\":\n+        return v1 == v2\n+    elif op == \"!=\":\n+        return v1 != v2\n+    elif op == \"<\":\n+        return v1 < v2\n+    elif op == \"<=\":\n+        return v1 <= v2\n+    elif op == \">\":\n+        return v1 > v2\n+    elif op == \">=\":\n+        return v1 >= v2\n+    elif op == \"^\":\n+        return v1.major == v2.major\n+    elif op == \"~\":\n+        return v1.major == v2.major and v1.minor == v2.minor\n+    else:\n+        logger.error(f\"Op not supported by sem_ver: {op}\")\n+        return None\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/file_watcher.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/file_watcher.py\nnew file mode 100644\nindex 00000000..0918981f\n--- /dev/null\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/file_watcher.py\n@@ -0,0 +1,89 @@\n+import json\n+import logging\n+import os\n+import re\n+import threading\n+import time\n+import typing\n+\n+import yaml\n+\n+from openfeature.event import ProviderEventDetails\n+from openfeature.exception import ParseError\n+from openfeature.provider.provider import AbstractProvider\n+\n+from .flags import Flag\n+\n+logger = logging.getLogger(\"openfeature.contrib\")\n+\n+\n+class FileWatcherFlagStore:\n+    def __init__(\n+        self,\n+        file_path: str,\n+        provider: AbstractProvider,\n+        poll_interval_seconds: float = 1.0,\n+    ):\n+        self.file_path = file_path\n+        self.provider = provider\n+        self.poll_interval_seconds = poll_interval_seconds\n+\n+        self.last_modified = 0.0\n+        self.flag_data: typing.Mapping[str, Flag] = {}\n+        self.load_data()\n+        self.thread = threading.Thread(target=self.refresh_file, daemon=True)\n+        self.thread.start()\n+\n+    def shutdown(self) -> None:\n+        pass\n+\n+    def get_flag(self, key: str) -> typing.Optional[Flag]:\n+        return self.flag_data.get(key)\n+\n+    def refresh_file(self) -> None:\n+        while True:\n+            time.sleep(self.poll_interval_seconds)\n+            logger.debug(\"checking for new flag store contents from file\")\n+            last_modified = os.path.getmtime(self.file_path)\n+            if last_modified > self.last_modified:\n+                self.load_data(last_modified)\n+\n+    def load_data(self, modified_time: typing.Optional[float] = None) -> None:\n+        try:\n+            with open(self.file_path) as file:\n+                if self.file_path.endswith(\".yaml\"):\n+                    data = yaml.safe_load(file)\n+                else:\n+                    data = json.load(file)\n+\n+                self.flag_data = self.parse_flags(data)\n+                logger.debug(f\"{self.flag_data=}\")\n+                self.provider.emit_provider_configuration_changed(\n+                    ProviderEventDetails(flags_changed=list(self.flag_data.keys()))\n+                )\n+            self.last_modified = modified_time or os.path.getmtime(self.file_path)\n+        except FileNotFoundError:\n+            logger.exception(\"Provided file path not valid\")\n+        except json.JSONDecodeError:\n+            logger.exception(\"Could not parse JSON flag data from file\")\n+        except yaml.error.YAMLError:\n+            logger.exception(\"Could not parse YAML flag data from file\")\n+        except ParseError:\n+            logger.exception(\"Could not parse flag data using flagd syntax\")\n+        except Exception:\n+            logger.exception(\"Could not read flags from file\")\n+\n+    def parse_flags(self, flags_data: dict) -> dict:\n+        flags = flags_data.get(\"flags\", {})\n+        evaluators: typing.Optional[dict] = flags_data.get(\"$evaluators\")\n+        if evaluators:\n+            transposed = json.dumps(flags)\n+            for name, rule in evaluators.items():\n+                transposed = re.sub(\n+                    rf\"{{\\s*\\\"\\$ref\\\":\\s*\\\"{name}\\\"\\s*}}\", json.dumps(rule), transposed\n+                )\n+            flags = json.loads(transposed)\n+\n+        if not isinstance(flags, dict):\n+            raise ParseError(\"`flags` key of configuration must be a dictionary\")\n+        return {key: Flag.from_dict(key, data) for key, data in flags.items()}\ndiff --git a/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/flags.py b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/flags.py\nnew file mode 100644\nindex 00000000..0354ac42\n--- /dev/null\n+++ b/providers/openfeature-provider-flagd/src/openfeature/contrib/provider/flagd/resolvers/process/flags.py\n@@ -0,0 +1,51 @@\n+import typing\n+from dataclasses import dataclass\n+\n+from openfeature.exception import ParseError\n+\n+\n+@dataclass\n+class Flag:\n+    key: str\n+    state: str\n+    variants: typing.Mapping[str, typing.Any]\n+    default_variant: typing.Union[bool, str]\n+    targeting: typing.Optional[dict] = None\n+\n+    def __post_init__(self) -> None:\n+        if not self.state or not isinstance(self.state, str):\n+            raise ParseError(\"Incorrect 'state' value provided in flag config\")\n+\n+        if not self.variants or not isinstance(self.variants, dict):\n+            raise ParseError(\"Incorrect 'variants' value provided in flag config\")\n+\n+        if not self.default_variant or not isinstance(\n+            self.default_variant, (str, bool)\n+        ):\n+            raise ParseError(\"Incorrect 'defaultVariant' value provided in flag config\")\n+\n+        if self.targeting and not isinstance(self.targeting, dict):\n+            raise ParseError(\"Incorrect 'targeting' value provided in flag config\")\n+\n+        if self.default_variant not in self.variants:\n+            raise ParseError(\"Default variant does not match set of variants\")\n+\n+    @classmethod\n+    def from_dict(cls, key: str, data: dict) -> \"Flag\":\n+        data[\"default_variant\"] = data[\"defaultVariant\"]\n+        del data[\"defaultVariant\"]\n+        flag = cls(key=key, **data)\n+\n+        return flag\n+\n+    @property\n+    def default(self) -> typing.Tuple[str, typing.Any]:\n+        return self.get_variant(self.default_variant)\n+\n+    def get_variant(\n+        self, variant_key: typing.Union[str, bool]\n+    ) -> typing.Tuple[str, typing.Any]:\n+        if isinstance(variant_key, bool):\n+            variant_key = str(variant_key).lower()\n+\n+        return variant_key, self.variants.get(variant_key)\n", "instance_id": "open-feature__python-sdk-contrib-74", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in its intent to onboard and configure Renovate, a dependency update tool, for a repository. It provides a detailed summary of the default configuration, detected package files, and expected pull requests for dependency updates. Additionally, it includes instructions for customization and links to documentation for further assistance. However, there are minor ambiguities and missing details. For instance, it does not explicitly clarify the specific actions or decisions required from the user beyond merging or closing the PR, nor does it detail potential risks or side effects of enabling Renovate. The problem statement also lacks specific examples of how the configuration might impact the repository or what custom configurations might look like in `renovate.json`. Despite these minor gaps, the overall goal and process are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty score of 0.75 reflects a hard problem due to several factors. First, the scope and depth of code changes are significant, as they span multiple files and introduce new functionality to the `openfeature-provider-flagd` module, including the addition of an in-process resolver for offline flag evaluation. This involves not only modifying existing provider logic but also creating new modules for custom operators, file watching, and flag parsing, which impacts the system's architecture by introducing a new evaluation mode. Second, the number of technical concepts required to understand and implement these changes is substantial, including familiarity with Python's type system, gRPC for remote flag resolution, JSONLogic for in-process evaluation, file I/O for flag source watching, and domain-specific knowledge of feature flag systems. Third, the changes require handling numerous edge cases, such as invalid flag configurations, file parsing errors, and variant resolution failures, which are evident in the detailed error handling logic added to the codebase. While not at the extreme end of difficulty (e.g., requiring distributed systems expertise), the combination of architectural changes, multiple technical concepts, and complex error handling places this problem in the hard category (0.6-0.8 range), specifically at 0.75 due to the depth of domain knowledge and refactoring required.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Code formatting\nWHO\nDevelopers\nWHAT\nHave properly formatted code in alphapeptstats\nWHY\nBest practises, less overhead when developing\n\nAC\n-    Code is formatted once using ruff\n-    pre-commit hook is in place that takes care about the formatting\n -   pre-commit hook runs in CI pipeline\ncf. [#153](https://github.com/MannLabs/alphabase/issues/153)\n", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nnew file mode 100644\nindex 00000000..60d64761\n--- /dev/null\n+++ b/.pre-commit-config.yaml\n@@ -0,0 +1,14 @@\n+# See https://pre-commit.com for more information\n+# See https://pre-commit.com/hooks.html for more hooks\n+repos:\n+- repo: https://github.com/pre-commit/pre-commit-hooks\n+  rev: v2.3.0\n+  hooks:\n+    - id: check-yaml\n+    - id: end-of-file-fixer\n+    - id: trailing-whitespace\n+- repo: https://github.com/astral-sh/ruff-pre-commit\n+  rev: v0.4.0\n+  hooks:\n+    - id: ruff-format\n+#    - id: ruff\ndiff --git a/README.md b/README.md\nindex 96805e27..2d70f05b 100644\n--- a/README.md\n+++ b/README.md\n@@ -116,6 +116,24 @@ AlphaStats was developed by the [Mann Group at the University of Copenhagen](htt\n \n If you like this software, you can give us a [star](https://github.com/MannLabs/alphapeptstats/stargazers) to boost our visibility! All direct contributions are also welcome. Feel free to post a new [issue](https://github.com/MannLabs/alphapeptstats/issues) or clone the repository and create a [pull request](https://github.com/MannLabs/alphapeptstats/pulls) with a new branch. For an even more interactive participation, check out the [discussions](https://github.com/MannLabs/alphapeptstats/discussions) and the [the Contributors License Agreement](misc/CLA.md).\n \n+\n+### Notes for developers\n+#### pre-commit hooks\n+It is highly recommended to use the provided pre-commit hooks, as the CI pipeline enforces all checks therein to\n+pass in order to merge a branch.\n+\n+The hooks need to be installed once by\n+```bash\n+pip install -r requirements_dev.txt\n+pre-commit install\n+```\n+You can run the checks yourself using:\n+```bash\n+pre-commit run --all-files\n+```\n+\n+\n+\n ---\n ## Changelog\n \ndiff --git a/requirements_dev.txt b/requirements_dev.txt\nnew file mode 100644\nindex 00000000..ff5aba65\n--- /dev/null\n+++ b/requirements_dev.txt\n@@ -0,0 +1,2 @@\n+# only needed for local development\n+pre-commit==3.5.0\n", "instance_id": "MannLabs__alphapeptstats-284", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to enforce code formatting in the `alphapeptstats` project using `ruff` and pre-commit hooks. The \"WHO,\" \"WHAT,\" and \"WHY\" sections provide a basic context for the task, and the acceptance criteria (AC) outline the expected deliverables, such as formatting the code with `ruff`, setting up a pre-commit hook, and ensuring it runs in the CI pipeline. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify which parts of the codebase need to be formatted or if there are specific formatting rules or configurations for `ruff`. Additionally, there is no mention of potential challenges, such as handling existing code that might not conform to the formatting rules or conflicts in the CI pipeline setup. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves straightforward and basic modifications to the codebase. The code changes primarily consist of adding a pre-commit configuration file (`.pre-commit-config.yaml`), updating the `README.md` with instructions for developers, and creating a `requirements_dev.txt` file for development dependencies. These changes are isolated, do not require deep understanding of the existing codebase, and involve minimal technical complexity. The concepts involved are basic: familiarity with pre-commit hooks and `ruff` for code formatting, which are well-documented and widely used tools in the Python ecosystem. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code changes. The scope of the changes is limited to configuration and documentation, with no impact on the system's architecture or logic. Overall, this task is very easy and suitable for a beginner or junior developer with basic knowledge of Python development workflows.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "limit_area argument is currently missing for the frame.pyi\nThe new `limit_area` argument is missing for the `ffill` overloads for `frame.pyi`. \r\n\r\n\r\nThis was seen in VSCode (pylance)\r\n```python\r\ndf = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\r\ndf2 = df.sort_values('a')  # This infers the type correctly\r\ndf3 = df.ffill(limit=10)  # This infers the type correctly\r\ndf4 = df.ffill(limit_area='inside')  # This cannot infer the type since the `.pyi` is missing `limit_area`\r\n```\r\nWhen running error checking for types with standard, the error message is:\r\n`No overloads for \"ffill\" match the provided arguments\r\n\u00a0\u00a0Argument types: (Literal['inside'])`\r\n\r\n\r\n\r\n - OS: Windows 10\r\n - python version: 3.12\r\n - version of type checker\r\n - version of installed `pandas-stubs`: This is using the latest version of pylance but I'm not sure how to check the version of pandas-stubs used. However, I don't see the entry in the [current latest commit in master](https://github.com/pandas-dev/pandas-stubs/blob/d2798db3afa01619a2fa18d0e1b82ae63f2a5ccf/pandas-stubs/core/frame.pyi#L1697-L1713)\r\n\r\n\r\n\r\n\n", "patch": "diff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex 80a1e8a6..9ba351f6 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -1656,6 +1656,7 @@ class DataFrame(NDFrame, OpsMixin):\n         axis: Axis | None = ...,\n         inplace: Literal[True],\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> None: ...\n     @overload\n@@ -1665,6 +1666,7 @@ class DataFrame(NDFrame, OpsMixin):\n         axis: Axis | None = ...,\n         inplace: Literal[False] = ...,\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> DataFrame: ...\n     def clip(\n@@ -1736,6 +1738,7 @@ class DataFrame(NDFrame, OpsMixin):\n         axis: Axis | None = ...,\n         inplace: Literal[True],\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> None: ...\n     @overload\n@@ -1745,6 +1748,7 @@ class DataFrame(NDFrame, OpsMixin):\n         axis: Axis | None = ...,\n         inplace: Literal[False] = ...,\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> DataFrame: ...\n     def filter(\ndiff --git a/pandas-stubs/core/series.pyi b/pandas-stubs/core/series.pyi\nindex 080c6a82..ce27d44e 100644\n--- a/pandas-stubs/core/series.pyi\n+++ b/pandas-stubs/core/series.pyi\n@@ -1300,6 +1300,7 @@ class Series(IndexOpsMixin[S1], NDFrame):\n         axis: AxisIndex | None = ...,\n         inplace: Literal[True],\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> None: ...\n     @overload\n@@ -1309,6 +1310,7 @@ class Series(IndexOpsMixin[S1], NDFrame):\n         axis: AxisIndex | None = ...,\n         inplace: Literal[False] = ...,\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> Series[S1]: ...\n     @overload\n@@ -1318,6 +1320,7 @@ class Series(IndexOpsMixin[S1], NDFrame):\n         axis: AxisIndex | None = ...,\n         inplace: Literal[True],\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> None: ...\n     @overload\n@@ -1327,18 +1330,9 @@ class Series(IndexOpsMixin[S1], NDFrame):\n         axis: AxisIndex | None = ...,\n         inplace: Literal[False] = ...,\n         limit: int | None = ...,\n+        limit_area: Literal[\"inside\", \"outside\"] | None = ...,\n         downcast: dict | None = ...,\n     ) -> Series[S1]: ...\n-    @overload\n-    def bfill(\n-        self,\n-        *,\n-        value: S1 | dict | Series[S1] | DataFrame,\n-        axis: AxisIndex = ...,\n-        inplace: _bool = ...,\n-        limit: int | None = ...,\n-        downcast: dict | None = ...,\n-    ) -> Series[S1] | None: ...\n     def interpolate(\n         self,\n         method: InterpolateOptions = ...,\n", "instance_id": "pandas-dev__pandas-stubs-1028", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `limit_area` argument is missing in the type hints for the `ffill` method in `frame.pyi` (and related files) of the `pandas-stubs` repository, leading to type inference issues in tools like Pylance. The provided code snippet and error message effectively illustrate the problem, and the reference to the specific GitHub commit adds context. However, there are minor ambiguities, such as the lack of explicit mention of whether this change aligns with the actual `pandas` library implementation (i.e., does `ffill` in `pandas` support `limit_area` officially, and if so, since which version?). Additionally, edge cases or specific constraints for `limit_area` are not discussed in the problem statement. Overall, the goal is clear, but some minor details are missing that could affect the implementation or validation of the solution.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The task involves adding a single argument (`limit_area`) to the type hints in a few method overloads across two files (`frame.pyi` and `series.pyi`) in the `pandas-stubs` repository. The code changes are minimal and repetitive, requiring only the addition of a parameter with a specific type annotation (`Literal[\"inside\", \"outside\"] | None`). There is no complex logic, algorithm design, or deep understanding of the `pandas` library required beyond basic familiarity with Python type hints and stub files (`.pyi`). The scope is limited to specific method signatures, with no impact on the broader codebase architecture or runtime behavior, as this is purely a type hinting issue. No edge cases or error handling need to be addressed in the code changes, as the task is confined to updating static type definitions. The only minor technical concept involved is understanding Python's type hinting system and the structure of stub files, which is straightforward for anyone with intermediate Python experience. Overall, this is a very easy task that requires minimal effort and expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "df.columns.str.match actually gives npt.NDArray[np.bool_], but mypy thinks it is pd.Index[str]\n# Describe the bug\r\n`df.columns.str.match(reg_ex_pattern)` actually gives `npt.NDArray[np.bool_]`, but mypy thinks it's `pd.Index[str]`\r\n\r\n# To Reproduce\r\n## Provide a minimal runnable `pandas` example that is not properly checked by the stubs.\r\n```py\r\nfrom typing import TYPE_CHECKING, cast\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nif TYPE_CHECKING:\r\n    from numpy import typing as npt\r\n\r\n\r\ndf = pd.DataFrame({\"1\": [2, 3], \"2\": 3, \"4\": 5, \"a\": 1})\r\nmask = df.columns.str.match(r\"\\d\")\r\nprint(mask)  # array([ True,  True,  True, False])\r\nprint(type(mask))  # <class 'numpy.ndarray'>\r\ndf.loc[:, mask]  # mypy: error: Invalid index type \"tuple[slice, Index[str]]\" for \"_LocIndexerFrame\"; expected type \"slice | ndarray[Any, dtype[integer[Any]]] | Index[Any] | list[int] | Series[int] | <6 more items>\"\r\ndf.loc[:, cast(\"npt.NDArray[np.bool_]\", mask)]  # mypy: fine\r\n```\r\n## Indicate which type checker you are using (`mypy` or  `pyright`).\r\nI am using `mypy`.\r\n## Show the error message received from that type checker while checking your example.\r\n```ps\r\nerror: Invalid index type \"tuple[slice, Index[str]]\" for \"_LocIndexerFrame\"; expected type \"slice | ndarray[Any, dtype[integer[Any]]] | Index[Any] | list[int] | Series[int] | <6 more items>\"\r\n```\r\n\r\n# Please complete the following information:\r\n - OS: Windows\r\n - OS Version:\r\n   ```ps\r\n   cmd /c ver  # Microsoft Windows [Version 10.0.19045.4651]\r\n   ```\r\n - python version\r\n   ```ps\r\n   poetry run python --version  # Python 3.12.5\r\n   ```\r\n - version of type checker\r\n   ```ps\r\n   poetry run mypy --version  # mypy 1.11.1 (compiled: yes)\r\n   ```\r\n - version of installed `pandas-stubs`\r\n   ```ps\r\n   poetry run pip freeze | findstr pandas-stubs  # pandas-stubs==2.2.2.240603\r\n   ```\r\n\r\n# Additional context\r\nNothing\r\n\n", "patch": "diff --git a/pandas-stubs/core/indexes/base.pyi b/pandas-stubs/core/indexes/base.pyi\nindex b8dc9444..297e9405 100644\n--- a/pandas-stubs/core/indexes/base.pyi\n+++ b/pandas-stubs/core/indexes/base.pyi\n@@ -260,7 +260,7 @@ class Index(IndexOpsMixin[S1]):\n         **kwargs,\n     ) -> Self: ...\n     @property\n-    def str(self) -> StringMethods[Self, MultiIndex]: ...\n+    def str(self) -> StringMethods[Self, MultiIndex, np_ndarray_bool]: ...\n     def is_(self, other) -> bool: ...\n     def __len__(self) -> int: ...\n     def __array__(self, dtype=...) -> np.ndarray: ...\ndiff --git a/pandas-stubs/core/series.pyi b/pandas-stubs/core/series.pyi\nindex 20923774..e12fa2cc 100644\n--- a/pandas-stubs/core/series.pyi\n+++ b/pandas-stubs/core/series.pyi\n@@ -1163,7 +1163,7 @@ class Series(IndexOpsMixin[S1], NDFrame):\n     ) -> Series[S1]: ...\n     def to_period(self, freq: _str | None = ..., copy: _bool = ...) -> DataFrame: ...\n     @property\n-    def str(self) -> StringMethods[Series, DataFrame]: ...\n+    def str(self) -> StringMethods[Series, DataFrame, Series[bool]]: ...\n     @property\n     def dt(self) -> CombinedDatetimelikeProperties: ...\n     @property\ndiff --git a/pandas-stubs/core/strings.pyi b/pandas-stubs/core/strings.pyi\nindex a21074da..a3596aa5 100644\n--- a/pandas-stubs/core/strings.pyi\n+++ b/pandas-stubs/core/strings.pyi\n@@ -23,12 +23,15 @@ from pandas.core.base import NoNewAttributesMixin\n from pandas._typing import (\n     JoinHow,\n     T,\n+    np_ndarray_bool,\n )\n \n # The _TS type is what is used for the result of str.split with expand=True\n _TS = TypeVar(\"_TS\", DataFrame, MultiIndex)\n+# The _TM type is what is used for the result of str.match\n+_TM = TypeVar(\"_TM\", Series[bool], np_ndarray_bool)\n \n-class StringMethods(NoNewAttributesMixin, Generic[T, _TS]):\n+class StringMethods(NoNewAttributesMixin, Generic[T, _TS, _TM]):\n     def __init__(self, data: T) -> None: ...\n     def __getitem__(self, key: slice | int) -> T: ...\n     def __iter__(self) -> T: ...\n@@ -100,7 +103,7 @@ class StringMethods(NoNewAttributesMixin, Generic[T, _TS]):\n     ) -> Series[bool]: ...\n     def match(\n         self, pat: str, case: bool = ..., flags: int = ..., na: Any = ...\n-    ) -> T: ...\n+    ) -> _TM: ...\n     def replace(\n         self,\n         pat: str,\n", "instance_id": "pandas-dev__pandas-stubs-990", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue: a type mismatch in the return type of `df.columns.str.match` as identified by the `mypy` type checker. The goal is explicit\u2014correct the type annotation to reflect the actual return type `npt.NDArray[np.bool_]` instead of `pd.Index[str]`. The statement includes a minimal reproducible example, the exact error message from `mypy`, and detailed environment information (OS, Python version, type checker version, and library versions). There are no significant ambiguities, and the provided context, including code output, helps in understanding the problem fully. All necessary details for input, output, and constraints are indirectly covered through the example and error message. Therefore, this problem statement merits a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes are confined to type annotations in stub files (`pandas-stubs`) across three files (`base.pyi`, `series.pyi`, `strings.pyi`). The modifications are minimal, involving updates to generic type parameters and return type annotations for the `str.match` method. There is no impact on the system's architecture or runtime behavior, as these are purely type hint changes for static type checking. The amount of code change is small, with only a few lines altered.\n\n2. **Number of Technical Concepts**: Solving this requires understanding of Python type annotations, specifically how generic types and type variables work in stub files for libraries like `pandas`. Familiarity with `mypy` and the `pandas-stubs` project structure is necessary, along with knowledge of NumPy and pandas data structures (`NDArray`, `Index`, `Series`). However, these concepts are not overly complex for someone with moderate experience in Python type systems or library maintenance.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not involve runtime logic or error handling modifications. The focus is solely on correcting type annotations to match the actual runtime behavior, so no additional edge case handling is required in the solution.\n\n4. **Overall Complexity**: The task does not require deep understanding of the broader `pandas` codebase or complex interactions between modules. It is a straightforward fix once the type mismatch is understood, involving no algorithmic complexity or performance considerations.\n\nGiven these points, the problem is rated at 0.25, as it requires understanding some specific type annotation logic and making simple modifications to stub files, but it does not involve significant complexity or broad impact on the codebase.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "In `DataFrame.select_dtypes` type values like `np.number` or `np.datetime64` should be allowed\n**Describe the bug**\r\nThe documentation of `DataFrame.select_dtypes` clearly states that it is allowed to use values like `np.number` or `np.datetime64` for the `include` and `exclude` parameters, but the current stubs don't allow it, they enforce `include: _str | list[_str] | None` and `exclude: _str | list[_str] | None`. This leads to typecheckers like `mypy` reporting false positives on such calls.\r\n\r\n**To Reproduce**\r\nIf I run mypy on the following short snippet I get an error:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame()\r\ndf.select_dtypes(np.number)\r\n```\r\n\r\nthe error is:\r\n\r\n```\r\nmin_example.py:5: error: Argument 1 to \"select_dtypes\" of \"DataFrame\" has incompatible type \"type[number[Any]]\"; expected \"str | list[str] | None\"  [arg-type]\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: Linux\r\n - OS Version: Arch Linux\r\n - python version: 3.11\r\n - version of type checker: mypy 1.8.0\r\n - version of installed `pandas-stubs`: 2.2.0.240218\nIn `DataFrame.select_dtypes` type values like `np.number` or `np.datetime64` should be allowed\n**Describe the bug**\r\nThe documentation of `DataFrame.select_dtypes` clearly states that it is allowed to use values like `np.number` or `np.datetime64` for the `include` and `exclude` parameters, but the current stubs don't allow it, they enforce `include: _str | list[_str] | None` and `exclude: _str | list[_str] | None`. This leads to typecheckers like `mypy` reporting false positives on such calls.\r\n\r\n**To Reproduce**\r\nIf I run mypy on the following short snippet I get an error:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame()\r\ndf.select_dtypes(np.number)\r\n```\r\n\r\nthe error is:\r\n\r\n```\r\nmin_example.py:5: error: Argument 1 to \"select_dtypes\" of \"DataFrame\" has incompatible type \"type[number[Any]]\"; expected \"str | list[str] | None\"  [arg-type]\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n**Please complete the following information:**\r\n - OS: Linux\r\n - OS Version: Arch Linux\r\n - python version: 3.11\r\n - version of type checker: mypy 1.8.0\r\n - version of installed `pandas-stubs`: 2.2.0.240218\n", "patch": "diff --git a/pandas-stubs/core/frame.pyi b/pandas-stubs/core/frame.pyi\nindex 44b3a080..066bedeb 100644\n--- a/pandas-stubs/core/frame.pyi\n+++ b/pandas-stubs/core/frame.pyi\n@@ -50,7 +50,11 @@ from pandas.core.window.rolling import (\n     Rolling,\n     Window,\n )\n-from typing_extensions import Self\n+from typing_extensions import (\n+    Never,\n+    Self,\n+    TypeAlias,\n+)\n import xarray as xr\n \n from pandas._libs.lib import NoDefault\n@@ -112,6 +116,7 @@ from pandas._typing import (\n     SortKind,\n     StataDateFormat,\n     StorageOptions,\n+    StrDtypeArg,\n     StrLike,\n     Suffixes,\n     T as _T,\n@@ -608,10 +613,47 @@ class DataFrame(NDFrame, OpsMixin):\n         self, expr: _str, *, inplace: Literal[False] = ..., **kwargs\n     ) -> DataFrame: ...\n     def eval(self, expr: _str, *, inplace: _bool = ..., **kwargs): ...\n+    AstypeArgExt: TypeAlias = (\n+        AstypeArg\n+        | Literal[\n+            \"number\",\n+            \"datetime64\",\n+            \"datetime\",\n+            \"timedelta\",\n+            \"timedelta64\",\n+            \"datetimetz\",\n+            \"datetime64[ns]\",\n+        ]\n+    )\n+    AstypeArgExtList: TypeAlias = AstypeArgExt | list[AstypeArgExt]\n+    @overload\n+    def select_dtypes(\n+        self, include: StrDtypeArg, exclude: AstypeArgExtList | None = ...\n+    ) -> Never: ...\n+    @overload\n+    def select_dtypes(\n+        self, include: AstypeArgExtList | None, exclude: StrDtypeArg\n+    ) -> Never: ...\n+    @overload\n+    def select_dtypes(self, exclude: StrDtypeArg) -> Never: ...\n+    @overload\n+    def select_dtypes(self, include: list[Never], exclude: list[Never]) -> Never: ...\n+    @overload\n+    def select_dtypes(\n+        self,\n+        include: AstypeArgExtList,\n+        exclude: AstypeArgExtList | None = ...,\n+    ) -> DataFrame: ...\n+    @overload\n+    def select_dtypes(\n+        self,\n+        include: AstypeArgExtList | None,\n+        exclude: AstypeArgExtList,\n+    ) -> DataFrame: ...\n+    @overload\n     def select_dtypes(\n         self,\n-        include: _str | list[_str] | None = ...,\n-        exclude: _str | list[_str] | None = ...,\n+        exclude: AstypeArgExtList,\n     ) -> DataFrame: ...\n     def insert(\n         self,\n", "instance_id": "pandas-dev__pandas-stubs-900", "clarity": 3, "difficulty": 0.35, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly describes the bug in the type hints for `DataFrame.select_dtypes` in the `pandas-stubs` library, where the current type annotations do not allow for values like `np.number` or `np.datetime64` as per the official documentation. The issue is supported by a reproducible code snippet and the exact error message from `mypy`, which helps in understanding the problem context. Additionally, the statement includes relevant environment details (OS, Python version, type checker version, and library version), ensuring there are no ambiguities about the setup. The goal is explicit: update the type hints to match the documented behavior. There are no missing critical details, and the provided example leaves little room for misinterpretation.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The modification is localized to a single file (`pandas-stubs/core/frame.pyi`) and specifically to the type annotations of the `select_dtypes` method. The diff shows a moderate amount of code change, primarily involving the addition of new type aliases (`AstypeArgExt` and `AstypeArgExtList`) and multiple overloads for the method signature. It does not impact the broader system architecture or require changes across multiple modules, as it is purely a type hint update in a stub file with no runtime implications.\n\n2. **Number of Technical Concepts:** Solving this requires a good understanding of Python type annotations, specifically the use of `TypeAlias`, `overload` decorators from `typing_extensions`, and familiarity with pandas' type system (e.g., `AstypeArg`, `StrDtypeArg`). While these concepts are not trivial, they are well-documented and commonly used in type stub maintenance for large libraries like pandas. No advanced algorithms, design patterns, or domain-specific knowledge beyond type hinting are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and since this is a type hint update, there are no runtime error handling requirements. The focus is on ensuring the type checker accepts valid inputs as per the documentation. The complexity of edge cases is minimal, as the change only involves expanding the allowed types for `include` and `exclude` parameters.\n\n4. **Overall Complexity:** The task requires careful crafting of type overloads to avoid conflicts and ensure compatibility with existing type hints, which adds a slight layer of complexity. However, it remains a straightforward fix for someone familiar with Python type systems and stub files. It does not require deep architectural changes or extensive debugging.\n\nGiven these factors, a score of 0.35 reflects an \"Easy\" problem that involves understanding specific type hinting logic and making targeted modifications to a single file, with no significant impact on the broader codebase or complex error handling.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "MDA daily binning misaligned due to mislabeled data in 8hour mean calculation\n**Describe the bug**\r\nPlease provide a clear and concise description of what the bug is.\r\n - Pyaerocom version: 0.22.dev0\r\n - Computing platform: NA\r\n - Configuration file (if applicable): NA\r\n - Error message (if applicable): NA\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Add the following parametrization to `test_mda8.py:test_calc_mda8`\r\n```\r\n        pytest.param(\r\n            xr.date_range(start=\"2024-01-01 06:00:00\", periods=30, freq=\"1h\"),\r\n            np.arange(30),\r\n            [np.nan, 25.5]\r\n        ),\r\n```\r\n2. The test fails because the last 8 hour mean included in the first day is for the period 0000-0800 the next day, instead of 1600-2400. Therefore not enough data is included in day two and it results in nan.\r\n\r\n**Expected behavior**\r\nMDA8 for 2024-01-02 should be a non-nan numeric value (25.5).\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nCaused by wrong alignment. xarray's DataArray.rolling labels data to the left, but labeling it to the right is expected by the current implementation. DataArray.rolling does not support a label-argument so another fix needs to be found. \r\n\r\nRelated: https://github.com/metno/pyaerocom/pull/1322\n", "patch": "diff --git a/pyaerocom/stats/mda8/mda8.py b/pyaerocom/stats/mda8/mda8.py\nindex 54b031c34..99ce4bbe4 100644\n--- a/pyaerocom/stats/mda8/mda8.py\n+++ b/pyaerocom/stats/mda8/mda8.py\n@@ -100,10 +100,11 @@ def _calc_mda8(data: xr.DataArray) -> xr.DataArray:\n \n \n def _rolling_average_8hr(arr: xr.DataArray) -> xr.DataArray:\n-    return arr.rolling(time=8, min_periods=6).mean()\n+    # Xarray labels the data right based on the last data point in the period for rolling.\n+    return arr.rolling(time=8, center=False, min_periods=6).mean()\n \n \n def _daily_max(arr: xr.DataArray) -> xr.DataArray:\n-    return arr.resample(time=\"24H\", offset=\"1h\").reduce(\n+    return arr.resample(time=\"24H\", origin=\"start_day\", label=\"left\", offset=\"1h\").reduce(\n         lambda x, axis: np.apply_along_axis(min_periods_max, 1, x, min_periods=18)\n     )\n", "instance_id": "metno__pyaerocom-1322", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to the misalignment of data in the 8-hour mean calculation for MDA8 (Maximum Daily 8-hour Average) in the `pyaerocom` library. It provides specific steps to reproduce the issue, including a test case with expected behavior, and identifies the root cause as an alignment issue with `xarray.DataArray.rolling`. However, there are minor ambiguities and missing details. For instance, the problem statement does not fully explain the expected alignment logic or provide a detailed description of how the data should be grouped for daily calculations beyond the test case. Additionally, while it mentions the need for a fix due to the lack of a `label` argument in `DataArray.rolling`, it does not suggest a clear direction for the solution beyond this observation. Edge cases or broader implications of the misalignment are also not explicitly discussed, which could leave room for interpretation. Overall, the statement is valid and clear enough to understand the issue, but it lacks some depth in explaining the desired behavior comprehensively.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively small, confined to a single file (`mda8.py`) and involving modifications to two functions (`_rolling_average_8hr` and `_daily_max`). The changes primarily adjust parameters in the `rolling` and `resample` methods of `xarray.DataArray` to correct data alignment, which does not impact the broader system architecture. However, solving this requires a solid understanding of the `xarray` library, particularly its time series handling and rolling window operations, as well as familiarity with temporal data alignment concepts. The problem also involves some domain-specific knowledge related to environmental data processing (MDA8 calculations), though this is not overly complex for someone familiar with the field or codebase. Edge cases, such as incomplete data periods or boundary conditions at the start/end of time series, are implicitly relevant but not explicitly handled in the provided changes, adding a moderate layer of complexity. Overall, this task requires understanding multiple concepts (time series manipulation, library-specific behavior) and making targeted but non-trivial modifications, justifying a score of 0.45, which sits at the lower end of the medium difficulty range.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Feature Request]: Support for multiple discrete treatments in `doubleml.DoubleMLIRM` \n### Describe the feature you want to propose or implement\n\nWould be nice to extend the `DoubleMLIRM` class to accommodate multiple discrete-valued treatments $w \\in \\mathcal{W}$, where the estimation primatives are the marginal counterfactual mean $Y^{(w)}$ under each treatment, and one can construct treatment effects as contrasts between them. The marginal mean construction is the same as in the binary treatment case\r\n\r\n$$\r\n\\phi_i^{(w)} = \\mu^{(w)}(x) + \\frac{1(W_i = w)}{\\pi^{w}(x)} \\left(Y_i - \\mu^{(w)}(x)\\right)\r\n$$\r\n\r\nwhich can be constructed at the unit level, and the subsequent analysis can be performed as usual (the binary influence function would be a special case where one constructs $\\psi_i = \\phi_i^{(1)} - \\phi_i^{(0)} - \\theta$ and takes the mean and variance for point estimate and standard error. \r\n\r\n---\r\nNuances:\r\n - ATT isn't a well defined concept for multiple-treatments (or at the very least one has to stipulate the subpopulation that is allowed to contribute to the estimator of $E[Y^({0}) \\mid W = 1]$, typically the 'pure control' (0-valued by convention) subpopulation might be reasonable)\r\n\n\n### Propose a possible solution or implementation\n\n\r\nThis approach is implemented in [Ed Kennedy's package](https://github.com/ehkennedy/npcausal/blob/master/R/ate.R), and I have a minimal implementation in [this](https://github.com/apoorvalal/aipyw) library. Wondering if there's interest in accommodating this use case (since this will involve changing `IIRM` from [`check_data`](https://github.com/DoubleML/doubleml-for-py/blob/main/doubleml/irm/irm.py#L246) onwards.\r\n\n\n### Did you consider alternatives to the proposed solution. If yes, please describe\n\n_No response_\n\n### Comments, context or references\n\n_No response_\n", "patch": "diff --git a/doubleml/did/did.py b/doubleml/did/did.py\nindex 733b76c9..c62c8e32 100644\n--- a/doubleml/did/did.py\n+++ b/doubleml/did/did.py\n@@ -343,7 +343,9 @@ def _sensitivity_element_est(self, preds):\n         element_dict = {'sigma2': sigma2,\n                         'nu2': nu2,\n                         'psi_sigma2': psi_sigma2,\n-                        'psi_nu2': psi_nu2}\n+                        'psi_nu2': psi_nu2,\n+                        'riesz_rep': rr,\n+                        }\n         return element_dict\n \n     def _nuisance_tuning(self, smpls, param_grids, scoring_methods, n_folds_tune, n_jobs_cv,\ndiff --git a/doubleml/did/did_cs.py b/doubleml/did/did_cs.py\nindex 766f0ea3..b1cd3d4f 100644\n--- a/doubleml/did/did_cs.py\n+++ b/doubleml/did/did_cs.py\n@@ -476,7 +476,9 @@ def _sensitivity_element_est(self, preds):\n         element_dict = {'sigma2': sigma2,\n                         'nu2': nu2,\n                         'psi_sigma2': psi_sigma2,\n-                        'psi_nu2': psi_nu2}\n+                        'psi_nu2': psi_nu2,\n+                        'riesz_rep': rr,\n+                        }\n         return element_dict\n \n     def _nuisance_tuning(self, smpls, param_grids, scoring_methods, n_folds_tune, n_jobs_cv,\ndiff --git a/doubleml/double_ml.py b/doubleml/double_ml.py\nindex 810686f2..c8bb7604 100644\n--- a/doubleml/double_ml.py\n+++ b/doubleml/double_ml.py\n@@ -8,16 +8,13 @@\n from scipy.stats import norm\n \n from abc import ABC, abstractmethod\n-from scipy.optimize import minimize_scalar\n \n from .double_ml_data import DoubleMLBaseData, DoubleMLClusterData\n from .double_ml_framework import DoubleMLFramework\n \n from .utils.resampling import DoubleMLResampling, DoubleMLClusterResampling\n from .utils._estimation import _rmse, _aggregate_coefs_and_ses, _var_est, _set_external_predictions\n-from .utils._checks import _check_in_zero_one, _check_integer, _check_float, _check_bool, \\\n-    _check_benchmarks, _check_external_predictions, _check_sample_splitting\n-from .utils._plots import _sensitivity_contour_plot\n+from .utils._checks import _check_external_predictions, _check_sample_splitting\n from .utils.gain_statistics import gain_statistics\n \n _implemented_data_backends = ['DoubleMLData', 'DoubleMLClusterData']\n@@ -335,8 +332,8 @@ def psi_elements(self):\n     def sensitivity_elements(self):\n         \"\"\"\n         Values of the sensitivity components after calling :meth:`fit`;\n-        If available (e.g., PLR, IRM) a dictionary with entries ``sigma2``, ``nu2``, ``psi_sigma2``\n-        and ``psi_nu2``.\n+        If available (e.g., PLR, IRM) a dictionary with entries ``sigma2``, ``nu2``, ``psi_sigma2``, ``psi_nu2``\n+        and ``riesz_rep``.\n         \"\"\"\n         return self._sensitivity_elements\n \n@@ -347,7 +344,11 @@ def sensitivity_params(self):\n         If available (e.g., PLR, IRM) a dictionary with entries ``theta``, ``se``, ``ci``, ``rv``\n         and ``rva``.\n         \"\"\"\n-        return self._sensitivity_params\n+        if self._framework is None:\n+            sensitivity_params = None\n+        else:\n+            sensitivity_params = self._framework.sensitivity_params\n+        return sensitivity_params\n \n     @property\n     def coef(self):\n@@ -534,15 +535,38 @@ def construct_framework(self):\n         scaled_psi_reshape = np.transpose(scaled_psi, (0, 2, 1))\n \n         doubleml_dict = {\n-            \"thetas\": self.coef,\n-            \"all_thetas\": self.all_coef,\n-            \"ses\": self.se,\n-            \"all_ses\": self.all_se,\n-            \"var_scaling_factors\": self._var_scaling_factors,\n-            \"scaled_psi\": scaled_psi_reshape,\n-            \"is_cluster_data\": self._is_cluster_data\n+            'thetas': self.coef,\n+            'all_thetas': self.all_coef,\n+            'ses': self.se,\n+            'all_ses': self.all_se,\n+            'var_scaling_factors': self._var_scaling_factors,\n+            'scaled_psi': scaled_psi_reshape,\n+            'is_cluster_data': self._is_cluster_data\n         }\n \n+        if self._sensitivity_implemented:\n+            # reshape sensitivity elements to (n_obs, n_coefs, n_rep)\n+            doubleml_dict.update({\n+                'sensitivity_elements': {\n+                    'sigma2': np.transpose(self.sensitivity_elements['sigma2'], (0, 2, 1)),\n+                    'nu2': np.transpose(self.sensitivity_elements['nu2'], (0, 2, 1)),\n+                    'psi_sigma2': np.transpose(self.sensitivity_elements['psi_sigma2'], (0, 2, 1)),\n+                    'psi_nu2': np.transpose(self.sensitivity_elements['psi_nu2'], (0, 2, 1)),\n+                    'riesz_rep': np.transpose(self.sensitivity_elements['riesz_rep'], (0, 2, 1))\n+                }\n+            })\n+\n+        if self._is_cluster_data:\n+            doubleml_dict.update({\n+                'is_cluster_data': True,\n+                'cluster_dict': {\n+                    'smpls': self._smpls,\n+                    'smpls_cluster': self._smpls_cluster,\n+                    'cluster_vars': self._dml_data.cluster_vars,\n+                    'n_folds_per_cluster': self._n_folds_per_cluster,\n+                }\n+            })\n+\n         doubleml_framework = DoubleMLFramework(doubleml_dict)\n         return doubleml_framework\n \n@@ -1324,14 +1348,15 @@ def _sensitivity_element_est(self, preds):\n \n     @property\n     def _sensitivity_element_names(self):\n-        return ['sigma2', 'nu2', 'psi_sigma2', 'psi_nu2']\n+        return ['sigma2', 'nu2', 'psi_sigma2', 'psi_nu2', 'riesz_rep']\n \n     # the dimensions will usually be (n_obs, n_rep, n_coefs) to be equal to the score dimensions psi\n     def _initialize_sensitivity_elements(self, score_dim):\n         sensitivity_elements = {'sigma2': np.full((1, score_dim[1], score_dim[2]), np.nan),\n                                 'nu2': np.full((1, score_dim[1], score_dim[2]), np.nan),\n                                 'psi_sigma2': np.full(score_dim, np.nan),\n-                                'psi_nu2': np.full(score_dim, np.nan)}\n+                                'psi_nu2': np.full(score_dim, np.nan),\n+                                'riesz_rep': np.full(score_dim, np.nan)}\n         return sensitivity_elements\n \n     def _get_sensitivity_elements(self, i_rep, i_treat):\n@@ -1350,122 +1375,6 @@ def _set_sensitivity_elements(self, sensitivity_elements, i_rep, i_treat):\n             self.sensitivity_elements[key][:, i_rep, i_treat] = sensitivity_elements[key]\n         return\n \n-    def _calc_sensitivity_analysis(self, cf_y, cf_d, rho, level):\n-        if self._sensitivity_elements is None:\n-            raise NotImplementedError(f'Sensitivity analysis not yet implemented for {self.__class__.__name__}.')\n-\n-        # checks\n-        _check_in_zero_one(cf_y, 'cf_y', include_one=False)\n-        _check_in_zero_one(cf_d, 'cf_d', include_one=False)\n-        if not isinstance(rho, float):\n-            raise TypeError(f'rho must be of float type. '\n-                            f'{str(rho)} of type {str(type(rho))} was passed.')\n-        _check_in_zero_one(abs(rho), 'The absolute value of rho')\n-        _check_in_zero_one(level, 'The confidence level', include_zero=False, include_one=False)\n-\n-        # set elements for readability\n-        sigma2 = self.sensitivity_elements['sigma2']\n-        nu2 = self.sensitivity_elements['nu2']\n-        psi_sigma = self.sensitivity_elements['psi_sigma2']\n-        psi_nu = self.sensitivity_elements['psi_nu2']\n-        psi_scaled = np.divide(self.psi, np.mean(self.psi_deriv, axis=0))\n-\n-        if (np.any(sigma2 < 0)) | (np.any(nu2 < 0)):\n-            raise ValueError('sensitivity_elements sigma2 and nu2 have to be positive. '\n-                             f\"Got sigma2 {str(sigma2)} and nu2 {str(nu2)}. \"\n-                             'Most likely this is due to low quality learners (especially propensity scores).')\n-\n-        # elementwise operations\n-        confounding_strength = np.multiply(np.abs(rho), np.sqrt(np.multiply(cf_y, np.divide(cf_d, 1.0-cf_d))))\n-        S = np.sqrt(np.multiply(sigma2, nu2))\n-\n-        # sigma2 and nu2 are of shape (1, n_rep, n_coefs), whereas the all_coefs is of shape (n_coefs, n_reps)\n-        all_theta_lower = self.all_coef - np.multiply(np.transpose(np.squeeze(S, axis=0)), confounding_strength)\n-        all_theta_upper = self.all_coef + np.multiply(np.transpose(np.squeeze(S, axis=0)), confounding_strength)\n-\n-        psi_S2 = np.multiply(sigma2, psi_nu) + np.multiply(nu2, psi_sigma)\n-        psi_bias = np.multiply(np.divide(confounding_strength, np.multiply(2.0, S)), psi_S2)\n-        psi_lower = psi_scaled - psi_bias\n-        psi_upper = psi_scaled + psi_bias\n-\n-        # transpose to obtain shape (n_coefs, n_reps); includes scaling with n^{-1/2}\n-        all_sigma_lower = np.full_like(all_theta_lower, fill_value=np.nan)\n-        all_sigma_upper = np.full_like(all_theta_upper, fill_value=np.nan)\n-        for i_rep in range(self.n_rep):\n-            self._i_rep = i_rep\n-            for i_d in range(self._dml_data.n_treat):\n-                self._i_treat = i_d\n-\n-                if not self._is_cluster_data:\n-                    cluster_vars = None\n-                    smpls_cluster = None\n-                    n_folds_per_cluster = None\n-                else:\n-                    cluster_vars = self._dml_data.cluster_vars\n-                    smpls_cluster = self.__smpls_cluster\n-                    n_folds_per_cluster = self._n_folds_per_cluster\n-\n-                sigma2_lower_hat, _ = _var_est(psi=psi_lower[:, i_rep, i_d],\n-                                               psi_deriv=np.ones_like(psi_lower[:, i_rep, i_d]),\n-                                               smpls=self.__smpls,\n-                                               is_cluster_data=self._is_cluster_data,\n-                                               cluster_vars=cluster_vars,\n-                                               smpls_cluster=smpls_cluster,\n-                                               n_folds_per_cluster=n_folds_per_cluster)\n-                sigma2_upper_hat, _ = _var_est(psi=psi_upper[:, i_rep, i_d],\n-                                               psi_deriv=np.ones_like(psi_upper[:, i_rep, i_d]),\n-                                               smpls=self.__smpls,\n-                                               is_cluster_data=self._is_cluster_data,\n-                                               cluster_vars=cluster_vars,\n-                                               smpls_cluster=smpls_cluster,\n-                                               n_folds_per_cluster=n_folds_per_cluster)\n-\n-                all_sigma_lower[self._i_treat, self._i_rep] = np.sqrt(sigma2_lower_hat)\n-                all_sigma_upper[self._i_treat, self._i_rep] = np.sqrt(sigma2_upper_hat)\n-\n-        # aggregate coefs and ses over n_rep\n-        theta_lower, sigma_lower = _aggregate_coefs_and_ses(all_theta_lower, all_sigma_lower, self._var_scaling_factors)\n-        theta_upper, sigma_upper = _aggregate_coefs_and_ses(all_theta_upper, all_sigma_upper, self._var_scaling_factors)\n-\n-        quant = norm.ppf(level)\n-        ci_lower = theta_lower - np.multiply(quant, sigma_lower)\n-        ci_upper = theta_upper + np.multiply(quant, sigma_upper)\n-\n-        theta_dict = {'lower': theta_lower,\n-                      'upper': theta_upper}\n-\n-        se_dict = {'lower': sigma_lower,\n-                   'upper': sigma_upper}\n-\n-        ci_dict = {'lower': ci_lower,\n-                   'upper': ci_upper}\n-\n-        res_dict = {'theta': theta_dict,\n-                    'se': se_dict,\n-                    'ci': ci_dict}\n-\n-        return res_dict\n-\n-    def _calc_robustness_value(self, null_hypothesis, level, rho, idx_treatment):\n-        _check_float(null_hypothesis, \"null_hypothesis\")\n-        _check_integer(idx_treatment, \"idx_treatment\", lower_bound=0, upper_bound=self._dml_data.n_treat-1)\n-\n-        # check which side is relvant\n-        bound = 'upper' if (null_hypothesis > self.coef[idx_treatment]) else 'lower'\n-\n-        # minimize the square to find boundary solutions\n-        def rv_fct(value, param):\n-            res = self._calc_sensitivity_analysis(cf_y=value,\n-                                                  cf_d=value,\n-                                                  rho=rho,\n-                                                  level=level)[param][bound][idx_treatment] - null_hypothesis\n-            return np.square(res)\n-\n-        rv = minimize_scalar(rv_fct, bounds=(0, 0.9999), method='bounded', args=('theta', )).x\n-        rva = minimize_scalar(rv_fct, bounds=(0, 0.9999), method='bounded', args=('ci', )).x\n-\n-        return rv, rva\n-\n     def sensitivity_analysis(self, cf_y=0.03, cf_d=0.03, rho=1.0, level=0.95, null_hypothesis=0.0):\n         \"\"\"\n         Performs a sensitivity analysis to account for unobserved confounders.\n@@ -1502,42 +1411,17 @@ def sensitivity_analysis(self, cf_y=0.03, cf_d=0.03, rho=1.0, level=0.95, null_h\n         -------\n         self : object\n         \"\"\"\n-        # compute sensitivity analysis\n-        sensitivity_dict = self._calc_sensitivity_analysis(cf_y=cf_y, cf_d=cf_d, rho=rho, level=level)\n-\n-        if isinstance(null_hypothesis, float):\n-            null_hypothesis_vec = np.full(shape=self._dml_data.n_treat, fill_value=null_hypothesis)\n-        elif isinstance(null_hypothesis, np.ndarray):\n-            if null_hypothesis.shape == (self._dml_data.n_treat,):\n-                null_hypothesis_vec = null_hypothesis\n-            else:\n-                raise ValueError(\"null_hypothesis is numpy.ndarray but does not have the required \"\n-                                 f\"shape ({self._dml_data.n_treat},). \"\n-                                 f'Array of shape {str(null_hypothesis.shape)} was passed.')\n-        else:\n-            raise TypeError(\"null_hypothesis has to be of type float or np.ndarry. \"\n-                            f\"{str(null_hypothesis)} of type {str(type(null_hypothesis))} was passed.\")\n \n-        # compute robustess values with respect to null_hypothesis\n-        rv = np.full(shape=self._dml_data.n_treat, fill_value=np.nan)\n-        rva = np.full(shape=self._dml_data.n_treat, fill_value=np.nan)\n-\n-        for i_treat in range(self._dml_data.n_treat):\n-            rv[i_treat], rva[i_treat] = self._calc_robustness_value(null_hypothesis=null_hypothesis_vec[i_treat],\n-                                                                    level=level, rho=rho, idx_treatment=i_treat)\n-\n-        sensitivity_dict['rv'] = rv\n-        sensitivity_dict['rva'] = rva\n-\n-        # add all input parameters\n-        input_params = {'cf_y': cf_y,\n-                        'cf_d': cf_d,\n-                        'rho': rho,\n-                        'level': level,\n-                        'null_hypothesis': null_hypothesis_vec}\n-        sensitivity_dict['input'] = input_params\n+        if self._framework is None:\n+            raise ValueError('Apply fit() before sensitivity_analysis().')\n+        self._framework.sensitivity_analysis(\n+            cf_y=cf_y,\n+            cf_d=cf_d,\n+            rho=rho,\n+            level=level,\n+            null_hypothesis=null_hypothesis\n+        )\n \n-        self._sensitivity_params = sensitivity_dict\n         return self\n \n     @property\n@@ -1560,19 +1444,19 @@ def sensitivity_summary(self):\n                               f'rho={self.sensitivity_params[\"input\"][\"rho\"]}'\n \n             theta_and_ci_col_names = ['CI lower', 'theta lower', ' theta', 'theta upper', 'CI upper']\n-            theta_and_ci = np.transpose(np.vstack((self._sensitivity_params['ci']['lower'],\n-                                                   self._sensitivity_params['theta']['lower'],\n+            theta_and_ci = np.transpose(np.vstack((self.sensitivity_params['ci']['lower'],\n+                                                   self.sensitivity_params['theta']['lower'],\n                                                    self.coef,\n-                                                   self._sensitivity_params['theta']['upper'],\n-                                                   self._sensitivity_params['ci']['upper'])))\n+                                                   self.sensitivity_params['theta']['upper'],\n+                                                   self.sensitivity_params['ci']['upper'])))\n             df_theta_and_ci = pd.DataFrame(theta_and_ci,\n                                            columns=theta_and_ci_col_names,\n                                            index=self._dml_data.d_cols)\n             theta_and_ci_summary = str(df_theta_and_ci)\n \n             rvs_col_names = ['H_0', 'RV (%)', 'RVa (%)']\n-            rvs = np.transpose(np.vstack((self._sensitivity_params['rv'],\n-                                          self._sensitivity_params['rva']))) * 100\n+            rvs = np.transpose(np.vstack((self.sensitivity_params['rv'],\n+                                          self.sensitivity_params['rva']))) * 100\n \n             df_rvs = pd.DataFrame(np.column_stack((self.sensitivity_params[\"input\"][\"null_hypothesis\"], rvs)),\n                                   columns=rvs_col_names,\n@@ -1589,8 +1473,8 @@ def sensitivity_summary(self):\n \n         return res\n \n-    def sensitivity_plot(self, idx_treatment=0, value='theta', include_scenario=True, benchmarks=None,\n-                         fill=True, grid_bounds=(0.15, 0.15), grid_size=100):\n+    def sensitivity_plot(self, idx_treatment=0, value='theta', rho=1.0, level=0.95, null_hypothesis=0.0,\n+                         include_scenario=True, benchmarks=None, fill=True, grid_bounds=(0.15, 0.15), grid_size=100):\n         \"\"\"\n         Contour plot of the sensivity with respect to latent/confounding variables.\n \n@@ -1605,6 +1489,19 @@ def sensitivity_plot(self, idx_treatment=0, value='theta', include_scenario=True\n             and ``'ci'`` (refers to the bounds including statistical uncertainty).\n             Default is ``'theta'``.\n \n+        rho: float\n+            The correlation between the differences in short and long representations in the main regression and\n+            Riesz representer. Has to be in [-1,1]. The absolute value determines the adversarial strength of the\n+            confounding (maximizes at 1.0).\n+            Default is ``1.0``.\n+\n+        level : float\n+            The confidence level.\n+            Default is ``0.95``.\n+\n+        null_hypothesis : float\n+            Null hypothesis for the effect. Determines the direction of the contour lines.\n+\n         include_scenario : bool\n             Indicates whether to highlight the scenario from the call of :meth:`sensitivity_analysis`.\n             Default is ``True``.\n@@ -1630,76 +1527,21 @@ def sensitivity_plot(self, idx_treatment=0, value='theta', include_scenario=True\n         fig : object\n             Plotly figure of the sensitivity contours.\n         \"\"\"\n-        if self.sensitivity_params is None:\n-            raise ValueError('Apply sensitivity_analysis() to include senario in sensitivity_plot. '\n-                             'The values of rho and the level are used for the scenario.')\n-        _check_integer(idx_treatment, \"idx_treatment\", lower_bound=0, upper_bound=self._dml_data.n_treat-1)\n-        if not isinstance(value, str):\n-            raise TypeError('value must be a string. '\n-                            f'{str(value)} of type {type(value)} was passed.')\n-        valid_values = ['theta', 'ci']\n-        if value not in valid_values:\n-            raise ValueError('Invalid value ' + value + '. ' +\n-                             'Valid values ' + ' or '.join(valid_values) + '.')\n-        _check_bool(include_scenario, 'include_scenario')\n-        _check_benchmarks(benchmarks)\n-        _check_bool(fill, 'fill')\n-        _check_in_zero_one(grid_bounds[0], \"grid_bounds\", include_zero=False, include_one=False)\n-        _check_in_zero_one(grid_bounds[1], \"grid_bounds\", include_zero=False, include_one=False)\n-        _check_integer(grid_size, \"grid_size\", lower_bound=10)\n-\n-        null_hypothesis = self.sensitivity_params['input']['null_hypothesis'][idx_treatment]\n-        unadjusted_theta = self.coef[idx_treatment]\n-        # check which side is relvant\n-        bound = 'upper' if (null_hypothesis > unadjusted_theta) else 'lower'\n-\n-        # create evaluation grid\n-        cf_d_vec = np.linspace(0, grid_bounds[0], grid_size)\n-        cf_y_vec = np.linspace(0, grid_bounds[1], grid_size)\n-\n-        # compute contour values\n-        contour_values = np.full(shape=(grid_size, grid_size), fill_value=np.nan)\n-        for i_cf_d_grid, cf_d_grid in enumerate(cf_d_vec):\n-            for i_cf_y_grid, cf_y_grid in enumerate(cf_y_vec):\n-                sens_dict = self._calc_sensitivity_analysis(cf_y=cf_y_grid,\n-                                                            cf_d=cf_d_grid,\n-                                                            rho=self.sensitivity_params['input']['rho'],\n-                                                            level=self.sensitivity_params['input']['level'])\n-                contour_values[i_cf_d_grid, i_cf_y_grid] = sens_dict[value][bound][idx_treatment]\n-\n-        # get the correct unadjusted value for confidence bands\n-        if value == 'theta':\n-            unadjusted_value = unadjusted_theta\n-        else:\n-            assert value == 'ci'\n-            ci = self.confint(level=self.sensitivity_params['input']['level'])\n-            if bound == 'upper':\n-                unadjusted_value = ci.iloc[idx_treatment, 1]\n-            else:\n-                unadjusted_value = ci.iloc[idx_treatment, 0]\n-\n-        # compute the values for the benchmarks\n-        benchmark_dict = copy.deepcopy(benchmarks)\n-        if benchmarks is not None:\n-            n_benchmarks = len(benchmarks['name'])\n-            benchmark_values = np.full(shape=(n_benchmarks,), fill_value=np.nan)\n-            for benchmark_idx in range(len(benchmarks['name'])):\n-                sens_dict_bench = self._calc_sensitivity_analysis(cf_y=benchmarks['cf_y'][benchmark_idx],\n-                                                                  cf_d=benchmarks['cf_d'][benchmark_idx],\n-                                                                  rho=self.sensitivity_params['input']['rho'],\n-                                                                  level=self.sensitivity_params['input']['level'])\n-                benchmark_values[benchmark_idx] = sens_dict_bench[value][bound][idx_treatment]\n-            benchmark_dict['value'] = benchmark_values\n-        fig = _sensitivity_contour_plot(x=cf_d_vec,\n-                                        y=cf_y_vec,\n-                                        contour_values=contour_values,\n-                                        unadjusted_value=unadjusted_value,\n-                                        scenario_x=self.sensitivity_params['input']['cf_d'],\n-                                        scenario_y=self.sensitivity_params['input']['cf_y'],\n-                                        scenario_value=self.sensitivity_params[value][bound][idx_treatment],\n-                                        include_scenario=include_scenario,\n-                                        benchmarks=benchmark_dict,\n-                                        fill=fill)\n+        if self._framework is None:\n+            raise ValueError('Apply fit() before sensitivity_plot().')\n+        fig = self._framework.sensitivity_plot(\n+            idx_treatment=idx_treatment,\n+            value=value,\n+            rho=rho,\n+            level=level,\n+            null_hypothesis=null_hypothesis,\n+            include_scenario=include_scenario,\n+            benchmarks=benchmarks,\n+            fill=fill,\n+            grid_bounds=grid_bounds,\n+            grid_size=grid_size\n+        )\n+\n         return fig\n \n     def sensitivity_benchmark(self, benchmarking_set, fit_args=None):\ndiff --git a/doubleml/double_ml_framework.py b/doubleml/double_ml_framework.py\nindex 9ae9fb59..feedee41 100644\n--- a/doubleml/double_ml_framework.py\n+++ b/doubleml/double_ml_framework.py\n@@ -1,11 +1,15 @@\n import numpy as np\n import pandas as pd\n+import copy\n \n from scipy.stats import norm\n+from scipy.optimize import minimize_scalar\n from statsmodels.stats.multitest import multipletests\n \n-from .utils._estimation import _draw_weights, _aggregate_coefs_and_ses\n-from .utils._checks import _check_bootstrap, _check_framework_compatibility\n+from .utils._estimation import _draw_weights, _aggregate_coefs_and_ses, _var_est\n+from .utils._checks import _check_bootstrap, _check_framework_compatibility, _check_in_zero_one, \\\n+    _check_float, _check_integer, _check_bool, _check_benchmarks\n+from .utils._plots import _sensitivity_contour_plot\n \n \n class DoubleMLFramework():\n@@ -27,7 +31,8 @@ def __init__(\n         self._is_cluster_data = False\n \n         # check input\n-        assert isinstance(doubleml_dict, dict), \"doubleml_dict must be a dictionary.\"\n+        if not isinstance(doubleml_dict, dict):\n+            raise TypeError('doubleml_dict must be a dictionary.')\n         expected_keys = ['thetas', 'ses', 'all_thetas', 'all_ses', 'var_scaling_factors', 'scaled_psi']\n         if not all(key in doubleml_dict.keys() for key in expected_keys):\n             raise ValueError('The dict must contain the following keys: ' + ', '.join(expected_keys))\n@@ -44,11 +49,14 @@ def __init__(\n         self._var_scaling_factors = doubleml_dict['var_scaling_factors']\n         self._scaled_psi = doubleml_dict['scaled_psi']\n \n-        if \"is_cluster_data\" in doubleml_dict.keys():\n-            self._is_cluster_data = doubleml_dict['is_cluster_data']\n+        # initialize cluster data\n+        self._check_and_set_cluster_data(doubleml_dict)\n+\n+        # initialize sensitivity analysis\n+        self._check_and_set_sensitivity_elements(doubleml_dict)\n \n         # check if all sizes match\n-        _check_framework_shapes(self)\n+        self._check_framework_shapes()\n         # initialize bootstrap distribution\n         self._boot_t_stat = None\n         self._boot_method = None\n@@ -170,12 +178,30 @@ def boot_t_stat(self):\n         \"\"\"\n         return self._boot_t_stat\n \n+    @property\n+    def sensitivity_elements(self):\n+        \"\"\"\n+        Values of the sensitivity components.\n+        If available (e.g., PLR, IRM) a dictionary with entries ``sigma2``, ``nu2``, ``psi_sigma2``, ``psi_nu2``\n+        and ``riesz_rep``.\n+        \"\"\"\n+        return self._sensitivity_elements\n+\n+    @property\n+    def sensitivity_params(self):\n+        \"\"\"\n+        Values of the sensitivity parameters after calling :meth:`sesitivity_analysis`;\n+        If available (e.g., PLR, IRM) a dictionary with entries ``theta``, ``se``, ``ci``, ``rv``\n+        and ``rva``.\n+        \"\"\"\n+        return self._sensitivity_params\n+\n     def __add__(self, other):\n \n         if isinstance(other, DoubleMLFramework):\n             # internal consistency check\n-            _check_framework_shapes(self)\n-            _check_framework_shapes(other)\n+            self._check_framework_shapes()\n+            other._check_framework_shapes()\n             _check_framework_compatibility(self, other, check_treatments=True)\n \n             all_thetas = self._all_thetas + other._all_thetas\n@@ -192,7 +218,6 @@ def __add__(self, other):\n             all_ses = np.sqrt(sigma2_hat)\n             thetas, ses = _aggregate_coefs_and_ses(all_thetas, all_ses, var_scaling_factors)\n \n-            is_cluster_data = self._is_cluster_data or other._is_cluster_data\n             doubleml_dict = {\n                 'thetas': thetas,\n                 'ses': ses,\n@@ -200,10 +225,28 @@ def __add__(self, other):\n                 'all_ses': all_ses,\n                 'var_scaling_factors': var_scaling_factors,\n                 'scaled_psi': scaled_psi,\n-                'is_cluster_data': is_cluster_data,\n+                'is_cluster_data': self._is_cluster_data,\n+                'cluster_dict': self._cluster_dict,\n             }\n-            new_obj = DoubleMLFramework(doubleml_dict)\n \n+            # sensitivity combination only available for same outcome and cond. expectation (e.g. IRM)\n+            if self._sensitivity_implemented and other._sensitivity_implemented:\n+                nu2_score_element = self._sensitivity_elements['psi_nu2'] + other._sensitivity_elements['psi_nu2'] - \\\n+                     np.multiply(2.0, np.multiply(self._sensitivity_elements['riesz_rep'],\n+                                                  self._sensitivity_elements['riesz_rep']))\n+                nu2 = np.mean(nu2_score_element, axis=0, keepdims=True)\n+                psi_nu2 = nu2_score_element - nu2\n+\n+                sensitivity_elements = {\n+                    'sigma2': self._sensitivity_elements['sigma2'],\n+                    'nu2': nu2,\n+                    'psi_sigma2': self._sensitivity_elements['psi_sigma2'],\n+                    'psi_nu2': psi_nu2,\n+                    'riesz_rep': self._sensitivity_elements['riesz_rep'] + other._sensitivity_elements['riesz_rep'],\n+                }\n+                doubleml_dict['sensitivity_elements'] = sensitivity_elements\n+\n+            new_obj = DoubleMLFramework(doubleml_dict)\n         else:\n             raise TypeError(f\"Unsupported operand type: {type(other)}\")\n \n@@ -216,8 +259,8 @@ def __sub__(self, other):\n \n         if isinstance(other, DoubleMLFramework):\n             # internal consistency check\n-            _check_framework_shapes(self)\n-            _check_framework_shapes(other)\n+            self._check_framework_shapes()\n+            other._check_framework_shapes()\n             _check_framework_compatibility(self, other, check_treatments=True)\n \n             all_thetas = self._all_thetas - other._all_thetas\n@@ -234,7 +277,6 @@ def __sub__(self, other):\n             all_ses = np.sqrt(sigma2_hat)\n             thetas, ses = _aggregate_coefs_and_ses(all_thetas, all_ses, var_scaling_factors)\n \n-            is_cluster_data = self._is_cluster_data or other._is_cluster_data\n             doubleml_dict = {\n                 'thetas': thetas,\n                 'ses': ses,\n@@ -242,10 +284,28 @@ def __sub__(self, other):\n                 'all_ses': all_ses,\n                 'var_scaling_factors': var_scaling_factors,\n                 'scaled_psi': scaled_psi,\n-                'is_cluster_data': is_cluster_data,\n+                'is_cluster_data': self._is_cluster_data,\n+                'cluster_dict': self._cluster_dict,\n             }\n-            new_obj = DoubleMLFramework(doubleml_dict)\n \n+            # sensitivity combination only available for same outcome and cond. expectation (e.g. IRM)\n+            if self._sensitivity_implemented and other._sensitivity_implemented:\n+                nu2_score_element = self._sensitivity_elements['psi_nu2'] - other._sensitivity_elements['psi_nu2'] + \\\n+                     np.multiply(2.0, np.multiply(self._sensitivity_elements['riesz_rep'],\n+                                                  self._sensitivity_elements['riesz_rep']))\n+                nu2 = np.mean(nu2_score_element, axis=0, keepdims=True)\n+                psi_nu2 = nu2_score_element - nu2\n+\n+                sensitivity_elements = {\n+                    'sigma2': self._sensitivity_elements['sigma2'],\n+                    'nu2': nu2,\n+                    'psi_sigma2': self._sensitivity_elements['psi_sigma2'],\n+                    'psi_nu2': psi_nu2,\n+                    'riesz_rep': self._sensitivity_elements['riesz_rep'] - other._sensitivity_elements['riesz_rep'],\n+                }\n+                doubleml_dict['sensitivity_elements'] = sensitivity_elements\n+\n+            new_obj = DoubleMLFramework(doubleml_dict)\n         else:\n             raise TypeError(f\"Unsupported operand type: {type(other)}\")\n \n@@ -265,7 +325,6 @@ def __mul__(self, other):\n             all_ses = np.multiply(other, self._all_ses)\n             scaled_psi = np.multiply(other, self._scaled_psi)\n \n-            is_cluster_data = self._is_cluster_data\n             doubleml_dict = {\n                 'thetas': thetas,\n                 'ses': ses,\n@@ -273,8 +332,25 @@ def __mul__(self, other):\n                 'all_ses': all_ses,\n                 'var_scaling_factors': var_scaling_factors,\n                 'scaled_psi': scaled_psi,\n-                'is_cluster_data': is_cluster_data,\n+                'is_cluster_data': self._is_cluster_data,\n+                'cluster_dict': self._cluster_dict,\n             }\n+\n+            # sensitivity combination only available for linear models\n+            if self._sensitivity_implemented:\n+                nu2_score_element = np.multiply(np.square(other), self._sensitivity_elements['psi_nu2'])\n+                nu2 = np.mean(nu2_score_element, axis=0, keepdims=True)\n+                psi_nu2 = nu2_score_element - nu2\n+\n+                sensitivity_elements = {\n+                    'sigma2': self._sensitivity_elements['sigma2'],\n+                    'nu2': nu2,\n+                    'psi_sigma2': self._sensitivity_elements['psi_sigma2'],\n+                    'psi_nu2': psi_nu2,\n+                    'riesz_rep': np.multiply(other, self._sensitivity_elements['riesz_rep']),\n+                }\n+                doubleml_dict['sensitivity_elements'] = sensitivity_elements\n+\n             new_obj = DoubleMLFramework(doubleml_dict)\n         else:\n             raise TypeError(f\"Unsupported operand type: {type(other)}\")\n@@ -284,6 +360,206 @@ def __mul__(self, other):\n     def __rmul__(self, other):\n         return self.__mul__(other)\n \n+    def _calc_sensitivity_analysis(self, cf_y, cf_d, rho, level):\n+        if not self._sensitivity_implemented:\n+            raise NotImplementedError('Sensitivity analysis is not implemented for this model.')\n+\n+        # input checks\n+        _check_in_zero_one(cf_y, 'cf_y', include_one=False)\n+        _check_in_zero_one(cf_d, 'cf_d', include_one=False)\n+        if not isinstance(rho, float):\n+            raise TypeError(f'rho must be of float type. '\n+                            f'{str(rho)} of type {str(type(rho))} was passed.')\n+        _check_in_zero_one(abs(rho), 'The absolute value of rho')\n+        _check_in_zero_one(level, 'The confidence level', include_zero=False, include_one=False)\n+\n+        # set elements for readability\n+        sigma2 = self.sensitivity_elements['sigma2']\n+        nu2 = self.sensitivity_elements['nu2']\n+        psi_sigma = self.sensitivity_elements['psi_sigma2']\n+        psi_nu = self.sensitivity_elements['psi_nu2']\n+        psi_scaled = self._scaled_psi\n+\n+        if (np.any(sigma2 < 0)) | (np.any(nu2 < 0)):\n+            raise ValueError('sensitivity_elements sigma2 and nu2 have to be positive. '\n+                             f\"Got sigma2 {str(sigma2)} and nu2 {str(nu2)}. \"\n+                             'Most likely this is due to low quality learners (especially propensity scores).')\n+\n+        # elementwise operations\n+        confounding_strength = np.multiply(np.abs(rho), np.sqrt(np.multiply(cf_y, np.divide(cf_d, 1.0-cf_d))))\n+        sensitivity_scaling = np.sqrt(np.multiply(sigma2, nu2))\n+\n+        # sigma2 and nu2 are of shape (1, n_thetas, n_rep), whereas the all_thetas is of shape (n_thetas, n_rep)\n+        all_theta_lower = self.all_thetas - np.multiply(np.squeeze(sensitivity_scaling, axis=0), confounding_strength)\n+        all_theta_upper = self.all_thetas + np.multiply(np.squeeze(sensitivity_scaling, axis=0), confounding_strength)\n+\n+        psi_variances = np.multiply(sigma2, psi_nu) + np.multiply(nu2, psi_sigma)\n+        psi_bias = np.multiply(np.divide(confounding_strength, np.multiply(2.0, sensitivity_scaling)), psi_variances)\n+        psi_lower = psi_scaled - psi_bias\n+        psi_upper = psi_scaled + psi_bias\n+\n+        # shape (n_thetas, n_reps); includes scaling with n^{-1/2}\n+        all_sigma_lower = np.full_like(all_theta_lower, fill_value=np.nan)\n+        all_sigma_upper = np.full_like(all_theta_upper, fill_value=np.nan)\n+\n+        for i_rep in range(self.n_rep):\n+            for i_theta in range(self.n_thetas):\n+\n+                if not self._is_cluster_data:\n+                    smpls = None\n+                    cluster_vars = None\n+                    smpls_cluster = None\n+                    n_folds_per_cluster = None\n+                else:\n+                    smpls = self._cluster_dict['smpls'][i_rep]\n+                    cluster_vars = self._cluster_dict['cluster_vars']\n+                    smpls_cluster = self._cluster_dict['smpls_cluster'][i_rep]\n+                    n_folds_per_cluster = self._cluster_dict['n_folds_per_cluster']\n+\n+                sigma2_lower_hat, _ = _var_est(psi=psi_lower[:, i_theta, i_rep],\n+                                               psi_deriv=np.ones_like(psi_lower[:, i_theta, i_rep]),\n+                                               smpls=smpls,\n+                                               is_cluster_data=self._is_cluster_data,\n+                                               cluster_vars=cluster_vars,\n+                                               smpls_cluster=smpls_cluster,\n+                                               n_folds_per_cluster=n_folds_per_cluster)\n+                sigma2_upper_hat, _ = _var_est(psi=psi_upper[:, i_theta, i_rep],\n+                                               psi_deriv=np.ones_like(psi_upper[:, i_theta, i_rep]),\n+                                               smpls=smpls,\n+                                               is_cluster_data=self._is_cluster_data,\n+                                               cluster_vars=cluster_vars,\n+                                               smpls_cluster=smpls_cluster,\n+                                               n_folds_per_cluster=n_folds_per_cluster)\n+\n+                all_sigma_lower[i_theta, i_rep] = np.sqrt(sigma2_lower_hat)\n+                all_sigma_upper[i_theta, i_rep] = np.sqrt(sigma2_upper_hat)\n+\n+        # aggregate coefs and ses over n_rep\n+        theta_lower, sigma_lower = _aggregate_coefs_and_ses(all_theta_lower, all_sigma_lower, self._var_scaling_factors)\n+        theta_upper, sigma_upper = _aggregate_coefs_and_ses(all_theta_upper, all_sigma_upper, self._var_scaling_factors)\n+\n+        # per repetition confidence intervals\n+        quant = norm.ppf(level)\n+        all_ci_lower = all_theta_lower - np.multiply(quant, all_sigma_lower)\n+        all_ci_upper = all_theta_upper + np.multiply(quant, all_sigma_upper)\n+\n+        ci_lower = np.median(all_ci_lower, axis=1)\n+        ci_upper = np.median(all_ci_upper, axis=1)\n+\n+        theta_dict = {'lower': theta_lower,\n+                      'upper': theta_upper}\n+\n+        se_dict = {'lower': sigma_lower,\n+                   'upper': sigma_upper}\n+\n+        ci_dict = {'lower': ci_lower,\n+                   'upper': ci_upper}\n+\n+        res_dict = {'theta': theta_dict,\n+                    'se': se_dict,\n+                    'ci': ci_dict}\n+\n+        return res_dict\n+\n+    def _calc_robustness_value(self, null_hypothesis, level, rho, idx_treatment):\n+        _check_float(null_hypothesis, \"null_hypothesis\")\n+        _check_integer(idx_treatment, \"idx_treatment\", lower_bound=0, upper_bound=self._n_thetas-1)\n+\n+        # check which side is relvant\n+        bound = 'upper' if (null_hypothesis > self.thetas[idx_treatment]) else 'lower'\n+\n+        # minimize the square to find boundary solutions\n+        def rv_fct(value, param):\n+            res = self._calc_sensitivity_analysis(cf_y=value,\n+                                                  cf_d=value,\n+                                                  rho=rho,\n+                                                  level=level)[param][bound][idx_treatment] - null_hypothesis\n+            return np.square(res)\n+\n+        rv = minimize_scalar(rv_fct, bounds=(0, 0.9999), method='bounded', args=('theta', )).x\n+        rva = minimize_scalar(rv_fct, bounds=(0, 0.9999), method='bounded', args=('ci', )).x\n+\n+        return rv, rva\n+\n+    def sensitivity_analysis(self, cf_y=0.03, cf_d=0.03, rho=1.0, level=0.95, null_hypothesis=0.0):\n+        \"\"\"\n+        Performs a sensitivity analysis to account for unobserved confounders.\n+\n+        The evaluated scenario is stored as a dictionary in the property ``sensitivity_params``.\n+\n+        Parameters\n+        ----------\n+        cf_y : float\n+            Percentage of the residual variation of the outcome explained by latent/confounding variables.\n+            Default is ``0.03``.\n+\n+        cf_d : float\n+            Percentage gains in the variation of the Riesz representer generated by latent/confounding variables.\n+            Default is ``0.03``.\n+\n+        rho : float\n+            The correlation between the differences in short and long representations in the main regression and\n+            Riesz representer. Has to be in [-1,1]. The absolute value determines the adversarial strength of the\n+            confounding (maximizes at 1.0).\n+            Default is ``1.0``.\n+\n+        level : float\n+            The confidence level.\n+            Default is ``0.95``.\n+\n+        null_hypothesis : float or numpy.ndarray\n+            Null hypothesis for the effect. Determines the robustness values.\n+            If it is a single float uses the same null hypothesis for all estimated parameters.\n+            Else the array has to be of shape (n_thetas,).\n+            Default is ``0.0``.\n+\n+        Returns\n+        -------\n+        self : object\n+        \"\"\"\n+        # check null_hypothesis\n+        if isinstance(null_hypothesis, float):\n+            null_hypothesis_vec = np.full(shape=self._n_thetas, fill_value=null_hypothesis)\n+        elif isinstance(null_hypothesis, np.ndarray):\n+            if null_hypothesis.shape == (self._n_thetas,):\n+                null_hypothesis_vec = null_hypothesis\n+            else:\n+                raise ValueError(\"null_hypothesis is numpy.ndarray but does not have the required \"\n+                                 f\"shape ({self._n_thetas},). \"\n+                                 f'Array of shape {str(null_hypothesis.shape)} was passed.')\n+        else:\n+            raise TypeError(\"null_hypothesis has to be of type float or np.ndarry. \"\n+                            f\"{str(null_hypothesis)} of type {str(type(null_hypothesis))} was passed.\")\n+\n+        # compute sensitivity analysis\n+        sensitivity_dict = self._calc_sensitivity_analysis(cf_y=cf_y, cf_d=cf_d, rho=rho, level=level)\n+\n+        # compute robustess values with respect to null_hypothesis\n+        rv = np.full(shape=self._n_thetas, fill_value=np.nan)\n+        rva = np.full(shape=self._n_thetas, fill_value=np.nan)\n+\n+        for i_theta in range(self._n_thetas):\n+            rv[i_theta], rva[i_theta] = self._calc_robustness_value(\n+                null_hypothesis=null_hypothesis_vec[i_theta],\n+                level=level,\n+                rho=rho,\n+                idx_treatment=i_theta\n+            )\n+\n+        sensitivity_dict['rv'] = rv\n+        sensitivity_dict['rva'] = rva\n+\n+        # add all input parameters\n+        input_params = {'cf_y': cf_y,\n+                        'cf_d': cf_d,\n+                        'rho': rho,\n+                        'level': level,\n+                        'null_hypothesis': null_hypothesis_vec}\n+        sensitivity_dict['input'] = input_params\n+\n+        self._sensitivity_params = sensitivity_dict\n+        return self\n+\n     def confint(self, joint=False, level=0.95):\n         \"\"\"\n         Confidence intervals for DoubleML models.\n@@ -445,6 +721,229 @@ def p_adjust(self, method='romano-wolf'):\n \n         return df_p_vals, all_p_vals_corrected\n \n+    def sensitivity_plot(self, idx_treatment=0, value='theta', rho=1.0, level=0.95, null_hypothesis=0.0,\n+                         include_scenario=True, benchmarks=None, fill=True, grid_bounds=(0.15, 0.15), grid_size=100):\n+        \"\"\"\n+        Contour plot of the sensivity with respect to latent/confounding variables.\n+\n+        Parameters\n+        ----------\n+        idx_treatment : int\n+            Index of the treatment to perform the sensitivity analysis.\n+            Default is ``0``.\n+\n+        value : str\n+            Determines which contours to plot. Valid values are ``'theta'`` (refers to the bounds)\n+            and ``'ci'`` (refers to the bounds including statistical uncertainty).\n+            Default is ``'theta'``.\n+\n+        rho: float\n+            The correlation between the differences in short and long representations in the main regression and\n+            Riesz representer. Has to be in [-1,1]. The absolute value determines the adversarial strength of the\n+            confounding (maximizes at 1.0).\n+            Default is ``1.0``.\n+\n+        level : float\n+            The confidence level.\n+            Default is ``0.95``.\n+\n+        null_hypothesis : float\n+            Null hypothesis for the effect. Determines the direction of the contour lines.\n+\n+        include_scenario : bool\n+            Indicates whether to highlight the scenario from the call of :meth:`sensitivity_analysis`.\n+            Default is ``True``.\n+\n+        benchmarks : dict or None\n+            Dictionary of benchmarks to be included in the plot. The keys are ``cf_y``, ``cf_d`` and ``name``.\n+            Default is ``None``.\n+\n+        fill : bool\n+            Indicates whether to use a heatmap style or only contour lines.\n+            Default is ``True``.\n+\n+        grid_bounds : tuple\n+            Determines the evaluation bounds of the grid for ``cf_d`` and ``cf_y``. Has to contain two floats in [0, 1).\n+            Default is ``(0.15, 0.15)``.\n+\n+        grid_size : int\n+            Determines the number of evaluation points of the grid.\n+            Default is ``100``.\n+\n+        Returns\n+        -------\n+        fig : object\n+            Plotly figure of the sensitivity contours.\n+        \"\"\"\n+        _check_integer(idx_treatment, \"idx_treatment\", lower_bound=0, upper_bound=self.n_thetas-1)\n+        if not isinstance(value, str):\n+            raise TypeError('value must be a string. '\n+                            f'{str(value)} of type {type(value)} was passed.')\n+        valid_values = ['theta', 'ci']\n+        if value not in valid_values:\n+            raise ValueError('Invalid value ' + value + '. ' +\n+                             'Valid values ' + ' or '.join(valid_values) + '.')\n+        _check_float(null_hypothesis, \"null_hypothesis\")\n+        _check_bool(include_scenario, 'include_scenario')\n+        if include_scenario and self.sensitivity_params is None:\n+            raise ValueError('Apply sensitivity_analysis() to include senario in sensitivity_plot. ')\n+        _check_benchmarks(benchmarks)\n+        _check_bool(fill, 'fill')\n+        _check_in_zero_one(grid_bounds[0], \"grid_bounds\", include_zero=False, include_one=False)\n+        _check_in_zero_one(grid_bounds[1], \"grid_bounds\", include_zero=False, include_one=False)\n+        _check_integer(grid_size, \"grid_size\", lower_bound=10)\n+\n+        null_hypothesis = self.sensitivity_params['input']['null_hypothesis'][idx_treatment]\n+        unadjusted_theta = self.thetas[idx_treatment]\n+        # check which side is relvant\n+        bound = 'upper' if (null_hypothesis > unadjusted_theta) else 'lower'\n+\n+        # create evaluation grid\n+        cf_d_vec = np.linspace(0, grid_bounds[0], grid_size)\n+        cf_y_vec = np.linspace(0, grid_bounds[1], grid_size)\n+\n+        # compute contour values\n+        contour_values = np.full(shape=(grid_size, grid_size), fill_value=np.nan)\n+        for i_cf_d_grid, cf_d_grid in enumerate(cf_d_vec):\n+            for i_cf_y_grid, cf_y_grid in enumerate(cf_y_vec):\n+\n+                sens_dict = self._calc_sensitivity_analysis(\n+                    cf_y=cf_y_grid,\n+                    cf_d=cf_d_grid,\n+                    rho=rho,\n+                    level=level,\n+                )\n+                contour_values[i_cf_d_grid, i_cf_y_grid] = sens_dict[value][bound][idx_treatment]\n+\n+        # get the correct unadjusted value for confidence bands\n+        if value == 'theta':\n+            unadjusted_value = unadjusted_theta\n+        else:\n+            assert value == 'ci'\n+            ci = self.confint(level=self.sensitivity_params['input']['level'])\n+            if bound == 'upper':\n+                unadjusted_value = ci.iloc[idx_treatment, 1]\n+            else:\n+                unadjusted_value = ci.iloc[idx_treatment, 0]\n+\n+        # compute the values for the benchmarks\n+        benchmark_dict = copy.deepcopy(benchmarks)\n+        if benchmarks is not None:\n+            n_benchmarks = len(benchmarks['name'])\n+            benchmark_values = np.full(shape=(n_benchmarks,), fill_value=np.nan)\n+            for benchmark_idx in range(len(benchmarks['name'])):\n+                sens_dict_bench = self._calc_sensitivity_analysis(\n+                    cf_y=benchmarks['cf_y'][benchmark_idx],\n+                    cf_d=benchmarks['cf_d'][benchmark_idx],\n+                    rho=self.sensitivity_params['input']['rho'],\n+                    level=self.sensitivity_params['input']['level']\n+                )\n+                benchmark_values[benchmark_idx] = sens_dict_bench[value][bound][idx_treatment]\n+            benchmark_dict['value'] = benchmark_values\n+        fig = _sensitivity_contour_plot(x=cf_d_vec,\n+                                        y=cf_y_vec,\n+                                        contour_values=contour_values,\n+                                        unadjusted_value=unadjusted_value,\n+                                        scenario_x=self.sensitivity_params['input']['cf_d'],\n+                                        scenario_y=self.sensitivity_params['input']['cf_y'],\n+                                        scenario_value=self.sensitivity_params[value][bound][idx_treatment],\n+                                        include_scenario=include_scenario,\n+                                        benchmarks=benchmark_dict,\n+                                        fill=fill)\n+        return fig\n+\n+    def _check_and_set_cluster_data(self, doubleml_dict):\n+        self._cluster_dict = None\n+\n+        if \"is_cluster_data\" in doubleml_dict.keys():\n+            _check_bool(doubleml_dict['is_cluster_data'], 'is_cluster_data')\n+            self._is_cluster_data = doubleml_dict['is_cluster_data']\n+\n+        if self._is_cluster_data:\n+            if not (\"cluster_dict\" in doubleml_dict.keys()):\n+                raise ValueError('If is_cluster_data is True, cluster_dict must be provided.')\n+\n+            if not isinstance(doubleml_dict['cluster_dict'], dict):\n+                raise TypeError('cluster_dict must be a dictionary.')\n+\n+            expected_keys_cluster = ['smpls', 'smpls_cluster', 'cluster_vars', 'n_folds_per_cluster']\n+            if not all(key in doubleml_dict['cluster_dict'].keys() for key in expected_keys_cluster):\n+                raise ValueError('The cluster_dict must contain the following keys: ' + ', '.join(expected_keys_cluster)\n+                                 + '. Got: ' + ', '.join(doubleml_dict['cluster_dict'].keys()) + '.')\n+\n+            self._cluster_dict = doubleml_dict['cluster_dict']\n+\n+        return\n+\n+    def _check_and_set_sensitivity_elements(self, doubleml_dict):\n+        if not (\"sensitivity_elements\" in doubleml_dict.keys()):\n+            sensitivity_implemented = False\n+            sensitivity_elements = None\n+\n+        else:\n+            if not isinstance(doubleml_dict['sensitivity_elements'], dict):\n+                raise TypeError('sensitivity_elements must be a dictionary.')\n+            expected_keys_sensitivity = ['sigma2', 'nu2', 'psi_sigma2', 'psi_nu2', 'riesz_rep']\n+            if not all(key in doubleml_dict['sensitivity_elements'].keys() for key in expected_keys_sensitivity):\n+                raise ValueError('The sensitivity_elements dict must contain the following '\n+                                 'keys: ' + ', '.join(expected_keys_sensitivity))\n+\n+            for key in expected_keys_sensitivity:\n+                if not isinstance(doubleml_dict['sensitivity_elements'][key], np.ndarray):\n+                    raise TypeError(f'The sensitivity element {key} must be a numpy array.')\n+\n+            # set sensitivity elements\n+            sensitivity_implemented = True\n+            sensitivity_elements = {\n+                'sigma2': doubleml_dict['sensitivity_elements']['sigma2'],\n+                'nu2': doubleml_dict['sensitivity_elements']['nu2'],\n+                'psi_sigma2': doubleml_dict['sensitivity_elements']['psi_sigma2'],\n+                'psi_nu2': doubleml_dict['sensitivity_elements']['psi_nu2'],\n+                'riesz_rep': doubleml_dict['sensitivity_elements']['riesz_rep'],\n+            }\n+\n+        self._sensitivity_implemented = sensitivity_implemented\n+        self._sensitivity_elements = sensitivity_elements\n+        self._sensitivity_params = None\n+\n+        return\n+\n+    def _check_framework_shapes(self):\n+        score_dim = (self._n_obs, self._n_thetas, self.n_rep)\n+        # check if all sizes match\n+        if self._thetas.shape != (self._n_thetas,):\n+            raise ValueError(f'The shape of thetas does not match the expected shape ({self._n_thetas},).')\n+        if self._ses.shape != (self._n_thetas,):\n+            raise ValueError(f'The shape of ses does not match the expected shape ({self._n_thetas},).')\n+        if self._all_thetas.shape != (self._n_thetas, self._n_rep):\n+            raise ValueError(f'The shape of all_thetas does not match the expected shape ({self._n_thetas}, {self._n_rep}).')\n+        if self._all_ses.shape != (self._n_thetas, self._n_rep):\n+            raise ValueError(f'The shape of all_ses does not match the expected shape ({self._n_thetas}, {self._n_rep}).')\n+        if self._var_scaling_factors.shape != (self._n_thetas,):\n+            raise ValueError(f'The shape of var_scaling_factors does not match the expected shape ({self._n_thetas},).')\n+        # dimension of scaled_psi is n_obs x n_thetas x n_rep (per default)\n+        if self._scaled_psi.shape != score_dim:\n+            raise ValueError(('The shape of scaled_psi does not match the expected '\n+                              f'shape ({self._n_obs}, {self._n_thetas}, {self._n_rep}).'))\n+\n+        if self._sensitivity_implemented:\n+            if self._sensitivity_elements['sigma2'].shape != (1, self._n_thetas, self.n_rep):\n+                raise ValueError('The shape of sigma2 does not match the expected shape '\n+                                 f'(1, {self._n_thetas}, {self._n_rep}).')\n+            if self._sensitivity_elements['nu2'].shape != (1, self._n_thetas, self.n_rep):\n+                raise ValueError(f'The shape of nu2 does not match the expected shape (1, {self._n_thetas}, {self._n_rep}).')\n+            if self._sensitivity_elements['psi_sigma2'].shape != score_dim:\n+                raise ValueError(('The shape of psi_sigma2 does not match the expected '\n+                                 f'shape ({self._n_obs}, {self._n_thetas}, {self._n_rep}).'))\n+            if self._sensitivity_elements['psi_nu2'].shape != score_dim:\n+                raise ValueError(('The shape of psi_nu2 does not match the expected '\n+                                 f'shape ({self._n_obs}, {self._n_thetas}, {self._n_rep}).'))\n+            if self._sensitivity_elements['riesz_rep'].shape != score_dim:\n+                raise ValueError(('The shape of riesz_rep does not match the expected '\n+                                 f'shape ({self._n_obs}, {self._n_thetas}, {self._n_rep}).'))\n+\n+        return None\n+\n \n def concat(objs):\n     \"\"\"\n@@ -457,7 +956,7 @@ def concat(objs):\n         raise TypeError('All objects must be of type DoubleMLFramework.')\n \n     # check on internal consitency of objects\n-    _ = [_check_framework_shapes(obj) for obj in objs]\n+    _ = [obj._check_framework_shapes() for obj in objs]\n     # check if all objects are compatible in n_obs and n_rep\n     _ = [_check_framework_compatibility(objs[0], obj, check_treatments=False) for obj in objs[1:]]\n \n@@ -469,7 +968,11 @@ def concat(objs):\n     thetas = np.concatenate([obj.thetas for obj in objs], axis=0)\n     ses = np.concatenate([obj.ses for obj in objs], axis=0)\n \n-    is_cluster_data = any(obj._is_cluster_data for obj in objs)\n+    if any(obj._is_cluster_data for obj in objs):\n+        raise NotImplementedError('concat not yet implemented with clustering.')\n+    else:\n+        is_cluster_data = False\n+\n     doubleml_dict = {\n         'thetas': thetas,\n         'ses': ses,\n@@ -479,29 +982,18 @@ def concat(objs):\n         'scaled_psi': scaled_psi,\n         'is_cluster_data': is_cluster_data,\n     }\n+\n+    if all(obj._sensitivity_implemented for obj in objs):\n+        sensitivity_elements = {}\n+        for key in ['sigma2', 'nu2', 'psi_sigma2', 'psi_nu2', 'riesz_rep']:\n+            assert all(key in obj._sensitivity_elements.keys() for obj in objs)\n+            sensitivity_elements[key] = np.concatenate([obj._sensitivity_elements[key] for obj in objs], axis=1)\n+\n+        doubleml_dict['sensitivity_elements'] = sensitivity_elements\n+\n     new_obj = DoubleMLFramework(doubleml_dict)\n \n     # check internal consistency of new object\n-    _check_framework_shapes(new_obj)\n+    new_obj._check_framework_shapes()\n \n     return new_obj\n-\n-\n-def _check_framework_shapes(self):\n-    # check if all sizes match\n-    if self._thetas.shape != (self._n_thetas,):\n-        raise ValueError(f'The shape of thetas does not match the expected shape ({self._n_thetas},).')\n-    if self._ses.shape != (self._n_thetas,):\n-        raise ValueError(f'The shape of ses does not match the expected shape ({self._n_thetas},).')\n-    if self._all_thetas.shape != (self._n_thetas, self._n_rep):\n-        raise ValueError(f'The shape of all_thetas does not match the expected shape ({self._n_thetas}, {self._n_rep}).')\n-    if self._all_ses.shape != (self._n_thetas, self._n_rep):\n-        raise ValueError(f'The shape of all_ses does not match the expected shape ({self._n_thetas}, {self._n_rep}).')\n-    if self._var_scaling_factors.shape != (self._n_thetas,):\n-        raise ValueError(f'The shape of var_scaling_factors does not match the expected shape ({self._n_thetas},).')\n-    # dimension of scaled_psi is n_obs x n_thetas x n_rep (per default)\n-    if self._scaled_psi.shape != (self._n_obs, self._n_thetas, self._n_rep):\n-        raise ValueError(('The shape of scaled_psi does not match the expected '\n-                          f'shape ({self._n_obs}, {self._n_thetas}, {self._n_rep}).'))\n-\n-    return None\ndiff --git a/doubleml/irm/irm.py b/doubleml/irm/irm.py\nindex 5dd2c1b0..82d22d1d 100644\n--- a/doubleml/irm/irm.py\n+++ b/doubleml/irm/irm.py\n@@ -400,7 +400,9 @@ def _sensitivity_element_est(self, preds):\n         element_dict = {'sigma2': sigma2,\n                         'nu2': nu2,\n                         'psi_sigma2': psi_sigma2,\n-                        'psi_nu2': psi_nu2}\n+                        'psi_nu2': psi_nu2,\n+                        'riesz_rep': rr,\n+                        }\n         return element_dict\n \n     def _nuisance_tuning(self, smpls, param_grids, scoring_methods, n_folds_tune, n_jobs_cv,\ndiff --git a/doubleml/plm/plr.py b/doubleml/plm/plr.py\nindex ab758c90..3b7d90f5 100644\n--- a/doubleml/plm/plr.py\n+++ b/doubleml/plm/plr.py\n@@ -279,13 +279,17 @@ def _sensitivity_element_est(self, preds):\n         sigma2 = np.mean(sigma2_score_element)\n         psi_sigma2 = sigma2_score_element - sigma2\n \n-        nu2 = np.divide(1.0, np.mean(np.square(d - m_hat)))\n-        psi_nu2 = nu2 - np.multiply(np.square(d-m_hat), np.square(nu2))\n+        treatment_residual = d - m_hat\n+        nu2 = np.divide(1.0, np.mean(np.square(treatment_residual)))\n+        psi_nu2 = nu2 - np.multiply(np.square(treatment_residual), np.square(nu2))\n+        rr = np.multiply(treatment_residual, nu2)\n \n         element_dict = {'sigma2': sigma2,\n                         'nu2': nu2,\n                         'psi_sigma2': psi_sigma2,\n-                        'psi_nu2': psi_nu2}\n+                        'psi_nu2': psi_nu2,\n+                        'riesz_rep': rr,\n+                        }\n         return element_dict\n \n     def _nuisance_tuning(self, smpls, param_grids, scoring_methods, n_folds_tune, n_jobs_cv,\ndiff --git a/doubleml/utils/_checks.py b/doubleml/utils/_checks.py\nindex a5a67110..e54c4041 100644\n--- a/doubleml/utils/_checks.py\n+++ b/doubleml/utils/_checks.py\n@@ -348,6 +348,10 @@ def _check_framework_compatibility(dml_framework_1, dml_framework_2, check_treat\n             raise ValueError('The number of parameters theta in DoubleMLFrameworks must be the same. '\n                              f'Got {str(dml_framework_1.n_thetas)} and {str(dml_framework_2.n_thetas)}.')\n \n+    if dml_framework_1._is_cluster_data != dml_framework_2._is_cluster_data:\n+        raise ValueError('The cluster structure in DoubleMLFrameworks must be the same. '\n+                         f'Got {str(dml_framework_1._is_cluster_data)} and {str(dml_framework_2._is_cluster_data)}.')\n+\n \n def _check_set(x):\n     return {x} if x is not None else {}\n", "instance_id": "DoubleML__doubleml-for-py-249", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in describing the feature request to extend the `DoubleMLIRM` class for supporting multiple discrete treatments. It provides a mathematical formulation of the marginal counterfactual mean and contrasts for treatment effects, which helps in understanding the goal. References to existing implementations and nuances like the ATT (Average Treatment Effect on the Treated) not being well-defined for multiple treatments add some context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats for the new functionality, nor does it specify constraints or edge cases (e.g., how many discrete treatments should be supported, or what happens with imbalanced data). Additionally, while it mentions changing `check_data` onwards, it lacks specifics on the exact modifications or integration points within the existing codebase. These gaps prevent it from being comprehensive, but the overall intent and technical foundation are clear enough for a developer to start working on it.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.75, placing it in the \"Hard\" category. This assessment is based on several factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are extensive, spanning multiple files (`did.py`, `did_cs.py`, `double_ml.py`, `double_ml_framework.py`, `irm.py`, `plr.py`) and involving significant modifications to the core functionality of the DoubleML library. The changes are not limited to a single module but affect the overall framework, including sensitivity analysis, Riesz representations, and data handling. This requires a deep understanding of the interactions between different components of the codebase, such as how sensitivity elements are computed and integrated into the framework. The addition of `riesz_rep` as a new sensitivity element and the refactoring of sensitivity analysis logic into `DoubleMLFramework` indicate a substantial architectural impact.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several advanced concepts, including:\n   - **Statistical and Econometric Methods**: Understanding of Double Machine Learning (DML), Interactive Regression Models (IRM), and sensitivity analysis for unobserved confounders.\n   - **Mathematical Formulations**: Knowledge of influence functions, counterfactual means, and treatment effect estimation for multiple discrete treatments.\n   - **Python and Library-Specific Knowledge**: Proficiency in Python, NumPy, and the specific design patterns used in the DoubleML library (e.g., object-oriented structure, framework-based computations).\n   - **Algorithmic Complexity**: Handling multi-dimensional arrays and ensuring numerical stability in computations like variance estimation and sensitivity analysis.\n   These concepts are relatively complex and require a solid background in both programming and domain-specific knowledge (causal inference).\n\n3. **Edge Cases and Error Handling**: While the problem statement does not explicitly mention edge cases, the code changes suggest the need to handle various scenarios, such as ensuring positive values for sensitivity elements (`sigma2` and `nu2`), managing cluster data compatibility, and maintaining numerical stability in computations. The refactoring of sensitivity analysis logic also implies potential error handling for invalid inputs (e.g., `cf_y`, `cf_d`, `rho`) and ensuring backward compatibility with existing binary treatment cases. These aspects add to the complexity of the implementation.\n\n4. **Overall Complexity**: The task goes beyond simple feature addition or bug fixing. It involves extending a core statistical method to handle a more general case (multiple treatments), which requires careful design to avoid breaking existing functionality. The changes impact critical parts of the system, such as how treatment effects are estimated and how sensitivity to unobserved confounders is analyzed. This level of modification demands a deep understanding of the library's architecture and the underlying statistical theory, along with rigorous testing to ensure correctness.\n\nGiven these factors, a score of 0.75 reflects the challenging nature of the problem, requiring significant expertise and effort to implement correctly. It falls short of \"Very Hard\" (0.8-1.0) because it does not appear to involve system-level considerations (e.g., distributed systems) or extremely novel algorithmic innovation, but it is still a complex task within the domain of causal inference and machine learning libraries.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use Python 3.8 for tox tests locally\nSometimes running our tests locally pass, then fail in CI. This is because in CI we test against Python 3.8, whereas we default to whatever the user has by default locally (which could be 3.10).\nShould be able to specify version in the tox config: https://tox.wiki/en/latest/config.html#python-options\n", "patch": "diff --git a/README.md b/README.md\nindex 2d2eb0252..ac0235460 100644\n--- a/README.md\n+++ b/README.md\n@@ -73,14 +73,30 @@ You should be able to pass arguments to these commands as if you were running Pl\n It is highly recommended to use `npm run e2e:docker` (instead of `npm run e2e`) as CI also uses the same environment. You can also use `npm run e2e:update-snapshots` to regenerate snapshots in said environment. Run Playwright in [UI Mode](https://playwright.dev/docs/test-ui-mode) with `npm run e2e:ui` when creating new tests or debugging, as this will allow you to run each test individually, see the browser as it runs it, inspect the console, evaluate locators, etc.\n \n ### Running Python tests\n-\n-The above steps will also set up `tox` to run tests for the python plugins that support it.\n+The [venv setup](#pre-commit-hookspython-formatting) steps will also set up `tox` to run tests for the python plugins that support it.\n+Note that `tox` sets up an isolated environment for running tests.  \n+Be default, `tox` will run against Python 3.8, which will need to be installed on your system before running tests.\n You can run tests with the following command from the `plugins/<plugin>` directory:\n-\n ```shell\n tox -e py\n ```\n \n+> [!IMPORTANT]\n+> Linux, and possibly other setups such as MacOS depending on method, may require additional packages to be installed to run Python 3.8.\n+> ```shell\n+> sudo apt install python3.8 python3.8-distutils libpython3.8\n+> # or just full install although it will include more packages than necessary\n+> sudo apt install python3.8-full\n+> ```\n+\n+You can also run tests against a specific version of python by appending the version to `py`  \n+This assumes that the version of Python you're targeting is installed on your system.  \n+For example, to run tests against Python 3.12, run:  \n+```shell\n+tox -e py3.12\n+```\n+\n+\n ### Running plugin against deephaven-core\n \n #### Building Python plugins for development\ndiff --git a/plugins/plotly-express/tox.ini b/plugins/plotly-express/tox.ini\nindex b214f738a..edcbe5445 100644\n--- a/plugins/plotly-express/tox.ini\n+++ b/plugins/plotly-express/tox.ini\n@@ -5,4 +5,22 @@ isolated_build = True\n deps =\n     deephaven-server\n commands =\n-    python -m unittest discover\n\\ No newline at end of file\n+    python -m unittest discover\n+basepython = python3.8\n+\n+[testenv:py3.8]\n+basepython = python3.8\n+\n+[testenv:py3.9]\n+basepython = python3.9\n+\n+[testenv:py3.10]\n+basepython = python3.10\n+\n+[testenv:py3.11]\n+basepython = python3.11\n+\n+[testenv:py3.12]\n+basepython = python3.12\n+\n+\ndiff --git a/plugins/ui/tox.ini b/plugins/ui/tox.ini\nindex a66bff89f..bdebb2457 100644\n--- a/plugins/ui/tox.ini\n+++ b/plugins/ui/tox.ini\n@@ -5,4 +5,20 @@ isolated_build = True\n deps =\n     deephaven-server\n commands =\n-    python -m unittest {posargs}\n\\ No newline at end of file\n+    python -m unittest {posargs}\n+basepython = python3.8\n+\n+[testenv:py3.8]\n+basepython = python3.8\n+\n+[testenv:py3.9]\n+basepython = python3.9\n+\n+[testenv:py3.10]\n+basepython = python3.10\n+\n+[testenv:py3.11]\n+basepython = python3.11\n+\n+[testenv:py3.12]\n+basepython = python3.12\n\\ No newline at end of file\n", "instance_id": "deephaven__deephaven-plugins-972", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: to ensure that tox tests run locally with Python 3.8 by default to match the CI environment, addressing discrepancies between local and CI test results. It provides a reference to the tox configuration documentation, which is helpful. However, there are minor ambiguities and missing details. For instance, it does not explicitly mention whether Python 3.8 is the only version to be supported or if other versions should also be configurable (though the code changes suggest multi-version support). Additionally, there are no examples of expected tox command usage or potential issues with Python version availability on different systems. Constraints or requirements for handling cases where Python 3.8 is not installed are also not specified in the problem statement itself, though the code changes address this partially in the README. Overall, the goal is clear, but minor details and edge cases are not fully articulated in the description.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The task involves modifying tox configuration files to specify Python versions and updating documentation in the README to guide users on running tests with specific Python versions. The scope of code changes is limited to a few files (tox.ini in two plugins and the README.md), and the changes are straightforward\u2014adding configuration options for different Python versions and updating instructions. The technical concepts required are minimal: basic understanding of tox configuration and Python virtual environments, which are not complex for a developer familiar with Python testing workflows. There are no significant architectural impacts or deep codebase interactions to consider. Edge cases, such as the unavailability of Python 3.8 on a user's system, are partially addressed in the README updates, but no complex error handling is required in the code itself. Overall, this is a simple task that requires basic modifications and minimal debugging or advanced knowledge.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add support for Qdrant VectorDB\n### Description\r\n\r\nThis pull request introduces a new feature that integrates Griptape with Qdrant VectorDB, enhancing the capabilities of Griptape for handling large-scale vector data.\r\n\r\n### Problem Statement\r\n\r\nManaging and querying large-scale vector data efficiently has been a challenge. The current solutions lack seamless integration for ingesting, processing, and indexing vector embeddings, leading to inefficiencies in AI and machine learning workflows.\r\n\r\n### Solution\r\n\r\nThe integration between Griptape and Qdrant VectorDB provides a scalable and efficient solution for ingesting, processing, and indexing vector embeddings. Leveraging Griptape's powerful data pipeline capabilities and Qdrant's advanced vector search technology, this integration supports a wide range of AI and machine learning applications.\r\n\r\n### Key Features\r\n\r\n- **Seamless Ingestion:** Efficiently ingest large-scale vector data into the system.\r\n- **Advanced Processing:** Utilize Griptape's robust data processing pipelines to handle vector data.\r\n- **Efficient Indexing:** Index vector embeddings using Qdrant VectorDB for optimized query performance.\r\n- **Scalability:** Support for large-scale data applications ensuring performance and reliability.\r\n\r\n### Alternatives Considered\r\n\r\n1. Using standalone solutions for data ingestion and vector search without integration.\r\n2. Employing other vector databases that might not provide the same level of performance or integration capabilities.\r\n3. Custom-built solutions for managing vector data, which require significant development effort.\r\n\r\n### Additional Context\r\n\r\nThis integration has been well tested with the following code, ensuring reliable performance and accuracy:\r\n```python\r\nimport os\r\nimport logging\r\nfrom transformers import AutoTokenizer\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom griptape.drivers import QdrantVectorStoreDriver, HuggingFaceHubEmbeddingDriver\r\nfrom griptape.tokenizers import HuggingFaceTokenizer\r\nfrom griptape.loaders import PdfLoader\r\nfrom griptape.chunkers import TextChunker\r\n\r\n# Configure logging\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n# Setting the models\r\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\r\nqdrant_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\r\nfile_name = \"test_file.pdf\"\r\nHUGGINGFACE_TOKEN = os.environ[\"HUGGINGFACE_HUB_ACCESS_TOKEN\"]\r\n\r\n# Using HuggingFaceTokenizer\r\ndef create_tokenizer(embedding_model):\r\n    tokenizer = HuggingFaceTokenizer(max_output_tokens=1024, tokenizer=AutoTokenizer.from_pretrained(embedding_model))\r\n    return tokenizer\r\n\r\ndef create_embedding_driver(embedding_model, tokenizer):\r\n    embedding_driver = HuggingFaceHubEmbeddingDriver(\r\n        api_token=HUGGINGFACE_TOKEN, model=embedding_model, tokenizer=tokenizer\r\n    )\r\n    return embedding_driver\r\n\r\n# Instantiating QdrantVectorStoreDriver\r\ndef create_vector_store_driver(url, collection_name, embedding_driver):\r\n    vector_store_driver = QdrantVectorStoreDriver(\r\n        url=url,\r\n        collection_name=collection_name,\r\n        content_payload_key=\"content\",\r\n        force_recreate=True,\r\n        embedding_driver=embedding_driver,\r\n    )\r\n    return vector_store_driver\r\n\r\n# Opening the file\r\ndef load_pdf(file_name, tokenizer):\r\n    with open(file_name, \"rb\") as f:\r\n        loader = PdfLoader(tokenizer=tokenizer, chunker=TextChunker(tokenizer=tokenizer, max_tokens=1024)).load(\r\n            f.read()\r\n        )\r\n    return loader\r\n\r\ndef main():\r\n    tokenizer = create_tokenizer(embedding_model)\r\n    embedding_driver = create_embedding_driver(embedding_model, tokenizer)\r\n    vector_store_driver = create_vector_store_driver(\r\n        url=\"http://localhost:6333\",\r\n        collection_name=\"test_collection\",\r\n        embedding_driver=embedding_driver,\r\n    )\r\n\r\n    # Loading the data\r\n    loader = load_pdf(file_name, tokenizer=tokenizer)\r\n\r\n    # Generate metadata for each chunk (example metadata)\r\n    metadata = [{\"source\": file_name, \"page\": i + 1} for i in range(len(loader))]\r\n\r\n    try:\r\n        for i, l in enumerate(loader):\r\n            content = str(l)\r\n            meta = metadata[i] if metadata else None\r\n            vector_store_driver.upsert_vector(embedding_driver.try_embed_chunk(content), meta=meta, content=content)\r\n        logging.info(\"Successfully upserted vectors with metadata.\")\r\n    except Exception as e:\r\n        logging.error(f\"Error during upsert_vector: {e}\")\r\n\r\n    query_string = \"Who created linux?\"\r\n\r\n    query_results = vector_store_driver.query(query_string, count=6, include_vectors=True)\r\n    # print(f\"Query results: {query_results}\")\r\n    for result in query_results:\r\n        print(f\"ID: {result.id}, Score: {result.score}, Vector: {result.vector}, Metadata: {result.meta}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n### Output\r\n- **Script Output:**\r\n```shell\r\nToken is valid (permission: write).\r\nYour token has been saved in your configured git credential helpers (osxkeychain,store).\r\nYour token has been saved to /Users/hkhajgiwale/.cache/huggingface/token\r\nLogin successful\r\n\r\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 350/350 [00:00<00:00, 2.20MB/s]\r\nvocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00<00:00, 532kB/s]\r\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 466k/466k [00:00<00:00, 714kB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:00<00:00, 783kB/s]\r\nWARNING:pypdf._reader:Xref table not zero-indexed. ID numbers for objects will be corrected.\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (566771 > 512). Running this sequence through the model will result in indexing errors\r\n\r\nWARNING:pypdf._reader:Xref table not zero-indexed. ID numbers for objects will be corrected.\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (566771 > 512). Running this sequence through the model will result in indexing errors\r\nInserted a total of 789 vectors into the Qdrant collection 'linux_bible'.\r\n\r\n```\r\n- **Query Result:**\r\n```shell\r\nID: 27c79364-047c-4f1a-8323-3da840df95f2, Score: 0.6420436, Vector: [-0.033319965, -0.033909258, -0.014088887, -0.026663534, 0.06426538, -0.08638656, -0.036058135, 0.13097586, 0.06465988, -0.025472835, 0.041268587, 0.01978924, -0.008526307, -0.094854966, 0.047591303, -0.09618401, -0.010990408, -0.027464865, 0.03230282, -0.072785355, -0.039835285, 0.044199828, -0.018777357, -0.010172242, 0.021567283, 0.04779191, 0.030713014, -0.014952045, 0.0342129, -0.021135217, -0.022233197, 0.11858495, 0.026467795, 0.008923172, -0.16770366, 0.061032046, 0.012555608, -0.04789152, -0.08986295, -0.08685627, -0.092642754, -0.056637816, -0.02444664, 0.0148750795, 0.022018181, -0.03189946, 0.00069411576, -0.05575728, 0.007895758, -0.059999734, -0.033308554, -0.040583532, 0.074766226, 0.028454041, 0.02604916, -0.064518526, -0.07666384, -0.04242904, 0.0011933906, 0.034024354, -0.03685598, -0.023715403, -0.06955432, 0.05775433, 0.031021109, -0.052915003, -0.0032551256, -0.036131177, 0.024844615, -0.13610701, -0.040748127, -0.038092654, 0.068615645, 0.10100414, -0.07935713, -0.007989107, -0.055178087, 0.084142484, 0.008011023, -0.0023816512, 0.06601421, 0.14551388, -0.058035214, 0.031445645, -0.048077744, 0.08887344, 0.013803761, 0.085468456, -0.013052811, -0.0697258, 0.048760545, -0.017685851, 0.015883174, -0.037387706, -0.041260507, -0.07488407, 0.089132234, 0.0061633103, 0.031875677, 0.06863733, 0.019712245, 0.0038533423, 0.013222222, 0.015128401, -0.0016742714, -0.026235057, 0.010969684, 0.045371763, 0.026369834, -0.001685722, -0.06804295, -0.04602395, -0.072482534, -0.02093926, 0.007999741, -0.08011011, -0.0267737, 0.020341981, 0.051036708, 0.0038343293, -0.00063315715, 0.026499316, -0.0860942, -0.021107929, 0.06657996, 0.054002028, -0.034046065, 4.5936933e-34, 0.06063078, 0.0041874545, -0.03279631, 0.04840063, 0.012547468, -0.0036221752, -0.007934127, 0.008459876, 0.0031696947, 0.025607022, 0.03910192, 0.040383086, -0.0033453074, 0.039749652, 0.11728535, -0.020587133, -0.059816215, -0.025107108, 0.060377054, 0.012229162, -0.018001657, 0.045607377, 0.031661518, 0.061065547, 0.071817026, 0.012916247, -0.062444888, -0.09693645, -0.016995782, -0.0007742, -0.025836606, 0.026911916, -0.028476752, 0.013248208, -0.0032107763, -0.044497356, -0.008737978, -0.118968375, 0.015046552, -0.04112476, -0.026331374, 0.0098819155, -0.02784592, -0.03217574, 0.08088148, -0.009977335, -0.0031248387, -0.0017853056, 0.02869835, -0.07050618, -0.039792247, 0.0085486155, 0.12939373, 0.014063032, 0.036617287, 0.035789784, -0.036874287, 0.020059178, 0.015771665, 0.16028489, -0.07761481, 0.06122184, -0.021478254, 0.021385562, -0.034210004, 0.036777332, -0.0012745084, 0.01997811, -0.027142646, 0.09602916, -0.06377781, 0.006446841, -0.024087435, 0.07697739, -0.038981833, 0.0531924, -0.014489998, -0.084309846, -0.020455772, 0.039448157, -0.030567326, 0.025330642, -0.019592656, 0.012218679, 0.053152625, 0.055347815, -0.0705193, -0.0059185554, -0.026899993, -0.085054144, 0.058190323, -0.036551163, -0.048517633, 0.07384973, 0.006800745, -4.2462456e-33, 0.020581827, 0.035085097, -0.05775891, -0.018412707, 0.0067332885, 0.05631842, -0.07016557, -0.03622008, -0.057021916, -0.08257919, -0.022888742, 0.01997705, 0.07339474, 0.043937683, 0.0332707, -0.0074235, -0.048726168, -0.053748786, -0.067911476, -0.0339103, -0.055709537, 0.061154872, -0.024023516, -0.08668337, 0.042099033, 0.02462188, -0.0096963635, 0.022291938, -0.04741452, 0.02477046, 0.09467683, -0.05747949, -0.0057552066, -0.0077939606, 0.1068827, 0.012320172, 0.0013870214, 0.028096963, -0.008436847, -0.03088452, 0.03593164, -0.014096904, 0.05320094, -0.0195332, -0.04015384, -0.03202634, -0.09061283, 0.027282033, -0.01103524, -0.10506805, 0.03868709, -0.013999539, 0.002591833, -0.02782352, 0.016672468, -0.10313568, -0.038809996, 0.020939296, 0.0106487945, 0.04167872, 0.012849844, -0.04614982, 0.018726261, 0.08172715, -0.079679236, -0.047526445, -0.059034318, 0.028333595, -0.025118697, -0.018119022, 0.027818607, 0.020889938, -0.027688514, -0.0064164065, 0.021720523, 0.05787196, 0.02540303, -0.051163625, -0.07985068, 0.036325943, 0.03132891, 0.03267965, 0.026869241, 0.04623621, -0.051324327, 0.04002144, 0.007920646, -0.026736474, 0.020703364, -0.02433869, 0.041062724, 0.022844095, -0.044597227, 0.14091592, -0.015348343, -5.287611e-08, 0.06937336, 0.0015003928, 0.0063034287, -0.0015761731, 0.0641838, -0.04084883, 0.069532774, -0.019854434, -0.057561822, 0.04487838, -0.04808787, -0.020828236, -0.039036937, -0.08314201, 0.1117453, 0.08481166, -0.05059345, 0.09510657, -0.03328913, -0.016192343, 0.08916456, -0.13047144, -0.020908123, 0.049791463, -0.01720267, -0.06338032, 0.050439823, 0.012224918, 0.06679014, 0.052619737, -0.028156029, -0.014997442, -0.02212743, 0.036619224, 0.102874845, -0.030618286, -0.030848863, -0.044709735, -0.002172509, -0.039022136, -0.049619682, -0.007745338, 0.0043051774, -0.012630724, -0.074661165, 0.10782143, -0.08718763, -0.01271671, 0.005002096, -0.0023495308, 0.008480507, 0.06494349, 0.021023551, 0.08271026, 0.009580766, -0.029174384, -0.028336622, -0.034248553, 0.023291852, -0.018573066, -0.041182272, -0.01851133, 0.108130485, 0.034235995], Metadata: {'content': 'ated the Linux kernel as a graduate student in Finland) is what gives Linux its name. The kernel is the software that starts up when you boot your computer and interfaces with the programs you use so they can communicate effectively and simply with your computer hardware.\\nNote\\nSee Appendix B for historic details on how the kernel and other free software came together to create the \\nLinux phenomenon. /square6\\nComponents such as administrative commands and applications from other free and open source \\nsoftware projects work with the kernel to make Linux a complete operating system. The GNU proj-ect (\\nwww.gnu.org ), in particular, contributed many implementations of standard UNIX compo-\\nnents that are now in Linux. Apache, KDE, GNOME, and other major open source projects in Linux have also contributed to the success of Linux. (See Chapter 2 for an explanation of how open source projects work and how you can get involved in them.) Those other projects added such things as\\nGraphical user interfaces (GUIs)\\n/circle6 \u2014Consisting of a graphical framework (typically the \\nX Window System), window managers, panels, icons, and menus. GUIs enable you to use Linux with a keyboard and mouse combination, instead of just typing commands (as was done in the old days).\\nAdministrative utilities\\n/circle6 \u2014Including hundreds (perhaps thousands) of commands and \\ngraphical windows to do such things as add users, manage disks, monitor the network, \\ninstall software, and generally secure and manage your computer.\\nApplications/circle6 \u2014Although no Linux distribution includes all of them, literally thousands \\nof games, of\ufb01  ce productivity tools, Web browsers, chat windows, multimedia players, \\nand other applications are available for Linux.Programming tools\\n/circle6 \u2014Including programming utilities for creating applications and \\nlibraries for implementing specialty interfaces.Server features\\n/circle6 \u2014Enabling you to offer services from your Linux computer to another \\ncomputer on the network. In other words, while Linux includes Web browsers to view \\nWeb pages, it can also be the computer that serves up Web pages to others. Popular server features include Web, mail, database, printer, \ufb01  le, DNS, and DHCP servers.\\nAfter Linus Torvalds and friends had a working Linux kernel, pulling together a complete open source operating system was possible because so much of the available \u201cfree\u201d software was\\nCovered by the GNU Public License (GPL) or similar license\\n/circle6 \u2014That allowed the \\nentire operating system to be freely distributed, provided guidelines were followed relating to how the source code for that software was made available going forward (see \\nhttp://www.gnu.org/licenses/gpl.html ).\\nBased on UNIX-like systems/circle6 \u2014Clones of virtually all the other user-level components \\nof a UNIX system had been created. Those and other utilities and applications were built to run on UNIX or other UNIX-like systems.\\n85057c01.indd   1285057c01.indd   12 10/15/09   2:22:54 PM10/15/09   2:22:54 PM\\n13Chapter 1: Starting with Linux\\nLinux has become one of the most popular culminations of the open source software movement. \\nBut the traditions of sharing code and building communities that made Linux possible started years before Linux was born. You could argue that it began in a comfortable think tank known as Bell Laboratories. Read Appendix B to learn more about the history of Linux.\\nLeveraging work done on UNIX and GNU projects helped to get Linux up and running quickly. \\nThe culture of sharing in the open source community and adoption of a wide array of tools for communicating on the Internet have helped Linux to move quickly through infancy and adoles-cence to become a mature operating system.\\nThe simple commitment to share code is probably the single most powerful contributor to the \\ngrowth of the open source software movement in general, and Linux in particular. That com-mitment has also encouraged involvement from the kind of people who are willing to contribute back to that community in all kinds of ways. The willingness of Linus Torvalds to incorporate code from others in the Linux kernel has also been critical to the success of Linux.', 'page': 84, 'source': 'linux_bible.pdf'}\r\nID: faf99e96-5c1a-47a0-9e5e-cedd04146bbe, Score: 0.5350301, Vector: [0.024331223, -0.07055378, -0.04639513, -0.06745808, 0.11607092, -0.06295789, -0.050465923, 0.13786222, 0.030159013, -0.004015608, -0.010676871, 0.043769825, 0.002562885, -0.10791365, 0.08188917, -0.06733823, 0.015235985, -0.04399143, -0.029254304, -0.07622137, -0.051291894, -0.042619515, -0.035561237, -0.03216422, -0.015353801, 0.007827441, 0.003842683, -0.024111375, 0.011969527, -0.073085405, -0.004015697, 0.05118515, 0.04462064, 0.016457165, -0.044123165, 0.029161211, 0.032725297, -0.09948047, -0.11717596, -0.07006103, -0.07375753, -0.06258994, -0.04566733, 0.07318526, -0.018762583, -0.028633866, 0.00589228, -0.025655907, 0.028555587, -0.05916732, 0.0030022084, -0.028934382, 0.045359496, 0.008084487, -0.0059830844, -0.06705739, -0.039903928, -0.05109666, 0.017328693, 0.019561894, 0.03865932, -0.031135602, -0.094755515, 0.0696307, 0.0079221, -0.0009801845, 0.04914509, -0.044889834, -0.014624922, -0.06107484, -0.08638306, 0.004438856, 0.07317532, 0.059987534, -0.084445775, -0.019608138, 0.03666604, 0.052662563, 0.014499273, 0.026804266, 0.064353116, 0.121469095, -0.095886566, 0.0138858305, -0.061298773, 0.042142395, -0.04137195, 0.016541636, 0.027204953, -0.03736728, 0.024546837, 0.04318625, 0.045745023, 0.018195828, 0.012363759, 0.009674581, 0.07117867, -0.05164778, -0.042451415, 0.10459722, -0.051867876, 0.039014667, 0.053602595, -0.013444189, -0.059383016, 0.026365539, 0.03386706, 0.052461583, -0.05169558, -0.016761262, -0.045771126, -0.05824901, -0.062215872, -0.043441582, -0.0011185831, -0.015508645, -0.07924711, 0.0067165955, 0.027270535, 0.0128110545, 0.055487428, 0.0096251005, -0.052989874, -0.035143983, 0.0745588, 0.008490463, -0.0061789937, 5.7813365e-33, -0.0077322903, 0.05089334, -0.06208537, -0.012271753, 0.037881814, -0.032028474, 0.031108173, 0.03932017, -0.113128036, 0.065454625, 0.04473485, 0.048975635, -0.0397826, -0.041689493, 0.15407364, -0.0093920585, -0.048749406, 0.01576997, 0.039169155, 0.028227244, 0.04160524, -0.009283758, -0.0039213984, 0.014405117, 0.071765065, -0.04113585, -0.06290899, -0.09136639, 0.06928661, 0.020446315, -0.05478173, 0.015546732, -0.06627345, 0.025668463, -0.02507427, -0.054088365, -0.09528659, -0.07856511, 0.02387049, 0.013679139, -0.075428315, 0.030132407, -0.0421513, -0.061015166, 0.039319675, 0.0005365387, 0.010995959, 0.0019172272, -0.0077442573, -0.0067242687, -0.032987166, 0.063831784, 0.07289842, 0.06597161, 0.021873735, -0.015787529, 0.008175407, -0.0051377737, 0.035549764, 0.13740769, -0.07484701, 0.01975612, 0.00097205286, -0.016821997, -0.02086878, -0.043551154, 0.029973619, 0.051265553, -0.009231957, 0.091875225, -0.040937915, -0.020471284, -0.013983976, 0.04625708, -0.011935796, 0.036329255, -0.02307174, -0.050468333, 0.034062546, -0.024977278, -0.053438656, 0.09805345, -0.04715404, 0.06499154, 0.0026042063, 0.048584484, -0.05834566, -0.017755663, -0.054628246, 0.04123106, 0.008707029, 0.0045137308, 0.015643694, 0.07451622, 0.0013959655, -5.6718363e-33, -0.032046035, -0.043305457, -0.0010658664, 0.0074834055, 0.033055622, 0.054477125, -0.08341126, 0.019022267, -0.03950564, 0.022333609, -0.07156558, 0.023374123, 0.03629009, -0.0011669495, 0.016282769, -0.073552124, 0.030347448, -0.055158608, -0.025584424, 0.00715488, -0.058990058, 0.15312871, 0.009879565, -0.097141646, 0.01882934, 0.069251604, -0.08145449, 0.0343667, -0.050875288, 0.019485632, 0.05586577, -0.015215959, -0.016180554, -0.03318689, 0.09547338, 0.023699587, 0.03604217, 0.048568293, -0.012289297, -0.0016615205, 0.08206157, -0.017569851, -0.06479148, -0.052218817, -0.008343782, 0.0065487307, -0.06361596, -0.015336794, 0.010537647, -0.075127974, 0.058192264, 0.011879299, -0.008780829, -0.040178724, -0.0033989523, -0.06746455, -0.044544596, 0.057894103, 0.031869363, 0.030301424, 0.06432371, -0.04518761, -0.047280554, 0.10941228, -0.0032685648, -0.051799525, 0.029878203, 0.086914174, -0.023249878, 0.054577317, -0.048716806, -0.022192033, 0.010497893, 0.007632791, 0.0058831843, 0.07827393, 0.03452274, -0.061196122, -0.061946824, 0.05277168, -0.0036381485, 0.01532919, 0.005320174, 0.056102376, -0.027673684, -0.026463313, 0.045050167, -0.060121257, 0.008630904, -0.05514328, -0.028198322, 0.06288215, -0.06112665, 0.12975036, 0.044362936, -5.1572744e-08, 0.06916056, -0.055261664, -0.0121382205, 0.05772162, 0.013674339, -0.009856258, 0.060743503, -0.0068293344, -0.042229537, 0.12912934, 0.022893893, -0.06768744, -0.034169108, -0.019764515, 0.021298576, 0.15513909, -0.047815453, 0.08062074, -0.06744864, -0.010472879, 0.093221396, 0.0092249, 0.033652484, 0.004951226, -0.0108227655, -0.08298904, 0.07829765, -0.02918012, 0.040861372, 0.07502183, -0.013328497, -0.007117863, -0.0010515214, 0.016050711, 0.07137554, -0.05032972, -0.055674076, 0.05260627, 0.06055885, 0.010967028, -0.044464994, 0.08605771, -0.008431955, -0.007856606, -0.08918092, -0.03736765, -0.10412424, -0.006117804, -0.0065842066, -0.035468876, -0.008603244, 0.017469272, -0.022496583, 0.070861705, 0.059056558, -0.026201164, -0.015157915, 0.02911126, 0.04120862, 0.038511287, -0.037579004, -0.0666424, 0.042541374, 0.0468446], Metadata: {'content': 'www.linuxdevices.com ) to game consoles (such as PlayStation 3) to \\nsupercomputers to Mars rovers. In short, Linux has become a system that almost anyone can run almost anywhere.\\nOn both desktop and server computers Linux has become a formidable \\noperating system across a variety of business applications. Today, large enterprises can deploy thousands of systems using Linux distributions from companies such as Red Hat, Inc. and Novell, Inc. Small businesses can put together the mixture of of\ufb01  ce and Internet services they need to keep their \\ncosts down.\\nThe free and open source software (FOSS) development model that \\nespoused sharing, freedom, and openness is now on a trajectory to sur-pass the quality of other operating systems outside of the traditional Linux servers and technical workstations. What were once weak components of Linux, such as easy-to-use desktops, multimedia codecs and limited driver availability, have improved at a rapid pace. In areas of security, usability, connectivity, and network services, Linux has continued to improve and outshine the competition.Starting with Linux\\nIN THIS CHAPTER\\nGetting started with Linux\\nUnderstanding LinuxLinux features and advantages\\n85057c01.indd   385057c01.indd   3 10/15/09   2:22:53 PM10/15/09   2:22:53 PM\\n4Part I: Getting off the Ground with Linux\\nComputer industry heavy-hitters such as Microsoft and Oracle have taken notice of Linux. \\nMicrosoft has struck agreements with Linux companies including Novell and Xandros to form partnerships that primarily protect those companies against threatened Microsoft lawsuits. Oracle began producing its own enterprise-targeted Linux system to try to stem the \ufb02  ow of cus-\\ntomers to Red Hat Enterprise Linux.\\nWhat does this all add up to? A growing swirl of excitement around the operating system that the \\nbig guys can\u2019t seem to get rid of. For people like yourself, who want the freedom to use your com-puter software as you like, it means great prospects for the future.\\nLet this book help you grab your \ufb01  rst look at the distributions, applications, services, and com-\\nmunity that make up the phenomenon that has become Linux.\\nTaking Your First Step\\nIn your hands, you have 18 different Linux distributions (on CD and DVD), thousands of applica-\\ntions, and descriptions for getting it all running on your own computer. For you right now, the worldwide Linux phenomenon is just a reboot away.\\nLinux Bible 2010 Edition brings you into the world of free and open source software that, through \\nsome strange twists and turns, has fallen most publicly under the \u201cLinux\u201d banner. Through descriptions and procedures, this book helps you\\nUnderstand what people do with Linux and how you can use Linux for all your comput-', 'page': 76, 'source': 'linux_bible.pdf'}\r\nID: 559b54c1-b769-4c3c-b2cd-f832a93fb99f, Score: 0.52532214, Vector: [0.00746728, -0.11844351, -0.007997972, 0.011819327, 0.16083239, -0.027890231, -0.030016694, 0.16784942, 0.009562944, 0.008335069, -0.013510524, 0.011549256, -0.002929275, -0.11596569, 0.076392695, -0.05113839, -0.09048634, -0.054655273, 0.030377481, -0.093296915, -0.003533422, 0.01996293, 0.03350387, -0.027454238, 0.030402206, 0.028024137, 0.027541379, -0.07663866, 0.034651108, -0.044327524, 0.0043478194, 0.1217768, 0.015323285, -0.011632312, -0.06851035, 0.025471015, 0.07107715, -0.06755785, -0.054285582, -0.020122241, -0.1076624, -0.043894358, -0.039558616, 0.012672985, 0.014954614, -0.041535545, -0.0032710384, -0.042225473, 0.013539396, -0.052253213, -0.03387083, -0.05464647, 0.0065770554, 0.019909745, 0.012807563, -0.04035854, -0.01783124, -0.09215303, -0.015678443, -0.07305397, 0.04806908, -0.051449973, -0.10381545, 0.069721416, -0.04161106, -0.014806088, -0.007637158, 0.077433415, 0.055192057, -0.090190195, -0.07149286, -0.03136405, 0.058312405, 0.08901262, -0.0028556655, -0.02804934, -0.006143313, 0.05121465, 0.019295719, 0.055265322, 0.05419187, 0.069835, -0.052713167, 0.05128869, -0.08664806, 0.059907466, 0.022448724, 0.038762752, 0.0010765686, -0.049246322, 0.039219253, 0.050062515, 0.013636117, -0.015030285, 0.019150577, -0.034392092, 0.07742463, -0.0508219, -0.0017189122, 0.09155765, -0.04176071, -0.008775968, -0.009357665, -0.026293984, -0.07783781, 0.015077545, 0.02026658, 0.025035856, 0.0037758523, -0.013756253, -0.017237144, -0.018613083, -0.028278133, -0.044136588, 0.021040194, -0.028701229, -0.050071683, -2.1271697e-05, -0.0068169152, 0.050660476, -0.020379832, 0.06683549, -0.04830146, -0.0068403357, 0.06609714, 0.03009027, -0.040599417, 1.713397e-33, 0.08282234, 0.085729845, -0.049530335, -0.0024655517, 0.02111573, -0.05216762, -0.0014215877, 0.03463404, -0.0848725, 0.08374194, 0.10646674, 0.101235144, -0.067771494, -0.017541744, 0.08444848, -0.07616206, -0.058477767, -0.022663856, 0.03657865, 0.0052248565, -0.009666332, 0.019909766, -0.00357673, 0.042327613, 0.013103364, -0.07006073, -0.05037492, -0.08474202, 0.045402613, -0.00415185, -0.08691769, 0.053092122, -0.052869864, -0.04253742, -0.041239712, -0.044962313, -0.07403936, -0.08684757, 0.042667765, -0.016743267, -0.06625975, 0.03955627, 0.011500935, -0.05137156, 0.062236868, 0.008678709, 0.071807645, 0.01332741, 0.054589562, -0.0337019, -0.07072625, -0.033138905, 0.016805813, 0.07812944, -0.03546032, 0.029750424, -0.020088248, -0.03131388, 0.02462189, 0.10450523, -0.019727007, 0.010109403, 0.027690204, -0.047882847, -0.06731156, 0.007277827, 0.044157535, 0.036198664, 0.012190073, 0.07966004, -0.058520474, -0.04031732, -0.02942285, 0.052512333, -0.041164894, 0.05458101, -0.01795128, -0.04443241, 0.01644369, -0.00730637, -0.054506686, 0.06160984, -0.02327273, 0.032622233, 0.05859294, 0.04919139, -0.0384079, -0.072201855, -0.063880034, 0.032740485, 0.08284124, -0.034969304, -0.043666027, 0.054361627, 0.0041896417, -3.5277522e-33, 0.020163145, -0.059797283, 0.0012108752, -0.0014657109, 0.04875801, 0.060138997, -0.09107212, 0.038635015, 0.013244924, -0.017910503, -0.0833695, 0.011083091, 0.04996642, 0.027823742, 0.0013634168, -0.094470024, 0.021212595, -0.015475093, -0.07432784, 0.00825962, -0.04147317, 0.08459714, -0.020067075, -0.11088987, 0.07192243, 0.045834888, -0.021038346, 0.027490195, -0.046723317, 0.08015263, 0.09808546, -0.029732263, -0.022613453, -0.0022434823, 0.052874245, 0.0043748957, 0.049254607, 0.025183193, 0.0063277683, -0.013529587, 0.06683558, 0.016061414, -0.0039460785, -0.080721654, -0.018246287, -0.024714625, -0.09032139, 0.019064434, 0.008227884, -0.104419395, 0.028195327, -0.012441221, 0.06800411, -0.06576471, 0.015530077, -0.04920336, -0.039293244, 0.043917872, 0.08952696, 0.010601837, -0.018011035, -0.0034346264, -0.046036497, 0.0957672, -0.042801708, -0.10759926, 0.009244587, 0.01889225, -0.013532307, -0.005812693, -0.054307435, 0.033654302, -0.023743318, -0.030277519, -0.0240804, 0.06364629, 0.00070645014, -0.025784686, -0.09093685, -0.010048218, 0.03863883, 0.037403032, 0.0033992024, 0.014415501, -0.047117464, 0.04075601, 0.08425122, -0.041142225, -0.007136692, -0.02124992, -0.025833722, 0.072112374, -0.0085339425, 0.099372335, 0.03907196, -5.1821072e-08, 0.043728434, -0.050226863, -0.019331686, 0.08265944, 0.0062530763, 0.00013409385, 0.032990552, -0.00386631, -0.029465502, 0.10428476, -0.022173971, -0.03215279, -0.018030467, 0.0009613294, 0.052704975, 0.09792233, -0.029748386, 0.059049904, -0.058105685, -0.032421697, 0.13109277, -0.02589744, 0.06618135, 0.021598492, 0.009302079, -0.061946012, 0.07017781, -0.0001900025, 0.053011436, 0.03306959, -0.0047116936, 0.019234378, -0.05053415, -0.017033761, 0.08826063, -0.0536139, -0.027290963, 0.03367373, 0.049046386, 0.026319882, -0.074161656, 0.0363738, 0.014411307, -0.019714253, -0.100796, 0.01724432, -0.06411304, 0.020741925, 0.03345999, -0.006161178, 0.04991325, -0.013879994, 0.03182722, 0.02034885, 0.03105859, 0.041133456, -0.06589748, 0.08011253, 0.04666577, -0.0033899012, -0.04469346, -0.079689756, 0.011030314, 0.042117614], Metadata: {'content': 'In the \ufb01  rst two chapters, you\u2019ll learn a lot of exciting ways Linux is being used today and see who \\nmany of the major players are in the free and open source software (FOSS) world. You will see \\nhow people are adapting Linux to run on handhelds, mini laptops, 32- and 64-bit PCs, Macs, mainframes, and super computers. Linux is truly everywhere!\\nHowever, if you are concerned that somehow \u201cfree\u201d software is too good to be true, skip ahead for \\nthe moment to the brief history of Linux in Appendix B. That appendix guides you through the strange and circuitous path of free and open source software development that led to the Linux phenomenon.\\nIf you are intrigued by what you learn here, I\u2019ll tell you how you can become part of the open \\nsource and free software communities, whose stars are known by a single name (such as Linus) or a few initials (such as rms). You\u2019ll \ufb01  nd a staggering number of open source projects, forums, \\nand mailing lists that are thriving today (and always looking for more people to get involved). \\nHow This Book Is Organized\\nLearn the basics of what goes into Linux and you will be able to use all sorts of devices and computers in the future. The book is organized in a way that enables you to start off at the very beginning with Linux, but still grow to the point where you can get going with some powerful server and programming features, if you care to.\\nPart I includes two short chapters designed to open your eyes to what you can do with Linux, \\nthen get your hands on it quickly. Those two chapters describe\\nHow others use Linux, how to transition to Linux from Windows, and how to start with \\n/circle6\\nLinux using the CD and DVD inside this book (Chapter 1)\\nWhat you can do, what you can make, and what you can become with Linux (Chapter 2)/circle6\\n85057flast.indd   xxvi85057flast.indd   xxvi 10/15/09   2:23:00 PM10/15/09   2:23:00 PM\\nxxviiIntroduction\\nIn Part II , you start in with details on how to use Linux desktops and associated applications. \\nChapters 3\u20137 describe\\nThe KDE, GNOME, and other desktop interfaces (Chapter 3)/circle6\\nTools for playing music and video (Chapter 4)/circle6\\nDesktop publishing and Web publishing using word processing, layout, drawing, and /circle6\\nimage manipulation tools, plus tools such as wikis, blogs, and content management sys-\\ntems for managing content online (Chapter 5)\\nApplications for e-mail and Web browsing/circle6  (Chapter 6)', 'page': 73, 'source': 'linux_bible.pdf'}\r\nID: edbaf079-9d34-4de7-b38b-e79ec9a16962, Score: 0.48150444, Vector: [-0.024073696, -0.06740264, -0.04933037, -0.056596126, 0.0640839, -0.08332419, 0.0027693151, 0.13868594, 0.057942413, 0.0045598065, 0.0018336017, 0.07531288, 0.018394172, -0.07559044, 0.0490967, -0.073607825, -0.013805403, -0.053357575, 0.028111273, -0.089254394, -0.05395274, 0.016952382, -0.024828944, 0.0049645705, 0.016095687, -0.0074422187, -0.0111827785, -0.0129252635, 0.015384097, -0.02754346, 0.027828624, 0.051317822, -0.024497803, -0.017709127, -0.070052184, 0.070469335, 0.043555737, -0.04935626, -0.043799397, -0.050337467, -0.070827074, -0.024127513, -0.020474128, 0.038182694, 0.0008033708, -0.03626968, -0.050244138, -0.014230472, -0.027392898, -0.05233938, -0.05788181, -0.0087971985, 0.07703545, 0.03716889, 0.0056894175, -0.08257068, -0.006536877, -0.048928753, -0.0030512463, -0.042867243, 0.026243336, -0.035939734, -0.084637694, 0.065710865, 0.001997814, -0.0055882614, 0.006594209, 0.02063613, 0.016229676, -0.09070771, -0.09010035, -0.047365617, 0.036659993, 0.11633981, -0.039124742, -0.021918172, 0.027612792, 0.007670698, 0.01996161, 0.014145673, 0.032564282, 0.120462574, -0.002979826, 0.050316617, -0.077059455, 0.06513185, 0.06011874, 0.05446738, 0.05895109, -0.047661755, 0.09689729, 0.04157286, 0.0069837584, -0.025893275, -0.04024964, -0.00027525637, 0.08663912, -0.063367985, -0.016695589, 0.07634073, -0.0046042968, -0.07553812, 0.016132131, -0.06466097, -0.046718005, -0.012522675, 0.040737897, 0.029054755, 0.0054848413, 0.020392884, -0.12974729, -0.04055431, -0.09623701, -0.024364535, -0.020406479, -0.076892, -0.06804503, 0.040718183, 0.031146491, 0.0005500672, 0.02577542, 0.059472144, -0.037667777, -0.06337242, 0.09070736, 0.011583493, 0.024941992, 2.911553e-33, 0.015209375, 0.05031743, -0.071215, -0.043480393, 0.027895976, -0.065813914, 0.0075986134, 0.045926567, -0.078338884, 0.05116739, 0.023592697, 0.06374172, -0.026450198, 0.016269743, 0.17122796, -0.021455862, -0.03337701, -0.01743979, 0.0930162, -0.03729465, -0.04721721, 0.010541696, 0.037904505, 0.023485055, 0.03921805, -0.05274242, -0.005406335, -0.034132645, 0.015480098, -0.01049564, -0.016216632, 0.011952389, 0.0054787714, -0.012547889, 0.0142544545, -0.01064859, -0.087488934, -0.09501916, 0.027134314, 0.007356856, -0.029017627, 0.036929734, -0.0067986315, 0.024368, 0.07348975, 0.03733985, 0.028452594, -0.021815332, 0.014123607, 0.013288786, -0.077107206, -0.020698708, 0.061511688, 0.042465787, -0.07053348, 0.053588647, 0.022194672, -0.033159703, 0.02148347, 0.083170116, -0.061057176, -0.051194493, 0.03694642, -0.05161562, -0.027916215, -0.060474515, 0.05438269, -0.002920457, 0.036952972, 0.05566183, -0.060088944, -0.06387428, 0.012257671, 0.03642768, -0.023741508, -0.0056332806, 0.009154998, -0.052247345, 0.023426965, 0.019512601, -0.09632673, 0.032396507, -0.06482864, 0.040396318, 0.08001134, 0.060092065, -0.09680037, 0.03586784, -0.021762831, 0.006366735, 0.038419466, -0.04474926, -0.014793833, 0.10049441, 0.0023396981, -4.9488852e-33, -0.08328449, -0.07763805, 0.0026429582, 0.04993892, 0.04945391, 0.050035976, -0.10315616, -6.121183e-05, -0.026305752, -0.008961132, -0.059943065, -0.0110602295, -0.002902582, 0.007216774, -0.0017162564, -0.07697368, -0.014351343, -0.029406307, -0.0030815566, 0.016400246, -0.04912495, 0.09657606, -0.004924246, -0.050857008, 0.032611314, 0.016854322, -0.028242879, 0.043756545, -0.0022887944, 0.016687715, 0.11364286, -0.047498297, -0.03072822, -0.011637853, 0.03166923, -0.015000611, 0.104585, 0.012948129, -0.035104573, -0.0007573287, 0.07480522, 0.017861715, -0.04618072, -0.06523287, -0.033361565, -0.02468294, -0.049310334, -0.012103423, -0.009812822, -0.0723652, 0.0133117335, -0.052323323, 0.059941348, -0.013579973, 0.0046214564, -0.041455805, -0.027436167, 0.05619888, 0.06613538, 0.03857493, 0.036038622, -0.023564767, 0.006929671, 0.12132061, -0.06090688, 0.0015805558, 0.05244593, 0.0139290495, -0.078118846, 0.04538711, -0.030370511, 0.03300274, -0.0024404058, -0.050177578, -0.04499654, 0.041831892, -0.026301343, -0.029057309, -0.101833805, -0.010659004, 0.008464277, 0.0019811208, -0.021909859, 0.03556323, -0.07210136, -0.05506824, 0.07401727, -0.09481533, 0.0062993583, -0.06668644, -0.011408364, 0.041455653, -0.031962853, 0.13377553, 0.028018326, -5.7048968e-08, 0.07315667, -0.099995196, 0.0065096393, 0.016581548, 0.012141264, -0.01728705, 0.03933864, -0.00065708964, -0.05199344, 0.074340224, 0.0500053, -0.07451712, -0.07489221, -0.042417005, 0.06900265, 0.11055208, 0.022807684, 0.12333708, -0.05916859, 0.0011295219, 0.102453664, -0.06302064, 0.08926409, 0.0017882853, 0.06728807, -0.04813749, 0.090594076, 0.026217436, 0.029793752, 0.0327958, -0.002881321, -0.026205432, 0.027978938, 0.046610203, 0.09486055, -0.02050504, -0.08508389, -0.03376538, 0.039941408, 0.07671597, -0.03209068, 0.039046388, 0.018367276, 0.006731865, -0.04051718, 0.0066023828, -0.119661875, -0.016749056, 0.042518236, -0.034705643, 0.051088512, -0.033405125, 6.3934596e-05, 0.038810138, 0.06264515, 0.007958509, 0.021373093, 0.01930552, 0.04081616, 0.093892, -0.024699429, 0.0017280468, 0.084522046, -0.030239053], Metadata: {'content': 'con\ufb01 gure system services, and secure your computers and networks.\\nA Linux server administrator/circle6 \u2014Using some of the world\u2019s best server software, you \\ncan set up your computer to be a Web server, \ufb01  le server, mail server, or print server.\\nA software developer/circle6 \u2014You can draw on thousands of open source programming tools \\nto develop your own software applications.\\nThe Linux systems you have in your hand don\u2019t contain trialware or otherwise-hobbled software. \\nOn the contrary, they feature software created by world-class development projects, the same teams that build the software that powers many professional businesses, schools, home desktops, and Internet service providers. In other words, this truly \ufb01  rst-rate software is from develop-\\ners who have made a commitment to producing software that can be used in the ways that you choose to use it.\\nMost of the Linux distributions offered on the DVD and CD that come with this book are live \\nCDs that let you try a Linux distribution without installing. Almost all of those live CDs include features that let you install the contents of those live CDs to your hard disk. For example, you can try out Fedora, Gentoo, Ubuntu, openSUSE, PCLinuxOS, and Mandriva as live CDs, and then install those distributions permanently to your hard drive from icons on the desktops of those live CDs.\\n85057flast.indd   xxv85057flast.indd   xxv 10/15/09   2:22:59 PM10/15/09   2:22:59 PM\\nxxviIntroduction\\nUnlike some other books on Linux, this book doesn\u2019t tie you to one Linux distribution. The book \\nteaches you the essentials of Linux graphical desktop interfaces, shell commands, and basic sys-tem administration. Separate chapters break down many of the major Linux distributions avail-able today. Then descriptions of the major software projects in most Linux distributions (KDE and GNOME desktops, Apache Web servers, Samba \ufb01  le and printer sharing, and so on) guide \\nyou in setting up and using those features, regardless of which Linux you choose.\\nUnderstanding the Linux Mystique\\nThis book is designed to spark your imagination about what is possible with Linux, then give you the software and instruction to jump right into Linux. From there, the approach is to help you learn by using it.', 'page': 72, 'source': 'linux_bible.pdf'}\r\nID: c83dcac1-b65a-44cc-a261-cae57bfc95d9, Score: 0.47938925, Vector: [-0.01984242, -0.009251191, 0.018457491, -0.03781013, 0.09261274, -0.1193788, -0.07727213, 0.12620927, 0.061431658, -0.008344117, 0.048700366, 0.061782468, 0.020591466, -0.08948288, 0.044110827, -0.04765848, 0.030769775, -0.042209804, -0.009124599, -0.0050316285, -0.027673768, 0.03784497, 0.01110022, -0.028833224, -0.03238227, -0.013826247, -0.013972005, -0.023391213, 0.045812063, -0.024594031, -0.031893104, 0.100631386, 0.05064631, 0.027371014, -0.046596188, 0.016476015, -0.012597444, -0.07095902, -0.12446081, -0.0748582, -0.03965642, -0.09973392, 0.000987572, 0.055254634, -0.010403949, 0.008403102, 0.027986538, -0.08947484, 0.022214811, -0.048560645, 0.007943433, 0.024376217, 0.041964628, -0.0047879405, -0.107106864, -0.08339744, -0.08268022, 0.0054734685, 0.06822181, 0.014187082, 0.068419866, 0.0043118214, -0.06377442, 0.039710492, -0.022174219, -0.018763384, -0.028919643, -0.08097144, 0.019838884, -0.11666613, -0.001925492, -0.054298107, 0.05674856, 0.064811416, -0.1130536, 0.055029638, 0.062297072, 0.03982368, 0.009177934, 0.0065956744, 0.07105321, 0.046975676, -0.05081734, -0.019804347, 0.025514895, 0.04018937, -0.03603348, 0.09892643, -0.015842075, -0.07272974, 0.028053425, 0.022715459, -0.024910647, -0.014285808, -0.062179208, -0.019571653, 0.050337255, -0.048734803, -0.044195447, 0.061492193, -0.034729943, 0.03594042, 0.004427827, 0.046833638, -0.0811618, -0.065017976, -0.0058287294, 0.038173143, 0.01884109, -0.0010024076, -0.03490026, -0.09206027, -0.042224646, -0.023313448, -0.008650974, -0.068659246, -0.07769698, 0.024585923, 0.07928828, 0.0507452, -0.02071425, -0.007857347, -0.054172, -0.085780405, 0.09351972, 0.026023535, -0.021894667, 2.9005352e-33, 0.009481408, 0.039077844, -0.058403946, -0.03236557, -0.0436744, -0.044231296, 0.013088542, -0.0015156794, -0.007417522, 0.07432986, 0.019383166, 0.05898135, -0.048191715, 0.05201061, 0.13181533, 0.01023683, -0.03131462, -0.094403155, 0.033403378, 0.02029259, -0.005610449, -0.020971322, 0.0025865599, 0.058985524, 0.10762909, -0.02880986, -0.009944965, -0.08452161, 0.042229496, 0.013476404, -0.11522525, -0.0024188985, -0.043511253, 0.0072523504, -0.020189347, 0.008922283, -0.097898155, -0.07456626, 0.04497631, -0.013212123, -0.054286465, 0.018808616, -0.021918701, -0.037321456, 0.009258915, -0.009115823, 0.026472611, 0.0015630679, -0.008941925, -0.0698581, -0.023157999, 0.04200182, 0.059763074, 0.031845745, 0.028716076, -0.06301514, 0.0072998507, -0.04026639, 0.035564926, 0.07956249, -0.034908388, 0.025232004, -0.013495203, 0.053126812, -0.049734563, -0.013398143, 0.060294926, -0.0136634465, -0.06469721, 0.111679986, -0.013956184, -0.020290876, -0.037272546, -0.010292192, -0.015074762, 0.057141107, 0.016747905, -0.04238848, 0.015225668, 0.0037201268, -0.010678778, 0.025131961, 0.0011383804, 0.076179445, 0.0059719067, 0.010766955, -0.08781016, -0.052099165, -0.01751505, 0.03523437, -0.014993603, -0.01481304, -0.05686633, 0.09258094, -0.014618604, -3.5376932e-33, -0.06829073, -0.05625367, 0.01898682, -0.018509958, 0.0141084455, 0.033197742, -0.006314207, -0.06868979, -0.057048954, -0.018725408, -0.09629635, 0.020050153, 0.06967098, 0.014687666, 0.040810168, -0.04380362, -0.020860473, -0.05163723, 0.035567094, -0.0116340425, -0.037055183, 0.1415086, 0.02216848, -0.032433484, 0.046441674, 0.0015570261, -0.03556, 0.033511095, -0.030493163, -0.034631, 0.0648899, -0.060514897, 0.024986625, 0.013376488, 0.13324007, 0.058048494, 0.056575127, 0.0039180466, 0.0019940243, -0.049429327, 0.112915434, -0.0021007357, -0.005317771, -0.0010912868, 0.024768947, -0.04099412, -0.10867125, 0.01139904, 0.010392324, -0.043638453, 0.06804612, 0.023437329, -0.0022319213, -0.027865846, -0.031080289, -0.051383488, 0.004178208, 0.016367517, 0.0064936453, -0.005538669, 0.10007223, -0.0513617, -0.014779815, 0.00809466, -0.092387974, -0.035935618, 0.07260038, 0.04789249, -0.030158034, 0.042474665, -0.0046493723, -0.024117976, 0.0284292, 0.015506525, 0.012158536, 0.11800204, -0.0076966677, -0.084179156, -0.02120884, -0.024012763, -0.0050964435, 0.036493853, 0.08855071, 0.02996459, -0.01967626, 0.029032387, 0.0044003217, 0.013221407, -0.0519412, -0.024593018, 0.017867127, -0.022305354, -0.07073401, 0.14142574, -0.050426587, -5.4846257e-08, 0.12618408, -0.027908817, 0.041142188, -0.011754587, -0.044638153, -0.039858174, 0.10112651, -0.07076211, 0.049010936, 0.09001549, 0.055804297, -0.0926332, -0.039273467, 0.03511042, 0.058552198, 0.17512369, 0.012697251, 0.05950258, -0.028028043, 0.022259632, 0.08422044, 0.008618497, 0.008055855, 0.020150276, 0.03209303, -0.029780453, 0.022748128, -0.037858676, 0.040491004, 0.005359437, -0.03923957, -0.02287067, 0.057957288, 0.05262032, 0.051870637, -0.04273112, -0.024588205, -0.0257344, 0.0607741, -0.0013320881, -0.002911388, 0.013258696, -0.038104214, 0.0073573994, -0.09144806, -0.047250293, -0.08016776, -0.08225485, -0.0037114276, 0.07447247, 0.012219077, -0.00036026005, 0.014231857, 0.024624372, 0.06254016, -0.0037182071, 0.025127977, -0.019237185, -0.009543065, 0.013651966, -0.053267457, -0.03690494, 0.06738827, 0.046157088], Metadata: {'content': 'lowing the progress of the Mars Exploration Rover Mission, refer to the project\u2019s Web site at the NASA Jet Propulsion Laboratory (\\nhttp://marsrovers.jpl.nasa.gov ).\\nLinux in gadgets\\nLots of commercial communications, entertainment, and other kinds of gadgets have Linux run-ning on the inside. Linux makes an excellent operating system for these specialty devices, not only because of its cost, but also because of its stability and adaptability. \\nLinux enthusiasts love these devices, referred to as embedded Linux systems, because they can \\noften adapt, add, or run different Linux software on these devices (whether the manufacturer \\nintended that or not). More and more, however, manufacturers are embracing the Linux enthusi-ast and hardware hacker and selling open devices for them to use. Here are some examples:\\nMobile Phones\\n/circle6 \u2014OpenMoko ( www.openmoko.com ) produces mass-market mobile \\nphones, such as the Neo FreeRunner, that are based on Linux. Like the software, the FreeRunner\u2019s hardware also follows an open design. Although the phone is intended for general consumer use, the phone\u2019s software is currently most appropriate for people who want to develop their own software for the phones. Figure 2-2 shows an example of the Neo FreeRunner.\\n85057c02.indd   1785057c02.indd   17 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n18Part I: Getting Off the Ground with Linux\\nFIGURE 2-2\\nModify software to use the Linux Neo FreeRunner as a phone, GPS device, clock, game player, and media \\nplayer.\\nMotorola, OpenMoko, and Tranzda Technologies each offer multiple Linux-based \\nmobile phones. Phone models running Linux on the inside include the Motorola Rokr EM30 (emphasizing music playing), Tranzda Technologies NewPlus phones (with WiFi, GPS, and a camera), Purple Labs Purple Magic phones (sub-$100 phone), and Grunig B700 (with keyboard and e-mail support).\\nSony PlayStation\\n/circle6 \u2014Not only can you install and run Linux on PlayStation, but Sony \\nencourages you to do it. In 2002, Sony released Linux Kit for PlayStation 2. Included \\nin that kit is a derivative of the Japanese Kondara MNU/Linux (which is based on Red Hat Linux). For PlayStation 3, several Linux distributions have been modi\ufb01  ed (ported) \\nto run on that hardware, including Fedora, OpenSUSE, Ubuntu, Gentoo, Debian, and a commercial Yellow Dog Linux product for PlayStation 3. (See Chapter 22 for a descrip-tion of Yellow Dog Linux.) \\nPersonal Video Recorders (PVR)\\n/circle6 \u2014If you have a TiVo PVR or a set-top for streaming \\nvideo from Net\ufb02  ix, you are already running Linux in your home. The Net\ufb02  ix PVR is \\nfrom Roku, Inc. ( www.roku.com ), which produces a range of Linux-based media play-\\ners. TiVo has produced Linux-based PVRs for years. The availability of the TiVo Linux \\nsource code ( www.tivo.com/linux ) has made TiVo one of the most popular devices \\nfor Linux enthusiasts to hack.\\nNetbooks/circle6 \u2014Shrinking laptops with shrinking prices have led to Netbooks. These \\nmini laptop computers have proven to be excellent devices for running Linux. With \\nlow-powered processors and small screens, Netbooks provide a good partnership with Linux systems that are tuned for these compact, ef\ufb01  cient devices. \\n85057c02.indd   1885057c02.indd   18 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n19Chapter 2: Linux Projects, Activities, and Careers\\nThe Asus Eee PC is one of the most popular Netbooks available today. Many Asus Eee \\nPCs have been sold with Xandros Linux preinstalled. However, industrious Linux enthusiasts have created ports of Fedora, Ubuntu, and other Linux distributions to run on the Eee PC. The Acer Aspire One is another popular netbook. Figure 2-3 shows an Eee PC with Fedora Linux running on it.\\nFIGURE 2-3\\nAn Asus Eee PC can run specially tuned Fedora or other Linux systems.\\nPersonal handheld devices/circle6 \u2014A whole range of personal digital assistants (PDAs), por-', 'page': 88, 'source': 'linux_bible.pdf'}\r\nID: e94f41b3-aebb-44a7-8628-2116e344541e, Score: 0.47909445, Vector: [-0.010300992, -0.06201704, -0.046429355, -0.020711467, 0.15296702, -0.06355586, -0.064711764, 0.12849095, 0.0124012, -0.007829238, -0.015711484, 0.08028847, -0.027482616, -0.074537545, 0.06093343, -0.025529772, -0.030304754, 0.006690845, -0.0096968, -0.060203716, -0.03571217, 0.08405562, 0.038551856, -0.017983817, -0.004565045, -0.04740941, -0.03339397, -0.053387243, 0.032289147, -0.06196493, -0.047502507, 0.10740722, -0.011146251, 0.0006463854, -0.001375539, 0.041849215, 0.07455948, -0.047944102, -0.05673705, 0.019862773, 0.0066904738, -0.05481761, 0.0051639997, 0.02902823, -0.022243315, 0.011124962, -0.037202362, 0.0013854739, -0.032971192, -0.0039838725, -0.06632171, -0.044125546, 0.04338726, -0.011582755, 0.057951145, -0.12746732, -0.03482658, -0.05431481, 0.044428058, -0.0540241, 0.075629696, 0.050348457, -0.10716757, 0.055063426, -0.044008967, -0.018426618, 0.0520913, 0.030205825, -0.0060597276, -0.12652613, -0.040145557, -0.01656209, 0.0632583, 0.09220716, -0.08039693, 0.030515926, 0.04136586, 0.060651947, 0.029749708, -0.07679328, 0.04748961, 0.09376876, -0.013562662, 0.019536197, -0.06977183, 0.021640256, 0.038197275, 0.09130066, 0.027071746, -0.0029820877, 0.045834366, 0.026631517, 0.07354314, -0.058127504, 0.023106199, 0.010141544, 0.010416328, -0.063011095, 0.0074689905, 0.08638861, -0.0062448145, -0.00063023536, -0.054977197, -0.049482856, 0.020968132, 0.036079757, -0.023055127, 0.08435904, -0.0077255587, 0.03469405, -0.008138906, 0.010986437, 0.006401985, -0.07772589, 0.07231693, -0.023943482, -0.029293371, -6.1269902e-06, 0.043515585, 0.025842533, 0.009960307, 0.0696169, -0.08994871, -0.051646095, 0.019839454, -0.0019147018, -0.019577768, 2.4558647e-33, 0.051334754, 0.121646635, -0.05315276, -0.010768901, 0.04252973, -0.052605424, -0.020486953, 0.0245449, -0.07791334, 0.03434429, 0.02008067, 0.07927704, -0.04048923, -0.052928265, 0.016566465, -0.03802563, -0.055800743, 0.028523887, -0.016138408, 0.018254908, 0.022699669, -0.010526441, -0.017334113, 0.07162002, 0.0062123733, -0.010634764, -0.037513033, -0.10960199, 0.027357003, 0.0015277078, -0.050305974, 0.00623781, 0.0154764345, -0.011034626, 0.006919724, 0.014764883, -0.111299485, -0.07058621, 0.013226075, -0.0148951905, -0.05644188, -0.008299337, -0.054403916, 0.012482914, 0.10260806, 0.010018166, -0.03929273, -0.12071573, 0.019646032, -0.13366917, -0.006228709, -0.009613113, 0.07321811, 0.026708957, 0.015480196, 0.04757488, 0.0016601183, -0.078985006, 0.041134793, 0.081338294, 0.022034533, 0.08186449, 0.0029554574, -0.07316424, -0.10174299, -0.010351725, -0.041926827, 0.019396342, -0.036740974, 0.059312947, 0.0062836953, -0.03918396, -0.037556905, 0.068071246, -0.12185273, 0.052621372, -0.023773942, -0.080995604, -0.0044211005, -0.017488264, 0.0210193, 0.027639898, 0.013936339, -0.024074338, 0.015879748, 0.003967011, -0.061624024, -0.02687933, -0.034902807, 0.06535775, 0.05245469, 0.021223512, -0.03312717, 0.04180811, 0.009369041, -3.6707265e-33, -0.07170997, -0.049262974, 0.005436494, 0.02097835, -0.017825106, 0.051876474, -0.080931135, 0.023526987, 0.030140061, -0.0046952427, -0.047185495, -0.02370666, 0.02544702, -0.0033252586, 0.010368276, -0.067364074, 0.029264536, -0.06660196, -0.06475247, 0.08037294, -0.051682677, 0.02745868, -0.09859848, -0.044254925, 0.104418464, 0.050720062, 0.033178568, -0.0054844175, -0.034017157, -0.006020143, 0.04121809, -0.045161247, -0.06913875, 0.011488584, 0.061367203, 0.015433793, 0.050012846, 0.00257793, -0.01239022, -0.04589155, 0.07498995, -0.0022535587, 0.020352928, 0.017522484, -0.03924848, -0.03774075, -0.07311946, 0.032017548, -0.012868123, -0.06858843, 0.021572534, -0.02578503, 0.09557191, -0.036143307, -0.026867026, 0.0065293913, -0.04974923, 0.017149953, 0.032028034, 0.06404519, 0.013959068, 0.043303844, -0.11753003, 0.08651047, -0.019341186, -0.10872248, -0.0069740573, 0.024993675, -0.07933814, 0.0064090686, 0.032578446, 0.032940682, -0.012422266, -0.079072, 0.008427305, 0.094235614, 0.080659576, -0.040665917, -0.08650399, 0.031105854, 0.020814111, 0.06437881, 0.010438581, 0.06726853, -0.011356185, 0.00449933, 0.050619584, -0.0041684816, -0.0037916205, -0.00862209, -0.036069583, -0.033092856, -0.08996839, 0.12527077, 0.05365839, -4.848567e-08, 0.05937584, -0.06420152, -0.016957618, 0.0072025787, 0.059579283, -0.002430824, 0.041962486, -0.0506395, -0.058409754, 0.114983946, -0.012802282, -0.02817539, -0.06369224, -0.055873036, 0.021985201, 0.027643684, -0.016343528, 0.020413827, -0.024268107, -0.019330945, 0.11020154, -0.031858914, 0.039179202, 0.021734178, 0.026570005, -0.05292101, 0.075765245, -0.02957554, 0.037306145, 0.031732593, -0.0484194, 0.03519231, -0.06711979, -0.037277885, 0.090817496, 0.022939693, -0.019892158, 0.024260553, 0.024938457, -0.007078374, -0.010788959, 0.010601572, 0.05336603, -0.0007359, -0.07832101, 0.05739783, -0.05445489, -0.038015023, 0.0633694, -0.0003000535, 0.025854977, -0.068209656, 0.088410355, 0.011995063, -0.006621311, 0.040278014, -0.009107122, 0.001905905, 0.016178312, -0.0037381025, -0.035165116, -0.039213505, 0.1156438, 0.106257774], Metadata: {'content': 'Digg \\n/circle6 (http://digg.com/linux_unix )\u2014Some say that Digg.com has become more \\npopular than Slashdot for providing articles relating to Linux. You can vote on which \\narticles are most interesting to you to gain more exposure for an article.\\nGroklaw/circle6  (www.groklaw.net )\u2014The place to look for information regarding legal \\nissues surrounding open source software.Linux Today\\n/circle6  (www.linuxtoday.com )\u2014This site gathers news that is of particular \\ninterest to software developers and IT managers.LWN.net\\n/circle6  (www.lwn.net )\u2014Produces a weekly newsletter covering a range of Linux \\ntopics.\\n85057c02.indd   2385057c02.indd   23 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n24Part I: Getting Off the Ground with Linux\\nNewsforge/circle6  (www.newsforge.com )\u2014Bills itself as the \u201cOnline Newspaper for Linux \\nand Open Source.\u201d Contains many original articles, as well as links to up-to-the-minute \\nopen source stories from other locations on the Web.\\nLinuxInsider/circle6  (www.linuxinsider.com )\u2014Covers news articles related to Linux \\nissues around the world.Linux at Wikipedia \\n/circle6 (http://en.wikipedia.org/wiki/Linux )\u2014Contains an \\nexcellent write-up of what Linux is, and includes other Wikipedia links to related top-\\nics, companies, and issues. Also, provides a good understanding of Linux history and relationships.\\nLinux.com\\n/circle6  (www.linux.com )\u2014Provides Linux information, news, tips, articles, and \\nreference material.CertCities.com\\n/circle6  (http://certcities.com/certs/linux_unix/columns/) \u2014\\nRegularly publishes articles on Linux and UNIX.\\nIf you need help or have questions about Linux, here are a few sites to try:\\nLinux Questions/circle6  (www.linuxquestions.org )\u2014In addition to offering forums on \\ndifferent Linux distributions, this is a great place to ask questions related to hardware \\ncompatibility, security, and networking. The site also has some good tutorials, as well as reviews of books and Linux distributions.\\nGoogle Linux\\n/circle6  (www.google.com/linux )\u2014Search for Linux-speci\ufb01  c information from \\nthis part of the Google search site.Linux Forums\\n/circle6  (www.linuxforums.org )\u2014Contains active forums on your favorite \\ndistributions and has active IRC channels as well.The Linux Documentation Project\\n/circle6  (www.tldp.org )\u2014Offers a wide range of \\nHOWTOs, guides, FAQs, man pages, and other documentation related to Linux.Linux Help\\n/circle6  (www.linuxhelp.net )\u2014Offers forums, news, and current information \\nabout the Linux kernel. Also contains information about \ufb01  nding Linux mailing lists, \\nnewsgroups, and user groups.Linux Online\\n/circle6  (www.linux.org )\u2014Provides a central source of information related to \\nLinux distributions, documentation, books, and people.Linux Kernel Archives\\n/circle6  (www.kernel.org )\u2014The primary site for Linux kernel devel-\\nopment. You can get the latest stable or testing versions of the Linux kernel. Not the \ufb01  rst \\nplace to start with Linux, but I thought you would want to know it\u2019s there.\\nMajor Linux projects\\nAs you may know, the name Linux comes from the Linux kernel created by Linus Torvalds. The \\ndesktop, application, server, and other software needed to create a full Linux system are added \\n85057c02.indd   2485057c02.indd   24 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n25Chapter 2: Linux Projects, Activities, and Careers\\nfrom other open source projects. The following is a list of some of the major open source software \\norganizations that usually have software included with Linux:\\nFree Software Foundation/circle6  (www.fsf.org )\u2014Supports the GNU project, which pro-\\nduces much of the software outside the kernel that is associated with Linux. In particu-lar, open source versions of nearly every early UNIX command have been implemented by the GNU project. \\nApache Software Foundation', 'page': 92, 'source': 'linux_bible.pdf'}\r\n\r\n```\r\nPlease refer to attached screenshot regarding the data insertion into Qdrant\r\n<img width=\"1364\" alt=\"qdrant_data_insertion\" src=\"https://github.com/griptape-ai/griptape/assets/13365920/bf65ff96-e8d8-44a3-b876-8221f35a3dc2\">\r\n\r\n### Special Note\r\nUtilization of batches is still in progress, (the code is already present in the MR) and we are actively working on it to further enhance the efficiency of the integration.\r\n\r\n\r\n<!-- readthedocs-preview griptape start -->\r\n----\n\ud83d\udcda Documentation preview \ud83d\udcda: https://griptape--812.org.readthedocs.build//812/\n\r\n<!-- readthedocs-preview griptape end -->\nAdd support for Qdrant VectorDB\n### Description\r\n\r\nThis pull request introduces a new feature that integrates Griptape with Qdrant VectorDB, enhancing the capabilities of Griptape for handling large-scale vector data.\r\n\r\n### Problem Statement\r\n\r\nManaging and querying large-scale vector data efficiently has been a challenge. The current solutions lack seamless integration for ingesting, processing, and indexing vector embeddings, leading to inefficiencies in AI and machine learning workflows.\r\n\r\n### Solution\r\n\r\nThe integration between Griptape and Qdrant VectorDB provides a scalable and efficient solution for ingesting, processing, and indexing vector embeddings. Leveraging Griptape's powerful data pipeline capabilities and Qdrant's advanced vector search technology, this integration supports a wide range of AI and machine learning applications.\r\n\r\n### Key Features\r\n\r\n- **Seamless Ingestion:** Efficiently ingest large-scale vector data into the system.\r\n- **Advanced Processing:** Utilize Griptape's robust data processing pipelines to handle vector data.\r\n- **Efficient Indexing:** Index vector embeddings using Qdrant VectorDB for optimized query performance.\r\n- **Scalability:** Support for large-scale data applications ensuring performance and reliability.\r\n\r\n### Alternatives Considered\r\n\r\n1. Using standalone solutions for data ingestion and vector search without integration.\r\n2. Employing other vector databases that might not provide the same level of performance or integration capabilities.\r\n3. Custom-built solutions for managing vector data, which require significant development effort.\r\n\r\n### Additional Context\r\n\r\nThis integration has been well tested with the following code, ensuring reliable performance and accuracy:\r\n```python\r\nimport os\r\nimport logging\r\nfrom transformers import AutoTokenizer\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom griptape.drivers import QdrantVectorStoreDriver, HuggingFaceHubEmbeddingDriver\r\nfrom griptape.tokenizers import HuggingFaceTokenizer\r\nfrom griptape.loaders import PdfLoader\r\nfrom griptape.chunkers import TextChunker\r\n\r\n# Configure logging\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n# Setting the models\r\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\r\nqdrant_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\r\nfile_name = \"test_file.pdf\"\r\nHUGGINGFACE_TOKEN = os.environ[\"HUGGINGFACE_HUB_ACCESS_TOKEN\"]\r\n\r\n# Using HuggingFaceTokenizer\r\ndef create_tokenizer(embedding_model):\r\n    tokenizer = HuggingFaceTokenizer(max_output_tokens=1024, tokenizer=AutoTokenizer.from_pretrained(embedding_model))\r\n    return tokenizer\r\n\r\ndef create_embedding_driver(embedding_model, tokenizer):\r\n    embedding_driver = HuggingFaceHubEmbeddingDriver(\r\n        api_token=HUGGINGFACE_TOKEN, model=embedding_model, tokenizer=tokenizer\r\n    )\r\n    return embedding_driver\r\n\r\n# Instantiating QdrantVectorStoreDriver\r\ndef create_vector_store_driver(url, collection_name, embedding_driver):\r\n    vector_store_driver = QdrantVectorStoreDriver(\r\n        url=url,\r\n        collection_name=collection_name,\r\n        content_payload_key=\"content\",\r\n        force_recreate=True,\r\n        embedding_driver=embedding_driver,\r\n    )\r\n    return vector_store_driver\r\n\r\n# Opening the file\r\ndef load_pdf(file_name, tokenizer):\r\n    with open(file_name, \"rb\") as f:\r\n        loader = PdfLoader(tokenizer=tokenizer, chunker=TextChunker(tokenizer=tokenizer, max_tokens=1024)).load(\r\n            f.read()\r\n        )\r\n    return loader\r\n\r\ndef main():\r\n    tokenizer = create_tokenizer(embedding_model)\r\n    embedding_driver = create_embedding_driver(embedding_model, tokenizer)\r\n    vector_store_driver = create_vector_store_driver(\r\n        url=\"http://localhost:6333\",\r\n        collection_name=\"test_collection\",\r\n        embedding_driver=embedding_driver,\r\n    )\r\n\r\n    # Loading the data\r\n    loader = load_pdf(file_name, tokenizer=tokenizer)\r\n\r\n    # Generate metadata for each chunk (example metadata)\r\n    metadata = [{\"source\": file_name, \"page\": i + 1} for i in range(len(loader))]\r\n\r\n    try:\r\n        for i, l in enumerate(loader):\r\n            content = str(l)\r\n            meta = metadata[i] if metadata else None\r\n            vector_store_driver.upsert_vector(embedding_driver.try_embed_chunk(content), meta=meta, content=content)\r\n        logging.info(\"Successfully upserted vectors with metadata.\")\r\n    except Exception as e:\r\n        logging.error(f\"Error during upsert_vector: {e}\")\r\n\r\n    query_string = \"Who created linux?\"\r\n\r\n    query_results = vector_store_driver.query(query_string, count=6, include_vectors=True)\r\n    # print(f\"Query results: {query_results}\")\r\n    for result in query_results:\r\n        print(f\"ID: {result.id}, Score: {result.score}, Vector: {result.vector}, Metadata: {result.meta}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n### Output\r\n- **Script Output:**\r\n```shell\r\nToken is valid (permission: write).\r\nYour token has been saved in your configured git credential helpers (osxkeychain,store).\r\nYour token has been saved to /Users/hkhajgiwale/.cache/huggingface/token\r\nLogin successful\r\n\r\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 350/350 [00:00<00:00, 2.20MB/s]\r\nvocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00<00:00, 532kB/s]\r\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 466k/466k [00:00<00:00, 714kB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:00<00:00, 783kB/s]\r\nWARNING:pypdf._reader:Xref table not zero-indexed. ID numbers for objects will be corrected.\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (566771 > 512). Running this sequence through the model will result in indexing errors\r\n\r\nWARNING:pypdf._reader:Xref table not zero-indexed. ID numbers for objects will be corrected.\r\nToken indices sequence length is longer than the specified maximum sequence length for this model (566771 > 512). Running this sequence through the model will result in indexing errors\r\nInserted a total of 789 vectors into the Qdrant collection 'linux_bible'.\r\n\r\n```\r\n- **Query Result:**\r\n```shell\r\nID: 27c79364-047c-4f1a-8323-3da840df95f2, Score: 0.6420436, Vector: [-0.033319965, -0.033909258, -0.014088887, -0.026663534, 0.06426538, -0.08638656, -0.036058135, 0.13097586, 0.06465988, -0.025472835, 0.041268587, 0.01978924, -0.008526307, -0.094854966, 0.047591303, -0.09618401, -0.010990408, -0.027464865, 0.03230282, -0.072785355, -0.039835285, 0.044199828, -0.018777357, -0.010172242, 0.021567283, 0.04779191, 0.030713014, -0.014952045, 0.0342129, -0.021135217, -0.022233197, 0.11858495, 0.026467795, 0.008923172, -0.16770366, 0.061032046, 0.012555608, -0.04789152, -0.08986295, -0.08685627, -0.092642754, -0.056637816, -0.02444664, 0.0148750795, 0.022018181, -0.03189946, 0.00069411576, -0.05575728, 0.007895758, -0.059999734, -0.033308554, -0.040583532, 0.074766226, 0.028454041, 0.02604916, -0.064518526, -0.07666384, -0.04242904, 0.0011933906, 0.034024354, -0.03685598, -0.023715403, -0.06955432, 0.05775433, 0.031021109, -0.052915003, -0.0032551256, -0.036131177, 0.024844615, -0.13610701, -0.040748127, -0.038092654, 0.068615645, 0.10100414, -0.07935713, -0.007989107, -0.055178087, 0.084142484, 0.008011023, -0.0023816512, 0.06601421, 0.14551388, -0.058035214, 0.031445645, -0.048077744, 0.08887344, 0.013803761, 0.085468456, -0.013052811, -0.0697258, 0.048760545, -0.017685851, 0.015883174, -0.037387706, -0.041260507, -0.07488407, 0.089132234, 0.0061633103, 0.031875677, 0.06863733, 0.019712245, 0.0038533423, 0.013222222, 0.015128401, -0.0016742714, -0.026235057, 0.010969684, 0.045371763, 0.026369834, -0.001685722, -0.06804295, -0.04602395, -0.072482534, -0.02093926, 0.007999741, -0.08011011, -0.0267737, 0.020341981, 0.051036708, 0.0038343293, -0.00063315715, 0.026499316, -0.0860942, -0.021107929, 0.06657996, 0.054002028, -0.034046065, 4.5936933e-34, 0.06063078, 0.0041874545, -0.03279631, 0.04840063, 0.012547468, -0.0036221752, -0.007934127, 0.008459876, 0.0031696947, 0.025607022, 0.03910192, 0.040383086, -0.0033453074, 0.039749652, 0.11728535, -0.020587133, -0.059816215, -0.025107108, 0.060377054, 0.012229162, -0.018001657, 0.045607377, 0.031661518, 0.061065547, 0.071817026, 0.012916247, -0.062444888, -0.09693645, -0.016995782, -0.0007742, -0.025836606, 0.026911916, -0.028476752, 0.013248208, -0.0032107763, -0.044497356, -0.008737978, -0.118968375, 0.015046552, -0.04112476, -0.026331374, 0.0098819155, -0.02784592, -0.03217574, 0.08088148, -0.009977335, -0.0031248387, -0.0017853056, 0.02869835, -0.07050618, -0.039792247, 0.0085486155, 0.12939373, 0.014063032, 0.036617287, 0.035789784, -0.036874287, 0.020059178, 0.015771665, 0.16028489, -0.07761481, 0.06122184, -0.021478254, 0.021385562, -0.034210004, 0.036777332, -0.0012745084, 0.01997811, -0.027142646, 0.09602916, -0.06377781, 0.006446841, -0.024087435, 0.07697739, -0.038981833, 0.0531924, -0.014489998, -0.084309846, -0.020455772, 0.039448157, -0.030567326, 0.025330642, -0.019592656, 0.012218679, 0.053152625, 0.055347815, -0.0705193, -0.0059185554, -0.026899993, -0.085054144, 0.058190323, -0.036551163, -0.048517633, 0.07384973, 0.006800745, -4.2462456e-33, 0.020581827, 0.035085097, -0.05775891, -0.018412707, 0.0067332885, 0.05631842, -0.07016557, -0.03622008, -0.057021916, -0.08257919, -0.022888742, 0.01997705, 0.07339474, 0.043937683, 0.0332707, -0.0074235, -0.048726168, -0.053748786, -0.067911476, -0.0339103, -0.055709537, 0.061154872, -0.024023516, -0.08668337, 0.042099033, 0.02462188, -0.0096963635, 0.022291938, -0.04741452, 0.02477046, 0.09467683, -0.05747949, -0.0057552066, -0.0077939606, 0.1068827, 0.012320172, 0.0013870214, 0.028096963, -0.008436847, -0.03088452, 0.03593164, -0.014096904, 0.05320094, -0.0195332, -0.04015384, -0.03202634, -0.09061283, 0.027282033, -0.01103524, -0.10506805, 0.03868709, -0.013999539, 0.002591833, -0.02782352, 0.016672468, -0.10313568, -0.038809996, 0.020939296, 0.0106487945, 0.04167872, 0.012849844, -0.04614982, 0.018726261, 0.08172715, -0.079679236, -0.047526445, -0.059034318, 0.028333595, -0.025118697, -0.018119022, 0.027818607, 0.020889938, -0.027688514, -0.0064164065, 0.021720523, 0.05787196, 0.02540303, -0.051163625, -0.07985068, 0.036325943, 0.03132891, 0.03267965, 0.026869241, 0.04623621, -0.051324327, 0.04002144, 0.007920646, -0.026736474, 0.020703364, -0.02433869, 0.041062724, 0.022844095, -0.044597227, 0.14091592, -0.015348343, -5.287611e-08, 0.06937336, 0.0015003928, 0.0063034287, -0.0015761731, 0.0641838, -0.04084883, 0.069532774, -0.019854434, -0.057561822, 0.04487838, -0.04808787, -0.020828236, -0.039036937, -0.08314201, 0.1117453, 0.08481166, -0.05059345, 0.09510657, -0.03328913, -0.016192343, 0.08916456, -0.13047144, -0.020908123, 0.049791463, -0.01720267, -0.06338032, 0.050439823, 0.012224918, 0.06679014, 0.052619737, -0.028156029, -0.014997442, -0.02212743, 0.036619224, 0.102874845, -0.030618286, -0.030848863, -0.044709735, -0.002172509, -0.039022136, -0.049619682, -0.007745338, 0.0043051774, -0.012630724, -0.074661165, 0.10782143, -0.08718763, -0.01271671, 0.005002096, -0.0023495308, 0.008480507, 0.06494349, 0.021023551, 0.08271026, 0.009580766, -0.029174384, -0.028336622, -0.034248553, 0.023291852, -0.018573066, -0.041182272, -0.01851133, 0.108130485, 0.034235995], Metadata: {'content': 'ated the Linux kernel as a graduate student in Finland) is what gives Linux its name. The kernel is the software that starts up when you boot your computer and interfaces with the programs you use so they can communicate effectively and simply with your computer hardware.\\nNote\\nSee Appendix B for historic details on how the kernel and other free software came together to create the \\nLinux phenomenon. /square6\\nComponents such as administrative commands and applications from other free and open source \\nsoftware projects work with the kernel to make Linux a complete operating system. The GNU proj-ect (\\nwww.gnu.org ), in particular, contributed many implementations of standard UNIX compo-\\nnents that are now in Linux. Apache, KDE, GNOME, and other major open source projects in Linux have also contributed to the success of Linux. (See Chapter 2 for an explanation of how open source projects work and how you can get involved in them.) Those other projects added such things as\\nGraphical user interfaces (GUIs)\\n/circle6 \u2014Consisting of a graphical framework (typically the \\nX Window System), window managers, panels, icons, and menus. GUIs enable you to use Linux with a keyboard and mouse combination, instead of just typing commands (as was done in the old days).\\nAdministrative utilities\\n/circle6 \u2014Including hundreds (perhaps thousands) of commands and \\ngraphical windows to do such things as add users, manage disks, monitor the network, \\ninstall software, and generally secure and manage your computer.\\nApplications/circle6 \u2014Although no Linux distribution includes all of them, literally thousands \\nof games, of\ufb01  ce productivity tools, Web browsers, chat windows, multimedia players, \\nand other applications are available for Linux.Programming tools\\n/circle6 \u2014Including programming utilities for creating applications and \\nlibraries for implementing specialty interfaces.Server features\\n/circle6 \u2014Enabling you to offer services from your Linux computer to another \\ncomputer on the network. In other words, while Linux includes Web browsers to view \\nWeb pages, it can also be the computer that serves up Web pages to others. Popular server features include Web, mail, database, printer, \ufb01  le, DNS, and DHCP servers.\\nAfter Linus Torvalds and friends had a working Linux kernel, pulling together a complete open source operating system was possible because so much of the available \u201cfree\u201d software was\\nCovered by the GNU Public License (GPL) or similar license\\n/circle6 \u2014That allowed the \\nentire operating system to be freely distributed, provided guidelines were followed relating to how the source code for that software was made available going forward (see \\nhttp://www.gnu.org/licenses/gpl.html ).\\nBased on UNIX-like systems/circle6 \u2014Clones of virtually all the other user-level components \\nof a UNIX system had been created. Those and other utilities and applications were built to run on UNIX or other UNIX-like systems.\\n85057c01.indd   1285057c01.indd   12 10/15/09   2:22:54 PM10/15/09   2:22:54 PM\\n13Chapter 1: Starting with Linux\\nLinux has become one of the most popular culminations of the open source software movement. \\nBut the traditions of sharing code and building communities that made Linux possible started years before Linux was born. You could argue that it began in a comfortable think tank known as Bell Laboratories. Read Appendix B to learn more about the history of Linux.\\nLeveraging work done on UNIX and GNU projects helped to get Linux up and running quickly. \\nThe culture of sharing in the open source community and adoption of a wide array of tools for communicating on the Internet have helped Linux to move quickly through infancy and adoles-cence to become a mature operating system.\\nThe simple commitment to share code is probably the single most powerful contributor to the \\ngrowth of the open source software movement in general, and Linux in particular. That com-mitment has also encouraged involvement from the kind of people who are willing to contribute back to that community in all kinds of ways. The willingness of Linus Torvalds to incorporate code from others in the Linux kernel has also been critical to the success of Linux.', 'page': 84, 'source': 'linux_bible.pdf'}\r\nID: faf99e96-5c1a-47a0-9e5e-cedd04146bbe, Score: 0.5350301, Vector: [0.024331223, -0.07055378, -0.04639513, -0.06745808, 0.11607092, -0.06295789, -0.050465923, 0.13786222, 0.030159013, -0.004015608, -0.010676871, 0.043769825, 0.002562885, -0.10791365, 0.08188917, -0.06733823, 0.015235985, -0.04399143, -0.029254304, -0.07622137, -0.051291894, -0.042619515, -0.035561237, -0.03216422, -0.015353801, 0.007827441, 0.003842683, -0.024111375, 0.011969527, -0.073085405, -0.004015697, 0.05118515, 0.04462064, 0.016457165, -0.044123165, 0.029161211, 0.032725297, -0.09948047, -0.11717596, -0.07006103, -0.07375753, -0.06258994, -0.04566733, 0.07318526, -0.018762583, -0.028633866, 0.00589228, -0.025655907, 0.028555587, -0.05916732, 0.0030022084, -0.028934382, 0.045359496, 0.008084487, -0.0059830844, -0.06705739, -0.039903928, -0.05109666, 0.017328693, 0.019561894, 0.03865932, -0.031135602, -0.094755515, 0.0696307, 0.0079221, -0.0009801845, 0.04914509, -0.044889834, -0.014624922, -0.06107484, -0.08638306, 0.004438856, 0.07317532, 0.059987534, -0.084445775, -0.019608138, 0.03666604, 0.052662563, 0.014499273, 0.026804266, 0.064353116, 0.121469095, -0.095886566, 0.0138858305, -0.061298773, 0.042142395, -0.04137195, 0.016541636, 0.027204953, -0.03736728, 0.024546837, 0.04318625, 0.045745023, 0.018195828, 0.012363759, 0.009674581, 0.07117867, -0.05164778, -0.042451415, 0.10459722, -0.051867876, 0.039014667, 0.053602595, -0.013444189, -0.059383016, 0.026365539, 0.03386706, 0.052461583, -0.05169558, -0.016761262, -0.045771126, -0.05824901, -0.062215872, -0.043441582, -0.0011185831, -0.015508645, -0.07924711, 0.0067165955, 0.027270535, 0.0128110545, 0.055487428, 0.0096251005, -0.052989874, -0.035143983, 0.0745588, 0.008490463, -0.0061789937, 5.7813365e-33, -0.0077322903, 0.05089334, -0.06208537, -0.012271753, 0.037881814, -0.032028474, 0.031108173, 0.03932017, -0.113128036, 0.065454625, 0.04473485, 0.048975635, -0.0397826, -0.041689493, 0.15407364, -0.0093920585, -0.048749406, 0.01576997, 0.039169155, 0.028227244, 0.04160524, -0.009283758, -0.0039213984, 0.014405117, 0.071765065, -0.04113585, -0.06290899, -0.09136639, 0.06928661, 0.020446315, -0.05478173, 0.015546732, -0.06627345, 0.025668463, -0.02507427, -0.054088365, -0.09528659, -0.07856511, 0.02387049, 0.013679139, -0.075428315, 0.030132407, -0.0421513, -0.061015166, 0.039319675, 0.0005365387, 0.010995959, 0.0019172272, -0.0077442573, -0.0067242687, -0.032987166, 0.063831784, 0.07289842, 0.06597161, 0.021873735, -0.015787529, 0.008175407, -0.0051377737, 0.035549764, 0.13740769, -0.07484701, 0.01975612, 0.00097205286, -0.016821997, -0.02086878, -0.043551154, 0.029973619, 0.051265553, -0.009231957, 0.091875225, -0.040937915, -0.020471284, -0.013983976, 0.04625708, -0.011935796, 0.036329255, -0.02307174, -0.050468333, 0.034062546, -0.024977278, -0.053438656, 0.09805345, -0.04715404, 0.06499154, 0.0026042063, 0.048584484, -0.05834566, -0.017755663, -0.054628246, 0.04123106, 0.008707029, 0.0045137308, 0.015643694, 0.07451622, 0.0013959655, -5.6718363e-33, -0.032046035, -0.043305457, -0.0010658664, 0.0074834055, 0.033055622, 0.054477125, -0.08341126, 0.019022267, -0.03950564, 0.022333609, -0.07156558, 0.023374123, 0.03629009, -0.0011669495, 0.016282769, -0.073552124, 0.030347448, -0.055158608, -0.025584424, 0.00715488, -0.058990058, 0.15312871, 0.009879565, -0.097141646, 0.01882934, 0.069251604, -0.08145449, 0.0343667, -0.050875288, 0.019485632, 0.05586577, -0.015215959, -0.016180554, -0.03318689, 0.09547338, 0.023699587, 0.03604217, 0.048568293, -0.012289297, -0.0016615205, 0.08206157, -0.017569851, -0.06479148, -0.052218817, -0.008343782, 0.0065487307, -0.06361596, -0.015336794, 0.010537647, -0.075127974, 0.058192264, 0.011879299, -0.008780829, -0.040178724, -0.0033989523, -0.06746455, -0.044544596, 0.057894103, 0.031869363, 0.030301424, 0.06432371, -0.04518761, -0.047280554, 0.10941228, -0.0032685648, -0.051799525, 0.029878203, 0.086914174, -0.023249878, 0.054577317, -0.048716806, -0.022192033, 0.010497893, 0.007632791, 0.0058831843, 0.07827393, 0.03452274, -0.061196122, -0.061946824, 0.05277168, -0.0036381485, 0.01532919, 0.005320174, 0.056102376, -0.027673684, -0.026463313, 0.045050167, -0.060121257, 0.008630904, -0.05514328, -0.028198322, 0.06288215, -0.06112665, 0.12975036, 0.044362936, -5.1572744e-08, 0.06916056, -0.055261664, -0.0121382205, 0.05772162, 0.013674339, -0.009856258, 0.060743503, -0.0068293344, -0.042229537, 0.12912934, 0.022893893, -0.06768744, -0.034169108, -0.019764515, 0.021298576, 0.15513909, -0.047815453, 0.08062074, -0.06744864, -0.010472879, 0.093221396, 0.0092249, 0.033652484, 0.004951226, -0.0108227655, -0.08298904, 0.07829765, -0.02918012, 0.040861372, 0.07502183, -0.013328497, -0.007117863, -0.0010515214, 0.016050711, 0.07137554, -0.05032972, -0.055674076, 0.05260627, 0.06055885, 0.010967028, -0.044464994, 0.08605771, -0.008431955, -0.007856606, -0.08918092, -0.03736765, -0.10412424, -0.006117804, -0.0065842066, -0.035468876, -0.008603244, 0.017469272, -0.022496583, 0.070861705, 0.059056558, -0.026201164, -0.015157915, 0.02911126, 0.04120862, 0.038511287, -0.037579004, -0.0666424, 0.042541374, 0.0468446], Metadata: {'content': 'www.linuxdevices.com ) to game consoles (such as PlayStation 3) to \\nsupercomputers to Mars rovers. In short, Linux has become a system that almost anyone can run almost anywhere.\\nOn both desktop and server computers Linux has become a formidable \\noperating system across a variety of business applications. Today, large enterprises can deploy thousands of systems using Linux distributions from companies such as Red Hat, Inc. and Novell, Inc. Small businesses can put together the mixture of of\ufb01  ce and Internet services they need to keep their \\ncosts down.\\nThe free and open source software (FOSS) development model that \\nespoused sharing, freedom, and openness is now on a trajectory to sur-pass the quality of other operating systems outside of the traditional Linux servers and technical workstations. What were once weak components of Linux, such as easy-to-use desktops, multimedia codecs and limited driver availability, have improved at a rapid pace. In areas of security, usability, connectivity, and network services, Linux has continued to improve and outshine the competition.Starting with Linux\\nIN THIS CHAPTER\\nGetting started with Linux\\nUnderstanding LinuxLinux features and advantages\\n85057c01.indd   385057c01.indd   3 10/15/09   2:22:53 PM10/15/09   2:22:53 PM\\n4Part I: Getting off the Ground with Linux\\nComputer industry heavy-hitters such as Microsoft and Oracle have taken notice of Linux. \\nMicrosoft has struck agreements with Linux companies including Novell and Xandros to form partnerships that primarily protect those companies against threatened Microsoft lawsuits. Oracle began producing its own enterprise-targeted Linux system to try to stem the \ufb02  ow of cus-\\ntomers to Red Hat Enterprise Linux.\\nWhat does this all add up to? A growing swirl of excitement around the operating system that the \\nbig guys can\u2019t seem to get rid of. For people like yourself, who want the freedom to use your com-puter software as you like, it means great prospects for the future.\\nLet this book help you grab your \ufb01  rst look at the distributions, applications, services, and com-\\nmunity that make up the phenomenon that has become Linux.\\nTaking Your First Step\\nIn your hands, you have 18 different Linux distributions (on CD and DVD), thousands of applica-\\ntions, and descriptions for getting it all running on your own computer. For you right now, the worldwide Linux phenomenon is just a reboot away.\\nLinux Bible 2010 Edition brings you into the world of free and open source software that, through \\nsome strange twists and turns, has fallen most publicly under the \u201cLinux\u201d banner. Through descriptions and procedures, this book helps you\\nUnderstand what people do with Linux and how you can use Linux for all your comput-', 'page': 76, 'source': 'linux_bible.pdf'}\r\nID: 559b54c1-b769-4c3c-b2cd-f832a93fb99f, Score: 0.52532214, Vector: [0.00746728, -0.11844351, -0.007997972, 0.011819327, 0.16083239, -0.027890231, -0.030016694, 0.16784942, 0.009562944, 0.008335069, -0.013510524, 0.011549256, -0.002929275, -0.11596569, 0.076392695, -0.05113839, -0.09048634, -0.054655273, 0.030377481, -0.093296915, -0.003533422, 0.01996293, 0.03350387, -0.027454238, 0.030402206, 0.028024137, 0.027541379, -0.07663866, 0.034651108, -0.044327524, 0.0043478194, 0.1217768, 0.015323285, -0.011632312, -0.06851035, 0.025471015, 0.07107715, -0.06755785, -0.054285582, -0.020122241, -0.1076624, -0.043894358, -0.039558616, 0.012672985, 0.014954614, -0.041535545, -0.0032710384, -0.042225473, 0.013539396, -0.052253213, -0.03387083, -0.05464647, 0.0065770554, 0.019909745, 0.012807563, -0.04035854, -0.01783124, -0.09215303, -0.015678443, -0.07305397, 0.04806908, -0.051449973, -0.10381545, 0.069721416, -0.04161106, -0.014806088, -0.007637158, 0.077433415, 0.055192057, -0.090190195, -0.07149286, -0.03136405, 0.058312405, 0.08901262, -0.0028556655, -0.02804934, -0.006143313, 0.05121465, 0.019295719, 0.055265322, 0.05419187, 0.069835, -0.052713167, 0.05128869, -0.08664806, 0.059907466, 0.022448724, 0.038762752, 0.0010765686, -0.049246322, 0.039219253, 0.050062515, 0.013636117, -0.015030285, 0.019150577, -0.034392092, 0.07742463, -0.0508219, -0.0017189122, 0.09155765, -0.04176071, -0.008775968, -0.009357665, -0.026293984, -0.07783781, 0.015077545, 0.02026658, 0.025035856, 0.0037758523, -0.013756253, -0.017237144, -0.018613083, -0.028278133, -0.044136588, 0.021040194, -0.028701229, -0.050071683, -2.1271697e-05, -0.0068169152, 0.050660476, -0.020379832, 0.06683549, -0.04830146, -0.0068403357, 0.06609714, 0.03009027, -0.040599417, 1.713397e-33, 0.08282234, 0.085729845, -0.049530335, -0.0024655517, 0.02111573, -0.05216762, -0.0014215877, 0.03463404, -0.0848725, 0.08374194, 0.10646674, 0.101235144, -0.067771494, -0.017541744, 0.08444848, -0.07616206, -0.058477767, -0.022663856, 0.03657865, 0.0052248565, -0.009666332, 0.019909766, -0.00357673, 0.042327613, 0.013103364, -0.07006073, -0.05037492, -0.08474202, 0.045402613, -0.00415185, -0.08691769, 0.053092122, -0.052869864, -0.04253742, -0.041239712, -0.044962313, -0.07403936, -0.08684757, 0.042667765, -0.016743267, -0.06625975, 0.03955627, 0.011500935, -0.05137156, 0.062236868, 0.008678709, 0.071807645, 0.01332741, 0.054589562, -0.0337019, -0.07072625, -0.033138905, 0.016805813, 0.07812944, -0.03546032, 0.029750424, -0.020088248, -0.03131388, 0.02462189, 0.10450523, -0.019727007, 0.010109403, 0.027690204, -0.047882847, -0.06731156, 0.007277827, 0.044157535, 0.036198664, 0.012190073, 0.07966004, -0.058520474, -0.04031732, -0.02942285, 0.052512333, -0.041164894, 0.05458101, -0.01795128, -0.04443241, 0.01644369, -0.00730637, -0.054506686, 0.06160984, -0.02327273, 0.032622233, 0.05859294, 0.04919139, -0.0384079, -0.072201855, -0.063880034, 0.032740485, 0.08284124, -0.034969304, -0.043666027, 0.054361627, 0.0041896417, -3.5277522e-33, 0.020163145, -0.059797283, 0.0012108752, -0.0014657109, 0.04875801, 0.060138997, -0.09107212, 0.038635015, 0.013244924, -0.017910503, -0.0833695, 0.011083091, 0.04996642, 0.027823742, 0.0013634168, -0.094470024, 0.021212595, -0.015475093, -0.07432784, 0.00825962, -0.04147317, 0.08459714, -0.020067075, -0.11088987, 0.07192243, 0.045834888, -0.021038346, 0.027490195, -0.046723317, 0.08015263, 0.09808546, -0.029732263, -0.022613453, -0.0022434823, 0.052874245, 0.0043748957, 0.049254607, 0.025183193, 0.0063277683, -0.013529587, 0.06683558, 0.016061414, -0.0039460785, -0.080721654, -0.018246287, -0.024714625, -0.09032139, 0.019064434, 0.008227884, -0.104419395, 0.028195327, -0.012441221, 0.06800411, -0.06576471, 0.015530077, -0.04920336, -0.039293244, 0.043917872, 0.08952696, 0.010601837, -0.018011035, -0.0034346264, -0.046036497, 0.0957672, -0.042801708, -0.10759926, 0.009244587, 0.01889225, -0.013532307, -0.005812693, -0.054307435, 0.033654302, -0.023743318, -0.030277519, -0.0240804, 0.06364629, 0.00070645014, -0.025784686, -0.09093685, -0.010048218, 0.03863883, 0.037403032, 0.0033992024, 0.014415501, -0.047117464, 0.04075601, 0.08425122, -0.041142225, -0.007136692, -0.02124992, -0.025833722, 0.072112374, -0.0085339425, 0.099372335, 0.03907196, -5.1821072e-08, 0.043728434, -0.050226863, -0.019331686, 0.08265944, 0.0062530763, 0.00013409385, 0.032990552, -0.00386631, -0.029465502, 0.10428476, -0.022173971, -0.03215279, -0.018030467, 0.0009613294, 0.052704975, 0.09792233, -0.029748386, 0.059049904, -0.058105685, -0.032421697, 0.13109277, -0.02589744, 0.06618135, 0.021598492, 0.009302079, -0.061946012, 0.07017781, -0.0001900025, 0.053011436, 0.03306959, -0.0047116936, 0.019234378, -0.05053415, -0.017033761, 0.08826063, -0.0536139, -0.027290963, 0.03367373, 0.049046386, 0.026319882, -0.074161656, 0.0363738, 0.014411307, -0.019714253, -0.100796, 0.01724432, -0.06411304, 0.020741925, 0.03345999, -0.006161178, 0.04991325, -0.013879994, 0.03182722, 0.02034885, 0.03105859, 0.041133456, -0.06589748, 0.08011253, 0.04666577, -0.0033899012, -0.04469346, -0.079689756, 0.011030314, 0.042117614], Metadata: {'content': 'In the \ufb01  rst two chapters, you\u2019ll learn a lot of exciting ways Linux is being used today and see who \\nmany of the major players are in the free and open source software (FOSS) world. You will see \\nhow people are adapting Linux to run on handhelds, mini laptops, 32- and 64-bit PCs, Macs, mainframes, and super computers. Linux is truly everywhere!\\nHowever, if you are concerned that somehow \u201cfree\u201d software is too good to be true, skip ahead for \\nthe moment to the brief history of Linux in Appendix B. That appendix guides you through the strange and circuitous path of free and open source software development that led to the Linux phenomenon.\\nIf you are intrigued by what you learn here, I\u2019ll tell you how you can become part of the open \\nsource and free software communities, whose stars are known by a single name (such as Linus) or a few initials (such as rms). You\u2019ll \ufb01  nd a staggering number of open source projects, forums, \\nand mailing lists that are thriving today (and always looking for more people to get involved). \\nHow This Book Is Organized\\nLearn the basics of what goes into Linux and you will be able to use all sorts of devices and computers in the future. The book is organized in a way that enables you to start off at the very beginning with Linux, but still grow to the point where you can get going with some powerful server and programming features, if you care to.\\nPart I includes two short chapters designed to open your eyes to what you can do with Linux, \\nthen get your hands on it quickly. Those two chapters describe\\nHow others use Linux, how to transition to Linux from Windows, and how to start with \\n/circle6\\nLinux using the CD and DVD inside this book (Chapter 1)\\nWhat you can do, what you can make, and what you can become with Linux (Chapter 2)/circle6\\n85057flast.indd   xxvi85057flast.indd   xxvi 10/15/09   2:23:00 PM10/15/09   2:23:00 PM\\nxxviiIntroduction\\nIn Part II , you start in with details on how to use Linux desktops and associated applications. \\nChapters 3\u20137 describe\\nThe KDE, GNOME, and other desktop interfaces (Chapter 3)/circle6\\nTools for playing music and video (Chapter 4)/circle6\\nDesktop publishing and Web publishing using word processing, layout, drawing, and /circle6\\nimage manipulation tools, plus tools such as wikis, blogs, and content management sys-\\ntems for managing content online (Chapter 5)\\nApplications for e-mail and Web browsing/circle6  (Chapter 6)', 'page': 73, 'source': 'linux_bible.pdf'}\r\nID: edbaf079-9d34-4de7-b38b-e79ec9a16962, Score: 0.48150444, Vector: [-0.024073696, -0.06740264, -0.04933037, -0.056596126, 0.0640839, -0.08332419, 0.0027693151, 0.13868594, 0.057942413, 0.0045598065, 0.0018336017, 0.07531288, 0.018394172, -0.07559044, 0.0490967, -0.073607825, -0.013805403, -0.053357575, 0.028111273, -0.089254394, -0.05395274, 0.016952382, -0.024828944, 0.0049645705, 0.016095687, -0.0074422187, -0.0111827785, -0.0129252635, 0.015384097, -0.02754346, 0.027828624, 0.051317822, -0.024497803, -0.017709127, -0.070052184, 0.070469335, 0.043555737, -0.04935626, -0.043799397, -0.050337467, -0.070827074, -0.024127513, -0.020474128, 0.038182694, 0.0008033708, -0.03626968, -0.050244138, -0.014230472, -0.027392898, -0.05233938, -0.05788181, -0.0087971985, 0.07703545, 0.03716889, 0.0056894175, -0.08257068, -0.006536877, -0.048928753, -0.0030512463, -0.042867243, 0.026243336, -0.035939734, -0.084637694, 0.065710865, 0.001997814, -0.0055882614, 0.006594209, 0.02063613, 0.016229676, -0.09070771, -0.09010035, -0.047365617, 0.036659993, 0.11633981, -0.039124742, -0.021918172, 0.027612792, 0.007670698, 0.01996161, 0.014145673, 0.032564282, 0.120462574, -0.002979826, 0.050316617, -0.077059455, 0.06513185, 0.06011874, 0.05446738, 0.05895109, -0.047661755, 0.09689729, 0.04157286, 0.0069837584, -0.025893275, -0.04024964, -0.00027525637, 0.08663912, -0.063367985, -0.016695589, 0.07634073, -0.0046042968, -0.07553812, 0.016132131, -0.06466097, -0.046718005, -0.012522675, 0.040737897, 0.029054755, 0.0054848413, 0.020392884, -0.12974729, -0.04055431, -0.09623701, -0.024364535, -0.020406479, -0.076892, -0.06804503, 0.040718183, 0.031146491, 0.0005500672, 0.02577542, 0.059472144, -0.037667777, -0.06337242, 0.09070736, 0.011583493, 0.024941992, 2.911553e-33, 0.015209375, 0.05031743, -0.071215, -0.043480393, 0.027895976, -0.065813914, 0.0075986134, 0.045926567, -0.078338884, 0.05116739, 0.023592697, 0.06374172, -0.026450198, 0.016269743, 0.17122796, -0.021455862, -0.03337701, -0.01743979, 0.0930162, -0.03729465, -0.04721721, 0.010541696, 0.037904505, 0.023485055, 0.03921805, -0.05274242, -0.005406335, -0.034132645, 0.015480098, -0.01049564, -0.016216632, 0.011952389, 0.0054787714, -0.012547889, 0.0142544545, -0.01064859, -0.087488934, -0.09501916, 0.027134314, 0.007356856, -0.029017627, 0.036929734, -0.0067986315, 0.024368, 0.07348975, 0.03733985, 0.028452594, -0.021815332, 0.014123607, 0.013288786, -0.077107206, -0.020698708, 0.061511688, 0.042465787, -0.07053348, 0.053588647, 0.022194672, -0.033159703, 0.02148347, 0.083170116, -0.061057176, -0.051194493, 0.03694642, -0.05161562, -0.027916215, -0.060474515, 0.05438269, -0.002920457, 0.036952972, 0.05566183, -0.060088944, -0.06387428, 0.012257671, 0.03642768, -0.023741508, -0.0056332806, 0.009154998, -0.052247345, 0.023426965, 0.019512601, -0.09632673, 0.032396507, -0.06482864, 0.040396318, 0.08001134, 0.060092065, -0.09680037, 0.03586784, -0.021762831, 0.006366735, 0.038419466, -0.04474926, -0.014793833, 0.10049441, 0.0023396981, -4.9488852e-33, -0.08328449, -0.07763805, 0.0026429582, 0.04993892, 0.04945391, 0.050035976, -0.10315616, -6.121183e-05, -0.026305752, -0.008961132, -0.059943065, -0.0110602295, -0.002902582, 0.007216774, -0.0017162564, -0.07697368, -0.014351343, -0.029406307, -0.0030815566, 0.016400246, -0.04912495, 0.09657606, -0.004924246, -0.050857008, 0.032611314, 0.016854322, -0.028242879, 0.043756545, -0.0022887944, 0.016687715, 0.11364286, -0.047498297, -0.03072822, -0.011637853, 0.03166923, -0.015000611, 0.104585, 0.012948129, -0.035104573, -0.0007573287, 0.07480522, 0.017861715, -0.04618072, -0.06523287, -0.033361565, -0.02468294, -0.049310334, -0.012103423, -0.009812822, -0.0723652, 0.0133117335, -0.052323323, 0.059941348, -0.013579973, 0.0046214564, -0.041455805, -0.027436167, 0.05619888, 0.06613538, 0.03857493, 0.036038622, -0.023564767, 0.006929671, 0.12132061, -0.06090688, 0.0015805558, 0.05244593, 0.0139290495, -0.078118846, 0.04538711, -0.030370511, 0.03300274, -0.0024404058, -0.050177578, -0.04499654, 0.041831892, -0.026301343, -0.029057309, -0.101833805, -0.010659004, 0.008464277, 0.0019811208, -0.021909859, 0.03556323, -0.07210136, -0.05506824, 0.07401727, -0.09481533, 0.0062993583, -0.06668644, -0.011408364, 0.041455653, -0.031962853, 0.13377553, 0.028018326, -5.7048968e-08, 0.07315667, -0.099995196, 0.0065096393, 0.016581548, 0.012141264, -0.01728705, 0.03933864, -0.00065708964, -0.05199344, 0.074340224, 0.0500053, -0.07451712, -0.07489221, -0.042417005, 0.06900265, 0.11055208, 0.022807684, 0.12333708, -0.05916859, 0.0011295219, 0.102453664, -0.06302064, 0.08926409, 0.0017882853, 0.06728807, -0.04813749, 0.090594076, 0.026217436, 0.029793752, 0.0327958, -0.002881321, -0.026205432, 0.027978938, 0.046610203, 0.09486055, -0.02050504, -0.08508389, -0.03376538, 0.039941408, 0.07671597, -0.03209068, 0.039046388, 0.018367276, 0.006731865, -0.04051718, 0.0066023828, -0.119661875, -0.016749056, 0.042518236, -0.034705643, 0.051088512, -0.033405125, 6.3934596e-05, 0.038810138, 0.06264515, 0.007958509, 0.021373093, 0.01930552, 0.04081616, 0.093892, -0.024699429, 0.0017280468, 0.084522046, -0.030239053], Metadata: {'content': 'con\ufb01 gure system services, and secure your computers and networks.\\nA Linux server administrator/circle6 \u2014Using some of the world\u2019s best server software, you \\ncan set up your computer to be a Web server, \ufb01  le server, mail server, or print server.\\nA software developer/circle6 \u2014You can draw on thousands of open source programming tools \\nto develop your own software applications.\\nThe Linux systems you have in your hand don\u2019t contain trialware or otherwise-hobbled software. \\nOn the contrary, they feature software created by world-class development projects, the same teams that build the software that powers many professional businesses, schools, home desktops, and Internet service providers. In other words, this truly \ufb01  rst-rate software is from develop-\\ners who have made a commitment to producing software that can be used in the ways that you choose to use it.\\nMost of the Linux distributions offered on the DVD and CD that come with this book are live \\nCDs that let you try a Linux distribution without installing. Almost all of those live CDs include features that let you install the contents of those live CDs to your hard disk. For example, you can try out Fedora, Gentoo, Ubuntu, openSUSE, PCLinuxOS, and Mandriva as live CDs, and then install those distributions permanently to your hard drive from icons on the desktops of those live CDs.\\n85057flast.indd   xxv85057flast.indd   xxv 10/15/09   2:22:59 PM10/15/09   2:22:59 PM\\nxxviIntroduction\\nUnlike some other books on Linux, this book doesn\u2019t tie you to one Linux distribution. The book \\nteaches you the essentials of Linux graphical desktop interfaces, shell commands, and basic sys-tem administration. Separate chapters break down many of the major Linux distributions avail-able today. Then descriptions of the major software projects in most Linux distributions (KDE and GNOME desktops, Apache Web servers, Samba \ufb01  le and printer sharing, and so on) guide \\nyou in setting up and using those features, regardless of which Linux you choose.\\nUnderstanding the Linux Mystique\\nThis book is designed to spark your imagination about what is possible with Linux, then give you the software and instruction to jump right into Linux. From there, the approach is to help you learn by using it.', 'page': 72, 'source': 'linux_bible.pdf'}\r\nID: c83dcac1-b65a-44cc-a261-cae57bfc95d9, Score: 0.47938925, Vector: [-0.01984242, -0.009251191, 0.018457491, -0.03781013, 0.09261274, -0.1193788, -0.07727213, 0.12620927, 0.061431658, -0.008344117, 0.048700366, 0.061782468, 0.020591466, -0.08948288, 0.044110827, -0.04765848, 0.030769775, -0.042209804, -0.009124599, -0.0050316285, -0.027673768, 0.03784497, 0.01110022, -0.028833224, -0.03238227, -0.013826247, -0.013972005, -0.023391213, 0.045812063, -0.024594031, -0.031893104, 0.100631386, 0.05064631, 0.027371014, -0.046596188, 0.016476015, -0.012597444, -0.07095902, -0.12446081, -0.0748582, -0.03965642, -0.09973392, 0.000987572, 0.055254634, -0.010403949, 0.008403102, 0.027986538, -0.08947484, 0.022214811, -0.048560645, 0.007943433, 0.024376217, 0.041964628, -0.0047879405, -0.107106864, -0.08339744, -0.08268022, 0.0054734685, 0.06822181, 0.014187082, 0.068419866, 0.0043118214, -0.06377442, 0.039710492, -0.022174219, -0.018763384, -0.028919643, -0.08097144, 0.019838884, -0.11666613, -0.001925492, -0.054298107, 0.05674856, 0.064811416, -0.1130536, 0.055029638, 0.062297072, 0.03982368, 0.009177934, 0.0065956744, 0.07105321, 0.046975676, -0.05081734, -0.019804347, 0.025514895, 0.04018937, -0.03603348, 0.09892643, -0.015842075, -0.07272974, 0.028053425, 0.022715459, -0.024910647, -0.014285808, -0.062179208, -0.019571653, 0.050337255, -0.048734803, -0.044195447, 0.061492193, -0.034729943, 0.03594042, 0.004427827, 0.046833638, -0.0811618, -0.065017976, -0.0058287294, 0.038173143, 0.01884109, -0.0010024076, -0.03490026, -0.09206027, -0.042224646, -0.023313448, -0.008650974, -0.068659246, -0.07769698, 0.024585923, 0.07928828, 0.0507452, -0.02071425, -0.007857347, -0.054172, -0.085780405, 0.09351972, 0.026023535, -0.021894667, 2.9005352e-33, 0.009481408, 0.039077844, -0.058403946, -0.03236557, -0.0436744, -0.044231296, 0.013088542, -0.0015156794, -0.007417522, 0.07432986, 0.019383166, 0.05898135, -0.048191715, 0.05201061, 0.13181533, 0.01023683, -0.03131462, -0.094403155, 0.033403378, 0.02029259, -0.005610449, -0.020971322, 0.0025865599, 0.058985524, 0.10762909, -0.02880986, -0.009944965, -0.08452161, 0.042229496, 0.013476404, -0.11522525, -0.0024188985, -0.043511253, 0.0072523504, -0.020189347, 0.008922283, -0.097898155, -0.07456626, 0.04497631, -0.013212123, -0.054286465, 0.018808616, -0.021918701, -0.037321456, 0.009258915, -0.009115823, 0.026472611, 0.0015630679, -0.008941925, -0.0698581, -0.023157999, 0.04200182, 0.059763074, 0.031845745, 0.028716076, -0.06301514, 0.0072998507, -0.04026639, 0.035564926, 0.07956249, -0.034908388, 0.025232004, -0.013495203, 0.053126812, -0.049734563, -0.013398143, 0.060294926, -0.0136634465, -0.06469721, 0.111679986, -0.013956184, -0.020290876, -0.037272546, -0.010292192, -0.015074762, 0.057141107, 0.016747905, -0.04238848, 0.015225668, 0.0037201268, -0.010678778, 0.025131961, 0.0011383804, 0.076179445, 0.0059719067, 0.010766955, -0.08781016, -0.052099165, -0.01751505, 0.03523437, -0.014993603, -0.01481304, -0.05686633, 0.09258094, -0.014618604, -3.5376932e-33, -0.06829073, -0.05625367, 0.01898682, -0.018509958, 0.0141084455, 0.033197742, -0.006314207, -0.06868979, -0.057048954, -0.018725408, -0.09629635, 0.020050153, 0.06967098, 0.014687666, 0.040810168, -0.04380362, -0.020860473, -0.05163723, 0.035567094, -0.0116340425, -0.037055183, 0.1415086, 0.02216848, -0.032433484, 0.046441674, 0.0015570261, -0.03556, 0.033511095, -0.030493163, -0.034631, 0.0648899, -0.060514897, 0.024986625, 0.013376488, 0.13324007, 0.058048494, 0.056575127, 0.0039180466, 0.0019940243, -0.049429327, 0.112915434, -0.0021007357, -0.005317771, -0.0010912868, 0.024768947, -0.04099412, -0.10867125, 0.01139904, 0.010392324, -0.043638453, 0.06804612, 0.023437329, -0.0022319213, -0.027865846, -0.031080289, -0.051383488, 0.004178208, 0.016367517, 0.0064936453, -0.005538669, 0.10007223, -0.0513617, -0.014779815, 0.00809466, -0.092387974, -0.035935618, 0.07260038, 0.04789249, -0.030158034, 0.042474665, -0.0046493723, -0.024117976, 0.0284292, 0.015506525, 0.012158536, 0.11800204, -0.0076966677, -0.084179156, -0.02120884, -0.024012763, -0.0050964435, 0.036493853, 0.08855071, 0.02996459, -0.01967626, 0.029032387, 0.0044003217, 0.013221407, -0.0519412, -0.024593018, 0.017867127, -0.022305354, -0.07073401, 0.14142574, -0.050426587, -5.4846257e-08, 0.12618408, -0.027908817, 0.041142188, -0.011754587, -0.044638153, -0.039858174, 0.10112651, -0.07076211, 0.049010936, 0.09001549, 0.055804297, -0.0926332, -0.039273467, 0.03511042, 0.058552198, 0.17512369, 0.012697251, 0.05950258, -0.028028043, 0.022259632, 0.08422044, 0.008618497, 0.008055855, 0.020150276, 0.03209303, -0.029780453, 0.022748128, -0.037858676, 0.040491004, 0.005359437, -0.03923957, -0.02287067, 0.057957288, 0.05262032, 0.051870637, -0.04273112, -0.024588205, -0.0257344, 0.0607741, -0.0013320881, -0.002911388, 0.013258696, -0.038104214, 0.0073573994, -0.09144806, -0.047250293, -0.08016776, -0.08225485, -0.0037114276, 0.07447247, 0.012219077, -0.00036026005, 0.014231857, 0.024624372, 0.06254016, -0.0037182071, 0.025127977, -0.019237185, -0.009543065, 0.013651966, -0.053267457, -0.03690494, 0.06738827, 0.046157088], Metadata: {'content': 'lowing the progress of the Mars Exploration Rover Mission, refer to the project\u2019s Web site at the NASA Jet Propulsion Laboratory (\\nhttp://marsrovers.jpl.nasa.gov ).\\nLinux in gadgets\\nLots of commercial communications, entertainment, and other kinds of gadgets have Linux run-ning on the inside. Linux makes an excellent operating system for these specialty devices, not only because of its cost, but also because of its stability and adaptability. \\nLinux enthusiasts love these devices, referred to as embedded Linux systems, because they can \\noften adapt, add, or run different Linux software on these devices (whether the manufacturer \\nintended that or not). More and more, however, manufacturers are embracing the Linux enthusi-ast and hardware hacker and selling open devices for them to use. Here are some examples:\\nMobile Phones\\n/circle6 \u2014OpenMoko ( www.openmoko.com ) produces mass-market mobile \\nphones, such as the Neo FreeRunner, that are based on Linux. Like the software, the FreeRunner\u2019s hardware also follows an open design. Although the phone is intended for general consumer use, the phone\u2019s software is currently most appropriate for people who want to develop their own software for the phones. Figure 2-2 shows an example of the Neo FreeRunner.\\n85057c02.indd   1785057c02.indd   17 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n18Part I: Getting Off the Ground with Linux\\nFIGURE 2-2\\nModify software to use the Linux Neo FreeRunner as a phone, GPS device, clock, game player, and media \\nplayer.\\nMotorola, OpenMoko, and Tranzda Technologies each offer multiple Linux-based \\nmobile phones. Phone models running Linux on the inside include the Motorola Rokr EM30 (emphasizing music playing), Tranzda Technologies NewPlus phones (with WiFi, GPS, and a camera), Purple Labs Purple Magic phones (sub-$100 phone), and Grunig B700 (with keyboard and e-mail support).\\nSony PlayStation\\n/circle6 \u2014Not only can you install and run Linux on PlayStation, but Sony \\nencourages you to do it. In 2002, Sony released Linux Kit for PlayStation 2. Included \\nin that kit is a derivative of the Japanese Kondara MNU/Linux (which is based on Red Hat Linux). For PlayStation 3, several Linux distributions have been modi\ufb01  ed (ported) \\nto run on that hardware, including Fedora, OpenSUSE, Ubuntu, Gentoo, Debian, and a commercial Yellow Dog Linux product for PlayStation 3. (See Chapter 22 for a descrip-tion of Yellow Dog Linux.) \\nPersonal Video Recorders (PVR)\\n/circle6 \u2014If you have a TiVo PVR or a set-top for streaming \\nvideo from Net\ufb02  ix, you are already running Linux in your home. The Net\ufb02  ix PVR is \\nfrom Roku, Inc. ( www.roku.com ), which produces a range of Linux-based media play-\\ners. TiVo has produced Linux-based PVRs for years. The availability of the TiVo Linux \\nsource code ( www.tivo.com/linux ) has made TiVo one of the most popular devices \\nfor Linux enthusiasts to hack.\\nNetbooks/circle6 \u2014Shrinking laptops with shrinking prices have led to Netbooks. These \\nmini laptop computers have proven to be excellent devices for running Linux. With \\nlow-powered processors and small screens, Netbooks provide a good partnership with Linux systems that are tuned for these compact, ef\ufb01  cient devices. \\n85057c02.indd   1885057c02.indd   18 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n19Chapter 2: Linux Projects, Activities, and Careers\\nThe Asus Eee PC is one of the most popular Netbooks available today. Many Asus Eee \\nPCs have been sold with Xandros Linux preinstalled. However, industrious Linux enthusiasts have created ports of Fedora, Ubuntu, and other Linux distributions to run on the Eee PC. The Acer Aspire One is another popular netbook. Figure 2-3 shows an Eee PC with Fedora Linux running on it.\\nFIGURE 2-3\\nAn Asus Eee PC can run specially tuned Fedora or other Linux systems.\\nPersonal handheld devices/circle6 \u2014A whole range of personal digital assistants (PDAs), por-', 'page': 88, 'source': 'linux_bible.pdf'}\r\nID: e94f41b3-aebb-44a7-8628-2116e344541e, Score: 0.47909445, Vector: [-0.010300992, -0.06201704, -0.046429355, -0.020711467, 0.15296702, -0.06355586, -0.064711764, 0.12849095, 0.0124012, -0.007829238, -0.015711484, 0.08028847, -0.027482616, -0.074537545, 0.06093343, -0.025529772, -0.030304754, 0.006690845, -0.0096968, -0.060203716, -0.03571217, 0.08405562, 0.038551856, -0.017983817, -0.004565045, -0.04740941, -0.03339397, -0.053387243, 0.032289147, -0.06196493, -0.047502507, 0.10740722, -0.011146251, 0.0006463854, -0.001375539, 0.041849215, 0.07455948, -0.047944102, -0.05673705, 0.019862773, 0.0066904738, -0.05481761, 0.0051639997, 0.02902823, -0.022243315, 0.011124962, -0.037202362, 0.0013854739, -0.032971192, -0.0039838725, -0.06632171, -0.044125546, 0.04338726, -0.011582755, 0.057951145, -0.12746732, -0.03482658, -0.05431481, 0.044428058, -0.0540241, 0.075629696, 0.050348457, -0.10716757, 0.055063426, -0.044008967, -0.018426618, 0.0520913, 0.030205825, -0.0060597276, -0.12652613, -0.040145557, -0.01656209, 0.0632583, 0.09220716, -0.08039693, 0.030515926, 0.04136586, 0.060651947, 0.029749708, -0.07679328, 0.04748961, 0.09376876, -0.013562662, 0.019536197, -0.06977183, 0.021640256, 0.038197275, 0.09130066, 0.027071746, -0.0029820877, 0.045834366, 0.026631517, 0.07354314, -0.058127504, 0.023106199, 0.010141544, 0.010416328, -0.063011095, 0.0074689905, 0.08638861, -0.0062448145, -0.00063023536, -0.054977197, -0.049482856, 0.020968132, 0.036079757, -0.023055127, 0.08435904, -0.0077255587, 0.03469405, -0.008138906, 0.010986437, 0.006401985, -0.07772589, 0.07231693, -0.023943482, -0.029293371, -6.1269902e-06, 0.043515585, 0.025842533, 0.009960307, 0.0696169, -0.08994871, -0.051646095, 0.019839454, -0.0019147018, -0.019577768, 2.4558647e-33, 0.051334754, 0.121646635, -0.05315276, -0.010768901, 0.04252973, -0.052605424, -0.020486953, 0.0245449, -0.07791334, 0.03434429, 0.02008067, 0.07927704, -0.04048923, -0.052928265, 0.016566465, -0.03802563, -0.055800743, 0.028523887, -0.016138408, 0.018254908, 0.022699669, -0.010526441, -0.017334113, 0.07162002, 0.0062123733, -0.010634764, -0.037513033, -0.10960199, 0.027357003, 0.0015277078, -0.050305974, 0.00623781, 0.0154764345, -0.011034626, 0.006919724, 0.014764883, -0.111299485, -0.07058621, 0.013226075, -0.0148951905, -0.05644188, -0.008299337, -0.054403916, 0.012482914, 0.10260806, 0.010018166, -0.03929273, -0.12071573, 0.019646032, -0.13366917, -0.006228709, -0.009613113, 0.07321811, 0.026708957, 0.015480196, 0.04757488, 0.0016601183, -0.078985006, 0.041134793, 0.081338294, 0.022034533, 0.08186449, 0.0029554574, -0.07316424, -0.10174299, -0.010351725, -0.041926827, 0.019396342, -0.036740974, 0.059312947, 0.0062836953, -0.03918396, -0.037556905, 0.068071246, -0.12185273, 0.052621372, -0.023773942, -0.080995604, -0.0044211005, -0.017488264, 0.0210193, 0.027639898, 0.013936339, -0.024074338, 0.015879748, 0.003967011, -0.061624024, -0.02687933, -0.034902807, 0.06535775, 0.05245469, 0.021223512, -0.03312717, 0.04180811, 0.009369041, -3.6707265e-33, -0.07170997, -0.049262974, 0.005436494, 0.02097835, -0.017825106, 0.051876474, -0.080931135, 0.023526987, 0.030140061, -0.0046952427, -0.047185495, -0.02370666, 0.02544702, -0.0033252586, 0.010368276, -0.067364074, 0.029264536, -0.06660196, -0.06475247, 0.08037294, -0.051682677, 0.02745868, -0.09859848, -0.044254925, 0.104418464, 0.050720062, 0.033178568, -0.0054844175, -0.034017157, -0.006020143, 0.04121809, -0.045161247, -0.06913875, 0.011488584, 0.061367203, 0.015433793, 0.050012846, 0.00257793, -0.01239022, -0.04589155, 0.07498995, -0.0022535587, 0.020352928, 0.017522484, -0.03924848, -0.03774075, -0.07311946, 0.032017548, -0.012868123, -0.06858843, 0.021572534, -0.02578503, 0.09557191, -0.036143307, -0.026867026, 0.0065293913, -0.04974923, 0.017149953, 0.032028034, 0.06404519, 0.013959068, 0.043303844, -0.11753003, 0.08651047, -0.019341186, -0.10872248, -0.0069740573, 0.024993675, -0.07933814, 0.0064090686, 0.032578446, 0.032940682, -0.012422266, -0.079072, 0.008427305, 0.094235614, 0.080659576, -0.040665917, -0.08650399, 0.031105854, 0.020814111, 0.06437881, 0.010438581, 0.06726853, -0.011356185, 0.00449933, 0.050619584, -0.0041684816, -0.0037916205, -0.00862209, -0.036069583, -0.033092856, -0.08996839, 0.12527077, 0.05365839, -4.848567e-08, 0.05937584, -0.06420152, -0.016957618, 0.0072025787, 0.059579283, -0.002430824, 0.041962486, -0.0506395, -0.058409754, 0.114983946, -0.012802282, -0.02817539, -0.06369224, -0.055873036, 0.021985201, 0.027643684, -0.016343528, 0.020413827, -0.024268107, -0.019330945, 0.11020154, -0.031858914, 0.039179202, 0.021734178, 0.026570005, -0.05292101, 0.075765245, -0.02957554, 0.037306145, 0.031732593, -0.0484194, 0.03519231, -0.06711979, -0.037277885, 0.090817496, 0.022939693, -0.019892158, 0.024260553, 0.024938457, -0.007078374, -0.010788959, 0.010601572, 0.05336603, -0.0007359, -0.07832101, 0.05739783, -0.05445489, -0.038015023, 0.0633694, -0.0003000535, 0.025854977, -0.068209656, 0.088410355, 0.011995063, -0.006621311, 0.040278014, -0.009107122, 0.001905905, 0.016178312, -0.0037381025, -0.035165116, -0.039213505, 0.1156438, 0.106257774], Metadata: {'content': 'Digg \\n/circle6 (http://digg.com/linux_unix )\u2014Some say that Digg.com has become more \\npopular than Slashdot for providing articles relating to Linux. You can vote on which \\narticles are most interesting to you to gain more exposure for an article.\\nGroklaw/circle6  (www.groklaw.net )\u2014The place to look for information regarding legal \\nissues surrounding open source software.Linux Today\\n/circle6  (www.linuxtoday.com )\u2014This site gathers news that is of particular \\ninterest to software developers and IT managers.LWN.net\\n/circle6  (www.lwn.net )\u2014Produces a weekly newsletter covering a range of Linux \\ntopics.\\n85057c02.indd   2385057c02.indd   23 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n24Part I: Getting Off the Ground with Linux\\nNewsforge/circle6  (www.newsforge.com )\u2014Bills itself as the \u201cOnline Newspaper for Linux \\nand Open Source.\u201d Contains many original articles, as well as links to up-to-the-minute \\nopen source stories from other locations on the Web.\\nLinuxInsider/circle6  (www.linuxinsider.com )\u2014Covers news articles related to Linux \\nissues around the world.Linux at Wikipedia \\n/circle6 (http://en.wikipedia.org/wiki/Linux )\u2014Contains an \\nexcellent write-up of what Linux is, and includes other Wikipedia links to related top-\\nics, companies, and issues. Also, provides a good understanding of Linux history and relationships.\\nLinux.com\\n/circle6  (www.linux.com )\u2014Provides Linux information, news, tips, articles, and \\nreference material.CertCities.com\\n/circle6  (http://certcities.com/certs/linux_unix/columns/) \u2014\\nRegularly publishes articles on Linux and UNIX.\\nIf you need help or have questions about Linux, here are a few sites to try:\\nLinux Questions/circle6  (www.linuxquestions.org )\u2014In addition to offering forums on \\ndifferent Linux distributions, this is a great place to ask questions related to hardware \\ncompatibility, security, and networking. The site also has some good tutorials, as well as reviews of books and Linux distributions.\\nGoogle Linux\\n/circle6  (www.google.com/linux )\u2014Search for Linux-speci\ufb01  c information from \\nthis part of the Google search site.Linux Forums\\n/circle6  (www.linuxforums.org )\u2014Contains active forums on your favorite \\ndistributions and has active IRC channels as well.The Linux Documentation Project\\n/circle6  (www.tldp.org )\u2014Offers a wide range of \\nHOWTOs, guides, FAQs, man pages, and other documentation related to Linux.Linux Help\\n/circle6  (www.linuxhelp.net )\u2014Offers forums, news, and current information \\nabout the Linux kernel. Also contains information about \ufb01  nding Linux mailing lists, \\nnewsgroups, and user groups.Linux Online\\n/circle6  (www.linux.org )\u2014Provides a central source of information related to \\nLinux distributions, documentation, books, and people.Linux Kernel Archives\\n/circle6  (www.kernel.org )\u2014The primary site for Linux kernel devel-\\nopment. You can get the latest stable or testing versions of the Linux kernel. Not the \ufb01  rst \\nplace to start with Linux, but I thought you would want to know it\u2019s there.\\nMajor Linux projects\\nAs you may know, the name Linux comes from the Linux kernel created by Linus Torvalds. The \\ndesktop, application, server, and other software needed to create a full Linux system are added \\n85057c02.indd   2485057c02.indd   24 10/18/09   11:26:05 PM10/18/09   11:26:05 PM\\n25Chapter 2: Linux Projects, Activities, and Careers\\nfrom other open source projects. The following is a list of some of the major open source software \\norganizations that usually have software included with Linux:\\nFree Software Foundation/circle6  (www.fsf.org )\u2014Supports the GNU project, which pro-\\nduces much of the software outside the kernel that is associated with Linux. In particu-lar, open source versions of nearly every early UNIX command have been implemented by the GNU project. \\nApache Software Foundation', 'page': 92, 'source': 'linux_bible.pdf'}\r\n\r\n```\r\nPlease refer to attached screenshot regarding the data insertion into Qdrant\r\n<img width=\"1364\" alt=\"qdrant_data_insertion\" src=\"https://github.com/griptape-ai/griptape/assets/13365920/bf65ff96-e8d8-44a3-b876-8221f35a3dc2\">\r\n\r\n### Special Note\r\nUtilization of batches is still in progress, (the code is already present in the MR) and we are actively working on it to further enhance the efficiency of the integration.\r\n\r\n\r\n<!-- readthedocs-preview griptape start -->\r\n----\n\ud83d\udcda Documentation preview \ud83d\udcda: https://griptape--812.org.readthedocs.build//812/\n\r\n<!-- readthedocs-preview griptape end -->\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 273fe5429..29ca44e2a 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -263,6 +263,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n - `MarkdownifyWebScraperDriver` for scraping text from web pages using playwright and converting to markdown using markdownify.\n - `VoyageAiEmbeddingDriver` for use with VoyageAi's embedding models. \n - `AnthropicStructureConfig` for providing Structures with Anthropic Prompt and VoyageAi Embedding Driver configuration.\n+- `QdrantVectorStoreDriver` to integrate with Qdrant vector databases.\n \n ### Fixed\n - Improved system prompt in `ToolTask` to support more use cases.\ndiff --git a/docs/griptape-framework/drivers/vector-store-drivers.md b/docs/griptape-framework/drivers/vector-store-drivers.md\nindex 095c14f22..da6fdc242 100644\n--- a/docs/griptape-framework/drivers/vector-store-drivers.md\n+++ b/docs/griptape-framework/drivers/vector-store-drivers.md\n@@ -406,3 +406,65 @@ vector_store_driver.upsert_text_artifacts(\n result = vector_store_driver.query(\"What is griptape?\")\n print(result)\n ```\n+\n+### Qdrant\n+\n+!!! info\n+    This driver requires the `drivers-vector-qdrant` [extra](../index.md#extras).\n+\n+The QdrantVectorStoreDriver supports the [Qdrant vector database](https://qdrant.tech/).\n+\n+Here is an example of how the driver can be used to query information in a Qdrant collection:\n+\n+```python\n+import os\n+from griptape.drivers import QdrantVectorStoreDriver, HuggingFaceHubEmbeddingDriver\n+from griptape.tokenizers import HuggingFaceTokenizer\n+from griptape.loaders import WebLoader\n+\n+# Set up environment variables\n+embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n+host = os.environ[\"QDRANT_CLUSTER_ENDPOINT\"]\n+huggingface_token = os.environ[\"HUGGINGFACE_HUB_ACCESS_TOKEN\"]\n+\n+# Initialize HuggingFace embedding driver\n+embedding_driver = HuggingFaceHubEmbeddingDriver(\n+    api_token=huggingface_token,\n+    model=embedding_model_name,\n+    tokenizer=HuggingFaceTokenizer(model=embedding_model_name, max_output_tokens=512),\n+)\n+\n+# Initialize Qdrant vector store driver\n+vector_store_driver = QdrantVectorStoreDriver(\n+    url=host,\n+    collection_name=\"griptape\",\n+    content_payload_key=\"content\",\n+    embedding_driver=embedding_driver,\n+    api_key=os.environ[\"QDRANT_CLUSTER_API_KEY\"],\n+)\n+\n+# Load data from the website\n+artifacts = WebLoader().load(\"https://www.griptape.ai\")\n+\n+# Encode text to get embeddings\n+embeddings = embedding_driver.embed_text_artifact(artifacts[0])\n+\n+# Recreate Qdrant collection\n+vector_store_driver.client.recreate_collection(\n+    collection_name=vector_store_driver.collection_name,\n+    vectors_config={\n+        \"size\": len(embeddings),\n+        \"distance\": vector_store_driver.distance\n+    },\n+)\n+\n+# Upsert vector into Qdrant\n+vector_store_driver.upsert_vector(\n+    vector=embeddings,\n+    vector_id=str(artifacts[0].id),\n+    content=artifacts[0].value\n+)\n+\n+print(\"Vectors successfully inserted into Qdrant.\")\n+\n+```\ndiff --git a/griptape/drivers/__init__.py b/griptape/drivers/__init__.py\nindex 34b43103b..fa2934a38 100644\n--- a/griptape/drivers/__init__.py\n+++ b/griptape/drivers/__init__.py\n@@ -39,6 +39,7 @@\n from .vector.pgvector_vector_store_driver import PgVectorVectorStoreDriver\n from .vector.azure_mongodb_vector_store_driver import AzureMongoDbVectorStoreDriver\n from .vector.dummy_vector_store_driver import DummyVectorStoreDriver\n+from .vector.qdrant_vector_store_driver import QdrantVectorStoreDriver\n \n from .sql.base_sql_driver import BaseSqlDriver\n from .sql.amazon_redshift_sql_driver import AmazonRedshiftSqlDriver\n@@ -144,6 +145,7 @@\n     \"OpenSearchVectorStoreDriver\",\n     \"AmazonOpenSearchVectorStoreDriver\",\n     \"PgVectorVectorStoreDriver\",\n+    \"QdrantVectorStoreDriver\",\n     \"DummyVectorStoreDriver\",\n     \"BaseSqlDriver\",\n     \"AmazonRedshiftSqlDriver\",\ndiff --git a/griptape/drivers/vector/qdrant_vector_store_driver.py b/griptape/drivers/vector/qdrant_vector_store_driver.py\nnew file mode 100644\nindex 000000000..5159bdb0a\n--- /dev/null\n+++ b/griptape/drivers/vector/qdrant_vector_store_driver.py\n@@ -0,0 +1,207 @@\n+from __future__ import annotations\n+from typing import Optional\n+from attrs import define, field\n+from griptape.drivers import BaseVectorStoreDriver\n+from griptape.utils import import_optional_dependency\n+import uuid\n+import logging\n+\n+DEFAULT_DISTANCE = \"Cosine\"\n+CONTENT_PAYLOAD_KEY = \"data\"\n+\n+\n+@define\n+class QdrantVectorStoreDriver(BaseVectorStoreDriver):\n+    \"\"\"\n+    Attributes:\n+        location: An optional location for the Qdrant client. If set to ':memory:', an in-memory client is used.\n+        url: An optional Qdrant API URL.\n+        host: An optional Qdrant host.\n+        path: Persistence path for QdrantLocal. Default: None\n+        port: The port number for the Qdrant client. Defaults: 6333.\n+        grpc_port: The gRPC port number for the Qdrant client. Defaults: 6334.\n+        prefer_grpc: A boolean indicating whether to prefer gRPC over HTTP. Defaults: False.\n+        force_disable_check_same_thread: For QdrantLocal, force disable check_same_thread. Default: False Only use this if you can guarantee that you can resolve the thread safety outside QdrantClient.\n+        timeout: Timeout for REST and gRPC API requests. Default: 5 seconds for REST and unlimited for gRPC\n+        api_key: API key for authentication in Qdrant Cloud. Defaults: False\n+        https: If true - use HTTPS(SSL) protocol. Default: None\n+        prefix: Add prefix to the REST URL path. Example: service/v1 will result in Example: service/v1 will result in http://localhost:6333/service/v1/{qdrant-endpoint} for REST API. Defaults: None\n+        distance: The distance metric to be used for the vectors. Defaults: 'COSINE'.\n+        collection_name: The name of the Qdrant collection.\n+        vector_name: An optional name for the vectors.\n+        content_payload_key: The key for the content payload in the metadata. Defaults: 'data'.\n+    \"\"\"\n+\n+    location: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    url: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    host: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    path: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    port: int = field(default=6333, kw_only=True, metadata={\"serializable\": True})\n+    grpc_port: int = field(default=6334, kw_only=True, metadata={\"serializable\": True})\n+    prefer_grpc: bool = field(default=False, kw_only=True, metadata={\"serializable\": True})\n+    api_key: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    https: bool = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    prefix: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    force_disable_check_same_thread: Optional[bool] = field(\n+        default=False, kw_only=True, metadata={\"serializable\": True}\n+    )\n+    timeout: Optional[int] = field(default=5, kw_only=True, metadata={\"serializable\": True})\n+    distance: str = field(default=DEFAULT_DISTANCE, kw_only=True, metadata={\"serializable\": True})\n+    collection_name: str = field(kw_only=True, metadata={\"serializable\": True})\n+    vector_name: Optional[str] = field(default=None, kw_only=True, metadata={\"serializable\": True})\n+    content_payload_key: str = field(default=CONTENT_PAYLOAD_KEY, kw_only=True, metadata={\"serializable\": True})\n+\n+    def __attrs_post_init__(self) -> None:\n+        self.client = import_optional_dependency(\"qdrant_client\").QdrantClient(\n+            location=self.location,\n+            url=self.url,\n+            host=self.host,\n+            path=self.path,\n+            port=self.port,\n+            prefer_grpc=self.prefer_grpc,\n+            grpc_port=self.grpc_port,\n+            api_key=self.api_key,\n+            https=self.https,\n+            prefix=self.prefix,\n+            force_disable_check_same_thread=self.force_disable_check_same_thread,\n+            timeout=self.timeout,\n+        )\n+\n+    def delete_vector(self, vector_id: str) -> None:\n+        \"\"\"\n+        Delete a vector from the Qdrant collection based on its ID.\n+\n+        Parameters:\n+            vector_id (str | id): ID of the vector to delete.\n+        \"\"\"\n+        deletion_response = self.client.delete(\n+            collection_name=self.collection_name,\n+            points_selector=import_optional_dependency(\"qdrant_client.http.models\").PointIdsList(points=[vector_id]),\n+        )\n+        if deletion_response.status == import_optional_dependency(\"qdrant_client.http.models\").UpdateStatus.COMPLETED:\n+            logging.info(f\"ID {vector_id} is successfully deleted\")\n+\n+    def query(\n+        self,\n+        query: str,\n+        count: Optional[int] = None,\n+        namespace: Optional[str] = None,\n+        include_vectors: bool = False,\n+        **kwargs,\n+    ) -> list[BaseVectorStoreDriver.Entry]:\n+        \"\"\"\n+        Query the Qdrant collection based on a query vector.\n+\n+        Parameters:\n+            query (str): Query string.\n+            count (Optional[int]): Optional number of results to return.\n+            namespace (Optional[str]): Optional namespace of the vectors.\n+            include_vectors (bool): Whether to include vectors in the results.\n+\n+        Returns:\n+            list[BaseVectorStoreDriver.Entry]: List of Entry objects.\n+        \"\"\"\n+        query_vector = self.embedding_driver.embed_string(query)\n+\n+        # Create a search request\n+        results = self.client.search(collection_name=self.collection_name, query_vector=query_vector, limit=count)\n+\n+        # Convert results to QueryResult objects\n+        query_results = [\n+            BaseVectorStoreDriver.Entry(\n+                id=result.id,\n+                vector=result.vector if include_vectors else [],\n+                score=result.score,\n+                meta={k: v for k, v in result.payload.items() if k not in [\"_score\", \"_tensor_facets\"]},\n+            )\n+            for result in results\n+        ]\n+        return query_results\n+\n+    def upsert_vector(\n+        self,\n+        vector: list[float],\n+        vector_id: Optional[str] = None,\n+        namespace: Optional[str] = None,\n+        meta: Optional[dict] = None,\n+        content: Optional[str] = None,\n+        **kwargs,\n+    ) -> str:\n+        \"\"\"\n+        Upsert vectors into the Qdrant collection.\n+\n+        Parameters:\n+            vector (list[float]): The vector to be upserted.\n+            vector_id (Optional[str]): Optional vector ID.\n+            namespace (Optional[str]): Optional namespace for the vector.\n+            meta (Optional[dict]): Optional dictionary containing metadata.\n+            content (Optional[str]): The text content to be included in the payload.\n+\n+        Returns:\n+            str: The ID of the upserted vector.\n+        \"\"\"\n+\n+        if vector_id is None:\n+            vector_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, str(vector)))\n+\n+        if meta is None:\n+            meta = {}\n+\n+        if content:\n+            meta[self.content_payload_key] = content\n+\n+        points = import_optional_dependency(\"qdrant_client.http.models\").Batch(\n+            ids=[vector_id], vectors=[vector], payloads=[meta] if meta else None\n+        )\n+\n+        self.client.upsert(collection_name=self.collection_name, points=points)\n+        return vector_id\n+\n+    def load_entry(self, vector_id: str, namespace: Optional[str] = None) -> Optional[BaseVectorStoreDriver.Entry]:\n+        \"\"\"\n+        Load a vector entry from the Qdrant collection based on its ID.\n+\n+        Parameters:\n+            vector_id (str): ID of the vector to load.\n+            namespace (str, optional): Optional namespace of the vector.\n+\n+        Returns:\n+            Optional[BaseVectorStoreDriver.Entry]: Vector entry if found, else None.\n+        \"\"\"\n+        results = self.client.retrieve(collection_name=self.collection_name, ids=[vector_id])\n+        if results:\n+            entry = results[0]\n+            return BaseVectorStoreDriver.Entry(\n+                id=entry.id,\n+                vector=entry.vector,\n+                meta={k: v for k, v in entry.payload.items() if k not in [\"_score\", \"_tensor_facets\"]},\n+            )\n+        else:\n+            return None\n+\n+    def load_entries(self, namespace: Optional[str] = None, **kwargs) -> list[BaseVectorStoreDriver.Entry]:\n+        \"\"\"\n+        Load vector entries from the Qdrant collection.\n+\n+        Parameters:\n+            namespace: Optional namespace of the vectors.\n+\n+        Returns:\n+            List of points.\n+        \"\"\"\n+\n+        results = self.client.retrieve(\n+            collection_name=self.collection_name,\n+            ids=kwargs.get(\"ids\", []),\n+            with_payload=kwargs.get(\"with_payload\", True),\n+            with_vectors=kwargs.get(\"with_vectors\", True),\n+        )\n+\n+        return [\n+            BaseVectorStoreDriver.Entry(\n+                id=entry.id,\n+                vector=entry.vector if kwargs.get(\"with_vectors\", True) else [],\n+                meta={k: v for k, v in entry.payload.items() if k not in [\"_score\", \"_tensor_facets\"]},\n+            )\n+            for entry in results\n+        ]\ndiff --git a/poetry.lock b/poetry.lock\nindex 768a4e8ae..64f01aa75 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -1844,6 +1844,74 @@ googleapis-common-protos = \">=1.5.5\"\n grpcio = \">=1.62.2\"\n protobuf = \">=4.21.6\"\n \n+[[package]]\n+name = \"grpcio-tools\"\n+version = \"1.62.2\"\n+description = \"Protobuf code generator for gRPC\"\n+optional = true\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"grpcio-tools-1.62.2.tar.gz\", hash = \"sha256:5fd5e1582b678e6b941ee5f5809340be5e0724691df5299aae8226640f94e18f\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-linux_armv7l.whl\", hash = \"sha256:1679b4903aed2dc5bd8cb22a452225b05dc8470a076f14fd703581efc0740cdb\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-macosx_12_0_universal2.whl\", hash = \"sha256:9d41e0e47dd075c075bb8f103422968a65dd0d8dc8613288f573ae91eb1053ba\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-manylinux_2_17_aarch64.whl\", hash = \"sha256:987e774f74296842bbffd55ea8826370f70c499e5b5f71a8cf3103838b6ee9c3\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:40cd4eeea4b25bcb6903b82930d579027d034ba944393c4751cdefd9c49e6989\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b6746bc823958499a3cf8963cc1de00072962fb5e629f26d658882d3f4c35095\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:2ed775e844566ce9ce089be9a81a8b928623b8ee5820f5e4d58c1a9d33dfc5ae\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:bdc5dd3f57b5368d5d661d5d3703bcaa38bceca59d25955dff66244dbc987271\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-win32.whl\", hash = \"sha256:3a8d6f07e64c0c7756f4e0c4781d9d5a2b9cc9cbd28f7032a6fb8d4f847d0445\"},\n+    {file = \"grpcio_tools-1.62.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:e33b59fb3efdddeb97ded988a871710033e8638534c826567738d3edce528752\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-linux_armv7l.whl\", hash = \"sha256:472505d030135d73afe4143b0873efe0dcb385bd6d847553b4f3afe07679af00\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-macosx_10_10_universal2.whl\", hash = \"sha256:ec674b4440ef4311ac1245a709e87b36aca493ddc6850eebe0b278d1f2b6e7d1\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-manylinux_2_17_aarch64.whl\", hash = \"sha256:184b4174d4bd82089d706e8223e46c42390a6ebac191073b9772abc77308f9fa\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c195d74fe98541178ece7a50dad2197d43991e0f77372b9a88da438be2486f12\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a34d97c62e61bfe9e6cff0410fe144ac8cca2fc979ad0be46b7edf026339d161\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:cbb8453ae83a1db2452b7fe0f4b78e4a8dd32be0f2b2b73591ae620d4d784d3d\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:4f989e5cebead3ae92c6abf6bf7b19949e1563a776aea896ac5933f143f0c45d\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-win32.whl\", hash = \"sha256:c48fabe40b9170f4e3d7dd2c252e4f1ff395dc24e49ac15fc724b1b6f11724da\"},\n+    {file = \"grpcio_tools-1.62.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:8c616d0ad872e3780693fce6a3ac8ef00fc0963e6d7815ce9dcfae68ba0fc287\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-linux_armv7l.whl\", hash = \"sha256:10cc3321704ecd17c93cf68c99c35467a8a97ffaaed53207e9b2da6ae0308ee1\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-macosx_10_10_universal2.whl\", hash = \"sha256:9be84ff6d47fd61462be7523b49d7ba01adf67ce4e1447eae37721ab32464dd8\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-manylinux_2_17_aarch64.whl\", hash = \"sha256:d82f681c9a9d933a9d8068e8e382977768e7779ddb8870fa0cf918d8250d1532\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:04c607029ae3660fb1624ed273811ffe09d57d84287d37e63b5b802a35897329\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:72b61332f1b439c14cbd3815174a8f1d35067a02047c32decd406b3a09bb9890\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:8214820990d01b52845f9fbcb92d2b7384a0c321b303e3ac614c219dc7d1d3af\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:462e0ab8dd7c7b70bfd6e3195eebc177549ede5cf3189814850c76f9a340d7ce\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-win32.whl\", hash = \"sha256:fa107460c842e4c1a6266150881694fefd4f33baa544ea9489601810c2210ef8\"},\n+    {file = \"grpcio_tools-1.62.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:759c60f24c33a181bbbc1232a6752f9b49fbb1583312a4917e2b389fea0fb0f2\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-linux_armv7l.whl\", hash = \"sha256:45db5da2bcfa88f2b86b57ef35daaae85c60bd6754a051d35d9449c959925b57\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-macosx_10_10_universal2.whl\", hash = \"sha256:ab84bae88597133f6ea7a2bdc57b2fda98a266fe8d8d4763652cbefd20e73ad7\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-manylinux_2_17_aarch64.whl\", hash = \"sha256:7a49bccae1c7d154b78e991885c3111c9ad8c8fa98e91233de425718f47c6139\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a7e439476b29d6dac363b321781a113794397afceeb97dad85349db5f1cb5e9a\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7ea369c4d1567d1acdf69c8ea74144f4ccad9e545df7f9a4fc64c94fa7684ba3\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:4f955702dc4b530696375251319d05223b729ed24e8673c2129f7a75d2caefbb\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:3708a747aa4b6b505727282ca887041174e146ae030ebcadaf4c1d346858df62\"},\n+    {file = \"grpcio_tools-1.62.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:2ce149ea55eadb486a7fb75a20f63ef3ac065ee6a0240ed25f3549ce7954c653\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-linux_armv7l.whl\", hash = \"sha256:58cbb24b3fa6ae35aa9c210fcea3a51aa5fef0cd25618eb4fd94f746d5a9b703\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-macosx_10_10_universal2.whl\", hash = \"sha256:6413581e14a80e0b4532577766cf0586de4dd33766a31b3eb5374a746771c07d\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-manylinux_2_17_aarch64.whl\", hash = \"sha256:47117c8a7e861382470d0e22d336e5a91fdc5f851d1db44fa784b9acea190d87\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:9f1ba79a253df9e553d20319c615fa2b429684580fa042dba618d7f6649ac7e4\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:04a394cf5e51ba9be412eb9f6c482b6270bd81016e033e8eb7d21b8cc28fe8b5\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:3c53b221378b035ae2f1881cbc3aca42a6075a8e90e1a342c2f205eb1d1aa6a1\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:c384c838b34d1b67068e51b5bbe49caa6aa3633acd158f1ab16b5da8d226bc53\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-win32.whl\", hash = \"sha256:19ea69e41c3565932aa28a202d1875ec56786aea46a2eab54a3b28e8a27f9517\"},\n+    {file = \"grpcio_tools-1.62.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:1d768a5c07279a4c461ebf52d0cec1c6ca85c6291c71ec2703fe3c3e7e28e8c4\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-linux_armv7l.whl\", hash = \"sha256:5b07b5874187e170edfbd7aa2ca3a54ebf3b2952487653e8c0b0d83601c33035\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-macosx_10_10_universal2.whl\", hash = \"sha256:d58389fe8be206ddfb4fa703db1e24c956856fcb9a81da62b13577b3a8f7fda7\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-manylinux_2_17_aarch64.whl\", hash = \"sha256:7d8b4e00c3d7237b92260fc18a561cd81f1da82e8be100db1b7d816250defc66\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1fe08d2038f2b7c53259b5c49e0ad08c8e0ce2b548d8185993e7ef67e8592cca\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:19216e1fb26dbe23d12a810517e1b3fbb8d4f98b1a3fbebeec9d93a79f092de4\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:b8574469ecc4ff41d6bb95f44e0297cdb0d95bade388552a9a444db9cd7485cd\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:4f6f32d39283ea834a493fccf0ebe9cfddee7577bdcc27736ad4be1732a36399\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-win32.whl\", hash = \"sha256:76eb459bdf3fb666e01883270beee18f3f11ed44488486b61cd210b4e0e17cc1\"},\n+    {file = \"grpcio_tools-1.62.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:217c2ee6a7ce519a55958b8622e21804f6fdb774db08c322f4c9536c35fdce7c\"},\n+]\n+\n+[package.dependencies]\n+grpcio = \">=1.62.2\"\n+protobuf = \">=4.21.6,<5.0dev\"\n+setuptools = \"*\"\n+\n [[package]]\n name = \"h11\"\n version = \"0.14.0\"\n@@ -1855,6 +1923,32 @@ files = [\n     {file = \"h11-0.14.0.tar.gz\", hash = \"sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d\"},\n ]\n \n+[[package]]\n+name = \"h2\"\n+version = \"4.1.0\"\n+description = \"HTTP/2 State-Machine based protocol implementation\"\n+optional = true\n+python-versions = \">=3.6.1\"\n+files = [\n+    {file = \"h2-4.1.0-py3-none-any.whl\", hash = \"sha256:03a46bcf682256c95b5fd9e9a99c1323584c3eec6440d379b9903d709476bc6d\"},\n+    {file = \"h2-4.1.0.tar.gz\", hash = \"sha256:a83aca08fbe7aacb79fec788c9c0bac936343560ed9ec18b82a13a12c28d2abb\"},\n+]\n+\n+[package.dependencies]\n+hpack = \">=4.0,<5\"\n+hyperframe = \">=6.0,<7\"\n+\n+[[package]]\n+name = \"hpack\"\n+version = \"4.0.0\"\n+description = \"Pure-Python HPACK header compression\"\n+optional = true\n+python-versions = \">=3.6.1\"\n+files = [\n+    {file = \"hpack-4.0.0-py3-none-any.whl\", hash = \"sha256:84a076fad3dc9a9f8063ccb8041ef100867b1878b25ef0ee63847a5d53818a6c\"},\n+    {file = \"hpack-4.0.0.tar.gz\", hash = \"sha256:fc41de0c63e687ebffde81187a948221294896f6bdc0ae2312708df339430095\"},\n+]\n+\n [[package]]\n name = \"htmldate\"\n version = \"1.8.1\"\n@@ -1926,6 +2020,7 @@ files = [\n [package.dependencies]\n anyio = \"*\"\n certifi = \"*\"\n+h2 = {version = \">=3,<5\", optional = true, markers = \"extra == \\\"http2\\\"\"}\n httpcore = \"==1.*\"\n idna = \"*\"\n sniffio = \"*\"\n@@ -1981,6 +2076,17 @@ testing = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gr\n torch = [\"safetensors\", \"torch\"]\n typing = [\"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\"]\n \n+[[package]]\n+name = \"hyperframe\"\n+version = \"6.0.1\"\n+description = \"HTTP/2 framing layer for Python\"\n+optional = true\n+python-versions = \">=3.6.1\"\n+files = [\n+    {file = \"hyperframe-6.0.1-py3-none-any.whl\", hash = \"sha256:0ec6bafd80d8ad2195c4f03aacba3a8265e57bc4cff261e802bf39970ed02a15\"},\n+    {file = \"hyperframe-6.0.1.tar.gz\", hash = \"sha256:ae510046231dc8e9ecb1a6586f63d2347bf4c8905914aa84ba585ae85f28a914\"},\n+]\n+\n [[package]]\n name = \"identify\"\n version = \"2.5.36\"\n@@ -3787,6 +3893,25 @@ files = [\n dev = [\"pre-commit\", \"tox\"]\n testing = [\"pytest\", \"pytest-benchmark\"]\n \n+[[package]]\n+name = \"portalocker\"\n+version = \"2.8.2\"\n+description = \"Wraps the portalocker recipe for easy usage\"\n+optional = true\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"portalocker-2.8.2-py3-none-any.whl\", hash = \"sha256:cfb86acc09b9aa7c3b43594e19be1345b9d16af3feb08bf92f23d4dce513a28e\"},\n+    {file = \"portalocker-2.8.2.tar.gz\", hash = \"sha256:2b035aa7828e46c58e9b31390ee1f169b98e1066ab10b9a6a861fe7e25ee4f33\"},\n+]\n+\n+[package.dependencies]\n+pywin32 = {version = \">=226\", markers = \"platform_system == \\\"Windows\\\"\"}\n+\n+[package.extras]\n+docs = [\"sphinx (>=1.7.1)\"]\n+redis = [\"redis\"]\n+tests = [\"pytest (>=5.4.1)\", \"pytest-cov (>=2.8.1)\", \"pytest-mypy (>=0.8.0)\", \"pytest-timeout (>=2.1.0)\", \"redis\", \"sphinx (>=6.0.0)\", \"types-redis\"]\n+\n [[package]]\n name = \"pprintpp\"\n version = \"0.4.0\"\n@@ -4597,6 +4722,7 @@ files = [\n     {file = \"PyYAML-6.0.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9\"},\n+    {file = \"PyYAML-6.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4\"},\n     {file = \"PyYAML-6.0.1-cp312-cp312-win32.whl\", hash = \"sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54\"},\n@@ -4645,6 +4771,32 @@ files = [\n [package.dependencies]\n pyyaml = \"*\"\n \n+[[package]]\n+name = \"qdrant-client\"\n+version = \"1.9.1\"\n+description = \"Client library for the Qdrant vector search engine\"\n+optional = true\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"qdrant_client-1.9.1-py3-none-any.whl\", hash = \"sha256:b9b7e0e5c1a51410d8bb5106a869a51e12f92ab45a99030f27aba790553bd2c8\"},\n+    {file = \"qdrant_client-1.9.1.tar.gz\", hash = \"sha256:186b9c31d95aefe8f2db84b7746402d7365bd63b305550e530e31bde2002ce79\"},\n+]\n+\n+[package.dependencies]\n+grpcio = \">=1.41.0\"\n+grpcio-tools = \">=1.41.0\"\n+httpx = {version = \">=0.20.0\", extras = [\"http2\"]}\n+numpy = [\n+    {version = \">=1.21\", markers = \"python_version >= \\\"3.8\\\" and python_version < \\\"3.12\\\"\"},\n+    {version = \">=1.26\", markers = \"python_version >= \\\"3.12\\\"\"},\n+]\n+portalocker = \">=2.7.0,<3.0.0\"\n+pydantic = \">=1.10.8\"\n+urllib3 = \">=1.26.14,<3\"\n+\n+[package.extras]\n+fastembed = [\"fastembed (==0.2.6)\"]\n+\n [[package]]\n name = \"readme-renderer\"\n version = \"43.0\"\n@@ -6369,7 +6521,7 @@ docs = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"rst.link\n testing = [\"big-O\", \"jaraco.functools\", \"jaraco.itertools\", \"more-itertools\", \"pytest (>=6)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=2.2)\", \"pytest-ignore-flaky\", \"pytest-mypy\", \"pytest-ruff (>=0.2.1)\"]\n \n [extras]\n-all = [\"anthropic\", \"beautifulsoup4\", \"boto3\", \"cohere\", \"duckduckgo-search\", \"elevenlabs\", \"filetype\", \"google-generativeai\", \"mail-parser\", \"markdownify\", \"marqo\", \"ollama\", \"opensearch-py\", \"pandas\", \"pgvector\", \"pillow\", \"pinecone-client\", \"playwright\", \"psycopg2-binary\", \"pusher\", \"pymongo\", \"pypdf\", \"redis\", \"snowflake-sqlalchemy\", \"sqlalchemy-redshift\", \"torch\", \"trafilatura\", \"transformers\", \"voyageai\"]\n+all = [\"anthropic\", \"beautifulsoup4\", \"boto3\", \"cohere\", \"duckduckgo-search\", \"elevenlabs\", \"filetype\", \"google-generativeai\", \"mail-parser\", \"markdownify\", \"marqo\", \"ollama\", \"opensearch-py\", \"pandas\", \"pgvector\", \"pillow\", \"pinecone-client\", \"playwright\", \"psycopg2-binary\", \"pusher\", \"pymongo\", \"pypdf\", \"qdrant-client\", \"redis\", \"snowflake-sqlalchemy\", \"sqlalchemy-redshift\", \"torch\", \"trafilatura\", \"transformers\", \"voyageai\"]\n drivers-embedding-amazon-bedrock = [\"boto3\"]\n drivers-embedding-amazon-sagemaker = [\"boto3\"]\n drivers-embedding-cohere = [\"cohere\"]\n@@ -6399,6 +6551,7 @@ drivers-vector-mongodb = [\"pymongo\"]\n drivers-vector-opensearch = [\"opensearch-py\"]\n drivers-vector-pinecone = [\"pinecone-client\"]\n drivers-vector-postgresql = [\"pgvector\", \"psycopg2-binary\"]\n+drivers-vector-qdrant = [\"qdrant-client\"]\n drivers-vector-redis = [\"redis\"]\n drivers-web-scraper-markdownify = [\"beautifulsoup4\", \"markdownify\", \"playwright\"]\n drivers-web-scraper-trafilatura = [\"trafilatura\"]\n@@ -6412,4 +6565,4 @@ loaders-pdf = [\"pypdf\"]\n [metadata]\n lock-version = \"2.0\"\n python-versions = \"^3.9\"\n-content-hash = \"937d5e870407a493038178b3060c663feb9a1b6e6fc16e55fa380a18d02b0c80\"\n+content-hash = \"d4a00633119a6b9616fc0ea31ae354f806d1fc363e4a930fa912d2d710aa8938\"\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 7fc62d6af..da32484a2 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -52,6 +52,7 @@ markdownify = {version = \"^0.11.6\", optional = true}\n voyageai = {version = \"^0.2.1\", optional = true}\n elevenlabs = {version = \"^1.1.2\", optional = true}\n torch = {version = \"^2.3.0\", optional = true}\n+qdrant-client = { version = \">=1.9.1\", optional = true }\n pusher = {version = \"^3.3.2\", optional = true}\n ollama = {version = \"^0.2.1\", optional = true} \n duckduckgo-search = {version = \"^6.1.6\", optional = true}\n@@ -87,6 +88,7 @@ drivers-vector-redis = [\"redis\"]\n drivers-vector-opensearch = [\"opensearch-py\"]\n drivers-vector-amazon-opensearch = [\"opensearch-py\", \"boto3\"]\n drivers-vector-postgresql = [\"pgvector\", \"psycopg2-binary\"]\n+drivers-vector-qdrant = [\"qdrant-client\"]\n \n drivers-embedding-amazon-bedrock = [\"boto3\"]\n drivers-embedding-amazon-sagemaker = [\"boto3\"]\n@@ -124,6 +126,7 @@ all = [\n     \"snowflake\",\n     \"marqo\",\n     \"pinecone-client\",\n+    \"qdrant-client\",\n     \"pymongo\",\n     \"redis\",\n     \"opensearch-py\",\n", "instance_id": "griptape-ai__griptape-928", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement for integrating Griptape with Qdrant VectorDB is mostly clear, with a well-defined goal of enhancing vector data handling capabilities for AI and machine learning workflows. The description outlines the problem of inefficient vector data management and proposes a solution through integration with Qdrant, highlighting key features like seamless ingestion, advanced processing, efficient indexing, and scalability. Additionally, it provides alternatives considered and includes a detailed example code snippet with outputs, which aids in understanding the intended functionality. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not explicitly define specific input/output formats or constraints for the integration (e.g., supported vector dimensions, payload size limits, or performance benchmarks). Edge cases, such as handling connection failures to the Qdrant server or large-scale data ingestion failures, are not addressed in the description. Furthermore, while the \"Special Note\" mentions batch processing as a work in progress, it lacks clarity on how this impacts the current implementation or future expectations. Overall, the statement is valid and clear but misses some critical details for a fully comprehensive understanding.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to the combination of technical concepts and the scope of code changes required. Here's a breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes involve adding a new driver (`QdrantVectorStoreDriver`) to the Griptape framework, which includes creating a new file with a comprehensive implementation (over 200 lines of code) for interacting with the Qdrant vector database. The changes also update documentation, project dependencies (via `pyproject.toml` and `poetry.lock`), and the changelog. While the modifications are focused primarily on a single new module, they require integration with existing Griptape components (e.g., embedding drivers) and affect the broader project structure through dependency management. This indicates a moderate impact on the codebase, though not a fundamental architectural change.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding several technical concepts, including vector databases (specifically Qdrant\u2019s API for upserting, querying, and deleting vectors), embeddings (integration with HuggingFace models for vector generation), and Python-specific libraries and patterns (e.g., `attrs` for class definition, optional dependencies via `import_optional_dependency`). Additionally, familiarity with gRPC and HTTP configurations for Qdrant client setup, as well as handling metadata and payloads, is necessary. These concepts are moderately complex, especially for someone unfamiliar with vector search technologies or distributed database interactions, but they are well-documented and do not require extremely advanced domain-specific knowledge.\n\n3. **Potential Edge Cases and Error Handling**: The provided code includes basic error handling (e.g., try-except blocks in the example script for upserting vectors), and the driver implementation supports configurable timeouts and connection options. However, the problem statement and code do not explicitly address complex edge cases such as network failures, Qdrant server unavailability, or handling very large datasets beyond the mention of batch processing being in progress. The implementation would likely need additional robustness for production use, but the current scope suggests a moderate level of complexity in error handling.\n\n4. **Overall Assessment**: Given the need to understand multiple concepts (vector databases, embeddings, and Griptape\u2019s driver architecture) and implement a new feature across several files (driver code, documentation, dependencies), this task is more than a simple bug fix or feature addition but does not reach the level of deep architectural refactoring or highly intricate logic. It requires a solid grasp of Python and some domain knowledge of vector search, placing it in the medium difficulty range. A score of 0.55 reflects this balance, acknowledging the moderate complexity of integrating a new external system while recognizing that the task does not involve advanced system-level challenges or extensive performance optimization at this stage.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Opener registration error for files in same parent directory\n## Expected behavior and actual behavior.\r\n\r\nI get an ``OpenerRegistrationError`` with nested ``rasterio.open()`` statements using the ``opener`` argument, where the files are different, but in the same parent directory.  \r\n\r\n```\r\n...\r\nFile rasterio/_vsiopener.pyx:294, in _opener_registration()\r\n\r\nOpenerRegistrationError: Opener already registered for urlpath.\r\n```\r\n\r\n## Steps to reproduce the problem.\r\n\r\nE.g. this should raise the error:\r\n\r\n``` python\r\nimport io\r\nimport rasterio as rio\r\n\r\nwith rio.open(\"tests/data/RGB.byte.tif\", opener=io.open) as im1:\r\n    with rio.open(\"tests/data/RGB2.byte.tif\", opener=io.open) as im2:\r\n        pass\r\n```\r\n\r\n#### Environment Information\r\n\r\n```\r\n  rasterio: 1.4a3\r\n      GDAL: 3.8.4\r\n      PROJ: 9.3.1\r\n      GEOS: 3.11.1\r\n PROJ DATA: /home/dugalh/python-venv/py311-geospatial/lib/python3.11/site-packages/rasterio/proj_data\r\n GDAL DATA: /home/dugalh/python-venv/py311-geospatial/lib/python3.11/site-packages/rasterio/gdal_data\r\n\r\nSystem:\r\n    python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\r\nexecutable: /home/dugalh/python-venv/py311-geospatial/bin/python3.11\r\n   machine: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n\r\nPython deps:\r\n    affine: 2.4.0\r\n     attrs: 23.1.0\r\n   certifi: 2023.07.22\r\n     click: 8.1.7\r\n     cligj: 0.7.2\r\n    cython: None\r\n     numpy: 1.26.0\r\nclick-plugins: None\r\nsetuptools: 68.2.2\r\n```\r\n\r\n## Installation Method\r\n\r\nPyPI wheel installed with pip 22.0.2.\n", "patch": "diff --git a/CHANGES.txt b/CHANGES.txt\nindex 1f5741af2..7c8668acf 100644\n--- a/CHANGES.txt\n+++ b/CHANGES.txt\n@@ -6,6 +6,12 @@ Next (TBD)\n \n Bug fixes:\n \n+- The Pyopener registry and VSI plugin have been rewritten to avoid filename\n+  conflicts and to be compatible with multithreading. Now, a new plugin handler\n+  is registered for each instance of using an opener (#3113). Before GDAL 3.9.0\n+  plugin handlers cannot not be removed and so it may be observed that the size\n+  of the Pyopener registry grows during the execution of a program.\n+- A CSLConstList ctypedef has been added and is used where appropriate (#3113).\n - Missing parentheses in the denominators of the max_pixels calculation in\n   calc() and merge() have been added (#3073, #3076). This constrains memory\n   use as originally intended.\ndiff --git a/rasterio/_env.pyx b/rasterio/_env.pyx\nindex 288fa0766..e8b824dd6 100644\n--- a/rasterio/_env.pyx\n+++ b/rasterio/_env.pyx\n@@ -20,7 +20,6 @@ import threading\n from rasterio._err import CPLE_BaseError\n from rasterio._err cimport exc_wrap_ogrerr, exc_wrap_int\n from rasterio._filepath cimport install_filepath_plugin\n-from rasterio._vsiopener cimport install_pyopener_plugin\n from rasterio._version import gdal_version\n \n from libc.stdio cimport stderr\n@@ -378,7 +377,6 @@ cdef class GDALEnv(ConfigEnv):\n                     GDALAllRegister()\n                     OGRRegisterAll()\n                     install_filepath_plugin(filepath_plugin)\n-                    install_pyopener_plugin(pyopener_plugin)\n \n                     if 'GDAL_DATA' in os.environ:\n                         log.debug(\"GDAL_DATA found in environment.\")\ndiff --git a/rasterio/_vsiopener.pxd b/rasterio/_vsiopener.pxd\nindex 983e23b8b..a91d7931c 100644\n--- a/rasterio/_vsiopener.pxd\n+++ b/rasterio/_vsiopener.pxd\n@@ -1,4 +1,1 @@\n include \"gdal.pxi\"\n-\n-cdef int install_pyopener_plugin(VSIFilesystemPluginCallbacksStruct *callbacks_struct)\n-cdef void uninstall_pyopener_plugin(VSIFilesystemPluginCallbacksStruct *callbacks_struct)\ndiff --git a/rasterio/_vsiopener.pyx b/rasterio/_vsiopener.pyx\nindex dde27f3d9..7ddbaa069 100644\n--- a/rasterio/_vsiopener.pyx\n+++ b/rasterio/_vsiopener.pyx\n@@ -1,18 +1,15 @@\n-# cython: language_level=3, boundscheck=False\n-# distutils: language = c++\n \"\"\"Bridge between Python file openers and GDAL VSI.\n \n Based on _filepath.pyx.\n \"\"\"\n \n-include \"gdal.pxi\"\n-\n import contextlib\n from contextvars import ContextVar\n import logging\n import os\n from pathlib import Path\n import stat\n+from uuid import uuid4\n \n from libc.string cimport memcpy\n \n@@ -20,10 +17,7 @@ from rasterio.errors import OpenerRegistrationError\n \n log = logging.getLogger(__name__)\n \n-# Prefix for all in-memory paths used by GDAL's VSI system\n-# Except for errors and log messages this shouldn't really be seen by the user\n-cdef str PREFIX = \"/vsiriopener/\"\n-cdef bytes PREFIX_BYTES = PREFIX.encode(\"utf-8\")\n+cdef str VSI_NS_ROOT = \"vsiriopener\"\n \n # This is global state for the Python filesystem plugin. It currently only\n # contains path -> PyOpenerBase (or subclass) instances. This is used by\n@@ -36,38 +30,12 @@ _OPEN_FILE_EXIT_STACKS = ContextVar(\"open_file_exit_stacks\")\n _OPEN_FILE_EXIT_STACKS.set({})\n \n \n-cdef int install_pyopener_plugin(VSIFilesystemPluginCallbacksStruct *callbacks_struct):\n-    \"\"\"Install handlers for python file openers if it isn't already installed.\"\"\"\n-    cdef char **registered_prefixes = VSIGetFileSystemsPrefixes()\n-    cdef int prefix_index = CSLFindString(registered_prefixes, PREFIX_BYTES)\n-    CSLDestroy(registered_prefixes)\n-\n-    if prefix_index < 0:\n-        log.debug(\"Installing Python opener handler plugin...\")\n-        callbacks_struct = VSIAllocFilesystemPluginCallbacksStruct()\n-        callbacks_struct.open = <VSIFilesystemPluginOpenCallback>pyopener_open\n-        callbacks_struct.eof = <VSIFilesystemPluginEofCallback>pyopener_eof\n-        callbacks_struct.tell = <VSIFilesystemPluginTellCallback>pyopener_tell\n-        callbacks_struct.seek = <VSIFilesystemPluginSeekCallback>pyopener_seek\n-        callbacks_struct.read = <VSIFilesystemPluginReadCallback>pyopener_read\n-        callbacks_struct.write = <VSIFilesystemPluginWriteCallback>pyopener_write\n-        callbacks_struct.flush = <VSIFilesystemPluginFlushCallback>pyopener_flush\n-        callbacks_struct.close = <VSIFilesystemPluginCloseCallback>pyopener_close\n-        callbacks_struct.read_dir = <VSIFilesystemPluginReadDirCallback>pyopener_read_dir\n-        callbacks_struct.stat = <VSIFilesystemPluginStatCallback>pyopener_stat\n-        callbacks_struct.pUserData = <void*>_OPENER_REGISTRY\n-        retval = VSIInstallPluginHandler(PREFIX_BYTES, callbacks_struct)\n-        VSIFreeFilesystemPluginCallbacksStruct(callbacks_struct)\n-        return retval\n-    else:\n-        return 0\n-\n-\n-cdef void uninstall_pyopener_plugin(VSIFilesystemPluginCallbacksStruct *callbacks_struct):\n-    if callbacks_struct is not NULL:\n-        callbacks_struct.pUserData = NULL\n-        VSIFreeFilesystemPluginCallbacksStruct(callbacks_struct)\n-    callbacks_struct = NULL\n+# When an opener is registered for a path, this structure captures the\n+# path and unique registration instance. VSI stat, read_dir, and open\n+# calls have access to the struct instance.\n+cdef struct FSData:\n+    char *path\n+    char *uuid\n \n \n cdef int pyopener_stat(\n@@ -77,14 +45,20 @@ cdef int pyopener_stat(\n     int nFlags\n ) with gil:\n     \"\"\"Provides POSIX stat data to GDAL from a Python filesystem.\"\"\"\n-    # Convert the given filename to a registry key.\n-    # Reminder: openers are registered by URI scheme, authority, and \n-    # *directory* path.\n+    cdef FSData *fsdata = <FSData *>pUserData\n+    path = fsdata.path.decode(\"utf-8\")\n+    uuid = fsdata.uuid.decode(\"utf-8\")\n+    key = (Path(path), uuid)\n     urlpath = pszFilename.decode(\"utf-8\")\n-    key = Path(urlpath).parent\n \n     registry = _OPENER_REGISTRY.get()\n-    log.debug(\"Looking up opener in pyopener_stat: registry=%r, key=%r\", registry, key)\n+    log.debug(\n+        \"Looking up opener in pyopener_stat: urlpath=%r, registry=%r, key=%r\",\n+        urlpath,\n+        registry,\n+        key\n+    )\n+\n     try:\n         file_opener = registry[key]\n     except KeyError as err:\n@@ -94,15 +68,15 @@ cdef int pyopener_stat(\n \n     try:\n         if file_opener.isfile(urlpath):\n-            fmode = 0o170000 | stat.S_IFREG\n+            fmode = stat.S_IFREG\n         elif file_opener.isdir(urlpath):\n-            fmode = 0o170000 | stat.S_IFDIR\n+            fmode = stat.S_IFDIR\n         else:\n             # No such file or directory.\n             return -1\n         size = file_opener.size(urlpath)\n         mtime = file_opener.mtime(urlpath)\n-    except (FileNotFoundError, KeyError):\n+    except (FileNotFoundError, KeyError) as err:\n         # No such file or directory.\n         return -1\n     except Exception as err:\n@@ -116,17 +90,64 @@ cdef int pyopener_stat(\n     return 0\n \n \n+cdef int pyopener_unlink(\n+    void *pUserData,\n+    const char *pszFilename,\n+) with gil:\n+    \"\"\"Unlink a file from a Python filesystem.\"\"\"\n+    cdef FSData *fsdata = <FSData *>pUserData\n+    path = fsdata.path.decode(\"utf-8\")\n+    uuid = fsdata.uuid.decode(\"utf-8\")\n+    key = (Path(path), uuid)\n+    urlpath = pszFilename.decode(\"utf-8\")\n+\n+    registry = _OPENER_REGISTRY.get()\n+    log.debug(\n+        \"Looking up opener in pyopener_unlink: urlpath=%r, registry=%r, key=%r\",\n+        urlpath,\n+        registry,\n+        key\n+    )\n+\n+    try:\n+        file_opener = registry[key]\n+    except KeyError as err:\n+        errmsg = f\"Opener not found: {repr(err)}\".encode(\"utf-8\")\n+        CPLError(CE_Failure, <CPLErrorNum>4, <const char *>\"%s\", <const char *>errmsg)\n+        return -1\n+\n+    try:\n+        file_opener.rm(urlpath)\n+        return 0\n+    except (FileNotFoundError, KeyError) as err:\n+        # No such file or directory.\n+        return -1\n+    except Exception as err:\n+        errmsg = f\"Opener failed to determine file info: {err!r}\".encode(\"utf-8\")\n+        CPLError(CE_Failure, <CPLErrorNum>4, <const char *>\"%s\", <const char *>errmsg)\n+        return -1\n+\n+\n cdef char ** pyopener_read_dir(\n     void *pUserData,\n     const char *pszDirname,\n     int nMaxFiles\n ) with gil:\n     \"\"\"Provides a directory listing to GDAL from a Python filesystem.\"\"\"\n+    cdef FSData *fsdata = <FSData *>pUserData\n+    path = fsdata.path.decode(\"utf-8\")\n+    uuid = fsdata.uuid.decode(\"utf-8\")\n+    key = (Path(path), uuid)\n     urlpath = pszDirname.decode(\"utf-8\")\n-    key = Path(urlpath)\n \n     registry = _OPENER_REGISTRY.get()\n-    log.debug(\"Looking up opener in pyopener_read_dir: registry=%r, key=%r\", registry, key)\n+    log.debug(\n+        \"Looking up opener in pyopener_read_dir: urlpath=%r, registry=%r, key=%r\",\n+        urlpath,\n+        registry,\n+        key\n+    )\n+\n     try:\n         file_opener = registry[key]\n     except KeyError as err:\n@@ -137,8 +158,7 @@ cdef char ** pyopener_read_dir(\n     try:\n         # GDAL wants relative file names.\n         contents = [Path(item).name for item in file_opener.ls(urlpath)]\n-        log.debug(\"Looking for dir contents: urlpath=%r, contents=%r\", urlpath, contents)\n-    except (FileNotFoundError, KeyError):\n+    except (FileNotFoundError, KeyError) as err:\n         # No such file or directory.\n         return NULL\n     except Exception as err:\n@@ -166,12 +186,24 @@ cdef void* pyopener_open(\n     GDAL may call this function multiple times per filename and each\n     result must be seperately seekable.\n     \"\"\"\n+    cdef FSData *fsdata = <FSData *>pUserData\n+    path = fsdata.path.decode(\"utf-8\")\n+    uuid = fsdata.uuid.decode(\"utf-8\")\n+    key = (Path(path), uuid)\n     urlpath = pszFilename.decode(\"utf-8\")\n+\n     mode = pszAccess.decode(\"utf-8\")\n-    key = Path(urlpath).parent\n+    if not \"b\" in mode:\n+        mode += \"b\"\n \n     registry = _OPENER_REGISTRY.get()\n-    log.debug(\"Looking up opener in pyopener_open: registry=%r, key=%r\", registry, key)\n+    log.debug(\n+        \"Looking up opener in pyopener_open: urlpath=%r, registry=%r, key=%r\",\n+        urlpath,\n+        registry,\n+        key\n+    )\n+\n     try:\n         file_opener = registry[key]\n     except KeyError as err:\n@@ -202,7 +234,6 @@ cdef void* pyopener_open(\n     try:\n         file_obj = stack.enter_context(file_obj)\n     except (AttributeError, TypeError) as err:\n-        log.error(\"File object is not a context manager: file_obj=%r\", file_obj)\n         errmsg = f\"Opener failed to open file with arguments ({repr(urlpath)}, {repr(mode)}): {repr(err)}\".encode(\"utf-8\")\n         CPLError(CE_Failure, <CPLErrorNum>4, <const char *>\"%s\", <const char *>errmsg)\n         return NULL\n@@ -210,10 +241,9 @@ cdef void* pyopener_open(\n         errmsg = \"OpenFile didn't resolve\".encode(\"utf-8\")\n         return NULL\n     else:\n-        exit_stacks = _OPEN_FILE_EXIT_STACKS.get()\n+        exit_stacks = _OPEN_FILE_EXIT_STACKS.get({})\n         exit_stacks[file_obj] = stack\n         _OPEN_FILE_EXIT_STACKS.set(exit_stacks)\n-        log.debug(\"Returning: file_obj=%r\", file_obj)\n         return <void *>file_obj\n \n \n@@ -244,7 +274,7 @@ cdef size_t pyopener_read(void *pFile, void *pBuffer, size_t nSize, size_t nCoun\n     cdef int num_bytes = len(python_data)\n     # NOTE: We have to cast to char* first, otherwise Cython doesn't do the conversion properly\n     memcpy(pBuffer, <void*><unsigned char*>python_data, num_bytes)\n-    return <size_t>(num_bytes // nSize)\n+    return <size_t>(num_bytes / nSize)\n \n \n cdef size_t pyopener_write(void *pFile, void *pBuffer, size_t nSize, size_t nCount) with gil:\n@@ -253,12 +283,16 @@ cdef size_t pyopener_write(void *pFile, void *pBuffer, size_t nSize, size_t nCou\n     cdef object file_obj = <object>pFile\n     buffer_len = nSize * nCount\n     cdef unsigned char [:] buff_view = <unsigned char[:buffer_len]>pBuffer\n-    log.debug(\"Writing data: file_obj=%r, buff_view=%r, buffer_len=%r\", file_obj, buff_view, buffer_len)\n+    log.debug(\n+        \"Writing data: file_obj=%r, buff_view=%r, buffer_len=%r\",\n+        file_obj,\n+        buff_view,\n+        buffer_len)\n     try:\n-        num_bytes = file_obj.write(buff_view)\n+        num = file_obj.write(buff_view)\n     except TypeError:\n-        num_bytes = file_obj.write(str(buff_view))\n-    return <size_t>(num_bytes // nSize)\n+        num = file_obj.write(str(buff_view))\n+    return <size_t>(num // nSize)\n \n \n cdef int pyopener_flush(void *pFile) with gil:\n@@ -283,32 +317,82 @@ cdef int pyopener_close(void *pFile) with gil:\n \n @contextlib.contextmanager\n def _opener_registration(urlpath, obj):\n-    key = Path(urlpath).parent\n+    cdef char **registered_prefixes = NULL\n+    cdef int prefix_index = 0\n+    cdef VSIFilesystemPluginCallbacksStruct *callbacks_struct = NULL\n+    cdef FSData fsdata\n+    cdef char *path_c = NULL\n+    cdef char *uuid_c = NULL\n+\n+    # To resolve issue 1406 we add the opener or filesystem id to the\n+    # registry key.\n+    kpath = Path(urlpath).parent\n+    kid = uuid4().hex\n+    key = (kpath, kid)\n+\n+    path_b = kpath.as_posix().encode(\"utf-8\")\n+    path_c = path_b\n+    uuid_b = kid.encode(\"utf-8\")\n+    uuid_c = uuid_b\n+\n+    fsdata = FSData(path_c, uuid_c)\n+\n+    namespace = f\"{VSI_NS_ROOT}_{kid}\"\n+    cdef bytes prefix_bytes = f\"/{namespace}/\".encode(\"utf-8\")\n \n     # Might raise.\n     opener = _create_opener(obj)\n \n-    registry = _OPENER_REGISTRY.get()\n+    registry = _OPENER_REGISTRY.get({})\n+\n     if key in registry:\n         if registry[key] != opener:\n             raise OpenerRegistrationError(f\"Opener already registered for urlpath.\")\n         else:\n             try:\n-                yield f\"{PREFIX}{urlpath}\"\n+                yield f\"/{namespace}/{urlpath}\"\n             finally:\n                 registry = _OPENER_REGISTRY.get()\n                 _ = registry.pop(key, None)\n                 _OPENER_REGISTRY.set(registry)\n+\n     else:\n+        # Install handler.\n+        registered_prefixes = VSIGetFileSystemsPrefixes()\n+        prefix_index = CSLFindString(<CSLConstList>registered_prefixes, prefix_bytes)\n+        CSLDestroy(registered_prefixes)\n+\n+        if prefix_index < 0:\n+            log.debug(\"Installing Python opener handler plugin: prefix_bytes=%r\", prefix_bytes)\n+            callbacks_struct = VSIAllocFilesystemPluginCallbacksStruct()\n+            callbacks_struct.open = <VSIFilesystemPluginOpenCallback>pyopener_open\n+            callbacks_struct.eof = <VSIFilesystemPluginEofCallback>pyopener_eof\n+            callbacks_struct.tell = <VSIFilesystemPluginTellCallback>pyopener_tell\n+            callbacks_struct.seek = <VSIFilesystemPluginSeekCallback>pyopener_seek\n+            callbacks_struct.read = <VSIFilesystemPluginReadCallback>pyopener_read\n+            callbacks_struct.write = <VSIFilesystemPluginWriteCallback>pyopener_write\n+            callbacks_struct.flush = <VSIFilesystemPluginFlushCallback>pyopener_flush\n+            callbacks_struct.close = <VSIFilesystemPluginCloseCallback>pyopener_close\n+            callbacks_struct.read_dir = <VSIFilesystemPluginReadDirCallback>pyopener_read_dir\n+            callbacks_struct.stat = <VSIFilesystemPluginStatCallback>pyopener_stat\n+            callbacks_struct.unlink = <VSIFilesystemPluginUnlinkCallback>pyopener_unlink\n+            callbacks_struct.pUserData = &fsdata\n+            retval = VSIInstallPluginHandler(prefix_bytes, callbacks_struct)\n+            VSIFreeFilesystemPluginCallbacksStruct(callbacks_struct)\n+\n         registry[key] = opener\n         _OPENER_REGISTRY.set(registry)\n+\n         try:\n-            yield f\"{PREFIX}{urlpath}\"\n+            yield f\"/{namespace}/{urlpath}\"\n         finally:\n             registry = _OPENER_REGISTRY.get()\n             _ = registry.pop(key, None)\n             _OPENER_REGISTRY.set(registry)\n \n+            IF (CTE_GDAL_MAJOR_VERSION, CTE_GDAL_MINOR_VERSION) >= (3, 9):\n+                retval = VSIRemovePluginHandler(prefix_bytes)\n+\n \n class _AbstractOpener:\n     \"\"\"Adapts a Python object to the opener interface.\"\"\"\n@@ -385,6 +469,19 @@ class _AbstractOpener:\n             Modification timestamp in seconds.\n         \"\"\"\n         raise NotImplementedError\n+    def rm(self, path):\n+        \"\"\"Remove a resource.\n+\n+        Parameters\n+        ----------\n+        path : str\n+            The identifier/locator for a resource within a filesystem.\n+\n+        Returns\n+        -------\n+        None\n+        \"\"\"\n+        raise NotImplementedError\n     def size(self, path):\n         \"\"\"Get the size, in bytes, of a resource..\n \n@@ -431,14 +528,16 @@ class _FilesystemOpener(_AbstractOpener):\n     def isdir(self, path):\n         return self._obj.isdir(path)\n     def ls(self, path):\n-        return self._obj.ls(path)\n+        # return value of ls() varies between file and zip fsspec filesystems.\n+        return [item if isinstance(item, str) else item[\"filename\"] for item in self._obj.ls(path)]\n     def mtime(self, path):\n         try:\n             mtime = int(self._obj.modified(path).timestamp())\n         except NotImplementedError:\n             mtime = 0\n-        log.debug(\"Modification time: mtime=%r\", mtime)\n         return mtime\n+    def rm(self, path):\n+        return self._obj.rm(path)\n     def size(self, path):\n         return self._obj.size(path)\n \n@@ -451,6 +550,8 @@ class _AltFilesystemOpener(_FilesystemOpener):\n         return self._obj.is_dir(path)\n     def mtime(self, path):\n         return 0\n+    def rm(self, path):\n+        self._obj.remove_file(path)\n     def size(self, path):\n         return self._obj.file_size(path)\n \ndiff --git a/rasterio/gdal.pxi b/rasterio/gdal.pxi\nindex 9033a8cd5..f4487fcda 100644\n--- a/rasterio/gdal.pxi\n+++ b/rasterio/gdal.pxi\n@@ -13,6 +13,10 @@ cdef extern from \"cpl_conv.h\" nogil:\n     const char *CPLFindFile(const char *pszClass, const char *pszBasename)\n \n \n+cdef extern from \"cpl_port.h\":\n+    ctypedef char **CSLConstList\n+\n+\n cdef extern from \"cpl_error.h\" nogil:\n \n     ctypedef enum CPLErr:\n@@ -142,8 +146,13 @@ cdef extern from \"cpl_vsi.h\" nogil:\n     size_t VSIFWriteL(void *buffer, size_t nSize, size_t nCount, VSILFILE *fp)\n     int VSIStatL(const char *pszFilename, VSIStatBufL *psStatBuf)\n \n-cdef extern from \"ogr_srs_api.h\" nogil:\n \n+IF (CTE_GDAL_MAJOR_VERSION, CTE_GDAL_MINOR_VERSION) >= (3, 9):\n+    cdef extern from \"cpl_vsi.h\" nogil:\n+        int VSIRemovePluginHandler(const char*)\n+\n+\n+cdef extern from \"ogr_srs_api.h\" nogil:\n     ctypedef int OGRErr\n     ctypedef void * OGRCoordinateTransformationH\n     ctypedef void * OGRSpatialReferenceH\ndiff --git a/setup.py b/setup.py\nindex e599ac61b..41492a92d 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -190,26 +190,24 @@ def copy_data_tree(datadir, destdir):\n except Exception:\n     pass\n \n-# GDAL 2.3 and newer requires C++11\n-if (gdal_major_version, gdal_minor_version) >= (2, 3):\n-    cpp11_flag = '-std=c++11'\n+cpp11_flag = '-std=c++11'\n \n-    # 'extra_compile_args' may not be defined\n-    eca = cpp_ext_options.get('extra_compile_args', [])\n+# 'extra_compile_args' may not be defined\n+eca = cpp_ext_options.get('extra_compile_args', [])\n \n-    if platform.system() == 'Darwin':\n+if platform.system() == 'Darwin':\n \n-        if cpp11_flag not in eca:\n-            eca.append(cpp11_flag)\n+    if cpp11_flag not in eca:\n+        eca.append(cpp11_flag)\n \n-        eca += [cpp11_flag, '-mmacosx-version-min=10.9', '-stdlib=libc++']\n+    eca += [cpp11_flag, '-mmacosx-version-min=10.9', '-stdlib=libc++']\n \n-    # TODO: Windows\n+# TODO: Windows\n \n-    elif cpp11_flag not in eca:\n-        eca.append(cpp11_flag)\n+elif cpp11_flag not in eca:\n+    eca.append(cpp11_flag)\n \n-    cpp_ext_options['extra_compile_args'] = eca\n+cpp_ext_options['extra_compile_args'] = eca\n \n # Configure optional Cython coverage.\n cythonize_options = {\"language_level\": sys.version_info[0]}\n@@ -238,7 +236,7 @@ def copy_data_tree(datadir, destdir):\n         Extension(\"rasterio._transform\", [\"rasterio/_transform.pyx\"], **ext_options),\n         Extension(\"rasterio._filepath\", [\"rasterio/_filepath.pyx\"], **cpp_ext_options),\n         Extension(\n-            \"rasterio._vsiopener\", [\"rasterio/_vsiopener.pyx\"], **cpp_ext_options\n+            \"rasterio._vsiopener\", [\"rasterio/_vsiopener.pyx\"], **ext_options\n         ),\n     ]\n     ext_modules = cythonize(\n", "instance_id": "rasterio__rasterio-3113", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an `OpenerRegistrationError` occurs when using nested `rasterio.open()` statements with the `opener` argument for files in the same parent directory. It provides a reproducible code snippet and relevant environment information, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the expected behavior is not explicitly detailed beyond stating the error should not occur, and there is no mention of specific edge cases or constraints (e.g., file types, size limits, or threading scenarios beyond the basic reproduction). Additionally, the problem statement does not clarify the desired outcome for multithreading or specific performance expectations. While the issue is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, involving a complete rewrite of the Pyopener registry and VSI plugin in `rasterio/_vsiopener.pyx`, impacting core functionality for handling file openers in a geospatial library. This requires modifying multiple parts of the codebase, including adding new structures (`FSData`), implementing unique identifier-based registration with UUIDs, and ensuring compatibility with multithreading. The changes span across several files (`_env.pyx`, `_vsiopener.pxd`, `_vsiopener.pyx`, `gdal.pxi`, and `setup.py`), indicating a broad impact on the system architecture.\n\nSecond, the technical concepts involved are complex. Solving this requires a deep understanding of Cython (for interfacing Python with C), GDAL's VSI (Virtual File System) plugin system, and multithreading considerations. Knowledge of specific GDAL versions (e.g., handler removal support in GDAL 3.9.0) and POSIX file operations (e.g., `stat`, `unlink`) is necessary. Additionally, the solution involves advanced design patterns like context managers and registry management to avoid filename conflicts.\n\nThird, the problem demands handling potential edge cases, such as concurrent access in multithreaded environments, file system errors, and compatibility with different GDAL versions. The code changes introduce new error handling logic (e.g., for `unlink` and `rm` operations) and modify existing logic to prevent conflicts, which adds to the complexity.\n\nWhile not at the extreme end of difficulty (e.g., requiring novel algorithms or system-level redesign), this problem necessitates a deep understanding of the `rasterio` library's internals, Cython-based extensions, and the underlying GDAL framework. Therefore, a difficulty score of 0.75 reflects the challenging nature of the task, requiring significant expertise and careful implementation to ensure correctness and compatibility.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Move out sdtype validations from multi-column transformers\n### Problem Description\r\n\r\nCurrently, all the sdtype validations for multi-column transformers are defined in their classes.\r\nWe want to move these validations out of the classes into a separate file so that they can be used without accessing the transformers. \r\n\r\n### Expected behavior\r\n\r\n- Create a `_validations.py` file inside `rdt/transformers`\r\n- Move out all the sdtype validations for the multi-column transformer inside this file\r\n- Make the changes so the multi-column transformer usage stays the same. \r\n\n", "patch": "diff --git a/rdt/transformers/_validators.py b/rdt/transformers/_validators.py\nnew file mode 100644\nindex 00000000..5c5f7fd8\n--- /dev/null\n+++ b/rdt/transformers/_validators.py\n@@ -0,0 +1,171 @@\n+\"\"\"Validations for multi-column transformers.\"\"\"\n+import importlib\n+\n+from rdt.errors import TransformerInputError\n+\n+\n+class BaseValidator:\n+    \"\"\"Base validation class.\n+\n+    The validation classes ensure that the input data is compatible with the transformers\n+    and that they can be imported.\n+    \"\"\"\n+\n+    SUPPORTED_SDTYPES = []\n+    VALIDATION_TYPE = None\n+\n+    @classmethod\n+    def _validate_supported_sdtypes(cls, columns_to_sdtypes):\n+        message = ''\n+        for column, sdtype in columns_to_sdtypes.items():\n+            if sdtype not in cls.SUPPORTED_SDTYPES:\n+                message += f\"Column '{column}' has an unsupported sdtype '{sdtype}'.\\n\"\n+\n+        if message:\n+            message += (\n+                f'Please provide a column that is compatible with {cls.VALIDATION_TYPE} data.'\n+            )\n+            raise TransformerInputError(message)\n+\n+    @classmethod\n+    def validate_sdtypes(cls, columns_to_sdtypes):\n+        \"\"\"Validate the columns to sdtypes mapping.\n+\n+        This method aims to call all other sdtype validation method in the class.\n+\n+        Args:\n+            columns_to_sdtypes (dict):\n+                Mapping of column names to sdtypes.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    @classmethod\n+    def validate_imports(cls):\n+        \"\"\"Check that the transformers can be imported.\"\"\"\n+        raise NotImplementedError\n+\n+    @classmethod\n+    def validate(cls, columns_to_sdtypes):\n+        \"\"\"Validate the input data.\n+\n+        Args:\n+            columns_to_sdtypes (dict):\n+                Mapping of column names to sdtypes.\n+        \"\"\"\n+        cls.validate_sdtypes(columns_to_sdtypes)\n+        cls.validate_imports()\n+\n+\n+class AddressValidator(BaseValidator):\n+    \"\"\"Validation class for Address data.\"\"\"\n+\n+    SUPPORTED_SDTYPES = [\n+        'country_code', 'administrative_unit', 'city', 'postcode',\n+        'street_address', 'secondary_address', 'state', 'state_abbr'\n+    ]\n+    VALIDATION_TYPE = 'Address'\n+\n+    @classmethod\n+    def _validate_number_columns(cls, columns_to_sdtypes):\n+        if len(columns_to_sdtypes) > 7:\n+            raise TransformerInputError(\n+                f'{cls.VALIDATION_TYPE} transformers takes up to 7 columns to transform. '\n+                'Please provide address data with valid fields.'\n+            )\n+\n+    @staticmethod\n+    def _validate_uniqueness_sdtype(columns_to_sdtypes):\n+        sdtypes_to_columns = {}\n+        for column, sdtype in columns_to_sdtypes.items():\n+            if sdtype not in sdtypes_to_columns:\n+                sdtypes_to_columns[sdtype] = []\n+\n+            sdtypes_to_columns[sdtype].append(column)\n+\n+        duplicate_fields = {\n+            value: keys for value, keys in sdtypes_to_columns.items() if len(keys) > 1\n+        }\n+\n+        if duplicate_fields:\n+            message = ''\n+            for sdtype, columns in duplicate_fields.items():\n+                to_print = \"', '\".join(columns)\n+                message += f\"Columns '{to_print}' have the same sdtype '{sdtype}'.\\n\"\n+\n+            message += 'Your address data cannot have duplicate fields.'\n+            raise TransformerInputError(message)\n+\n+    @classmethod\n+    def _validate_administrative_unit(cls, columns_to_sdtypes):\n+        num_column_administrative_unit = sum(\n+            1 for itm in columns_to_sdtypes.values() if itm in ['administrative_unit', 'state']\n+        )\n+        if num_column_administrative_unit > 1:\n+            raise TransformerInputError(\n+                f\"The {cls.__name__} can have up to 1 column with sdtype 'state'\"\n+                f\" or 'administrative_unit'. Please provide address data with valid fields.\"\n+            )\n+\n+    @classmethod\n+    def validate_sdtypes(cls, columns_to_sdtypes):\n+        \"\"\"Validate the columns to sdtypes mapping.\"\"\"\n+        cls._validate_supported_sdtypes(columns_to_sdtypes)\n+        cls._validate_number_columns(columns_to_sdtypes)\n+        cls._validate_uniqueness_sdtype(columns_to_sdtypes)\n+        cls._validate_administrative_unit(columns_to_sdtypes)\n+\n+    @classmethod\n+    def validate_imports(cls):\n+        \"\"\"Check that the address transformers can be imported.\"\"\"\n+        error_message = (\n+            'You must have SDV Enterprise with the address add-on to use the address features.'\n+        )\n+\n+        try:\n+            address_module = importlib.import_module('rdt.transformers.address')\n+        except ModuleNotFoundError:\n+            raise ImportError(error_message) from None\n+\n+        required_classes = ['RandomLocationGenerator', 'RegionalAnonymizer']\n+        for class_name in required_classes:\n+            if not hasattr(address_module, class_name):\n+                raise ImportError(error_message)\n+\n+\n+class GPSValidator(BaseValidator):\n+    \"\"\"Validation class for GPS data.\"\"\"\n+\n+    SUPPORTED_SDTYPES = ['latitude', 'longitude']\n+    VALIDATION_TYPE = 'GPS'\n+\n+    @staticmethod\n+    def _validate_uniqueness_sdtype(columns_to_sdtypes):\n+        sdtypes_to_columns = {sdtype: column for column, sdtype in columns_to_sdtypes.items()}\n+        if len(sdtypes_to_columns) != 2:\n+            raise TransformerInputError(\n+                'The GPS columns must have one latitude and on longitude columns sdtypes. '\n+                'Please provide GPS data with valid fields.'\n+            )\n+\n+    @classmethod\n+    def validate_sdtypes(cls, columns_to_sdtypes):\n+        \"\"\"Validate the columns to sdtypes mapping.\"\"\"\n+        cls._validate_supported_sdtypes(columns_to_sdtypes)\n+        cls._validate_uniqueness_sdtype(columns_to_sdtypes)\n+\n+    @classmethod\n+    def validate_imports(cls):\n+        \"\"\"Check that the GPS transformers can be imported.\"\"\"\n+        error_message = (\n+            'You must have SDV Enterprise with the gps add-on to use the GPS features.'\n+        )\n+\n+        try:\n+            gps_module = importlib.import_module('rdt.transformers.gps')\n+        except ModuleNotFoundError:\n+            raise ImportError(error_message) from None\n+\n+        required_classes = ['RandomLocationGenerator', 'MetroAreaAnonymizer', 'GPSNoiser']\n+        for class_name in required_classes:\n+            if not hasattr(gps_module, class_name):\n+                raise ImportError(error_message)\n", "instance_id": "sdv-dev__RDT-779", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to refactor sdtype validations for multi-column transformers into a separate file. The goal is explicitly stated: move validations out of transformer classes into a new `_validations.py` file while maintaining the existing usage of the transformers. The expected behavior is outlined with specific steps, such as creating a new file and moving validations. However, there are minor ambiguities and missing details. For instance, the problem does not specify which multi-column transformers are involved (though the code changes imply Address and GPS), nor does it provide examples of the validations to be moved or how the usage should remain unchanged. Additionally, there is no mention of potential edge cases or constraints regarding the refactoring process. While the intent is clear, these missing details prevent it from being comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The task involves creating a new file (`_validations.py`) and moving existing validation logic from multi-column transformer classes into this file. The provided diff shows a significant amount of code (171 lines) being added to the new file, but it appears to be a direct extraction or reorganization of existing logic rather than writing complex new functionality. The changes are localized to a single new file, and while the problem mentions maintaining transformer usage, the diff does not show modifications to other parts of the codebase. This suggests the impact on the overall architecture is minimal, though some understanding of how the validations integrate with the transformers is required.\n\n2. **Number of Technical Concepts**: The task requires familiarity with Python class structures, inheritance (as seen with `BaseValidator` and its subclasses), and error handling (raising custom exceptions like `TransformerInputError`). It also involves basic dynamic module importing (`importlib.import_module`) to validate dependencies. These concepts are relatively straightforward for an intermediate Python developer. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic data validation logic (e.g., checking supported types, uniqueness) are needed.\n\n3. **Edge Cases and Error Handling**: The code changes include detailed validation logic for edge cases, such as unsupported sdtypes, duplicate fields, and import failures for required modules. However, these are already implemented in the provided diff, suggesting the developer does not need to design new error handling from scratch but rather ensure the moved logic works as intended. The problem statement itself does not explicitly mention edge cases, which slightly increases the difficulty as the developer must infer them from the codebase.\n\n4. **Overall Complexity**: The task is primarily a refactoring exercise, which requires understanding the existing validation logic and ensuring it functions correctly in its new location. While this involves some code organization and testing to confirm unchanged behavior, it does not require deep architectural changes or complex problem-solving. The main challenge lies in correctly extracting and integrating the validation logic without breaking existing functionality, which is a moderate but not overly complex task.\n\nGiven these considerations, a difficulty score of 0.35 is appropriate. It reflects an \"Easy\" task that requires understanding some code logic and making structured modifications, but it does not demand advanced technical expertise or extensive codebase knowledge beyond the immediate context of the validations.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "``statistics`` does not work on a ParquetFile subset?\n**Describe the issue**:\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport pandas as pd\r\nimport fastparquet as fp\r\n\r\nfp.write(filename=\"small_df\", data=pd.DataFrame({'a':[1,2,3]}), row_group_offsets=[0,2], file_scheme=\"hive\")\r\npf = fp.ParquetFile(\"small_df\")\r\n\r\n# Works\r\npf.statistics\r\n{'min': {'a': [1, 3]},\r\n 'max': {'a': [2, 3]},\r\n 'null_count': {'a': [0, 0]},\r\n 'distinct_count': {'a': [None, None]}}\r\n\r\n# Does not work\r\npf[-1].statistics\r\nAttributeError: 'ParquetFile' object has no attribute '_statistics'\r\n\r\n```\r\n\r\nSelecting row groups should still result in a ``ParquetFile`` and ``statistics`` should work?\r\n\r\n**Environment**:\r\n\r\n- Python version: 3.10\r\n- Operating System: Ubutnu\r\n- Install method (conda, pip, source): conda\r\n\n", "patch": "diff --git a/fastparquet/api.py b/fastparquet/api.py\nindex c54e5eb5..3f6f7815 100644\n--- a/fastparquet/api.py\n+++ b/fastparquet/api.py\n@@ -260,7 +260,7 @@ def columns(self):\n \n     @property\n     def statistics(self):\n-        if self._statistics is None:\n+        if not hasattr(self, '_statistics') or self._statistics is None:\n             self._statistics = statistics(self)\n         return self._statistics\n \n", "instance_id": "dask__fastparquet-940", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `statistics` property does not work on a subset of a `ParquetFile` when selecting row groups (e.g., `pf[-1]`). It provides a minimal reproducible example that demonstrates the issue, including the expected behavior and the actual error (`AttributeError`). The goal is implied\u2014to ensure that `statistics` works on subsets of `ParquetFile` objects. However, there are minor ambiguities: the problem does not explicitly define the expected output for the subset case (e.g., should the statistics reflect only the selected row group?), nor does it mention potential edge cases or constraints (e.g., behavior with empty subsets or invalid indices). Additionally, there is no discussion of whether this is a bug or a missing feature, which could impact the solution approach. Despite these minor gaps, the problem is valid and mostly clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The code change provided is minimal, involving a single line modification in the `statistics` property of the `ParquetFile` class to handle cases where the `_statistics` attribute might not exist. This suggests the issue is a simple bug fix related to attribute initialization or access. The scope of the change is limited to a single file and a single property, with no apparent impact on the broader codebase architecture or interactions between modules. The technical concepts involved are basic\u2014understanding Python properties and attribute checking (`hasattr`). There is no indication of complex algorithms, design patterns, or domain-specific knowledge (e.g., deep understanding of Parquet file formats) required beyond the immediate fix. Edge cases and error handling are not explicitly mentioned in the problem statement, and the provided code change does not address any beyond the immediate attribute error. While a complete solution might require minor additional testing (e.g., ensuring statistics are computed correctly for subsets), the overall effort and complexity remain low. Therefore, I assign a difficulty score of 0.25, reflecting an easy problem with straightforward code logic and minimal changes.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update `MetricWrapper` to work with the update/compute from torchmetrics\nCurrently, we cannot compute train metrics because it requires to save all the preds and targets over the epoch.\r\nThe problem also affects the val and test sets, but since they're smaller, the effect is less noticeable.\r\n\r\nTo fix this, the [MetricWrapper](https://graphium-docs.datamol.io/stable/api/graphium.trainer.html#metrics) should have an update method taking the preds and target that calls the underlying self.metrics.update, and the compute method which no longer takes in the pred and target, but instead calls the self.metric.compute\r\n\r\n- [x] Add the `update` to the `MetricWrapper`\r\n- [x] Modify the `MetricWrapper.compute` to work with the update\r\n- [x] How to deal with missing labels??\r\n\r\nAlso, all [TorchMetrics in spaces.py](https://github.com/datamol-io/graphium/blob/41a1172a16d2da1138c87d4e472d36ba5f8d2e8b/graphium/utils/spaces.py#L81) should become their class equivalent rather than functions.\r\n\r\n- [x] Change the `spaces.py` to use classes rather than functions. Make sure the classes get initialized.\n", "patch": "diff --git a/.gitignore b/.gitignore\nindex 41dbc0e45..2afd2115e 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -39,6 +39,7 @@ graphium/data/cache/\n graphium/data/b3lyp/\n graphium/data/PCQM4Mv2/\n graphium/data/PCQM4M/\n+graphium/data/largemix/\n graphium/data/neurips2023/small-dataset/\n graphium/data/neurips2023/large-dataset/\n graphium/data/neurips2023/dummy-dataset/\ndiff --git a/docs/api/graphium.ipu.md b/docs/api/graphium.ipu.md\nindex 2fdf82416..3943e78a4 100644\n--- a/docs/api/graphium.ipu.md\n+++ b/docs/api/graphium.ipu.md\n@@ -22,11 +22,6 @@ Code for adapting to run on IPU\n ::: graphium.ipu.ipu_losses\n \n \n-## IPU Metrics\n-------------\n-::: graphium.ipu.ipu_metrics\n-\n-\n ## IPU Simple Lightning\n ------------\n ::: graphium.ipu.ipu_simple_lightning\ndiff --git a/env.yml b/env.yml\nindex 62bf95a3c..d3a3cbc23 100644\n--- a/env.yml\n+++ b/env.yml\n@@ -31,7 +31,7 @@ dependencies:\n   - cuda-version # works also with CPU-only system.\n   - pytorch >=1.12\n   - lightning >=2.0\n-  - torchmetrics >=0.7.0,<0.11\n+  - torchmetrics\n   - ogb\n   - pytorch_geometric >=2.0 # Use `pyg` for Windows instead of `pytorch_geometric`\n   - wandb\ndiff --git a/expts/hydra-configs/tasks/loss_metrics_datamodule/toymix.yaml b/expts/hydra-configs/tasks/loss_metrics_datamodule/toymix.yaml\nindex 9ac744a52..ceb4e6d69 100644\n--- a/expts/hydra-configs/tasks/loss_metrics_datamodule/toymix.yaml\n+++ b/expts/hydra-configs/tasks/loss_metrics_datamodule/toymix.yaml\n@@ -1,6 +1,8 @@\n # @package _global_\n \n predictor:\n+  target_nan_mask: ignore\n+  multitask_handling: flatten\n   metrics_on_progress_bar:\n     qm9: [\"mae\"]\n     tox21: [\"auroc\"]\n@@ -13,29 +15,31 @@ predictor:\n metrics:\n   qm9: &qm9_metrics\n     - name: mae\n-      metric: mae_ipu\n-      target_nan_mask: null\n+      metric: mae\n+      target_nan_mask: ignore\n       multitask_handling: flatten\n       threshold_kwargs: null\n     - name: pearsonr\n-      metric: pearsonr_ipu\n+      metric: pearsonr\n       threshold_kwargs: null\n-      target_nan_mask: null\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n     - name: r2_score\n-      metric: r2_score_ipu\n-      target_nan_mask: null\n+      metric: r2_score\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n       threshold_kwargs: null\n   tox21:\n     - name: auroc\n-      metric: auroc_ipu\n+      metric: auroc\n       task: binary\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n       threshold_kwargs: null\n     - name: avpr\n-      metric: average_precision_ipu\n+      metric: averageprecision\n       task: binary\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n       threshold_kwargs: null\n     - name: f1 > 0.5\n@@ -44,6 +48,8 @@ metrics:\n       target_to_int: True\n       num_classes: 2\n       average: micro\n+      task: binary\n+      target_nan_mask: ignore\n       threshold_kwargs: &threshold_05\n         operator: greater\n         threshold: 0.5\n@@ -53,6 +59,8 @@ metrics:\n       metric: precision\n       multitask_handling: mean-per-label\n       average: micro\n+      task: binary\n+      target_nan_mask: ignore\n       threshold_kwargs: *threshold_05\n   zinc: *qm9_metrics\n \ndiff --git a/expts/hydra-configs/training/toymix.yaml b/expts/hydra-configs/training/toymix.yaml\nindex 4afcbd56a..dc6c174b9 100644\n--- a/expts/hydra-configs/training/toymix.yaml\n+++ b/expts/hydra-configs/training/toymix.yaml\n@@ -23,4 +23,4 @@ trainer:\n     precision: 16\n     max_epochs: ${constants.max_epochs}\n     min_epochs: 1\n-    check_val_every_n_epoch: 20\n\\ No newline at end of file\n+    check_val_every_n_epoch: 2\n\\ No newline at end of file\ndiff --git a/graphium/config/_loader.py b/graphium/config/_loader.py\nindex 2b9055a83..9596e2eac 100644\n--- a/graphium/config/_loader.py\n+++ b/graphium/config/_loader.py\n@@ -15,7 +15,7 @@\n # Misc\n import os\n from copy import deepcopy\n-from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Type, Union\n+from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Type, Union, Iterable\n \n import joblib\n import mup\n@@ -40,10 +40,10 @@\n from graphium.trainer.metrics import MetricWrapper\n from graphium.trainer.predictor import PredictorModule\n from graphium.utils.command_line_utils import get_anchors_and_aliases, update_config\n+from graphium.trainer.progress_bar import ProgressBarMetrics\n \n # Graphium\n from graphium.utils.mup import set_base_shapes\n-from graphium.utils.spaces import DATAMODULE_DICT, GRAPHIUM_PRETRAINED_MODELS_DICT\n from graphium.utils import fs\n \n \n@@ -111,6 +111,8 @@ def load_datamodule(\n         datamodule: The datamodule used to process and load the data\n     \"\"\"\n \n+    from graphium.utils.spaces import DATAMODULE_DICT # Avoid circular imports with `spaces.py`\n+\n     cfg_data = config[\"datamodule\"][\"args\"]\n \n     # Instanciate the datamodule\n@@ -298,9 +300,6 @@ def load_predictor(\n     accelerator_type: str,\n     featurization: Dict[str, str] = None,\n     task_norms: Optional[Dict[Callable, Any]] = None,\n-    replicas: int = 1,\n-    gradient_acc: int = 1,\n-    global_bs: int = 1,\n ) -> PredictorModule:\n     \"\"\"\n     Defining the predictor module, which handles the training logic from `lightning.LighningModule`\n@@ -326,9 +325,6 @@ def load_predictor(\n         task_levels=task_levels,\n         featurization=featurization,\n         task_norms=task_norms,\n-        replicas=replicas,\n-        gradient_acc=gradient_acc,\n-        global_bs=global_bs,\n         **cfg_pred,\n     )\n \n@@ -345,9 +341,6 @@ def load_predictor(\n             task_levels=task_levels,\n             featurization=featurization,\n             task_norms=task_norms,\n-            replicas=replicas,\n-            gradient_acc=gradient_acc,\n-            global_bs=global_bs,\n             **cfg_pred,\n         )\n \n@@ -384,6 +377,7 @@ def load_trainer(\n     config: Union[omegaconf.DictConfig, Dict[str, Any]],\n     accelerator_type: str,\n     date_time_suffix: str = \"\",\n+    metrics_on_progress_bar: Optional[Iterable[str]] = None,\n ) -> Trainer:\n     \"\"\"\n     Defining the pytorch-lightning Trainer module.\n@@ -449,12 +443,15 @@ def load_trainer(\n             name += f\"_{date_time_suffix}\"\n         trainer_kwargs[\"logger\"] = WandbLogger(name=name, **wandb_cfg)\n \n-    trainer_kwargs[\"callbacks\"] = callbacks\n+    progress_bar_callback = ProgressBarMetrics(metrics_on_progress_bar = metrics_on_progress_bar)\n+    callbacks.append(progress_bar_callback)\n+\n     trainer = Trainer(\n         detect_anomaly=True,\n         strategy=strategy,\n         accelerator=accelerator_type,\n         devices=devices,\n+        callbacks=callbacks,\n         **cfg_trainer[\"trainer\"],\n         **trainer_kwargs,\n     )\n@@ -625,6 +622,8 @@ def get_checkpoint_path(config: Union[omegaconf.DictConfig, Dict[str, Any]]) ->\n     Otherwise, assume it refers to a file in the checkpointing dir.\n     \"\"\"\n \n+    from graphium.utils.spaces import GRAPHIUM_PRETRAINED_MODELS_DICT # Avoid circular imports with `spaces.py`\n+\n     cfg_trainer = config[\"trainer\"]\n \n     path = config.get(\"ckpt_name_for_testing\", \"last.ckpt\")\ndiff --git a/graphium/config/dummy_finetuning_from_gnn.yaml b/graphium/config/dummy_finetuning_from_gnn.yaml\nindex 4de1e79bc..75848c40f 100644\n--- a/graphium/config/dummy_finetuning_from_gnn.yaml\n+++ b/graphium/config/dummy_finetuning_from_gnn.yaml\n@@ -55,7 +55,7 @@ finetuning:\n \n constants:\n   seed: 42\n-  max_epochs: 2\n+  max_epochs: 5\n \n accelerator:\n   float32_matmul_precision: medium\n@@ -64,14 +64,14 @@ accelerator:\n predictor:\n   random_seed: ${constants.seed}\n   optim_kwargs:\n-    lr: 4.e-5\n+    lr: 1.e-3\n   scheduler_kwargs: null\n-  target_nan_mask: null\n+  target_nan_mask: ignore\n   multitask_handling: flatten # flatten, mean-per-label\n   \n   torch_scheduler_kwargs:\n     module_type: WarmUpLinearLR\n-    max_num_epochs: 2\n+    max_num_epochs: 4\n     warmup_epochs: 1\n     verbose: False\n   \n@@ -84,22 +84,22 @@ metrics:\n   lipophilicity_astrazeneca:\n     - name: mae\n       metric: mae\n-      target_nan_mask: null\n+      target_nan_mask: ignore\n       multitask_handling: flatten\n       threshold_kwargs: null\n     - name: spearman\n       metric: spearmanr\n       threshold_kwargs: null\n-      target_nan_mask: null\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n     - name: pearson\n       metric: pearsonr\n       threshold_kwargs: null\n-      target_nan_mask: null\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n     - name: r2_score\n       metric: r2_score\n-      target_nan_mask: null\n+      target_nan_mask: ignore\n       multitask_handling: mean-per-label\n       threshold_kwargs: null\n \n@@ -107,7 +107,7 @@ trainer:\n   seed: ${constants.seed}\n   trainer:\n     precision: 32\n-    max_epochs: 2\n+    max_epochs: 5\n     min_epochs: 1\n     check_val_every_n_epoch: 1\n     accumulate_grad_batches: 1\n@@ -122,12 +122,12 @@ datamodule:\n \n   module_type: \"ADMETBenchmarkDataModule\"\n   args:\n+    processed_graph_data_path: datacache/processed_graph_data/dummy_finetuning_from_gnn\n     # TDC specific\n     tdc_benchmark_names: [lipophilicity_astrazeneca]\n     tdc_train_val_seed: ${constants.seed}\n-    \n-    batch_size_training: 200\n-    batch_size_inference: 200\n+    batch_size_training: 20\n+    batch_size_inference: 20\n     num_workers: 0\n \n     persistent_workers: False\n\\ No newline at end of file\ndiff --git a/graphium/config/dummy_finetuning_from_task_head.yaml b/graphium/config/dummy_finetuning_from_task_head.yaml\nindex 90b0d5341..373bc6e7e 100644\n--- a/graphium/config/dummy_finetuning_from_task_head.yaml\n+++ b/graphium/config/dummy_finetuning_from_task_head.yaml\n@@ -61,7 +61,7 @@ finetuning:\n \n constants:\n   seed: 42\n-  max_epochs: 2\n+  max_epochs: 5\n \n accelerator:\n   float32_matmul_precision: medium\n@@ -70,14 +70,14 @@ accelerator:\n predictor:\n   random_seed: ${constants.seed}\n   optim_kwargs:\n-    lr: 4.e-5\n+    lr: 1.e-3\n   scheduler_kwargs: null\n   target_nan_mask: null\n   multitask_handling: flatten # flatten, mean-per-label\n   \n   torch_scheduler_kwargs:\n     module_type: WarmUpLinearLR\n-    max_num_epochs: 2\n+    max_num_epochs: 5\n     warmup_epochs: 1\n     verbose: False\n   \n@@ -113,7 +113,7 @@ trainer:\n   seed: ${constants.seed}\n   trainer:\n     precision: 32\n-    max_epochs: 2\n+    max_epochs: 4\n     min_epochs: 1\n     check_val_every_n_epoch: 1\n     accumulate_grad_batches: 1\n@@ -128,12 +128,12 @@ datamodule:\n \n   module_type: \"ADMETBenchmarkDataModule\"\n   args:\n+    processed_graph_data_path: datacache/processed_graph_data/dummy_finetuning_task_head\n     # TDC specific\n     tdc_benchmark_names: [lipophilicity_astrazeneca]\n     tdc_train_val_seed: ${constants.seed}\n-    \n-    batch_size_training: 200\n-    batch_size_inference: 200\n+    batch_size_training: 20\n+    batch_size_inference: 20\n     num_workers: 0\n \n     persistent_workers: False\ndiff --git a/graphium/data/datamodule.py b/graphium/data/datamodule.py\nindex e2368d3b3..edc5e207d 100644\n--- a/graphium/data/datamodule.py\n+++ b/graphium/data/datamodule.py\n@@ -845,6 +845,7 @@ def __init__(\n         )\n         IPUDataModuleModifier.__init__(self, **kwargs)\n \n+        self._len = None\n         self.task_specific_args = task_specific_args\n \n         self.task_dataset_processing_params = {}\n@@ -929,6 +930,30 @@ def encode_feature_options(options, name, encoding_function):\n \n         if self._ready_to_load_all_from_file():\n             self._data_is_prepared = True\n+            self._len = self._get_len_from_cached_file()\n+\n+    def _get_len_from_cached_file(self):\n+        if self._ready_to_load_all_from_file():\n+            self._data_is_prepared = True\n+            train_metadata = graphium_cpp.load_metadata_tensors(\n+                        self.processed_graph_data_path, \"train\", self.data_hash\n+                    )\n+            val_metadata = graphium_cpp.load_metadata_tensors(\n+                    self.processed_graph_data_path, \"val\", self.data_hash\n+                )\n+            test_metadata = graphium_cpp.load_metadata_tensors(\n+                    self.processed_graph_data_path, \"test\", self.data_hash\n+                )\n+            length = 0\n+            if len(train_metadata) > 0:\n+                length += len(train_metadata[2])\n+            if len(val_metadata) > 0:\n+                length += len(val_metadata[2])\n+            if len(test_metadata) > 0:\n+                length += len(test_metadata[2])\n+        else:\n+            raise ValueError(\"Data is not prepared. Please call prepare_data() first.\")\n+        return length\n \n     def _parse_caching_args(self, processed_graph_data_path):\n         \"\"\"\n@@ -1102,6 +1127,7 @@ def prepare_data(self):\n                 num_molecules,\n                 split_val=self.task_dataset_processing_params[task].split_val,\n                 split_test=self.task_dataset_processing_params[task].split_test,\n+                sample_idx=sample_idx,\n                 split_seed=self.task_dataset_processing_params[task].seed,\n                 splits_path=self.task_dataset_processing_params[task].splits_path,\n                 split_names=self.task_dataset_processing_params[task].split_names,\n@@ -1138,6 +1164,7 @@ def prepare_data(self):\n             self.explicit_H,\n             self.preprocessing_n_jobs,\n         )\n+        self._len = self._get_len_from_cached_file()\n \n         for task, stats in all_stats.items():\n             if len(stats) < 4:\n@@ -1718,18 +1745,14 @@ def get_data_hash(self):\n \n     def __len__(self) -> int:\n         r\"\"\"\n-        Returns the number of elements of the current DataModule, which is the combined size of all single-task datasets given.\n+        Returns the number of smiles of the current DataModule, which depends on all the smiles from all tasks.\n+        If `prepare_data` is not called, the length is unknown and will raise an error.\n         Returns:\n             num_elements: Number of elements in the current DataModule\n         \"\"\"\n-        num_elements = 0\n-        for task, args in self.task_dataset_processing_params.items():\n-            if args.df is None:\n-                df = self._read_table(args.df_path, usecols=[args.smiles_col])\n-                num_elements += len(df)\n-            else:\n-                num_elements += len(args.df)\n-        return num_elements\n+        if self._len is None:\n+            raise ValueError(\"The length of the dataset is unknown. Please call `prepare_data` first.\")\n+        return self._len\n \n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"\n@@ -2075,6 +2098,12 @@ def __init__(\n             for t in tdc_benchmark_names\n         }\n \n+        # Create a temporary `processed_graph_data_path` to store the processed graphs and labels\n+        if processed_graph_data_path is None:\n+            processed_graph_data_path = fs.join(tdc_cache_dir, \"processed_graph_data\")\n+            if not fs.exists(processed_graph_data_path):\n+                fs.mkdir(processed_graph_data_path)\n+\n         super().__init__(\n             task_specific_args=task_specific_args,\n             featurization=featurization,\ndiff --git a/graphium/finetuning/finetuning.py b/graphium/finetuning/finetuning.py\nindex 97d6d7fc7..59902a921 100644\n--- a/graphium/finetuning/finetuning.py\n+++ b/graphium/finetuning/finetuning.py\n@@ -17,10 +17,10 @@\n from collections import OrderedDict\n \n import torch.nn as nn\n-import pytorch_lightning as pl\n+import lightning.pytorch as pl\n \n from torch.optim.optimizer import Optimizer\n-from pytorch_lightning.callbacks import BaseFinetuning\n+from lightning.pytorch.callbacks import BaseFinetuning\n \n \n class GraphFinetuning(BaseFinetuning):\ndiff --git a/graphium/finetuning/finetuning_architecture.py b/graphium/finetuning/finetuning_architecture.py\nindex 864016141..4b0de1607 100644\n--- a/graphium/finetuning/finetuning_architecture.py\n+++ b/graphium/finetuning/finetuning_architecture.py\n@@ -21,7 +21,6 @@\n \n from graphium.nn.utils import MupMixin\n from graphium.trainer.predictor import PredictorModule\n-from graphium.utils.spaces import FINETUNING_HEADS_DICT\n \n \n class FullGraphFinetuningNetwork(nn.Module, MupMixin):\n@@ -309,6 +308,8 @@ def __init__(self, finetuning_head_kwargs: Dict[str, Any]):\n \n         \"\"\"\n \n+        from graphium.utils.spaces import FINETUNING_HEADS_DICT # Avoiding circular imports with `spaces.py`\n+        \n         super().__init__()\n         self.task = finetuning_head_kwargs.pop(\"task\", None)\n         self.previous_module = finetuning_head_kwargs.pop(\"previous_module\", \"task_heads\")\ndiff --git a/graphium/graphium_cpp/pybind11 b/graphium/graphium_cpp/pybind11\nnew file mode 160000\nindex 000000000..ccefee4c3\n--- /dev/null\n+++ b/graphium/graphium_cpp/pybind11\n@@ -0,0 +1,1 @@\n+Subproject commit ccefee4c3187c2892fcf4590b1bbc850134b84bb\ndiff --git a/graphium/ipu/ipu_metrics.py b/graphium/ipu/ipu_metrics.py\ndeleted file mode 100644\nindex 9029d3e00..000000000\n--- a/graphium/ipu/ipu_metrics.py\n+++ /dev/null\n@@ -1,907 +0,0 @@\n-\"\"\"\n---------------------------------------------------------------------------------\n-Copyright (c) 2023 Valence Labs, Recursion Pharmaceuticals and Graphcore Limited.\n-\n-Use of this software is subject to the terms and conditions outlined in the LICENSE file.\n-Unauthorized modification, distribution, or use is prohibited. Provided 'as is' without\n-warranties of any kind.\n-\n-Valence Labs, Recursion Pharmaceuticals and Graphcore Limited are not liable for any damages arising from its use.\n-Refer to the LICENSE file for the full terms and conditions.\n---------------------------------------------------------------------------------\n-\"\"\"\n-\n-\n-from typing import Optional, Tuple, Sequence, Literal\n-\n-import torch\n-from torch import BoolTensor, IntTensor, Tensor\n-from torchmetrics.functional import auroc, average_precision, pearson_corrcoef, r2_score\n-from torchmetrics.utilities.checks import _input_squeeze\n-from torchmetrics.functional.classification.accuracy import (\n-    _mode,\n-    _check_subset_validity,\n-    _accuracy_compute,\n-    _accuracy_update,\n-)\n-from torchmetrics.functional.classification.precision_recall import _precision_compute, _recall_compute\n-from torchmetrics.functional.classification.f_beta import _fbeta_compute\n-from torchmetrics.functional import mean_squared_error, mean_absolute_error\n-from torchmetrics.utilities.checks import _input_squeeze\n-from torchmetrics.utilities.enums import AverageMethod\n-\n-from graphium.utils.tensor import nan_mean\n-from graphium.ipu.ipu_utils import import_poptorch\n-\n-\n-def auroc_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    num_classes: Optional[int] = None,\n-    task: Optional[Literal[\"binary\", \"multiclass\", \"multilabel\"]] = None,\n-    pos_label: Optional[int] = None,\n-    average: Optional[str] = \"macro\",\n-    max_fpr: Optional[float] = None,\n-    sample_weights: Optional[Sequence] = None,\n-):\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.auroc` that can ignore NaNs\n-    by giving them the same value for both `preds` and `target`.\n-    This allows it to work with compilation\n-    and IPUs since it doesn't modify the tensor's shape.\n-    \"\"\"\n-\n-    target = target.clone()\n-    preds = preds.clone()\n-\n-    # Replace the nan-targets in the preds/target tensors by 0\n-    nan_targets = target.isnan()\n-    preds[nan_targets] = 0.0\n-    target[nan_targets] = 0.0\n-\n-    # Get the original weight matrix. If None, set all weights = 1\n-    if sample_weights is None:\n-        sample_weights = torch.ones(target.shape[0], dtype=preds.dtype, device=preds.device)\n-    sample_weights[nan_targets] = 0.0\n-\n-    # Compute the loss, and rescale by the number of nan elements\n-    score = auroc(\n-        preds=preds,\n-        target=target.to(int),\n-        num_classes=num_classes,\n-        task=task,\n-        pos_label=pos_label,\n-        average=average,\n-        max_fpr=max_fpr,\n-        sample_weights=sample_weights,\n-    )\n-\n-    return score\n-\n-\n-def average_precision_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    num_classes: Optional[int] = None,\n-    task: Optional[Literal[\"binary\", \"multiclass\", \"multilabel\"]] = None,\n-    ignore_index: Optional[int] = None,\n-    pos_label: Optional[int] = None,\n-    average: Optional[str] = \"macro\",\n-    sample_weights: Optional[Sequence] = None,\n-):\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.average_precision` that can ignore NaNs\n-    by giving them the same value for both `preds` and `target`.\n-    This allows it to work with compilation\n-    and IPUs since it doesn't modify the tensor's shape.\n-    \"\"\"\n-\n-    target = target.clone()\n-    preds = preds.clone()\n-\n-    # Replace the nan-targets in the preds/target tensors by 0\n-    # Average precision is not sensitive to true negatives\n-    nan_targets = target.isnan()\n-    preds[nan_targets] = 0.0\n-    target[nan_targets] = 0.0\n-\n-    # No need to use sample weights (which is no longer supported in torchmetrics >=0.10)\n-    # # Get the original weight matrix. If None, set all weights = 1\n-    # if sample_weights is None:\n-    #     sample_weights = torch.ones(target.shape[0], dtype=preds.dtype, device=preds.device)\n-    # sample_weights[nan_targets] = 0.0\n-\n-    # Compute the loss, and rescale by the number of nan elements\n-    score = average_precision(\n-        preds=preds,\n-        target=target,\n-        num_classes=num_classes,\n-        task=task,\n-        ignore_index=ignore_index,\n-        pos_label=pos_label,\n-        average=average,\n-        # sample_weights=sample_weights,\n-    )\n-\n-    return score\n-\n-\n-def precision_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    average: Optional[str] = \"micro\",\n-    mdmc_average: Optional[str] = None,\n-    ignore_index: Optional[int] = None,\n-    num_classes: Optional[int] = None,\n-    threshold: float = 0.5,\n-    top_k: Optional[int] = None,\n-    multiclass: Optional[bool] = None,\n-):\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.precision` that can ignore NaNs\n-    by giving them the same value for both `preds` and `target`.\n-    This allows it to work with compilation\n-    and IPUs since it doesn't modify the tensor's shape.\n-    \"\"\"\n-\n-    (tp, fp, tn, fn), mode = get_confusion_matrix(\n-        preds=preds,\n-        target=target,\n-        average=average,\n-        mdmc_average=mdmc_average,\n-        threshold=threshold,\n-        top_k=top_k,\n-        subset_accuracy=False,\n-        num_classes=num_classes,\n-        multiclass=multiclass,\n-        ignore_index=ignore_index,\n-    )\n-\n-    return _precision_compute(tp, fp, fn, average, mdmc_average)\n-\n-\n-def recall_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    average: Optional[str] = \"micro\",\n-    mdmc_average: Optional[str] = None,\n-    ignore_index: Optional[int] = None,\n-    num_classes: Optional[int] = None,\n-    threshold: float = 0.5,\n-    top_k: Optional[int] = None,\n-    multiclass: Optional[bool] = None,\n-):\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.recall` that can ignore NaNs\n-    by giving them the same value for both `preds` and `target`.\n-    This allows it to work with compilation\n-    and IPUs since it doesn't modify the tensor's shape.\n-    \"\"\"\n-\n-    (tp, fp, tn, fn), mode = get_confusion_matrix(\n-        preds=preds,\n-        target=target,\n-        average=average,\n-        mdmc_average=mdmc_average,\n-        threshold=threshold,\n-        top_k=top_k,\n-        num_classes=num_classes,\n-        multiclass=multiclass,\n-        ignore_index=ignore_index,\n-    )\n-\n-    return _recall_compute(tp, fp, fn, average, mdmc_average)\n-\n-\n-def accuracy_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    average: Optional[str] = \"micro\",\n-    mdmc_average: Optional[str] = \"global\",\n-    threshold: float = 0.5,\n-    top_k: Optional[int] = None,\n-    subset_accuracy: bool = False,\n-    num_classes: Optional[int] = None,\n-    multiclass: Optional[bool] = None,\n-    ignore_index: Optional[int] = None,\n-) -> Tensor:\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.accuracy` that can ignore NaNs\n-    by giving them the same value for both `preds` and `target`.\n-    This allows it to work with compilation\n-    and IPUs since it doesn't modify the tensor's shape.\n-\n-    Args:\n-        preds: Predictions from model (probabilities, logits or labels)\n-        target: Ground truth labels\n-        average:\n-            Defines the reduction that is applied. Should be one of the following:\n-\n-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.\n-            - ``'macro'``: Calculate the metric for each class separately, and average the\n-              metrics across classes (with equal weights for each class).\n-            - ``'weighted'``: Calculate the metric for each class separately, and average the\n-              metrics across classes, weighting each class by its support (``tp + fn``).\n-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return\n-              the metric for every class.\n-            - ``'samples'``: Calculate the metric for each sample, and average the metrics\n-              across samples (with equal weights for each sample).\n-\n-            .. note:: What is considered a sample in the multi-dimensional multi-class case\n-                depends on the value of ``mdmc_average``.\n-\n-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,\n-                the value for the class will be ``nan``.\n-\n-        mdmc_average:\n-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the\n-            ``average`` parameter). Should be one of the following:\n-\n-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.\n-\n-            - ``'samplewise'``: In this case, the statistics are computed separately for each\n-              sample on the ``N`` axis, and then averaged over samples.\n-              The computation for each sample is done by treating the flattened extra axes ``...``\n-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,\n-              and computing the metric for the sample based on that.\n-\n-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs\n-              (see :ref:`pages/classification:input types`)\n-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they\n-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.\n-\n-        num_classes:\n-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.\n-\n-        threshold:\n-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case\n-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.\n-        top_k:\n-            Number of the highest probability or logit score predictions considered finding the correct label,\n-            relevant only for (multi-dimensional) multi-class inputs. The\n-            default value (``None``) will be interpreted as 1 for these inputs.\n-\n-            Should be left at default (``None``) for all other types of inputs.\n-        multiclass:\n-            Used only in certain special cases, where you want to treat inputs as a different type\n-            than what they appear to be. See the parameter's\n-            :ref:`documentation section <pages/classification:using the multiclass parameter>`\n-            for a more detailed explanation and examples.\n-        ignore_index:\n-            Integer specifying a target class to ignore. If given, this class index does not contribute\n-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``\n-            or ``'none'``, the score for the ignored class will be returned as ``nan``.\n-        subset_accuracy:\n-            Whether to compute subset accuracy for multi-label and multi-dimensional\n-            multi-class inputs (has no effect for other input types).\n-\n-            - For multi-label inputs, if the parameter is set to ``True``, then all labels for\n-              each sample must be correctly predicted for the sample to count as correct. If it\n-              is set to ``False``, then all labels are counted separately - this is equivalent to\n-              flattening inputs beforehand (i.e. ``preds = preds.flatten()`` and same for ``target``).\n-\n-            - For multi-dimensional multi-class inputs, if the parameter is set to ``True``, then all\n-              sub-sample (on the extra axis) must be correct for the sample to be counted as correct.\n-              If it is set to ``False``, then all sub-samples are counter separately - this is equivalent,\n-              in the case of label predictions, to flattening the inputs beforehand (i.e.\n-              ``preds = preds.flatten()`` and same for ``target``). Note that the ``top_k`` parameter\n-              still applies in both cases, if set.\n-\n-    Raises:\n-        ValueError:\n-            If ``top_k`` parameter is set for ``multi-label`` inputs.\n-        ValueError:\n-            If ``average`` is none of ``\"micro\"``, ``\"macro\"``, ``\"weighted\"``, ``\"samples\"``, ``\"none\"``, ``None``.\n-        ValueError:\n-            If ``mdmc_average`` is not one of ``None``, ``\"samplewise\"``, ``\"global\"``.\n-        ValueError:\n-            If ``average`` is set but ``num_classes`` is not provided.\n-        ValueError:\n-            If ``num_classes`` is set\n-            and ``ignore_index`` is not in the range ``[0, num_classes)``.\n-        ValueError:\n-            If ``top_k`` is not an ``integer`` larger than ``0``.\n-    \"\"\"\n-\n-    (tp, fp, tn, fn), mode = get_confusion_matrix(\n-        preds=preds,\n-        target=target,\n-        average=average,\n-        mdmc_average=mdmc_average,\n-        threshold=threshold,\n-        top_k=top_k,\n-        subset_accuracy=subset_accuracy,\n-        num_classes=num_classes,\n-        multiclass=multiclass,\n-        ignore_index=ignore_index,\n-    )\n-\n-    return _accuracy_compute(tp, fp, tn, fn, average, mdmc_average, mode)\n-\n-\n-def get_confusion_matrix(\n-    preds: Tensor,\n-    target: Tensor,\n-    average: Optional[str] = \"micro\",\n-    mdmc_average: Optional[str] = \"global\",\n-    threshold: float = 0.5,\n-    top_k: Optional[int] = None,\n-    subset_accuracy: bool = False,\n-    num_classes: Optional[int] = None,\n-    multiclass: Optional[bool] = None,\n-    ignore_index: Optional[int] = None,\n-) -> Tuple[Tuple[Tensor], Tensor]:\n-    \"\"\"\n-    Calculates the confusion matrix according to the specified average method.\n-\n-    Args:\n-        preds: Predictions from model (probabilities, logits or labels)\n-        target: Ground truth labels\n-        average:\n-            Defines the reduction that is applied. Should be one of the following:\n-\n-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.\n-            - ``'macro'``: Calculate the metric for each class separately, and average the\n-              metrics across classes (with equal weights for each class).\n-            - ``'weighted'``: Calculate the metric for each class separately, and average the\n-              metrics across classes, weighting each class by its support (``tp + fn``).\n-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return\n-              the metric for every class.\n-            - ``'samples'``: Calculate the metric for each sample, and average the metrics\n-              across samples (with equal weights for each sample).\n-\n-            .. note:: What is considered a sample in the multi-dimensional multi-class case\n-                depends on the value of ``mdmc_average``.\n-\n-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,\n-                the value for the class will be ``nan``.\n-\n-        mdmc_average:\n-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the\n-            ``average`` parameter). Should be one of the following:\n-\n-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.\n-\n-            - ``'samplewise'``: In this case, the statistics are computed separately for each\n-              sample on the ``N`` axis, and then averaged over samples.\n-              The computation for each sample is done by treating the flattened extra axes ``...``\n-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,\n-              and computing the metric for the sample based on that.\n-\n-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs\n-              (see :ref:`pages/classification:input types`)\n-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they\n-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.\n-\n-        num_classes:\n-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.\n-\n-        threshold:\n-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case\n-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.\n-        top_k:\n-            Number of the highest probability or logit score predictions considered finding the correct label,\n-            relevant only for (multi-dimensional) multi-class inputs. The\n-            default value (``None``) will be interpreted as 1 for these inputs.\n-\n-            Should be left at default (``None``) for all other types of inputs.\n-        multiclass:\n-            Used only in certain special cases, where you want to treat inputs as a different type\n-            than what they appear to be. See the parameter's\n-            :ref:`documentation section <pages/classification:using the multiclass parameter>`\n-            for a more detailed explanation and examples.\n-        ignore_index:\n-            Integer specifying a target class to ignore. If given, this class index does not contribute\n-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``\n-    \"\"\"\n-    allowed_average = [\"micro\", \"macro\", \"weighted\", \"samples\", \"none\", None]\n-    if average not in allowed_average:\n-        raise ValueError(f\"The `average` has to be one of {allowed_average}, got {average}.\")\n-\n-    if average in [\"macro\", \"weighted\", \"none\", None] and (not num_classes or num_classes < 1):\n-        raise ValueError(f\"When you set `average` as {average}, you have to provide the number of classes.\")\n-\n-    allowed_mdmc_average = [None, \"samplewise\", \"global\"]\n-    if mdmc_average not in allowed_mdmc_average:\n-        raise ValueError(f\"The `mdmc_average` has to be one of {allowed_mdmc_average}, got {mdmc_average}.\")\n-\n-    if num_classes and ignore_index is not None and (not ignore_index < num_classes or num_classes == 1):\n-        raise ValueError(\n-            f\"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes\"\n-        )\n-\n-    if top_k is not None and (not isinstance(top_k, int) or top_k <= 0):\n-        raise ValueError(f\"The `top_k` should be an integer larger than 0, got {top_k}\")\n-\n-    #### ADDED ####\n-    # Put all the NaNs as the 0-class\n-    nans = torch.isnan(target)\n-    target[nans] = 0\n-    preds[nans] = 0\n-    if (preds.ndim > 1) and (preds.shape[1] > 1):\n-        preds[nans, 0] = 1\n-    target = target.to(int)\n-    #### END ADDED ####\n-\n-    preds, target = _input_squeeze(preds, target)\n-    mode = _mode(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\n-    reduce = \"macro\" if average in [\"weighted\", \"none\", None] else average\n-\n-    if subset_accuracy and _check_subset_validity(mode):\n-        # correct, total = _subset_accuracy_update(preds, target, threshold, top_k, ignore_index)\n-        # return _subset_accuracy_compute(correct, total)\n-        raise NotImplementedError(\"subset_accuracy not implemented\")\n-    tp, fp, tn, fn = _accuracy_update(\n-        preds, target, reduce, mdmc_average, threshold, num_classes, top_k, multiclass, ignore_index, mode\n-    )\n-\n-    #### ADDED ####\n-    num_nans = nans.sum(0)\n-    if tp.numel() > 1:\n-        tp[0] = tp[0] - num_nans\n-        tn[1:] = tn[1:] - num_nans\n-    else:\n-        tn = tn - num_nans\n-        if (preds.ndim > 1) and (preds.shape[1] > 1):\n-            tp = tp - num_nans\n-    #### END ADDED ####\n-\n-    return (tp, fp, tn, fn), mode\n-\n-\n-class NaNTensor(Tensor):\n-    \"\"\"\n-    Class to create and manage a NaN tensor along it's properties\n-\n-    The goal of the class is to override the regular tensor such that the basic\n-    operations (sum, mean, max, etc) ignore the NaNs in the input.\n-    It also supports NaNs in integer tensors (as the lowest integer possible).\n-    \"\"\"\n-\n-    @property\n-    def get_nans(self) -> BoolTensor:\n-        \"\"\"\n-        Gets the boolean Tensor containing the location of NaNs.\n-        In the case of an integer tensor, this returns where the tensor is equal to its minimal value\n-        In the case of a boolean tensor, this returns a Tensor filled with `False`\n-        \"\"\"\n-        if self.is_floating_point():\n-            return self.isnan()\n-        elif self.is_signed():\n-            return self == torch.iinfo(self.dtype).min\n-        else:\n-            return torch.zeros(self.shape, device=self.device, dtype=bool)\n-\n-    def sum(self, *args, **kwargs) -> Tensor:\n-        \"\"\"\n-        Overloads the traditional sum to ignore the NaNs\n-        \"\"\"\n-        tensor = self.to(float)\n-        tensor[self.get_nans] = float(\"nan\")\n-        if self.is_floating_point():\n-            dtype = self.dtype\n-        else:\n-            dtype = torch.int64\n-        return tensor.nansum(*args, **kwargs).to(dtype)\n-\n-    def mean(self, *args, **kwargs) -> Tensor:\n-        \"\"\"\n-        Overloads the traditional mean to ignore the NaNs\n-        \"\"\"\n-        tensor = self.to(float)\n-        tensor[self.get_nans] = float(\"nan\")\n-        return nan_mean(tensor, *args, **kwargs).to(self.dtype)\n-\n-    def numel(self) -> int:\n-        \"\"\"\n-        Returns the number of non-NaN elements.\n-        \"\"\"\n-        return super(NaNTensor, ~self.get_nans).sum()\n-\n-    def min(self, *args, **kwargs) -> Tensor:\n-        \"\"\"\n-        Returns the min vale of a tensor whitout NaNs\n-        \"\"\"\n-        tensor = self\n-        tensor = tensor[~self.get_nans]\n-        return super(NaNTensor, tensor).min(*args, **kwargs)\n-\n-    def max(self, *args, **kwargs) -> Tensor:\n-        \"\"\"\n-        Returns the max vale of a tensor whitout NaNs\n-        \"\"\"\n-        tensor = self\n-        tensor = tensor[~self.get_nans]\n-        return super(NaNTensor, tensor).max(*args, **kwargs)\n-\n-    def argsort(self, dim=-1, descending=False) -> IntTensor:\n-        \"\"\"\n-        Return the indices that sort the tensor, while putting all the NaNs to the end of the sorting.\n-        \"\"\"\n-        tensor = self\n-        if descending:\n-            tensor[tensor.get_nans] = float(\"-inf\")\n-        else:\n-            tensor[tensor.get_nans] = float(\"inf\")\n-        return super(NaNTensor, tensor).argsort(dim=dim, descending=descending)\n-\n-    def size(self, dim) -> Tensor:\n-        \"\"\"\n-        Instead of returning the size, return the number of non-NaN elements in\n-        a specific dimension. Useful for the `r2_score` metric.\n-        \"\"\"\n-        return (~self.get_nans).sum(dim=dim)\n-\n-    def __lt__(self, other) -> Tensor:\n-        \"\"\"\n-        Stupid fix that allows the code to work with `r2_score`,\n-        since it requires the size to be > 2. But since `self.size` now returns\n-        a Tensor instead of a value, we check that all elements are > 2.\n-        \"\"\"\n-        if (not isinstance(other, Tensor)) and (other == 2):\n-            return super().__lt__(other).all()\n-        else:\n-            return super().__lt__(other)\n-\n-    @classmethod\n-    def __torch_function__(cls, func, types, args=(), kwargs=None):\n-        \"\"\"\n-        This __torch_function__ implementation wraps subclasses such that\n-        methods called on subclasses return a subclass instance instead of\n-        a ``torch.Tensor`` instance.\n-\n-        One corollary to this is that you need coverage for torch.Tensor\n-        methods if implementing __torch_function__ for subclasses.\n-\n-        Affects the call torch.sum() as to behave the same way as NaNTensor.sum()\n-\n-        We recommend always calling ``super().__torch_function__`` as the base\n-        case when doing the above.\n-\n-        While not mandatory, we recommend making `__torch_function__` a classmethod.\n-        \"\"\"\n-        if func.__name__ == \"sum\":\n-            kwargs = {} if kwargs is None else kwargs\n-            return args[0].sum(*args[1:], **kwargs)\n-        else:\n-            return super().__torch_function__(func, types, args=args, kwargs=kwargs)\n-\n-\n-def pearson_ipu(preds, target):\n-    \"\"\"Computes pearson correlation coefficient.\n-\n-    Handles NaNs in the target without reshaping tensors in order to work on IPU.\n-\n-    Args:\n-        preds: estimated scores\n-        target: ground truth scores\n-    \"\"\"\n-    preds = NaNTensor(preds)\n-    target = NaNTensor(target)\n-    preds[target.get_nans] = float(\"nan\")\n-    pearson = pearson_corrcoef(preds, target.to(preds.dtype))\n-    return Tensor(pearson)\n-\n-\n-def spearman_ipu(preds, target):\n-    \"\"\"Computes spearman rank correlation coefficient.\n-\n-    Handles NaNs in the target without reshaping tensors in order to work on IPU.\n-\n-    Args:\n-        preds: estimated scores\n-        target: ground truth scores\n-    \"\"\"\n-    nans = target.isnan()\n-    dtype = preds.dtype\n-    preds[nans] = float(\"inf\")\n-    target[nans] = float(\"inf\")\n-    preds_sort = _rank_data(preds).to(dtype=dtype)\n-    target_sort = _rank_data(target).to(dtype=dtype)\n-    target_sort[nans] = float(\"nan\")\n-    spearman = pearson_ipu(preds_sort, target_sort)\n-    return Tensor(spearman)\n-\n-\n-def _rank_data(data: Tensor) -> Tensor:\n-    \"\"\"Calculate the rank for each element of a tensor.\n-\n-    The rank refers to the indices of an element in the corresponding sorted tensor (starting from 1).\n-    Duplicates of the same value will be assigned the mean of their rank.\n-\n-    Adopted from `Rank of element tensor`_\n-    \"\"\"\n-    n = data.numel()\n-    rank = torch.empty_like(data)\n-    idx = data.argsort()\n-    rank[idx] = torch.arange(1, n + 1, dtype=data.dtype, device=data.device)\n-\n-    # TODO: Repeats not yet supported\n-    # repeats = _find_repeats(data)\n-    # for r in repeats:\n-    #     condition = data == r\n-    #     rank[condition] = rank[condition].mean()\n-    return rank\n-\n-\n-def r2_score_ipu(preds, target, *args, **kwargs) -> Tensor:\n-    \"\"\"\n-    Computes r2 score also known as `R2 Score_Coefficient Determination`_:\n-\n-    .. math:: R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n-\n-    where :math:`SS_{res}=\\sum_i (y_i - f(x_i))^2` is the sum of residual squares, and\n-    :math:`SS_{tot}=\\sum_i (y_i - \\bar{y})^2` is total sum of squares. Can also calculate\n-    adjusted r2 score given by\n-\n-    .. math:: R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\n-\n-    where the parameter :math:`k` (the number of independent regressors) should\n-    be provided as the ``adjusted`` argument.\n-    Handles NaNs without reshaping tensors in order to work on IPU.\n-\n-    Args:\n-        preds: estimated labels\n-        target: ground truth labels\n-        adjusted: number of independent regressors for calculating adjusted r2 score.\n-        multioutput: Defines aggregation in the case of multiple output scores. Can be one of the following strings:\n-\n-            * ``'raw_values'`` returns full set of scores\n-            * ``'uniform_average'`` scores are uniformly averaged\n-            * ``'variance_weighted'`` scores are weighted by their individual variances\n-    \"\"\"\n-    preds = NaNTensor(preds)\n-    target = NaNTensor(target)\n-    preds[target.get_nans] = float(\"nan\")\n-    score = r2_score(preds, target, *args, **kwargs)\n-    return Tensor(score)\n-\n-\n-def fbeta_score_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    beta: float = 1.0,\n-    average: Optional[str] = \"micro\",\n-    mdmc_average: Optional[str] = None,\n-    ignore_index: Optional[int] = None,\n-    num_classes: Optional[int] = None,\n-    threshold: float = 0.5,\n-    top_k: Optional[int] = None,\n-    multiclass: Optional[bool] = None,\n-):\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.classification.f_beta._fbeta_compute`\n-    that can ignore NaNs by giving them the same value for both `preds` and `target`.\n-    This allows it to work with compilation\n-    and IPUs since it doesn't modify the tensor's shape.\n-\n-    Args:\n-        preds: Predictions from model (probabilities, logits or labels)\n-        target: Ground truth labels\n-        average:\n-            Defines the reduction that is applied. Should be one of the following:\n-\n-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.\n-            - ``'macro'``: Calculate the metric for each class separately, and average the\n-              metrics across classes (with equal weights for each class).\n-            - ``'weighted'``: Calculate the metric for each class separately, and average the\n-              metrics across classes, weighting each class by its support (``tp + fn``).\n-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return\n-              the metric for every class.\n-            - ``'samples'``: Calculate the metric for each sample, and average the metrics\n-              across samples (with equal weights for each sample).\n-\n-            .. note:: What is considered a sample in the multi-dimensional multi-class case\n-                depends on the value of ``mdmc_average``.\n-\n-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,\n-                the value for the class will be ``nan``.\n-\n-        mdmc_average:\n-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the\n-            ``average`` parameter). Should be one of the following:\n-\n-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.\n-\n-            - ``'samplewise'``: In this case, the statistics are computed separately for each\n-              sample on the ``N`` axis, and then averaged over samples.\n-              The computation for each sample is done by treating the flattened extra axes ``...``\n-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,\n-              and computing the metric for the sample based on that.\n-\n-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs\n-              (see :ref:`pages/classification:input types`)\n-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they\n-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.\n-\n-        num_classes:\n-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.\n-\n-        threshold:\n-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case\n-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.\n-        top_k:\n-            Number of the highest probability or logit score predictions considered finding the correct label,\n-            relevant only for (multi-dimensional) multi-class inputs. The\n-            default value (``None``) will be interpreted as 1 for these inputs.\n-\n-            Should be left at default (``None``) for all other types of inputs.\n-        multiclass:\n-            Used only in certain special cases, where you want to treat inputs as a different type\n-            than what they appear to be. See the parameter's\n-            :ref:`documentation section <pages/classification:using the multiclass parameter>`\n-            for a more detailed explanation and examples.\n-        ignore_index:\n-            Integer specifying a target class to ignore. If given, this class index does not contribute\n-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``\n-            or ``'none'``, the score for the ignored class will be returned as ``nan``.\n-        subset_accuracy:\n-            Whether to compute subset accuracy for multi-label and multi-dimensional\n-            multi-class inputs (has no effect for other input types).\n-\n-            - For multi-label inputs, if the parameter is set to ``True``, then all labels for\n-              each sample must be correctly predicted for the sample to count as correct. If it\n-              is set to ``False``, then all labels are counted separately - this is equivalent to\n-              flattening inputs beforehand (i.e. ``preds = preds.flatten()`` and same for ``target``).\n-\n-            - For multi-dimensional multi-class inputs, if the parameter is set to ``True``, then all\n-              sub-sample (on the extra axis) must be correct for the sample to be counted as correct.\n-              If it is set to ``False``, then all sub-samples are counter separately - this is equivalent,\n-              in the case of label predictions, to flattening the inputs beforehand (i.e.\n-              ``preds = preds.flatten()`` and same for ``target``). Note that the ``top_k`` parameter\n-              still applies in both cases, if set.\n-\n-    Raises:\n-        ValueError:\n-            If ``top_k`` parameter is set for ``multi-label`` inputs.\n-        ValueError:\n-            If ``average`` is none of ``\"micro\"``, ``\"macro\"``, ``\"weighted\"``, ``\"samples\"``, ``\"none\"``, ``None``.\n-        ValueError:\n-            If ``mdmc_average`` is not one of ``None``, ``\"samplewise\"``, ``\"global\"``.\n-        ValueError:\n-            If ``average`` is set but ``num_classes`` is not provided.\n-        ValueError:\n-            If ``num_classes`` is set\n-            and ``ignore_index`` is not in the range ``[0, num_classes)``.\n-        ValueError:\n-            If ``top_k`` is not an ``integer`` larger than ``0``.\n-    \"\"\"\n-\n-    (tp, fp, tn, fn), mode = get_confusion_matrix(\n-        preds=preds,\n-        target=target,\n-        average=average,\n-        mdmc_average=mdmc_average,\n-        ignore_index=ignore_index,\n-        num_classes=num_classes,\n-        threshold=threshold,\n-        top_k=top_k,\n-        multiclass=multiclass,\n-    )\n-\n-    b2 = beta**2\n-    fbeta = ((1 + b2) * tp) / ((1 + b2) * tp + b2 * fn + fp)\n-\n-    if average in (None, \"none\", AverageMethod.NONE):\n-        pass\n-    elif average == AverageMethod.MICRO:\n-        pass\n-    elif average == AverageMethod.MACRO:\n-        fbeta = fbeta.mean()\n-    elif average == AverageMethod.WEIGHTED:\n-        weights = tp + fn\n-        fbeta = (weights * fbeta).sum() / weights.sum()\n-    else:\n-        raise ValueError(\n-            f\"`average={average}` not yet supported. Chose between None, Micro, Macro, or Weighted\"\n-        )\n-\n-    return fbeta\n-\n-\n-def f1_score_ipu(\n-    preds: Tensor,\n-    target: Tensor,\n-    beta: float = 1.0,\n-    average: Optional[str] = \"micro\",\n-    mdmc_average: Optional[str] = None,\n-    ignore_index: Optional[int] = None,\n-    num_classes: Optional[int] = None,\n-    threshold: float = 0.5,\n-    top_k: Optional[int] = None,\n-    multiclass: Optional[bool] = None,\n-):\n-    \"\"\"\n-    A modified version of the `torchmetrics.functional.classification.f_beta._fbeta_compute`\n-    that can ignore NaNs by giving them the same value for both `preds` and `target`.\n-    Used to calculate the f1_score on IPU with beta parameter equal to 1.0\n-    This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.\n-\n-    Computes f_beta metric from stat scores: true positives, false positives, true negatives, false negatives.\n-\n-    Args:\n-        tp: True positives\n-        fp: False positives\n-        tn: True negatives\n-        fn: False negatives\n-        beta: The parameter `beta` (which determines the weight of recall in the combined score)\n-        ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute\n-            to the returned score, regardless of reduction method\n-        average: Defines the reduction that is applied\n-        mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the\n-            ``average`` parameter)\n-    \"\"\"\n-\n-    return fbeta_score_ipu(\n-        preds,\n-        target,\n-        beta=beta,\n-        average=average,\n-        mdmc_average=mdmc_average,\n-        ignore_index=ignore_index,\n-        num_classes=num_classes,\n-        threshold=threshold,\n-        top_k=top_k,\n-        multiclass=multiclass,\n-    )\n-\n-\n-def mean_squared_error_ipu(preds: Tensor, target: Tensor, squared: bool) -> Tensor:\n-    \"\"\"Computes mean squared error.\n-\n-    Handles NaNs without reshaping tensors in order to work on IPU.\n-\n-    Args:\n-        preds: estimated labels\n-        target: ground truth labels\n-        squared: returns RMSE value if set to False\n-\n-    Return:\n-        Tensor with MSE\n-    \"\"\"\n-    target = target.clone()\n-    preds = preds.clone()\n-\n-    # Replace the nan-targets in the preds/target tensors by 0\n-    nan_targets = target.isnan()\n-    preds[nan_targets] = 0.0\n-    target[nan_targets] = 0.0\n-\n-    # Compute the loss, and rescale by the number of nan elements\n-    loss = mean_squared_error(preds, target, squared)\n-\n-    if squared:\n-        factor = nan_targets.numel() / ((~nan_targets).sum())\n-    else:\n-        factor = (nan_targets.numel() / ((~nan_targets).sum())).sqrt()\n-\n-    loss = loss * factor\n-\n-    return loss\n-\n-\n-def mean_absolute_error_ipu(preds: Tensor, target: Tensor) -> Tensor:\n-    \"\"\"Computes mean absolute error.\n-\n-    Handles NaNs without reshaping tensors in order to work on IPU.\n-\n-    Args:\n-        preds: estimated labels\n-        target: ground truth labels\n-\n-    Return:\n-        Tensor with MAE\n-    \"\"\"\n-    target = target.clone()\n-    preds = preds.clone()\n-\n-    # Replace the nan-targets in the preds/target tensors by 0\n-    nan_targets = target.isnan()\n-    preds[nan_targets] = 0.0\n-    target[nan_targets] = 0.0\n-\n-    # Compute the loss, and rescale by the number of nan elements\n-    loss = mean_absolute_error(preds, target)\n-    loss = loss * nan_targets.numel() / ((~nan_targets).sum())\n-\n-    return loss\ndiff --git a/graphium/ipu/ipu_wrapper.py b/graphium/ipu/ipu_wrapper.py\nindex 0ac04b883..305326015 100644\n--- a/graphium/ipu/ipu_wrapper.py\n+++ b/graphium/ipu/ipu_wrapper.py\n@@ -122,7 +122,7 @@ def training_step(self, batch, batch_idx) -> Dict[str, Any]:\n         features, labels = batch[\"features\"], batch[\"labels\"]\n         features, labels = self.squeeze_input_dims(features, labels)\n         dict_input = {\"features\": features, \"labels\": labels}\n-        step_dict = super().training_step(dict_input, to_cpu=False)\n+        step_dict = super().training_step(dict_input)\n \n         loss = step_dict.pop(\"loss\")\n         step_dict[\"loss\"] = self.poptorch.identity_loss(loss, reduction=\"mean\")\n@@ -132,7 +132,7 @@ def validation_step(self, batch, batch_idx) -> Dict[str, Any]:\n         features, labels = batch[\"features\"], batch[\"labels\"]\n         features, labels = self.squeeze_input_dims(features, labels)\n         dict_input = {\"features\": features, \"labels\": labels}\n-        step_dict = super().validation_step(dict_input, to_cpu=False)\n+        step_dict = super().validation_step(dict_input)\n \n         return step_dict\n \n@@ -141,14 +141,14 @@ def test_step(self, batch, batch_idx) -> Dict[str, Any]:\n         features, labels = batch[\"features\"], batch[\"labels\"]\n         features, labels = self.squeeze_input_dims(features, labels)\n         dict_input = {\"features\": features, \"labels\": labels}\n-        step_dict = super().test_step(dict_input, to_cpu=False)\n+        step_dict = super().test_step(dict_input)\n \n         return step_dict\n \n     def predict_step(self, **inputs) -> Dict[str, Any]:\n         # Build a dictionary from the tuples\n         dict_input = inputs\n-        step_dict = super().predict_step(dict_input, to_cpu=False)\n+        step_dict = super().predict_step(dict_input)\n \n         return step_dict\n \ndiff --git a/graphium/trainer/__init__.py b/graphium/trainer/__init__.py\nindex ed2cbf2a4..1e1682e2f 100644\n--- a/graphium/trainer/__init__.py\n+++ b/graphium/trainer/__init__.py\n@@ -2,3 +2,6 @@\n from . import metrics\n \n from .predictor import PredictorModule\n+from .predictor_summaries import SingleTaskSummary\n+from .predictor_summaries import MultiTaskSummary\n+from .progress_bar import ProgressBarMetrics\ndiff --git a/graphium/trainer/metrics.py b/graphium/trainer/metrics.py\nindex 22361faa6..7a20e0049 100644\n--- a/graphium/trainer/metrics.py\n+++ b/graphium/trainer/metrics.py\n@@ -12,16 +12,22 @@\n \"\"\"\n \n \n-from typing import Union, Callable, Optional, Dict, Any\n+from typing import Union, Callable, Optional, Dict, Any, Literal, List, Tuple\n \n import sys\n \n import torch\n from torch import Tensor\n+import torch.distributed as dist\n import operator as op\n+from copy import deepcopy\n+from loguru import logger\n \n+from torch.nn.modules.loss import _Loss\n from torchmetrics.utilities.distributed import reduce\n import torchmetrics.functional.regression.mae\n+from torchmetrics.utilities.data import dim_zero_cat\n+from torchmetrics import Metric\n \n from graphium.utils.tensor import nan_mean\n \n@@ -38,7 +44,7 @@ class Thresholder:\n     def __init__(\n         self,\n         threshold: float,\n-        operator: Union[str, Callable] = \"greater\",\n+        operator: Union[Literal[\"greater\", \"gt\", \">\", \"lower\", \"lt\", \"<\"], Callable] = \"greater\",\n         th_on_preds: bool = True,\n         th_on_target: bool = False,\n     ):\n@@ -74,10 +80,10 @@ def _get_operator(operator):\n         \"\"\"Operator can either be a string, or a callable\"\"\"\n         if isinstance(operator, str):\n             op_name = operator.lower()\n-            if op_name in [\"greater\", \"gt\"]:\n+            if op_name in [\"greater\", \"gt\", \">\"]:\n                 op_str = \">\"\n                 operator = op.gt\n-            elif op_name in [\"lower\", \"lt\"]:\n+            elif op_name in [\"lower\", \"lt\", \"<\"]:\n                 op_str = \"<\"\n                 operator = op.lt\n             else:\n@@ -129,6 +135,28 @@ def __eq__(self, obj) -> bool:\n         return all(is_eq)\n \n \n+def _filter_nans(preds: Tensor, target: Tensor, target_nan_mask: Union[Literal[None, \"none\", \"ignore\"], int]) -> Tuple[Tensor, Tensor]:\n+    \"\"\"Handle the NaNs according to the chosen options\"\"\"\n+\n+    if target_nan_mask is None: # No NaN handling\n+        return preds, target\n+    \n+    if target.dtype in [torch.int, torch.int16, torch.int32, torch.int64, torch.int8]:\n+        target_nans = (torch.iinfo(target.dtype).min == target) | (torch.iinfo(target.dtype).max == target)\n+    else:\n+        target_nans = torch.isnan(target)\n+    if ~target_nans.any(): # No NaNs\n+        return preds, target\n+    elif isinstance(target_nan_mask, (int, float)): # Replace NaNs\n+        target = target.clone()\n+        target[target_nans] = target_nan_mask\n+    elif target_nan_mask == \"ignore\": # Remove NaNs\n+        target = target[~target_nans]\n+        preds = preds[~target_nans]\n+    else:\n+        raise ValueError(f\"Invalid option `{target_nan_mask}`\")\n+    return preds, target\n+\n class MetricWrapper:\n     r\"\"\"\n     Allows to initialize a metric from a name or Callable, and initialize the\n@@ -137,10 +165,10 @@ class MetricWrapper:\n \n     def __init__(\n         self,\n-        metric: Union[str, Callable],\n+        metric: Union[str, torchmetrics.Metric, torch.nn.modules.loss._Loss],\n         threshold_kwargs: Optional[Dict[str, Any]] = None,\n-        target_nan_mask: Optional[Union[str, int]] = None,\n-        multitask_handling: Optional[str] = None,\n+        target_nan_mask: Union[Literal[None, \"none\", \"ignore\"], int] = None,\n+        multitask_handling: Literal[None, \"none\", \"flatten\", \"mean-per-label\"] = None,\n         squeeze_targets: bool = False,\n         target_to_int: bool = False,\n         **kwargs,\n@@ -187,7 +215,7 @@ def __init__(\n                 Other arguments to call with the metric\n         \"\"\"\n \n-        self.metric, self.metric_name = self._get_metric(metric)\n+        metric_class, self.metric_name = self._get_metric_class(metric)\n         self.thresholder = None\n         if threshold_kwargs is not None:\n             self.thresholder = Thresholder(**threshold_kwargs)\n@@ -198,6 +226,34 @@ def __init__(\n         self.target_to_int = target_to_int\n         self.kwargs = kwargs\n \n+        self.metric, self.kwargs = self._initialize_metric(metric_class, self.target_nan_mask, self.multitask_handling, **self.kwargs)\n+\n+    @staticmethod\n+    def _initialize_metric(metric, target_nan_mask, multitask_handling, **kwargs):\n+        r\"\"\"\n+        Initialize the metric with the provided kwargs\n+        \"\"\"\n+    \n+        if not isinstance(metric, type):\n+            if callable(metric):\n+                metric = MetricToConcatenatedTorchMetrics(\n+                    metric_fn=metric,\n+                    target_nan_mask=target_nan_mask, \n+                    multitask_handling=multitask_handling, \n+                    **kwargs)\n+                return metric, kwargs\n+            elif all(hasattr(metric, method) for method in [\"update\", \"compute\", \"reset\", \"to\"]):\n+                return metric, kwargs\n+            else:\n+                raise ValueError(f\"metric must be a callable, or a class with 'update', 'compute', 'reset', 'to', provided: `{type(metric)}`\")\n+        \n+        metric = metric(**kwargs)\n+        if not all(hasattr(metric, method) for method in [\"update\", \"compute\", \"reset\", \"to\"]):\n+            raise ValueError(f\"metric must be a callable, or a class with 'update', 'compute', 'reset', 'to', provided: `{type(metric)}`\")\n+\n+        return metric, kwargs\n+\n+\n     @staticmethod\n     def _parse_target_nan_mask(target_nan_mask):\n         \"\"\"\n@@ -254,20 +310,35 @@ def _parse_multitask_handling(multitask_handling, target_nan_mask):\n         return multitask_handling\n \n     @staticmethod\n-    def _get_metric(metric):\n+    def _get_metric_class(metric):\n         from graphium.utils.spaces import METRICS_DICT\n \n         if isinstance(metric, str):\n-            metric_name = metric\n-            metric = METRICS_DICT[metric]\n+            metric_name = MetricWrapper._ipu_metrics_name_conversion(metric)\n+            metric = METRICS_DICT[metric_name]\n         else:\n             metric_name = None\n             metric = metric\n         return metric, metric_name\n-\n-    def compute(self, preds: Tensor, target: Tensor) -> Tensor:\n+    \n+    @staticmethod\n+    def _ipu_metrics_name_conversion(metric, warning=True):\n         r\"\"\"\n-        Compute the metric, apply the thresholder if provided, and manage the NaNs\n+        Convert the metric name from the removed ipu metrics to the regular torchmetrics metrics\n+        \"\"\"\n+        metric_name = metric\n+        if metric_name.endswith(\"_ipu\"): # For backward compatibility when loading models with metrics for ipu\n+            metric_name = metric_name[:-4]\n+            if metric_name == \"average_precision\": # A previous typo in the `spaces.py`\n+                metric_name = \"averageprecision\"\n+            if warning:\n+                logger.warning(f\"Using the metric `{metric_name}` instead of `{metric}`\")\n+        return metric_name\n+\n+    def update(self, preds: Tensor, target: Tensor) -> Tensor:\n+        r\"\"\"\n+        Update the parameters of the metric, apply the thresholder if provided, and manage the NaNs.\n+        See `torchmetrics.Metric.update` for more details.\n         \"\"\"\n         if preds.ndim == 1:\n             preds = preds.unsqueeze(-1)\n@@ -279,8 +350,6 @@ def compute(self, preds: Tensor, target: Tensor) -> Tensor:\n         if self.thresholder is not None:\n             preds, target = self.thresholder(preds, target)\n \n-        target_nans = torch.isnan(target)\n-\n         # for the classifigression task, cast predictions from\n         # (batch_size, n_targets * n_brackets) to (batch_size, n_targets, n_brackets)\n         # TODO: make this more flexible to the target shape in the future\n@@ -290,7 +359,7 @@ def compute(self, preds: Tensor, target: Tensor) -> Tensor:\n         else:\n             classifigression = False\n \n-        if self.multitask_handling is None:\n+        if (self.multitask_handling is None):\n             # In case of no multi-task handling, apply the nan filtering, then compute the metrics\n             assert (\n                 self.target_nan_mask != \"ignore\"\n@@ -300,7 +369,9 @@ def compute(self, preds: Tensor, target: Tensor) -> Tensor:\n                 target = target.squeeze()\n             if self.target_to_int:\n                 target = target.to(int)\n-            metric_val = self.metric(preds, target, **self.kwargs)\n+            self.metric.update(preds, target)\n+\n+\n         elif self.multitask_handling == \"flatten\":\n             # Flatten the tensors, apply the nan filtering, then compute the metrics\n             if classifigression:\n@@ -313,16 +384,26 @@ def compute(self, preds: Tensor, target: Tensor) -> Tensor:\n                 target = target.squeeze()\n             if self.target_to_int:\n                 target = target.to(int)\n-            metric_val = self.metric(preds, target, **self.kwargs)\n+            self.metric.update(preds, target)\n+\n+        elif isinstance(self.metric, MetricToConcatenatedTorchMetrics):\n+            # NaN's and multitask handling are handled by the MetricToConcatenatedTorchMetrics\n+            if self.squeeze_targets:\n+                target = target.squeeze()\n+            if self.target_to_int:\n+                target = target.to(int)\n+            self.metric.update(preds, target)\n+        \n         elif self.multitask_handling == \"mean-per-label\":\n             # Loop the columns (last dim) of the tensors, apply the nan filtering, compute the metrics per column, then average the metrics\n-            target_list = [target[..., ii][~target_nans[..., ii]] for ii in range(target.shape[-1])]\n-            # TODO: make this more flexible to the target shape in the future\n+            target_list = [target[..., ii] for ii in range(target.shape[-1])]\n             if classifigression:\n-                preds_list = [preds[..., i, :][~target_nans[..., i]] for i in range(preds.shape[1])]\n+                preds_list = [preds[..., ii, :] for ii in range(preds.shape[1])]\n             else:\n-                preds_list = [preds[..., ii][~target_nans[..., ii]] for ii in range(preds.shape[-1])]\n-            metric_val = []\n+                preds_list = [preds[..., ii] for ii in range(preds.shape[-1])]\n+\n+            if not isinstance(self.metric, list):\n+                self.metric = [deepcopy(self.metric) for _ in range(len(target_list))]\n             for ii in range(len(target_list)):\n                 try:\n                     this_preds, this_target = self._filter_nans(preds_list[ii], target_list[ii])\n@@ -330,44 +411,78 @@ def compute(self, preds: Tensor, target: Tensor) -> Tensor:\n                         this_target = this_target.squeeze()\n                     if self.target_to_int:\n                         this_target = this_target.to(int)\n-                    metric_val.append(self.metric(this_preds, this_target, **self.kwargs))\n+                    self.metric[ii].update(this_preds, this_target)\n                 except:\n                     pass\n-            # Average the metric\n-            metric_val = nan_mean(torch.stack(metric_val))\n         else:\n             # Wrong option\n             raise ValueError(f\"Invalid option `self.multitask_handling={self.multitask_handling}`\")\n \n-        return metric_val\n+    def compute(self) -> Tensor:\n+        r\"\"\"\n+        Compute the metric with the method `self.compute`\n+        \"\"\"\n+        if isinstance(self.metric, list):\n+            metrics = [metric.compute() for metric in self.metric]\n+            return nan_mean(torch.stack(metrics))\n+\n+        return self.metric.compute()\n \n-    def _filter_nans(self, preds: Tensor, target: Tensor):\n-        \"\"\"Handle the NaNs according to the chosen options\"\"\"\n-        target_nans = torch.isnan(target)\n+    def update_compute(self, preds: Tensor, target: Tensor) -> Tensor:\n+        r\"\"\"\n+        Update the parameters of the metric, apply the thresholder if provided, and manage the NaNs.\n+        Then compute the metric with the method `self.compute`\n+        \"\"\"\n \n-        if self.target_nan_mask is None:\n-            pass\n-        elif isinstance(self.target_nan_mask, (int, float)):\n-            target = target.clone()\n-            target[torch.isnan(target)] = self.target_nan_mask\n-        elif self.target_nan_mask == \"ignore\":\n-            target = target[~target_nans]\n-            preds = preds[~target_nans]\n+        self.update(preds, target)\n+        return self.compute()\n+\n+    def reset(self) -> None:\n+        r\"\"\"\n+        Reset the metric with the method `self.metric.reset`\n+        \"\"\"\n+        if isinstance(self.metric, list):\n+            for metric in self.metric:\n+                metric.reset()\n         else:\n-            raise ValueError(f\"Invalid option `{self.target_nan_mask}`\")\n-        return preds, target\n+            self.metric.reset()\n+\n+    def to(self, device: Union[str, torch.device]) -> None:\n+        r\"\"\"\n+        Move the metric to the device with the method `self.metric.to`\n+        \"\"\"\n+        if isinstance(self.metric, list):\n+            for metric in self.metric:\n+                metric.to(device)\n+        else:\n+            self.metric.to(device)\n+\n+    @property\n+    def device(self) -> torch.device:\n+        r\"\"\"\n+        Return the device of the metric with the method `self.metric.device` or `self.metric[0].device`\n+        \"\"\"\n+        if isinstance(self.metric, list):\n+            return self.metric[0].device\n+        return self.metric.device\n+\n+\n+    def _filter_nans(self, preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n+        \"\"\"Handle the NaNs according to the chosen options\"\"\"\n+\n+        return _filter_nans(preds, target, self.target_nan_mask)\n \n     def __call__(self, preds: Tensor, target: Tensor) -> Tensor:\n         r\"\"\"\n         Compute the metric with the method `self.compute`\n         \"\"\"\n-        return self.compute(preds, target)\n+        return self.update_compute(preds, target)\n \n     def __repr__(self):\n         r\"\"\"\n         Control how the class is printed\n         \"\"\"\n-        full_str = f\"{self.metric.__name__}\"\n+        full_str = f\"{self.metric.__repr__()}\"\n         if self.thresholder is not None:\n             full_str += f\"({self.thresholder})\"\n \n@@ -405,10 +520,198 @@ def __getstate__(self):\n \n     def __setstate__(self, state: dict):\n         \"\"\"Reload the class from pickling.\"\"\"\n-        state[\"metric\"], state[\"metric_name\"] = self._get_metric(state[\"metric\"])\n+        state[\"metric\"], state[\"metric_name\"] = self._get_metric_class(state[\"metric\"])\n         thresholder = state.pop(\"threshold_kwargs\", None)\n         if thresholder is not None:\n             thresholder = Thresholder(**thresholder)\n         state[\"thresholder\"] = thresholder\n+        state[\"metric\"], state[\"at_compute_kwargs\"] = self._initialize_metric(state[\"metric\"], state[\"target_nan_mask\"], state[\"multitask_handling\"], **state[\"kwargs\"])\n \n         self.__dict__.update(state)\n+\n+class LossWrapper():\n+    r\"\"\"\n+    A simple wrapper to convert any metric or loss to an equivalent of `torchmetrics.Metric`\n+    by adding the `update`, `compute`, and `reset` methods to make it compatible with `MetricWrapper`.\n+    However, it is simply limited to computing the average of the metric over all the updates.\n+    \"\"\"\n+\n+    def __init__(self, loss):\n+        self.loss = loss\n+        self.scores: List[Tensor] = []\n+\n+    def update(self, preds: Tensor, target: Tensor):\n+        self.scores.append(self.loss(preds, target))\n+\n+    def compute(self):\n+        if len(self.scores) == 0:\n+            raise ValueError(\"No scores to compute\")\n+        elif len(self.scores) == 1:\n+            return self.scores[0]\n+        return nan_mean(torch.stack(self.scores))\n+    \n+    def to(self, device: Union[str, torch.device]):\n+        for ii in range(len(self.scores)):\n+            self.scores[ii] = self.scores[ii].to(device)\n+\n+    @property\n+    def device(self) -> torch.device:\n+        self.loss.device\n+\n+    def reset(self):\n+        self.scores = []\n+\n+\n+class MetricToMeanTorchMetrics(Metric):\n+    r\"\"\"\n+    A simple wrapper to convert any metric or loss to an equivalent of `torchmetrics.Metric`\n+    by adding the `update`, `compute`, and `reset` methods to make it compatible with `MetricWrapper`.\n+\n+    However, it is limited in functionality. At each `.update()`, it computes the metric and stores in a list.\n+    Then at `.compute()` it returns the average of the computed metric, while ignoring NaNs.\n+    \"\"\"\n+    scores: List[Tensor] = []\n+\n+    def __init__(self, metric_fn):\n+        super().__init__(dist_sync_on_step=False)\n+        self.metric_fn = metric_fn\n+        self.add_state(\"scores\", default=[], dist_reduce_fx=\"cat\")\n+\n+    def update(self, preds: Tensor, target: Tensor):\n+        self.scores.append(self.metric_fn(preds.detach(), target))\n+\n+    def compute(self):\n+        if len(self.scores) == 0:\n+            raise ValueError(\"No scores to compute\")\n+        elif len(self.scores) == 1:\n+            return self.scores[0]\n+        return nan_mean(torch.stack(self.scores))\n+\n+\n+class MetricToConcatenatedTorchMetrics(Metric):\n+\n+    preds: List[Tensor] # Always on CPU\n+    target: List[Tensor] # Always on CPU\n+\n+    def __init__(self, \n+                    metric_fn: Callable,\n+                    target_nan_mask: Union[Literal[None, \"none\", \"ignore\"], int] = None,\n+                    multitask_handling: Literal[None, \"none\", \"flatten\", \"mean-per-label\"] = None,\n+                    **kwargs,\n+                 ):\n+        r\"\"\"\n+            A wrapper around the `torchmetrics.Metric` to handle the saving and syncing of `preds` and `target` tensors,\n+            and moving them to the CPU.\n+            This is useful for certain metrics that require to save all preds and targets, such as auroc and average_precision.\n+            Otherwise, if using `MetricWrapper` with the option `mean-per-label`, the `preds` and `target` would be\n+            duplicated for each label, causing major memory spikes. \n+            On top of that, all preds and targets would be on the GPU, which would cause the memory to increase at every step, \n+            and potentially lead to out-of-memory before the end of the epoch.\n+\n+            Parameters\n+            ----------\n+\n+            metric_fn:\n+                The metric function to use. This function should take `preds` and `target` as input, and return a scalar value.\n+\n+            target_nan_mask:\n+                - None: Do not change behaviour if there are NaNs\n+\n+                - int, float: Value used to replace NaNs. For example, if `target_nan_mask==0`, then\n+                  all NaNs will be replaced by zeros\n+\n+                - 'ignore': The NaN values will be removed from the tensor before computing the metrics.\n+                  Must be coupled with the `multitask_handling='flatten'` or `multitask_handling='mean-per-label'`.\n+\n+            multitask_handling:\n+                - None: Do not process the tensor before passing it to the metric.\n+                  Cannot use the option `multitask_handling=None` when `target_nan_mask=ignore`.\n+                  Use either 'flatten' or 'mean-per-label'.\n+\n+                - 'flatten': Flatten the tensor to produce the equivalent of a single task\n+\n+                - 'mean-per-label': Loop all the labels columns, process them as a single task,\n+                    and average the results over each task\n+                  *This option might slow down the computation if there are too many labels*\n+\n+        \"\"\"\n+                 \n+        super().__init__(compute_on_cpu=True, dist_sync_on_step=False, sync_on_compute=False)\n+        self.metric_fn = metric_fn\n+        self.target_nan_mask = target_nan_mask\n+        self.multitask_handling = multitask_handling\n+        self.kwargs = kwargs\n+        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n+        self.add_state(\"target\", default=[], dist_reduce_fx=\"cat\")\n+        self._to_device_warned: bool = False\n+        super().to(\"cpu\")\n+\n+    def update(self, preds: Tensor, target: Tensor):\n+\n+        # If distributed, gather the preds and target tensors\n+        if self.dist_sync_fn is not None:\n+            preds_list = self.dist_sync_fn(preds, self.process_group)\n+            target_list = self.dist_sync_fn(target, self.process_group)\n+            preds = dim_zero_cat(preds_list)\n+            target = dim_zero_cat(target_list)\n+\n+        # Move the tensors to the CPU after gathering them\n+        self.preds.append(preds.detach().cpu())\n+        self.target.append(target.cpu())\n+\n+    def compute(self):\n+        preds = dim_zero_cat(self.preds)\n+        target = dim_zero_cat(self.target)\n+\n+        if (self.multitask_handling is None) or (self.multitask_handling in [\"none\", \"flatten\"]):\n+            preds, target = _filter_nans(preds, target, self.target_nan_mask)\n+            value = self.metric_fn(preds, target,  **self.kwargs)\n+\n+        elif self.multitask_handling == \"mean-per-label\":\n+            value = []\n+            # Loop the columns (last dim) of the tensors, apply the nan filtering, compute the metrics per column, then average the metrics\n+            target_list = [target[..., ii] for ii in range(target.shape[-1])]\n+            preds_list = [preds[..., ii] for ii in range(preds.shape[-1])]\n+            for ii in range(len(target_list)):\n+                try:\n+                    this_preds, this_target = _filter_nans(preds_list[ii], target_list[ii], self.target_nan_mask)\n+                    value.append(self.metric_fn(this_preds, this_target, **self.kwargs))\n+                except:\n+                    pass\n+            value = nan_mean(torch.stack(value))\n+        else:\n+            # Wrong option\n+            raise ValueError(f\"Invalid option `self.multitask_handling={self.multitask_handling}`\")\n+        return value\n+    \n+    def to(self, device: Union[str, torch.device]):\n+        \"\"\"\n+        Disables the moving of the metric to another device. Stays on CPU to avoid overflow.\n+        \"\"\"\n+        device = torch.device(device)\n+        if device == torch.device(\"cpu\"):\n+            return\n+        if not self._to_device_warned:\n+            self._to_device_warned = True\n+            logger.warning(f\"{self.get_obj_name(self)}({self.get_obj_name(self.metric_fn)}) stays on `{self.device}`, won't move to `{device}`\")\n+        \n+    @staticmethod\n+    def get_obj_name(obj):\n+        \"\"\"\n+        Returns the name of a function, class, or instance of a class.\n+        \n+        Parameters:\n+        - obj: The object to get the name of.\n+        \n+        Returns:\n+        - The name of the object as a string.\n+        \"\"\"\n+        # If the object is a class or function, return its __name__\n+        if hasattr(obj, '__name__'):\n+            return obj.__name__\n+        # If the object is an instance of a class, return its class's __name__\n+        elif hasattr(obj, '__class__'):\n+            return obj.__class__.__name__\n+        else:\n+            return str(obj)  # Fallback to converting the object to string\n+\ndiff --git a/graphium/trainer/predictor.py b/graphium/trainer/predictor.py\nindex 95d2c3166..f15521268 100644\n--- a/graphium/trainer/predictor.py\n+++ b/graphium/trainer/predictor.py\n@@ -14,7 +14,7 @@\n \n import time\n from copy import deepcopy\n-from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, Literal, Mapping\n \n import lightning\n import numpy as np\n@@ -23,20 +23,20 @@\n from mup.optim import MuAdam\n from torch import Tensor, nn\n from torch_geometric.data import Batch, Data\n+from torchmetrics import Metric\n \n from graphium.config.config_convert import recursive_config_reformating\n from graphium.data.datamodule import BaseDataModule\n-from graphium.trainer.metrics import MetricWrapper\n+from graphium.trainer.metrics import MetricWrapper, LossWrapper\n from graphium.trainer.predictor_options import (\n     EvalOptions,\n     FlagOptions,\n     ModelOptions,\n     OptimOptions,\n )\n-from graphium.trainer.predictor_summaries import TaskSummaries\n+from graphium.trainer.predictor_summaries import MultiTaskSummary, GradientNormMetric\n from graphium.utils import fs\n from graphium.utils.moving_average_tracker import MovingAverageTracker\n-from graphium.utils.spaces import GRAPHIUM_PRETRAINED_MODELS_DICT\n from graphium.utils.tensor import dict_tensor_fp16_to_fp32\n \n \n@@ -54,15 +54,12 @@ def __init__(\n         scheduler_kwargs: Optional[Dict[str, Any]] = None,\n         target_nan_mask: Optional[Union[str, int]] = None,\n         multitask_handling: Optional[str] = None,\n-        metrics: Dict[str, Callable] = None,\n+        metrics: Dict[str, Dict[str, Union[Metric, \"MetricWrapper\"]]] = None,\n         metrics_on_progress_bar: Dict[str, List[str]] = [],\n         metrics_on_training_set: Optional[Dict[str, List[str]]] = None,\n         flag_kwargs: Dict[str, Any] = None,\n         task_norms: Optional[Dict[Callable, Any]] = None,\n         metrics_every_n_train_steps: Optional[int] = None,\n-        replicas: int = 1,\n-        gradient_acc: int = 1,\n-        global_bs: Optional[int] = 1,\n     ):\n         \"\"\"\n         The Lightning module responsible for handling the predictions, losses, metrics, optimization, etc.\n@@ -139,11 +136,12 @@ def __init__(\n \n         # Task-specific evalutation attributes\n         self.loss_fun = {}\n+        loss_names = {}\n         self.metrics = {}\n         self.metrics_on_progress_bar = {}\n         self.metrics_on_training_set = {}\n         for task in self.tasks:\n-            self.loss_fun[task] = EvalOptions.parse_loss_fun(loss_fun[task])\n+            loss_names[task], self.loss_fun[task] = EvalOptions.parse_loss_fun(loss_fun[task])\n             self.metrics[task] = (\n                 self._eval_options_dict[task].metrics\n                 if self._eval_options_dict[task].metrics is not None\n@@ -164,36 +162,51 @@ def __init__(\n         # Set the parameters for optimizer options\n         self.optim_options.set_kwargs()\n \n+        # Add the loss to the metrics\n+        metrics_with_loss = deepcopy(self.metrics)\n+        for task in self.tasks:\n+            metrics_with_loss[task][f\"loss_{loss_names[task]}\"] = MetricWrapper(\n+                metric=LossWrapper(self.loss_fun[task]),\n+                target_nan_mask=self.target_nan_mask,\n+                multitask_handling=self.multitask_handling,\n+            )\n+        \n         # Initialize the epoch summary\n-        monitor = self.optim_options.scheduler_kwargs[\"monitor\"].split(\"/\")[0]\n-        mode = self.optim_options.scheduler_kwargs[\"mode\"]\n-\n-        self.task_epoch_summary = TaskSummaries(\n-            task_loss_fun=self.loss_fun,\n-            task_metrics=self.metrics,\n-            task_metrics_on_training_set=self.metrics_on_training_set,\n-            task_metrics_on_progress_bar=self.metrics_on_progress_bar,\n-            monitor=monitor,\n-            mode=mode,\n-        )\n+        self.task_epoch_summary = {\n+            \"train\": MultiTaskSummary(\n+                task_metrics=metrics_with_loss, \n+                step_name=\"train\", \n+                task_metrics_on_progress_bar=None,\n+                task_metrics_on_training_set=self.metrics_on_training_set,\n+                ),\n+            \"val\": MultiTaskSummary(\n+                task_metrics=metrics_with_loss, \n+                step_name=\"val\", \n+                task_metrics_on_progress_bar=self.metrics_on_progress_bar,\n+                task_metrics_on_training_set=None,\n+                ),\n+            \"test\": MultiTaskSummary(\n+                task_metrics=metrics_with_loss,\n+                step_name=\"test\",\n+                task_metrics_on_progress_bar=None,\n+                task_metrics_on_training_set=None,\n+            ),\n+        }\n \n         # This helps avoid a bug when saving hparams to yaml with different dict or str formats\n         self._set_hparams(recursive_config_reformating(self.hparams))\n \n         # throughput estimation\n-        self.mean_val_time_tracker = MovingAverageTracker()\n-        self.mean_val_tput_tracker = MovingAverageTracker()\n-        self.validation_step_outputs = []\n-        self.test_step_outputs = []\n-        self.epoch_start_time = None\n+        self.mean_time_tracker = MovingAverageTracker()\n+        self.mean_tput_tracker = MovingAverageTracker()\n+        self.epoch_start_time = {}\n \n         # Decide whether to log every step or once at the end\n         # of the epoch.\n         self.metrics_every_n_train_steps = metrics_every_n_train_steps\n         # Wether save preds and targets for each training step.\n \n-        self.samples_seen = 0\n-        self.global_bs = global_bs\n+        self.model_grad = GradientNormMetric()\n \n     def forward(\n         self, inputs: Dict\n@@ -234,6 +247,22 @@ def _get_task_key(self, task_level: str, task: str):\n         if not task.startswith(task_prefix):\n             task = task_prefix + task\n         return task\n+    \n+    def _get_average_loss_from_outputs(self, outputs: Dict[Literal[\"loss\", \"task_losses\"], Tensor], step_name: Literal[\"train\", \"val\", \"test\"]) -> Dict[str, Tensor]:\n+        r\"\"\"\n+        Averages the loss over the different tasks\n+        \"\"\"\n+        global_loss = torch.as_tensor(outputs[\"loss\"]).detach()\n+        if global_loss.numel() > 1:\n+            global_loss = global_loss[global_loss != 0].mean()\n+        average_losses = {f\"_global/loss/{step_name}\": global_loss}\n+        for task in self.tasks:\n+            this_losses = torch.as_tensor(outputs[\"task_losses\"][task]).detach()\n+            if this_losses.numel() > 1:\n+                this_losses = this_losses[this_losses != 0].mean()\n+            average_losses[f\"{task}/loss/{step_name}\"] = this_losses\n+        return average_losses\n+\n \n     def configure_optimizers(self, impl=None):\n         if impl is None:\n@@ -306,7 +335,7 @@ def compute_loss(\n \n         wrapped_loss_fun_dict = {\n             task: MetricWrapper(\n-                metric=loss,\n+                metric=LossWrapper(loss),\n                 threshold_kwargs=None,\n                 target_nan_mask=target_nan_mask,\n                 multitask_handling=multitask_handling,\n@@ -316,16 +345,18 @@ def compute_loss(\n \n         if weights is not None:\n             raise NotImplementedError(\"Weights are no longer supported in the loss\")\n+\n         all_task_losses = {\n-            task: wrapped(preds=preds[task], target=targets[task])\n+            task: wrapped.update_compute(preds=preds[task], target=targets[task])\n             for task, wrapped in wrapped_loss_fun_dict.items()\n         }\n+\n         total_loss = torch.sum(torch.stack(list(all_task_losses.values())), dim=0)\n         num_tasks = len(all_task_losses.keys())\n         weighted_loss = total_loss / num_tasks\n         return weighted_loss, all_task_losses\n \n-    def _general_step(self, batch: Dict[str, Tensor], step_name: str, to_cpu: bool) -> Dict[str, Any]:\n+    def _general_step(self, batch: Dict[str, Tensor], step_name: Literal[\"train\", \"val\", \"test\"]) -> Dict[str, Any]:\n         r\"\"\"Common code for training_step, validation_step and testing_step\"\"\"\n         preds = self.forward(batch)  # The dictionary of predictions\n \n@@ -366,7 +397,6 @@ def _general_step(self, batch: Dict[str, Tensor], step_name: str, to_cpu: bool)\n             multitask_handling=self.multitask_handling,\n         )\n \n-        device = \"cpu\" if to_cpu else None\n         for task in preds:\n             task_specific_norm = self.task_norms[task] if self.task_norms is not None else None\n             if hasattr(task_specific_norm, \"normalize_val_test\"):\n@@ -379,28 +409,18 @@ def _general_step(self, batch: Dict[str, Tensor], step_name: str, to_cpu: bool)\n                 # if normalize_val_test is true, no denormalization is applied, all losses and metrics are normalized version\n                 preds[task] = task_specific_norm.denormalize(preds[task])\n                 targets_dict[task] = task_specific_norm.denormalize(targets_dict[task])\n-            preds[task] = preds[task].detach().to(device=device)\n-            targets_dict[task] = targets_dict[task].detach().to(device=device)\n-        if weights is not None:\n-            weights = weights.detach().to(device=device)\n+            preds[task] = preds[task].detach()\n+            targets_dict[task] = targets_dict[task].detach()\n \n-        step_dict = {\"preds\": preds, \"targets\": targets_dict, \"weights\": weights}\n-        # step_dict[f\"{self.loss_fun._get_name()}/{step_name}\"] = loss.detach().cpu()            original\n-\n-        # step_dict[f\"weighted_loss/{step_name}\"] = loss.detach().cpu()\n-        # step_dict[f\"loss/{step_name}\"] = loss.detach().cpu()\n-        for task in self.tasks:\n-            step_dict[\n-                self.task_epoch_summary.metric_log_name(task, self.loss_fun[task]._get_name(), step_name)\n-            ] = loss.detach()\n+        self.task_epoch_summary[step_name].update(preds, targets_dict)\n \n+        step_dict = {}\n         step_dict[\"loss\"] = loss\n-        # print(\"loss \", self.global_step, self.current_epoch, loss)\n         step_dict[\"task_losses\"] = task_losses\n-        step_dict[\"gradient_norm\"] = self.get_gradient_norm()\n         return step_dict\n \n-    def flag_step(self, batch: Dict[str, Tensor], step_name: str, to_cpu: bool) -> Dict[str, Any]:\n+\n+    def flag_step(self, batch: Dict[str, Tensor], step_name: Literal[\"train\", \"val\", \"test\"]) -> Dict[str, Any]:\n         r\"\"\"\n         Perform adversarial data agumentation during one training step using FLAG.\n         Paper: https://arxiv.org/abs/2010.09891\n@@ -456,56 +476,51 @@ def flag_step(self, batch: Dict[str, Tensor], step_name: str, to_cpu: bool) -> D\n             )\n             loss = loss / n_steps\n \n-        device = \"cpu\" if to_cpu else None\n         for key in preds.keys():\n-            preds[key] = preds[key].detach().to(device=device)\n-            targets[key] = targets[key].detach().to(device=device)\n+            preds[key] = preds[key].detach()\n+            targets[key] = targets[key].detach()\n         if weights is not None:\n-            weights = weights.detach().to(device=device)\n+            weights = weights.detach()\n \n-        step_dict = {\"preds\": preds, \"targets\": targets, \"weights\": weights}\n+        step_dict = {}\n         step_dict[f\"loss/{step_name}\"] = loss.detach().cpu()\n         step_dict[\"loss\"] = loss\n         step_dict[\"task_losses\"] = task_losses\n+        self.task_epoch_summary[step_name].update(preds, targets)\n         return step_dict\n \n     def on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:\n-        self.train_batch_start_time = time.time()\n+\n+        self.model_grad.reset()\n+        self.task_epoch_summary[\"train\"].reset()\n+        self.batch_start_time = time.time()\n         self.skip_log_train_metrics = (self.metrics_every_n_train_steps is None) or (\n             (batch_idx % self.metrics_every_n_train_steps) != 0\n         )\n         return super().on_train_batch_start(batch, batch_idx)\n \n     def on_train_batch_end(self, outputs, batch: Any, batch_idx: int) -> None:\n-        train_batch_time = time.time() - self.train_batch_start_time  # To be used for throughput calculation\n+        train_batch_time = time.time() - self.batch_start_time  # To be used for throughput calculation\n \n         # Get the metrics that are logged at every step (loss, grad_norm, batch_time, batch_tput)\n-        concatenated_metrics_logs = {}\n-        concatenated_metrics_logs[\"train/loss\"] = outputs[\"loss\"]\n-        concatenated_metrics_logs[\"epoch_count\"] = self.current_epoch\n-        # Incriment by the batch size\n-        self.samples_seen += self.global_bs\n-        concatenated_metrics_logs[\"samples_seen\"] = self.samples_seen\n+        metrics_logs = {}\n \n         # report the training loss for each individual tasks\n-        for task in self.tasks:\n-            concatenated_metrics_logs[f\"train/loss/{task}\"] = outputs[\"task_losses\"][task]\n-\n         # get the mean loss value for individual tasks as they are a tensor of size --> gradient accumulation * replication * device_iter\n         # filter zeros out for the individual losses\n-        for key in concatenated_metrics_logs:\n-            if isinstance(concatenated_metrics_logs[key], torch.Tensor):\n-                if concatenated_metrics_logs[key].numel() > 1:\n-                    concatenated_metrics_logs[key] = concatenated_metrics_logs[key][\n-                        concatenated_metrics_logs[key] != 0\n-                    ].mean()\n+        losses = self._get_average_loss_from_outputs(outputs, step_name=\"train\")\n+\n+        metrics_logs.update(losses)\n+        metrics_logs.update(self.task_epoch_summary[\"train\"].compute())\n \n         # If logging is skipped for this step, then log the important metrics anyway and return\n         if self.skip_log_train_metrics:\n-            if self.logger is not None:\n-                self.logger.log_metrics(\n-                    concatenated_metrics_logs, step=self.global_step\n-                )  # This is a pytorch lightning function call\n+            self.log_dict(\n+                dictionary=metrics_logs,\n+                logger=True,\n+                on_step=True,\n+                prog_bar=True,\n+            )\n             return\n \n         ### The code below is not executed if the logging is skipped for this step ###\n@@ -513,165 +528,130 @@ def on_train_batch_end(self, outputs, batch: Any, batch_idx: int) -> None:\n         # Get the throughput of the batch\n         num_graphs = self.get_num_graphs(batch[\"features\"])\n         tput = num_graphs / train_batch_time\n-        concatenated_metrics_logs[\"train/batch_time\"] = train_batch_time\n-        concatenated_metrics_logs[\"train/batch_tput\"] = tput\n-\n-        # Compute all the metrics for the training set\n-        self.task_epoch_summary.update_predictor_state(\n-            step_name=\"train\",\n-            targets=outputs[\"targets\"],\n-            preds=outputs[\"preds\"],\n-            loss=outputs[\"loss\"],  # This is the weighted loss for now, but change to task-specific loss\n-            task_losses=outputs[\"task_losses\"],\n-            n_epochs=self.current_epoch,\n-        )\n-        metrics_logs = self.task_epoch_summary.get_metrics_logs()  # Dict[task, metric_logs]\n-        metrics_logs[\"_global\"][\"grad_norm\"] = self.get_gradient_norm()\n-        concatenated_metrics_logs.update(metrics_logs)\n+        metrics_logs[\"_global/batch_time/train\"] = train_batch_time\n+        metrics_logs[\"_global/batch_tput/train\"] = tput\n+        self.mean_time_tracker.update(train_batch_time)\n+        self.mean_tput_tracker.update(tput)\n+\n+        metrics_computed = self.task_epoch_summary[\"train\"].compute()\n+        self.task_epoch_summary[\"train\"].reset()\n+        metrics_logs.update(metrics_computed)\n+        metrics_logs[\"_global/grad_norm/train\"] = self.model_grad.compute()\n+        self.model_grad.reset()\n \n         # Log the metrics\n-        if self.logger is not None:\n-            self.logger.log_metrics(\n-                concatenated_metrics_logs, step=self.global_step\n-            )  # This is a pytorch lightning function call\n+        self.log_dict(\n+            dictionary=metrics_logs,\n+            logger=True,\n+            on_step=True,\n+            prog_bar=True,\n+        )\n \n-    def training_step(self, batch: Dict[str, Tensor], to_cpu: bool = True) -> Dict[str, Any]:\n+    def training_step(self, batch: Dict[str, Tensor]) -> Dict[str, Any]:\n         step_dict = None\n \n         # Train using FLAG\n         if self.flag_kwargs[\"n_steps\"] > 0:\n-            step_dict = self.flag_step(batch=batch, step_name=\"train\", to_cpu=to_cpu)\n+            step_dict = self.flag_step(batch=batch, step_name=\"train\")\n         # Train normally, without using FLAG\n         elif self.flag_kwargs[\"n_steps\"] == 0:\n-            # step_dict = self._general_step(batch=batch, step_name=\"train\", to_cpu=True)\n-            step_dict = self._general_step(batch=batch, step_name=\"train\", to_cpu=to_cpu)\n+            # step_dict = self._general_step(batch=batch, step_name=\"train\")\n+            step_dict = self._general_step(batch=batch, step_name=\"train\")\n+\n+        # Update the gradients\n+        self.model_grad.update(self.model)\n \n-        # Remove the preds and targets if no logging is required\n-        if self.skip_log_train_metrics:\n-            step_dict.pop(\"preds\")\n-            step_dict.pop(\"targets\")\n         return step_dict  # Returning the metrics_logs with the loss\n \n-    def get_gradient_norm(self):\n-        # compute the norm\n-        total_norm = torch.tensor(0.0)\n-        for p in self.parameters():\n-            if p.grad is not None:\n-                param_norm = p.grad.detach().data.norm(2)\n-                total_norm += param_norm.detach().cpu() ** 2\n-        total_norm = total_norm**0.5\n-        return total_norm\n+    def validation_step(self, batch: Dict[str, Tensor]) -> Dict[str, Any]:\n+        return self._general_step(batch=batch, step_name=\"val\")\n \n+    def test_step(self, batch: Dict[str, Tensor]) -> Dict[str, Any]:\n+        return self._general_step(batch=batch, step_name=\"test\")\n+    \n+    def _general_epoch_start(self, step_name: Literal[\"train\", \"val\", \"test\"]) -> None:\n+        self.task_epoch_summary[step_name].reset()\n+        self.epoch_start_time[step_name] = time.time()\n+        self.mean_time_tracker.reset()\n+        self.mean_tput_tracker.reset()\n     \n     def predict_step(self, batch: Dict[str, Tensor]) -> Dict[str, Any]:\n         preds = self.forward(batch)  # The dictionary of predictions\n         targets_dict = batch.get(\"labels\")\n \n         return preds, targets_dict\n-    \n-    def validation_step(self, batch: Dict[str, Tensor], to_cpu: bool = True) -> Dict[str, Any]:\n-        return self._general_step(batch=batch, step_name=\"val\", to_cpu=to_cpu)\n \n-    def test_step(self, batch: Dict[str, Tensor], to_cpu: bool = True) -> Dict[str, Any]:\n-        return self._general_step(batch=batch, step_name=\"test\", to_cpu=to_cpu)\n \n-    def _general_epoch_end(self, outputs: Dict[str, Any], step_name: str, device: str) -> None:\n+    def _general_epoch_end(self, step_name: Literal[\"train\", \"val\", \"test\"]) -> Dict[str, Tensor]:\n         r\"\"\"Common code for training_epoch_end, validation_epoch_end and testing_epoch_end\"\"\"\n         # Transform the list of dict of dict, into a dict of list of dict\n-        preds = {}\n-        targets = {}\n-        for task in self.tasks:\n-            preds[task] = torch.cat([out[\"preds\"][task].to(device) for out in outputs], dim=0)\n-            targets[task] = torch.cat([out[\"targets\"][task].to(device) for out in outputs], dim=0)\n-        if (\"weights\" in outputs[0].keys()) and (outputs[0][\"weights\"] is not None):\n-            weights = torch.cat([out[\"weights\"].to(device) for out in outputs], dim=0)\n-        else:\n-            weights = None\n+        \n+        metric_logs = self.task_epoch_summary[step_name].compute()\n+        self.task_epoch_summary[step_name].reset()\n+        metric_logs_cpu = {k: v for k, v in metric_logs.items() if v.device == torch.device(\"cpu\")}\n+        if len(metric_logs_cpu) > 0:\n+            self.log_dict(metric_logs_cpu, logger=True, prog_bar=True, sync_dist=False, on_epoch=True)\n+        \n+        metric_logs_accelerator = {k: v for k, v in metric_logs.items() if v.device != torch.device(\"cpu\")}\n+        if len(metric_logs_accelerator) > 0:\n+            self.log_dict(metric_logs_accelerator, logger=True, prog_bar=True, sync_dist=True, on_epoch=True)\n \n-        # NOTE: Computing the loss over the entire split may cause\n-        # overflow issues when using fp16\n-        loss, task_losses = self.compute_loss(\n-            preds=dict_tensor_fp16_to_fp32(preds),\n-            targets=dict_tensor_fp16_to_fp32(targets),\n-            weights=weights,\n-            target_nan_mask=self.target_nan_mask,\n-            multitask_handling=self.multitask_handling,\n-            loss_fun=self.loss_fun,\n-        )\n+        # Time metrics are tracked always on CPU, without progress bar, so we log them separatly\n+        time_metrics = {}\n+        time_metrics[f\"_global/mean_batch_time/{step_name}\"] = torch.tensor(self.mean_time_tracker.mean_value)\n+        time_metrics[f\"_global/mean_tput/{step_name}\"] = self.mean_tput_tracker.mean_value\n+        time_metrics[f\"_global/epoch_time/{step_name}\"] = torch.tensor(time.time() - self.epoch_start_time[step_name])\n \n-        self.task_epoch_summary.update_predictor_state(\n-            step_name=step_name,\n-            preds=preds,\n-            targets=targets,\n-            loss=loss,\n-            task_losses=task_losses,\n-            n_epochs=self.current_epoch,\n-        )\n-        metrics_logs = self.task_epoch_summary.get_metrics_logs()\n-        \n-        for task in metrics_logs.keys():\n-            for key, val in metrics_logs[task].items():\n-                metrics_logs[task][key] = val.to(self.device)\n+        self.log_dict(time_metrics, logger=True, prog_bar=False, sync_dist=False, on_epoch=True)\n \n-        return metrics_logs  # Consider returning concatenated dict for logging\n+        return metric_logs\n \n     def on_train_epoch_start(self) -> None:\n-        self.epoch_start_time = time.time()\n+        self._general_epoch_start(step_name=\"train\")\n \n     def on_train_epoch_end(self) -> None:\n-        if self.epoch_start_time is None:\n-            logger.warning(\"epoch timer not initialized\")\n-        else:\n-            epoch_time = time.time() - self.epoch_start_time\n-            self.epoch_start_time = None\n-            self.log(\"epoch_time\", torch.tensor(epoch_time).to(self.device), sync_dist=True)\n+        self._general_epoch_end(step_name=\"train\")\n \n     def on_validation_epoch_start(self) -> None:\n-        self.mean_val_time_tracker.reset()\n-        self.mean_val_tput_tracker.reset()\n+        self._general_epoch_start(step_name=\"val\")\n         return super().on_validation_epoch_start()\n \n     def on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n-        self.validation_batch_start_time = time.time()\n+        self.batch_start_time = time.time()\n         return super().on_validation_batch_start(batch, batch_idx, dataloader_idx)\n \n     def on_validation_batch_end(\n-        self, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int = 0\n+        self, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0\n     ) -> None:\n-        val_batch_time = time.time() - self.validation_batch_start_time\n-        self.validation_step_outputs.append(outputs)\n-        self.mean_val_time_tracker.update(val_batch_time)\n+        val_batch_time = time.time() - self.batch_start_time\n+        self.mean_time_tracker.update(val_batch_time)\n         num_graphs = self.get_num_graphs(batch[\"features\"])\n-        self.mean_val_tput_tracker.update(num_graphs / val_batch_time)\n+        self.mean_tput_tracker.update(num_graphs / val_batch_time)\n         return super().on_validation_batch_end(outputs, batch, batch_idx, dataloader_idx)\n \n     def on_validation_epoch_end(self) -> None:\n-        metrics_logs = self._general_epoch_end(\n-            outputs=self.validation_step_outputs, step_name=\"val\", device=\"cpu\"\n-        )\n-        self.validation_step_outputs.clear()\n-        concatenated_metrics_logs = self.task_epoch_summary.concatenate_metrics_logs(metrics_logs, device=self.device)\n-        concatenated_metrics_logs[\"val/mean_time\"] = torch.tensor(self.mean_val_time_tracker.mean_value).to(self.device)\n-        concatenated_metrics_logs[\"val/mean_tput\"] = self.mean_val_tput_tracker.mean_value.to(self.device)\n-        self.log_dict(concatenated_metrics_logs, sync_dist=True)\n+        self._general_epoch_end(step_name=\"val\")\n+        return super().on_validation_epoch_end()\n \n-        # Save yaml file with the per-task metrics summaries\n-        full_dict = {}\n-        full_dict.update(self.task_epoch_summary.get_dict_summary())\n-\n-    def on_test_batch_end(self, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n-        self.test_step_outputs.append(outputs)\n+    def on_test_epoch_start(self) -> None:\n+        self._general_epoch_start(step_name=\"test\")\n+        return super().on_test_epoch_start()\n \n     def on_test_epoch_end(self) -> None:\n-        metrics_logs = self._general_epoch_end(outputs=self.test_step_outputs, step_name=\"test\", device=\"cpu\")\n-        self.test_step_outputs.clear()\n-        concatenated_metrics_logs = self.task_epoch_summary.concatenate_metrics_logs(metrics_logs, device=self.device)\n-\n-        self.log_dict(concatenated_metrics_logs, sync_dist=True)\n \n-        # Save yaml file with the per-task metrics summaries\n-        full_dict = {}\n-        full_dict.update(self.task_epoch_summary.get_dict_summary())\n+        self._general_epoch_end(step_name=\"test\")\n+        return super().on_test_epoch_end()\n+    \n+    def on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n+        self.batch_start_time = time.time()\n+        return super().on_test_batch_start(batch, batch_idx, dataloader_idx)\n+    \n+    def on_test_batch_end(self, outputs: Tensor | Mapping[str, Any] | None, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\n+        test_batch_time = time.time() - self.batch_start_time\n+        self.mean_time_tracker.update(test_batch_time)\n+        num_graphs = self.get_num_graphs(batch[\"features\"])\n+        self.mean_tput_tracker.update(num_graphs / test_batch_time)\n+        return super().on_test_batch_end(outputs, batch, batch_idx, dataloader_idx)\n \n     def on_train_start(self):\n         hparams_log = deepcopy(self.hparams)\n@@ -679,16 +659,15 @@ def on_train_start(self):\n         if self.logger is not None:\n             self.logger.log_hyperparams(hparams_log)\n \n-    def get_progress_bar_dict(self) -> Dict[str, float]:\n-        prog_dict = {}\n-        prog_dict[\"loss\"] = self.task_epoch_summary.weighted_loss.detach().cpu()\n-        results_on_progress_bar = self.task_epoch_summary.get_results_on_progress_bar(\"val\")\n-        for task in self.tasks:\n-            prog_dict[self.task_epoch_summary.metric_log_name(task, \"loss\", \"val\")] = (\n-                self.task_epoch_summary.task_summaries[task].summaries[\"val\"].loss\n-            )\n-            prog_dict.update(results_on_progress_bar)\n-        return prog_dict\n+    @property\n+    def get_metrics_on_progress_bar(self) -> List[str]:\n+        prog_list = [\"_global/loss/train\"]\n+        for task_name in self.tasks:\n+            for metric in self.metrics_on_progress_bar[task_name]:\n+                this_summary = self.task_epoch_summary[\"val\"][task_name]\n+                prog_list.append(this_summary.metric_log_name(metric))\n+\n+        return prog_list\n \n     def __repr__(self) -> str:\n         r\"\"\"\n@@ -702,10 +681,12 @@ def __repr__(self) -> str:\n     @staticmethod\n     def list_pretrained_models():\n         \"\"\"List available pretrained models.\"\"\"\n-        return GRAPHIUM_PRETRAINED_MODELS_DICT\n+        from graphium.utils.spaces import GRAPHIUM_PRETRAINED_MODELS_DICT\n+\n+        return GRAPHIUM_PRETRAINED_MODELS_DICT # Avoiding circular imports with `space.py`\n \n     @staticmethod\n-    def load_pretrained_model(name_or_path: str, device: str = None):\n+    def load_pretrained_model(name_or_path: str, device: str = None, strict: bool = True, **kwargs):\n         \"\"\"Load a pretrained model from its name.\n \n         Args:\n@@ -713,11 +694,13 @@ def load_pretrained_model(name_or_path: str, device: str = None):\n                 from `graphium.trainer.PredictorModule.list_pretrained_models()`.\n         \"\"\"\n \n+        from graphium.utils.spaces import GRAPHIUM_PRETRAINED_MODELS_DICT # Avoiding circular imports with `space.py`\n+\n         name = GRAPHIUM_PRETRAINED_MODELS_DICT.get(name_or_path)\n \n         if name is not None:\n             return PredictorModule.load_from_checkpoint(\n-                GRAPHIUM_PRETRAINED_MODELS_DICT[name_or_path], map_location=device\n+                GRAPHIUM_PRETRAINED_MODELS_DICT[name_or_path], map_location=device, strict=strict, **kwargs\n             )\n \n         if name is None and not (fs.exists(name_or_path) and fs.get_extension(name_or_path) == \"ckpt\"):\n@@ -726,7 +709,7 @@ def load_pretrained_model(name_or_path: str, device: str = None):\n                 \"or pass a valid checkpoint (.ckpt) path.\"\n             )\n \n-        return PredictorModule.load_from_checkpoint(name_or_path, map_location=device)\n+        return PredictorModule.load_from_checkpoint(name_or_path, map_location=device, strict=strict, **kwargs)\n \n     def set_max_nodes_edges_per_graph(self, datamodule: BaseDataModule, stages: Optional[List[str]] = None):\n         datamodule.setup()\ndiff --git a/graphium/trainer/predictor_options.py b/graphium/trainer/predictor_options.py\nindex 04a62e84b..5303976a9 100644\n--- a/graphium/trainer/predictor_options.py\n+++ b/graphium/trainer/predictor_options.py\n@@ -30,10 +30,6 @@\n \n from torch import nn\n \n-from graphium.utils.spaces import LOSS_DICT\n-from graphium.utils.spaces import SCHEDULER_DICT\n-\n-\n @dataclass\n class ModelOptions:\n     r\"\"\"\n@@ -117,6 +113,7 @@ def set_kwargs(self):\n         scheduler_class = torch_scheduler_kwargs.pop(\"module_type\")\n         if self.scheduler_class is None:\n             if isinstance(scheduler_class, str):\n+                from graphium.utils.spaces import SCHEDULER_DICT\n                 self.scheduler_class = SCHEDULER_DICT[scheduler_class]\n             elif isclass(scheduler_class):\n                 self.scheduler_class = scheduler_class\n@@ -196,12 +193,15 @@ def parse_loss_fun(loss_fun: Union[str, Dict, Callable]) -> Callable:\n                 Function or callable to compute the loss, takes `preds` and `targets` as inputs.\n         \"\"\"\n \n+        from graphium.utils.spaces import LOSS_DICT # Avoiding circular imports with `spaces.py`\n+\n         if isinstance(loss_fun, str):\n             if loss_fun not in LOSS_DICT.keys():\n                 raise ValueError(\n                     f\"`loss_fun` expected to be one of the strings in {LOSS_DICT.keys()}. \"\n                     f\"Provided: {loss_fun}.\"\n                 )\n+            loss_name = loss_fun\n             loss_fun = LOSS_DICT[loss_fun]()\n         elif isinstance(loss_fun, dict):\n             if loss_fun.get(\"name\") is None:\n@@ -214,10 +214,12 @@ def parse_loss_fun(loss_fun: Union[str, Dict, Callable]) -> Callable:\n             loss_fun = deepcopy(loss_fun)\n             loss_name = loss_fun.pop(\"name\")\n             loss_fun = LOSS_DICT[loss_name](**loss_fun)\n-        elif not callable(loss_fun):\n+        elif callable(loss_fun):\n+            loss_name = str(loss_fun)\n+        else:\n             raise ValueError(f\"`loss_fun` must be `str`, `dict` or `callable`. Provided: {type(loss_fun)}\")\n \n-        return loss_fun\n+        return loss_name, loss_fun\n \n \n @dataclass\ndiff --git a/graphium/trainer/predictor_summaries.py b/graphium/trainer/predictor_summaries.py\nindex 7a4d9292f..06e762dde 100644\n--- a/graphium/trainer/predictor_summaries.py\n+++ b/graphium/trainer/predictor_summaries.py\n@@ -14,14 +14,18 @@\n \n r\"\"\"Classes to store information about resulting evaluation metrics when using a Predictor Module.\"\"\"\n \n-from typing import Any, Callable, Dict, List, Optional, Union\n+from typing import Any, Callable, Dict, List, Optional, Union, Literal, Iterable, Set\n from loguru import logger\n+from copy import deepcopy\n+import inspect\n \n import numpy as np\n import torch\n from torch import Tensor\n+from torchmetrics import MeanMetric, Metric\n+from torchmetrics.aggregation import BaseAggregator\n \n-from graphium.utils.tensor import nan_mean, nan_std, nan_median, tensor_fp16_to_fp32\n+from graphium.trainer.metrics import MetricToConcatenatedTorchMetrics\n \n \n class SummaryInterface(object):\n@@ -29,37 +33,30 @@ class SummaryInterface(object):\n     An interface to define the functions implemented by summary classes that implement SummaryInterface.\n     \"\"\"\n \n-    def set_results(self, **kwargs):\n+    def update(self, preds: Tensor, targets: Tensor) -> None:\n         raise NotImplementedError()\n \n-    def get_dict_summary(self):\n+    def compute(self, **kwargs) -> Tensor:\n         raise NotImplementedError()\n-\n-    def update_predictor_state(self, **kwargs):\n-        raise NotImplementedError()\n-\n-    def get_metrics_logs(self, **kwargs):\n+    \n+    def reset(self) -> None:\n         raise NotImplementedError()\n \n \n-class Summary(SummaryInterface):\n-    # TODO (Gabriela): Default argument cannot be []\n+class SingleTaskSummary(SummaryInterface):\n     def __init__(\n         self,\n-        loss_fun: Union[str, Callable],\n-        metrics: Dict[str, Callable],\n-        metrics_on_training_set: List[str] = [],\n-        metrics_on_progress_bar: List[str] = [],\n-        monitor: str = \"loss\",\n-        mode: str = \"min\",\n+        metrics: Dict[str, Union[Metric, \"MetricWrapper\"]],\n+        step_name: str,\n+        metrics_on_training_set: Optional[List[str]] = None,\n+        metrics_on_progress_bar: Optional[List[str]] = None,\n         task_name: Optional[str] = None,\n+        compute_mean: bool = True,\n+        compute_std: bool = True,\n     ):\n         r\"\"\"\n         A container to be used by the Predictor Module that stores the results for the given metrics on the predictions and targets provided.\n         Parameters:\n-            loss_fun:\n-            Loss function used during training. Acceptable strings are 'mse', 'bce', 'mae', 'cosine'.\n-            Otherwise, a callable object must be provided, with a method `loss_fun._get_name()`.\n \n             metrics:\n             A dictionnary of metrics to compute on the prediction, other than the loss function.\n@@ -67,394 +64,293 @@ def __init__(\n \n             metrics_on_training_set:\n             The metrics names from `metrics` to be computed on the training set for each iteration.\n-            If `None`, all the metrics are computed. Using less metrics can significantly improve\n-            performance, depending on the number of readouts.\n+            If `None`, no metrics are computed.\n \n             metrics_on_progress_bar:\n-            The metrics names from `metrics` to display also on the progress bar of the training\n-\n-            monitor:\n-            `str` metric to track (Default=`\"loss/val\"`)\n+            The metrics names from `metrics` to display also on the progress bar of the training.\n+            If `None`, no metrics are displayed.\n \n             task_name:\n             name of the task (Default=`None`)\n \n+            compute_mean:\n+            whether to compute the mean of the predictions and targets\n+\n+            compute_std:\n+            whether to compute the standard deviation of the predictions and targets\n+\n         \"\"\"\n-        self.loss_fun = loss_fun\n-        self.metrics = metrics\n-        self.metrics_on_training_set = metrics_on_training_set\n-        self.metrics_on_progress_bar = metrics_on_progress_bar\n-        self.monitor = monitor\n-        self.mode = mode\n+        self.step_name = step_name\n+        self.compute_mean = compute_mean\n+        self.compute_std = compute_std\n \n-        self.summaries = {}\n-        self.best_summaries = {}\n+        if not isinstance(metrics, dict):\n+            raise ValueError(f\"metrics must be a dictionary. Got {type(metrics)}\")\n+        self.metrics = deepcopy(metrics)\n \n         # Current predictor state\n         # self.predictor_outputs = None\n-        self.step_name: str = None\n-        self.targets: Tensor = None\n-        self.preds: Tensor = None\n-        self.loss = None  # What type?\n-        self.n_epochs: int = None\n-\n         self.task_name = task_name\n-        self.logged_metrics_exceptions = []  # Track which metric exceptions have been logged\n+        self.logged_metrics_exceptions: List[str] = []  # Track which metric exceptions have been logged\n+        self.last_metrics_exceptions: List[str] = []  # Track which metric exceptions have been logged\n+\n+        # Add default metrics\n+        if (\"mean_preds\" not in self.metrics) and compute_mean:\n+            self.metrics[\"mean_preds\"] = MeanMetric(nan_strategy=\"ignore\")\n+        if (\"mean_target\" not in self.metrics) and compute_mean:\n+            self.metrics[\"mean_target\"] = MeanMetric(nan_strategy=\"ignore\")\n+        if (\"std_preds\" not in self.metrics) and compute_std:\n+            self.metrics[\"std_preds\"] = STDMetric(nan_strategy=\"ignore\")\n+        if (\"std_target\" not in self.metrics) and compute_std:\n+            self.metrics[\"std_target\"] = STDMetric(nan_strategy=\"ignore\")\n+\n+        # Parse the metrics filters\n+        self.metrics_on_training_set = self._parse_metrics_filter(metrics_on_training_set)\n+        self.metrics_on_progress_bar = self._parse_metrics_filter(metrics_on_progress_bar)\n+\n+        # Update the metrics to compute on the training set\n+        if self.compute_mean:\n+            self.metrics_on_training_set.update([\"mean_preds\", \"mean_target\"])\n+        if self.compute_std:\n+            self.metrics_on_training_set.update([\"std_preds\", \"std_target\"])\n+\n+        self._cached_metrics: Dict[str, Tensor] = {}\n+        self._logged_warnings: Set[str] = set() # Set to track which metrics have been logged\n+        self._device: torch.device = None\n+\n+    @property\n+    def get_cached_metrics(self) -> Dict[str, Tensor]:\n+        return deepcopy(self._cached_metrics)\n+\n+    def _parse_metrics_filter(self, filter: Optional[Union[List[str], Dict[str, Any]]]) -> List[str]:\n+        if filter is None:\n+            filter = []\n+        elif isinstance(filter, dict):\n+            filter = list(filter.keys())\n+        elif isinstance(filter, (list, tuple, set)):\n+            filter = list(filter)\n+        elif isinstance(filter, str):\n+            filter = [filter]\n+        else:\n+            raise ValueError(f\"metrics_to_use must be a list or a dictionary. Got {type(filter)}\")\n \n-    def update_predictor_state(\n-        self, step_name: str, targets: Tensor, preds: Tensor, loss: Tensor, n_epochs: int\n-    ):\n+        # Ensure that the filter is a subset of the metrics\n+        all_metrics = set(self.metrics.keys())\n+        filter = set(filter)\n+        if not filter.issubset(all_metrics):\n+            raise ValueError(f\"metrics_to_use must be a subset of the metrics. Got {filter - all_metrics}, available {all_metrics}\")\n+\n+        return filter\n+\n+    @property\n+    def metrics_to_use(self) -> Dict[str, Callable]:\n         r\"\"\"\n-        update the state of the predictor\n-        Parameters:\n-            step_name: which stage you are in, e.g. \"train\"\n-            targets: the targets tensor\n-            predictions: the predictions tensor\n-            loss: the loss tensor\n-            n_epochs: the number of epochs\n+        return the metrics to use by filtering the metrics dictionary if it is the training step. Otherwise, return all metrics.\n         \"\"\"\n-        self.step_name = step_name\n-        self.targets = targets\n-        self.preds = preds\n-        self.loss = loss\n-        self.n_epochs = n_epochs\n \n-    def set_results(\n-        self,\n-        metrics: Dict[str, Tensor],\n-    ):\n+        if self.step_name == \"train\":\n+            metrics_to_use = {\n+                key: metric for key, metric in self.metrics.items() if key in self.metrics_on_training_set\n+            }\n+\n+            return metrics_to_use\n+        return self.metrics\n+    \n+    @staticmethod\n+    def _update(metric_key:str, metric_obj, preds: Tensor, targets: Tensor) -> None:\n         r\"\"\"\n-        set the reults from the metrics\n-        [!] This function requires that self.update_predictor_state() be called before it.\n+        update the state of the metrics\n         Parameters:\n-            metrics: a dictionary of metrics\n+            targets: the targets tensor\n+            predictions: the predictions tensor\n         \"\"\"\n \n-        # Include the task_name in the loss for logging, and similarly for other metrics\n-        metrics[self.metric_log_name(self.task_name, \"loss\", self.step_name)] = self.loss\n-        self.summaries[self.step_name] = Summary.Results(\n-            targets=self.targets,\n-            preds=self.preds,\n-            loss=self.loss,\n-            metrics=metrics,  # Should include task name from get_metrics_logs()\n-            monitored_metric=f\"{self.monitor}/{self.step_name}\",  # Include task name?\n-            n_epochs=self.n_epochs,\n-        )\n-        if self.is_best_epoch(self.step_name, self.loss, metrics):\n-            self.best_summaries[self.step_name] = self.summaries[self.step_name]\n+        # Check the `metric_obj.update` signature to know if it takes `preds` and `targets` or only one of them\n+        varnames = [val.name for val in inspect.signature(metric_obj.update).parameters.values()]\n+        if (\"preds\" == varnames[0]) and (\"target\" == varnames[1]):\n+            # The typical case of `torchmetrics`\n+            metric_obj.update(preds, targets)\n+        elif (\"preds\" == varnames[1]) and (\"target\" == varnames[0]):\n+            # Unusual case where the order of the arguments is reversed\n+            metric_obj.update(targets, preds)\n+        elif (\"value\" == varnames[0]) and (\"preds\" in metric_key):\n+            # The case where the metric takes only one value, and it is the prediction\n+            metric_obj.update(preds)\n+        elif (\"value\" == varnames[0]) and (\"target\" in metric_key):\n+            # The case where the metric takes only one value, and it is the target\n+            metric_obj.update(targets)\n+        else:\n+            raise ValueError(f\"Metric {metric_key} update method signature `{varnames}` is not recognized.\")\n \n-    def is_best_epoch(self, step_name: str, loss: Tensor, metrics: Dict[str, Tensor]) -> bool:\n+\n+    def update(self, preds: Tensor, targets: Tensor) -> None:\n         r\"\"\"\n-        check if the current epoch is the best epoch based on self.mode criteria\n+        update the state of the metrics\n         Parameters:\n-            step_name: which stage you are in, e.g. \"train\"\n-            loss: the loss tensor\n-            metrics: a dictionary of metrics\n+            targets: the targets tensor\n+            predictions: the predictions tensor\n         \"\"\"\n \n-        # TODO (Gabriela): Check for bugs related to monitor_name\n-        if not (step_name in self.best_summaries.keys()):\n-            return True\n-\n-        # Include the task_name in the loss for logging, and similarly for other metrics\n-        metrics[self.metric_log_name(self.task_name, \"loss\", self.step_name)] = loss\n-        monitor_name = f\"{self.monitor}/{step_name}\"  # Include task_name?\n-        if (\n-            not monitor_name in self.best_summaries.keys()\n-        ):  # Feels like there's a bug here. What is this trying to do???\n-            return True\n-\n-        if self.mode == \"max\":\n-            return metrics[monitor_name] > self.best_summaries[step_name].monitored\n-        elif self.mode == \"min\":\n-            return metrics[monitor_name] < self.best_summaries[step_name].monitored\n+        self._device = preds.device\n+\n+        for metric_key, metric_obj in self.metrics_to_use.items():\n+            metric_obj.to(self.device)\n+            try:\n+                self._update(metric_key, metric_obj, preds, targets)\n+            except Exception as err:\n+                err_msg = f\"Error for metric {metric_key} on task {self.task_name} and step {self.step_name}. Exception: {err}\"\n+                # Check if the error is due to the device mismatch, cast to the device, and retry\n+\n+                if err_msg not in self._logged_warnings:\n+                    logger.warning(err_msg)\n+                    self._logged_warnings.add(err_msg)\n+                \n+\n+    def _compute(self, metrics_to_use: Optional[Union[List[str], Dict[str, Any]]] = None) -> Dict[str, Tensor]:\n+\n+        # Parse the metrics to use\n+        if metrics_to_use is None:\n+            metrics_to_use = list(self.metrics.keys())\n+        elif isinstance(metrics_to_use, dict):\n+            metrics_to_use = list(metrics_to_use.keys())\n         else:\n-            ValueError(f\"Mode must be 'min' or 'max', provided `{self.mode}`\")\n+            raise ValueError(f\"metrics_to_use must be a list or a dictionary. Got {type(metrics_to_use)}\")\n+        \n+        self.last_metrics_exceptions = []  # Reset the exceptions for this step\n+\n+        # Compute the metrics\n+        computed_metrics = {}\n+        for metric_key in metrics_to_use:\n+            metric_name = self.metric_log_name(metric_key)\n+            metric_obj = self.metrics[metric_key]\n+            try:\n+                computed_metrics[f\"{metric_name}\"] = metric_obj.compute()\n+            except Exception as e:\n+                # If the metric computation fails, return NaN and log a warning only once\n+                computed_metrics[f\"{metric_name}\"] = torch.tensor(torch.nan, device=self.device)\n+                # Warn only if it's the first warning for that metric\n+                if metric_name not in self.logged_metrics_exceptions:\n+                    self.logged_metrics_exceptions.append(metric_name)\n+                    logger.warning(f\"Error for metric {metric_name}. NaN is returned. Exception: {e}\")\n+                self.last_metrics_exceptions.append(metric_name)\n \n-    def get_results(\n-        self,\n-        step_name: str,\n-    ):\n+        return computed_metrics\n+\n+    def compute(self) -> Dict[str, Tensor]:\n         r\"\"\"\n-        retrieve the results for a given step\n-        Parameters:\n-            step_name: which stage you are in, e.g. \"train\"\n+        compute the metrics\n         Returns:\n-            the results for the given step\n+            the computed metrics\n         \"\"\"\n-        return self.summaries[step_name]\n+        computed_metrics = self._compute(metrics_to_use=self.metrics_to_use)\n+        self._cached_metrics = computed_metrics\n \n-    def get_best_results(\n-        self,\n-        step_name: str,\n-    ):\n+        return computed_metrics\n+    \n+    def reset(self) -> None:\n         r\"\"\"\n-        retrieve the best results for a given step\n-        Parameters:\n-            step_name: which stage you are in, e.g. \"train\"\n-        Returns:\n-            the best results for the given step\n+        reset the state of the metrics\n         \"\"\"\n-        return self.best_summaries[step_name]\n+        for metric_key, metric in self.metrics.items():\n+            try:\n+                metric.reset()\n+            except AttributeError as e:\n+                metric_name = self.metric_log_name(metric_key)\n+                # Skip error if the message is `AttributeError: 'Tensor' object has no attribute 'clear'. Did you mean: 'char'?`\n+                # This error happens when there's nothing to reset, usually because the metric failed.\n+                if (metric_name not in self.last_metrics_exceptions) or (\"'Tensor' object has no attribute 'clear'\" not in str(e)):\n+                    raise e\n \n     def get_results_on_progress_bar(\n         self,\n-        step_name: str,\n     ) -> Dict[str, Tensor]:\n         r\"\"\"\n         retrieve the results to be displayed on the progress bar for a given step\n-        Parameters:\n-            step_name: which stage you are in, e.g. \"train\"\n-        Returns:\n-            the results to be displayed on the progress bar for the given step\n-        \"\"\"\n-        results = self.summaries[step_name]\n-        results_prog = {\n-            # f\"{kk}/{step_name}\": results.metrics[f\"{kk}/{step_name}\"] for kk in self.metrics_on_progress_bar\n-            self.metric_log_name(self.task_name, kk, step_name): results.metrics[\n-                self.metric_log_name(self.task_name, kk, step_name)\n-            ]\n-            for kk in self.metrics_on_progress_bar\n-        }\n-        return results_prog\n \n-    def get_dict_summary(self) -> Dict[str, Any]:\n-        r\"\"\"\n-        retrieve the full summary in a dictionary\n         Returns:\n-            the full summary in a dictionary\n-        \"\"\"\n-        full_dict = {}\n-        # Get metric summaries\n-        full_dict[\"metric_summaries\"] = {}\n-        for key, val in self.summaries.items():\n-            full_dict[\"metric_summaries\"][key] = {k: v for k, v in val.metrics.items()}\n-            full_dict[\"metric_summaries\"][key][\"n_epochs\"] = val.n_epochs\n-\n-        # Get metric summaries at best epoch\n-        full_dict[\"best_epoch_metric_summaries\"] = {}\n-        for key, val in self.best_summaries.items():\n-            full_dict[\"best_epoch_metric_summaries\"][key] = val.metrics\n-            full_dict[\"best_epoch_metric_summaries\"][key][\"n_epochs\"] = val.n_epochs\n-\n-        return full_dict\n-\n-    def get_metrics_logs(self) -> Dict[str, Any]:\n-        r\"\"\"\n-        Get the data about metrics to log.\n-        Note: This function requires that self.update_predictor_state() be called before it.\n-        Returns:\n-            A dictionary of metrics to log.\n+            the results to be displayed on the progress bar for the given step\n         \"\"\"\n+        cached_metrics = self.get_cached_metrics\n+        if cached_metrics is None:\n+            results_prog = self._compute(metrics_to_use=self.metrics_on_progress_bar)\n+        else:\n+            results_prog = {}\n+            for metric_key in self.metrics_on_progress_bar:\n+                metric_name = self.metric_log_name(metric_key)\n+                results_prog[metric_name] = cached_metrics[metric_name]\n \n-        targets = tensor_fp16_to_fp32(self.targets)\n-        preds = tensor_fp16_to_fp32(self.preds)\n-\n-        targets = targets.to(dtype=preds.dtype, device=preds.device)\n-\n-        # Compute the metrics always used in regression tasks\n-        metric_logs = {}\n-        metric_logs[self.metric_log_name(self.task_name, \"mean_pred\", self.step_name)] = nan_mean(preds)\n-        metric_logs[self.metric_log_name(self.task_name, \"std_pred\", self.step_name)] = nan_std(preds)\n-        metric_logs[self.metric_log_name(self.task_name, \"median_pred\", self.step_name)] = nan_median(preds)\n-        metric_logs[self.metric_log_name(self.task_name, \"mean_target\", self.step_name)] = nan_mean(targets)\n-        metric_logs[self.metric_log_name(self.task_name, \"std_target\", self.step_name)] = nan_std(targets)\n-        metric_logs[self.metric_log_name(self.task_name, \"median_target\", self.step_name)] = nan_median(\n-            targets\n-        )\n-\n-        # Specify which metrics to use\n-        metrics_to_use = self.metrics\n-        if self.step_name == \"train\":\n-            metrics_to_use = {\n-                key: metric for key, metric in metrics_to_use.items() if key in self.metrics_on_training_set\n-            }\n-        # Compute the additional metrics\n-        for key, metric in metrics_to_use.items():\n-            metric_name = self.metric_log_name(\n-                self.task_name, key, self.step_name\n-            )  # f\"{key}/{self.step_name}\"\n-            try:\n-                metric_logs[metric_name] = metric(preds, targets)\n-            except Exception as e:\n-                metric_logs[metric_name] = torch.as_tensor(float(\"nan\"))\n-                # Warn only if it's the first warning for that metric\n-                if metric_name not in self.logged_metrics_exceptions:\n-                    self.logged_metrics_exceptions.append(metric_name)\n-                    logger.warning(f\"Error for metric {metric_name}. NaN is returned. Exception: {e}\")\n-\n-        # Convert all metrics to CPU, except for the loss\n-        # metric_logs[f\"{self.loss_fun._get_name()}/{self.step_name}\"] = self.loss.detach().cpu()\n-        metric_logs[\n-            self.metric_log_name(self.task_name, self.loss_fun._get_name(), self.step_name)\n-        ] = self.loss.detach().cpu()\n-        # print(\"Metrics logs keys: \", metric_logs.keys())\n-        metric_logs = {key: metric.detach().cpu() for key, metric in metric_logs.items()}\n-\n-        return metric_logs\n+        return results_prog\n \n-    def metric_log_name(self, task_name, metric_name, step_name):\n-        if task_name is None:\n-            return f\"{metric_name}/{step_name}\"\n+    def metric_log_name(self, metric_name):\n+        if self.task_name is None:\n+            return f\"{metric_name}/{self.step_name}\"\n         else:\n-            return f\"{task_name}/{metric_name}/{step_name}\"\n-\n-    class Results:\n-        def __init__(\n-            self,\n-            targets: Tensor = None,\n-            preds: Tensor = None,\n-            loss: float = None,  # Is this supposed to be a Tensor or float?\n-            metrics: dict = None,\n-            monitored_metric: str = None,\n-            n_epochs: int = None,\n-        ):\n-            r\"\"\"\n-            This inner class is used as a container for storing the results of the summary.\n-            Parameters:\n-                targets: the targets\n-                preds: the prediction tensor\n-                loss: the loss, float or tensor\n-                metrics: the metrics\n-                monitored_metric: the monitored metric\n-                n_epochs: the number of epochs\n-            \"\"\"\n-            self.targets = targets.detach().cpu()\n-            self.preds = preds.detach().cpu()\n-            self.loss = loss.item() if isinstance(loss, Tensor) else loss\n-            self.monitored_metric = monitored_metric\n-            if monitored_metric in metrics.keys():\n-                self.monitored = metrics[monitored_metric].detach().cpu()\n-            self.metrics = {\n-                key: value.tolist() if isinstance(value, (Tensor, np.ndarray)) else value\n-                for key, value in metrics.items()\n-            }\n-            self.n_epochs = n_epochs\n+            return f\"{self.task_name}/{metric_name}/{self.step_name}\"\n+        \n+    @property\n+    def device(self) -> Optional[torch.device]:\n+        return self._device\n \n \n-class TaskSummaries(SummaryInterface):\n+class MultiTaskSummary(SummaryInterface):\n     def __init__(\n         self,\n-        task_loss_fun: Callable,\n-        task_metrics: Dict[str, Callable],\n-        task_metrics_on_training_set: List[str],\n-        task_metrics_on_progress_bar: List[str],\n-        monitor: str = \"loss\",\n-        mode: str = \"min\",\n+        task_metrics: Dict[str, Dict[str, Union[Metric, \"MetricWrapper\"]]],\n+        step_name: str,\n+        task_metrics_on_training_set: Optional[Dict[str, List[str]]] = None,\n+        task_metrics_on_progress_bar: Optional[Dict[str, List[str]]] = None,\n+        compute_mean: bool = True,\n+        compute_std: bool = True,\n     ):\n         r\"\"\"\n         class to store the summaries of the tasks\n         Parameters:\n-            task_loss_fun: the loss function for each task\n-            task_metrics: the metrics for each task\n-            task_metrics_on_training_set: the metrics to use on the training set\n-            task_metrics_on_progress_bar: the metrics to use on the progress bar\n-            monitor: the metric to monitor\n-            mode: the mode of the metric to monitor\n+\n+        \n+            compute_mean:\n+            whether to compute the mean of the predictions and targets\n+\n+            compute_std:\n+            whether to compute the standard deviation of the predictions and targets\n+\n         \"\"\"\n-        self.task_loss_fun = task_loss_fun\n         self.task_metrics = task_metrics\n-        self.task_metrics_on_progress_bar = task_metrics_on_progress_bar\n-        self.task_metrics_on_training_set = task_metrics_on_training_set\n-        self.monitor = monitor\n-        self.mode = mode\n-\n-        self.task_summaries: Dict[str, Summary] = {}\n-        self.task_best_summaries: Dict[str, Summary] = {}\n-        self.tasks = list(task_loss_fun.keys())\n+        self.task_metrics_on_progress_bar = task_metrics_on_progress_bar if task_metrics_on_progress_bar is not None else {}\n+        self.task_metrics_on_training_set = task_metrics_on_training_set if task_metrics_on_training_set is not None else {}\n \n+        # Initialize all the single-task summaries\n+        self.tasks = list(task_metrics.keys())\n+        self.task_summaries: Dict[str, SingleTaskSummary] = {}\n         for task in self.tasks:\n-            self.task_summaries[task] = Summary(\n-                self.task_loss_fun[task],\n-                self.task_metrics[task],\n-                self.task_metrics_on_training_set[task],\n-                self.task_metrics_on_progress_bar[task],\n-                self.monitor,\n-                self.mode,\n-                task_name=task,\n+            self.task_summaries[task] = SingleTaskSummary(\n+                metrics = self.task_metrics[task],\n+                step_name = step_name,\n+                metrics_on_training_set = self.task_metrics_on_training_set[task] if task in self.task_metrics_on_training_set else None,\n+                metrics_on_progress_bar = self.task_metrics_on_progress_bar[task] if task in self.task_metrics_on_progress_bar else None,\n+                task_name = task,\n+                compute_mean = compute_mean,\n+                compute_std = compute_std,\n             )\n \n-        # Current predictor state\n-        self.weighted_loss = None\n-        self.step_name = None\n+    def __getitem__(self, task: str) -> SingleTaskSummary:\n+        return self.task_summaries[task]\n+    \n+    def keys(self) -> List[str]:\n+        return self.tasks\n \n-    def update_predictor_state(\n-        self,\n-        step_name: str,\n-        targets: Dict[str, Tensor],\n-        preds: Dict[str, Tensor],\n-        loss: Tensor,\n-        task_losses: Dict[str, Tensor],\n-        n_epochs: int,\n-    ):\n+    def update(self, preds: Dict[str, Tensor], targets: Dict[str, Tensor]) -> None:\n         r\"\"\"\n         update the state for all predictors\n         Parameters:\n-            step_name: the name of the step\n             targets: the target tensors\n             preds: the prediction tensors\n-            loss: the loss tensor\n-            task_losses: the task losses\n-            n_epochs: the number of epochs\n         \"\"\"\n-        self.weighted_loss = loss\n-        self.step_name = step_name\n         for task in self.tasks:\n-            self.task_summaries[task].update_predictor_state(\n-                step_name,\n-                targets[task],\n+            self.task_summaries[task].update(\n                 preds[task].detach(),\n-                task_losses[task].detach(),\n-                n_epochs,\n+                targets[task],\n             )\n \n-    def set_results(self, task_metrics: Dict[str, Dict[str, Tensor]]):\n-        \"\"\"\n-        set the results for all tasks\n-        Parameters:\n-            task_metrics: the metrics for each task\n-        \"\"\"\n-        for task in self.tasks:\n-            self.task_summaries[task].set_results(task_metrics[task])\n-            step_name = self.task_summaries[task].step_name\n-            loss = self.task_summaries[task].loss\n-            if self.task_summaries[task].is_best_epoch(step_name, loss, task_metrics[task]):\n-                self.task_summaries[task].best_summaries[step_name] = self.task_summaries[task].summaries[\n-                    step_name\n-                ]\n-\n-    def get_results(\n-        self,\n-        step_name: str,\n-    ) -> Dict[str, Dict[str, Any]]:\n-        \"\"\"\n-        retrieve the results\n-        Parameters:\n-            step_name: the name of the step, i.e. \"train\"\n-        Returns:\n-            the results\n-        \"\"\"\n-        results = {}\n-        for task in self.tasks:\n-            results[task] = self.task_summaries[task].get_results(step_name)\n-        return results\n-\n-    def get_best_results(\n-        self,\n-        step_name: str,\n-    ) -> Dict[str, Dict[str, Any]]:\n-        \"\"\"\n-        retrieve the best results\n-        Parameters:\n-            step_name: the name of the step, i.e. \"train\"\n-        Returns:\n-            the best results\n-        \"\"\"\n-        results = {}\n-        for task in self.tasks:\n-            results[task] = self.task_summaries[task].get_best_results(step_name)\n-        return results\n-\n     def get_results_on_progress_bar(\n         self,\n         step_name: str,\n@@ -469,78 +365,112 @@ def get_results_on_progress_bar(\n         \"\"\"\n         task_results_prog = {}\n         for task in self.tasks:\n-            # task_results_prog[task] = self.task_summaries[task].get_results_on_progress_bar(step_name)\n             task_results_prog.update(self.task_summaries[task].get_results_on_progress_bar(step_name))\n         return task_results_prog\n \n-    def get_dict_summary(\n-        self,\n-    ) -> Dict[str, Dict[str, Any]]:\n+    def compute(self) -> Dict[str, Tensor]:\n         r\"\"\"\n-        get task summaries in a dictionary\n+        compute the metrics for all tasks\n         Returns:\n-            the task summaries\n+            the computed metrics for all tasks\n         \"\"\"\n-        task_full_dict = {}\n+        computed_metrics = {}\n         for task in self.tasks:\n-            task_full_dict[task] = self.task_summaries[task].get_dict_summary()\n-        return task_full_dict\n-\n-    def get_metrics_logs(\n-        self,\n-    ) -> Dict[str, Dict[str, Tensor]]:\n+            computed_metrics.update(self.task_summaries[task].compute())\n+        return computed_metrics\n+    \n+    def reset(self) -> None:\n         r\"\"\"\n-        get the logs for the metrics\n-        Returns:\n-            the task logs for the metrics\n+        reset the state of the metrics\n         \"\"\"\n-        task_metrics_logs = {}\n         for task in self.tasks:\n-            task_metrics_logs[task] = self.task_summaries[task].get_metrics_logs()\n-            # average metrics\n-            for key in task_metrics_logs[task]:\n-                if isinstance(task_metrics_logs[task][key], torch.Tensor):\n-                    if task_metrics_logs[task][key].numel() > 1:\n-                        task_metrics_logs[task][key] = task_metrics_logs[task][key][\n-                            task_metrics_logs[task][key] != 0\n-                        ].mean()\n-\n-        # Include global (weighted loss)\n-        task_metrics_logs[\"_global\"] = {}\n-        task_metrics_logs[\"_global\"][f\"loss/{self.step_name}\"] = self.weighted_loss.detach().cpu()\n-        return task_metrics_logs\n-\n-    # TODO (Gabriela): This works to fix the logging on TB, but make it more efficient\n-    def concatenate_metrics_logs(\n-        self,\n-        metrics_logs: Dict[str, Dict[str, Tensor]],\n-        device: str,\n-    ) -> Dict[str, Tensor]:\n-        r\"\"\"\n-        concatenate the metrics logs\n-        Parameters:\n-            metrics_logs: the metrics logs\n-        Returns:\n-            the concatenated metrics logs\n-        \"\"\"\n-        concatenated_metrics_logs = {}\n-        for task in list(self.tasks) + [\"_global\"]:\n-            concatenated_metrics_logs.update(metrics_logs[task])\n-        concatenated_metrics_logs[f\"loss/{self.step_name}\"] = self.weighted_loss.detach().to(device)\n-        return concatenated_metrics_logs\n+            self.task_summaries[task].reset()\n \n-    def metric_log_name(\n-        self,\n-        task_name: str,\n-        metric_name: str,\n-        step_name: str,\n-    ) -> str:\n-        r\"\"\"\n-        print the metric name, task name and step name\n-        Returns:\n-            the metric name, task name and step name\n-        \"\"\"\n-        if task_name is None:\n-            return f\"{metric_name}/{step_name}\"\n+\n+class STDMetric(BaseAggregator):\n+    \"\"\"\n+    A metric to compute the standard deviation of the predictions or targets.\n+    Based on `torchmetrics.Metric`, with a similar implementation to `torchmetric.MeanMetric`.\n+\n+    Parameters:\n+        correction: \n+            The correction to apply to the standard deviation. Instead of dividing by number of samples `N`,\n+            we divide by `N-correction`.\n+\n+        nan_strategy: options:\n+            - ``'error'``: if any `nan` values are encountered will give a RuntimeError\n+            - ``'warn'``: if any `nan` values are encountered will give a warning and continue\n+            - ``'ignore'``: all `nan` values are silently removed\n+            - a float: if a float is provided will impute any `nan` values with this value\n+\n+    \"\"\"\n+    def __init__(self, nan_strategy: Union[Literal[\"error\", \"warn\", \"ignore\"], float]=\"warn\", correction:int=0, **kwargs):\n+        super().__init__(\n+            \"sum\",\n+            default_value=torch.tensor(0.0, dtype=torch.get_default_dtype()),\n+            nan_strategy=nan_strategy,\n+            state_name=\"mean_value\",\n+            **kwargs,\n+        )\n+        self.add_state(\"sum\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n+        self.add_state(\"sum_of_squares\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n+        self.add_state(\"total_weight\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n+        self.correction = correction\n+\n+    def update(self, value: Union[float, Tensor], weight: Union[float, Tensor] = 1.0) -> None:\n+        if not isinstance(value, Tensor):\n+            value = torch.as_tensor(value, dtype=torch.float32, device=value.device)\n+        if not isinstance(weight, Tensor):\n+            weight = torch.as_tensor(weight, dtype=torch.float32, device=value.device)\n+\n+        weight = torch.broadcast_to(weight, value.shape).clone()\n+        # Check whether `_cast_and_nan_check_input` takes in `weight`\n+        if \"weight\" in inspect.signature(self._cast_and_nan_check_input).parameters:\n+            value, weight = self._cast_and_nan_check_input(value, weight)\n         else:\n-            return f\"{task_name}/{metric_name}/{step_name}\"\n+            weight[value.isnan()] = torch.nan\n+            value = self._cast_and_nan_check_input(value)\n+            weight = self._cast_and_nan_check_input(weight)\n+\n+        if value.numel() == 0:\n+            return\n+\n+        self.sum += (value * weight).sum()\n+        self.sum_of_squares += (value * value * weight).sum()\n+        self.total_weight += weight.sum()\n+\n+    def compute(self) -> Tensor:\n+        dividor = max(0, self.total_weight - self.correction)\n+        mean = self.sum / self.total_weight\n+        mean_of_squares = self.sum_of_squares / self.total_weight\n+        variance = mean_of_squares - mean ** 2\n+        variance_corr = variance * (self.total_weight / dividor)\n+        return torch.sqrt(variance_corr)\n+\n+class GradientNormMetric(Metric):\n+    \"\"\"\n+    A metric to compute the norm of the gradient.\n+    Based on `torchmetrics.Metric`.\n+\n+    Warning:\n+        This metric is not compatible with other metrics since it doesn't take\n+        the predictions and targets as input. It takes the model as input.\n+        It also doesn't work per task, but for the full model\n+    \"\"\"\n+    def __init__(self, dist_sync_on_step=False):\n+        super().__init__(dist_sync_on_step=dist_sync_on_step)\n+        self.add_state(\"gradient_norm_sq\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n+        self.add_state(\"total_steps\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n+\n+    def update(self, model: torch.nn.Module) -> None:\n+        total_norm = torch.tensor(0.0, device=self.device)\n+        for p in model.parameters():\n+            if p.grad is not None:\n+                param_norm = p.grad.detach().data.norm(2)\n+                total_norm += param_norm.detach() ** 2\n+        self.gradient_norm_sq += total_norm\n+        self.total_steps += 1\n+\n+    def compute(self) -> Tensor:\n+        return (self.gradient_norm_sq / self.total_steps).sqrt()\n+\ndiff --git a/graphium/trainer/progress_bar.py b/graphium/trainer/progress_bar.py\nnew file mode 100644\nindex 000000000..a5dde1293\n--- /dev/null\n+++ b/graphium/trainer/progress_bar.py\n@@ -0,0 +1,27 @@\n+import sys\n+from typing import Any, Callable, Dict, List, Optional, Union, Literal, Iterable\n+from lightning.pytorch.callbacks import TQDMProgressBar\n+\n+\n+\n+class ProgressBarMetrics(TQDMProgressBar):\n+    def __init__(self, metrics_on_progress_bar: Optional[Iterable[str]] = None, loss_alias:Optional[str]=\"_global/loss/train\") -> None:\n+        super().__init__()\n+        if metrics_on_progress_bar is None:\n+            metrics_on_progress_bar = {}\n+        self.metrics_on_progress_bar = set(metrics_on_progress_bar)\n+        self.loss_alias = loss_alias\n+\n+    def get_metrics(self, trainer, pl_module) -> Dict[str, Union[int, str, float, Dict[str, float]]]:\n+        \n+        metrics = super().get_metrics(trainer, pl_module)\n+        filtered_metrics = {}\n+        for key, metric in metrics.items():\n+            if key in self.metrics_on_progress_bar:\n+                if key == self.loss_alias:\n+                    filtered_metrics[\"loss\"] = metric\n+                else:\n+                    filtered_metrics[key] = metric\n+        \n+        return filtered_metrics\n+\ndiff --git a/graphium/utils/spaces.py b/graphium/utils/spaces.py\nindex 12f07a1ed..c7b6a7ac9 100644\n--- a/graphium/utils/spaces.py\n+++ b/graphium/utils/spaces.py\n@@ -15,7 +15,7 @@\n from copy import deepcopy\n import torch\n import torch.optim.lr_scheduler as sc\n-import torchmetrics.functional as TorchMetrics\n+import torchmetrics as TorchMetrics\n \n import graphium.nn.base_layers as BaseLayers\n import graphium.nn.ensemble_layers as EnsembleLayers\n@@ -23,7 +23,6 @@\n import graphium.utils.custom_lr as CustomLR\n import graphium.data.datamodule as Datamodules\n import graphium.ipu.ipu_losses as IPULosses\n-import graphium.ipu.ipu_metrics as Metrics\n import graphium.nn.pyg_layers as PygLayers\n import graphium.nn.residual_connections as Residuals\n import graphium.nn.encoders as Encoders\n@@ -102,39 +101,27 @@\n }\n \n METRICS_CLASSIFICATION = {\n-    \"accuracy\": TorchMetrics.accuracy,\n-    \"averageprecision\": TorchMetrics.average_precision,\n-    \"auroc\": TorchMetrics.auroc,\n-    \"confusionmatrix\": TorchMetrics.confusion_matrix,\n-    \"f1\": TorchMetrics.f1_score,\n-    \"fbeta\": TorchMetrics.fbeta_score,\n-    \"precisionrecallcurve\": TorchMetrics.precision_recall_curve,\n-    \"precision\": TorchMetrics.precision,\n-    \"recall\": TorchMetrics.recall,\n-    \"mcc\": TorchMetrics.matthews_corrcoef,\n-    \"auroc_ipu\": Metrics.auroc_ipu,\n-    \"accuracy_ipu\": Metrics.accuracy_ipu,\n-    \"average_precision_ipu\": Metrics.average_precision_ipu,\n-    \"f1_ipu\": Metrics.f1_score_ipu,\n-    \"fbeta_ipu\": Metrics.fbeta_score_ipu,\n-    \"precision_ipu\": Metrics.precision_ipu,\n-    \"recall_ipu\": Metrics.recall_ipu,\n+    \"accuracy\": TorchMetrics.Accuracy,\n+    \"averageprecision\": TorchMetrics.functional.average_precision,  # Not using a class to better handle concatenation of preds and targets\n+    \"auroc\": TorchMetrics.functional.auroc,  # Not using a class to better handle concatenation of preds and targets\n+    \"confusionmatrix\": TorchMetrics.ConfusionMatrix,\n+    \"f1\": TorchMetrics.F1Score,\n+    \"fbeta\": TorchMetrics.FBetaScore,\n+    \"precisionrecallcurve\": TorchMetrics.PrecisionRecallCurve,\n+    \"precision\": TorchMetrics.Precision,\n+    \"recall\": TorchMetrics.Recall,\n+    \"mcc\": TorchMetrics.MatthewsCorrCoef,\n }\n \n METRICS_REGRESSION = {\n-    \"mae\": TorchMetrics.mean_absolute_error,\n-    \"mape\": TorchMetrics.mean_absolute_percentage_error,\n-    \"mse\": TorchMetrics.mean_squared_error,\n-    \"msle\": TorchMetrics.mean_squared_log_error,\n-    \"pearsonr\": TorchMetrics.pearson_corrcoef,\n-    \"spearmanr\": TorchMetrics.spearman_corrcoef,\n-    \"r2_score\": TorchMetrics.r2_score,\n-    \"cosine\": TorchMetrics.cosine_similarity,\n-    \"pearsonr_ipu\": Metrics.pearson_ipu,\n-    \"spearmanr_ipu\": Metrics.spearman_ipu,\n-    \"r2_score_ipu\": Metrics.r2_score_ipu,\n-    \"mae_ipu\": Metrics.mean_absolute_error_ipu,\n-    \"mse_ipu\": Metrics.mean_squared_error_ipu,\n+    \"mae\": TorchMetrics.MeanAbsoluteError,\n+    \"mape\": TorchMetrics.MeanAbsolutePercentageError,\n+    \"mse\": TorchMetrics.MeanSquaredError,\n+    \"msle\": TorchMetrics.MeanSquaredLogError,\n+    \"pearsonr\": TorchMetrics.PearsonCorrCoef,\n+    \"spearmanr\": TorchMetrics.SpearmanCorrCoef,\n+    \"r2_score\": TorchMetrics.R2Score,\n+    \"cosine\": TorchMetrics.CosineSimilarity,\n }\n \n METRICS_DICT = deepcopy(METRICS_CLASSIFICATION)\n", "instance_id": "datamol-io__graphium-517", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in describing the goal of updating the `MetricWrapper` class to support `update` and `compute` methods from `torchmetrics`, addressing an issue with saving predictions and targets over an epoch for training, validation, and test sets. It also mentions converting metrics in `spaces.py` from functions to their class equivalents. However, there are minor ambiguities and missing details. For instance, the statement about \"how to deal with missing labels\" is marked as a task but lacks specifics on what is expected or how it should be handled. Additionally, while the general intent is clear, there are no explicit examples of input/output formats or detailed constraints for the updates to `MetricWrapper`. Edge cases, such as handling large datasets or specific failure modes during metric computation, are not addressed. Overall, the problem is valid and mostly clear but lacks some finer details that would make it comprehensive.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.65, placing this problem in the \"Hard\" category. This evaluation is based on several factors:\n\n1. **Scope and Depth of Code Changes**: The changes span multiple files and modules, including `MetricWrapper` in `metrics.py`, `spaces.py`, and various configuration files. The updates to `MetricWrapper` involve significant modifications to support stateful metric computation with `update` and `compute` methods, which requires a deep understanding of how metrics are handled across training loops. Additionally, converting metrics in `spaces.py` from functions to classes impacts how metrics are initialized and used throughout the codebase, potentially affecting many downstream components. The removal of IPU-specific metrics and related code (e.g., `ipu_metrics.py`) indicates a broader refactoring effort.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several advanced concepts, including the `torchmetrics` library and its stateful metric API, PyTorch Lightning's training loop and callback mechanisms, and handling of multi-task learning scenarios (e.g., `multitask_handling` options like `flatten` and `mean-per-label`). Understanding distributed training nuances (e.g., syncing tensors across devices) and memory management (e.g., moving tensors to CPU to avoid GPU memory issues) is also necessary. The transition from function-based to class-based metrics in `spaces.py` requires knowledge of object-oriented design in Python and how to adapt existing code to new initialization patterns.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement hints at handling missing labels, though it lacks detail. The code changes introduce logic for NaN handling (`target_nan_mask` options like `ignore`), which requires careful consideration of edge cases such as empty tensors after filtering NaNs or ensuring metrics behave correctly with partial data. Error handling for metric computation failures (e.g., returning NaN and logging warnings) is implemented, adding complexity to ensure robustness across different scenarios.\n\n4. **Impact on Codebase Architecture**: The changes significantly impact the training and evaluation pipeline by altering how metrics are computed and logged. The introduction of `MetricToConcatenatedTorchMetrics` to handle memory-efficient concatenation of predictions and targets suggests a focus on performance optimization, which is a non-trivial concern in large-scale machine learning systems. The removal of IPU-specific code and updates to configuration files indicate a broader architectural shift, potentially affecting compatibility with previous setups.\n\nOverall, this problem requires a deep understanding of the codebase, particularly in the areas of metric computation, training loops, and multi-task learning. The modifications are complex, span multiple components, and involve performance considerations, justifying a score of 0.65. It falls short of \"Very Hard\" (0.8-1.0) as it does not appear to require advanced domain-specific knowledge beyond machine learning frameworks or system-level optimizations like distributed consensus protocols.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`event_loop` not found for multiple test modules\nI just want to use module scope fixture but I get an error:\r\n\r\n**[environment]**\r\npython==3.10.14\r\npytest==8.2.2\r\npytest-asyncio==0.23.7\r\n\r\n**[Directory]**\r\n\r\n`tests/conftest.py`:\r\n```python\r\nimport pytest_asyncio\r\n@pytest_asyncio.fixture(scope=\"module\")\r\nasync def foo():\r\n    yield \"a value\"\r\n```\r\n`tests/test_1.py`:\r\n```python\r\nimport pytest\r\n@pytest.mark.asyncio\r\nasync def test_func(foo):\r\n        print(\"test_1 test_func\", foo)\r\n```\r\n`tests/test_2.py`:\r\n```python\r\nimport pytest\r\n@pytest.mark.asyncio\r\nasync def test_func(foo):\r\n    print(\"test_2 test_func\", foo)\r\n```\r\n**[error]**\r\n\r\n```text\r\n# pytest -s tests/\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.10.14, pytest-8.2.2, pluggy-1.5.0\r\nrootdir: /home/sunyw\r\nplugins: asyncio-0.23.7\r\nasyncio: mode=strict\r\ncollected 2 items                                                              \r\n\r\ntests/test_1.py .                                                        [ 50%]\r\ntests/test_2.py E                                                        [100%]\r\n\r\n==================================== ERRORS ====================================\r\n_________________________ ERROR at setup of test_func __________________________\r\nfile /home/xxx/tests/test_2.py, line 3\r\n  @pytest.mark.asyncio\r\n  async def test_func(foo):\r\n      print(\"test_2 test_func\", foo)\r\nfile /home/xxx/tests/conftest.py, line 13\r\n  @pytest_asyncio.fixture(scope=\"module\")\r\n  async def foo():\r\n      yield \"a value\"\r\nE       fixture 'tests/test_1.py::<event_loop>' not found\r\n>       available fixtures: _session_event_loop, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, event_loop, event_loop_policy, foo, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tests/test_2.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\r\n>       use 'pytest --fixtures [testpath]' for help on them.\r\n\r\n/home/xxx/tests/conftest.py:13\r\n=========================== short test summary info ============================\r\nERROR tests/test_2.py::test_func\r\n========================== 1 passed, 1 error in 0.01s ==========================\r\n```\r\n\r\naccording to the document [Decorators](https://pytest-asyncio.readthedocs.io/en/latest/reference/decorators/index.html):\r\n> All scopes are supported, but if you use a non-function scope you will need to redefine the event_loop fixture to have the same or broader scope. Async fixtures need the event loop, and so must have the same or narrower scope than the event_loop fixture.\r\n\r\nI add `event_loop` fixture in `conftest.py`:\r\n\r\n```python\r\nimport pytest_asyncio\r\nimport asyncio\r\nimport pytest\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef event_loop(request):\r\n    loop = asyncio.get_event_loop_policy().get_event_loop()\r\n    yield loop\r\n    loop.close()\r\n\r\n@pytest_asyncio.fixture(scope=\"module\")\r\nasync def foo():\r\n    yield \"a value\"\r\n```\r\nBut this error is still there.\r\n\r\n\r\n\n", "patch": "diff --git a/dependencies/default/requirements.txt b/dependencies/default/requirements.txt\nindex 3ac25aba..42cfc8d3 100644\n--- a/dependencies/default/requirements.txt\n+++ b/dependencies/default/requirements.txt\n@@ -1,3 +1,3 @@\n # Always adjust install_requires in setup.cfg and pytest-min-requirements.txt\n # when changing runtime dependencies\n-pytest >= 7.0.0,<9\n+pytest >= 8.2,<9\ndiff --git a/docs/source/reference/changelog.rst b/docs/source/reference/changelog.rst\nindex c976b8da..abf27a0a 100644\n--- a/docs/source/reference/changelog.rst\n+++ b/docs/source/reference/changelog.rst\n@@ -4,9 +4,11 @@ Changelog\n \n 0.24.0 (UNRELEASED)\n ===================\n+- BREAKING: Updated minimum supported pytest version to v8.2.0\n - Adds an optional `loop_scope` keyword argument to `pytest.mark.asyncio`. This argument controls which event loop is used to run the marked async test. `#706`_, `#871 <https://github.com/pytest-dev/pytest-asyncio/pull/871>`_\n - Deprecates the optional `scope` keyword argument to `pytest.mark.asyncio` for API consistency with ``pytest_asyncio.fixture``. Users are encouraged to use the `loop_scope` keyword argument, which does exactly the same.\n - Raises an error when passing `scope` or `loop_scope` as a positional argument to ``@pytest.mark.asyncio``. `#812 <https://github.com/pytest-dev/pytest-asyncio/issues/812>`_\n+- Fixes a bug that caused module-scoped async fixtures to fail when reused in other modules `#862 <https://github.com/pytest-dev/pytest-asyncio/issues/862>`_ `#668 <https://github.com/pytest-dev/pytest-asyncio/issues/668>`_\n \n \n 0.23.8 (2024-07-17)\ndiff --git a/setup.cfg b/setup.cfg\nindex 9947cbe3..c04d3884 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -41,7 +41,7 @@ include_package_data = True\n \n # Always adjust requirements.txt and pytest-min-requirements.txt when changing runtime dependencies\n install_requires =\n-  pytest >= 7.0.0,<9\n+  pytest >= 8.2,<9\n \n [options.extras_require]\n testing =\n", "instance_id": "pytest-dev__pytest-asyncio-906", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the user is encountering an error when using a module-scoped async fixture with pytest-asyncio across multiple test modules. The environment, directory structure, code snippets, and error messages are provided, which help in understanding the context. Additionally, the user references the relevant documentation and shows an attempt to resolve the issue by adding an `event_loop` fixture. However, there are minor ambiguities, such as the lack of explicit mention of the desired outcome (e.g., whether the goal is to share the fixture across modules or isolate it) and missing details about potential constraints or specific behaviors expected from the solution. Edge cases or specific test scenarios that might be failing are also not mentioned. Overall, the problem is valid and mostly clear, but it lacks some finer details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes appears to be relatively limited, as seen in the provided diff, which involves updating the minimum pytest version and documenting a fix for module-scoped async fixtures in pytest-asyncio. This suggests the solution may already be implemented in a newer version or requires a small configuration tweak. However, understanding the root cause requires familiarity with pytest, pytest-asyncio, async programming in Python, and the specific behavior of event loops and fixture scopes, which are moderately complex concepts. The problem also involves interactions between test modules and fixtures, requiring a grasp of pytest's fixture resolution mechanism. While the code changes in the diff are minimal and do not impact the broader architecture, solving the issue for the user might involve debugging or adjusting fixture definitions in their test setup, which could span multiple files. Edge cases, such as differing event loop policies or fixture reuse across modules, are implied but not explicitly detailed, adding a layer of complexity to ensure robust error handling. Overall, this problem requires understanding multiple technical concepts and potentially making targeted modifications, placing it in the medium difficulty range of 0.4-0.6.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[BUG] some pytest configurations not used\n**Describe the bug**\nSince #16851, all python packages have a `pytest.ini` in `package/tests/`. However, some of the packages previously had pytest-specific configuration in the `pytest.tool.ini-options` section of `pyproject.toml`. These settings are now ignored (as per https://docs.pytest.org/en/8.2.x/reference/customize.html#finding-the-rootdir).\n\nWe should pick a single configuration file that is used (and recommended in the dev guide).\n\nFWIW, my preference is for pyproject.toml, since that's where all the other tool-specific config is going.\n", "patch": "diff --git a/python/cudf/pyproject.toml b/python/cudf/pyproject.toml\nindex 1b730ffd13c..c0776fd0de6 100644\n--- a/python/cudf/pyproject.toml\n+++ b/python/cudf/pyproject.toml\n@@ -124,6 +124,27 @@ skip = [\n     \"__init__.py\",\n ]\n \n+[tool.pytest.ini_options]\n+addopts = \"--tb=native --strict-config --strict-markers\"\n+empty_parameter_set_mark = \"fail_at_collect\"\n+filterwarnings = [\n+    \"error\",\n+    \"ignore:::.*xdist.*\",\n+    \"ignore:::.*pytest.*\",\n+    # some third-party dependencies (e.g. 'boto3') still using datetime.datetime.utcnow()\n+    \"ignore:.*datetime.*utcnow.*scheduled for removal.*:DeprecationWarning:botocore\",\n+    # Deprecation warning from Pyarrow Table.to_pandas() with pandas-2.2+\n+    \"ignore:Passing a BlockManager to DataFrame is deprecated:DeprecationWarning\",\n+    # PerformanceWarning from cupy warming up the JIT cache\n+    \"ignore:Jitify is performing a one-time only warm-up to populate the persistent cache:cupy._util.PerformanceWarning\",\n+    # Ignore numba PEP 456 warning specific to arm machines\n+    \"ignore:FNV hashing is not implemented in Numba.*:UserWarning\"\n+]\n+markers = [\n+    \"spilling: mark benchmark a good candidate to run with `CUDF_SPILL=ON`\"\n+]\n+xfail_strict = true\n+\n [tool.rapids-build-backend]\n build-backend = \"scikit_build_core.build\"\n dependencies-file = \"../../dependencies.yaml\"\ndiff --git a/python/cudf_kafka/pyproject.toml b/python/cudf_kafka/pyproject.toml\nindex a1a3ec37842..87e19a2bccf 100644\n--- a/python/cudf_kafka/pyproject.toml\n+++ b/python/cudf_kafka/pyproject.toml\n@@ -79,9 +79,12 @@ skip = [\n ]\n \n [tool.pytest.ini_options]\n+addopts = \"--tb=native --strict-config --strict-markers\"\n+empty_parameter_set_mark = \"fail_at_collect\"\n filterwarnings = [\n   \"error\"\n ]\n+xfail_strict = true\n \n [tool.scikit-build]\n build-dir = \"build/{wheel_tag}\"\ndiff --git a/python/cudf_polars/pyproject.toml b/python/cudf_polars/pyproject.toml\nindex f55031e0826..5345fad41a2 100644\n--- a/python/cudf_polars/pyproject.toml\n+++ b/python/cudf_polars/pyproject.toml\n@@ -50,6 +50,11 @@ license-files = [\"LICENSE\"]\n version = {file = \"cudf_polars/VERSION\"}\n \n [tool.pytest.ini_options]\n+addopts = \"--tb=native --strict-config --strict-markers\"\n+empty_parameter_set_mark = \"fail_at_collect\"\n+filterwarnings = [\n+  \"error\"\n+]\n xfail_strict = true\n \n [tool.coverage.report]\ndiff --git a/python/custreamz/pyproject.toml b/python/custreamz/pyproject.toml\nindex 85ab0024bb5..af45f49d9b4 100644\n--- a/python/custreamz/pyproject.toml\n+++ b/python/custreamz/pyproject.toml\n@@ -111,6 +111,8 @@ skip = [\n ]\n \n [tool.pytest.ini_options]\n+addopts = \"--tb=native --strict-config --strict-markers\"\n+empty_parameter_set_mark = \"fail_at_collect\"\n filterwarnings = [\n     \"error\",\n     \"ignore:unclosed <socket.socket:ResourceWarning\",\n@@ -118,4 +120,8 @@ filterwarnings = [\n     # Should be fixed in the next streamz release\n     # https://github.com/python-streamz/streamz/commit/2812f1f961dfcb3f17e948d8b12a12472975558e\n     \"ignore:pkg_resources is deprecated as an API:DeprecationWarning:streamz\",\n+    \"ignore:Deprecated call to `pkg_resources.declare_namespace:DeprecationWarning\",\n+    # Ignore numba PEP 456 warning specific to arm machines\n+    \"ignore:FNV hashing is not implemented in Numba.*:UserWarning\"\n ]\n+xfail_strict = true\ndiff --git a/python/dask_cudf/pyproject.toml b/python/dask_cudf/pyproject.toml\nindex ce825c7647b..fbcd7ae5dfb 100644\n--- a/python/dask_cudf/pyproject.toml\n+++ b/python/dask_cudf/pyproject.toml\n@@ -115,6 +115,8 @@ skip = [\n ]\n \n [tool.pytest.ini_options]\n+addopts = \"--tb=native --strict-config --strict-markers\"\n+empty_parameter_set_mark = \"fail_at_collect\"\n filterwarnings = [\n     \"error::FutureWarning\",\n     \"error::DeprecationWarning\",\n@@ -125,3 +127,4 @@ filterwarnings = [\n     \"ignore:Passing a BlockManager to DataFrame is deprecated and will raise in a future version. Use public APIs instead.:DeprecationWarning\",\n     \"ignore:String support for `aggregate_files` is experimental. Behavior may change in the future.:FutureWarning:dask\",\n ]\n+xfail_strict = true\ndiff --git a/python/pylibcudf/pyproject.toml b/python/pylibcudf/pyproject.toml\nindex a8224f54e1c..be65142850f 100644\n--- a/python/pylibcudf/pyproject.toml\n+++ b/python/pylibcudf/pyproject.toml\n@@ -95,6 +95,16 @@ skip = [\n     \"__init__.py\",\n ]\n \n+[tool.pytest.ini_options]\n+addopts = \"--tb=native --strict-config --strict-markers\"\n+empty_parameter_set_mark = \"fail_at_collect\"\n+filterwarnings = [\n+  \"error\",\n+  \"ignore:::.*xdist.*\",\n+  \"ignore:::.*pytest.*\"\n+]\n+xfail_strict = true\n+\n [tool.rapids-build-backend]\n build-backend = \"scikit_build_core.build\"\n dependencies-file = \"../../dependencies.yaml\"\n", "instance_id": "rapidsai__cudf-16977", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: pytest configurations in `pyproject.toml` are being ignored due to the presence of `pytest.ini` files in package test directories after a specific change (#16851). The goal is to consolidate configurations into a single file, with a preference for `pyproject.toml`. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define the expected outcome (e.g., should all `pytest.ini` files be removed, or should configurations be merged?). Additionally, there are no examples or references to specific packages or configurations that are most problematic, which could help in understanding the scope. Constraints or potential risks of moving configurations are also not mentioned. Despite these minor gaps, the problem is valid and the intent is understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes involve modifying `pyproject.toml` files across multiple Python packages (e.g., cudf, cudf_kafka, custreamz, etc.) to include pytest configurations under the `[tool.pytest.ini_options]` section. The changes are repetitive and localized to configuration files, not requiring modifications to functional code or system architecture. The amount of code change is moderate but straightforward, as it involves adding or updating configuration blocks in six files.\n\n2. **Number of Technical Concepts:** The problem requires basic familiarity with Python project configuration, specifically with `pyproject.toml` and pytest customization. Understanding pytest's behavior regarding configuration file precedence (as per the provided documentation link) is necessary but not complex. No advanced algorithms, design patterns, or domain-specific knowledge are needed. The concepts involved are relatively simple and well-documented.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not involve functional logic where error handling would be a concern. However, there is a minor implicit risk of misconfiguration or conflicts if existing `pytest.ini` files are not handled properly (though this is not addressed in the changes shown). The complexity of edge cases is low, as the task is primarily about configuration consistency.\n\n4. **Overall Assessment:** This task requires understanding a specific issue with pytest configuration precedence and applying a consistent fix across multiple files. It does not involve deep codebase knowledge, complex logic, or significant architectural impact. The repetitive nature of the changes and the simplicity of the required knowledge make this an Easy problem, though slightly above the Very Easy range due to the need to handle multiple files and understand configuration precedence rules. Hence, a score of 0.25 is appropriate.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}