{"problem_statement": "[feature request] add a subcommand for activation of previously deployed Stylus smart contract\nToday, if i have deployed a Stylus smart contract to an Arbitrum chain using cargo-stylus, it gets activated as part of the deployment step. This is awesome but in either scenario where (1) a year has elapsed or (2) a Stylus upgrade has taken place on the chain, I will need to re-activate my Stylus contract that has already been deployed.\r\n\r\nTo do this, I have to manually call `activateProgram` on the `ArbWasm` precompile. \r\n\r\nI would like the \"activation\" part of my Stylus dev workflow to be a separate subcommand within cargo-stylus. The UX should be:\r\n\r\n```\r\ncargo-stylus activate \\\r\n      --private-key-path=$PRIVATE_KEY\r\n      --address=0x123\r\n```\n", "patch": "diff --git a/check/src/activate.rs b/check/src/activate.rs\nnew file mode 100644\nindex 0000000..8a80481\n--- /dev/null\n+++ b/check/src/activate.rs\n@@ -0,0 +1,87 @@\n+// Copyright 2023-2024, Offchain Labs, Inc.\n+// For licensing, see https://github.com/OffchainLabs/cargo-stylus/blob/stylus/licenses/COPYRIGHT.md\n+\n+use alloy_primitives::Address;\n+use alloy_sol_macro::sol;\n+use alloy_sol_types::SolCall;\n+use cargo_stylus_util::color::{Color, DebugColor};\n+use cargo_stylus_util::sys;\n+use ethers::middleware::{Middleware, SignerMiddleware};\n+use ethers::signers::Signer;\n+use ethers::types::transaction::eip2718::TypedTransaction;\n+use ethers::types::{Eip1559TransactionRequest, U256};\n+use ethers::utils::format_units;\n+use eyre::{bail, Context, Result};\n+\n+use crate::check::check_activate;\n+use crate::constants::ARB_WASM_H160;\n+use crate::macros::greyln;\n+\n+use crate::ActivateConfig;\n+\n+sol! {\n+    interface ArbWasm {\n+        function activateProgram(address program)\n+            external\n+            payable\n+            returns (uint16 version, uint256 dataFee);\n+    }\n+}\n+\n+/// Activates an already deployed Stylus program by address.\n+pub async fn activate_program(cfg: &ActivateConfig) -> Result<()> {\n+    let provider = sys::new_provider(&cfg.common_cfg.endpoint)?;\n+    let chain_id = provider\n+        .get_chainid()\n+        .await\n+        .wrap_err(\"failed to get chain id\")?;\n+\n+    let wallet = cfg.auth.wallet().wrap_err(\"failed to load wallet\")?;\n+    let wallet = wallet.with_chain_id(chain_id.as_u64());\n+    let client = SignerMiddleware::new(provider.clone(), wallet);\n+\n+    let code = client.get_code(cfg.address, None).await?;\n+    let data_fee = check_activate(code, cfg.address, &provider).await?;\n+    let mut data_fee = alloy_ethers_typecast::alloy_u256_to_ethers(data_fee);\n+\n+    greyln!(\n+        \"obtained estimated activation data fee {}\",\n+        format_units(data_fee, \"ether\")?.debug_lavender()\n+    );\n+    greyln!(\n+        \"bumping estimated activation data fee by {}%\",\n+        cfg.data_fee_bump_percent.debug_lavender()\n+    );\n+    data_fee = bump_data_fee(data_fee, cfg.data_fee_bump_percent);\n+\n+    let program: Address = cfg.address.to_fixed_bytes().into();\n+    let data = ArbWasm::activateProgramCall { program }.abi_encode();\n+    let tx = Eip1559TransactionRequest::new()\n+        .from(client.address())\n+        .to(*ARB_WASM_H160)\n+        .value(data_fee)\n+        .data(data);\n+    let tx = TypedTransaction::Eip1559(tx);\n+    let tx = client.send_transaction(tx, None).await?;\n+    match tx.await? {\n+        Some(receipt) => {\n+            greyln!(\n+                \"successfully activated program 0x{} with tx {}\",\n+                hex::encode(cfg.address),\n+                hex::encode(receipt.transaction_hash).debug_lavender()\n+            );\n+        }\n+        None => {\n+            bail!(\n+                \"failed to fetch receipt for program activation {}\",\n+                cfg.address\n+            );\n+        }\n+    }\n+    Ok(())\n+}\n+\n+fn bump_data_fee(fee: U256, pct: u64) -> U256 {\n+    let num = 100 + pct;\n+    fee * U256::from(num) / U256::from(100)\n+}\ndiff --git a/check/src/cache.rs b/check/src/cache.rs\nindex b270587..607db6c 100644\n--- a/check/src/cache.rs\n+++ b/check/src/cache.rs\n@@ -56,7 +56,7 @@ pub async fn cache_program(cfg: &CacheConfig) -> Result<()> {\n     if cache_manager_addrs.is_empty() {\n         bail!(\"no cache managers found in ArbWasmCache, perhaps the Stylus cache is not yet enabled on this chain\");\n     }\n-    let cache_manager = cache_manager_addrs.last().unwrap().clone();\n+    let cache_manager = *cache_manager_addrs.last().unwrap();\n     let cache_manager = H160::from_slice(cache_manager.as_slice());\n \n     let program: Address = cfg.address.to_fixed_bytes().into();\ndiff --git a/check/src/check.rs b/check/src/check.rs\nindex ebe2626..137a14d 100644\n--- a/check/src/check.rs\n+++ b/check/src/check.rs\n@@ -241,7 +241,7 @@ async fn program_exists(codehash: B256, provider: &Provider<Http>) -> Result<boo\n }\n \n /// Checks program activation, returning the data fee.\n-async fn check_activate(code: Bytes, address: H160, provider: &Provider<Http>) -> Result<U256> {\n+pub async fn check_activate(code: Bytes, address: H160, provider: &Provider<Http>) -> Result<U256> {\n     let program = Address::from(address.to_fixed_bytes());\n     let data = ArbWasm::activateProgramCall { program }.abi_encode();\n     let tx = Eip1559TransactionRequest::new()\ndiff --git a/check/src/docker.rs b/check/src/docker.rs\nindex 0c327d6..89a433d 100644\n--- a/check/src/docker.rs\n+++ b/check/src/docker.rs\n@@ -41,7 +41,7 @@ a reproducible deployment, or opt out by using the --no-verify flag for local bu\n }\n \n fn create_image(version: &str) -> Result<()> {\n-    let name = version_to_image_name(&version);\n+    let name = version_to_image_name(version);\n     if image_exists(&name)? {\n         return Ok(());\n     }\ndiff --git a/check/src/main.rs b/check/src/main.rs\nindex c7f60bf..a57b1c3 100644\n--- a/check/src/main.rs\n+++ b/check/src/main.rs\n@@ -8,6 +8,7 @@ use std::fmt;\n use std::path::PathBuf;\n use tokio::runtime::Builder;\n \n+mod activate;\n mod cache;\n mod check;\n mod constants;\n@@ -50,6 +51,9 @@ enum Apis {\n         #[arg(long)]\n         json: bool,\n     },\n+    /// Activate an already deployed contract.\n+    #[command(alias = \"a\")]\n+    Activate(ActivateConfig),\n     /// Cache a contract using the Stylus CacheManager for Arbitrum chains.\n     Cache(CacheConfig),\n     /// Check a contract.\n@@ -98,6 +102,21 @@ pub struct CacheConfig {\n     bid: Option<u64>,\n }\n \n+#[derive(Args, Clone, Debug)]\n+pub struct ActivateConfig {\n+    #[command(flatten)]\n+    common_cfg: CommonConfig,\n+    /// Wallet source to use.\n+    #[command(flatten)]\n+    auth: AuthOpts,\n+    /// Deployed Stylus program address to activate.\n+    #[arg(long)]\n+    address: H160,\n+    /// Percent to bump the estimated activation data fee by. Default of 20%\n+    #[arg(long, default_value = \"20\")]\n+    data_fee_bump_percent: u64,\n+}\n+\n #[derive(Args, Clone, Debug)]\n pub struct CheckConfig {\n     #[command(flatten)]\n@@ -188,9 +207,9 @@ impl fmt::Display for CheckConfig {\n         write!(\n             f,\n             \"{} {} {} {}\",\n-            self.common_cfg.to_string(),\n+            self.common_cfg,\n             match &self.wasm_file {\n-                Some(path) => format!(\"--wasm-file={}\", path.display().to_string()),\n+                Some(path) => format!(\"--wasm-file={}\", path.display()),\n                 None => \"\".to_string(),\n             },\n             match &self.program_address {\n@@ -210,8 +229,8 @@ impl fmt::Display for DeployConfig {\n         write!(\n             f,\n             \"{} {} {}\",\n-            self.check_config.to_string(),\n-            self.auth.to_string(),\n+            self.check_config,\n+            self.auth,\n             match self.estimate_gas {\n                 true => \"--estimate-gas\".to_string(),\n                 false => \"\".to_string(),\n@@ -226,7 +245,7 @@ impl fmt::Display for AuthOpts {\n             f,\n             \"{} {} {} {}\",\n             match &self.private_key_path {\n-                Some(path) => format!(\"--private-key-path={}\", path.display().to_string()),\n+                Some(path) => format!(\"--private-key-path={}\", path.display()),\n                 None => \"\".to_string(),\n             },\n             match &self.private_key {\n@@ -238,7 +257,7 @@ impl fmt::Display for AuthOpts {\n                 None => \"\".to_string(),\n             },\n             match &self.keystore_password_path {\n-                Some(path) => format!(\"--keystore-password-path={}\", path.display().to_string()),\n+                Some(path) => format!(\"--keystore-password-path={}\", path.display()),\n                 None => \"\".to_string(),\n             }\n         )\n@@ -250,7 +269,7 @@ impl fmt::Display for VerifyConfig {\n         write!(\n             f,\n             \"{} --deployment-tx={} {}\",\n-            self.common_cfg.to_string(),\n+            self.common_cfg,\n             self.deployment_tx,\n             match self.no_verify {\n                 true => \"--no-verify\".to_string(),\n@@ -280,6 +299,12 @@ async fn main_impl(args: Opts) -> Result<()> {\n         Apis::ExportAbi { json, output } => {\n             run!(export_abi::export_abi(output, json), \"failed to export abi\");\n         }\n+        Apis::Activate(config) => {\n+            run!(\n+                activate::activate_program(&config).await,\n+                \"stylus activate failed\"\n+            );\n+        }\n         Apis::Cache(config) => {\n             run!(cache::cache_program(&config).await, \"stylus cache failed\");\n         }\ndiff --git a/check/src/verify.rs b/check/src/verify.rs\nindex 668deee..8eee240 100644\n--- a/check/src/verify.rs\n+++ b/check/src/verify.rs\n@@ -72,7 +72,7 @@ pub async fn verify(cfg: VerifyConfig) -> eyre::Result<()> {\n     if deployment_data == *result.input {\n         println!(\"Verified - program matches local project's file hashes\");\n     } else {\n-        let tx_prelude = extract_program_evm_deployment_prelude(&*result.input);\n+        let tx_prelude = extract_program_evm_deployment_prelude(&result.input);\n         let reconstructed_prelude = extract_program_evm_deployment_prelude(&deployment_data);\n         println!(\n             \"{} - program deployment did not verify against local project's file hashes\",\n@@ -94,7 +94,7 @@ pub async fn verify(cfg: VerifyConfig) -> eyre::Result<()> {\n         );\n         println!(\n             \"Compressed code length of deployment tx {}\",\n-            extract_compressed_wasm(&*result.input).len()\n+            extract_compressed_wasm(&result.input).len()\n         );\n     }\n     Ok(())\ndiff --git a/main/src/main.rs b/main/src/main.rs\nindex 175ce04..a6ee7d6 100644\n--- a/main/src/main.rs\n+++ b/main/src/main.rs\n@@ -38,6 +38,9 @@ enum Subcommands {\n     /// Check a contract.\n     #[command(alias = \"c\")]\n     Check,\n+    /// Activate an already deployed program\n+    #[command(alias = \"a\")]\n+    Activate,\n     /// Deploy a contract.\n     #[command(alias = \"d\")]\n     Deploy,\n@@ -71,6 +74,7 @@ const COMMANDS: &[Binary] = &[\n             \"check\",\n             \"deploy\",\n             \"verify\",\n+            \"a\",\n             \"n\",\n             \"x\",\n             \"c\",\n", "instance_id": "OffchainLabs__cargo-stylus-60", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a subcommand for activating a previously deployed Stylus smart contract. It specifies the desired user experience with a command-line example and explains the context (e.g., reactivation after a year or after a Stylus upgrade). However, there are minor ambiguities and missing details. For instance, it does not explicitly mention the expected behavior if the activation fails (e.g., error messages or retry mechanisms), nor does it specify constraints like supported chain types or potential limitations of the activation process. Additionally, edge cases such as invalid addresses or insufficient funds for activation fees are not addressed. Despite these minor gaps, the overall intent and requirements are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files (e.g., `activate.rs` as a new file, and modifications to `main.rs`, `check.rs`, etc.), requiring integration into an existing CLI tool (`cargo-stylus`). While the changes are not architecturally significant, they do span multiple modules and necessitate understanding the interaction between command-line argument parsing, Ethereum client interactions, and smart contract activation logic. Second, the number of technical concepts involved is moderate, including familiarity with Rust, Ethereum smart contract interactions (using `ethers` and `alloy` libraries), EIP-1559 transaction handling, and ABI encoding/decoding. Third, the code changes include some error handling (e.g., checking transaction receipts and provider errors), but edge cases like invalid addresses, network failures, or gas estimation issues are not fully addressed in the problem statement or code, which adds a layer of complexity for a complete solution. Finally, the amount of new code (e.g., 87 lines in `activate.rs`) and modifications to existing logic indicate a moderate workload. Overall, this task requires a solid understanding of Rust and blockchain interactions but does not involve deep architectural changes or highly complex algorithms, justifying a difficulty score of 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add CairoRunner:: initialize_program_builtins method\nAdd a `initialize_program_builtins` method to the CairoRunner struct.\r\nThe goal of this method, is to initialize just the `&self.program.builtins`\r\nThis function should be the same as the `initialize_all_builtins` but with some modifications:\r\n\r\nRemove these chuncks of code:\r\n\r\n```\r\n        let starknet_preset_builtins = vec![\r\n            BuiltinName::pedersen,\r\n            BuiltinName::range_check,\r\n            BuiltinName::output,\r\n            BuiltinName::ecdsa,\r\n            BuiltinName::bitwise,\r\n            BuiltinName::ec_op,\r\n            BuiltinName::keccak,\r\n            BuiltinName::poseidon,\r\n        ];\r\n```\r\n\r\n```\r\n        for builtin_name in starknet_preset_builtins {\r\n            if !self.program.builtins.contains(&builtin_name) {\r\n                initialize_builtin(builtin_name, &mut self.vm, add_segment_arena_builtin)\r\n            }\r\n        }\r\n```\r\n\r\nnote: We can avoid the segment arena flag, because it is present in `self.program.builtins`\r\n\r\n* Remove the old `initialize_all_builtins` method, is not necessary\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 97f230196d..5470fe2254 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -2,6 +2,8 @@\n \n #### Upcoming Changes\n \n+* fix(BREAKING): Replace `CairoRunner` method `initialize_all_builtins` with `initialize_program_builtins`. Now it only initializes program builtins instead of all of them\n+\n #### [1.0.0] - 2024-08-01\n \n * chore: bump `cairo-lang-` dependencies to 2.7.0 [#1813](https://github.com/lambdaclass/cairo-vm/pull/1813)\ndiff --git a/vm/src/vm/runners/cairo_runner.rs b/vm/src/vm/runners/cairo_runner.rs\nindex 5bb9305939..518d8299f7 100644\n--- a/vm/src/vm/runners/cairo_runner.rs\n+++ b/vm/src/vm/runners/cairo_runner.rs\n@@ -377,28 +377,10 @@ impl CairoRunner {\n             || self.runner_mode == RunnerMode::ProofModeCairo1\n     }\n \n-    // Initialize all the builtins. Values used are the original one from the CairoFunctionRunner\n+    // Initialize all program builtins. Values used are the original one from the CairoFunctionRunner\n     // Values extracted from here: https://github.com/starkware-libs/cairo-lang/blob/4fb83010ab77aa7ead0c9df4b0c05e030bc70b87/src/starkware/cairo/common/cairo_function_runner.py#L28\n-    fn initialize_all_builtins(\n-        &mut self,\n-        add_segment_arena_builtin: bool,\n-    ) -> Result<(), RunnerError> {\n-        let starknet_preset_builtins = vec![\n-            BuiltinName::pedersen,\n-            BuiltinName::range_check,\n-            BuiltinName::output,\n-            BuiltinName::ecdsa,\n-            BuiltinName::bitwise,\n-            BuiltinName::ec_op,\n-            BuiltinName::keccak,\n-            BuiltinName::poseidon,\n-        ];\n-\n-        fn initialize_builtin(\n-            name: BuiltinName,\n-            vm: &mut VirtualMachine,\n-            add_segment_arena_builtin: bool,\n-        ) {\n+    pub fn initialize_program_builtins(&mut self) -> Result<(), RunnerError> {\n+        fn initialize_builtin(name: BuiltinName, vm: &mut VirtualMachine) {\n             match name {\n                 BuiltinName::pedersen => vm\n                     .builtin_runners\n@@ -424,12 +406,9 @@ impl CairoRunner {\n                 BuiltinName::poseidon => vm\n                     .builtin_runners\n                     .push(PoseidonBuiltinRunner::new(Some(1), true).into()),\n-                BuiltinName::segment_arena => {\n-                    if add_segment_arena_builtin {\n-                        vm.builtin_runners\n-                            .push(SegmentArenaBuiltinRunner::new(true).into())\n-                    }\n-                }\n+                BuiltinName::segment_arena => vm\n+                    .builtin_runners\n+                    .push(SegmentArenaBuiltinRunner::new(true).into()),\n                 BuiltinName::range_check96 => vm\n                     .builtin_runners\n                     .push(RangeCheckBuiltinRunner::<RC_N_PARTS_96>::new(Some(1), true).into()),\n@@ -445,12 +424,7 @@ impl CairoRunner {\n         }\n \n         for builtin_name in &self.program.builtins {\n-            initialize_builtin(*builtin_name, &mut self.vm, add_segment_arena_builtin);\n-        }\n-        for builtin_name in starknet_preset_builtins {\n-            if !self.program.builtins.contains(&builtin_name) {\n-                initialize_builtin(builtin_name, &mut self.vm, add_segment_arena_builtin)\n-            }\n+            initialize_builtin(*builtin_name, &mut self.vm);\n         }\n         Ok(())\n     }\n@@ -1204,25 +1178,22 @@ impl CairoRunner {\n     }\n \n     /// Intitializes the runner in order to run cairo 1 contract entrypoints\n-    /// Initializes builtins & segments\n-    /// All builtins are initialized regardless of use\n     /// Swaps the program's builtins field with program_builtins\n-    /// Adds the segment_arena builtin if present in the program_builtins\n+    /// Initializes program builtins & segments\n     pub fn initialize_function_runner_cairo_1(\n         &mut self,\n         program_builtins: &[BuiltinName],\n     ) -> Result<(), RunnerError> {\n         self.program.builtins = program_builtins.to_vec();\n-        self.initialize_all_builtins(true)?;\n+        self.initialize_program_builtins()?;\n         self.initialize_segments(self.program_base);\n         Ok(())\n     }\n \n     /// Intitializes the runner in order to run cairo 0 contract entrypoints\n-    /// Initializes builtins & segments\n-    /// All builtins are initialized regardless of use\n+    /// Initializes program builtins & segments\n     pub fn initialize_function_runner(&mut self) -> Result<(), RunnerError> {\n-        self.initialize_all_builtins(false)?;\n+        self.initialize_program_builtins()?;\n         self.initialize_segments(self.program_base);\n         Ok(())\n     }\n@@ -4282,48 +4253,33 @@ mod tests {\n \n     #[test]\n     #[cfg_attr(target_arch = \"wasm32\", wasm_bindgen_test)]\n-    fn initialize_all_builtins() {\n-        let program = program!();\n-\n-        let mut cairo_runner = cairo_runner!(program);\n-\n-        cairo_runner\n-            .initialize_all_builtins(false)\n-            .expect(\"Builtin initialization failed.\");\n-\n-        let given_output = cairo_runner.vm.get_builtin_runners();\n-\n-        assert_eq!(given_output[0].name(), BuiltinName::pedersen);\n-        assert_eq!(given_output[1].name(), BuiltinName::range_check);\n-        assert_eq!(given_output[2].name(), BuiltinName::output);\n-        assert_eq!(given_output[3].name(), BuiltinName::ecdsa);\n-        assert_eq!(given_output[4].name(), BuiltinName::bitwise);\n-        assert_eq!(given_output[5].name(), BuiltinName::ec_op);\n-        assert_eq!(given_output[6].name(), BuiltinName::keccak);\n-        assert_eq!(given_output[7].name(), BuiltinName::poseidon);\n-    }\n+    fn initialize_all_program_builtins() {\n+        let mut program = program!();\n \n-    #[test]\n-    #[cfg_attr(target_arch = \"wasm32\", wasm_bindgen_test)]\n-    fn initialize_all_builtins_maintain_program_order() {\n-        let program = program![\n+        // we manually add the builtins and check that they exist later\n+        program.builtins = vec![\n             BuiltinName::pedersen,\n             BuiltinName::range_check,\n-            BuiltinName::ecdsa\n+            BuiltinName::output,\n+            BuiltinName::ecdsa,\n+            BuiltinName::bitwise,\n+            BuiltinName::ec_op,\n+            BuiltinName::keccak,\n+            BuiltinName::poseidon,\n         ];\n \n         let mut cairo_runner = cairo_runner!(program);\n \n         cairo_runner\n-            .initialize_all_builtins(false)\n+            .initialize_program_builtins()\n             .expect(\"Builtin initialization failed.\");\n \n         let given_output = cairo_runner.vm.get_builtin_runners();\n \n         assert_eq!(given_output[0].name(), BuiltinName::pedersen);\n         assert_eq!(given_output[1].name(), BuiltinName::range_check);\n-        assert_eq!(given_output[2].name(), BuiltinName::ecdsa);\n-        assert_eq!(given_output[3].name(), BuiltinName::output);\n+        assert_eq!(given_output[2].name(), BuiltinName::output);\n+        assert_eq!(given_output[3].name(), BuiltinName::ecdsa);\n         assert_eq!(given_output[4].name(), BuiltinName::bitwise);\n         assert_eq!(given_output[5].name(), BuiltinName::ec_op);\n         assert_eq!(given_output[6].name(), BuiltinName::keccak);\n@@ -4332,35 +4288,7 @@ mod tests {\n \n     #[test]\n     #[cfg_attr(target_arch = \"wasm32\", wasm_bindgen_test)]\n-    fn initialize_all_builtins_maintain_program_order_add_segment_arena() {\n-        let program = program![\n-            BuiltinName::pedersen,\n-            BuiltinName::range_check,\n-            BuiltinName::ecdsa,\n-            BuiltinName::segment_arena\n-        ];\n-\n-        let mut cairo_runner = cairo_runner!(program);\n-\n-        cairo_runner\n-            .initialize_all_builtins(true)\n-            .expect(\"Builtin initialization failed.\");\n-\n-        let given_output = cairo_runner.vm.get_builtin_runners();\n-\n-        assert_eq!(given_output[0].name(), BuiltinName::pedersen);\n-        assert_eq!(given_output[1].name(), BuiltinName::range_check);\n-        assert_eq!(given_output[2].name(), BuiltinName::ecdsa);\n-        assert_eq!(given_output[3].name(), BuiltinName::segment_arena);\n-        assert_eq!(given_output[4].name(), BuiltinName::output);\n-        assert_eq!(given_output[5].name(), BuiltinName::bitwise);\n-        assert_eq!(given_output[6].name(), BuiltinName::ec_op);\n-        assert_eq!(given_output[7].name(), BuiltinName::keccak);\n-    }\n-\n-    #[test]\n-    #[cfg_attr(target_arch = \"wasm32\", wasm_bindgen_test)]\n-    fn initialize_function_runner() {\n+    fn initialize_function_runner_without_builtins() {\n         let program = program!();\n \n         let mut cairo_runner = cairo_runner!(program);\n@@ -4369,17 +4297,6 @@ mod tests {\n             .initialize_function_runner()\n             .expect(\"initialize_function_runner failed.\");\n \n-        let builtin_runners = cairo_runner.vm.get_builtin_runners();\n-\n-        assert_eq!(builtin_runners[0].name(), BuiltinName::pedersen);\n-        assert_eq!(builtin_runners[1].name(), BuiltinName::range_check);\n-        assert_eq!(builtin_runners[2].name(), BuiltinName::output);\n-        assert_eq!(builtin_runners[3].name(), BuiltinName::ecdsa);\n-        assert_eq!(builtin_runners[4].name(), BuiltinName::bitwise);\n-        assert_eq!(builtin_runners[5].name(), BuiltinName::ec_op);\n-        assert_eq!(builtin_runners[6].name(), BuiltinName::keccak);\n-        assert_eq!(builtin_runners[7].name(), BuiltinName::poseidon);\n-\n         assert_eq!(\n             cairo_runner.program_base,\n             Some(Relocatable {\n@@ -4394,12 +4311,12 @@ mod tests {\n                 offset: 0,\n             })\n         );\n-        assert_eq!(cairo_runner.vm.segments.num_segments(), 10);\n+        assert_eq!(cairo_runner.vm.segments.num_segments(), 2);\n     }\n \n     #[test]\n     #[cfg_attr(target_arch = \"wasm32\", wasm_bindgen_test)]\n-    fn initialize_function_runner_add_segment_arena_builtin() {\n+    fn initialize_function_runner_with_segment_arena_builtin() {\n         let program = program!();\n \n         let mut cairo_runner = cairo_runner!(program);\n@@ -4411,14 +4328,6 @@ mod tests {\n         let builtin_runners = cairo_runner.vm.get_builtin_runners();\n \n         assert_eq!(builtin_runners[0].name(), BuiltinName::segment_arena);\n-        assert_eq!(builtin_runners[1].name(), BuiltinName::pedersen);\n-        assert_eq!(builtin_runners[2].name(), BuiltinName::range_check);\n-        assert_eq!(builtin_runners[3].name(), BuiltinName::output);\n-        assert_eq!(builtin_runners[4].name(), BuiltinName::ecdsa);\n-        assert_eq!(builtin_runners[5].name(), BuiltinName::bitwise);\n-        assert_eq!(builtin_runners[6].name(), BuiltinName::ec_op);\n-        assert_eq!(builtin_runners[7].name(), BuiltinName::keccak);\n-        assert_eq!(builtin_runners[8].name(), BuiltinName::poseidon);\n \n         assert_eq!(\n             cairo_runner.program_base,\n@@ -4434,7 +4343,8 @@ mod tests {\n                 offset: 0,\n             })\n         );\n-        assert_eq!(cairo_runner.vm.segments.num_segments(), 12);\n+        // segment arena builtin adds 2 segments\n+        assert_eq!(cairo_runner.vm.segments.num_segments(), 4);\n     }\n \n     #[test]\n", "instance_id": "lambdaclass__cairo-vm-1815", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent to replace the `initialize_all_builtins` method with a new `initialize_program_builtins` method in the `CairoRunner` struct, focusing only on initializing program-specific builtins. It provides specific instructions on code chunks to remove and mentions the removal of a parameter related to the segment arena flag. However, there are minor ambiguities and missing details. For instance, it does not explicitly explain why the segment arena flag can be avoided or provide context about the broader impact of this change on the system (e.g., how it affects other parts of the codebase or runtime behavior). Additionally, edge cases or potential issues arising from this change are not mentioned. Despite these minor gaps, the goal and required modifications are understandable with the provided diff and context.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized primarily to a single file (`cairo_runner.rs`) and involve replacing one method with another, along with updating related calls and test cases. The diff shows a moderate amount of code modification, but it is mostly straightforward\u2014removing specific code chunks, renaming functions, and adjusting test assertions. The changes do not significantly impact the system's architecture, as they are confined to initialization logic without altering core functionality or introducing new dependencies.\n\n2. **Number of Technical Concepts**: The problem requires a basic understanding of Rust syntax, struct methods, and how builtins are managed in the context of the Cairo virtual machine. Familiarity with the codebase's `CairoRunner` and `VirtualMachine` components is necessary, but the concepts involved (e.g., iterating over a list of builtins and initializing them) are not particularly complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond the immediate context of the VM are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not mention specific edge cases or new error conditions to handle. The code changes preserve the existing error handling structure (returning `Result<(), RunnerError>`), and no additional complexity is introduced in this regard. However, a developer might need to consider whether initializing only program builtins (instead of all builtins) could lead to uninitialized states or runtime issues, though this is not explicitly addressed in the problem.\n\n4. **Overall Complexity**: The task involves understanding the intent behind the change (limiting initialization to program builtins) and applying it consistently across the method and related test cases. While this requires some code logic comprehension and attention to detail (e.g., ensuring test cases reflect the new behavior), it does not demand deep architectural knowledge or complex refactoring. The primary challenge lies in correctly interpreting the instructions and verifying that no unintended side effects are introduced, which is manageable for a developer with moderate experience in Rust and familiarity with the codebase.\n\nThus, a difficulty score of 0.30 reflects the straightforward nature of the changes, balanced by the need for some contextual understanding and careful implementation to avoid subtle bugs.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove byteorder dependency\nCurrent usage of `byteorder::WriteBytesExt` is not no_std compatible. Can be replaced with `to_be_bytes()`\n", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex 4731738d0..e416e7b4a 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -77,7 +77,6 @@ serde_with = { version = \"3.11.0\", default-features = false, features = [\n ] }\n rand = \"0.8.5\"\n bytes = { version = \"1.1\", default-features = false }\n-byteorder = \"1\"\n futures = \"0.3\"\n tokio = { version = \"1.15.0\", features = [\"full\"] }\n tokio-stream = { version = \"0.1.8\", features = [\"sync\", \"time\"] }\ndiff --git a/ergo-chain-generation/Cargo.toml b/ergo-chain-generation/Cargo.toml\nindex b3ef0d5cf..9191aeac8 100644\n--- a/ergo-chain-generation/Cargo.toml\n+++ b/ergo-chain-generation/Cargo.toml\n@@ -10,7 +10,6 @@ edition.workspace = true\n [dependencies]\n base16 = { workspace = true }\n blake2 = { workspace = true }\n-byteorder = { workspace = true }\n ergo-lib = { workspace = true }\n num-bigint = { workspace = true }\n rand = { workspace = true }\ndiff --git a/ergo-chain-generation/src/chain_generation.rs b/ergo-chain-generation/src/chain_generation.rs\nindex 9367cb013..20ded310b 100644\n--- a/ergo-chain-generation/src/chain_generation.rs\n+++ b/ergo-chain-generation/src/chain_generation.rs\n@@ -202,10 +202,7 @@ fn prove_block(\n     let x = DlogProverInput::random();\n     let x_bigint = BigInt::from_bytes_be(Sign::Plus, &x.to_bytes());\n \n-    use byteorder::{BigEndian, WriteBytesExt};\n-    let mut height_bytes = Vec::with_capacity(4);\n-    #[allow(clippy::unwrap_used)]\n-    height_bytes.write_u32::<BigEndian>(header.height).unwrap();\n+    let height_bytes = header.height.to_be_bytes();\n     let popow_algos = ergo_nipopow::NipopowAlgos::default();\n     let big_n = popow_algos.pow_scheme.calc_big_n(version, height);\n \n@@ -217,11 +214,7 @@ fn prove_block(\n     let p1 = sk.public_image_bytes().unwrap();\n     let p2 = x.public_image().h.sigma_serialize_bytes().unwrap();\n     for i in min_nonce..max_nonce {\n-        let nonce = {\n-            let mut bytes = vec![];\n-            bytes.write_i64::<BigEndian>(i).unwrap();\n-            bytes\n-        };\n+        let nonce = i.to_be_bytes();\n         let seed_hash = if version == 1 {\n             let mut seed = msg.clone();\n             seed.extend(&nonce);\n@@ -238,8 +231,7 @@ fn prove_block(\n             .gen_indexes(&seed_hash, big_n)\n             .into_iter()\n             .map(|ix| {\n-                let mut index_bytes = vec![];\n-                index_bytes.write_u32::<BigEndian>(ix).unwrap();\n+                let index_bytes = ix.to_be_bytes();\n                 generate_element(version, &msg, &p1, &p2, &index_bytes, &height_bytes)\n             })\n             .fold(BigInt::from(0_u8), |acc, e| acc + e);\n@@ -253,7 +245,7 @@ fn prove_block(\n             let autolykos_solution = AutolykosSolution {\n                 miner_pk: sk.public_key().unwrap().public_key.into(),\n                 pow_onetime_pk: Some(x.public_image().h),\n-                nonce,\n+                nonce: nonce.to_vec(),\n                 pow_distance: Some(d),\n             };\n             // Compute header ID\ndiff --git a/ergo-lib/Cargo.toml b/ergo-lib/Cargo.toml\nindex d21362e81..8a831408f 100644\n--- a/ergo-lib/Cargo.toml\n+++ b/ergo-lib/Cargo.toml\n@@ -76,7 +76,6 @@ ergo-merkle-tree = { workspace = true, features = [\"arbitrary\"] }\n sigma-test-util = { workspace = true }\n pretty_assertions = { workspace = true }\n bs58 = { workspace = true }\n-byteorder = { workspace = true }\n expect-test = { workspace = true }\n \n \ndiff --git a/ergo-nipopow/Cargo.toml b/ergo-nipopow/Cargo.toml\nindex b1cd55b92..8b5d584b1 100644\n--- a/ergo-nipopow/Cargo.toml\n+++ b/ergo-nipopow/Cargo.toml\n@@ -14,7 +14,6 @@ crate-type = [\"cdylib\", \"rlib\"]\n [dependencies]\n base16 = { workspace = true }\n bounded-integer = { workspace = true }\n-byteorder = { workspace = true }\n thiserror = { workspace = true }\n derive_more = { workspace = true }\n ergotree-ir = { workspace = true, features = [\"json\"] }\ndiff --git a/ergo-nipopow/src/autolykos_pow_scheme.rs b/ergo-nipopow/src/autolykos_pow_scheme.rs\nindex b596288f8..9ada5ca9f 100644\n--- a/ergo-nipopow/src/autolykos_pow_scheme.rs\n+++ b/ergo-nipopow/src/autolykos_pow_scheme.rs\n@@ -37,13 +37,10 @@ impl AutolykosPowScheme {\n                 .cloned()\n                 .ok_or(AutolykosPowSchemeError::MissingPowDistanceParameter)\n         } else {\n-            use byteorder::{BigEndian, WriteBytesExt};\n             // hit for version 2\n             let msg = blake2b256_hash(&header.serialize_without_pow()?).to_vec();\n             let nonce = header.autolykos_solution.nonce.clone();\n-            let mut height_bytes = Vec::with_capacity(4);\n-            #[allow(clippy::unwrap_used)]\n-            height_bytes.write_u32::<BigEndian>(header.height).unwrap();\n+            let height_bytes = header.height.to_be_bytes();\n \n             let mut concat = msg.clone();\n             concat.extend(&nonce);\n@@ -56,8 +53,7 @@ impl AutolykosPowScheme {\n             let f2 = indexes.into_iter().fold(BigInt::from(0u32), |acc, idx| {\n                 // This is specific to autolykos v2.\n                 let mut concat = vec![];\n-                #[allow(clippy::unwrap_used)]\n-                concat.write_u32::<BigEndian>(idx).unwrap();\n+                concat.extend_from_slice(&idx.to_be_bytes());\n                 concat.extend(&height_bytes);\n                 concat.extend(&self.calc_big_m());\n                 acc + BigInt::from_bytes_be(Sign::Plus, &blake2b256_hash(&concat)[1..])\n@@ -72,15 +68,7 @@ impl AutolykosPowScheme {\n \n     /// Constant data to be added to hash function to increase its calculation time\n     pub fn calc_big_m(&self) -> Vec<u8> {\n-        use byteorder::{BigEndian, WriteBytesExt};\n-        (0u64..1024)\n-            .flat_map(|x| {\n-                let mut bytes = Vec::with_capacity(8);\n-                #[allow(clippy::unwrap_used)]\n-                bytes.write_u64::<BigEndian>(x).unwrap();\n-                bytes\n-            })\n-            .collect()\n+        (0u64..1024).flat_map(|x| x.to_be_bytes()).collect()\n     }\n \n     /// Computes `J` (denoted by `seed` in Ergo implementation) line 4, algorithm 1 of Autolykos v2\n", "instance_id": "ergoplatform__sigma-rust-807", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: it aims to remove the dependency on the `byteorder` crate to achieve `no_std` compatibility by replacing its usage with the `to_be_bytes()` method. The goal is straightforward, and the suggested solution is explicitly mentioned. However, the statement lacks critical details such as the specific context of `no_std` compatibility (e.g., why it is needed or what environments are targeted), any potential side effects of this change, or constraints regarding backward compatibility. Additionally, there are no examples or edge cases mentioned that might arise from this replacement. While the intent is clear, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) for several reasons based on the evaluated factors:\n\n1. **Scope and Depth of Code Changes:** The changes span multiple files (Cargo.toml files and source code in different modules), but the modifications are relatively straightforward. They involve removing the `byteorder` dependency from several dependency lists and replacing specific calls to `byteorder::WriteBytesExt` with Rust's built-in `to_be_bytes()` method. The changes are localized to specific lines and do not impact the broader architecture of the system. The overall amount of code change is small and repetitive in nature.\n\n2. **Number of Technical Concepts:** The problem requires basic familiarity with Rust's standard library, specifically the `to_be_bytes()` method for integer types, and an understanding of dependency management in Cargo.toml. No advanced language features, complex algorithms, design patterns, or domain-specific knowledge are needed. The concept of `no_std` compatibility is mentioned but does not require deep understanding to implement the changes, as the solution is already provided.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention any specific edge cases or error conditions to consider. The code changes replace a method that previously used `unwrap()` (indicating potential error handling via panics) with a method that does not panic, but no additional error handling logic is introduced or required. The simplicity of the replacement suggests minimal risk of introducing new edge cases, though byte order consistency across platforms could be a minor concern not addressed in the statement.\n\n4. **Overall Complexity:** The task is a simple refactoring exercise that involves understanding a small part of the codebase and making mechanical replacements. It does not require deep knowledge of the surrounding logic (e.g., the purpose of the `AutolykosPowScheme` or the specifics of the proof-of-work algorithm) to complete the task correctly. The impact on the codebase is minimal, as it is primarily a dependency cleanup with direct code substitutions.\n\nGiven these factors, a difficulty score of 0.25 reflects the ease of the task, requiring only basic code modifications and minimal conceptual understanding beyond standard Rust practices. It is slightly above the \"Very Easy\" range due to the need to modify multiple files and ensure consistency in the replacement approach, but it remains a low-complexity problem suitable for a junior or intermediate developer with basic Rust experience.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Move the location for generated man pages inside target dir\nThe cargo release process expects that the `build.rs` file will not modify anything outside of the `target` directory. Currently, we are doing this for the man pages, which results in the following error when running `cargo publish`:\r\n\r\n```\r\n  Source directory was modified by build.rs during cargo publish. Build scripts should not modify anything outside of OUT_DIR.\r\n  Added: /home/soichiro/repos/oxipng/target/package/oxipng-9.1.0/generated\r\n        /home/soichiro/repos/oxipng/target/package/oxipng-9.1.0/generated/assets\r\n        /home/soichiro/repos/oxipng/target/package/oxipng-9.1.0/generated/assets/oxipng.1\r\n```\r\n\r\nThis change moves the `generated` directory to within `target` in order to resolve this.\r\n\r\nFixes #611 (will need to tag a 9.1.1 and `cargo publish` after)\n", "patch": "diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml\nindex c1dfd28d..e41443f0 100644\n--- a/.github/workflows/deploy.yml\n+++ b/.github/workflows/deploy.yml\n@@ -90,11 +90,12 @@ jobs:\n         run: |\n           mkdir -p \"target/${{ matrix.target }}/release\"\n           mv target/oxipng \"target/${{ matrix.target }}/release\"\n+          mv target/debug/assets \"target/${{ matrix.target }}/release\"\n           cargo install --locked cargo-deb\n           cargo deb --target \"${{ matrix.target }}\" --no-build --no-strip\n \n       - name: Create release notes\n-        run: tail -n +2 CHANGELOG.md | sed -e '/^$/,$d' > RELEASE_NOTES.txt\n+        run: tail -n +3 CHANGELOG.md | sed -e '/^$/,$d' > RELEASE_NOTES.txt\n \n       - name: Create release\n         uses: softprops/action-gh-release@v2\ndiff --git a/.gitignore b/.gitignore\nindex 868a47a4..c03f402b 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -4,4 +4,3 @@ target\n *.out.png\n /.idea\n /node_modules\n-/generated\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 7a218a72..d2a3cb09 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -104,7 +104,7 @@ panic = \"abort\"\n [package.metadata.deb]\n assets = [\n     [\"target/release/oxipng\", \"usr/bin/\", \"755\"],\n-    [\"generated/assets/oxipng.1\", \"usr/share/man/man1/\", \"644\"],\n+    [\"target/release/assets/oxipng.1\", \"usr/share/man/man1/\", \"644\"],\n     [\"README.md\", \"usr/share/doc/oxipng/\", \"644\"],\n     [\"CHANGELOG.md\", \"usr/share/doc/oxipng/\", \"644\"],\n ]\ndiff --git a/build.rs b/build.rs\nindex 91cd1b6d..12b545b1 100644\n--- a/build.rs\n+++ b/build.rs\n@@ -24,8 +24,14 @@ fn main() -> Result<(), Error> {\n     println!(\"cargo:rerun-if-changed=src/cli.rs\");\n     println!(\"cargo:rerun-if-changed=src/display_chunks.rs\");\n \n-    // Create `generated/assets/` folder.\n-    let path = env::current_dir()?.join(\"generated\").join(\"assets\");\n+    // Create `target/<debug|release>/assets/` folder.\n+    let outdir = match env::var_os(\"OUT_DIR\") {\n+        None => return Ok(()),\n+        Some(outdir) => outdir,\n+    };\n+    let out_path = PathBuf::from(outdir);\n+    let mut path = out_path.ancestors().nth(3).unwrap().to_owned();\n+    path.push(\"assets\");\n     std::fs::create_dir_all(&path).unwrap();\n \n     build_manpages(&path)?;\n", "instance_id": "shssoichiro__oxipng-613", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `build.rs` script is modifying files outside the `target` directory, which violates Cargo's expectations during `cargo publish`. The goal of moving the generated man pages into the `target` directory is explicitly stated, and the provided error message adds context to the problem. However, there are minor ambiguities, such as the lack of detail on how the directory structure within `target` should be determined or whether there are specific conventions to follow for the new location. Additionally, edge cases or potential side effects of this change (e.g., impact on other build processes or deployment scripts) are not mentioned. Overall, the statement is valid and clear but misses some minor details that could affect implementation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category (0.2-0.4) due to the following reasons:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting the `build.rs` script to modify the output path for generated man pages. Additional updates are needed in configuration files like `Cargo.toml`, `.gitignore`, and a GitHub workflow file (`.github/workflows/deploy.yml`). These changes are straightforward and do not impact the core architecture of the system. The overall amount of code change is small, involving only path adjustments and minor updates to related scripts.\n\n2. **Technical Concepts Involved**: The solution requires basic familiarity with Rust's build system (specifically `build.rs` and the `OUT_DIR` environment variable), as well as an understanding of file path manipulation using the standard library (`std::path::PathBuf`). No advanced algorithms, design patterns, or domain-specific knowledge are needed. The concepts involved are fundamental and accessible to developers with basic-to-intermediate Rust experience.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not introduce complex error handling. The primary concern is ensuring the new path is correctly constructed and directories are created as needed, which is handled in the provided diff. There might be implicit edge cases, such as ensuring compatibility with different build environments or handling permissions issues, but these are not addressed in the problem statement and do not significantly increase complexity.\n\n4. **Overall Complexity**: The task involves understanding a small part of the build process and making targeted modifications. It does not require deep knowledge of the broader codebase or intricate interactions between modules. The impact is limited to the build and deployment process, with no performance or runtime considerations.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic modifications and a moderate understanding of Rust's build system. It is slightly above the \"Very Easy\" range due to the need to coordinate changes across multiple files and understand the build environment, but it remains a straightforward fix for most developers familiar with Rust.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Bad short hex format handling in macro color!()\n### Is your issue REALLY a bug?\n\n- [X] My issue is indeed a bug!\n- [X] I am not crazy! I will not fill out this form just to ask a question or request a feature. Pinky promise.\n\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\nUsing the macro color!() with the value 0x0000ff produces the wrong color.\r\nHere is the debug print:\r\n`color!(0x0000ff) = Color {\r\n    r: 0.0,\r\n    g: 1.0,\r\n    b: 1.0,\r\n    a: 1.0,\r\n}`\n\n### What is the expected behavior?\n\nExpected behavior is:\r\n`color!(0x0000ff) = Color {\r\n    r: 0.0,\r\n    g: 0.0,\r\n    b: 1.0,\r\n    a: 1.0,\r\n}`\r\n\r\nI believe the problem is in this line.\r\nIts a pretty bad way to handle short form hex.\r\nhttps://github.com/iced-rs/iced/blob/b78b8bc8e3c8dcb778b4e76bb9ef0ab1b88f633d/core/src/color.rs#L269\n\n### Version\n\ncrates.io release\n\n### Operating System\n\nmacOS\n\n### Do you have any log output?\n\n_No response_\n", "patch": "diff --git a/core/src/color.rs b/core/src/color.rs\nindex 46fe9ecd97..827d028953 100644\n--- a/core/src/color.rs\n+++ b/core/src/color.rs\n@@ -229,13 +229,12 @@ impl From<[f32; 4]> for Color {\n /// assert_eq!(color!(0, 0, 0, 0.0), Color::TRANSPARENT);\n /// assert_eq!(color!(0xffffff), Color::from_rgb(1.0, 1.0, 1.0));\n /// assert_eq!(color!(0xffffff, 0.), Color::from_rgba(1.0, 1.0, 1.0, 0.0));\n-/// assert_eq!(color!(0x123), Color::from_rgba8(0x11, 0x22, 0x33, 1.0));\n-/// assert_eq!(color!(0x123), color!(0x112233));\n+/// assert_eq!(color!(0x0000ff), Color::from_rgba(0.0, 0.0, 1.0, 1.0));\n /// ```\n #[macro_export]\n macro_rules! color {\n     ($r:expr, $g:expr, $b:expr) => {\n-        color!($r, $g, $b, 1.0)\n+        $crate::color!($r, $g, $b, 1.0)\n     };\n     ($r:expr, $g:expr, $b:expr, $a:expr) => {{\n         let r = $r as f32 / 255.0;\n@@ -261,29 +260,18 @@ macro_rules! color {\n         $crate::Color { r, g, b, a: $a }\n     }};\n     ($hex:expr) => {{\n-        color!($hex, 1.0)\n+        $crate::color!($hex, 1.0)\n     }};\n     ($hex:expr, $a:expr) => {{\n         let hex = $hex as u32;\n \n-        if hex <= 0xfff {\n-            let r = (hex & 0xf00) >> 8;\n-            let g = (hex & 0x0f0) >> 4;\n-            let b = (hex & 0x00f);\n+        debug_assert!(hex <= 0xffffff, \"color! value must not exceed 0xffffff\");\n \n-            color!((r << 4 | r), (g << 4 | g), (b << 4 | b), $a)\n-        } else {\n-            debug_assert!(\n-                hex <= 0xffffff,\n-                \"color! value must not exceed 0xffffff\"\n-            );\n+        let r = (hex & 0xff0000) >> 16;\n+        let g = (hex & 0xff00) >> 8;\n+        let b = (hex & 0xff);\n \n-            let r = (hex & 0xff0000) >> 16;\n-            let g = (hex & 0xff00) >> 8;\n-            let b = (hex & 0xff);\n-\n-            color!(r, g, b, $a)\n-        }\n+        $crate::color!(r, g, b, $a)\n     }};\n }\n \n", "instance_id": "iced-rs__iced-2592", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `color!()` macro when handling short hex formats like `0x0000ff`. It provides a specific example of the incorrect output and contrasts it with the expected behavior, which helps in understanding the bug. Additionally, the reporter points to a specific line in the codebase as the potential source of the issue, which adds clarity. However, there are minor ambiguities: the problem statement does not explicitly discuss edge cases (e.g., other short hex formats or invalid inputs) or constraints on the hex values beyond the example. It also lacks a detailed explanation of why the current implementation is incorrect beyond labeling it as a \"pretty bad way to handle short form hex.\" Overall, while the goal and issue are clear, some minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to several factors. First, the scope of the code change is limited to a single macro definition within one file (`color.rs`), and the modification involves a straightforward fix to the logic for parsing hex color codes. The change removes the special handling for short hex formats (values <= 0xfff) and standardizes the extraction of RGB components for all hex values up to 0xffffff, which is a simple adjustment. Second, the technical concepts required are minimal\u2014basic bitwise operations and macro usage in Rust, which are not particularly complex for an experienced developer. Third, the problem does not appear to impact the broader architecture or require understanding intricate interactions across the codebase. Finally, while edge cases like invalid hex values are implicitly handled via a `debug_assert!`, no additional complex error handling or edge case logic is introduced in the provided diff. The primary challenge is understanding the intent of the original macro logic and ensuring the fix aligns with expected behavior, but this is relatively straightforward. Therefore, I assign a difficulty score of 0.25, indicating an easy problem that requires understanding some code logic and making a simple modification.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Clap-generated shell completion failing on macOS /bin/bash with: conditional binary operator expected: syntax error near `IFS'\n### Please complete the following tasks\r\n\r\n- [X] I have searched the [discussions](https://github.com/clap-rs/clap/discussions)\r\n- [X] I have searched the [open](https://github.com/clap-rs/clap/issues) and [rejected](https://github.com/clap-rs/clap/issues?q=is%3Aissue+label%3AS-wont-fix+is%3Aclosed) issues\r\n\r\n### Rust Version\r\n\r\n```console\r\n\u276f rustc -V\r\nrustc 1.77.0 (aedd173a2 2024-03-17)\r\n```\r\n\r\n### Clap Version\r\n\r\n4.5.3\r\n\r\n### Minimal reproducible code\r\n\r\n```toml\r\n# Cargo.toml\r\n[package]\r\nname = \"my-app\"\r\nversion = \"0.1.0\"\r\nedition = \"2021\"\r\n\r\n[dependencies]\r\nclap = { version = \"4.5.3\", features = [\r\n  \"debug\",\r\n  \"derive\",\r\n  \"env\",\r\n  \"string\",\r\n  \"wrap_help\",\r\n] }\r\nclap_complete = \"=4.5.1\"\r\n```\r\n\r\n````rust\r\n// src/main.rs\r\nuse clap::{CommandFactory, Parser, ValueHint};\r\nuse clap_complete::{generate, Shell};\r\nuse std::path::PathBuf;\r\n\r\n#[derive(Parser)]\r\nstruct Opt {\r\n    #[arg(short, long, value_hint = ValueHint::FilePath)]\r\n    file: Option<PathBuf>,\r\n}\r\n\r\nfn main() {\r\n    generate(\r\n        Shell::Bash,\r\n        &mut Opt::command(),\r\n        \"my-app\",\r\n        &mut std::io::stdout(),\r\n    );\r\n}\r\n````\r\n\r\n\r\n### Steps to reproduce the bug with the above code\r\n\r\n(on a Mac)\r\n\r\n```shell\r\ncargo run | /bin/bash\r\n```\r\n\r\n### Actual Behaviour\r\n\r\nOutput:\r\n\r\n```console\r\n\u276f cargo run | /bin/bash\r\n    Finished dev [unoptimized + debuginfo] target(s) in 0.01s\r\n     Running `target/debug/my-app`\r\n/bin/bash: line 30: conditional binary operator expected\r\n/bin/bash: line 30: syntax error near `IFS'\r\n/bin/bash: line 30: `                    if [[ -v IFS ]]; then'\r\n```\r\n\r\n### Expected Behaviour\r\n\r\nOutput:\r\n\r\n```console\r\n\u276f cargo run | /bin/bash\r\n    Finished dev [unoptimized + debuginfo] target(s) in 0.01s\r\n     Running `target/debug/my-app`\r\n```\r\n\r\n### Additional Context\r\n\r\nOn clap_complete 4.4.9 this works, on 4.4.10 this fails. I _think_ that this is a regression introduced by https://github.com/clap-rs/clap/pull/5336 . So the issue is probably the same as https://github.com/clap-rs/clap/issues/5190 , which was fixed by https://github.com/clap-rs/clap/pull/5278.\r\n\r\n### Debug Output\r\n\r\n```\r\n[clap_builder::builder::command]Command::_build: name=\"my-app\"\r\n[clap_builder::builder::command]Command::_propagate:my-app\r\n[clap_builder::builder::command]Command::_check_help_and_version:my-app expand_help_tree=true\r\n[clap_builder::builder::command]Command::long_help_exists\r\n[clap_builder::builder::command]Command::_check_help_and_version: Building default --help\r\n[clap_builder::builder::command]Command::_propagate_global_args:my-app\r\n[clap_builder::builder::debug_asserts]Command::_debug_asserts\r\n[clap_builder::builder::debug_asserts]Arg::_debug_asserts:file\r\n[clap_builder::builder::debug_asserts]Arg::_debug_asserts:help\r\n[clap_builder::builder::debug_asserts]Command::_verify_positionals\r\n[clap_builder::builder::command]Command::_build_bin_names\r\n[ clap_builder::output::usage]Usage::get_required_usage_from: incls=[], matcher=false, incl_last=true\r\n[ clap_builder::output::usage]Usage::get_required_usage_from: unrolled_reqs=[]\r\n[ clap_builder::output::usage]Usage::get_required_usage_from: ret_val=[]\r\n```\n", "patch": "diff --git a/clap_complete/src/shells/bash.rs b/clap_complete/src/shells/bash.rs\nindex 5fb39f4b965..bf361313667 100644\n--- a/clap_complete/src/shells/bash.rs\n+++ b/clap_complete/src/shells/bash.rs\n@@ -183,12 +183,12 @@ fn option_details_for_path(cmd: &Command, path: &str) -> String {\n                 if o.get_value_hint() == ValueHint::FilePath {\n                     v.extend([\n                         \"local oldifs\".to_string(),\n-                        \"if [[ -v IFS ]]; then\".to_string(),\n+                        r#\"if [ -n \"${IFS+x}\" ]; then\"#.to_string(),\n                         r#\"    oldifs=\"$IFS\"\"#.to_string(),\n                         \"fi\".to_string(),\n                         r#\"IFS=$'\\n'\"#.to_string(),\n                         format!(\"COMPREPLY=({})\", vals_for(o)),\n-                        \"if [[ -v oldifs ]]; then\".to_string(),\n+                        r#\"if [ -n \"${oldifs+x}\" ]; then\"#.to_string(),\n                         r#\"    IFS=\"$oldifs\"\"#.to_string(),\n                         \"fi\".to_string(),\n                     ]);\n@@ -216,12 +216,12 @@ fn option_details_for_path(cmd: &Command, path: &str) -> String {\n                 if o.get_value_hint() == ValueHint::FilePath {\n                     v.extend([\n                         \"local oldifs\".to_string(),\n-                        \"if [[ -v IFS ]]; then\".to_string(),\n+                        r#\"if [ -n \"${IFS+x}\" ]; then\"#.to_string(),\n                         r#\"    oldifs=\"$IFS\"\"#.to_string(),\n                         \"fi\".to_string(),\n                         r#\"IFS=$'\\n'\"#.to_string(),\n                         format!(\"COMPREPLY=({})\", vals_for(o)),\n-                        \"if [[ -v oldifs ]]; then\".to_string(),\n+                        r#\"if [ -n \"${oldifs+x}\" ]; then\"#.to_string(),\n                         r#\"    IFS=\"$oldifs\"\"#.to_string(),\n                         \"fi\".to_string(),\n                     ]);\n", "instance_id": "clap-rs__clap-5444", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a regression in the `clap_complete` library causing a syntax error in Bash shell completion scripts on macOS with `/bin/bash`. It provides a minimal reproducible code example, steps to reproduce, actual vs. expected behavior, and additional context pointing to a specific PR that likely introduced the regression. However, there are minor ambiguities, such as the lack of explicit mention of the root cause in the problem statement itself (though it is inferred via the linked PR and issue). Additionally, edge cases or specific constraints (e.g., whether this affects other shells or Bash versions) are not fully detailed. Overall, the statement is valid and clear but misses some minor details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue is a specific syntax error in Bash script generation caused by the use of `[[ -v IFS ]]` which is not supported in older Bash versions (like those on macOS). The code change provided replaces this with a more compatible check `[ -n \"${IFS+x}\" ]`, which is a straightforward fix. The scope of the change is limited to a single file (`bash.rs`) and involves modifying just a few lines of code in two places, with no impact on the broader architecture or other modules. The technical concepts required are basic: understanding Bash scripting, variable existence checks, and compatibility issues across Bash versions. No complex algorithms, design patterns, or domain-specific knowledge are needed. Edge cases and error handling are minimal, as the fix addresses a specific compatibility issue without introducing new logic or requiring extensive testing beyond verifying the script runs without syntax errors on affected systems. Overall, this is a simple bug fix requiring minimal effort and understanding of the codebase.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "build(deps): update opentelemetry-prometheus requirement from 0.16.0 to 0.17.0\nUpdates the requirements on [opentelemetry-prometheus](https://github.com/open-telemetry/opentelemetry-rust) to permit the latest version.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/open-telemetry/opentelemetry-rust/releases\">opentelemetry-prometheus's releases</a>.</em></p>\n<blockquote>\n<h2>v0.17.0</h2>\n<h3>Changed</h3>\n<ul>\n<li>Implement <code>Serialize</code> &amp; <code>Deserialize</code> for <code>Sampler</code>, <code>SpanLimits</code> <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/622\">#622</a>, <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/626\">#626</a></li>\n<li>Allow <code>&amp;'static str</code> and <code>string</code> in span methods <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/654\">#654</a></li>\n<li>Allow <code>String</code> data in instrumentation library. <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/670\">#670</a></li>\n<li>Remove <code>std::fmt::Debug</code> and <code>'static</code> requirements from <code>TracerProvider</code>,\n<code>Tracer</code>, and <code>Span</code> <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/664\">#664</a></li>\n<li>Remove unused <code>Tracer::invalid</code> method <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/683\">#683</a></li>\n<li>Split <code>TracerProvider::tracer</code> and <code>TracerProvider::versioned_tracer</code> methods <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/682\">#682</a></li>\n<li>Reduce dependency on <code>futures</code> crate <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/684\">#684</a></li>\n<li>Switch to parent context references <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/687\">#687</a></li>\n<li>Spec-compliant trace and span ids <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/689\">#689</a></li>\n<li>Optimize span creation internals <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/693\">#693</a></li>\n<li>Add instrumentation library to <code>ShouldSample</code> parameters <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/695\">#695</a></li>\n</ul>\n<h3>Fixed</h3>\n<ul>\n<li>Fix default resource detection for tracer provider <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/641\">#641</a></li>\n<li>Detect <code>service.name</code> from <code>OTEL_SERVICE_NAME</code> and <code>OTEL_RESOURCE_ATTRIBUTES</code> <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/662\">#662</a></li>\n<li>Fix <code>TraceState::valid_key</code> crashes <a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/665\">#665</a></li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/383c7d13b6fbda22dff993a39ce4f3aa1ff0da62\"><code>383c7d1</code></a> [opentelemetry-prometheus]: Prepare 0.17.0 release using opentelemetry 0.24 (...</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/2b026d9759f297de61b0d5ec5624ce6ccd773b5c\"><code>2b026d9</code></a> Fix PartialEq for Value in opentelemetry-stdout (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1955\">#1955</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/57bcd59989ecea1eca7b6b8c73dd87fddeb4e07c\"><code>57bcd59</code></a> fix ci (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1960\">#1960</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/406e31b76beed5787cb820d672fa6938b6d82980\"><code>406e31b</code></a> zstd compression for tonic exporter (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1947\">#1947</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/a1f02faf7c3a4f25f0ccebbc7e0cb8ccf1e80d82\"><code>a1f02fa</code></a> chore: update proto definition to v1.3.2 (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1945\">#1945</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/acd6cbe4f78d032b097b845548dc5b998575f89f\"><code>acd6cbe</code></a> Adding <code>PartialEq</code> for <code>LogRecord</code> and <code>SpanContext</code> to help unit testing (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1\">#1</a>...</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/e1a3494c34a9170f5ef21209c21e3e39b4186532\"><code>e1a3494</code></a> When deserializing JSON, accept both int and string in 'intValue' (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1906\">#1906</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/0e8f259cf33a8cc4bbb20c50b3ee9a902a24d159\"><code>0e8f259</code></a> sdk: expose hidden Tracer::should_sample() method (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1937\">#1937</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/a9b8621cd1004e7f083002071b9f9a57748c6b99\"><code>a9b8621</code></a> Flatten field + fix u64 to string serde (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1911\">#1911</a>)</li>\n<li><a href=\"https://github.com/open-telemetry/opentelemetry-rust/commit/47e52f1feb7b3605607a8e8d3a4fd4d2921a845f\"><code>47e52f1</code></a> Fix Stress Test (<a href=\"https://redirect.github.com/open-telemetry/opentelemetry-rust/issues/1935\">#1935</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/open-telemetry/opentelemetry-rust/compare/opentelemetry-prometheus-0.16.0...opentelemetry-prometheus-0.17.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\nYou can trigger a rebase of this PR by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>\n\n> **Note**\n> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.\n\n", "patch": "diff --git a/poem/Cargo.toml b/poem/Cargo.toml\nindex 6c88f62059..7f50d492b8 100644\n--- a/poem/Cargo.toml\n+++ b/poem/Cargo.toml\n@@ -131,7 +131,9 @@ libcookie = { package = \"cookie\", version = \"0.18\", features = [\n     \"secure\",\n ], optional = true }\n opentelemetry-http = { version = \"0.27.0\", optional = true }\n-opentelemetry-semantic-conventions = { version = \"0.16.0\", optional = true }\n+opentelemetry-semantic-conventions = { version = \"0.27.0\", optional = true, features = [\n+    \"semconv_experimental\"\n+] }\n opentelemetry-prometheus = { version = \"0.17.0\", optional = true }\n libprometheus = { package = \"prometheus\", version = \"0.13.0\", optional = true }\n libopentelemetry = { package = \"opentelemetry\", version = \"0.27.0\", features = [\n", "instance_id": "poem-web__poem-916", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and clear. It explicitly states the goal of updating the `opentelemetry-prometheus` dependency from version 0.16.0 to 0.17.0, and provides detailed release notes and commit history from the upstream repository. These notes outline the changes, improvements, and fixes in the new version, which helps in understanding the potential impact of the update. Additionally, the context of the update is supported by information about related dependencies (e.g., `opentelemetry-semantic-conventions` and `opentelemetry-http` also being updated to version 0.27.0). The problem statement includes instructions for triggering actions via Dependabot, which further clarifies the workflow for handling this update. There are no significant ambiguities or missing critical details regarding the intent or scope of the change.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The code change is minimal and straightforward, involving only a version update in a single file (`Cargo.toml`) for the `opentelemetry-prometheus` dependency, along with related updates to other OpenTelemetry dependencies. The scope of the change is limited to dependency management and does not require deep understanding of the codebase's logic, architecture, or interactions between modules. No complex technical concepts, algorithms, or domain-specific knowledge are needed beyond basic familiarity with Rust's dependency management using Cargo. The release notes indicate several changes in the updated version (e.g., API modifications, optimizations, and fixes), but the code change itself does not involve adapting to these updates directly\u2014any potential downstream impact would be handled in separate tasks if breaking changes are encountered. Edge cases and error handling are not a concern for this specific change, as it is purely a version bump. The primary effort lies in verifying compatibility and running tests to ensure the update does not introduce issues, which is a routine task for dependency updates. Therefore, I assign a difficulty score of 0.15 to reflect the simplicity of the task while acknowledging the minor due diligence required.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc1b emoji not supported in CSS parser\n### Environment information\r\n\r\n<details>\r\n\r\n<summary>biome rage</summary>\r\n\r\n```\r\nCLI:\r\n  Version:                      1.9.3\r\n  Color support:                true\r\n\r\nPlatform:\r\n  CPU Architecture:             aarch64\r\n  OS:                           macos\r\n\r\nEnvironment:\r\n  BIOME_LOG_DIR:                unset\r\n  NO_COLOR:                     unset\r\n  TERM:                         \"xterm-256color\"\r\n  JS_RUNTIME_VERSION:           \"v20.16.0\"\r\n  JS_RUNTIME_NAME:              \"node\"\r\n  NODE_PACKAGE_MANAGER:         \"pnpm/9.6.0\"\r\n\r\nBiome Configuration:\r\n  Status:                       unset\r\n```\r\n\r\n</details>\r\n\r\n\r\n### What happened?\r\n\r\nWhen a custom property name contains an emoji, Biome fails to parse it.\r\n\r\n```css\r\np {\r\n  --\u2728-color: red;\r\n  color: var(--\u2728-color);\r\n}\r\n```\r\n\r\nI get errors in my IDE from `biome(parse)` in the form of \"Expected a declaration, or an at rule but instead found '--\u2728'.\" and \"Expected a declaration item but instead found '--\u2728'.\"\r\n\r\n~~See [playground](https://biomejs.dev/playground/?files.main.css=cAAgAHsACgAgACAALQAtAD7YVN0tAGMAbwBsAG8AcgA6ACAAcgBlAGQAOwAKACAAIABjAG8AbABvAHIAOgAgAHYAYQByACgALQAtAD7YVN0tAGMAbwBsAG8AcgApADsACgB9AA%3D%3D).~~\r\n\r\nSee [updated playground](https://biomejs.dev/playground/?files.main.css=cAAgAHsACgAgACAALQAtAD7YVN0tAGMAbwBsAG8AcgAtADAAOgAgAHYAYQByACgALQAtAD3YG9wsACAAcgBlAGQAKQA7AAoAIAAgAAoAIAAgAC0ALQACJg%2F%2BLQBjAG8AbABvAHIALQAxADoAIAB2AGEAcgAoAC0ALQA%2B2FTdLQBjAG8AbABvAHIALQAwACkAOwAKACAAIAAKACAAIAAtAC0AKCctAGMAbwBsAG8AcgAtADIAOgAgAHYAYQByACgALQAtAAImD%2F5jAG8AbABvAHIALQAxACkAOwAKACAAIAAKACAAIABjAG8AbABvAHIAOgAgAHYAYQByACgALQAtACgnLQBjAG8AbABvAHIALQAyACkAOwAKAH0A) and [related comment](https://github.com/biomejs/biome/issues/3627#issuecomment-2392388022).\r\n\r\n### Expected result\r\n\r\nEmojis are allowed in custom property names, so Biome should be able to parse them.\r\n\r\n![](https://github.com/user-attachments/assets/68e3fbd1-1bd6-481f-8e60-39d85346c483)\r\n\r\n~~It's worth noting that since Biome doesn't support formatting/linting CSS, it should not even be parsing my CSS files.~~\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Biome's Code of Conduct\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a352995ed9cd..2e30e56364d6 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -29,7 +29,21 @@ our [guidelines for writing a good changelog entry](https://github.com/biomejs/b\n \n ### Linter\n \n-### Bug fixes\n+#### Bug Fixes\n+\n+- The CSS parser now accepts more emoji in identifiers ([#3627](https://github.com/biomejs/biome/issues/3627#issuecomment-2392388022)).\n+\n+  Browsers accept more emoji than the standard allows.\n+  Biome now accepts these additional emoji.\n+\n+  The following code is now correctly parsed:\n+\n+  ```css\n+  p {\n+    --\u2728-color: red;\n+    color: var(--\u2728-color);\n+  }\n+  ```\n \n - Biome no longer crashes when it encounters a string that contain a multibyte character ([#4181](https://github.com/biomejs/biome/issues/4181)).\n \ndiff --git a/crates/biome_unicode_table/src/lib.rs b/crates/biome_unicode_table/src/lib.rs\nindex 0077e68f8893..12de5f1df2c8 100644\n--- a/crates/biome_unicode_table/src/lib.rs\n+++ b/crates/biome_unicode_table/src/lib.rs\n@@ -13,7 +13,14 @@ pub fn is_html_id_start(c: char) -> bool {\n }\n \n /// Is `c` a CSS non-ascii character.\n-/// See https://drafts.csswg.org/css-syntax-3/#ident-token-diagram\n+/// See <https://drafts.csswg.org/css-syntax-3/#ident-token-diagram>\n+/// See <https://drafts.csswg.org/css-syntax-3/#non-ascii-ident-code-point>\n+///\n+/// In contrast to the standard we also accept all characters from:\n+/// - the Miscellaneous Symbols Unicode block\n+/// - the Dingbats Unicode block\n+///\n+/// We also accept some characters of the Miscellaneous Technical Unicode block.\n #[inline]\n pub fn is_css_non_ascii(c: char) -> bool {\n     matches!(\n@@ -28,6 +35,13 @@ pub fn is_css_non_ascii(c: char) -> bool {\n         | 0x203F\n         | 0x2040\n         | 0x2070..=0x218F\n+        // https://en.wikipedia.org/wiki/List_of_Unicode_characters#Miscellaneous_Technical\n+        | 0x2318 | 0x231A | 0x231B | 0x2328 | 0x2399\n+        | 0x23E9..=0x23F3\n+        | 0x23F9..=0x23FE\n+        // https://en.wikipedia.org/wiki/List_of_Unicode_characters#Miscellaneous_Symbols\n+        // https://en.wikipedia.org/wiki/Dingbats_(Unicode_block)\n+        | 0x2600..=0x27BF\n         | 0x2C00..=0x2FEF\n         | 0x3001..=0xD7FF\n         | 0xF900..=0xFDCF\n", "instance_id": "biomejs__biome-4175", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Biome's CSS parser fails to handle emojis in custom property names, which should be supported as per browser behavior. It provides a specific example of the failing CSS code and references the expected behavior with a screenshot. Additionally, it links to a playground and a related comment for further context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the full range of emojis or Unicode blocks that should be supported, leaving some room for interpretation. It also includes a struck-through comment about Biome not parsing CSS files, which introduces unnecessary confusion. While the goal (parsing emojis in CSS identifiers) and the issue are clear, the lack of exhaustive detail about the scope of supported characters or edge cases prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are localized to a single file (`crates/biome_unicode_table/src/lib.rs`) and involve modifying a specific function (`is_css_non_ascii`) to include additional Unicode ranges for emojis and other symbols. The changeset is small, with only a few lines added to extend the range of accepted characters. There is no impact on the broader system architecture or interactions between multiple modules, making the scope limited.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Unicode character sets and how they are handled in Rust, specifically matching character ranges. The concept of extending a character classification function is straightforward for someone familiar with Rust's pattern matching and character handling. No advanced algorithms, design patterns, or domain-specific knowledge beyond CSS identifier rules and Unicode blocks are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention specific edge cases beyond the provided example (an emoji in a custom property name). The code change itself does not introduce or modify error handling logic; it simply expands the set of accepted characters. While there might be implicit edge cases (e.g., performance implications of broader Unicode ranges or invalid emoji sequences), these are not addressed in the problem or changeset, keeping the complexity low.\n\n4. **Overall Complexity**: The task involves a simple bug fix by updating a character range check to align with browser behavior, which is a relatively routine modification in a parser. It requires minimal debugging or deep architectural understanding of the Biome codebase beyond the specific function being modified.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires understanding some code logic (Unicode handling in Rust) and making a straightforward modification. It does not involve complex interactions, significant edge case handling, or advanced technical concepts, keeping it on the lower end of the difficulty spectrum.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`transpose` silently discards list streams\n### Describe the bug\n\nWhen `transpose` is passed a non-table list stream, it returns an empty list.\n\n### How to reproduce\n\n```Nushell\r\n~> generate {|i=1| {out: $i, next: ($i * 2)} } | take 7 | transpose\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 empty list \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n```\n\n### Expected behavior\n\nThrow an error, like `transpose` does for regular lists.\n\n### Configuration\n\n| key                | value                                                        |\r\n| ------------------ | ------------------------------------------------------------ |\r\n| version            | 0.97.1                                                       |\r\n| major              | 0                                                            |\r\n| minor              | 97                                                           |\r\n| patch              | 1                                                            |\r\n| branch             |                                                              |\r\n| commit_hash        |                                                              |\r\n| build_os           | linux-x86_64                                                 |\r\n| build_target       | x86_64-alpine-linux-musl                                     |\r\n| rust_version       | rustc 1.80.1 (3f5fd8dd4 2024-08-06) (Alpine Linux 1.80.1-r0) |\r\n| cargo_version      | cargo 1.80.1 (376290515 2024-07-16)                          |\r\n| build_time         | 2024-08-29 21:40:07 +00:00                                   |\r\n| build_rust_channel | release                                                      |\r\n| allocator          | mimalloc                                                     |\r\n| features           | default, sqlite, system-clipboard, trash                     |\r\n| installed_plugins  | endecode 0.2.0, template 0.1.0                               |\n", "patch": "diff --git a/crates/nu-command/src/filters/transpose.rs b/crates/nu-command/src/filters/transpose.rs\nindex 873a21c90db87..a85e95863edf3 100644\n--- a/crates/nu-command/src/filters/transpose.rs\n+++ b/crates/nu-command/src/filters/transpose.rs\n@@ -175,10 +175,22 @@ pub fn transpose(\n \n     let metadata = input.metadata();\n     let input: Vec<_> = input.into_iter().collect();\n-    // Ensure error values are propagated\n-    for i in input.iter() {\n-        if let Value::Error { .. } = i {\n-            return Ok(i.clone().into_pipeline_data_with_metadata(metadata));\n+\n+    // Ensure error values are propagated and non-record values are rejected\n+    for value in input.iter() {\n+        match value {\n+            Value::Error { .. } => {\n+                return Ok(value.clone().into_pipeline_data_with_metadata(metadata))\n+            }\n+            Value::Record { .. } => {} // go on, this is what we're looking for\n+            _ => {\n+                return Err(ShellError::OnlySupportsThisInputType {\n+                    exp_input_type: \"table or record\".into(),\n+                    wrong_type: \"list<any>\".into(),\n+                    dst_span: call.head,\n+                    src_span: value.span(),\n+                })\n+            }\n         }\n     }\n \n", "instance_id": "nushell__nushell-15151", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the bug: the `transpose` command in Nushell silently discards non-table list streams and returns an empty list, which is inconsistent with its behavior for regular lists where it throws an error. The goal (to throw an error for non-table list streams) is implied through the expected behavior section. A reproduction example is provided, which helps in understanding the issue. However, there are minor ambiguities: the problem statement does not explicitly define what constitutes a \"non-table list stream\" or provide detailed context about the expected error message format. Additionally, edge cases or specific constraints (e.g., behavior with mixed input types) are not mentioned. Overall, while the intent is clear, some minor details are missing that could affect implementation or testing.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows that the change is localized to a single file (`transpose.rs`) and involves a small modification to an existing loop. The change adds a type check to reject non-record values with an appropriate error message. It does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is minimal, around 10-15 lines.\n\n2. **Number of Technical Concepts**: The solution requires basic familiarity with Rust, specifically pattern matching with `match` expressions and error handling using the `ShellError` type. It also requires understanding the `Value` enum and its variants (`Record`, `Error`, etc.) within the Nushell codebase. These concepts are straightforward for someone with moderate Rust experience and do not involve complex algorithms, design patterns, or domain-specific knowledge beyond the immediate context of the codebase.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change introduces error handling for non-record values. The complexity of edge cases appears minimal, as the change simply rejects unsupported input types with a clear error message. There are no indications of performance considerations or intricate error conditions to handle.\n\n4. **Overall Assessment**: This task involves a simple bug fix that requires understanding a small part of the codebase and making a targeted modification. It does not demand deep architectural knowledge or complex logic. The difficulty is rated at 0.25, reflecting an easy problem that a developer with basic to intermediate Rust skills and minimal familiarity with the codebase could resolve quickly.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`explore` Add `expand` shortcut in selection mode\n### Related problem\n\nRight now expanding a selected cell only works by typing the `:expand` command. Expanding with `e` already works in non-selection mode, so I expect it to work in selection mode as well\n\n### Describe the solution you'd like\n\nPressing `e` already expands the current view when not in selection mode and typing `:expand` already works on selected cells. It would make sense to have a shortcut in selection mode as well.\n\nI would gladly put money where my mouth is and do this task myself\n\n### Describe alternatives you've considered\n\n_No response_\n\n### Additional context and details\n\n_No response_\n", "patch": "diff --git a/crates/nu-explore/src/views/record/mod.rs b/crates/nu-explore/src/views/record/mod.rs\nindex e00334825873d..3e7d00615d9f3 100644\n--- a/crates/nu-explore/src/views/record/mod.rs\n+++ b/crates/nu-explore/src/views/record/mod.rs\n@@ -414,10 +414,7 @@ impl CursorMoveHandler for RecordView {\n         Transition::Ok\n     }\n     fn handle_expand(&mut self) -> Transition {\n-        match self.mode {\n-            UIMode::View => Transition::Cmd(String::from(\"expand\")),\n-            _ => Transition::None,\n-        }\n+        Transition::Cmd(String::from(\"expand\"))\n     }\n     fn handle_transpose(&mut self) -> Transition {\n         match self.mode {\n", "instance_id": "nushell__nushell-15000", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the goal: adding a shortcut ('e') to expand selected cells in selection mode, mirroring functionality already present in non-selection mode and via the `:expand` command. The intent and desired outcome are evident, and the problem is contextualized with references to existing behavior. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define \"selection mode\" or provide examples of what \"expanding a selected cell\" entails (e.g., visual or data transformation). Additionally, there are no mentions of potential constraints, edge cases, or specific requirements for the shortcut's behavior (e.g., should it work only on certain types of selections?). While these omissions do not render the problem incomprehensible, they leave room for interpretation, which prevents a perfect clarity score.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward code modification with minimal complexity. Analyzing the provided code change, the task requires altering a single function (`handle_expand`) in one file to remove a conditional check based on the UI mode, thus enabling the `expand` command in all modes, including selection mode. The scope of the change is extremely limited, affecting only a few lines of code with no impact on the broader codebase architecture or interactions between modules. The technical concepts involved are basic\u2014understanding a simple state machine or mode-based logic in Rust and modifying a return value. No advanced language features, libraries, algorithms, or domain-specific knowledge are required. Furthermore, the problem statement and code changes do not indicate any specific edge cases or error handling requirements beyond what is already implemented. Overall, this is a very easy task that a junior developer with basic familiarity with Rust could accomplish with minimal effort.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`error make` ability to reference caller span\n### Related problem\n\nWhen throwing an error via `error make` in a custom command, the default span points to the `error make`.  It would be much more user-friendly (and useful) if there was some way to point it to the command itself.\r\n\r\nExample:\r\n\r\n```nu\r\ndef problem [] {\r\n  let error_condition = true\r\n  if $error_condition {\r\n    error make {\r\n      msg: \"Oops\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```\r\n> problem\r\nError:   \u00d7 Oops\r\n   \u256d\u2500[entry #14:4:5]\r\n 3 \u2502   if $error_condition {\r\n 4 \u2502     error make {\r\n   \u00b7     \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\r\n   \u00b7          \u2570\u2500\u2500 originates from here\r\n 5 \u2502       msg: \"Oops\"\r\n   \u2570\u2500\u2500\u2500\u2500\r\n```\n\n### Describe the solution you'd like\n\nSome way in the `error make` to specify that the span should be the calling custom command.  Preferably also some way to point to its input or arguments as the culprit.\r\n\r\nE.g.:\r\n\r\n```nu\r\ndef problem [] {\r\n  let error_condition = true\r\n  if $error_condition {\r\n    error make {\r\n      msg: \"Oops\"\r\n      source: \"caller\" # and/or perhaps \"pipeline\" or \"arguments\"\r\n    }\r\n  }\r\n}\r\n```\n\n### Describe alternatives you've considered\n\nI can obtain the *entire* caller via `view files | last`, but this isn't helpful if the previous \"file\" called the command multiple times.\n\n### Additional context and details\n\n_No response_\n", "patch": "diff --git a/crates/nu-engine/src/eval_ir.rs b/crates/nu-engine/src/eval_ir.rs\nindex ecdebbac51780..25b4cc1921de1 100644\n--- a/crates/nu-engine/src/eval_ir.rs\n+++ b/crates/nu-engine/src/eval_ir.rs\n@@ -187,6 +187,7 @@ fn eval_ir_block_impl<D: DebugContext>(\n \n     // Program counter, starts at zero.\n     let mut pc = 0;\n+    let need_backtrace = ctx.engine_state.get_env_var(\"NU_BACKTRACE\").is_some();\n \n     while pc < ir_block.instructions.len() {\n         let instruction = &ir_block.instructions[pc];\n@@ -195,7 +196,7 @@ fn eval_ir_block_impl<D: DebugContext>(\n \n         D::enter_instruction(ctx.engine_state, ir_block, pc, ctx.registers);\n \n-        let result = eval_instruction::<D>(ctx, instruction, span, ast);\n+        let result = eval_instruction::<D>(ctx, instruction, span, ast, need_backtrace);\n \n         D::leave_instruction(\n             ctx.engine_state,\n@@ -228,8 +229,10 @@ fn eval_ir_block_impl<D: DebugContext>(\n                     // If an error handler is set, branch there\n                     prepare_error_handler(ctx, error_handler, Some(err.into_spanned(*span)));\n                     pc = error_handler.handler_index;\n+                } else if need_backtrace {\n+                    let err = ShellError::into_chainned(err, *span);\n+                    return Err(err);\n                 } else {\n-                    // If not, exit the block with the error\n                     return Err(err);\n                 }\n             }\n@@ -285,6 +288,7 @@ fn eval_instruction<D: DebugContext>(\n     instruction: &Instruction,\n     span: &Span,\n     ast: &Option<IrAstRef>,\n+    need_backtrace: bool,\n ) -> Result<InstructionResult, ShellError> {\n     use self::InstructionResult::*;\n \n@@ -548,7 +552,14 @@ fn eval_instruction<D: DebugContext>(\n         }\n         Instruction::Call { decl_id, src_dst } => {\n             let input = ctx.take_reg(*src_dst);\n-            let result = eval_call::<D>(ctx, *decl_id, *span, input)?;\n+            let mut result = eval_call::<D>(ctx, *decl_id, *span, input)?;\n+            if need_backtrace {\n+                match &mut result {\n+                    PipelineData::ByteStream(s, ..) => s.push_caller_span(*span),\n+                    PipelineData::ListStream(s, ..) => s.push_caller_span(*span),\n+                    _ => (),\n+                };\n+            }\n             ctx.put_reg(*src_dst, result);\n             Ok(Continue)\n         }\n@@ -1457,14 +1468,40 @@ fn drain(ctx: &mut EvalContext<'_>, data: PipelineData) -> Result<InstructionRes\n     match data {\n         PipelineData::ByteStream(stream, ..) => {\n             let span = stream.span();\n-            if let Err(err) = stream.drain() {\n+            let callback_spans = stream.get_caller_spans().clone();\n+            if let Err(mut err) = stream.drain() {\n                 ctx.stack.set_last_error(&err);\n-                return Err(err);\n+                if callback_spans.is_empty() {\n+                    return Err(err);\n+                } else {\n+                    for s in callback_spans {\n+                        err = ShellError::EvalBlockWithInput {\n+                            span: s,\n+                            sources: vec![err],\n+                        }\n+                    }\n+                    return Err(err);\n+                }\n             } else {\n                 ctx.stack.set_last_exit_code(0, span);\n             }\n         }\n-        PipelineData::ListStream(stream, ..) => stream.drain()?,\n+        PipelineData::ListStream(stream, ..) => {\n+            let callback_spans = stream.get_caller_spans().clone();\n+            if let Err(mut err) = stream.drain() {\n+                if callback_spans.is_empty() {\n+                    return Err(err);\n+                } else {\n+                    for s in callback_spans {\n+                        err = ShellError::EvalBlockWithInput {\n+                            span: s,\n+                            sources: vec![err],\n+                        }\n+                    }\n+                    return Err(err);\n+                }\n+            }\n+        }\n         PipelineData::Value(..) | PipelineData::Empty => {}\n     }\n     Ok(Continue)\ndiff --git a/crates/nu-protocol/src/errors/chained_error.rs b/crates/nu-protocol/src/errors/chained_error.rs\nnew file mode 100644\nindex 0000000000000..ad45c441578a8\n--- /dev/null\n+++ b/crates/nu-protocol/src/errors/chained_error.rs\n@@ -0,0 +1,120 @@\n+use super::shell_error::ShellError;\n+use crate::Span;\n+use miette::{LabeledSpan, Severity, SourceCode};\n+use thiserror::Error;\n+\n+/// An error struct that contains source errors.\n+///\n+/// However, it's a bit special; if the error is constructed for the first time using\n+/// [`ChainedError::new`], it will behave the same as the single source error.\n+///\n+/// If it's constructed nestedly using [`ChainedError::new_chained`], it will treat all underlying errors as related.\n+///\n+/// For a usage example, please check [`ShellError::into_chainned`].\n+#[derive(Debug, Clone, PartialEq, Error)]\n+pub struct ChainedError {\n+    first: bool,\n+    pub(crate) sources: Vec<ShellError>,\n+    span: Span,\n+}\n+\n+impl std::fmt::Display for ChainedError {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        if self.first {\n+            write!(f, \"{}\", self.sources[0])\n+        } else {\n+            write!(f, \"oops\")\n+        }\n+    }\n+}\n+\n+impl ChainedError {\n+    pub fn new(source: ShellError, span: Span) -> Self {\n+        Self {\n+            first: true,\n+            sources: vec![source],\n+            span,\n+        }\n+    }\n+\n+    pub fn new_chained(sources: Self, span: Span) -> Self {\n+        Self {\n+            first: false,\n+            sources: vec![ShellError::ChainedError(sources)],\n+            span,\n+        }\n+    }\n+}\n+\n+impl miette::Diagnostic for ChainedError {\n+    fn related<'a>(&'a self) -> Option<Box<dyn Iterator<Item = &'a dyn miette::Diagnostic> + 'a>> {\n+        if self.first {\n+            self.sources[0].related()\n+        } else {\n+            Some(Box::new(self.sources.iter().map(|s| s as _)))\n+        }\n+    }\n+\n+    fn code<'a>(&'a self) -> Option<Box<dyn std::fmt::Display + 'a>> {\n+        if self.first {\n+            self.sources[0].code()\n+        } else {\n+            Some(Box::new(\"chained_error\"))\n+        }\n+    }\n+\n+    fn severity(&self) -> Option<Severity> {\n+        if self.first {\n+            self.sources[0].severity()\n+        } else {\n+            None\n+        }\n+    }\n+\n+    fn help<'a>(&'a self) -> Option<Box<dyn std::fmt::Display + 'a>> {\n+        if self.first {\n+            self.sources[0].help()\n+        } else {\n+            None\n+        }\n+    }\n+\n+    fn url<'a>(&'a self) -> Option<Box<dyn std::fmt::Display + 'a>> {\n+        if self.first {\n+            self.sources[0].url()\n+        } else {\n+            None\n+        }\n+    }\n+\n+    fn labels<'a>(&'a self) -> Option<Box<dyn Iterator<Item = LabeledSpan> + 'a>> {\n+        if self.first {\n+            self.sources[0].labels()\n+        } else {\n+            Some(Box::new(\n+                vec![LabeledSpan::new_with_span(\n+                    Some(\"error happened when running this\".to_string()),\n+                    self.span,\n+                )]\n+                .into_iter(),\n+            ))\n+        }\n+    }\n+\n+    // Finally, we redirect the source_code method to our own source.\n+    fn source_code(&self) -> Option<&dyn SourceCode> {\n+        if self.first {\n+            self.sources[0].source_code()\n+        } else {\n+            None\n+        }\n+    }\n+\n+    fn diagnostic_source(&self) -> Option<&dyn miette::Diagnostic> {\n+        if self.first {\n+            self.sources[0].diagnostic_source()\n+        } else {\n+            None\n+        }\n+    }\n+}\ndiff --git a/crates/nu-protocol/src/errors/cli_error.rs b/crates/nu-protocol/src/errors/cli_error.rs\nindex 7439d1df793b0..a53fec0600b88 100644\n--- a/crates/nu-protocol/src/errors/cli_error.rs\n+++ b/crates/nu-protocol/src/errors/cli_error.rs\n@@ -50,6 +50,10 @@ pub fn report_compile_error(working_set: &StateWorkingSet, error: &CompileError)\n \n fn report_error(working_set: &StateWorkingSet, error: &dyn miette::Diagnostic) {\n     eprintln!(\"Error: {:?}\", CliError(error, working_set));\n+    let have_no_backtrace = working_set.get_env_var(\"NU_BACKTRACE\").is_none();\n+    if have_no_backtrace {\n+        eprintln!(\"set the `NU_BACKTRACE=1` environment variable to display a backtrace.\")\n+    }\n     // reset vt processing, aka ansi because illbehaved externals can break it\n     #[cfg(windows)]\n     {\ndiff --git a/crates/nu-protocol/src/errors/mod.rs b/crates/nu-protocol/src/errors/mod.rs\nindex 5f385a21ee0c6..9c729dc8f430b 100644\n--- a/crates/nu-protocol/src/errors/mod.rs\n+++ b/crates/nu-protocol/src/errors/mod.rs\n@@ -1,3 +1,4 @@\n+mod chained_error;\n pub mod cli_error;\n mod compile_error;\n mod config_error;\ndiff --git a/crates/nu-protocol/src/errors/shell_error/mod.rs b/crates/nu-protocol/src/errors/shell_error/mod.rs\nindex 6f651836ca78d..c7be1dfec5b80 100644\n--- a/crates/nu-protocol/src/errors/shell_error/mod.rs\n+++ b/crates/nu-protocol/src/errors/shell_error/mod.rs\n@@ -1,3 +1,4 @@\n+use super::chained_error::ChainedError;\n use crate::{\n     ast::Operator, engine::StateWorkingSet, format_shell_error, record, ConfigError, LabeledError,\n     ParseError, Span, Spanned, Type, Value,\n@@ -1325,6 +1326,10 @@ On Windows, this would be %USERPROFILE%\\AppData\\Roaming\"#\n         #[label = \"while running this code\"]\n         span: Option<Span>,\n     },\n+\n+    #[error(transparent)]\n+    #[diagnostic(transparent)]\n+    ChainedError(ChainedError),\n }\n \n impl ShellError {\n@@ -1373,6 +1378,16 @@ impl ShellError {\n             span,\n         )\n     }\n+\n+    /// Convert self error to a [`ShellError::ChainedError`] variant.\n+    pub fn into_chainned(self, span: Span) -> Self {\n+        match self {\n+            ShellError::ChainedError(inner) => {\n+                ShellError::ChainedError(ChainedError::new_chained(inner, span))\n+            }\n+            other => ShellError::ChainedError(ChainedError::new(other, span)),\n+        }\n+    }\n }\n \n impl From<Box<dyn std::error::Error>> for ShellError {\ndiff --git a/crates/nu-protocol/src/pipeline/byte_stream.rs b/crates/nu-protocol/src/pipeline/byte_stream.rs\nindex adefbdf9f0a9a..2a7c79e1285c1 100644\n--- a/crates/nu-protocol/src/pipeline/byte_stream.rs\n+++ b/crates/nu-protocol/src/pipeline/byte_stream.rs\n@@ -193,6 +193,7 @@ pub struct ByteStream {\n     signals: Signals,\n     type_: ByteStreamType,\n     known_size: Option<u64>,\n+    caller_spans: Vec<Span>,\n }\n \n impl ByteStream {\n@@ -209,9 +210,22 @@ impl ByteStream {\n             signals,\n             type_,\n             known_size: None,\n+            caller_spans: vec![],\n         }\n     }\n \n+    /// Push a caller [`Span`] to the bytestream, it's useful to construct a backtrace.\n+    pub fn push_caller_span(&mut self, span: Span) {\n+        if span != self.span {\n+            self.caller_spans.push(span)\n+        }\n+    }\n+\n+    /// Get all caller [`Span`], it's useful to construct a backtrace.\n+    pub fn get_caller_spans(&self) -> &Vec<Span> {\n+        &self.caller_spans\n+    }\n+\n     /// Create a [`ByteStream`] from an arbitrary reader. The type must be provided.\n     pub fn read(\n         reader: impl Read + Send + 'static,\ndiff --git a/crates/nu-protocol/src/pipeline/list_stream.rs b/crates/nu-protocol/src/pipeline/list_stream.rs\nindex 55ae4bfee020f..f300a443d6934 100644\n--- a/crates/nu-protocol/src/pipeline/list_stream.rs\n+++ b/crates/nu-protocol/src/pipeline/list_stream.rs\n@@ -15,6 +15,7 @@ pub type ValueIterator = Box<dyn Iterator<Item = Value> + Send + 'static>;\n pub struct ListStream {\n     stream: ValueIterator,\n     span: Span,\n+    caller_spans: Vec<Span>,\n }\n \n impl ListStream {\n@@ -27,6 +28,7 @@ impl ListStream {\n         Self {\n             stream: Box::new(InterruptIter::new(iter, signals)),\n             span,\n+            caller_spans: vec![],\n         }\n     }\n \n@@ -35,6 +37,18 @@ impl ListStream {\n         self.span\n     }\n \n+    /// Push a caller [`Span`] to the bytestream, it's useful to construct a backtrace.\n+    pub fn push_caller_span(&mut self, span: Span) {\n+        if span != self.span {\n+            self.caller_spans.push(span)\n+        }\n+    }\n+\n+    /// Get all caller [`Span`], it's useful to construct a backtrace.\n+    pub fn get_caller_spans(&self) -> &Vec<Span> {\n+        &self.caller_spans\n+    }\n+\n     /// Changes the [`Span`] associated with this [`ListStream`].\n     pub fn with_span(mut self, span: Span) -> Self {\n         self.span = span;\n@@ -94,6 +108,7 @@ impl ListStream {\n         Self {\n             stream: Box::new(f(self.stream)),\n             span: self.span,\n+            caller_spans: self.caller_spans,\n         }\n     }\n \n", "instance_id": "nushell__nushell-14945", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `error make` command in the Nushell environment, where the error span currently points to the `error make` call rather than the calling custom command or its arguments. The goal of allowing the error span to reference the caller or related context is articulated, and an example is provided to illustrate the current behavior and desired outcome. However, there are minor ambiguities and missing details. For instance, the statement does not fully specify how the \"caller\" or \"pipeline\" source options should behave in complex scenarios (e.g., nested calls or multiple arguments). Additionally, edge cases such as how to handle multiple callers or invalid spans are not addressed. While alternatives are mentioned, they lack depth in explaining why they are insufficient. Overall, the problem is valid and mostly clear but lacks comprehensive detail on edge cases and precise behavior expectations.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, impacting multiple files and modules within the Nushell codebase (`nu-engine`, `nu-protocol`), including core components like error handling and pipeline data structures. The changes involve modifications to the evaluation logic (`eval_ir.rs`), error reporting mechanisms (`shell_error.rs`, `cli_error.rs`), and data structures (`byte_stream.rs`, `list_stream.rs`), as well as the introduction of a new error chaining mechanism (`chained_error.rs`). This requires a deep understanding of the codebase architecture and the interactions between instruction evaluation, error propagation, and span tracking.\n\nSecond, the number of technical concepts involved is substantial. The solution demands familiarity with Rust's ownership and borrowing model, complex error handling using libraries like `miette` and `thiserror`, and custom data structures for pipeline processing (`ByteStream`, `ListStream`). Additionally, it requires knowledge of how spans are managed in a shell interpreter and how to construct a backtrace-like mechanism for error reporting. The implementation of `ChainedError` and the logic to track caller spans add further complexity.\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes reveal the need to handle scenarios like nested errors, multiple caller spans, and ensuring that spans are correctly associated with errors during pipeline processing. The error handling logic must be robust to avoid breaking existing functionality, which adds to the challenge.\n\nFinally, the impact on the system is notable as it modifies core error reporting behavior, which is critical for user experience in a shell environment. A mistake in this area could lead to confusing error messages or runtime issues, requiring careful design and testing. Given the depth of understanding required, the cross-module changes, and the potential for subtle bugs in error propagation, I rate this problem at 0.75, indicating a hard but not extremely challenging task. It does not reach the 0.8-1.0 range as it does not involve system-level or highly domain-specific complexities beyond the shell's internal architecture.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Mercury decode panics when mem_limit is set to None\nIt seems to be introduced a long time ago. I'm bisecting.\r\n\r\nSteps to reproduce:\r\n\r\n1. navigate to `mercury/src/internal/pack/decode.rs:test_pack_decode_with_ref_delta`\r\n2. set te second parameter of `Pack::new()` to `None`\r\n3. run the test and observe the crash\r\n\r\nEnvironment: \r\n```\r\ncargo 1.85.0-nightly (20a443231 2024-12-06)\r\nrelease: 1.85.0-nightly\r\ncommit-hash: 20a443231846b81c7b909691ec3f15eb173f2b18\r\ncommit-date: 2024-12-06\r\nhost: x86_64-unknown-linux-gnu\r\nlibgit2: 1.8.1 (sys:0.19.0 vendored)\r\nlibcurl: 8.9.0-DEV (sys:0.4.74+curl-8.9.0 vendored ssl:OpenSSL/1.1.1w)\r\nssl: OpenSSL 1.1.1w  11 Sep 2023\r\nos: Arch Linux [64-bit]\r\n```\r\n\r\nStacktrace: \r\n```\r\n---- internal::pack::decode::tests::test_pack_decode_without_delta stdout ----\r\nthread '<unnamed>' panicked at mercury/src/internal/pack/cache.rs:84:35:\r\ncalled `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: \"No such file or directory\" }\r\nstack backtrace:\r\n   0: rust_begin_unwind\r\n             at /rustc/21fe748be15271ea5804e0507cd699b675efe038/library/std/src/panicking.rs:681:5\r\n   1: core::panicking::panic_fmt\r\n             at /rustc/21fe748be15271ea5804e0507cd699b675efe038/library/core/src/panicking.rs:75:14\r\n   2: core::result::unwrap_failed\r\n             at /rustc/21fe748be15271ea5804e0507cd699b675efe038/library/core/src/result.rs:1699:5\r\n   3: core::result::Result<T,E>::unwrap\r\n             at /home/iz0/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/result.rs:1104:23\r\n   4: mercury::internal::pack::cache::Caches::generate_temp_path\r\n             at ./src/internal/pack/cache.rs:84:13\r\n   5: <mercury::internal::pack::cache::Caches as mercury::internal::pack::cache::_Cache>::insert\r\n             at ./src/internal/pack/cache.rs:156:34\r\n   6: mercury::internal::pack::decode::<impl mercury::internal::pack::Pack>::cache_obj_and_process_waitlist\r\n             at ./src/internal/pack/decode.rs:535:23\r\n   7: mercury::internal::pack::decode::<impl mercury::internal::pack::Pack>::decode::{{closure}}\r\n             at ./src/internal/pack/decode.rs:391:33\r\n   8: <F as threadpool::FnBox>::call_box\r\n             at /home/iz0/.cargo/registry/src/index.crates.io-6f17d22bba15001f/threadpool-1.8.1/src/lib.rs:95:9\r\n   9: threadpool::spawn_in_pool::{{closure}}\r\n \r\nthread '<unnamed>' panicked at mercury/src/internal/pack/cache.rs:150:49:\r\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\r\n```\n", "patch": "diff --git a/mercury/src/internal/pack/cache.rs b/mercury/src/internal/pack/cache.rs\nindex 64c4cd40..244a12f3 100644\n--- a/mercury/src/internal/pack/cache.rs\n+++ b/mercury/src/internal/pack/cache.rs\n@@ -159,7 +159,9 @@ impl _Cache for Caches {\n                 self.complete_signal.clone(),\n                 Some(self.pool.clone()),\n             );\n-            a_obj.set_store_path(Caches::generate_temp_path(&self.tmp_path, hash));\n+            if self.mem_size.is_some() {\n+                a_obj.set_store_path(Caches::generate_temp_path(&self.tmp_path, hash));\n+            }\n             let _ = map.insert(hash, a_obj);\n         }\n         //order maters as for reading in 'get_by_offset()'\ndiff --git a/mercury/src/internal/pack/decode.rs b/mercury/src/internal/pack/decode.rs\nindex 2766e596..7755a03e 100644\n--- a/mercury/src/internal/pack/decode.rs\n+++ b/mercury/src/internal/pack/decode.rs\n@@ -75,7 +75,7 @@ impl Pack {\n             pool: Arc::new(ThreadPool::new(thread_num)),\n             waitlist: Arc::new(Waitlist::new()),\n             caches:  Arc::new(Caches::new(cache_mem_size, temp_path, thread_num)),\n-            mem_limit: mem_limit.unwrap_or(usize::MAX),\n+            mem_limit,\n             cache_objs_mem: Arc::new(AtomicUsize::default()),\n             clean_tmp,\n         }\n@@ -363,7 +363,8 @@ impl Pack {\n             }\n             // 3 parts: Waitlist + TheadPool + Caches\n             // hardcode the limit of the tasks of threads_pool queue, to limit memory\n-            while self.memory_used() > self.mem_limit || self.pool.queued_count() > 2000 {\n+            while self.pool.queued_count() > 2000 \n+                || self.mem_limit.map(|limit| self.memory_used() > limit).unwrap_or(false) {\n                 thread::yield_now();\n             }\n             let r: Result<CacheObject, GitError> = self.decode_pack_object(&mut reader, &mut offset);\n@@ -709,6 +710,19 @@ mod tests {\n         p.decode(&mut buffered,|_,_|{}).unwrap();\n     }\n \n+    #[test]\n+    fn test_pack_decode_no_mem_limit() {\n+        let mut source = PathBuf::from(env::current_dir().unwrap().parent().unwrap());\n+        source.push(\"tests/data/packs/pack-1d0e6c14760c956c173ede71cb28f33d921e232f.pack\");\n+\n+        let tmp = PathBuf::from(\"/tmp/.cache_temp\");\n+\n+        let f = fs::File::open(source).unwrap();\n+        let mut buffered = BufReader::new(f);\n+        let mut p = Pack::new(None, None, Some(tmp), true);\n+        p.decode(&mut buffered, |_,_|{}).unwrap();\n+    }\n+\n     #[test]\n     fn test_pack_decode_with_large_file_with_delta_without_ref() {\n         init_logger();\ndiff --git a/mercury/src/internal/pack/mod.rs b/mercury/src/internal/pack/mod.rs\nindex 3f942ca2..360204ce 100644\n--- a/mercury/src/internal/pack/mod.rs\n+++ b/mercury/src/internal/pack/mod.rs\n@@ -29,7 +29,7 @@ pub struct Pack {\n     pub pool: Arc<ThreadPool>,\n     pub waitlist: Arc<Waitlist>,\n     pub caches: Arc<Caches>,\n-    pub mem_limit: usize,\n+    pub mem_limit: Option<usize>,\n     pub cache_objs_mem: Arc<AtomicUsize>, // the memory size of CacheObjects in this Pack\n     pub clean_tmp: bool,\n }\n", "instance_id": "web3infra-foundation__mega-746", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a panic occurs in the Mercury decode functionality when the `mem_limit` is set to `None`. The steps to reproduce the issue are provided, along with the environment details and a stack trace, which helps in understanding the context of the failure. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior when `mem_limit` is `None`\u2014should it disable memory limiting entirely, or should there be a default fallback value? Additionally, there are no explicit mentions of edge cases or constraints beyond the specific test case provided. While the stack trace points to specific lines of code, the root cause analysis is left to the developer, and the statement lacks comprehensive examples or detailed requirements for the fix. Overall, it is clear enough to start working on but leaves some room for interpretation.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively contained, affecting a few specific files (`cache.rs`, `decode.rs`, and `mod.rs`) with targeted modifications to handle the `Option<usize>` type for `mem_limit`. The changes involve conditional logic to avoid panics when `mem_limit` is `None` and adjusting memory limit checks, which requires a moderate understanding of the codebase's logic around memory management and caching. Second, the technical concepts involved include Rust's ownership and borrowing model, handling `Option` types, and working with concurrent structures like `Arc` and `ThreadPool`, which are moderately complex but not overly advanced. Third, the problem touches on error handling, as the panic is caused by an unwrap on a `Result` and potential mutex poisoning, but the provided changes address this by adding conditional checks rather than introducing complex error recovery mechanisms. Finally, while the changes do not significantly impact the system's architecture, they require understanding interactions between memory limits, caching, and decoding processes. There are also implicit edge cases to consider, such as behavior under no memory limit and potential performance implications, though these are not explicitly detailed in the problem statement. Overall, this problem requires a moderate level of expertise in Rust and familiarity with the codebase, placing it at a difficulty of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add features for Trash management\nAs broot now includes the `trashy` crate for the `:trash` internal (if i understand it correctly), it could make use of more of its features for managing the Trash.\r\n\r\nI would like to have commands to\r\n- `cd` into the system trash directory\r\n- restore files\r\n- clear the Trash directory\r\n\r\nThis way, broot could replace [trashy](https://github.com/oberblastmeister/trashy), which is not up-to-date anyway. \n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex c5e42046..62154d04 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,5 +1,6 @@\n ### next\n - fix build on Android - thanks @dead10ck\n+- `:open_trash` shows the content of the trash. Other new internals & verbs: `:delete_trashed_file`, `:restore_trashed_file`, `:purge_trash` - Fix #855\n \n ### v1.38.0 - 2024-05-04\n <a name=\"v1.38.0\"></a>\ndiff --git a/bacon.toml b/bacon.toml\nindex ee7b1065..7105efbd 100644\n--- a/bacon.toml\n+++ b/bacon.toml\n@@ -14,7 +14,7 @@ watch = [\"tests\", \"benches\", \"examples\"]\n command = [\n \t\"cargo\", \"check\",\n \t\"--color\", \"always\",\n-\t\"--features\", \"clipboard, kitty-csi-check\",\n+\t\"--features\", \"clipboard kitty-csi-check trash\",\n ]\n need_stdout = false\n watch = [\"benches\"]\ndiff --git a/src/app/app.rs b/src/app/app.rs\nindex 9ccde9e8..c5603a02 100644\n--- a/src/app/app.rs\n+++ b/src/app/app.rs\n@@ -512,6 +512,12 @@ impl App {\n                     self.mut_panel().clear_input_invocation(con);\n                 }\n             }\n+            Message(md) => {\n+                if is_input_invocation {\n+                    self.mut_panel().clear_input_invocation(con);\n+                }\n+                self.mut_panel().set_message(md);\n+            }\n             Launch(launchable) => {\n                 self.launch_at_end = Some(*launchable);\n                 self.quitting = true;\ndiff --git a/src/app/cmd_result.rs b/src/app/cmd_result.rs\nindex ff7b4627..91e38c05 100644\n--- a/src/app/cmd_result.rs\n+++ b/src/app/cmd_result.rs\n@@ -42,6 +42,7 @@ pub enum CmdResult {\n     },\n     HandleInApp(Internal), // command must be handled at the app level\n     Keep,\n+    Message(String),\n     Launch(Box<Launchable>),\n     NewPanel {\n         state: Box<dyn PanelState>,\n@@ -118,6 +119,7 @@ impl fmt::Debug for CmdResult {\n                 CmdResult::DisplayError(_) => \"DisplayError\",\n                 CmdResult::ExecuteSequence{ .. } => \"ExecuteSequence\",\n                 CmdResult::Keep => \"Keep\",\n+                CmdResult::Message { .. } => \"Message\",\n                 CmdResult::Launch(_) => \"Launch\",\n                 CmdResult::NewState { .. } => \"NewState\",\n                 CmdResult::NewPanel { .. } => \"NewPanel\",\ndiff --git a/src/app/panel_state.rs b/src/app/panel_state.rs\nindex a593edf3..b003dc2a 100644\n--- a/src/app/panel_state.rs\n+++ b/src/app/panel_state.rs\n@@ -157,6 +157,41 @@ pub trait PanelState {\n                 validate_purpose: false,\n                 panel_ref: PanelReference::Active,\n             },\n+            #[cfg(feature = \"trash\")]\n+            Internal::purge_trash => {\n+                let res = trash::os_limited::list()\n+                    .and_then(|items| {\n+                        trash::os_limited::purge_all(items)\n+                    });\n+                match res {\n+                    Ok(()) => CmdResult::RefreshState { clear_cache: false },\n+                    Err(e) => CmdResult::DisplayError(format!(\"{e}\")),\n+                }\n+            }\n+            #[cfg(feature = \"trash\")]\n+            Internal::open_trash => {\n+                let trash_state = crate::trash::TrashState::new(\n+                    self.tree_options(),\n+                    con,\n+                );\n+                match trash_state {\n+                    Ok(state) => {\n+                        let bang = input_invocation\n+                            .map(|inv| inv.bang)\n+                            .unwrap_or(internal_exec.bang);\n+                        if bang && cc.app.preview_panel.is_none() {\n+                            CmdResult::NewPanel {\n+                                state: Box::new(state),\n+                                purpose: PanelPurpose::None,\n+                                direction: HDir::Right,\n+                            }\n+                        } else {\n+                            CmdResult::new_state(Box::new(state))\n+                        }\n+                    }\n+                    Err(e) => CmdResult::DisplayError(format!(\"{e}\")),\n+                }\n+            }\n             #[cfg(unix)]\n             Internal::filesystems => {\n                 let fs_state = crate::filesystems::FilesystemState::new(\ndiff --git a/src/app/standard_status.rs b/src/app/standard_status.rs\nindex 17169a58..f6828191 100644\n--- a/src/app/standard_status.rs\n+++ b/src/app/standard_status.rs\n@@ -223,10 +223,13 @@ impl<'s> StandardStatusBuilder<'s> {\n                 }\n             }\n             PanelStateType::Fs => {\n-                warn!(\"TODO fs status\");\n+                // TODO fs status\n             }\n             PanelStateType::Stage => {\n-                warn!(\"TODO stage status\");\n+                // TODO stage status\n+            }\n+            PanelStateType::Trash => {\n+                // TODO stage status ? Maybe the shortcuts to restore or delete ?\n             }\n         }\n         parts.to_status(self.width)\ndiff --git a/src/app/state_type.rs b/src/app/state_type.rs\nindex 81a49613..b0454826 100644\n--- a/src/app/state_type.rs\n+++ b/src/app/state_type.rs\n@@ -8,9 +8,6 @@ use {\n #[serde(rename_all = \"snake_case\")]\n pub enum PanelStateType {\n \n-    /// standard browsing tree\n-    Tree,\n-\n     /// filesystems\n     Fs,\n \n@@ -22,4 +19,10 @@ pub enum PanelStateType {\n \n     /// stage panel, never alone on screen\n     Stage,\n+\n+    /// content of the trash\n+    Trash,\n+\n+    /// standard browsing tree\n+    Tree,\n }\ndiff --git a/src/command/panel_input.rs b/src/command/panel_input.rs\nindex e7384938..ca310532 100644\n--- a/src/command/panel_input.rs\n+++ b/src/command/panel_input.rs\n@@ -377,8 +377,6 @@ impl PanelInput {\n         let raw = self.input_field.get_content();\n         let parts = CommandParts::from(raw.clone());\n \n-        info!(\"parts: {:#?}\", parts);\n-\n         let verb = if self.is_key_allowed_for_verb(key, mode) {\n             self.find_key_verb(\n                 key,\ndiff --git a/src/display/matched_string.rs b/src/display/matched_string.rs\nindex db4920e2..ea8610c4 100644\n--- a/src/display/matched_string.rs\n+++ b/src/display/matched_string.rs\n@@ -77,7 +77,7 @@ impl<'a, 'w> MatchedString<'a> {\n         let mut width = self.width();\n         for (idx, c) in self.string.char_indices() {\n             if width <= max_width { break; }\n-            break_idx = idx;\n+            break_idx = idx + c.len_utf8();\n             let char_width = c.width().unwrap_or(0);\n             if char_width > width {\n                 warn!(\"inconsistent char/str widths\");\ndiff --git a/src/errors.rs b/src/errors.rs\nindex 86e0d880..bfdc8e5e 100644\n--- a/src/errors.rs\n+++ b/src/errors.rs\n@@ -32,6 +32,7 @@ custom_error! {pub ProgramError\n     UnprintableFile = \"File can't be printed\", // has characters that can't be printed without escaping\n     Unrecognized {token: String} = \"Unrecognized: {token}\",\n     ZeroLenFile = \"File seems empty\",\n+    Trash {message: String} = \"Trash error: {message}\",\n }\n \n custom_error! {pub ShellInstallError\ndiff --git a/src/lib.rs b/src/lib.rs\nindex bb22ab89..4bd5199e 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -30,6 +30,7 @@ pub mod skin;\n pub mod syntactic;\n pub mod task_sync;\n pub mod terminal;\n+pub mod trash;\n pub mod tree;\n pub mod tree_build;\n pub mod verb;\ndiff --git a/src/trash/mod.rs b/src/trash/mod.rs\nnew file mode 100644\nindex 00000000..8757b625\n--- /dev/null\n+++ b/src/trash/mod.rs\n@@ -0,0 +1,6 @@\n+\n+#[cfg(feature = \"trash\")]\n+mod trash_state;\n+\n+#[cfg(feature = \"trash\")]\n+pub use trash_state::*;\ndiff --git a/src/trash/trash_state.rs b/src/trash/trash_state.rs\nnew file mode 100644\nindex 00000000..c2e969e7\n--- /dev/null\n+++ b/src/trash/trash_state.rs\n@@ -0,0 +1,583 @@\n+use {\n+    crate::{\n+        app::*,\n+        command::*,\n+        display::*,\n+        errors::ProgramError,\n+        pattern::*,\n+        tree::TreeOptions,\n+        verb::*,\n+    },\n+    crokey::crossterm::{\n+        cursor,\n+        style::Color,\n+        QueueableCommand,\n+    },\n+    std::{\n+        path::Path,\n+    },\n+    termimad::{\n+        minimad::Alignment,\n+        *,\n+    },\n+    trash::{\n+        self as trash_crate,\n+        TrashItem,\n+    },\n+    unicode_width::UnicodeWidthStr,\n+};\n+\n+struct FilteredContent {\n+    pattern: Pattern,\n+    items: Vec<TrashItem>,\n+    selection_idx: Option<usize>,\n+}\n+\n+/// an application state showing the content of the trash\n+pub struct TrashState {\n+    items: Vec<TrashItem>,\n+    selection_idx: Option<usize>,\n+    scroll: usize,\n+    page_height: usize,\n+    tree_options: TreeOptions,\n+    filtered: Option<FilteredContent>,\n+    mode: Mode,\n+}\n+\n+impl TrashState {\n+    /// create a state listing the content of the system's trash\n+    pub fn new(\n+        tree_options: TreeOptions,\n+        con: &AppContext,\n+    ) -> Result<TrashState, ProgramError> {\n+        let items = trash::os_limited::list()\n+            .map_err(|e| ProgramError::Trash { message: e.to_string() })?;\n+        let selection_idx = None;\n+        Ok(TrashState {\n+            items,\n+            selection_idx,\n+            scroll: 0,\n+            page_height: 0,\n+            tree_options,\n+            filtered: None,\n+            mode: con.initial_mode(),\n+        })\n+    }\n+    pub fn count(&self) -> usize {\n+        self.filtered\n+            .as_ref()\n+            .map(|f| f.items.len())\n+            .unwrap_or_else(|| self.items.len().into())\n+    }\n+    pub fn try_scroll(\n+        &mut self,\n+        cmd: ScrollCommand,\n+    ) -> bool {\n+        let old_scroll = self.scroll;\n+        self.scroll = cmd.apply(self.scroll, self.count(), self.page_height);\n+        // move selection to an item in view\n+        if let Some(f) = self.filtered.as_mut() {\n+            if let Some(idx) = f.selection_idx {\n+                if idx < self.scroll {\n+                    f.selection_idx = Some(self.scroll);\n+                } else if idx >= self.scroll + self.page_height {\n+                    f.selection_idx = Some(self.scroll + self.page_height - 1);\n+                }\n+            }\n+        } else {\n+            if let Some(idx) = self.selection_idx {\n+                if idx < self.scroll {\n+                    self.selection_idx = Some(self.scroll);\n+                } else if idx >= self.scroll + self.page_height {\n+                    self.selection_idx = Some(self.scroll + self.page_height - 1);\n+                }\n+            }\n+        }\n+        self.scroll != old_scroll\n+    }\n+    /// If there's a selection, adjust the scroll to make it visible\n+    pub fn show_selection(&mut self) {\n+        let selection_idx = if let Some(f) = self.filtered.as_ref() {\n+            f.selection_idx\n+        } else {\n+            self.selection_idx\n+        };\n+        if let Some(idx) = selection_idx {\n+            if idx < self.scroll {\n+                self.scroll = idx;\n+            } else if idx >= self.scroll + self.page_height {\n+                self.scroll = idx - self.page_height + 1;\n+            }\n+        }\n+    }\n+\n+    /// change the selection\n+    fn move_line(\n+        &mut self,\n+        internal_exec: &InternalExecution,\n+        input_invocation: Option<&VerbInvocation>,\n+        dir: i32, // -1 for up, 1 for down\n+        cycle: bool,\n+    ) -> CmdResult {\n+        let count = get_arg(input_invocation, internal_exec, 1);\n+        let dec = dir * count;\n+        let selection_idx;\n+        if let Some(f) = self.filtered.as_mut() {\n+            selection_idx = if let Some(idx) = f.selection_idx {\n+                Some(move_sel(idx, f.items.len(), dec, cycle))\n+            } else if !f.items.is_empty() {\n+                Some(if dec > 0 { 0 } else { f.items.len() - 1 })\n+            } else {\n+                None\n+            };\n+            f.selection_idx = selection_idx;\n+        } else {\n+            selection_idx = if let Some(idx) = self.selection_idx {\n+                Some(move_sel(idx, self.items.len(), dec, cycle))\n+            } else if !self.items.is_empty() {\n+                Some(if dec > 0 { 0 } else { self.items.len() - 1 })\n+            } else {\n+                None\n+            };\n+            self.selection_idx = selection_idx;\n+        }\n+        if let Some(selection_idx) = selection_idx {\n+            if selection_idx < self.scroll {\n+                self.scroll = selection_idx;\n+            } else if selection_idx >= self.scroll + self.page_height {\n+                self.scroll = selection_idx + 1 - self.page_height;\n+            }\n+        }\n+        CmdResult::Keep\n+    }\n+\n+    fn selected_item(&self) -> Option<&TrashItem> {\n+        if let Some(f) = self.filtered.as_ref() {\n+            f.selection_idx.map(|idx| &f.items[idx])\n+        } else {\n+            self.selection_idx.map(|idx| &self.items[idx])\n+        }\n+    }\n+\n+    fn take_selected_item(&mut self) -> Option<TrashItem> {\n+        if let Some(f) = self.filtered.as_mut() {\n+            if let Some(idx) = f.selection_idx {\n+                let item = f.items.remove(idx);\n+                if f.items.is_empty() {\n+                    f.selection_idx = None;\n+                } else if idx == f.items.len() {\n+                    f.selection_idx = Some(idx - 1);\n+                }\n+                Some(item)\n+            } else {\n+                None\n+            }\n+        } else {\n+            if let Some(idx) = self.selection_idx {\n+                let item = self.items.remove(idx);\n+                if self.items.is_empty() {\n+                    self.selection_idx = None;\n+                } else if idx == self.items.len() {\n+                    self.selection_idx = Some(idx - 1);\n+                }\n+                Some(item)\n+            } else {\n+                None\n+            }\n+        }\n+    }\n+}\n+\n+impl PanelState for TrashState {\n+    fn get_type(&self) -> PanelStateType {\n+        PanelStateType::Trash\n+    }\n+\n+    fn set_mode(\n+        &mut self,\n+        mode: Mode,\n+    ) {\n+        self.mode = mode;\n+    }\n+\n+    fn get_mode(&self) -> Mode {\n+        self.mode\n+    }\n+\n+    /// We don't want to expose path to verbs because you can't\n+    /// normally access files in the trash\n+    fn selected_path(&self) -> Option<&Path> {\n+        None\n+    }\n+\n+    fn tree_options(&self) -> TreeOptions {\n+        self.tree_options.clone()\n+    }\n+\n+    fn with_new_options(\n+        &mut self,\n+        _screen: Screen,\n+        change_options: &dyn Fn(&mut TreeOptions) -> &'static str,\n+        _in_new_panel: bool, // TODO open tree if true\n+        _con: &AppContext,\n+    ) -> CmdResult {\n+        change_options(&mut self.tree_options);\n+        CmdResult::Keep\n+    }\n+\n+    /// We don't want to expose path to verbs because you can't\n+    /// normally access files in the trash\n+    fn selection(&self) -> Option<Selection<'_>> {\n+        None\n+    }\n+\n+    fn refresh(\n+        &mut self,\n+        _screen: Screen,\n+        _con: &AppContext,\n+    ) -> Command {\n+        // minimal implementation. It would be better to keep filtering, and\n+        // also selection & scroll whenever possible\n+        if let Ok(items) = trash::os_limited::list() {\n+            self.items = items;\n+            self.selection_idx = None;\n+            self.scroll = 0;\n+        }\n+        Command::empty()\n+    }\n+\n+    fn on_pattern(\n+        &mut self,\n+        pattern: InputPattern,\n+        _app_state: &AppState,\n+        _con: &AppContext,\n+    ) -> Result<CmdResult, ProgramError> {\n+        if pattern.is_none() {\n+            if let Some(f) = self.filtered.take() {\n+                if let Some(idx) = f.selection_idx {\n+                    self.selection_idx = self.items\n+                        .iter()\n+                        .position(|m| m.id == f.items[idx].id);\n+                }\n+            }\n+        } else {\n+            let pattern = pattern.pattern;\n+            let mut best_score = 0;\n+            let mut selection_idx = None;\n+            let mut items = Vec::new();\n+            for item in &self.items {\n+                let score = pattern.score_of_string(&item.name).unwrap_or(0)\n+                    + pattern\n+                        .score_of_string(&item.original_parent.to_string_lossy())\n+                        .unwrap_or(0);\n+                if score > 0 {\n+                    items.push(item.clone());\n+                    if score > best_score {\n+                        best_score = score;\n+                        selection_idx = Some(items.len() - 1);\n+                    }\n+                }\n+            }\n+            self.filtered = Some(FilteredContent {\n+                pattern,\n+                items,\n+                selection_idx,\n+            });\n+        }\n+        self.show_selection();\n+        Ok(CmdResult::Keep)\n+    }\n+\n+    fn display(\n+        &mut self,\n+        w: &mut W,\n+        disc: &DisplayContext,\n+    ) -> Result<(), ProgramError> {\n+        let title_parent = \"Original parent\";\n+        let title_name = \"Deleted file name\";\n+        let area = &disc.state_area;\n+        let con = &disc.con;\n+        self.page_height = area.height as usize - 2;\n+        let (items, selection_idx) = if let Some(filtered) = &self.filtered {\n+            (filtered.items.as_slice(), filtered.selection_idx)\n+        } else {\n+            (self.items.as_slice(), self.selection_idx)\n+        };\n+        let scrollbar = area.scrollbar(self.scroll, items.len());\n+        //- style preparation\n+        let styles = &disc.panel_skin.styles;\n+        let selection_bg = styles\n+            .selected_line\n+            .get_bg()\n+            .unwrap_or(Color::AnsiValue(240));\n+        let match_style = &styles.char_match;\n+        let mut selected_match_style = styles.char_match.clone();\n+        selected_match_style.set_bg(selection_bg);\n+        let border_style = &styles.help_table_border;\n+        let mut selected_border_style = styles.help_table_border.clone();\n+        selected_border_style.set_bg(selection_bg);\n+        //- width computations\n+        let width = area.width as usize;\n+        let optimal_parent_width = items\n+            .iter()\n+            .map(|m| m.original_parent.to_string_lossy().width())\n+            .max()\n+            .unwrap_or(0)\n+            .max(title_parent.len());\n+        let optimal_name_width = items\n+            .iter()\n+            .map(|m| m.name.width())\n+            .max()\n+            .unwrap_or(0)\n+            .max(title_name.len());\n+        let available_width = if con.show_selection_mark {\n+            width - 1\n+        } else {\n+            width\n+        };\n+        let mut w_parent = optimal_parent_width;\n+        let mut w_name = optimal_name_width;\n+        if w_name + w_parent > available_width {\n+            w_name = (width * 2 / 3).min(optimal_name_width);\n+            w_parent = width - w_name;\n+        }\n+        info!(\"optimal_parent_width: {}, optimal_name_width: {}\", optimal_parent_width, optimal_name_width);\n+        info!(\"available_width: {}, w_parent: {}, w_name: {}\", available_width, w_parent, w_name);\n+        //- titles\n+        w.queue(cursor::MoveTo(area.left, area.top))?;\n+        let mut cw = CropWriter::new(w, width);\n+        if con.show_selection_mark {\n+            cw.queue_char(&styles.default, ' ')?;\n+        }\n+        let title = if title_parent.len() > w_parent {\n+            &title_parent[..w_parent]\n+        } else {\n+            title_parent\n+        };\n+        cw.queue_g_string(&styles.default, format!(\"{:^w_parent$}\", title))?;\n+        cw.queue_char(border_style, '\u2502')?;\n+        let title = if title_name.len() > w_name {\n+            &title_name[..w_name]\n+        } else {\n+            title_name\n+        };\n+        cw.queue_g_string(&styles.default, format!(\"{:^w_name$}\", title))?;\n+        cw.fill(border_style, &SPACE_FILLING)?;\n+        //- horizontal line\n+        w.queue(cursor::MoveTo(area.left, 1 + area.top))?;\n+        let mut cw = CropWriter::new(w, width);\n+        if con.show_selection_mark {\n+            cw.queue_char(&styles.default, ' ')?;\n+        }\n+        cw.queue_g_string(border_style, format!(\"{:\u2500>width$}\", '\u253c', width = w_parent + 1))?;\n+        cw.fill(border_style, &BRANCH_FILLING)?;\n+        //- content\n+        let mut idx = self.scroll;\n+        for y in 2..area.height {\n+            w.queue(cursor::MoveTo(area.left, y + area.top))?;\n+            let selected = selection_idx == Some(idx);\n+            let mut cw = CropWriter::new(w, width - 1); // -1 for scrollbar\n+            let txt_style = if selected {\n+                &styles.selected_line\n+            } else {\n+                &styles.default\n+            };\n+            if let Some(item) = items.get(idx) {\n+                let match_style = if selected {\n+                    &selected_match_style\n+                } else {\n+                    match_style\n+                };\n+                let border_style = if selected {\n+                    &selected_border_style\n+                } else {\n+                    border_style\n+                };\n+                if con.show_selection_mark {\n+                    cw.queue_char(txt_style, if selected { '\u25b6' } else { ' ' })?;\n+                }\n+                // parent\n+                let s = item.original_parent.to_string_lossy();\n+                let mut matched_string = MatchedString::new(\n+                    self.filtered\n+                        .as_ref()\n+                        .and_then(|f| f.pattern.search_string(&s)),\n+                    &s,\n+                    txt_style,\n+                    match_style,\n+                );\n+                if s.width() > w_parent {\n+                    //info!(\"CUT w_parent: {}, s.width(): {}\", w_parent, s.width());\n+                    cw.queue_char(txt_style, '\u2026')?;\n+                    matched_string.cut_left_to_fit(w_parent - 1);\n+                    //info!(\" cut string width: {}\", matched_string.string.width());\n+                    matched_string.queue_on(&mut cw)?;\n+                } else {\n+                    matched_string.fill(w_parent, Alignment::Left);\n+                    matched_string.queue_on(&mut cw)?;\n+                }\n+                cw.queue_char(border_style, '\u2502')?;\n+                // name\n+                let s = &item.name;\n+                let mut matched_string = MatchedString::new(\n+                    self.filtered\n+                        .as_ref()\n+                        .and_then(|f| f.pattern.search_string(s)),\n+                    s,\n+                    txt_style,\n+                    match_style,\n+                );\n+                matched_string.fill(w_name, Alignment::Left);\n+                matched_string.queue_on(&mut cw)?;\n+                idx += 1;\n+            } else {\n+                if con.show_selection_mark {\n+                    cw.queue_char(&styles.default, ' ')?;\n+                }\n+                cw.queue_g_string(border_style, format!(\"{: >width$}\", '\u2502', width = w_parent + 1))?;\n+            }\n+            cw.fill(txt_style, &SPACE_FILLING)?;\n+            let scrollbar_style = if ScrollCommand::is_thumb(y, scrollbar) {\n+                &styles.scrollbar_thumb\n+            } else {\n+                &styles.scrollbar_track\n+            };\n+            scrollbar_style.queue_str(w, \"\u2590\")?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn on_internal(\n+        &mut self,\n+        w: &mut W,\n+        invocation_parser: Option<&InvocationParser>,\n+        internal_exec: &InternalExecution,\n+        input_invocation: Option<&VerbInvocation>,\n+        trigger_type: TriggerType,\n+        app_state: &mut AppState,\n+        cc: &CmdContext,\n+    ) -> Result<CmdResult, ProgramError> {\n+        use Internal::*;\n+        Ok(match internal_exec.internal {\n+            Internal::restore_trashed_file => {\n+                if let Some(item) = self.selected_item() {\n+                    match trash_crate::os_limited::restore_all([item.clone()]) {\n+                        Ok(_) => {\n+                            let path = item.original_path();\n+                            self.take_selected_item();\n+                            CmdResult::Message(format!(\n+                                \"File *{}* restored\",\n+                                path.to_string_lossy(),\n+                            ))\n+                        }\n+                        Err(trash_crate::Error::RestoreCollision { path, .. }) => {\n+                            CmdResult::DisplayError(format!(\n+                                \"collision: *{}* already exists\",\n+                                path.to_string_lossy(),\n+                            ))\n+                        }\n+                        Err(e) => {\n+                            CmdResult::DisplayError(format!(\n+                                \"restore failed: {}\",\n+                                e.to_string(),\n+                            ))\n+                        }\n+                    }\n+                } else {\n+                    CmdResult::DisplayError(\n+                        \"an item must be selected\".to_string(),\n+                    )\n+                }\n+            }\n+            Internal::delete_trashed_file => {\n+                if let Some(item) = self.selected_item() {\n+                    match trash_crate::os_limited::purge_all([item.clone()]) {\n+                        Ok(_) => {\n+                            let path = item.original_path();\n+                            self.take_selected_item();\n+                            CmdResult::Message(format!(\n+                                \"File *{}* restored\",\n+                                path.to_string_lossy(),\n+                            ))\n+                        }\n+                        Err(e) => {\n+                            CmdResult::DisplayError(format!(\n+                                \"deletion failed: {}\",\n+                                e.to_string(),\n+                            ))\n+                        }\n+                    }\n+                } else {\n+                    CmdResult::DisplayError(\n+                        \"an item must be selected\".to_string(),\n+                    )\n+                }\n+            }\n+            Internal::back => {\n+                if let Some(f) = self.filtered.take() {\n+                    if let Some(idx) = f.selection_idx {\n+                        self.selection_idx = self.items\n+                            .iter()\n+                            .position(|m| m.id == f.items[idx].id);\n+                    }\n+                    self.show_selection();\n+                    CmdResult::Keep\n+                } else {\n+                    CmdResult::PopState\n+                }\n+            }\n+            Internal::line_down => self.move_line(internal_exec, input_invocation, 1, true),\n+            Internal::line_up => self.move_line(internal_exec, input_invocation, -1, true),\n+            Internal::line_down_no_cycle => {\n+                self.move_line(internal_exec, input_invocation, 1, false)\n+            }\n+            Internal::line_up_no_cycle => {\n+                self.move_line(internal_exec, input_invocation, -1, false)\n+            }\n+            Internal::open_stay => {\n+                // it would probably be a good idea to bind enter to restore_trash_file ?\n+                CmdResult::DisplayError(\"can't open a file from the trash\".to_string())\n+            }\n+            Internal::panel_left_no_open => CmdResult::HandleInApp(Internal::panel_left_no_open),\n+            Internal::panel_right_no_open => CmdResult::HandleInApp(Internal::panel_right_no_open),\n+            Internal::page_down => {\n+                if !self.try_scroll(ScrollCommand::Pages(1)) {\n+                    self.selection_idx = Some(self.count() - 1);\n+                }\n+                CmdResult::Keep\n+            }\n+            Internal::page_up => {\n+                if !self.try_scroll(ScrollCommand::Pages(-1)) {\n+                    self.selection_idx = Some(0);\n+                }\n+                CmdResult::Keep\n+            }\n+            open_leave => CmdResult::PopStateAndReapply,\n+            _ => self.on_internal_generic(\n+                w,\n+                invocation_parser,\n+                internal_exec,\n+                input_invocation,\n+                trigger_type,\n+                app_state,\n+                cc,\n+            )?,\n+        })\n+    }\n+\n+    fn on_click(\n+        &mut self,\n+        _x: u16,\n+        y: u16,\n+        _screen: Screen,\n+        _con: &AppContext,\n+    ) -> Result<CmdResult, ProgramError> {\n+        if y >= 2 {\n+            let y = y as usize - 2 + self.scroll;\n+            if y < self.items.len().into() {\n+                self.selection_idx = Some(y);\n+            }\n+        }\n+        Ok(CmdResult::Keep)\n+    }\n+}\ndiff --git a/src/verb/internal.rs b/src/verb/internal.rs\nindex 47231163..90f14867 100644\n--- a/src/verb/internal.rs\n+++ b/src/verb/internal.rs\n@@ -96,6 +96,7 @@ Internals! {\n     open_staging_area: \"open the staging area\" false,\n     open_stay: \"open file or directory according to OS (stay in broot)\" true,\n     open_stay_filter: \"display the directory, keeping the current pattern\" true,\n+    open_trash: \"show the content of the trash\" false,\n     page_down: \"scroll one page down\" false,\n     page_up: \"scroll one page up\" false,\n     panel_left: \"focus or open panel on left\" false,\n@@ -114,6 +115,9 @@ Internals! {\n     print_tree: \"print tree and leaves broot\" true,\n     quit: \"quit Broot\" false,\n     refresh: \"refresh tree and clear size cache\" false,\n+    delete_trashed_file: \"irreversibly delete a file which is in the trash\" false,\n+    restore_trashed_file: \"restore a file which is in the trash\" false,\n+    purge_trash: \"irreversibly delete the trash's content\" false,\n     root_down: \"move tree root down\" true,\n     root_up: \"move tree root up\" true,\n     select: \"select a file by path\" true,\ndiff --git a/src/verb/verb_store.rs b/src/verb/verb_store.rs\nindex e6019126..10a65791 100644\n--- a/src/verb/verb_store.rs\n+++ b/src/verb/verb_store.rs\n@@ -147,6 +147,20 @@ impl VerbStore {\n             StayInBroot,\n         )\n             .with_shortcut(\"cpp\");\n+        #[cfg(feature = \"trash\")]\n+        self.add_internal(trash);\n+        #[cfg(feature = \"trash\")]\n+        self.add_internal(open_trash)\n+            .with_shortcut(\"ot\");\n+        #[cfg(feature = \"trash\")]\n+        self.add_internal(restore_trashed_file)\n+            .with_shortcut(\"rt\");\n+        #[cfg(feature = \"trash\")]\n+        self.add_internal(delete_trashed_file)\n+            .with_shortcut(\"dt\");\n+        #[cfg(feature = \"trash\")]\n+        self.add_internal(purge_trash)\n+            .with_shortcut(\"pt\");\n         #[cfg(unix)]\n         self.add_internal(filesystems)\n             .with_shortcut(\"fs\");\n@@ -311,7 +325,6 @@ impl VerbStore {\n         self.add_internal(toggle_perm).with_shortcut(\"perm\");\n         self.add_internal(toggle_sizes).with_shortcut(\"sizes\");\n         self.add_internal(toggle_trim_root);\n-        self.add_internal(trash);\n         self.add_internal(total_search).with_key(key!(ctrl-s));\n         self.add_internal(up_tree).with_shortcut(\"up\");\n \ndiff --git a/website/docs/trash.md b/website/docs/trash.md\nnew file mode 100644\nindex 00000000..aa746c7a\n--- /dev/null\n+++ b/website/docs/trash.md\n@@ -0,0 +1,13 @@\n+\n+## Commands\n+\n+When broot is compiled with the `trash` feature enabled (which is done for Linux and Windows in standard distributions), some additional commands are available:\n+\n+* `:trash` : move the selected file to the trash\n+* `:open_trash` : display the content of the trash\n+* `:restore_trashed_file` : restore the file to its original location\n+* `:delete_trashed_file` : irreversibly delete a file which is in the trash\n+* `:purge_trash` : irreversibly delete the whole content of the trash\n+\n+`:restore_trashed_file` and `:delete_trashed_file` are only available when the trash content is displayed.\n+\ndiff --git a/website/mkdocs.yml b/website/mkdocs.yml\nindex 7e11a633..72402cdd 100644\n--- a/website/mkdocs.yml\n+++ b/website/mkdocs.yml\n@@ -18,6 +18,7 @@ nav:\n                 - Verbs & Commands: verbs.md\n                 - Panels: panels.md\n                 - Staging Area: staging-area.md\n+                - Trash: trash.md\n                 - Tree export: export.md\n                 - Tips & tricks: tricks.md\n                 - Common file operations: file-operations.md\n", "instance_id": "Canop__broot-882", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to add trash management features to the `broot` application, leveraging the `trashy` crate. It specifies the desired functionalities: navigating to the system trash directory, restoring files, and clearing the trash. However, it lacks critical details such as specific input/output formats, constraints, or examples of usage scenarios. Additionally, edge cases (e.g., handling conflicts during file restoration, permissions issues, or empty trash scenarios) are not mentioned, which could lead to ambiguity during implementation. While the goal is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and modules (e.g., `app.rs`, `panel_state.rs`, `trash_state.rs`, `verb_store.rs`, etc.), and involves adding a new state (`TrashState`) to the application. This requires a deep understanding of the `broot` codebase architecture, particularly how states, panels, and commands interact. Second, the implementation touches on several technical concepts, including Rust feature flags (`#[cfg(feature = \"trash\")]`), integration with an external crate (`trashy`), UI rendering for trash content, and handling user interactions (e.g., selection, scrolling, and filtering). Third, the code changes introduce new error handling logic (e.g., for restoration collisions and deletion failures), which adds complexity, though edge cases are not exhaustively covered in the problem statement. Finally, while the changes do not fundamentally alter the system's architecture, they require careful integration into the existing command and state management systems. A score of 0.65 reflects the need for a solid grasp of Rust, the `broot` internals, and moderate-to-complex feature integration, without reaching the level of very hard problems that involve system-level or highly intricate domain-specific challenges.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "chore(deps): update rust crate webview2-com to 0.36\nThis PR contains the following updates:\n\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [webview2-com](https://redirect.github.com/wravery/webview2-rs) | dependencies | minor | `0.35` -> `0.36` |\n\n---\n\n### Configuration\n\n\ud83d\udcc5 **Schedule**: Branch creation - At any time (no schedule defined), Automerge - At any time (no schedule defined).\n\n\ud83d\udea6 **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\n\u267b **Rebasing**: Whenever PR is behind base branch, or you tick the rebase/retry checkbox.\n\n\ud83d\udd15 **Ignore**: Close this PR and you won't be reminded about this update again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/tauri-apps/wry).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xNzYuMiIsInVwZGF0ZWRJblZlciI6IjM5LjE3Ni4yIiwidGFyZ2V0QnJhbmNoIjoiZGV2IiwibGFiZWxzIjpbXX0=-->\n\n", "patch": "diff --git a/.changes/webview2-0.36-windows-0.60.md b/.changes/webview2-0.36-windows-0.60.md\nnew file mode 100644\nindex 000000000..cff0e0172\n--- /dev/null\n+++ b/.changes/webview2-0.36-windows-0.60.md\n@@ -0,0 +1,5 @@\n+---\n+wry: minor\n+---\n+\n+Updated `webview2-com` to `0.36`, `windows` to `0.60`.\ndiff --git a/Cargo.lock b/Cargo.lock\nindex 30a805fbd..119500753 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -197,9 +197,9 @@ dependencies = [\n \n [[package]]\n name = \"bumpalo\"\n-version = \"3.16.0\"\n+version = \"3.17.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"79296716171880943b8470b5f8d03aa55eb2e645a4874bdbb28adb49162e012c\"\n+checksum = \"1628fb46dfa0b37568d12e5edd512553eccf6a22a78e8bde00bb4aed84d5bdbf\"\n \n [[package]]\n name = \"bytemuck\"\n@@ -215,9 +215,9 @@ checksum = \"1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b\"\n \n [[package]]\n name = \"bytes\"\n-version = \"1.9.0\"\n+version = \"1.10.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"325918d6fe32f23b19878fe4b34794ae41fc19ddbe53b10571a4874d44ffd39b\"\n+checksum = \"f61dac84819c6588b558454b194026eb1f09c293b9036ae9b159e74e73ab6cf9\"\n \n [[package]]\n name = \"cairo-rs\"\n@@ -272,9 +272,9 @@ dependencies = [\n \n [[package]]\n name = \"cc\"\n-version = \"1.2.10\"\n+version = \"1.2.15\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"13208fcbb66eaeffe09b99fffbe1af420f00a7b35aa99ad683dfc1aa76145229\"\n+checksum = \"c736e259eea577f443d5c86c304f9f4ae0295c43f3ba05c21f1d66b5f06001af\"\n dependencies = [\n  \"jobserver\",\n  \"libc\",\n@@ -315,36 +315,6 @@ version = \"0.2.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"613afe47fcd5fac7ccf1db93babcb082c5994d996f20b8b159f2ad1658eb5724\"\n \n-[[package]]\n-name = \"cocoa\"\n-version = \"0.26.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f79398230a6e2c08f5c9760610eb6924b52aa9e7950a619602baba59dcbbdbb2\"\n-dependencies = [\n- \"bitflags 2.8.0\",\n- \"block\",\n- \"cocoa-foundation\",\n- \"core-foundation 0.10.0\",\n- \"core-graphics 0.24.0\",\n- \"foreign-types\",\n- \"libc\",\n- \"objc\",\n-]\n-\n-[[package]]\n-name = \"cocoa-foundation\"\n-version = \"0.2.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"e14045fb83be07b5acf1c0884b2180461635b433455fa35d1cd6f17f1450679d\"\n-dependencies = [\n- \"bitflags 2.8.0\",\n- \"block\",\n- \"core-foundation 0.10.0\",\n- \"core-graphics-types 0.2.0\",\n- \"libc\",\n- \"objc\",\n-]\n-\n [[package]]\n name = \"codespan-reporting\"\n version = \"0.11.1\"\n@@ -522,7 +492,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"13b588ba4ac1a99f7f2964d24b3d896ddc6bf847ee3855dbd4366f058cfcd331\"\n dependencies = [\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -542,15 +512,15 @@ dependencies = [\n \n [[package]]\n name = \"derive_more\"\n-version = \"0.99.18\"\n+version = \"0.99.19\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5f33878137e4dafd7fa914ad4e259e18a4e8e532b9617a2d0150262bf53abfce\"\n+checksum = \"3da29a38df43d6f156149c9b43ded5e018ddff2a855cf2cfd62e8cd7d079c69f\"\n dependencies = [\n  \"convert_case\",\n  \"proc-macro2\",\n  \"quote\",\n  \"rustc_version\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -577,7 +547,7 @@ checksum = \"97369cbbc041bc366949bc74d34658d6cda5621039731c6310521892a3a20ae0\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -609,14 +579,14 @@ checksum = \"f2b99bf03862d7f545ebc28ddd33a665b50865f4dfd84031a393823879bd4c54\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n name = \"document-features\"\n-version = \"0.2.10\"\n+version = \"0.2.11\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"cb6969eaabd2421f8a2775cfd2471a2b634372b4a25d41e3bd647b79912850a0\"\n+checksum = \"95249b50c6c185bee49034bcb378a49dc2b5dff0be90ff6616d31d64febab05d\"\n dependencies = [\n  \"litrs\",\n ]\n@@ -659,9 +629,9 @@ checksum = \"92773504d58c093f6de2459af4af33faa518c13451eb8f2b5698ed3d36e7c813\"\n \n [[package]]\n name = \"equivalent\"\n-version = \"1.0.1\"\n+version = \"1.0.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5\"\n+checksum = \"877a4ace8713b0bcf2a4e7eec82529c029f1d0619886d18145fea96c3ffe5c0f\"\n \n [[package]]\n name = \"errno\"\n@@ -713,7 +683,7 @@ checksum = \"1a5c6c585bc94aaf2c7b51dd4c2ba22680844aba4c687be581871a6f518c5742\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -781,7 +751,7 @@ checksum = \"162ee34ebcb7c64a8abebc059ce0fee27c2262618d7b60ed8faf72fef13c3650\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -1043,7 +1013,7 @@ dependencies = [\n  \"proc-macro-error\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -1188,7 +1158,7 @@ dependencies = [\n  \"proc-macro-error\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -1376,7 +1346,7 @@ checksum = \"1ec89e9337638ecdc08744df490b221a7399bf8d164eb52a665454e60e075ad6\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -1556,7 +1526,7 @@ checksum = \"c0ff37bd590ca25063e35af745c343cb7a0271906fb7b37e4813e8f79f00268d\"\n dependencies = [\n  \"bitflags 2.8.0\",\n  \"libc\",\n- \"redox_syscall 0.5.8\",\n+ \"redox_syscall 0.5.9\",\n ]\n \n [[package]]\n@@ -1589,9 +1559,9 @@ dependencies = [\n \n [[package]]\n name = \"log\"\n-version = \"0.4.25\"\n+version = \"0.4.26\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"04cbf5b083de1c7e0222a7a51dbfdba1cbe1c6ab0b15e29fff3f6c077fd9cd9f\"\n+checksum = \"30bde2b3dc3671ae49d8e2e9f044c7c005836e7a023ee57cffa25ab82764bb9e\"\n \n [[package]]\n name = \"mac\"\n@@ -1760,10 +1730,10 @@ version = \"0.7.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"af1844ef2428cc3e1cb900be36181049ef3d3193c63e43026cfe202983b27a56\"\n dependencies = [\n- \"proc-macro-crate 1.3.1\",\n+ \"proc-macro-crate 2.0.2\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -2058,9 +2028,9 @@ dependencies = [\n \n [[package]]\n name = \"once_cell\"\n-version = \"1.20.2\"\n+version = \"1.20.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1261fe7e33c73b354eab43b1273a57c8f967d0391e80353e51f764ac02cf6775\"\n+checksum = \"945462a4b81e43c4e3ba96bd7b49d834c6f61198356aa858733bc4acf3cbe62e\"\n \n [[package]]\n name = \"orbclient\"\n@@ -2123,7 +2093,7 @@ checksum = \"1e401f977ab385c9e4e3ab30627d6f26d00e2c73eef317493c4ec6d468726cf8\"\n dependencies = [\n  \"cfg-if\",\n  \"libc\",\n- \"redox_syscall 0.5.8\",\n+ \"redox_syscall 0.5.9\",\n  \"smallvec\",\n  \"windows-targets 0.52.6\",\n ]\n@@ -2200,6 +2170,16 @@ dependencies = [\n  \"rand 0.8.5\",\n ]\n \n+[[package]]\n+name = \"phf_generator\"\n+version = \"0.11.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"3c80231409c20246a13fddb31776fb942c38553c51e871f8cbd687a4cfb5843d\"\n+dependencies = [\n+ \"phf_shared 0.11.3\",\n+ \"rand 0.8.5\",\n+]\n+\n [[package]]\n name = \"phf_macros\"\n version = \"0.8.0\"\n@@ -2220,7 +2200,7 @@ version = \"0.8.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"c00cf8b9eafe68dde5e9eaa2cef8ee84a9336a47d566ec55ca16589633b65af7\"\n dependencies = [\n- \"siphasher\",\n+ \"siphasher 0.3.11\",\n ]\n \n [[package]]\n@@ -2229,27 +2209,36 @@ version = \"0.10.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"b6796ad771acdc0123d2a88dc428b5e38ef24456743ddb1744ed628f9815c096\"\n dependencies = [\n- \"siphasher\",\n+ \"siphasher 0.3.11\",\n+]\n+\n+[[package]]\n+name = \"phf_shared\"\n+version = \"0.11.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"67eabc2ef2a60eb7faa00097bd1ffdb5bd28e62bf39990626a582201b7a754e5\"\n+dependencies = [\n+ \"siphasher 1.0.1\",\n ]\n \n [[package]]\n name = \"pin-project\"\n-version = \"1.1.8\"\n+version = \"1.1.9\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1e2ec53ad785f4d35dac0adea7f7dc6f1bb277ad84a680c7afefeae05d1f5916\"\n+checksum = \"dfe2e71e1471fe07709406bf725f710b02927c9c54b2b5b2ec0e8087d97c327d\"\n dependencies = [\n  \"pin-project-internal\",\n ]\n \n [[package]]\n name = \"pin-project-internal\"\n-version = \"1.1.8\"\n+version = \"1.1.9\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d56a66c0c55993aa927429d0f8a0abfd74f084e4d9c192cffed01e418d83eefb\"\n+checksum = \"f6e859e6e5bd50440ab63c47e3ebabc90f26251f7c73c3d3e837b74a1cc3fa67\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -2385,9 +2374,9 @@ checksum = \"afbdc74edc00b6f6a218ca6a5364d6226a259d4b8ea1af4a0ea063f27e179f4d\"\n \n [[package]]\n name = \"quick-xml\"\n-version = \"0.36.2\"\n+version = \"0.37.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f7649a7b4df05aed9ea7ec6f628c67c9953a43869b8bc50929569b2999d443fe\"\n+checksum = \"165859e9e55f79d67b96c5d96f4e88b6f2695a1972849c15a6a3f5c59fc2c003\"\n dependencies = [\n  \"memchr\",\n ]\n@@ -2505,9 +2494,9 @@ dependencies = [\n \n [[package]]\n name = \"redox_syscall\"\n-version = \"0.5.8\"\n+version = \"0.5.9\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"03a862b389f93e68874fbf580b9de08dd02facb9a788ebadaf4a3fd33cf58834\"\n+checksum = \"82b568323e98e49e2a0899dcee453dd679fae22d69adf9b11dd508d1549b7e2f\"\n dependencies = [\n  \"bitflags 2.8.0\",\n ]\n@@ -2614,22 +2603,22 @@ checksum = \"f79dfe2d285b0488816f30e700a7438c5a73d816b5b7d3ac72fbc48b0d185e03\"\n \n [[package]]\n name = \"serde\"\n-version = \"1.0.217\"\n+version = \"1.0.218\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"02fc4265df13d6fa1d00ecff087228cc0a2b5f3c0e87e258d8b94a156e984c70\"\n+checksum = \"e8dfc9d19bdbf6d17e22319da49161d5d0108e4188e8b680aef6299eed22df60\"\n dependencies = [\n  \"serde_derive\",\n ]\n \n [[package]]\n name = \"serde_derive\"\n-version = \"1.0.217\"\n+version = \"1.0.218\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5a9bf7cf98d04a2b28aead066b7496853d4779c9cc183c440dbac457641e19a0\"\n+checksum = \"f09503e191f4e797cb8aac08e9a4a4695c5edf6a2e70e376d961ddd5c969f82b\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -2674,6 +2663,12 @@ version = \"0.3.11\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"38b58827f4464d87d377d175e90bf58eb00fd8716ff0a62f80356b5e61555d0d\"\n \n+[[package]]\n+name = \"siphasher\"\n+version = \"1.0.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"56199f7ddabf13fe5074ce809e7d3f42b42ae711800501b5b16ea82ad029c39d\"\n+\n [[package]]\n name = \"slab\"\n version = \"0.4.9\"\n@@ -2694,9 +2689,9 @@ dependencies = [\n \n [[package]]\n name = \"smallvec\"\n-version = \"1.13.2\"\n+version = \"1.14.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3c5e1a9a646d36c3599cd173a41282daf47c44583ad367b8e6837255952e5c67\"\n+checksum = \"7fcf8323ef1faaee30a44a340193b1ac6814fd9b7b4e88e9d4519a3e4abe1cfd\"\n \n [[package]]\n name = \"smithay-client-toolkit\"\n@@ -2787,26 +2782,25 @@ checksum = \"6637bab7722d379c8b41ba849228d680cc12d0a45ba1fa2b48f2a30577a06731\"\n \n [[package]]\n name = \"string_cache\"\n-version = \"0.8.7\"\n+version = \"0.8.8\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f91138e76242f575eb1d3b38b4f1362f10d3a43f47d182a5b359af488a02293b\"\n+checksum = \"938d512196766101d333398efde81bc1f37b00cb42c2f8350e5df639f040bbbe\"\n dependencies = [\n  \"new_debug_unreachable\",\n- \"once_cell\",\n  \"parking_lot\",\n- \"phf_shared 0.10.0\",\n+ \"phf_shared 0.11.3\",\n  \"precomputed-hash\",\n  \"serde\",\n ]\n \n [[package]]\n name = \"string_cache_codegen\"\n-version = \"0.5.2\"\n+version = \"0.5.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"6bb30289b722be4ff74a408c3cc27edeaad656e06cb1fe8fa9231fa59c728988\"\n+checksum = \"c711928715f1fe0fe509c53b43e993a9a557babc2d0a3567d0a3006f1ac931a0\"\n dependencies = [\n- \"phf_generator 0.10.0\",\n- \"phf_shared 0.10.0\",\n+ \"phf_generator 0.11.3\",\n+ \"phf_shared 0.11.3\",\n  \"proc-macro2\",\n  \"quote\",\n ]\n@@ -2824,9 +2818,9 @@ dependencies = [\n \n [[package]]\n name = \"syn\"\n-version = \"2.0.96\"\n+version = \"2.0.98\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d5d0adab1ae378d7f53bdebc67a39f1f151407ef230f0ce2883572f5d8985c80\"\n+checksum = \"36147f1a48ae0ec2b5b3bc5b537d267457555a10dc06f3dbc8cb11ba3006d3b1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -2841,7 +2835,7 @@ checksum = \"c8af7666ab7b6390ab78131fb5b0fce11d6b7a6951602017c35fa82800708971\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -2859,12 +2853,11 @@ dependencies = [\n \n [[package]]\n name = \"tao\"\n-version = \"0.31.1\"\n+version = \"0.32.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3731d04d4ac210cd5f344087733943b9bfb1a32654387dad4d1c70de21aee2c9\"\n+checksum = \"8d96d84dbf4ad70179129e6533738e30604f8977c8646723da684521645f5f95\"\n dependencies = [\n  \"bitflags 2.8.0\",\n- \"cocoa\",\n  \"core-foundation 0.10.0\",\n  \"core-graphics 0.24.0\",\n  \"crossbeam-channel\",\n@@ -2881,7 +2874,9 @@ dependencies = [\n  \"ndk\",\n  \"ndk-context\",\n  \"ndk-sys 0.6.0+11769913\",\n- \"objc\",\n+ \"objc2 0.6.0\",\n+ \"objc2-app-kit 0.3.0\",\n+ \"objc2-foundation 0.3.0\",\n  \"once_cell\",\n  \"parking_lot\",\n  \"raw-window-handle\",\n@@ -2889,8 +2884,8 @@ dependencies = [\n  \"tao-macros\",\n  \"unicode-segmentation\",\n  \"url\",\n- \"windows 0.58.0\",\n- \"windows-core 0.58.0\",\n+ \"windows 0.60.0\",\n+ \"windows-core 0.60.1\",\n  \"windows-version\",\n  \"x11-dl\",\n ]\n@@ -2903,7 +2898,7 @@ checksum = \"f4e16beb8b2ac17db28eab8bca40e62dbfbb34c0fcdc6d9826b11b7b5d047dfd\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -2964,7 +2959,7 @@ checksum = \"4fee6c4efc90059e10f81e6d42c60a18f76588c3d74cb83a0b242a2b6c7504c1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -2975,7 +2970,7 @@ checksum = \"26afc1baea8a989337eeb52b6e72a039780ce45c3edfcc9c5b9d112feeb173c2\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -3108,7 +3103,7 @@ checksum = \"395ae124c09f9e6918a2310af6038fba074bcf474ac352496d5910dd59a2226d\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -3128,15 +3123,15 @@ checksum = \"d2df906b07856748fa3f6e0ad0cbaa047052d4a7dd609e231c4f72cee8c36f31\"\n \n [[package]]\n name = \"typenum\"\n-version = \"1.17.0\"\n+version = \"1.18.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"42ff0bf0c66b8238c6f3b578df37d0b7848e55df8577b3f74f92a69acceeb825\"\n+checksum = \"1dccffe3ce07af9386bfd29e80c0ab1a8205a2fc34e4bcd40364df902cfa8f3f\"\n \n [[package]]\n name = \"unicode-ident\"\n-version = \"1.0.15\"\n+version = \"1.0.17\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"11cd88e12b17c6494200a9c1b683a04fcac9573ed74cd1b62aeb2727c5592243\"\n+checksum = \"00e2473a93778eb0bad35909dff6a10d28e63f792f16ed15e404fca9d5eeedbe\"\n \n [[package]]\n name = \"unicode-segmentation\"\n@@ -3250,7 +3245,7 @@ dependencies = [\n  \"log\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n  \"wasm-bindgen-shared\",\n ]\n \n@@ -3285,7 +3280,7 @@ checksum = \"8ae87ea40c9f689fc23f209965b6fb8a99ad69aeeb0231408be24920604395de\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n  \"wasm-bindgen-backend\",\n  \"wasm-bindgen-shared\",\n ]\n@@ -3301,9 +3296,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-backend\"\n-version = \"0.3.7\"\n+version = \"0.3.8\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"056535ced7a150d45159d3a8dc30f91a2e2d588ca0b23f70e56033622b8016f6\"\n+checksum = \"b7208998eaa3870dad37ec8836979581506e0c5c64c20c9e79e9d2a10d6f47bf\"\n dependencies = [\n  \"cc\",\n  \"downcast-rs\",\n@@ -3315,9 +3310,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-client\"\n-version = \"0.31.7\"\n+version = \"0.31.8\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b66249d3fc69f76fd74c82cc319300faa554e9d865dab1f7cd66cc20db10b280\"\n+checksum = \"c2120de3d33638aaef5b9f4472bff75f07c56379cf76ea320bd3a3d65ecaf73f\"\n dependencies = [\n  \"bitflags 2.8.0\",\n  \"rustix\",\n@@ -3338,9 +3333,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-cursor\"\n-version = \"0.31.7\"\n+version = \"0.31.8\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"32b08bc3aafdb0035e7fe0fdf17ba0c09c268732707dca4ae098f60cb28c9e4c\"\n+checksum = \"a93029cbb6650748881a00e4922b076092a6a08c11e7fbdb923f064b23968c5d\"\n dependencies = [\n  \"rustix\",\n  \"wayland-client\",\n@@ -3349,9 +3344,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-protocols\"\n-version = \"0.32.5\"\n+version = \"0.32.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"7cd0ade57c4e6e9a8952741325c30bf82f4246885dca8bf561898b86d0c1f58e\"\n+checksum = \"0781cf46869b37e36928f7b432273c0995aa8aed9552c556fb18754420541efc\"\n dependencies = [\n  \"bitflags 2.8.0\",\n  \"wayland-backend\",\n@@ -3361,9 +3356,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-protocols-plasma\"\n-version = \"0.3.5\"\n+version = \"0.3.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"9b31cab548ee68c7eb155517f2212049dc151f7cd7910c2b66abfd31c3ee12bd\"\n+checksum = \"7ccaacc76703fefd6763022ac565b590fcade92202492381c95b2edfdf7d46b3\"\n dependencies = [\n  \"bitflags 2.8.0\",\n  \"wayland-backend\",\n@@ -3374,9 +3369,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-protocols-wlr\"\n-version = \"0.3.5\"\n+version = \"0.3.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"782e12f6cd923c3c316130d56205ebab53f55d6666b7faddfad36cecaeeb4022\"\n+checksum = \"248a02e6f595aad796561fa82d25601bd2c8c3b145b1c7453fc8f94c1a58f8b2\"\n dependencies = [\n  \"bitflags 2.8.0\",\n  \"wayland-backend\",\n@@ -3387,9 +3382,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-scanner\"\n-version = \"0.31.5\"\n+version = \"0.31.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"597f2001b2e5fc1121e3d5b9791d3e78f05ba6bfa4641053846248e3a13661c3\"\n+checksum = \"896fdafd5d28145fce7958917d69f2fd44469b1d4e861cb5961bcbeebc6d1484\"\n dependencies = [\n  \"proc-macro2\",\n  \"quick-xml\",\n@@ -3398,9 +3393,9 @@ dependencies = [\n \n [[package]]\n name = \"wayland-sys\"\n-version = \"0.31.5\"\n+version = \"0.31.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"efa8ac0d8e8ed3e3b5c9fc92c7881406a268e11555abe36493efabe649a29e09\"\n+checksum = \"dbcebb399c77d5aa9fa5db874806ee7b4eba4e73650948e8f93963f128896615\"\n dependencies = [\n  \"dlib\",\n  \"log\",\n@@ -3474,14 +3469,14 @@ dependencies = [\n \n [[package]]\n name = \"webview2-com\"\n-version = \"0.35.0\"\n+version = \"0.36.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"1730fcfc2f6b32d92096f5e97bc2b5ccafe14ad9d4787ef106daab2a20896def\"\n+checksum = \"b0d606f600e5272b514dbb66539dd068211cc20155be8d3958201b4b5bd79ed3\"\n dependencies = [\n  \"webview2-com-macros\",\n  \"webview2-com-sys\",\n- \"windows 0.59.0\",\n- \"windows-core 0.59.0\",\n+ \"windows 0.60.0\",\n+ \"windows-core 0.60.1\",\n  \"windows-implement 0.59.0\",\n  \"windows-interface 0.59.0\",\n ]\n@@ -3494,19 +3489,18 @@ checksum = \"1d228f15bba3b9d56dde8bddbee66fa24545bd17b48d5128ccf4a8742b18e431\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n name = \"webview2-com-sys\"\n-version = \"0.35.0\"\n+version = \"0.36.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"39cfcc1cdcddbb723d5385f20d718a83ad2a1dc856de1a925358f2bdf5a58b93\"\n+checksum = \"bfb27fccd3c27f68e9a6af1bcf48c2d82534b8675b83608a4d81446d095a17ac\"\n dependencies = [\n  \"thiserror 2.0.11\",\n- \"windows 0.59.0\",\n- \"windows-core 0.59.0\",\n- \"windows-targets 0.53.0\",\n+ \"windows 0.60.0\",\n+ \"windows-core 0.60.1\",\n ]\n \n [[package]]\n@@ -3658,12 +3652,24 @@ dependencies = [\n \n [[package]]\n name = \"windows\"\n-version = \"0.59.0\"\n+version = \"0.60.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"ddf874e74c7a99773e62b1c671427abf01a425e77c3d3fb9fb1e4883ea934529\"\n+dependencies = [\n+ \"windows-collections\",\n+ \"windows-core 0.60.1\",\n+ \"windows-future\",\n+ \"windows-link\",\n+ \"windows-numerics\",\n+]\n+\n+[[package]]\n+name = \"windows-collections\"\n+version = \"0.1.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"7f919aee0a93304be7f62e8e5027811bbba96bcb1de84d6618be56e43f8a32a1\"\n+checksum = \"5467f79cc1ba3f52ebb2ed41dbb459b8e7db636cc3429458d9a852e15bc24dec\"\n dependencies = [\n- \"windows-core 0.59.0\",\n- \"windows-targets 0.53.0\",\n+ \"windows-core 0.60.1\",\n ]\n \n [[package]]\n@@ -3681,15 +3687,25 @@ dependencies = [\n \n [[package]]\n name = \"windows-core\"\n-version = \"0.59.0\"\n+version = \"0.60.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"810ce18ed2112484b0d4e15d022e5f598113e220c53e373fb31e67e21670c1ce\"\n+checksum = \"ca21a92a9cae9bf4ccae5cf8368dce0837100ddf6e6d57936749e85f152f6247\"\n dependencies = [\n  \"windows-implement 0.59.0\",\n  \"windows-interface 0.59.0\",\n- \"windows-result 0.3.0\",\n- \"windows-strings 0.3.0\",\n- \"windows-targets 0.53.0\",\n+ \"windows-link\",\n+ \"windows-result 0.3.1\",\n+ \"windows-strings 0.3.1\",\n+]\n+\n+[[package]]\n+name = \"windows-future\"\n+version = \"0.1.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"a787db4595e7eb80239b74ce8babfb1363d8e343ab072f2ffe901400c03349f0\"\n+dependencies = [\n+ \"windows-core 0.60.1\",\n+ \"windows-link\",\n ]\n \n [[package]]\n@@ -3700,7 +3716,7 @@ checksum = \"2bbd5b46c938e506ecbce286b6628a02171d56153ba733b6c741fc627ec9579b\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -3711,7 +3727,7 @@ checksum = \"83577b051e2f49a058c308f17f273b570a6a758386fc291b5f6a934dd84e48c1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -3722,7 +3738,7 @@ checksum = \"053c4c462dc91d3b1504c6fe5a726dd15e216ba718e84a0e46a88fbe5ded3515\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -3733,7 +3749,23 @@ checksum = \"cb26fd936d991781ea39e87c3a27285081e3c0da5ca0fcbc02d368cc6f52ff01\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n+]\n+\n+[[package]]\n+name = \"windows-link\"\n+version = \"0.1.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"6dccfd733ce2b1753b03b6d3c65edf020262ea35e20ccdf3e288043e6dd620e3\"\n+\n+[[package]]\n+name = \"windows-numerics\"\n+version = \"0.1.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"005dea54e2f6499f2cee279b8f703b3cf3b5734a2d8d21867c8f44003182eeed\"\n+dependencies = [\n+ \"windows-core 0.60.1\",\n+ \"windows-link\",\n ]\n \n [[package]]\n@@ -3747,11 +3779,11 @@ dependencies = [\n \n [[package]]\n name = \"windows-result\"\n-version = \"0.3.0\"\n+version = \"0.3.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d08106ce80268c4067c0571ca55a9b4e9516518eaa1a1fe9b37ca403ae1d1a34\"\n+checksum = \"06374efe858fab7e4f881500e6e86ec8bc28f9462c47e5a9941a0142ad86b189\"\n dependencies = [\n- \"windows-targets 0.53.0\",\n+ \"windows-link\",\n ]\n \n [[package]]\n@@ -3766,11 +3798,11 @@ dependencies = [\n \n [[package]]\n name = \"windows-strings\"\n-version = \"0.3.0\"\n+version = \"0.3.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b888f919960b42ea4e11c2f408fadb55f78a9f236d5eef084103c8ce52893491\"\n+checksum = \"87fa48cc5d406560701792be122a10132491cff9d0aeb23583cc2dcafc847319\"\n dependencies = [\n- \"windows-targets 0.53.0\",\n+ \"windows-link\",\n ]\n \n [[package]]\n@@ -3839,36 +3871,20 @@ dependencies = [\n  \"windows_aarch64_gnullvm 0.52.6\",\n  \"windows_aarch64_msvc 0.52.6\",\n  \"windows_i686_gnu 0.52.6\",\n- \"windows_i686_gnullvm 0.52.6\",\n+ \"windows_i686_gnullvm\",\n  \"windows_i686_msvc 0.52.6\",\n  \"windows_x86_64_gnu 0.52.6\",\n  \"windows_x86_64_gnullvm 0.52.6\",\n  \"windows_x86_64_msvc 0.52.6\",\n ]\n \n-[[package]]\n-name = \"windows-targets\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b1e4c7e8ceaaf9cb7d7507c974735728ab453b67ef8f18febdd7c11fe59dca8b\"\n-dependencies = [\n- \"windows_aarch64_gnullvm 0.53.0\",\n- \"windows_aarch64_msvc 0.53.0\",\n- \"windows_i686_gnu 0.53.0\",\n- \"windows_i686_gnullvm 0.53.0\",\n- \"windows_i686_msvc 0.53.0\",\n- \"windows_x86_64_gnu 0.53.0\",\n- \"windows_x86_64_gnullvm 0.53.0\",\n- \"windows_x86_64_msvc 0.53.0\",\n-]\n-\n [[package]]\n name = \"windows-version\"\n-version = \"0.1.2\"\n+version = \"0.1.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c12476c23a74725c539b24eae8bfc0dac4029c39cdb561d9f23616accd4ae26d\"\n+checksum = \"7bfbcc4996dd183ff1376a20ade1242da0d2dcaff83cc76710a588d24fd4c5db\"\n dependencies = [\n- \"windows-targets 0.53.0\",\n+ \"windows-link\",\n ]\n \n [[package]]\n@@ -3889,12 +3905,6 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3\"\n \n-[[package]]\n-name = \"windows_aarch64_gnullvm\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"86b8d5f90ddd19cb4a147a5fa63ca848db3df085e25fee3cc10b39b6eebae764\"\n-\n [[package]]\n name = \"windows_aarch64_msvc\"\n version = \"0.42.2\"\n@@ -3913,12 +3923,6 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469\"\n \n-[[package]]\n-name = \"windows_aarch64_msvc\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c7651a1f62a11b8cbd5e0d42526e55f2c99886c77e007179efff86c2b137e66c\"\n-\n [[package]]\n name = \"windows_i686_gnu\"\n version = \"0.42.2\"\n@@ -3937,24 +3941,12 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b\"\n \n-[[package]]\n-name = \"windows_i686_gnu\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c1dc67659d35f387f5f6c479dc4e28f1d4bb90ddd1a5d3da2e5d97b42d6272c3\"\n-\n [[package]]\n name = \"windows_i686_gnullvm\"\n version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66\"\n \n-[[package]]\n-name = \"windows_i686_gnullvm\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"9ce6ccbdedbf6d6354471319e781c0dfef054c81fbc7cf83f338a4296c0cae11\"\n-\n [[package]]\n name = \"windows_i686_msvc\"\n version = \"0.42.2\"\n@@ -3973,12 +3965,6 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66\"\n \n-[[package]]\n-name = \"windows_i686_msvc\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"581fee95406bb13382d2f65cd4a908ca7b1e4c2f1917f143ba16efe98a589b5d\"\n-\n [[package]]\n name = \"windows_x86_64_gnu\"\n version = \"0.42.2\"\n@@ -3997,12 +3983,6 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78\"\n \n-[[package]]\n-name = \"windows_x86_64_gnu\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2e55b5ac9ea33f2fc1716d1742db15574fd6fc8dadc51caab1c16a3d3b4190ba\"\n-\n [[package]]\n name = \"windows_x86_64_gnullvm\"\n version = \"0.42.2\"\n@@ -4021,12 +4001,6 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d\"\n \n-[[package]]\n-name = \"windows_x86_64_gnullvm\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0a6e035dd0599267ce1ee132e51c27dd29437f63325753051e71dd9e42406c57\"\n-\n [[package]]\n name = \"windows_x86_64_msvc\"\n version = \"0.42.2\"\n@@ -4045,12 +4019,6 @@ version = \"0.52.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec\"\n \n-[[package]]\n-name = \"windows_x86_64_msvc\"\n-version = \"0.53.0\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"271414315aff87387382ec3d271b52d7ae78726f5d44ac98b4f4030c91880486\"\n-\n [[package]]\n name = \"winit\"\n version = \"0.30.9\"\n@@ -4135,7 +4103,7 @@ checksum = \"1e9df38ee2d2c3c5948ea468a8406ff0db0b29ae1ffde1bcf20ef305bcc95c51\"\n \n [[package]]\n name = \"wry\"\n-version = \"0.48.1\"\n+version = \"0.49.0\"\n dependencies = [\n  \"base64\",\n  \"block2 0.6.0\",\n@@ -4175,8 +4143,8 @@ dependencies = [\n  \"webkit2gtk-sys\",\n  \"webview2-com\",\n  \"wgpu\",\n- \"windows 0.59.0\",\n- \"windows-core 0.59.0\",\n+ \"windows 0.60.0\",\n+ \"windows-core 0.60.1\",\n  \"windows-version\",\n  \"winit\",\n  \"x11-dl\",\n@@ -4275,7 +4243,7 @@ checksum = \"2380878cad4ac9aac1e2435f3eb4020e8374b5f13c296cb75b4620ff8e229154\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n  \"synstructure\",\n ]\n \n@@ -4297,7 +4265,7 @@ checksum = \"fa4f8080344d4671fb4e831a13ad1e68092748387dfc4f55e356242fae12ce3e\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\n \n [[package]]\n@@ -4317,7 +4285,7 @@ checksum = \"595eed982f7d355beb85837f651fa22e90b3c044842dc7f2c2842c086f295808\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n  \"synstructure\",\n ]\n \n@@ -4340,5 +4308,5 @@ checksum = \"6eafa6dfb17584ea3e2bd6e76e0cc15ad7af12b09abdd1ca55961bed9b1063c6\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.96\",\n+ \"syn 2.0.98\",\n ]\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 9b2ab1274..a74c78142 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -68,13 +68,13 @@ gdkx11 = { version = \"0.18\", optional = true }\n percent-encoding = \"2.3\"\n \n [target.\"cfg(target_os = \\\"windows\\\")\".dependencies]\n-webview2-com = \"0.35\"\n+webview2-com = \"0.36\"\n windows-version = \"0.1\"\n-windows-core = \"0.59\"\n+windows-core = \"0.60\"\n dunce = \"1\"\n \n [target.\"cfg(target_os = \\\"windows\\\")\".dependencies.windows]\n-version = \"0.59\"\n+version = \"0.60\"\n features = [\n   \"Win32_Foundation\",\n   \"Win32_Graphics_Gdi\",\n@@ -194,7 +194,7 @@ libc = \"0.2\"\n \n [dev-dependencies]\n pollster = \"0.4.0\"\n-tao = \"0.31\"\n+tao = \"0.32\"\n wgpu = \"23\"\n winit = \"0.30\"\n getrandom = \"0.3\"\ndiff --git a/src/webview2/drag_drop.rs b/src/webview2/drag_drop.rs\nindex 67111c985..9be77d1c5 100644\n--- a/src/webview2/drag_drop.rs\n+++ b/src/webview2/drag_drop.rs\n@@ -16,9 +16,9 @@ use std::{\n };\n \n use windows::{\n-  core::implement,\n+  core::{implement, BOOL},\n   Win32::{\n-    Foundation::{BOOL, DRAGDROP_E_INVALIDHWND, HWND, LPARAM, POINT, POINTL},\n+    Foundation::{DRAGDROP_E_INVALIDHWND, HWND, LPARAM, POINT, POINTL},\n     Graphics::Gdi::ScreenToClient,\n     System::{\n       Com::{IDataObject, DVASPECT_CONTENT, FORMATETC, TYMED_HGLOBAL},\ndiff --git a/src/webview2/mod.rs b/src/webview2/mod.rs\nindex a7b3d5d3c..45c3d5a8c 100644\n--- a/src/webview2/mod.rs\n+++ b/src/webview2/mod.rs\n@@ -16,12 +16,12 @@ use once_cell::sync::Lazy;\n use raw_window_handle::{HasWindowHandle, RawWindowHandle};\n use webview2_com::{Microsoft::Web::WebView2::Win32::*, *};\n use windows::{\n-  core::{s, w, Interface, HSTRING, PCWSTR, PWSTR},\n+  core::{s, w, Interface, BOOL, HSTRING, PCWSTR, PWSTR},\n   Win32::{\n     Foundation::*,\n     Globalization::*,\n     Graphics::Gdi::*,\n-    System::{Com::*, LibraryLoader::GetModuleHandleW, WinRT::EventRegistrationToken},\n+    System::{Com::*, LibraryLoader::GetModuleHandleW},\n     UI::{Input::KeyboardAndMouse::SetFocus, Shell::*, WindowsAndMessaging::*},\n   },\n };\n@@ -33,6 +33,8 @@ use crate::{\n   WebViewAttributes, RGBA,\n };\n \n+type EventRegistrationToken = i64;\n+\n const PARENT_SUBCLASS_ID: u32 = WM_USER + 0x64;\n const PARENT_DESTROY_MESSAGE: u32 = WM_USER + 0x65;\n const MAIN_THREAD_DISPATCHER_SUBCLASS_ID: u32 = WM_USER + 0x66;\n", "instance_id": "tauri-apps__wry-1492", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to update the `webview2-com` crate from version 0.35 to 0.36, along with related dependencies like `windows` to version 0.60. It provides a structured table of the update and additional context about the PR being generated by Mend Renovate. However, it lacks critical details about the specific changes or potential breaking changes introduced by the updated crates, compatibility issues, or any specific requirements for testing or validation post-update. There are no examples or mentions of edge cases that might arise due to the update. While the goal is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The code changes primarily involve updating dependency versions in the `Cargo.toml` file and regenerating the `Cargo.lock` file with updated versions of multiple crates. The scope of changes is minimal, mostly automated, and confined to configuration files with a few minor code adjustments in the `webview2` module to align with API changes (e.g., updating type imports and removing unused imports). It does not require deep understanding of the codebase architecture, complex logic, or handling of edge cases beyond ensuring the build compiles with the new versions. The task involves basic familiarity with Rust dependency management and minimal technical concepts, such as understanding crate versioning and potential minor API adjustments. The impact on the system's architecture is negligible, as this is a routine dependency update with no significant refactoring or feature addition.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "RUSTSEC-2025-0007: *ring* is unmaintained\n\n> *ring* is unmaintained\n\n| Details             |                                                |\n| ------------------- | ---------------------------------------------- |\n| Status              | unmaintained                |\n| Package             | `ring`                      |\n| Version             | `0.17.10`                   |\n| URL                 | [https://github.com/briansmith/ring/discussions/2414](https://github.com/briansmith/ring/discussions/2414) |\n| Date                | 2025-02-20                         |\n\nThe author has announced an indefinite hiatus in its development, noting that\nany reported security vulnerabilities may go unaddressed for prolonged periods\nof time.\n\nSee [advisory page](https://rustsec.org/advisories/RUSTSEC-2025-0007.html) for additional details.\n\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex c28e41fa2..f14c9048e 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -241,6 +241,47 @@ version = \"1.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"ace50bade8e6234aa140d9a2f552bbee1db4d353f69b8217bc503490fc1a9f26\"\n \n+[[package]]\n+name = \"aws-lc-fips-sys\"\n+version = \"0.13.3\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"29003a681b2b9465c1139bfb726da452a841a8b025f35953f3bce71139f10b21\"\n+dependencies = [\n+ \"bindgen 0.69.5\",\n+ \"cc\",\n+ \"cmake\",\n+ \"dunce\",\n+ \"fs_extra\",\n+ \"paste\",\n+ \"regex\",\n+]\n+\n+[[package]]\n+name = \"aws-lc-rs\"\n+version = \"1.12.4\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"4cd755adf9707cf671e31d944a189be3deaaeee11c8bc1d669bb8022ac90fbd0\"\n+dependencies = [\n+ \"aws-lc-fips-sys\",\n+ \"aws-lc-sys\",\n+ \"paste\",\n+ \"zeroize\",\n+]\n+\n+[[package]]\n+name = \"aws-lc-sys\"\n+version = \"0.26.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"0f9dd2e03ee80ca2822dd6ea431163d2ef259f2066a4d6ccaca6d9dcb386aa43\"\n+dependencies = [\n+ \"bindgen 0.69.5\",\n+ \"cc\",\n+ \"cmake\",\n+ \"dunce\",\n+ \"fs_extra\",\n+ \"paste\",\n+]\n+\n [[package]]\n name = \"backtrace\"\n version = \"0.3.74\"\n@@ -268,6 +309,29 @@ version = \"1.6.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"8c3c1a368f70d6cf7302d78f8f7093da241fb8e8807c05cc9e51a125895a6d5b\"\n \n+[[package]]\n+name = \"bindgen\"\n+version = \"0.69.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"271383c67ccabffb7381723dea0672a673f292304fcb45c01cc648c7a8d58088\"\n+dependencies = [\n+ \"bitflags\",\n+ \"cexpr\",\n+ \"clang-sys\",\n+ \"itertools 0.10.5\",\n+ \"lazy_static\",\n+ \"lazycell\",\n+ \"log\",\n+ \"prettyplease\",\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"regex\",\n+ \"rustc-hash\",\n+ \"shlex\",\n+ \"syn 2.0.89\",\n+ \"which\",\n+]\n+\n [[package]]\n name = \"bindgen\"\n version = \"0.70.1\"\n@@ -335,7 +399,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"777cf31ea10f7eb87d46b30848f8d7acbd65cb4a25cd51bee1808ef601de0416\"\n dependencies = [\n  \"autocfg\",\n- \"bindgen\",\n+ \"bindgen 0.70.1\",\n  \"cmake\",\n  \"fs_extra\",\n  \"fslock\",\n@@ -371,6 +435,8 @@ version = \"1.2.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"fd9de9f2205d5ef3fd67e685b0df337994ddd4495e2a28d185500d0e1edfea47\"\n dependencies = [\n+ \"jobserver\",\n+ \"libc\",\n  \"shlex\",\n ]\n \n@@ -664,6 +730,12 @@ dependencies = [\n  \"syn 2.0.89\",\n ]\n \n+[[package]]\n+name = \"dunce\"\n+version = \"1.0.5\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"92773504d58c093f6de2459af4af33faa518c13451eb8f2b5698ed3d36e7c813\"\n+\n [[package]]\n name = \"ed25519\"\n version = \"2.2.3\"\n@@ -969,6 +1041,15 @@ version = \"0.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"fbf6a919d6cf397374f7dfeeea91d974c7c0a7221d0d0f4f20d859d329e53fcc\"\n \n+[[package]]\n+name = \"home\"\n+version = \"0.5.9\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"e3d1354bf6b7235cb4a0576c2619fd4ed18183f689b12b006a0ee7329eeff9a5\"\n+dependencies = [\n+ \"windows-sys 0.52.0\",\n+]\n+\n [[package]]\n name = \"hostname\"\n version = \"0.4.0\"\n@@ -1160,6 +1241,15 @@ version = \"1.0.14\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"d75a2a4b1b190afb6f5425f10f6a8f959d2ea0b9c2b1d79553551850539e4674\"\n \n+[[package]]\n+name = \"jobserver\"\n+version = \"0.1.32\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"48d1dbcbbeb6a7fec7e059840aa538bd62aaccf972c7346c4d9d2059312853d0\"\n+dependencies = [\n+ \"libc\",\n+]\n+\n [[package]]\n name = \"js-sys\"\n version = \"0.3.72\"\n@@ -1187,6 +1277,12 @@ dependencies = [\n  \"spin\",\n ]\n \n+[[package]]\n+name = \"lazycell\"\n+version = \"1.3.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"830d08ce1d1d941e6b30645f1a0eb5643013d835ce3779a5fc208261dbe10f55\"\n+\n [[package]]\n name = \"lettre\"\n version = \"0.11.13\"\n@@ -1509,6 +1605,12 @@ version = \"2.2.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"f38d5652c16fde515bb1ecef450ab0f6a219d619a7274976324d5e377f7dceba\"\n \n+[[package]]\n+name = \"paste\"\n+version = \"1.0.15\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"57c0d7b74b563b49d38dae00a0c37d4d6de9b432382b2892f0574ddcae73fd0a\"\n+\n [[package]]\n name = \"pem-rfc7468\"\n version = \"0.7.0\"\n@@ -1636,6 +1738,16 @@ dependencies = [\n  \"yansi\",\n ]\n \n+[[package]]\n+name = \"prettyplease\"\n+version = \"0.2.25\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"64d1ec885c64d0457d564db4ec299b2dae3f9c02808b8ad9c3a089c591b18033\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"syn 2.0.89\",\n+]\n+\n [[package]]\n name = \"proc-macro-error\"\n version = \"1.0.4\"\n@@ -1845,6 +1957,7 @@ version = \"0.23.18\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"9c9cc1d47e243d655ace55ed38201c19ae02c148ae56412ab8750e8f0166ab7f\"\n dependencies = [\n+ \"aws-lc-rs\",\n  \"log\",\n  \"once_cell\",\n  \"ring\",\n@@ -1878,6 +1991,7 @@ version = \"0.102.8\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"64ca1bc8749bd4cf37b5ce386cc146580777b4e8572c7b97baf22c83f444bee9\"\n dependencies = [\n+ \"aws-lc-rs\",\n  \"ring\",\n  \"rustls-pki-types\",\n  \"untrusted\",\n@@ -2460,6 +2574,18 @@ dependencies = [\n  \"rustls-pki-types\",\n ]\n \n+[[package]]\n+name = \"which\"\n+version = \"4.4.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"87ba24419a2078cd2b0f2ede2691b6c66d8e47836da3b6db8265ebad47afbfc7\"\n+dependencies = [\n+ \"either\",\n+ \"home\",\n+ \"once_cell\",\n+ \"rustix\",\n+]\n+\n [[package]]\n name = \"winapi\"\n version = \"0.3.9\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex d724fb622..4cddf4f69 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -48,7 +48,7 @@ percent-encoding = { version = \"2.3\", optional = true }\n \n ## tls\n native-tls = { version = \"0.2.9\", optional = true } # feature\n-rustls = { version = \"0.23.5\", default-features = false, features = [\"ring\", \"logging\", \"std\", \"tls12\"], optional = true }\n+rustls = { version = \"0.23.5\", default-features = false, features = [\"logging\", \"std\", \"tls12\"], optional = true }\n rustls-native-certs = { version = \"0.8\", optional = true }\n webpki-roots = { version = \"0.26\", optional = true }\n boring = { version = \"4\", optional = true }\n@@ -60,12 +60,12 @@ async-trait = { version = \"0.1\", optional = true }\n \n ## async-std\n async-std = { version = \"1.8\", optional = true }\n-futures-rustls = { version = \"0.26\", default-features = false, features = [\"logging\", \"tls12\", \"ring\"], optional = true }\n+futures-rustls = { version = \"0.26\", default-features = false, features = [\"logging\", \"tls12\"], optional = true }\n \n ## tokio\n tokio1_crate = { package = \"tokio\", version = \"1\", optional = true }\n tokio1_native_tls_crate = { package = \"tokio-native-tls\", version = \"0.3\", optional = true }\n-tokio1_rustls = { package = \"tokio-rustls\", version = \"0.26\", default-features = false, features = [\"logging\", \"tls12\", \"ring\"], optional = true }\n+tokio1_rustls = { package = \"tokio-rustls\", version = \"0.26\", default-features = false, features = [\"logging\", \"tls12\"], optional = true }\n tokio1_boring = { package = \"tokio-boring\", version = \"4\", optional = true }\n \n ## dkim\n@@ -109,16 +109,26 @@ smtp-transport = [\"dep:base64\", \"dep:nom\", \"dep:socket2\", \"dep:url\", \"dep:percen\n \n pool = [\"dep:futures-util\"]\n \n-rustls-tls = [\"dep:webpki-roots\", \"dep:rustls\"]\n+rustls = [\"dep:rustls\"]\n+aws-lc-rs = [\"rustls?/aws-lc-rs\"]\n+fips = [\"aws-lc-rs\", \"rustls?/fips\"]\n+ring = [\"rustls?/ring\"]\n+webpki-roots = [\"dep:webpki-roots\"]\n+# deprecated\n+rustls-tls = [\"webpki-roots\", \"rustls\", \"ring\"]\n \n boring-tls = [\"dep:boring\"]\n \n # async\n async-std1 = [\"dep:async-std\", \"dep:async-trait\", \"dep:futures-io\", \"dep:futures-util\"]\n-async-std1-rustls-tls = [\"async-std1\", \"rustls-tls\", \"dep:futures-rustls\"]\n+async-std1-rustls = [\"async-std1\", \"rustls\", \"dep:futures-rustls\"]\n+# deprecated\n+async-std1-rustls-tls = [\"async-std1-rustls\", \"rustls-tls\"]\n tokio1 = [\"dep:tokio1_crate\", \"dep:async-trait\", \"dep:futures-io\", \"dep:futures-util\"]\n tokio1-native-tls = [\"tokio1\", \"native-tls\", \"dep:tokio1_native_tls_crate\"]\n-tokio1-rustls-tls = [\"tokio1\", \"rustls-tls\", \"dep:tokio1_rustls\"]\n+tokio1-rustls = [\"tokio1\", \"rustls\", \"dep:tokio1_rustls\"]\n+# deprecated\n+tokio1-rustls-tls = [\"tokio1-rustls\", \"rustls-tls\"]\n tokio1-boring-tls = [\"tokio1\", \"boring-tls\", \"dep:tokio1_boring\"]\n \n dkim = [\"dep:base64\", \"dep:sha2\", \"dep:rsa\", \"dep:ed25519-dalek\"]\ndiff --git a/src/executor.rs b/src/executor.rs\nindex d698cb2af..f221d5eee 100644\n--- a/src/executor.rs\n+++ b/src/executor.rs\n@@ -133,7 +133,7 @@ impl Executor for Tokio1Executor {\n     ) -> Result<AsyncSmtpConnection, Error> {\n         #[allow(clippy::match_single_binding)]\n         let tls_parameters = match tls {\n-            #[cfg(any(feature = \"tokio1-native-tls\", feature = \"tokio1-rustls-tls\"))]\n+            #[cfg(any(feature = \"tokio1-native-tls\", feature = \"tokio1-rustls\"))]\n             Tls::Wrapper(tls_parameters) => Some(tls_parameters.clone()),\n             _ => None,\n         };\n@@ -147,7 +147,7 @@ impl Executor for Tokio1Executor {\n         )\n         .await?;\n \n-        #[cfg(any(feature = \"tokio1-native-tls\", feature = \"tokio1-rustls-tls\"))]\n+        #[cfg(any(feature = \"tokio1-native-tls\", feature = \"tokio1-rustls\"))]\n         match tls {\n             Tls::Opportunistic(tls_parameters) => {\n                 if conn.can_starttls() {\n@@ -230,7 +230,7 @@ impl Executor for AsyncStd1Executor {\n     ) -> Result<AsyncSmtpConnection, Error> {\n         #[allow(clippy::match_single_binding)]\n         let tls_parameters = match tls {\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             Tls::Wrapper(tls_parameters) => Some(tls_parameters.clone()),\n             _ => None,\n         };\n@@ -243,7 +243,7 @@ impl Executor for AsyncStd1Executor {\n         )\n         .await?;\n \n-        #[cfg(feature = \"async-std1-rustls-tls\")]\n+        #[cfg(feature = \"async-std1-rustls\")]\n         match tls {\n             Tls::Opportunistic(tls_parameters) => {\n                 if conn.can_starttls() {\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 648db8b4a..3ac796712 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -64,13 +64,46 @@\n //!\n //! #### SMTP over TLS via the rustls crate\n //!\n-//! _Secure SMTP connections using TLS from the `rustls-tls` crate_\n+//! _Secure SMTP connections using TLS from the `rustls` crate_\n //!\n-//! Rustls uses [ring] as the cryptography implementation. As a result, [not all Rust's targets are supported][ring-support].\n+//! * **rustls**: TLS support for the synchronous version of the API\n+//! * **tokio1-rustls**: TLS support for the `tokio1` async version of the API\n+//! * **async-std1-rustls**: TLS support for the `async-std1` async version of the API\n //!\n-//! * **rustls-tls**: TLS support for the synchronous version of the API\n-//! * **tokio1-rustls-tls**: TLS support for the `tokio1` async version of the API\n-//! * **async-std1-rustls-tls**: TLS support for the `async-std1` async version of the API\n+//! ##### rustls crypto backends\n+//!\n+//! _The crypto implementation to use with rustls_\n+//!\n+//! When the `rustls` feature is enabled, one of the following crypto backends MUST also\n+//! be enabled.\n+//!\n+//! * **aws-lc-rs**: use [AWS-LC] (via [`aws-lc-rs`]) as the `rustls` crypto backend\n+//! * **ring**: use [`ring`] as the `rustls` crypto backend\n+//!\n+//! When enabling `aws-lc-rs`, the `fips` feature can also be enabled to have\n+//! rustls use the FIPS certified module of AWS-LC.\n+//!\n+//! `aws-lc-rs` may require cmake on some platforms to compile.\n+//! `fips` always requires cmake and the Go compiler to compile.\n+//!\n+//! ##### rustls certificate verification backend\n+//!\n+//! _The TLS certificate verification backend to use with rustls_\n+//!\n+//! When the `rustls` feature is enabled, one of the following verification backends\n+//! MUST also be enabled.\n+//!\n+//! * **rustls-native-certs**: verify TLS certificates using the platform's native certificate store (see [`rustls-native-certs`])\n+//! * **webpki-roots**: verify TLS certificates against Mozilla's root certificates (see [`webpki-roots`])\n+//!\n+//! For the `rustls-native-certs` backend to work correctly, the following packages\n+//! will need to be installed in order for the build stage and the compiled program\n+//! to run properly.\n+//!\n+//! | Distro       | Build-time packages        | Runtime packages             |\n+//! | ------------ | -------------------------- | ---------------------------- |\n+//! | Debian       | none                       | `ca-certificates`            |\n+//! | Alpine Linux | none                       | `ca-certificates`            |\n //!\n //! ### Sendmail transport\n //!\n@@ -115,8 +148,11 @@\n //! [`ContentType`]: crate::message::header::ContentType\n //! [tokio]: https://docs.rs/tokio/1\n //! [async-std]: https://docs.rs/async-std/1\n-//! [ring]: https://github.com/briansmith/ring#ring\n-//! [ring-support]: https://github.com/briansmith/ring#online-automated-testing\n+//! [AWS-LC]: https://github.com/aws/aws-lc\n+//! [`aws-lc-rs`]: https://crates.io/crates/aws-lc-rs\n+//! [`ring`]: https://crates.io/crates/ring\n+//! [`rustls-native-certs`]: https://crates.io/crates/rustls-native-certs\n+//! [`webpki-roots`]: https://crates.io/crates/webpki-roots\n //! [Tokio 1.x]: https://docs.rs/tokio/1\n //! [async-std 1.x]: https://docs.rs/async-std/1\n //! [mime 0.3]: https://docs.rs/mime/0.3\n@@ -163,6 +199,22 @@\n \n #[cfg(not(lettre_ignore_tls_mismatch))]\n mod compiletime_checks {\n+    #[cfg(all(feature = \"rustls\", not(feature = \"aws-lc-rs\"), not(feature = \"ring\")))]\n+    compile_error!(\n+        \"feature `rustls` also requires either the `aws-lc-rs` or the `ring` feature to\n+    be enabled\"\n+    );\n+\n+    #[cfg(all(\n+        feature = \"rustls\",\n+        not(feature = \"rustls-native-certs\"),\n+        not(feature = \"webpki-roots\")\n+    ))]\n+    compile_error!(\n+        \"feature `rustls` also requires either the `rustls-native-certs` or the `webpki-roots` feature to\n+    be enabled\"\n+    );\n+\n     #[cfg(all(feature = \"native-tls\", feature = \"boring-tls\"))]\n     compile_error!(\"feature \\\"native-tls\\\" and feature \\\"boring-tls\\\" cannot be enabled at the same time, otherwise\n     the executable will fail to link.\");\n@@ -173,16 +225,12 @@ mod compiletime_checks {\n         not(feature = \"tokio1-native-tls\")\n     ))]\n     compile_error!(\"Lettre is being built with the `tokio1` and the `native-tls` features, but the `tokio1-native-tls` feature hasn't been turned on.\n-    If you were trying to opt into `rustls-tls` and did not activate `native-tls`, disable the default-features of lettre in `Cargo.toml` and manually add the required features.\n+    If you were trying to opt into `rustls` and did not activate `native-tls`, disable the default-features of lettre in `Cargo.toml` and manually add the required features.\n     Make sure to apply the same to any of your crate dependencies that use the `lettre` crate.\");\n \n-    #[cfg(all(\n-        feature = \"tokio1\",\n-        feature = \"rustls-tls\",\n-        not(feature = \"tokio1-rustls-tls\")\n-    ))]\n-    compile_error!(\"Lettre is being built with the `tokio1` and the `rustls-tls` features, but the `tokio1-rustls-tls` feature hasn't been turned on.\n-    If you'd like to use `native-tls` make sure that the `rustls-tls` feature hasn't been enabled by mistake.\n+    #[cfg(all(feature = \"tokio1\", feature = \"rustls\", not(feature = \"tokio1-rustls\")))]\n+    compile_error!(\"Lettre is being built with the `tokio1` and the `rustls` features, but the `tokio1-rustls` feature hasn't been turned on.\n+    If you'd like to use `native-tls` make sure that the `rustls` feature hasn't been enabled by mistake.\n     Make sure to apply the same to any of your crate dependencies that use the `lettre` crate.\");\n \n     #[cfg(all(\n@@ -191,22 +239,22 @@ mod compiletime_checks {\n         not(feature = \"tokio1-boring-tls\")\n     ))]\n     compile_error!(\"Lettre is being built with the `tokio1` and the `boring-tls` features, but the `tokio1-boring-tls` feature hasn't been turned on.\n-    If you'd like to use `boring-tls` make sure that the `rustls-tls` feature hasn't been enabled by mistake.\n+    If you'd like to use `boring-tls` make sure that the `rustls` feature hasn't been enabled by mistake.\n     Make sure to apply the same to any of your crate dependencies that use the `lettre` crate.\");\n \n-    #[cfg(all(feature = \"async-std1\", feature = \"native-tls\",))]\n+    #[cfg(all(feature = \"async-std1\", feature = \"native-tls\"))]\n     compile_error!(\"Lettre is being built with the `async-std1` and the `native-tls` features, but the async-std integration doesn't support native-tls yet.\n If you'd like to work on the issue please take a look at https://github.com/lettre/lettre/issues/576.\n-If you were trying to opt into `rustls-tls` and did not activate `native-tls`, disable the default-features of lettre in `Cargo.toml` and manually add the required features.\n+If you were trying to opt into `rustls` and did not activate `native-tls`, disable the default-features of lettre in `Cargo.toml` and manually add the required features.\n Make sure to apply the same to any of your crate dependencies that use the `lettre` crate.\");\n \n     #[cfg(all(\n         feature = \"async-std1\",\n-        feature = \"rustls-tls\",\n-        not(feature = \"async-std1-rustls-tls\")\n+        feature = \"rustls\",\n+        not(feature = \"async-std1-rustls\")\n     ))]\n-    compile_error!(\"Lettre is being built with the `async-std1` and the `rustls-tls` features, but the `async-std1-rustls-tls` feature hasn't been turned on.\n-If you'd like to use `native-tls` make sure that the `rustls-tls` hasn't been enabled by mistake.\n+    compile_error!(\"Lettre is being built with the `async-std1` and the `rustls` features, but the `async-std1-rustls` feature hasn't been turned on.\n+If you'd like to use `native-tls` make sure that the `rustls` hasn't been enabled by mistake.\n Make sure to apply the same to any of your crate dependencies that use the `lettre` crate.\");\n }\n \n@@ -219,6 +267,8 @@ mod executor;\n #[cfg(feature = \"builder\")]\n #[cfg_attr(docsrs, doc(cfg(feature = \"builder\")))]\n pub mod message;\n+#[cfg(feature = \"rustls\")]\n+mod rustls_crypto;\n mod time;\n pub mod transport;\n \ndiff --git a/src/rustls_crypto.rs b/src/rustls_crypto.rs\nnew file mode 100644\nindex 000000000..d2f1c4265\n--- /dev/null\n+++ b/src/rustls_crypto.rs\n@@ -0,0 +1,14 @@\n+use std::sync::Arc;\n+\n+use rustls::crypto::CryptoProvider;\n+\n+pub(crate) fn crypto_provider() -> Arc<CryptoProvider> {\n+    CryptoProvider::get_default().cloned().unwrap_or_else(|| {\n+        #[cfg(feature = \"aws-lc-rs\")]\n+        let provider = rustls::crypto::aws_lc_rs::default_provider();\n+        #[cfg(not(feature = \"aws-lc-rs\"))]\n+        let provider = rustls::crypto::ring::default_provider();\n+\n+        Arc::new(provider)\n+    })\n+}\ndiff --git a/src/transport/smtp/async_transport.rs b/src/transport/smtp/async_transport.rs\nindex 4baebd270..e2ce51a74 100644\n--- a/src/transport/smtp/async_transport.rs\n+++ b/src/transport/smtp/async_transport.rs\n@@ -14,8 +14,8 @@ use super::pool::async_impl::Pool;\n use super::PoolConfig;\n #[cfg(any(\n     feature = \"tokio1-native-tls\",\n-    feature = \"tokio1-rustls-tls\",\n-    feature = \"async-std1-rustls-tls\"\n+    feature = \"tokio1-rustls\",\n+    feature = \"async-std1-rustls\"\n ))]\n use super::Tls;\n use super::{\n@@ -111,15 +111,15 @@ where\n     /// to validate TLS certificates.\n     #[cfg(any(\n         feature = \"tokio1-native-tls\",\n-        feature = \"tokio1-rustls-tls\",\n-        feature = \"async-std1-rustls-tls\"\n+        feature = \"tokio1-rustls\",\n+        feature = \"async-std1-rustls\"\n     ))]\n     #[cfg_attr(\n         docsrs,\n         doc(cfg(any(\n             feature = \"tokio1-native-tls\",\n-            feature = \"tokio1-rustls-tls\",\n-            feature = \"async-std1-rustls-tls\"\n+            feature = \"tokio1-rustls\",\n+            feature = \"async-std1-rustls\"\n         )))\n     )]\n     pub fn relay(relay: &str) -> Result<AsyncSmtpTransportBuilder, Error> {\n@@ -145,15 +145,15 @@ where\n     /// or emails will be sent to the server, protecting from downgrade attacks.\n     #[cfg(any(\n         feature = \"tokio1-native-tls\",\n-        feature = \"tokio1-rustls-tls\",\n-        feature = \"async-std1-rustls-tls\"\n+        feature = \"tokio1-rustls\",\n+        feature = \"async-std1-rustls\"\n     ))]\n     #[cfg_attr(\n         docsrs,\n         doc(cfg(any(\n             feature = \"tokio1-native-tls\",\n-            feature = \"tokio1-rustls-tls\",\n-            feature = \"async-std1-rustls-tls\"\n+            feature = \"tokio1-rustls\",\n+            feature = \"async-std1-rustls\"\n         )))\n     )]\n     pub fn starttls_relay(relay: &str) -> Result<AsyncSmtpTransportBuilder, Error> {\n@@ -288,10 +288,10 @@ where\n     /// # Ok(())\n     /// # }\n     /// ```\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn from_url(connection_url: &str) -> Result<AsyncSmtpTransportBuilder, Error> {\n         super::connection_url::from_connection_url(connection_url)\n@@ -416,15 +416,15 @@ impl AsyncSmtpTransportBuilder {\n     /// lead to hard to debug IO errors coming from the TLS library.\n     #[cfg(any(\n         feature = \"tokio1-native-tls\",\n-        feature = \"tokio1-rustls-tls\",\n-        feature = \"async-std1-rustls-tls\"\n+        feature = \"tokio1-rustls\",\n+        feature = \"async-std1-rustls\"\n     ))]\n     #[cfg_attr(\n         docsrs,\n         doc(cfg(any(\n             feature = \"tokio1-native-tls\",\n-            feature = \"tokio1-rustls-tls\",\n-            feature = \"async-std1-rustls-tls\"\n+            feature = \"tokio1-rustls\",\n+            feature = \"async-std1-rustls\"\n         )))\n     )]\n     pub fn tls(mut self, tls: Tls) -> Self {\ndiff --git a/src/transport/smtp/client/async_connection.rs b/src/transport/smtp/client/async_connection.rs\nindex 576934936..3149d6f2b 100644\n--- a/src/transport/smtp/client/async_connection.rs\n+++ b/src/transport/smtp/client/async_connection.rs\n@@ -375,7 +375,7 @@ impl AsyncSmtpConnection {\n     }\n \n     /// The X509 certificate of the server (DER encoded)\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn peer_certificate(&self) -> Result<Vec<u8>, Error> {\n         self.stream.get_ref().peer_certificate()\n     }\n@@ -397,7 +397,7 @@ impl AsyncSmtpConnection {\n     }\n \n     /// All the X509 certificates of the chain (DER encoded)\n-    #[cfg(any(feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn certificate_chain(&self) -> Result<Vec<Vec<u8>>, Error> {\n         self.stream.get_ref().certificate_chain()\n     }\ndiff --git a/src/transport/smtp/client/async_net.rs b/src/transport/smtp/client/async_net.rs\nindex 2c0173f3e..47b66db9b 100644\n--- a/src/transport/smtp/client/async_net.rs\n+++ b/src/transport/smtp/client/async_net.rs\n@@ -12,9 +12,9 @@ use futures_io::{\n     AsyncRead as FuturesAsyncRead, AsyncWrite as FuturesAsyncWrite, Error as IoError, ErrorKind,\n     Result as IoResult,\n };\n-#[cfg(feature = \"async-std1-rustls-tls\")]\n+#[cfg(feature = \"async-std1-rustls\")]\n use futures_rustls::client::TlsStream as AsyncStd1RustlsTlsStream;\n-#[cfg(any(feature = \"tokio1-rustls-tls\", feature = \"async-std1-rustls-tls\"))]\n+#[cfg(any(feature = \"tokio1-rustls\", feature = \"async-std1-rustls\"))]\n use rustls::pki_types::ServerName;\n #[cfg(feature = \"tokio1-boring-tls\")]\n use tokio1_boring::SslStream as Tokio1SslStream;\n@@ -27,14 +27,14 @@ use tokio1_crate::net::{\n };\n #[cfg(feature = \"tokio1-native-tls\")]\n use tokio1_native_tls_crate::TlsStream as Tokio1TlsStream;\n-#[cfg(feature = \"tokio1-rustls-tls\")]\n+#[cfg(feature = \"tokio1-rustls\")]\n use tokio1_rustls::client::TlsStream as Tokio1RustlsTlsStream;\n \n #[cfg(any(\n     feature = \"tokio1-native-tls\",\n-    feature = \"tokio1-rustls-tls\",\n+    feature = \"tokio1-rustls\",\n     feature = \"tokio1-boring-tls\",\n-    feature = \"async-std1-rustls-tls\"\n+    feature = \"async-std1-rustls\"\n ))]\n use super::InnerTlsParameters;\n use super::TlsParameters;\n@@ -78,7 +78,7 @@ enum InnerAsyncNetworkStream {\n     #[cfg(feature = \"tokio1-native-tls\")]\n     Tokio1NativeTls(Tokio1TlsStream<Box<dyn AsyncTokioStream>>),\n     /// Encrypted Tokio 1.x TCP stream\n-    #[cfg(feature = \"tokio1-rustls-tls\")]\n+    #[cfg(feature = \"tokio1-rustls\")]\n     Tokio1RustlsTls(Tokio1RustlsTlsStream<Box<dyn AsyncTokioStream>>),\n     /// Encrypted Tokio 1.x TCP stream\n     #[cfg(feature = \"tokio1-boring-tls\")]\n@@ -87,7 +87,7 @@ enum InnerAsyncNetworkStream {\n     #[cfg(feature = \"async-std1\")]\n     AsyncStd1Tcp(AsyncStd1TcpStream),\n     /// Encrypted Tokio 1.x TCP stream\n-    #[cfg(feature = \"async-std1-rustls-tls\")]\n+    #[cfg(feature = \"async-std1-rustls\")]\n     AsyncStd1RustlsTls(AsyncStd1RustlsTlsStream<AsyncStd1TcpStream>),\n     /// Can't be built\n     None,\n@@ -112,13 +112,13 @@ impl AsyncNetworkStream {\n             InnerAsyncNetworkStream::Tokio1NativeTls(s) => {\n                 s.get_ref().get_ref().get_ref().peer_addr()\n             }\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(s) => s.get_ref().0.peer_addr(),\n             #[cfg(feature = \"tokio1-boring-tls\")]\n             InnerAsyncNetworkStream::Tokio1BoringTls(s) => s.get_ref().peer_addr(),\n             #[cfg(feature = \"async-std1\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(s) => s.peer_addr(),\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(s) => s.get_ref().0.peer_addr(),\n             InnerAsyncNetworkStream::None => {\n                 debug_assert!(false, \"InnerAsyncNetworkStream::None must never be built\");\n@@ -258,18 +258,18 @@ impl AsyncNetworkStream {\n                 feature = \"tokio1\",\n                 not(any(\n                     feature = \"tokio1-native-tls\",\n-                    feature = \"tokio1-rustls-tls\",\n+                    feature = \"tokio1-rustls\",\n                     feature = \"tokio1-boring-tls\"\n                 ))\n             ))]\n             InnerAsyncNetworkStream::Tokio1Tcp(_) => {\n                 let _ = tls_parameters;\n-                panic!(\"Trying to upgrade an AsyncNetworkStream without having enabled either the tokio1-native-tls or the tokio1-rustls-tls feature\");\n+                panic!(\"Trying to upgrade an AsyncNetworkStream without having enabled either the tokio1-native-tls or the tokio1-rustls feature\");\n             }\n \n             #[cfg(any(\n                 feature = \"tokio1-native-tls\",\n-                feature = \"tokio1-rustls-tls\",\n+                feature = \"tokio1-rustls\",\n                 feature = \"tokio1-boring-tls\"\n             ))]\n             InnerAsyncNetworkStream::Tokio1Tcp(_) => {\n@@ -284,13 +284,13 @@ impl AsyncNetworkStream {\n                     .map_err(error::connection)?;\n                 Ok(())\n             }\n-            #[cfg(all(feature = \"async-std1\", not(feature = \"async-std1-rustls-tls\")))]\n+            #[cfg(all(feature = \"async-std1\", not(feature = \"async-std1-rustls\")))]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(_) => {\n                 let _ = tls_parameters;\n-                panic!(\"Trying to upgrade an AsyncNetworkStream without having enabled the async-std1-rustls-tls feature\");\n+                panic!(\"Trying to upgrade an AsyncNetworkStream without having enabled the async-std1-rustls feature\");\n             }\n \n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(_) => {\n                 // get owned TcpStream\n                 let tcp_stream = mem::replace(&mut self.inner, InnerAsyncNetworkStream::None);\n@@ -310,7 +310,7 @@ impl AsyncNetworkStream {\n     #[allow(unused_variables)]\n     #[cfg(any(\n         feature = \"tokio1-native-tls\",\n-        feature = \"tokio1-rustls-tls\",\n+        feature = \"tokio1-rustls\",\n         feature = \"tokio1-boring-tls\"\n     ))]\n     async fn upgrade_tokio1_tls(\n@@ -337,12 +337,12 @@ impl AsyncNetworkStream {\n                     Ok(InnerAsyncNetworkStream::Tokio1NativeTls(stream))\n                 };\n             }\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerTlsParameters::RustlsTls(config) => {\n-                #[cfg(not(feature = \"tokio1-rustls-tls\"))]\n-                panic!(\"built without the tokio1-rustls-tls feature\");\n+                #[cfg(not(feature = \"tokio1-rustls\"))]\n+                panic!(\"built without the tokio1-rustls feature\");\n \n-                #[cfg(feature = \"tokio1-rustls-tls\")]\n+                #[cfg(feature = \"tokio1-rustls\")]\n                 return {\n                     use tokio1_rustls::TlsConnector;\n \n@@ -377,7 +377,7 @@ impl AsyncNetworkStream {\n     }\n \n     #[allow(unused_variables)]\n-    #[cfg(feature = \"async-std1-rustls-tls\")]\n+    #[cfg(feature = \"async-std1-rustls\")]\n     async fn upgrade_asyncstd1_tls(\n         tcp_stream: AsyncStd1TcpStream,\n         mut tls_parameters: TlsParameters,\n@@ -389,12 +389,12 @@ impl AsyncNetworkStream {\n             InnerTlsParameters::NativeTls(connector) => {\n                 panic!(\"native-tls isn't supported with async-std yet. See https://github.com/lettre/lettre/pull/531#issuecomment-757893531\");\n             }\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerTlsParameters::RustlsTls(config) => {\n-                #[cfg(not(feature = \"async-std1-rustls-tls\"))]\n-                panic!(\"built without the async-std1-rustls-tls feature\");\n+                #[cfg(not(feature = \"async-std1-rustls\"))]\n+                panic!(\"built without the async-std1-rustls feature\");\n \n-                #[cfg(feature = \"async-std1-rustls-tls\")]\n+                #[cfg(feature = \"async-std1-rustls\")]\n                 return {\n                     use futures_rustls::TlsConnector;\n \n@@ -422,13 +422,13 @@ impl AsyncNetworkStream {\n             InnerAsyncNetworkStream::Tokio1Tcp(_) => false,\n             #[cfg(feature = \"tokio1-native-tls\")]\n             InnerAsyncNetworkStream::Tokio1NativeTls(_) => true,\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(_) => true,\n             #[cfg(feature = \"tokio1-boring-tls\")]\n             InnerAsyncNetworkStream::Tokio1BoringTls(_) => true,\n             #[cfg(feature = \"async-std1\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(_) => false,\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(_) => true,\n             InnerAsyncNetworkStream::None => false,\n         }\n@@ -443,7 +443,7 @@ impl AsyncNetworkStream {\n             }\n             #[cfg(feature = \"tokio1-native-tls\")]\n             InnerAsyncNetworkStream::Tokio1NativeTls(_) => panic!(\"Unsupported\"),\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(_) => panic!(\"Unsupported\"),\n             #[cfg(feature = \"tokio1-boring-tls\")]\n             InnerAsyncNetworkStream::Tokio1BoringTls(stream) => {\n@@ -453,7 +453,7 @@ impl AsyncNetworkStream {\n             InnerAsyncNetworkStream::AsyncStd1Tcp(_) => {\n                 Err(error::client(\"Connection is not encrypted\"))\n             }\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(_) => panic!(\"Unsupported\"),\n             InnerAsyncNetworkStream::None => panic!(\"InnerNetworkStream::None must never be built\"),\n         }\n@@ -466,7 +466,7 @@ impl AsyncNetworkStream {\n             }\n             #[cfg(feature = \"tokio1-native-tls\")]\n             InnerAsyncNetworkStream::Tokio1NativeTls(_) => panic!(\"Unsupported\"),\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(stream) => Ok(stream\n                 .get_ref()\n                 .1\n@@ -487,7 +487,7 @@ impl AsyncNetworkStream {\n             InnerAsyncNetworkStream::AsyncStd1Tcp(_) => {\n                 Err(error::client(\"Connection is not encrypted\"))\n             }\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(stream) => Ok(stream\n                 .get_ref()\n                 .1\n@@ -514,7 +514,7 @@ impl AsyncNetworkStream {\n                 .unwrap()\n                 .to_der()\n                 .map_err(error::tls)?),\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(stream) => Ok(stream\n                 .get_ref()\n                 .1\n@@ -534,7 +534,7 @@ impl AsyncNetworkStream {\n             InnerAsyncNetworkStream::AsyncStd1Tcp(_) => {\n                 Err(error::client(\"Connection is not encrypted\"))\n             }\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(stream) => Ok(stream\n                 .get_ref()\n                 .1\n@@ -574,7 +574,7 @@ impl FuturesAsyncRead for AsyncNetworkStream {\n                     Poll::Pending => Poll::Pending,\n                 }\n             }\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(s) => {\n                 let mut b = Tokio1ReadBuf::new(buf);\n                 match Pin::new(s).poll_read(cx, &mut b) {\n@@ -594,7 +594,7 @@ impl FuturesAsyncRead for AsyncNetworkStream {\n             }\n             #[cfg(feature = \"async-std1\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(s) => Pin::new(s).poll_read(cx, buf),\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(s) => Pin::new(s).poll_read(cx, buf),\n             InnerAsyncNetworkStream::None => {\n                 debug_assert!(false, \"InnerAsyncNetworkStream::None must never be built\");\n@@ -616,13 +616,13 @@ impl FuturesAsyncWrite for AsyncNetworkStream {\n             InnerAsyncNetworkStream::Tokio1Tcp(s) => Pin::new(s).poll_write(cx, buf),\n             #[cfg(feature = \"tokio1-native-tls\")]\n             InnerAsyncNetworkStream::Tokio1NativeTls(s) => Pin::new(s).poll_write(cx, buf),\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(s) => Pin::new(s).poll_write(cx, buf),\n             #[cfg(feature = \"tokio1-boring-tls\")]\n             InnerAsyncNetworkStream::Tokio1BoringTls(s) => Pin::new(s).poll_write(cx, buf),\n             #[cfg(feature = \"async-std1\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(s) => Pin::new(s).poll_write(cx, buf),\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(s) => Pin::new(s).poll_write(cx, buf),\n             InnerAsyncNetworkStream::None => {\n                 debug_assert!(false, \"InnerAsyncNetworkStream::None must never be built\");\n@@ -637,13 +637,13 @@ impl FuturesAsyncWrite for AsyncNetworkStream {\n             InnerAsyncNetworkStream::Tokio1Tcp(s) => Pin::new(s).poll_flush(cx),\n             #[cfg(feature = \"tokio1-native-tls\")]\n             InnerAsyncNetworkStream::Tokio1NativeTls(s) => Pin::new(s).poll_flush(cx),\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(s) => Pin::new(s).poll_flush(cx),\n             #[cfg(feature = \"tokio1-boring-tls\")]\n             InnerAsyncNetworkStream::Tokio1BoringTls(s) => Pin::new(s).poll_flush(cx),\n             #[cfg(feature = \"async-std1\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(s) => Pin::new(s).poll_flush(cx),\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(s) => Pin::new(s).poll_flush(cx),\n             InnerAsyncNetworkStream::None => {\n                 debug_assert!(false, \"InnerAsyncNetworkStream::None must never be built\");\n@@ -658,13 +658,13 @@ impl FuturesAsyncWrite for AsyncNetworkStream {\n             InnerAsyncNetworkStream::Tokio1Tcp(s) => Pin::new(s).poll_shutdown(cx),\n             #[cfg(feature = \"tokio1-native-tls\")]\n             InnerAsyncNetworkStream::Tokio1NativeTls(s) => Pin::new(s).poll_shutdown(cx),\n-            #[cfg(feature = \"tokio1-rustls-tls\")]\n+            #[cfg(feature = \"tokio1-rustls\")]\n             InnerAsyncNetworkStream::Tokio1RustlsTls(s) => Pin::new(s).poll_shutdown(cx),\n             #[cfg(feature = \"tokio1-boring-tls\")]\n             InnerAsyncNetworkStream::Tokio1BoringTls(s) => Pin::new(s).poll_shutdown(cx),\n             #[cfg(feature = \"async-std1\")]\n             InnerAsyncNetworkStream::AsyncStd1Tcp(s) => Pin::new(s).poll_close(cx),\n-            #[cfg(feature = \"async-std1-rustls-tls\")]\n+            #[cfg(feature = \"async-std1-rustls\")]\n             InnerAsyncNetworkStream::AsyncStd1RustlsTls(s) => Pin::new(s).poll_close(cx),\n             InnerAsyncNetworkStream::None => {\n                 debug_assert!(false, \"InnerAsyncNetworkStream::None must never be built\");\ndiff --git a/src/transport/smtp/client/connection.rs b/src/transport/smtp/client/connection.rs\nindex 76f778f42..659d51dca 100644\n--- a/src/transport/smtp/client/connection.rs\n+++ b/src/transport/smtp/client/connection.rs\n@@ -143,7 +143,7 @@ impl SmtpConnection {\n         hello_name: &ClientId,\n     ) -> Result<(), Error> {\n         if self.server_info.supports_feature(Extension::StartTls) {\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             {\n                 try_smtp!(self.command(Starttls), self);\n                 self.stream.get_mut().upgrade_tls(tls_parameters)?;\n@@ -153,11 +153,7 @@ impl SmtpConnection {\n                 try_smtp!(self.ehlo(hello_name), self);\n                 Ok(())\n             }\n-            #[cfg(not(any(\n-                feature = \"native-tls\",\n-                feature = \"rustls-tls\",\n-                feature = \"boring-tls\"\n-            )))]\n+            #[cfg(not(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))]\n             // This should never happen as `Tls` can only be created\n             // when a TLS library is enabled\n             unreachable!(\"TLS support required but not supported\");\n@@ -303,7 +299,7 @@ impl SmtpConnection {\n     }\n \n     /// The X509 certificate of the server (DER encoded)\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn peer_certificate(&self) -> Result<Vec<u8>, Error> {\n         self.stream.get_ref().peer_certificate()\n     }\n@@ -325,7 +321,7 @@ impl SmtpConnection {\n     }\n \n     /// All the X509 certificates of the chain (DER encoded)\n-    #[cfg(any(feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn certificate_chain(&self) -> Result<Vec<Vec<u8>>, Error> {\n         self.stream.get_ref().certificate_chain()\n     }\ndiff --git a/src/transport/smtp/client/mod.rs b/src/transport/smtp/client/mod.rs\nindex df2f742dc..d393157ac 100644\n--- a/src/transport/smtp/client/mod.rs\n+++ b/src/transport/smtp/client/mod.rs\n@@ -33,9 +33,9 @@ pub use self::async_net::AsyncNetworkStream;\n #[cfg(feature = \"tokio1\")]\n pub use self::async_net::AsyncTokioStream;\n use self::net::NetworkStream;\n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n pub(super) use self::tls::InnerTlsParameters;\n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n pub use self::tls::TlsVersion;\n pub use self::{\n     connection::SmtpConnection,\ndiff --git a/src/transport/smtp/client/net.rs b/src/transport/smtp/client/net.rs\nindex 6be0a992c..dd4e431c5 100644\n--- a/src/transport/smtp/client/net.rs\n+++ b/src/transport/smtp/client/net.rs\n@@ -1,4 +1,4 @@\n-#[cfg(feature = \"rustls-tls\")]\n+#[cfg(feature = \"rustls\")]\n use std::sync::Arc;\n use std::{\n     io::{self, Read, Write},\n@@ -11,11 +11,11 @@ use std::{\n use boring::ssl::SslStream;\n #[cfg(feature = \"native-tls\")]\n use native_tls::TlsStream;\n-#[cfg(feature = \"rustls-tls\")]\n+#[cfg(feature = \"rustls\")]\n use rustls::{pki_types::ServerName, ClientConnection, StreamOwned};\n use socket2::{Domain, Protocol, Type};\n \n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n use super::InnerTlsParameters;\n use super::TlsParameters;\n use crate::transport::smtp::{error, Error};\n@@ -36,7 +36,7 @@ enum InnerNetworkStream {\n     #[cfg(feature = \"native-tls\")]\n     NativeTls(TlsStream<TcpStream>),\n     /// Encrypted TCP stream\n-    #[cfg(feature = \"rustls-tls\")]\n+    #[cfg(feature = \"rustls\")]\n     RustlsTls(StreamOwned<ClientConnection, TcpStream>),\n     #[cfg(feature = \"boring-tls\")]\n     BoringTls(SslStream<TcpStream>),\n@@ -59,7 +59,7 @@ impl NetworkStream {\n             InnerNetworkStream::Tcp(s) => s.peer_addr(),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(s) => s.get_ref().peer_addr(),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(s) => s.get_ref().peer_addr(),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(s) => s.get_ref().peer_addr(),\n@@ -79,7 +79,7 @@ impl NetworkStream {\n             InnerNetworkStream::Tcp(s) => s.shutdown(how),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(s) => s.get_ref().shutdown(how),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(s) => s.get_ref().shutdown(how),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(s) => s.get_ref().shutdown(how),\n@@ -146,17 +146,13 @@ impl NetworkStream {\n \n     pub fn upgrade_tls(&mut self, tls_parameters: &TlsParameters) -> Result<(), Error> {\n         match &self.inner {\n-            #[cfg(not(any(\n-                feature = \"native-tls\",\n-                feature = \"rustls-tls\",\n-                feature = \"boring-tls\"\n-            )))]\n+            #[cfg(not(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))]\n             InnerNetworkStream::Tcp(_) => {\n                 let _ = tls_parameters;\n-                panic!(\"Trying to upgrade an NetworkStream without having enabled either the native-tls or the rustls-tls feature\");\n+                panic!(\"Trying to upgrade an NetworkStream without having enabled either the `native-tls` or the `rustls` feature\");\n             }\n \n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             InnerNetworkStream::Tcp(_) => {\n                 // get owned TcpStream\n                 let tcp_stream = mem::replace(&mut self.inner, InnerNetworkStream::None);\n@@ -171,7 +167,7 @@ impl NetworkStream {\n         }\n     }\n \n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     fn upgrade_tls_impl(\n         tcp_stream: TcpStream,\n         tls_parameters: &TlsParameters,\n@@ -184,7 +180,7 @@ impl NetworkStream {\n                     .map_err(error::connection)?;\n                 InnerNetworkStream::NativeTls(stream)\n             }\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerTlsParameters::RustlsTls(connector) => {\n                 let domain = ServerName::try_from(tls_parameters.domain())\n                     .map_err(|_| error::connection(\"domain isn't a valid DNS name\"))?;\n@@ -211,7 +207,7 @@ impl NetworkStream {\n             InnerNetworkStream::Tcp(_) => false,\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(_) => true,\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(_) => true,\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(_) => true,\n@@ -228,7 +224,7 @@ impl NetworkStream {\n             InnerNetworkStream::Tcp(_) => Err(error::client(\"Connection is not encrypted\")),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(_) => panic!(\"Unsupported\"),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(_) => panic!(\"Unsupported\"),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(stream) => {\n@@ -238,13 +234,13 @@ impl NetworkStream {\n         }\n     }\n \n-    #[cfg(any(feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn certificate_chain(&self) -> Result<Vec<Vec<u8>>, Error> {\n         match &self.inner {\n             InnerNetworkStream::Tcp(_) => Err(error::client(\"Connection is not encrypted\")),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(_) => panic!(\"Unsupported\"),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(stream) => Ok(stream\n                 .conn\n                 .peer_certificates()\n@@ -264,7 +260,7 @@ impl NetworkStream {\n         }\n     }\n \n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn peer_certificate(&self) -> Result<Vec<u8>, Error> {\n         match &self.inner {\n             InnerNetworkStream::Tcp(_) => Err(error::client(\"Connection is not encrypted\")),\n@@ -275,7 +271,7 @@ impl NetworkStream {\n                 .unwrap()\n                 .to_der()\n                 .map_err(error::tls)?),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(stream) => Ok(stream\n                 .conn\n                 .peer_certificates()\n@@ -299,7 +295,7 @@ impl NetworkStream {\n             InnerNetworkStream::Tcp(stream) => stream.set_read_timeout(duration),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(stream) => stream.get_ref().set_read_timeout(duration),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(stream) => stream.get_ref().set_read_timeout(duration),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(stream) => stream.get_ref().set_read_timeout(duration),\n@@ -317,7 +313,7 @@ impl NetworkStream {\n \n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(stream) => stream.get_ref().set_write_timeout(duration),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(stream) => stream.get_ref().set_write_timeout(duration),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(stream) => stream.get_ref().set_write_timeout(duration),\n@@ -335,7 +331,7 @@ impl Read for NetworkStream {\n             InnerNetworkStream::Tcp(s) => s.read(buf),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(s) => s.read(buf),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(s) => s.read(buf),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(s) => s.read(buf),\n@@ -353,7 +349,7 @@ impl Write for NetworkStream {\n             InnerNetworkStream::Tcp(s) => s.write(buf),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(s) => s.write(buf),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(s) => s.write(buf),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(s) => s.write(buf),\n@@ -369,7 +365,7 @@ impl Write for NetworkStream {\n             InnerNetworkStream::Tcp(s) => s.flush(),\n             #[cfg(feature = \"native-tls\")]\n             InnerNetworkStream::NativeTls(s) => s.flush(),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             InnerNetworkStream::RustlsTls(s) => s.flush(),\n             #[cfg(feature = \"boring-tls\")]\n             InnerNetworkStream::BoringTls(s) => s.flush(),\ndiff --git a/src/transport/smtp/client/tls.rs b/src/transport/smtp/client/tls.rs\nindex df407e505..513f4ff5b 100644\n--- a/src/transport/smtp/client/tls.rs\n+++ b/src/transport/smtp/client/tls.rs\n@@ -1,5 +1,5 @@\n use std::fmt::{self, Debug};\n-#[cfg(feature = \"rustls-tls\")]\n+#[cfg(feature = \"rustls\")]\n use std::sync::Arc;\n \n #[cfg(feature = \"boring-tls\")]\n@@ -10,7 +10,7 @@ use boring::{\n };\n #[cfg(feature = \"native-tls\")]\n use native_tls::{Protocol, TlsConnector};\n-#[cfg(feature = \"rustls-tls\")]\n+#[cfg(feature = \"rustls\")]\n use rustls::{\n     client::danger::{HandshakeSignatureValid, ServerCertVerified, ServerCertVerifier},\n     crypto::{verify_tls12_signature, verify_tls13_signature, CryptoProvider},\n@@ -19,13 +19,13 @@ use rustls::{\n     ClientConfig, DigitallySignedStruct, Error as TlsError, RootCertStore, SignatureScheme,\n };\n \n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n use crate::transport::smtp::{error, Error};\n \n /// TLS protocol versions.\n #[derive(Debug, Copy, Clone)]\n #[non_exhaustive]\n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n pub enum TlsVersion {\n     /// TLS 1.0\n     ///\n@@ -85,10 +85,10 @@ pub enum Tls {\n     ///\n     /// Warning: A malicious intermediary could intercept the `STARTTLS` flag,\n     /// causing lettre to believe the server only supports plaintext connections.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     Opportunistic(TlsParameters),\n     /// Begin with a plaintext connection and require `STARTTLS` for security.\n@@ -100,10 +100,10 @@ pub enum Tls {\n     /// Unlike [`Tls::Opportunistic`], this option is secure against MITM attacks.\n     /// For optimal security and performance, consider using [`Tls::Wrapper`] instead,\n     /// as it requires fewer roundtrips to establish a secure connection.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     Required(TlsParameters),\n     /// Establish a connection wrapped in TLS from the start.\n@@ -113,10 +113,10 @@ pub enum Tls {\n     /// transmitting any sensitive data.\n     ///\n     /// This is the fastest and most secure option for establishing a connection.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     Wrapper(TlsParameters),\n }\n@@ -125,11 +125,11 @@ impl Debug for Tls {\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n         match &self {\n             Self::None => f.pad(\"None\"),\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             Self::Opportunistic(_) => f.pad(\"Opportunistic\"),\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             Self::Required(_) => f.pad(\"Required\"),\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             Self::Wrapper(_) => f.pad(\"Wrapper\"),\n         }\n     }\n@@ -153,7 +153,7 @@ pub enum CertificateStore {\n     /// Use a hardcoded set of Mozilla roots via the `webpki-roots` crate.\n     ///\n     /// This option is only available in the rustls backend.\n-    #[cfg(feature = \"rustls-tls\")]\n+    #[cfg(all(feature = \"rustls\", feature = \"webpki-roots\"))]\n     WebpkiRoots,\n     /// Don't use any system certificates.\n     None,\n@@ -178,7 +178,7 @@ pub struct TlsParametersBuilder {\n     identity: Option<Identity>,\n     accept_invalid_hostnames: bool,\n     accept_invalid_certs: bool,\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     min_tls_version: TlsVersion,\n }\n \n@@ -192,7 +192,7 @@ impl TlsParametersBuilder {\n             identity: None,\n             accept_invalid_hostnames: false,\n             accept_invalid_certs: false,\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             min_tls_version: TlsVersion::Tlsv12,\n         }\n     }\n@@ -230,10 +230,10 @@ impl TlsParametersBuilder {\n     /// including those from other sites, are trusted.\n     ///\n     /// This method introduces significant vulnerabilities to man-in-the-middle attacks.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn dangerous_accept_invalid_hostnames(mut self, accept_invalid_hostnames: bool) -> Self {\n         self.accept_invalid_hostnames = accept_invalid_hostnames;\n@@ -243,7 +243,7 @@ impl TlsParametersBuilder {\n     /// Controls which minimum TLS version is allowed\n     ///\n     /// Defaults to [`Tlsv12`][TlsVersion::Tlsv12].\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     pub fn set_min_tls_version(mut self, min_tls_version: TlsVersion) -> Self {\n         self.min_tls_version = min_tls_version;\n         self\n@@ -272,17 +272,17 @@ impl TlsParametersBuilder {\n \n     /// Creates a new `TlsParameters` using native-tls, boring-tls or rustls\n     /// depending on which one is available\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn build(self) -> Result<TlsParameters, Error> {\n-        #[cfg(feature = \"rustls-tls\")]\n+        #[cfg(feature = \"rustls\")]\n         return self.build_rustls();\n-        #[cfg(all(not(feature = \"rustls-tls\"), feature = \"native-tls\"))]\n+        #[cfg(all(not(feature = \"rustls\"), feature = \"native-tls\"))]\n         return self.build_native();\n-        #[cfg(all(not(feature = \"rustls-tls\"), feature = \"boring-tls\"))]\n+        #[cfg(all(not(feature = \"rustls\"), feature = \"boring-tls\"))]\n         return self.build_boring();\n     }\n \n@@ -396,8 +396,8 @@ impl TlsParametersBuilder {\n     }\n \n     /// Creates a new `TlsParameters` using rustls with the provided configuration\n-    #[cfg(feature = \"rustls-tls\")]\n-    #[cfg_attr(docsrs, doc(cfg(feature = \"rustls-tls\")))]\n+    #[cfg(feature = \"rustls\")]\n+    #[cfg_attr(docsrs, doc(cfg(feature = \"rustls\")))]\n     pub fn build_rustls(self) -> Result<TlsParameters, Error> {\n         let just_version3 = &[&rustls::version::TLS13];\n         let supported_versions = match self.min_tls_version {\n@@ -411,9 +411,7 @@ impl TlsParametersBuilder {\n             TlsVersion::Tlsv13 => just_version3,\n         };\n \n-        let crypto_provider = CryptoProvider::get_default()\n-            .cloned()\n-            .unwrap_or_else(|| Arc::new(rustls::crypto::ring::default_provider()));\n+        let crypto_provider = crate::rustls_crypto::crypto_provider();\n         let tls = ClientConfig::builder_with_provider(Arc::clone(&crypto_provider))\n             .with_protocol_versions(supported_versions)\n             .map_err(error::tls)?;\n@@ -436,7 +434,7 @@ impl TlsParametersBuilder {\n             let _ = (errors_len, added, ignored);\n         }\n \n-        #[cfg(feature = \"rustls-tls\")]\n+        #[cfg(all(feature = \"rustls\", feature = \"webpki-roots\"))]\n         fn load_webpki_roots(store: &mut RootCertStore) {\n             store.extend(webpki_roots::TLS_SERVER_ROOTS.iter().cloned());\n         }\n@@ -445,10 +443,10 @@ impl TlsParametersBuilder {\n             CertificateStore::Default => {\n                 #[cfg(feature = \"rustls-native-certs\")]\n                 load_native_roots(&mut root_cert_store);\n-                #[cfg(not(feature = \"rustls-native-certs\"))]\n+                #[cfg(all(not(feature = \"rustls-native-certs\"), feature = \"webpki-roots\"))]\n                 load_webpki_roots(&mut root_cert_store);\n             }\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(all(feature = \"rustls\", feature = \"webpki-roots\"))]\n             CertificateStore::WebpkiRoots => {\n                 load_webpki_roots(&mut root_cert_store);\n             }\n@@ -495,7 +493,7 @@ impl TlsParametersBuilder {\n pub enum InnerTlsParameters {\n     #[cfg(feature = \"native-tls\")]\n     NativeTls(TlsConnector),\n-    #[cfg(feature = \"rustls-tls\")]\n+    #[cfg(feature = \"rustls\")]\n     RustlsTls(Arc<ClientConfig>),\n     #[cfg(feature = \"boring-tls\")]\n     BoringTls(SslConnector),\n@@ -504,10 +502,10 @@ pub enum InnerTlsParameters {\n impl TlsParameters {\n     /// Creates a new `TlsParameters` using native-tls or rustls\n     /// depending on which one is available\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn new(domain: String) -> Result<Self, Error> {\n         TlsParametersBuilder::new(domain).build()\n@@ -526,8 +524,8 @@ impl TlsParameters {\n     }\n \n     /// Creates a new `TlsParameters` using rustls\n-    #[cfg(feature = \"rustls-tls\")]\n-    #[cfg_attr(docsrs, doc(cfg(feature = \"rustls-tls\")))]\n+    #[cfg(feature = \"rustls\")]\n+    #[cfg_attr(docsrs, doc(cfg(feature = \"rustls\")))]\n     pub fn new_rustls(domain: String) -> Result<Self, Error> {\n         TlsParametersBuilder::new(domain).build_rustls()\n     }\n@@ -550,13 +548,13 @@ impl TlsParameters {\n pub struct Certificate {\n     #[cfg(feature = \"native-tls\")]\n     native_tls: native_tls::Certificate,\n-    #[cfg(feature = \"rustls-tls\")]\n+    #[cfg(feature = \"rustls\")]\n     rustls: Vec<CertificateDer<'static>>,\n     #[cfg(feature = \"boring-tls\")]\n     boring_tls: boring::x509::X509,\n }\n \n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n impl Certificate {\n     /// Create a `Certificate` from a DER encoded certificate\n     pub fn from_der(der: Vec<u8>) -> Result<Self, Error> {\n@@ -569,7 +567,7 @@ impl Certificate {\n         Ok(Self {\n             #[cfg(feature = \"native-tls\")]\n             native_tls: native_tls_cert,\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             rustls: vec![der.into()],\n             #[cfg(feature = \"boring-tls\")]\n             boring_tls: boring_tls_cert,\n@@ -584,7 +582,7 @@ impl Certificate {\n         #[cfg(feature = \"boring-tls\")]\n         let boring_tls_cert = boring::x509::X509::from_pem(pem).map_err(error::tls)?;\n \n-        #[cfg(feature = \"rustls-tls\")]\n+        #[cfg(feature = \"rustls\")]\n         let rustls_cert = {\n             CertificateDer::pem_slice_iter(pem)\n                 .collect::<Result<Vec<_>, pki_types::pem::Error>>()\n@@ -594,7 +592,7 @@ impl Certificate {\n         Ok(Self {\n             #[cfg(feature = \"native-tls\")]\n             native_tls: native_tls_cert,\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             rustls: rustls_cert,\n             #[cfg(feature = \"boring-tls\")]\n             boring_tls: boring_tls_cert,\n@@ -613,7 +611,7 @@ impl Debug for Certificate {\n pub struct Identity {\n     #[cfg(feature = \"native-tls\")]\n     native_tls: native_tls::Identity,\n-    #[cfg(feature = \"rustls-tls\")]\n+    #[cfg(feature = \"rustls\")]\n     rustls_tls: (Vec<CertificateDer<'static>>, PrivateKeyDer<'static>),\n     #[cfg(feature = \"boring-tls\")]\n     boring_tls: (boring::x509::X509, PKey<boring::pkey::Private>),\n@@ -630,7 +628,7 @@ impl Clone for Identity {\n         Identity {\n             #[cfg(feature = \"native-tls\")]\n             native_tls: self.native_tls.clone(),\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             rustls_tls: (self.rustls_tls.0.clone(), self.rustls_tls.1.clone_key()),\n             #[cfg(feature = \"boring-tls\")]\n             boring_tls: (self.boring_tls.0.clone(), self.boring_tls.1.clone()),\n@@ -638,13 +636,13 @@ impl Clone for Identity {\n     }\n }\n \n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n impl Identity {\n     pub fn from_pem(pem: &[u8], key: &[u8]) -> Result<Self, Error> {\n         Ok(Self {\n             #[cfg(feature = \"native-tls\")]\n             native_tls: Identity::from_pem_native_tls(pem, key)?,\n-            #[cfg(feature = \"rustls-tls\")]\n+            #[cfg(feature = \"rustls\")]\n             rustls_tls: Identity::from_pem_rustls_tls(pem, key)?,\n             #[cfg(feature = \"boring-tls\")]\n             boring_tls: Identity::from_pem_boring_tls(pem, key)?,\n@@ -656,7 +654,7 @@ impl Identity {\n         native_tls::Identity::from_pkcs8(pem, key).map_err(error::tls)\n     }\n \n-    #[cfg(feature = \"rustls-tls\")]\n+    #[cfg(feature = \"rustls\")]\n     fn from_pem_rustls_tls(\n         pem: &[u8],\n         key: &[u8],\n@@ -683,7 +681,7 @@ impl Identity {\n     }\n }\n \n-#[cfg(feature = \"rustls-tls\")]\n+#[cfg(feature = \"rustls\")]\n #[derive(Debug)]\n struct InvalidCertsVerifier {\n     ignore_invalid_hostnames: bool,\n@@ -692,7 +690,7 @@ struct InvalidCertsVerifier {\n     crypto_provider: Arc<CryptoProvider>,\n }\n \n-#[cfg(feature = \"rustls-tls\")]\n+#[cfg(feature = \"rustls\")]\n impl ServerCertVerifier for InvalidCertsVerifier {\n     fn verify_server_cert(\n         &self,\ndiff --git a/src/transport/smtp/connection_url.rs b/src/transport/smtp/connection_url.rs\nindex 8084b0553..eb2b121a3 100644\n--- a/src/transport/smtp/connection_url.rs\n+++ b/src/transport/smtp/connection_url.rs\n@@ -2,7 +2,7 @@ use std::borrow::Cow;\n \n use url::Url;\n \n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n use super::client::{Tls, TlsParameters};\n #[cfg(any(feature = \"tokio1\", feature = \"async-std1\"))]\n use super::AsyncSmtpTransportBuilder;\n@@ -82,19 +82,19 @@ pub(crate) fn from_connection_url<B: TransportBuilder>(connection_url: &str) ->\n         (\"smtp\", None) => {\n             builder = builder.port(connection_url.port().unwrap_or(SMTP_PORT));\n         }\n-        #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+        #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n         (\"smtp\", Some(\"required\")) => {\n             builder = builder\n                 .port(connection_url.port().unwrap_or(SUBMISSION_PORT))\n                 .tls(Tls::Required(TlsParameters::new(host.into())?));\n         }\n-        #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+        #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n         (\"smtp\", Some(\"opportunistic\")) => {\n             builder = builder\n                 .port(connection_url.port().unwrap_or(SUBMISSION_PORT))\n                 .tls(Tls::Opportunistic(TlsParameters::new(host.into())?));\n         }\n-        #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+        #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n         (\"smtps\", _) => {\n             builder = builder\n                 .port(connection_url.port().unwrap_or(SUBMISSIONS_PORT))\ndiff --git a/src/transport/smtp/error.rs b/src/transport/smtp/error.rs\nindex ca59cdcd7..4131fb0f9 100644\n--- a/src/transport/smtp/error.rs\n+++ b/src/transport/smtp/error.rs\n@@ -68,10 +68,10 @@ impl Error {\n     }\n \n     /// Returns true if the error is from TLS\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn is_tls(&self) -> bool {\n         matches!(self.inner.kind, Kind::Tls)\n@@ -107,9 +107,9 @@ pub(crate) enum Kind {\n     /// TLS error\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     Tls,\n }\n \n@@ -134,7 +134,7 @@ impl fmt::Display for Error {\n             Kind::Client => f.write_str(\"internal client error\")?,\n             Kind::Network => f.write_str(\"network error\")?,\n             Kind::Connection => f.write_str(\"Connection error\")?,\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             Kind::Tls => f.write_str(\"tls error\")?,\n             Kind::Transient(code) => {\n                 write!(f, \"transient error ({code})\")?;\n@@ -185,7 +185,7 @@ pub(crate) fn connection<E: Into<BoxError>>(e: E) -> Error {\n     Error::new(Kind::Connection, Some(e))\n }\n \n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n pub(crate) fn tls<E: Into<BoxError>>(e: E) -> Error {\n     Error::new(Kind::Tls, Some(e))\n }\ndiff --git a/src/transport/smtp/mod.rs b/src/transport/smtp/mod.rs\nindex 563fc5bd6..20373145c 100644\n--- a/src/transport/smtp/mod.rs\n+++ b/src/transport/smtp/mod.rs\n@@ -32,7 +32,7 @@\n //! do the following:\n //!\n //! ```rust,no_run\n-//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls-tls\")))]\n+//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls\")))]\n //! # fn test() -> Result<(), Box<dyn std::error::Error>> {\n //! use lettre::{\n //!     message::header::ContentType,\n@@ -73,7 +73,7 @@\n //! For more information take a look at [`SmtpTransport::from_url`] or [`AsyncSmtpTransport::from_url`].\n //!\n //! ```rust,no_run\n-//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls-tls\")))]\n+//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls\")))]\n //! # fn test() -> Result<(), Box<dyn std::error::Error>> {\n //! use lettre::{\n //!     message::header::ContentType,\n@@ -101,7 +101,7 @@\n //! #### Advanced configuration with custom TLS settings\n //!\n //! ```rust,no_run\n-//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls-tls\")))]\n+//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls\")))]\n //! # fn test() -> Result<(), Box<dyn std::error::Error>> {\n //! use std::fs;\n //!\n@@ -146,7 +146,7 @@\n //! In a webserver context it may go about this:\n //!\n //! ```rust,no_run\n-//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls-tls\")))]\n+//! # #[cfg(all(feature = \"builder\", any(feature = \"native-tls\", feature = \"rustls\")))]\n //! # fn test() {\n //! use lettre::{\n //!     message::header::ContentType,\n@@ -199,7 +199,7 @@ pub use self::{\n     error::Error,\n     transport::{SmtpTransport, SmtpTransportBuilder},\n };\n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n use crate::transport::smtp::client::TlsParameters;\n use crate::transport::smtp::{\n     authentication::{Credentials, Mechanism, DEFAULT_MECHANISMS},\ndiff --git a/src/transport/smtp/transport.rs b/src/transport/smtp/transport.rs\nindex edf039abb..a6c92bf27 100644\n--- a/src/transport/smtp/transport.rs\n+++ b/src/transport/smtp/transport.rs\n@@ -7,7 +7,7 @@ use super::pool::sync_impl::Pool;\n #[cfg(feature = \"pool\")]\n use super::PoolConfig;\n use super::{ClientId, Credentials, Error, Mechanism, Response, SmtpConnection, SmtpInfo};\n-#[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+#[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n use super::{Tls, TlsParameters, SUBMISSIONS_PORT, SUBMISSION_PORT};\n use crate::{address::Envelope, Transport};\n \n@@ -77,10 +77,10 @@ impl SmtpTransport {\n     ///\n     /// Creates an encrypted transport over submissions port, using the provided domain\n     /// to validate TLS certificates.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn relay(relay: &str) -> Result<SmtpTransportBuilder, Error> {\n         let tls_parameters = TlsParameters::new(relay.into())?;\n@@ -101,10 +101,10 @@ impl SmtpTransport {\n     ///\n     /// An error is returned if the connection can't be upgraded. No credentials\n     /// or emails will be sent to the server, protecting from downgrade attacks.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn starttls_relay(relay: &str) -> Result<SmtpTransportBuilder, Error> {\n         let tls_parameters = TlsParameters::new(relay.into())?;\n@@ -230,10 +230,10 @@ impl SmtpTransport {\n     /// # Ok(())\n     /// # }\n     /// ```\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn from_url(connection_url: &str) -> Result<SmtpTransportBuilder, Error> {\n         super::connection_url::from_connection_url(connection_url)\n@@ -333,10 +333,10 @@ impl SmtpTransportBuilder {\n     ///\n     /// Using the wrong [`Tls`] and [`Self::port`] combination may\n     /// lead to hard to debug IO errors coming from the TLS library.\n-    #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+    #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n     #[cfg_attr(\n         docsrs,\n-        doc(cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\")))\n+        doc(cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\")))\n     )]\n     pub fn tls(mut self, tls: Tls) -> Self {\n         self.info.tls = tls;\n@@ -380,7 +380,7 @@ impl SmtpClient {\n     pub fn connection(&self) -> Result<SmtpConnection, Error> {\n         #[allow(clippy::match_single_binding)]\n         let tls_parameters = match &self.info.tls {\n-            #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+            #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n             Tls::Wrapper(tls_parameters) => Some(tls_parameters),\n             _ => None,\n         };\n@@ -394,7 +394,7 @@ impl SmtpClient {\n             None,\n         )?;\n \n-        #[cfg(any(feature = \"native-tls\", feature = \"rustls-tls\", feature = \"boring-tls\"))]\n+        #[cfg(any(feature = \"native-tls\", feature = \"rustls\", feature = \"boring-tls\"))]\n         match &self.info.tls {\n             Tls::Opportunistic(tls_parameters) => {\n                 if conn.can_starttls() {\n", "instance_id": "lettre__lettre-1054", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `ring` crate, used as a cryptographic backend for `rustls`, is unmaintained as of RUSTSEC-2025-0007, which poses a security risk due to potential unaddressed vulnerabilities. The goal is implicitly to address this by updating or replacing the dependency. However, the statement lacks explicit guidance on the expected solution (e.g., whether to replace `ring` with another backend like `aws-lc-rs` or to update configurations). It also does not specify constraints or requirements for compatibility with existing features. While the advisory link provides additional context, the problem statement itself misses critical details on the desired outcome or potential trade-offs (e.g., performance or platform compatibility). The code changes provided do clarify the intent to move away from `ring` by introducing `aws-lc-rs` as an alternative backend, but this is not explicitly stated in the problem description. Hence, it is rated as \"Mostly Clear\" with minor details missing.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is significant, affecting multiple files including `Cargo.toml`, `Cargo.lock`, and various source files in the `lettre` crate (a Rust library for email handling). The changes involve updating dependencies, modifying feature flags, and introducing a new cryptographic backend (`aws-lc-rs`) for `rustls`, which requires understanding Rust's dependency management and feature system. Second, the technical concepts involved include Rust's TLS implementations, conditional compilation with feature flags, and cryptographic backends, which demand a moderate level of expertise in Rust and security libraries. Third, the changes impact the system's architecture by altering how TLS is handled across synchronous and asynchronous APIs, requiring careful consideration of compatibility (e.g., ensuring `aws-lc-rs` works across platforms and with FIPS compliance where needed). However, the problem does not appear to involve complex edge cases or performance optimizations beyond ensuring the new backend functions correctly, and the provided code changes are mostly straightforward updates to feature names and dependency configurations. There is also a need to update documentation and compile-time checks to reflect the new backend options, adding to the workload but not significantly increasing complexity. Overall, this rates as a medium difficulty task (0.55), requiring understanding of multiple concepts and careful modifications across several files, but not reaching the level of deep architectural refactoring or advanced domain-specific challenges.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug]: atuin does not compile on windows since daemon was added\n### What did you expect to happen?\n\nWhen compiling atuin on Windows using `cargo build --no-default-features --features=client,sync,clipboard,check-update,server` the proccess completes and I get a fresh and shiny `atuin.exe`.\n\n### What happened?\n\nWhen compiling atuin on Windows using `cargo build --no-default-features --features=client,sync,clipboard,check-update,server` the build fails with errors:\r\n\r\n```\r\n$ cargo build --no-default-features --features=client,sync,clipboard,check-update,server\r\n   Compiling atuin-client v18.2.0 (D:\\workspaces\\atuin\\crates\\atuin-client)\r\n   Compiling atuin v18.2.0 (D:\\workspaces\\atuin\\crates\\atuin)\r\n   Compiling atuin-history v0.1.0 (D:\\workspaces\\atuin\\crates\\atuin-history)\r\n   Compiling atuin-dotfiles v0.2.0 (D:\\workspaces\\atuin\\crates\\atuin-dotfiles)\r\n   Compiling atuin-daemon v0.1.0 (D:\\workspaces\\atuin\\crates\\atuin-daemon)\r\nerror[E0432]: unresolved import `tokio::net::UnixStream`\r\n  --> crates\\atuin-daemon\\src\\client.rs:2:5\r\n   |\r\n2  | use tokio::net::UnixStream;\r\n   |     ^^^^^^^^^^^^^^^^^^^^^^ no `UnixStream` in `net`\r\n   |\r\nnote: found an item that was configured out\r\n  --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-1.36.0\\src\\net\\mod.rs:51:27\r\n   |\r\n51 |     pub use unix::stream::UnixStream;\r\n   |                           ^^^^^^^^^^\r\n\r\nerror[E0432]: unresolved import `tokio::net::UnixListener`\r\n  --> crates\\atuin-daemon\\src\\server.rs:16:5\r\n   |\r\n16 | use tokio::net::UnixListener;\r\n   |     ^^^^^^^^^^^^------------\r\n   |     |           |\r\n   |     |           help: a similar name exists in the module: `TcpListener`\r\n   |     no `UnixListener` in `net`\r\n   |\r\nnote: found an item that was configured out\r\n  --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-1.36.0\\src\\net\\mod.rs:50:29\r\n   |\r\n50 |     pub use unix::listener::UnixListener;\r\n   |                             ^^^^^^^^^^^^\r\n\r\nerror[E0432]: unresolved import `tokio_stream::wrappers::UnixListenerStream`\r\n  --> crates\\atuin-daemon\\src\\server.rs:17:5\r\n   |\r\n17 | use tokio_stream::wrappers::UnixListenerStream;\r\n   |     ^^^^^^^^^^^^^^^^^^^^^^^^------------------\r\n   |     |                       |\r\n   |     |                       help: a similar name exists in the module: `TcpListenerStream`\r\n   |     no `UnixListenerStream` in `wrappers`\r\n   |\r\nnote: found an item that was configured out\r\n  --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-stream-0.1.14\\src\\wrappers.rs:48:28\r\n   |\r\n48 |     pub use unix_listener::UnixListenerStream;\r\n   |                            ^^^^^^^^^^^^^^^^^^\r\n\r\nerror[E0433]: failed to resolve: could not find `unix` in `signal`\r\n   --> crates\\atuin-daemon\\src\\server.rs:138:35\r\n    |\r\n138 |     let mut term = tokio::signal::unix::signal(tokio::signal::unix::SignalKind::terminate())\r\n    |                                   ^^^^ could not find `unix` in `signal`\r\n    |\r\nnote: found an item that was configured out\r\n   --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-1.36.0\\src\\signal\\mod.rs:61:9\r\n    |\r\n61  | pub mod unix;\r\n    |         ^^^^\r\n\r\nerror[E0433]: failed to resolve: could not find `unix` in `signal`\r\n   --> crates\\atuin-daemon\\src\\server.rs:138:63\r\n    |\r\n138 |     let mut term = tokio::signal::unix::signal(tokio::signal::unix::SignalKind::terminate())\r\n    |                                                               ^^^^ could not find `unix` in `signal`\r\n    |\r\nnote: found an item that was configured out\r\n   --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-1.36.0\\src\\signal\\mod.rs:61:9\r\n    |\r\n61  | pub mod unix;\r\n    |         ^^^^\r\n\r\nerror[E0433]: failed to resolve: could not find `unix` in `signal`\r\n   --> crates\\atuin-daemon\\src\\server.rs:140:34\r\n    |\r\n140 |     let mut int = tokio::signal::unix::signal(tokio::signal::unix::SignalKind::interrupt())\r\n    |                                  ^^^^ could not find `unix` in `signal`\r\n    |\r\nnote: found an item that was configured out\r\n   --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-1.36.0\\src\\signal\\mod.rs:61:9\r\n    |\r\n61  | pub mod unix;\r\n    |         ^^^^\r\n\r\nerror[E0433]: failed to resolve: could not find `unix` in `signal`\r\n   --> crates\\atuin-daemon\\src\\server.rs:140:62\r\n    |\r\n140 |     let mut int = tokio::signal::unix::signal(tokio::signal::unix::SignalKind::interrupt())\r\n    |                                                              ^^^^ could not find `unix` in `signal`\r\n    |\r\nnote: found an item that was configured out\r\n   --> C:/Develop/msys64/home/Christian/.cargo\\registry\\src\\index.crates.io-6f17d22bba15001f\\tokio-1.36.0\\src\\signal\\mod.rs:61:9\r\n    |\r\n61  | pub mod unix;\r\n    |         ^^^^\r\n\r\nSome errors have detailed explanations: E0432, E0433.\r\nFor more information about an error, try `rustc --explain E0432`.\r\nerror: could not compile `atuin-daemon` (lib) due to 7 previous errors\r\n```\r\n\r\nTested on `x86_64-pc-windows-gnu` and `x86_64-pc-windows-msvc`.\n\n### Atuin doctor output\n\n```yaml\natuin:\r\n  version: 18.2.0\r\n  sync:\r\n    cloud: false\r\n    records: true\r\n    auto_sync: true\r\n    last_sync: 2024-05-08 17:54:15.2530676 +00:00:00\r\nshell:\r\n  name: zsh.exe\r\n  default: unknown\r\n  plugins:\r\n  - atuin\r\nsystem:\r\n  os: Windows\r\n  arch: x86\r\n  version: 11 (22621)\r\n  disks:\r\n  - name: Zeh\r\n    filesystem: NTFS\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct\n", "patch": "diff --git a/README.md b/README.md\nindex 64f126b89a2..1bff59ec791 100644\n--- a/README.md\n+++ b/README.md\n@@ -29,7 +29,7 @@\n Atuin replaces your existing shell history with a SQLite database, and records\n additional context for your commands. Additionally, it provides optional and\n _fully encrypted_ synchronisation of your history between machines, via an Atuin\n-server.  \n+server.\n \n \n \n@@ -90,7 +90,7 @@ I wanted to. And I **really** don't want to.\n - fish\n - nushell\n - xonsh\n- \n+\n ## Community\n \n ### Forum\n@@ -102,11 +102,11 @@ Atuin has a community forum, please ask here for help and support: https://forum\n Atuin also has a community Discord, available [here](https://discord.gg/jR3tfchVvW)\n \n # Quickstart\n-  \n+\n ## With the default sync server\n-  \n+\n This will sign you up for the default sync server, hosted by me. Everything is end-to-end encrypted, so your secrets are safe!\n-  \n+\n Read more below for offline-only usage, or for hosting your own server.\n \n ```\n@@ -130,10 +130,10 @@ Then restart your shell!\n > section below.\n \n ## Offline only (no sync)\n-  \n+\n ```\n bash <(curl https://raw.githubusercontent.com/atuinsh/atuin/main/install.sh)\n-            \n+\n atuin import auto\n ```\n \n@@ -171,25 +171,28 @@ toolchain, then you can run:\n ```\n cargo install atuin\n ```\n-  \n+\n And then follow [the shell setup](#shell-plugin)\n \n+#### Daemon Build Dependencies\n+[Protoc](https://grpc.io/docs/protoc-installation/) is required to build the daemon, which is enabled by default. This is because we use protobufs for the message serialization\n+\n ### Homebrew\n \n ```\n brew install atuin\n ```\n-  \n+\n And then follow [the shell setup](#shell-plugin)\n-  \n+\n ### MacPorts\n \n-Atuin is also available in [MacPorts](https://ports.macports.org/port/atuin/)  \n-  \n+Atuin is also available in [MacPorts](https://ports.macports.org/port/atuin/)\n+\n ```\n sudo port install atuin\n ```\n-  \n+\n And then follow [the shell setup](#shell-plugin)\n \n ### Cave\n@@ -223,7 +226,7 @@ Atuin is available in the Arch Linux [[extra] repository](https://archlinux.org/\n ```\n pacman -S atuin\n ```\n-  \n+\n And then follow [the shell setup](#shell-plugin)\n \n ### Xbps\n@@ -243,7 +246,7 @@ Atuin is available in the Termux package repository:\n ```\n pkg install atuin\n ```\n-  \n+\n And then follow [the shell setup](#shell-plugin)\n \n ### From source\n@@ -253,7 +256,7 @@ git clone https://github.com/atuinsh/atuin.git\n cd atuin/crates/atuin\n cargo install --path .\n ```\n-  \n+\n And then follow [the shell setup](#shell-plugin)\n \n ## Shell plugin\n@@ -273,9 +276,9 @@ echo 'eval \"$(atuin init zsh)\"' >> ~/.zshrc\n zinit load atuinsh/atuin\n ```\n \n-#### Antigen  \n-  \n-```sh  \n+#### Antigen\n+\n+```sh\n antigen bundle atuinsh/atuin@main\n ```\n \n@@ -345,7 +348,7 @@ atuin init fish | source\n ```\n \n to your `is-interactive` block in your `~/.config/fish/config.fish` file\n-  \n+\n ### Nushell\n \n Run in *Nushell*:\n@@ -371,7 +374,7 @@ to the end of your `~/.xonshrc`\n \n # Security\n \n-If you find any security issues, we'd appreciate it if you could alert ellie@atuin.sh \n+If you find any security issues, we'd appreciate it if you could alert ellie@atuin.sh\n \n # Contributors\n \ndiff --git a/crates/atuin-client/config.toml b/crates/atuin-client/config.toml\nindex 50db6952832..1de7308c2ef 100644\n--- a/crates/atuin-client/config.toml\n+++ b/crates/atuin-client/config.toml\n@@ -210,3 +210,18 @@ records = true\n ## auto: length of the selected command.\n ## static: length of the longest command stored in the history.\n # strategy = \"auto\"\n+\n+[daemon]\n+## Enables using the daemon to sync. Requires the daemon to be running in the background. Start it with `atuin daemon`\n+# enabled = false\n+\n+## How often the daemon should sync in seconds\n+# sync_frequency = 300\n+\n+## The path to the unix socket used by the daemon (on unix systems)\n+## linux/mac: ~/.local/share/atuin/atuin.sock\n+## windows: Not Supported\n+# socket_path = \"~/atuin.sock\"\n+\n+## The port that should be used for TCP on non unix systems\n+# tcp_port = 8889\ndiff --git a/crates/atuin-client/src/settings.rs b/crates/atuin-client/src/settings.rs\nindex ad7f95fc6b6..ed1c0a29d04 100644\n--- a/crates/atuin-client/src/settings.rs\n+++ b/crates/atuin-client/src/settings.rs\n@@ -353,6 +353,9 @@ pub struct Daemon {\n \n     /// The path to the unix socket used by the daemon\n     pub socket_path: String,\n+\n+    /// The port that should be used for TCP on non unix systems\n+    pub tcp_port: u64,\n }\n \n impl Default for Preview {\n@@ -369,6 +372,7 @@ impl Default for Daemon {\n             enabled: false,\n             sync_frequency: 300,\n             socket_path: \"\".to_string(),\n+            tcp_port: 8889,\n         }\n     }\n }\n@@ -706,6 +710,7 @@ impl Settings {\n             .set_default(\"daemon.sync_frequency\", 300)?\n             .set_default(\"daemon.enabled\", false)?\n             .set_default(\"daemon.socket_path\", socket_path.to_str())?\n+            .set_default(\"daemon.tcp_port\", 8889)?\n             .set_default(\n                 \"prefers_reduced_motion\",\n                 std::env::var(\"NO_MOTION\")\ndiff --git a/crates/atuin-daemon/src/client.rs b/crates/atuin-daemon/src/client.rs\nindex a832f9a9543..f3fecdbc199 100644\n--- a/crates/atuin-daemon/src/client.rs\n+++ b/crates/atuin-daemon/src/client.rs\n@@ -1,8 +1,12 @@\n use eyre::{eyre, Result};\n-use tokio::net::UnixStream;\n+#[cfg(windows)]\n+use tokio::net::TcpStream;\n use tonic::transport::{Channel, Endpoint, Uri};\n use tower::service_fn;\n \n+#[cfg(unix)]\n+use tokio::net::UnixStream;\n+\n use atuin_client::history::History;\n \n use crate::history::{\n@@ -15,6 +19,7 @@ pub struct HistoryClient {\n \n // Wrap the grpc client\n impl HistoryClient {\n+    #[cfg(unix)]\n     pub async fn new(path: String) -> Result<Self> {\n         let channel = Endpoint::try_from(\"http://atuin_local_daemon:0\")?\n             .connect_with_connector(service_fn(move |_: Uri| {\n@@ -30,6 +35,21 @@ impl HistoryClient {\n         Ok(HistoryClient { client })\n     }\n \n+    #[cfg(not(unix))]\n+    pub async fn new(port: u64) -> Result<Self> {\n+        let channel = Endpoint::try_from(\"http://atuin_local_daemon:0\")?\n+            .connect_with_connector(service_fn(move |_: Uri| {\n+                let url = format!(\"127.0.0.1:{}\", port);\n+                TcpStream::connect(url)\n+            }))\n+            .await\n+            .map_err(|_| eyre!(\"failed to connect to local atuin daemon. Is it running?\"))?;\n+\n+        let client = HistoryServiceClient::new(channel);\n+\n+        Ok(HistoryClient { client })\n+    }\n+\n     pub async fn start_history(&mut self, h: History) -> Result<String> {\n         let req = StartHistoryRequest {\n             command: h.command,\ndiff --git a/crates/atuin-daemon/src/server.rs b/crates/atuin-daemon/src/server.rs\nindex 72305737f7f..77824f60c79 100644\n--- a/crates/atuin-daemon/src/server.rs\n+++ b/crates/atuin-daemon/src/server.rs\n@@ -13,8 +13,6 @@ use atuin_client::database::{Database, Sqlite as HistoryDatabase};\n use atuin_client::history::{History, HistoryId};\n use dashmap::DashMap;\n use eyre::Result;\n-use tokio::net::UnixListener;\n-use tokio_stream::wrappers::UnixListenerStream;\n use tonic::{transport::Server, Request, Response, Status};\n \n use crate::history::history_server::{History as HistorySvc, HistoryServer};\n@@ -134,6 +132,7 @@ impl HistorySvc for HistoryService {\n     }\n }\n \n+#[cfg(unix)]\n async fn shutdown_signal(socket: PathBuf) {\n     let mut term = tokio::signal::unix::signal(tokio::signal::unix::SignalKind::terminate())\n         .expect(\"failed to register sigterm handler\");\n@@ -150,6 +149,52 @@ async fn shutdown_signal(socket: PathBuf) {\n     eprintln!(\"Shutting down...\");\n }\n \n+#[cfg(windows)]\n+async fn shutdown_signal() {\n+    tokio::signal::windows::ctrl_c()\n+        .expect(\"failed to register signal handler\")\n+        .recv()\n+        .await;\n+    eprintln!(\"Shutting down...\");\n+}\n+\n+#[cfg(unix)]\n+async fn start_server(settings: Settings, history: HistoryService) -> Result<()> {\n+    use tokio::net::UnixListener;\n+    use tokio_stream::wrappers::UnixListenerStream;\n+\n+    let socket = settings.daemon.socket_path.clone();\n+\n+    let uds = UnixListener::bind(socket.clone())?;\n+    let uds_stream = UnixListenerStream::new(uds);\n+\n+    tracing::info!(\"listening on unix socket {:?}\", socket);\n+    Server::builder()\n+        .add_service(HistoryServer::new(history))\n+        .serve_with_incoming_shutdown(uds_stream, shutdown_signal(socket.into()))\n+        .await?;\n+    Ok(())\n+}\n+\n+#[cfg(not(unix))]\n+async fn start_server(settings: Settings, history: HistoryService) -> Result<()> {\n+    use tokio::net::TcpListener;\n+    use tokio_stream::wrappers::TcpListenerStream;\n+\n+    let port = settings.daemon.tcp_port;\n+    let url = format!(\"127.0.0.1:{}\", port);\n+    let tcp = TcpListener::bind(url).await?;\n+    let tcp_stream = TcpListenerStream::new(tcp);\n+\n+    tracing::info!(\"listening on tcp port {:?}\", port);\n+\n+    Server::builder()\n+        .add_service(HistoryServer::new(history))\n+        .serve_with_incoming_shutdown(tcp_stream, shutdown_signal())\n+        .await?;\n+    Ok(())\n+}\n+\n // break the above down when we end up with multiple services\n \n /// Listen on a unix socket\n@@ -168,12 +213,6 @@ pub async fn listen(\n \n     let history = HistoryService::new(history_store.clone(), history_db.clone());\n \n-    let socket = settings.daemon.socket_path.clone();\n-    let uds = UnixListener::bind(socket.clone())?;\n-    let uds_stream = UnixListenerStream::new(uds);\n-\n-    tracing::info!(\"listening on unix socket {:?}\", socket);\n-\n     // start services\n     tokio::spawn(sync::worker(\n         settings.clone(),\n@@ -182,10 +221,5 @@ pub async fn listen(\n         history_db,\n     ));\n \n-    Server::builder()\n-        .add_service(HistoryServer::new(history))\n-        .serve_with_incoming_shutdown(uds_stream, shutdown_signal(socket.into()))\n-        .await?;\n-\n-    Ok(())\n+    start_server(settings, history).await\n }\ndiff --git a/crates/atuin/src/command/client/history.rs b/crates/atuin/src/command/client/history.rs\nindex f966c30294c..9d1453fc3e5 100644\n--- a/crates/atuin/src/command/client/history.rs\n+++ b/crates/atuin/src/command/client/history.rs\n@@ -315,11 +315,15 @@ impl Cmd {\n         }\n \n         if settings.daemon.enabled {\n-            let resp =\n-                atuin_daemon::client::HistoryClient::new(settings.daemon.socket_path.clone())\n-                    .await?\n-                    .start_history(h)\n-                    .await?;\n+            let resp = atuin_daemon::client::HistoryClient::new(\n+                #[cfg(not(unix))]\n+                settings.daemon.tcp_port,\n+                #[cfg(unix)]\n+                settings.daemon.socket_path.clone(),\n+            )\n+            .await?\n+            .start_history(h)\n+            .await?;\n \n             // print the ID\n             // we use this as the key for calling end\n@@ -350,10 +354,15 @@ impl Cmd {\n         // We will need to keep the old code around for a while.\n         // At the very least, while this is opt-in\n         if settings.daemon.enabled {\n-            atuin_daemon::client::HistoryClient::new(settings.daemon.socket_path.clone())\n-                .await?\n-                .end_history(id.to_string(), duration.unwrap_or(0), exit)\n-                .await?;\n+            let resp = atuin_daemon::client::HistoryClient::new(\n+                #[cfg(not(unix))]\n+                settings.daemon.tcp_port,\n+                #[cfg(unix)]\n+                settings.daemon.socket_path.clone(),\n+            )\n+            .await?\n+            .end_history(id.to_string(), duration.unwrap_or(0), exit)\n+            .await?;\n \n             return Ok(());\n         }\n", "instance_id": "atuinsh__atuin-2014", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the Atuin project fails to compile on Windows due to the addition of the daemon feature, which relies on Unix-specific functionality (e.g., `UnixStream`, `UnixListener`) from the Tokio library. The expected behavior (successful compilation) and the actual behavior (compilation errors) are explicitly stated, along with detailed error logs and system information via the \"Atuin doctor output\". However, there are minor ambiguities, such as the lack of explicit mention of the desired solution approach (e.g., whether to disable the daemon on Windows, provide an alternative implementation, or conditionally compile features). Additionally, edge cases or specific constraints for Windows compatibility are not discussed. Despite these minor gaps, the problem is well-defined enough to understand the core issue and proceed with a solution.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files and modules (`atuin-daemon`, `atuin-client`, etc.), requiring conditional compilation logic to handle platform-specific behavior (Unix vs. Windows). The changes include modifying the daemon's client and server implementations to use TCP streams on Windows instead of Unix sockets, as well as updating configuration settings and history command handling. This necessitates understanding Rust's conditional compilation (`#[cfg]` attributes), Tokio's networking APIs (`TcpStream`, `TcpListener`), and gRPC/Tonic for service communication, which are moderately complex concepts. Additionally, the problem requires ensuring that the daemon's functionality remains consistent across platforms, which introduces potential edge cases (e.g., network connection issues on Windows, port conflicts). However, the issue does not impact the core architecture of the system significantly, nor does it require advanced domain-specific knowledge or complex algorithms. The provided code changes are logical and focused, reducing the overall complexity. Therefore, a score of 0.55 reflects a medium difficulty level, requiring a solid understanding of Rust and cross-platform development but not posing an exceptionally challenging problem.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "QSV_AUTOINDEX_SIZE\nIn the documentation and help the variable still shows as QSV_AUTOINDEX instead of QSV_AUTOINDEX_SIZE\n", "patch": "diff --git a/src/cmd/index.rs b/src/cmd/index.rs\nindex 2440cb166..52a82dcf6 100644\n--- a/src/cmd/index.rs\n+++ b/src/cmd/index.rs\n@@ -8,9 +8,9 @@ automatically used by commands that can benefit from it. If the original CSV\n data changes after the index is made, commands that try to use it will result\n in an error (you have to regenerate the index before it can be used again).\n \n-However, if the environment variable QSV_AUTOINDEX is set, qsv will automatically\n-create an index when none is detected, and stale indices will be automatically\n-updated as well.\n+However, if the environment variable QSV_AUTOINDEX_SIZE is set, qsv will\n+automatically create an index when the input file size >= specified size (bytes).\n+It will also automatically update stale indices as well.\n \n Usage:\n     qsv index [options] <input>\ndiff --git a/src/config.rs b/src/config.rs\nindex b141830b1..dc9c2a416 100644\n--- a/src/config.rs\n+++ b/src/config.rs\n@@ -599,7 +599,7 @@ impl Config {\n     /// Check if the index file exists and is newer than the CSV file.\n     /// If so, return the index file.\n     /// If not, return None.\n-    /// Unless QSV_AUTOINDEX is set, in which case, we'll recreate the\n+    /// Unless QSV_AUTOINDEX_SIZE is set, in which case, we'll recreate the\n     /// stale index automatically\n     #[inline]\n     pub fn indexed(&self) -> CliResult<Option<Indexed<fs::File, fs::File>>> {\n", "instance_id": "dathere__qsv-2484", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the environment variable name in the documentation and code comments is outdated and needs to be updated from `QSV_AUTOINDEX` to `QSV_AUTOINDEX_SIZE`. The goal is straightforward\u2014correcting a naming inconsistency. However, the statement lacks some critical details, such as whether the functionality of the variable has changed (beyond the name) and if there are any specific behaviors or edge cases tied to the new name (e.g., how the size threshold is interpreted or validated). Additionally, there are no examples or further context about the expected behavior when `QSV_AUTOINDEX_SIZE` is set. Despite these minor ambiguities, the intent of the problem is understandable, and the provided code changes align with the stated goal.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a simple textual replacement of an environment variable name in documentation and comments across a couple of files. The scope of the code changes is minimal, affecting only a few lines in two files (`index.rs` and `config.rs`), and does not involve any functional or logical modifications to the codebase. No deep understanding of the system's architecture, algorithms, or complex language features is required\u2014just a basic search-and-replace operation. There are no edge cases or error handling considerations mentioned or implied in the problem statement or code changes. This task is purely cosmetic and falls into the \"very easy\" category, requiring only basic attention to detail to ensure the correct term is updated consistently.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "BUG extsort CSV MODE issues\n---\r\n**Describe the bug**\r\nI want to sort a CSV file with **extsort** in CSV MODE but sometimes get either a message\r\n\"`io error: invalid record index 18446744073709551615 (there are 16 records`)\" or the file is sorted wrongly.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nInput file `test_ids.csv`:\r\n```\r\npnm,tc_id,pc_id\r\n405,139280,9730000630075\r\n405,139281,9730000630075\r\n131,139282862,9730065908379\r\n138,139282863,9730065908379\r\n138,139282864,9730065908379\r\n405,139282865,9730065908379\r\n138,139282866,9730065908379\r\n138,139282867,9730065908379\r\n138,139282868,9730065908379\r\n138,139282869,9730065908379\r\n138,139282870,9730065908379\r\n138,139282871,9730065908379\r\n252,139282,9730000630075\r\n241,139283,9730000630075\r\n272,139284,9730000630075\r\n273,139285,9730000630075\r\n```\r\n\r\n_Commands:_\r\n```\r\nqsv index test_ids.csv\r\nqsv extsort --select tc_id test_ids.csv sorted.csv\r\n```\r\n\r\n=> `io error: invalid record index 18446744073709551615 (there are 16 records)`\r\n\r\n**Expected behavior**\r\nNo error.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows 11 64bit\r\n - qsv Version : qsv 1.0.0-mimalloc-apply;fetch;foreach;geocode;Luau 0.653;prompt;to;polars-0.44.2-31b7bb9;self_update-8-8;4.75 GiB-1.74   GiB-1.44 GiB-5.94 GiB (x86_64-pc-windows-msvc compiled with Rust 1.83) prebuilt\r\n\r\n**Additional context**\r\nIn other cases with big files the extsort command works, but \r\n`qsv dedup --select tc_id --sorted sorted.csv | qsv select tc_id -o out.csv`\r\nshows an error:\r\n`Aborting! Input not sorted! ByteRecord([\"138\" ... is greater than ByteRecord([\" ...`\r\n=> extsort seems to sort wrongly in these cases.\r\n\n", "patch": "diff --git a/src/cmd/extsort.rs b/src/cmd/extsort.rs\nindex 829889788..7f508585f 100644\n--- a/src/cmd/extsort.rs\n+++ b/src/cmd/extsort.rs\n@@ -237,7 +237,7 @@ fn sort_csv(\n             return fail!(\"Failed to retrieve position: invalid integer\");\n         };\n \n-        idxfile.seek(position - position_delta)?;\n+        idxfile.seek(position.saturating_sub(position_delta))?;\n         idxfile.read_byte_record(&mut record_wrk)?;\n         sorted_csv_wtr.write_byte_record(&record_wrk)?;\n     }\n", "instance_id": "dathere__qsv-2412", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to the `extsort` command in CSV mode for the `qsv` tool. It provides a specific input file example, the commands to reproduce the issue, and the expected behavior (no error). Additionally, it mentions two distinct issues: an invalid record index error and incorrect sorting in some cases with larger files. However, there are minor ambiguities and missing details. For instance, the problem statement does not fully clarify the conditions under which the errors occur (e.g., specific file sizes or data patterns beyond the provided example). It also lacks explicit mention of edge cases or constraints that might be relevant to the sorting behavior. While the reproduction steps and example input are helpful, a more comprehensive description of the expected output for the provided input or additional context about the CSV mode's intended behavior would make it fully clear. Hence, I rate it as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) with a score of 0.30. Here's the reasoning based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is minimal, confined to a single line in a single file (`src/cmd/extsort.rs`). The modification replaces a subtraction operation with a `saturating_sub` method to prevent underflow, which suggests a straightforward fix for an integer overflow or underflow issue when calculating file positions. This change does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is trivial, indicating low complexity in implementation.\n\n2. **Number of Technical Concepts:** The solution requires understanding basic Rust concepts, specifically integer overflow/underflow handling and file I/O operations (`seek` method). The use of `saturating_sub` is a standard Rust idiom for safe arithmetic, which is not particularly complex for someone familiar with the language. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic file handling and CSV processing are needed. The technical depth is minimal.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement mentions an invalid record index error (likely due to an underflow resulting in a large unsigned integer value) and incorrect sorting in some cases. The code change directly addresses the underflow issue by using `saturating_sub`, which prevents negative results from causing invalid positions. However, the problem statement also hints at incorrect sorting with larger files, which may not be fully resolved by this change. While the fix handles a specific edge case (underflow in position calculation), it is unclear if other edge cases related to sorting logic remain unaddressed. The error handling modification is simple and does not introduce significant complexity.\n\n4. **Overall Assessment:** The problem requires understanding a small part of the codebase related to file positioning in the `extsort` command. The fix is localized and does not demand deep knowledge of the entire system or complex refactoring. It is a bug fix that involves a simple adjustment to prevent an arithmetic error, fitting well within the \"Easy\" category. I assign a score of 0.30, slightly above the lower end of the easy range, to account for the need to understand the context of file seeking and CSV record handling, as well as the potential for additional sorting issues not covered by the provided diff.\n\nIn summary, this is a relatively straightforward bug fix with a clear target issue (underflow in position calculation), minimal code changes, and basic technical requirements, justifying a difficulty score of 0.30.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Inconsistency in zooming with Zoom in and Zoom out button or keyboard shortcut and no keyboard shortcut to reset zoom\n**Description** \r\n\r\nPress the Zoom out and Zoom in button a few times or press `ctrl+-` and `ctrl++`. For example here is what happens when I press `ctrl+-` five times and then  `ctrl++` five times:\r\n\r\nZoom level: 90, 81, 73, 66, 59 | 65, 71, 79, 86, 95\r\n\r\nDo it multiple times, and you will reach even more different zoom levels.\r\n\r\n**Suggested behavior** \r\n\r\nAny of the two behaviors would be okay:\r\n\r\n1. Zoom level: 90, 80, 70, 60, 50 | 60, 70, 80, 90, 100\r\n2. Zoom level: 90, 81, 73, 66, 59 | 66, 73, 81, 90, 100\r\n\r\n**Additional context**  \r\n\r\nIt would be better to set zoom in shortcut to `ctrl+=` as `ctrl++` means `ctrl+shift+=`.\r\nAlso, many apps use `ctrl+0` to reset zoom. But there is no keyboard shortcut to reset zoom in rnote.\r\n\n", "patch": "diff --git a/crates/rnote-ui/src/appwindow/actions.rs b/crates/rnote-ui/src/appwindow/actions.rs\nindex 500a05ac7f..ef2a3d34dc 100644\n--- a/crates/rnote-ui/src/appwindow/actions.rs\n+++ b/crates/rnote-ui/src/appwindow/actions.rs\n@@ -719,8 +719,8 @@ impl RnAppWindow {\n                     return;\n                 };\n                 let viewport_center = canvas.engine_ref().camera.viewport_center();\n-                let new_zoom =\n-                    canvas.engine_ref().camera.total_zoom() * (1.0 - RnCanvas::ZOOM_SCROLL_STEP);\n+                let new_zoom = canvas.engine_ref().camera.total_zoom()\n+                    * (1.0 / (1.0 + RnCanvas::ZOOM_SCROLL_STEP));\n                 let mut widget_flags = canvas.engine_mut().zoom_w_timeout(new_zoom);\n                 widget_flags |= canvas\n                     .engine_mut()\n@@ -1125,20 +1125,24 @@ impl RnAppWindow {\n         app.set_accels_for_action(\"win.print-doc\", &[\"<Ctrl>p\"]);\n         app.set_accels_for_action(\"win.add-page-to-doc\", &[\"<Ctrl><Shift>a\"]);\n         app.set_accels_for_action(\"win.remove-page-from-doc\", &[\"<Ctrl><Shift>r\"]);\n-        app.set_accels_for_action(\"win.zoom-in\", &[\"<Ctrl>plus\"]);\n-        app.set_accels_for_action(\"win.zoom-out\", &[\"<Ctrl>minus\"]);\n+        app.set_accels_for_action(\n+            \"win.zoom-in\",\n+            &[\"<Ctrl>plus\", \"<Ctrl>equal\", \"<Ctrl>KP_Add\"],\n+        );\n+        app.set_accels_for_action(\"win.zoom-reset\", &[\"<Ctrl>0\", \"<Ctrl>KP_0\"]);\n+        app.set_accels_for_action(\"win.zoom-out\", &[\"<Ctrl>minus\", \"<Ctrl>KP_Subtract\"]);\n         app.set_accels_for_action(\"win.import-file\", &[\"<Ctrl>i\"]);\n         app.set_accels_for_action(\"win.undo\", &[\"<Ctrl>z\"]);\n         app.set_accels_for_action(\"win.redo\", &[\"<Ctrl><Shift>z\"]);\n         app.set_accels_for_action(\"win.clipboard-copy\", &[\"<Ctrl>c\"]);\n         app.set_accels_for_action(\"win.clipboard-cut\", &[\"<Ctrl>x\"]);\n         app.set_accels_for_action(\"win.clipboard-paste\", &[\"<Ctrl>v\"]);\n-        app.set_accels_for_action(\"win.pen-style::brush\", &[\"<Ctrl>1\"]);\n-        app.set_accels_for_action(\"win.pen-style::shaper\", &[\"<Ctrl>2\"]);\n-        app.set_accels_for_action(\"win.pen-style::typewriter\", &[\"<Ctrl>3\"]);\n-        app.set_accels_for_action(\"win.pen-style::eraser\", &[\"<Ctrl>4\"]);\n-        app.set_accels_for_action(\"win.pen-style::selector\", &[\"<Ctrl>5\"]);\n-        app.set_accels_for_action(\"win.pen-style::tools\", &[\"<Ctrl>6\"]);\n+        app.set_accels_for_action(\"win.pen-style::brush\", &[\"<Ctrl>1\", \"<Ctrl>KP_1\"]);\n+        app.set_accels_for_action(\"win.pen-style::shaper\", &[\"<Ctrl>2\", \"<Ctrl>KP_2\"]);\n+        app.set_accels_for_action(\"win.pen-style::typewriter\", &[\"<Ctrl>3\", \"<Ctrl>KP_3\"]);\n+        app.set_accels_for_action(\"win.pen-style::eraser\", &[\"<Ctrl>4\", \"<Ctrl>KP_4\"]);\n+        app.set_accels_for_action(\"win.pen-style::selector\", &[\"<Ctrl>5\", \"<Ctrl>KP_5\"]);\n+        app.set_accels_for_action(\"win.pen-style::tools\", &[\"<Ctrl>6\", \"<Ctrl>KP_6\"]);\n \n         // shortcuts for devel build\n         if config::PROFILE.to_lowercase().as_str() == \"devel\" {\ndiff --git a/crates/rnote-ui/src/canvaswrapper.rs b/crates/rnote-ui/src/canvaswrapper.rs\nindex 59521ff51c..e94ad1bdba 100644\n--- a/crates/rnote-ui/src/canvaswrapper.rs\n+++ b/crates/rnote-ui/src/canvaswrapper.rs\n@@ -348,7 +348,11 @@ mod imp {\n                         }\n                         let canvas = canvaswrapper.canvas();\n                         let old_zoom = canvas.engine_ref().camera.total_zoom();\n-                        let new_zoom = old_zoom * (1.0 - dy * RnCanvas::ZOOM_SCROLL_STEP);\n+                        let new_zoom = if dy < 0.0 {\n+                            old_zoom * (1.0 - dy * RnCanvas::ZOOM_SCROLL_STEP)\n+                        } else {\n+                            old_zoom * (1.0 / (1.0 + dy * RnCanvas::ZOOM_SCROLL_STEP))\n+                        };\n \n                         if (Camera::ZOOM_MIN..=Camera::ZOOM_MAX).contains(&new_zoom) {\n                             let camera_offset = canvas.engine_ref().camera.offset();\n", "instance_id": "flxzt__rnote-1285", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue with inconsistent zoom behavior when using zoom in and zoom out buttons or keyboard shortcuts in the application. It provides specific examples of the current behavior (e.g., zoom levels changing inconsistently) and suggests two acceptable solutions for the desired behavior. Additionally, it highlights the lack of a keyboard shortcut for resetting zoom and suggests a change in the zoom-in shortcut. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the exact zoom steps or constraints (e.g., minimum/maximum zoom levels) that should be enforced beyond the suggested sequences. Edge cases, such as behavior at zoom boundaries or potential rounding errors in zoom calculations, are not addressed. Overall, the problem is valid and mostly clear, but these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are relatively localized, affecting two files (`actions.rs` and `canvaswrapper.rs`) in the `rnote-ui` crate. The modifications involve adjusting the zoom calculation logic for zoom-out and scroll events to ensure consistent behavior and adding new keyboard shortcuts for zoom actions. The changes are small in terms of lines of code and do not impact the broader system architecture.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Rust syntax, arithmetic operations for zoom calculations, and familiarity with the application's event handling and keyboard shortcut system (likely using a framework like GTK, given the context). The concepts are straightforward\u2014modifying multiplication factors for zoom and updating shortcut mappings. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond inconsistent zoom levels, and the provided code changes do not introduce new error handling logic. However, the developer must consider potential issues like zoom level boundaries (as seen in the `Camera::ZOOM_MIN` and `Camera::ZOOM_MAX` checks in the code) to prevent unexpected behavior. These are relatively simple to handle and do not significantly increase the difficulty.\n\n4. **Overall Complexity**: The task involves understanding a small part of the codebase related to zoom functionality and making targeted modifications. It does not require deep knowledge of the entire application or complex refactoring. The primary challenge lies in ensuring that the zoom in and zoom out operations are symmetric, which is a simple mathematical adjustment (as seen in the change from multiplication to division for zoom-out).\n\nGiven these points, a difficulty score of 0.30 reflects an easy problem that requires some understanding of the code logic and minor modifications to achieve the desired behavior. It is slightly above the very easy range due to the need to ensure consistency in zoom calculations and handle multiple shortcut mappings.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug: order by literal with limit return error\n### Search before asking\r\n\r\n- [X] I had searched in the [issues](https://github.com/databendlabs/databend/issues) and found no similar issues.\r\n\r\n\r\n### Version\r\n\r\nv1.2.662\r\n\r\n\r\n\r\n### What's Wrong?\r\n\r\norder by literal with limit return error\r\n\r\nwe can ignore order by item if it is a scalar value\r\n\r\n### How to Reproduce?\r\n\r\n```sql\r\nMySQL [(none)]> create table tt(a int, b int);\r\nQuery OK, 0 rows affected (0.187 sec)\r\n\r\nMySQL [(none)]> insert into tt values(1, 2),(3, 4);\r\nQuery OK, 2 rows affected (0.209 sec)\r\n\r\nMySQL [(none)]> select eq(tt.a, b) FROM tt ORDER BY 'a' LIMIT 89, 7;\r\nERROR 1105 (HY000): BadArguments. Code: 1006, Text = Unable to get field named \"0\". Valid fields: [\"4\"].\r\n```\r\n\r\n### Are you willing to submit PR?\r\n\r\n- [ ] Yes I am willing to submit a PR!\n", "patch": "diff --git a/src/query/sql/src/planner/binder/sort.rs b/src/query/sql/src/planner/binder/sort.rs\nindex 8af8d17a89cf8..a036e42939b07 100644\n--- a/src/query/sql/src/planner/binder/sort.rs\n+++ b/src/query/sql/src/planner/binder/sort.rs\n@@ -148,6 +148,10 @@ impl Binder {\n                             )\n                             .map_err(|e| ErrorCode::SemanticError(e.message()))?;\n \n+                        if let ScalarExpr::ConstantExpr(..) = rewrite_scalar {\n+                            continue;\n+                        }\n+\n                         let column_binding =\n                             if let ScalarExpr::BoundColumnRef(col) = &rewrite_scalar {\n                                 col.column.clone()\n", "instance_id": "databendlabs__databend-16967", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an error occurs when using \"ORDER BY\" with a literal value and a \"LIMIT\" clause in a SQL query. It provides a reproducible example with SQL queries, which helps in understanding the bug. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior beyond \"ignoring order by item if it is a scalar value,\" nor does it specify any edge cases or constraints (e.g., what types of literals should be ignored, or how this should interact with other query components). Additionally, there is no mention of performance considerations or compatibility with other parts of the system. Despite these gaps, the core issue is understandable with the provided context and example, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The code change provided is minimal, involving a single conditional check in the `sort.rs` file to skip processing of constant scalar expressions in the \"ORDER BY\" clause. This modification is localized to a single function and does not require understanding complex interactions across multiple modules or the broader architecture of the codebase. The technical concepts involved are straightforward\u2014basic Rust syntax and understanding of the internal representation of SQL expressions (e.g., `ScalarExpr::ConstantExpr`). There are no advanced algorithms, design patterns, or domain-specific knowledge required beyond familiarity with SQL query processing at a basic level. Edge cases and error handling are not explicitly mentioned in the problem statement, and the code change does not introduce new error handling logic. The simplicity and narrow scope of the fix justify a difficulty score of 0.25, as it requires only a basic understanding of the code logic and a simple modification to resolve the bug.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add a `.sort()` method to expose `IndexMap::sort_keys()`\nThere isn't a good way that I know of to get sorted output when `preserve_order` is enabled (and disabling it isn't an option), which is important for getting canonical JSON and for comparing JSON strings. `IndexMap` has a `sort_keys`, could this be exposed somehow?\r\n\r\nI'm imagining `Value::sort(&mut self)` and `Map::sort(&mut self)`, either of which would just walk the tree and call the methods on all nested items. When `preserve_order` is disabled, this would be a nop.\n", "patch": "diff --git a/src/map.rs b/src/map.rs\nindex 90da86fed..7ff325b9d 100644\n--- a/src/map.rs\n+++ b/src/map.rs\n@@ -348,6 +348,28 @@ impl Map<String, Value> {\n     {\n         self.map.retain(f);\n     }\n+\n+    /// Sorts this map's entries in-place using `str`'s usual ordering.\n+    ///\n+    /// If serde_json's \"preserve_order\" feature is not enabled, this method\n+    /// does no work because all JSON maps are always kept in a sorted state.\n+    ///\n+    /// If serde_json's \"preserve_order\" feature is enabled, this method\n+    /// destroys the original source order or insertion order of this map in\n+    /// favor of an alphanumerical order that matches how a BTreeMap with the\n+    /// same contents would be ordered. This takes **O(n log n + c)** time where\n+    /// _n_ is the length of the map and _c_ is the capacity.\n+    ///\n+    /// Other maps nested within the values of this map are not sorted. If you\n+    /// need the entire data structure to be sorted at all levels, you must also\n+    /// call `map.`[`values_mut`]`().for_each(`[`Value::sort_all_objects`]`)`.\n+    ///\n+    /// [`values_mut`]: Map::values_mut\n+    #[inline]\n+    pub fn sort_keys(&mut self) {\n+        #[cfg(feature = \"preserve_order\")]\n+        self.map.sort_unstable_keys();\n+    }\n }\n \n #[allow(clippy::derivable_impls)] // clippy bug: https://github.com/rust-lang/rust-clippy/issues/7655\ndiff --git a/src/value/mod.rs b/src/value/mod.rs\nindex 026f10dcb..6b40f9a52 100644\n--- a/src/value/mod.rs\n+++ b/src/value/mod.rs\n@@ -861,6 +861,32 @@ impl Value {\n     pub fn take(&mut self) -> Value {\n         mem::replace(self, Value::Null)\n     }\n+\n+    /// Reorders the entries of all `Value::Object` nested within this JSON\n+    /// value according to `str`'s usual ordering.\n+    ///\n+    /// If serde_json's \"preserve_order\" feature is not enabled, this method\n+    /// does no work because all JSON maps are always kept in a sorted state.\n+    ///\n+    /// If serde_json's \"preserve_order\" feature is enabled, this method\n+    /// destroys the original source order or insertion order of the JSON\n+    /// objects in favor of an alphanumerical order that matches how a BTreeMap\n+    /// with the same contents would be ordered.\n+    pub fn sort_all_objects(&mut self) {\n+        #[cfg(feature = \"preserve_order\")]\n+        {\n+            match self {\n+                Value::Object(map) => {\n+                    map.sort_keys();\n+                    map.values_mut().for_each(Value::sort_all_objects);\n+                }\n+                Value::Array(list) => {\n+                    list.iter_mut().for_each(Value::sort_all_objects);\n+                }\n+                _ => {}\n+            }\n+        }\n+    }\n }\n \n /// The default value is `Value::Null`.\n", "instance_id": "serde-rs__json-1199", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add a `.sort()` method to expose `IndexMap::sort_keys()` for achieving sorted output in JSON when the `preserve_order` feature is enabled. The goal is well-defined: to allow sorting of map keys for canonical JSON output and comparison purposes. The proposed solution (adding `Value::sort()` and `Map::sort()`) is also described at a high level. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss edge cases, such as how nested structures should be handled (though the code changes address this). Additionally, there is no mention of performance implications or constraints, which could be relevant given the sorting operation's time complexity. Examples of input/output or usage scenarios are also absent, which would have made the requirements more concrete. Overall, while the problem is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, affecting two files (`map.rs` and `value/mod.rs`) and adding two new methods (`Map::sort_keys()` and `Value::sort_all_objects()`). The modifications are straightforward, involving direct calls to existing functionality (`sort_unstable_keys()` from `IndexMap`) and recursive traversal of nested structures. There is no significant impact on the broader system architecture, and the amount of code added is minimal (around 50 lines, including documentation). The changes do not require deep understanding of interactions across multiple modules beyond the immediate context of `Map` and `Value`.\n\n2. **Number of Technical Concepts:** The problem requires understanding a few specific concepts, including Rust's feature flags (`#[cfg(feature = \"preserve_order\")]`), the `IndexMap` data structure and its `sort_unstable_keys()` method, and basic recursive traversal of a data structure (`Value::Array` and `Value::Object`). These concepts are not particularly complex for a developer familiar with Rust and the `serde_json` library. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond standard library usage.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes handle nested structures (arrays and objects) recursively, which addresses a common concern. There are no explicit error conditions to handle, as the sorting operation is in-place and does not appear to introduce failure modes beyond what `IndexMap::sort_unstable_keys()` already handles. The complexity of edge cases is low, as the operation is deterministic and operates on well-defined data structures.\n\n4. **Overall Complexity:** The task involves adding a feature that leverages existing functionality with minimal risk of introducing bugs or performance issues (beyond the documented O(n log n) time complexity). The conditional compilation aspect (`preserve_order` feature) adds a slight layer of complexity but is standard in Rust projects. The problem does not require deep architectural changes or advanced problem-solving skills.\n\nGiven these factors, a difficulty score of 0.35 reflects an \"Easy\" task that requires understanding some code logic and making simple, well-contained modifications. It is slightly above the lower end of the easy range due to the need to handle nested structures recursively and understand feature flags, but it does not approach the complexity of a medium-difficulty problem.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Pack multiple handshake messages into single TLS message\nAt the moment each handshake message we send is contained in its own outer TLS message. This means:\r\n\r\n- handshakes take a little more data that is actually necessary, due to repeated encryption overhead.\r\n- there is likely some performance left on the table (since N encryptions of M bytes will be slower than one encryption of N * M bytes).\n", "patch": "diff --git a/rustls/src/client/tls13.rs b/rustls/src/client/tls13.rs\nindex 28af4fde573..17b59f1c364 100644\n--- a/rustls/src/client/tls13.rs\n+++ b/rustls/src/client/tls13.rs\n@@ -12,7 +12,9 @@ use crate::check::inappropriate_handshake_message;\n use crate::client::common::{ClientAuthDetails, ClientHelloDetails, ServerCertDetails};\n use crate::client::ech::{self, EchState, EchStatus};\n use crate::client::{hs, ClientConfig, ClientSessionStore};\n-use crate::common_state::{CommonState, HandshakeKind, KxState, Protocol, Side, State};\n+use crate::common_state::{\n+    CommonState, HandshakeFlightTls13, HandshakeKind, KxState, Protocol, Side, State,\n+};\n use crate::conn::ConnectionRandoms;\n use crate::crypto::ActiveKeyExchange;\n use crate::enums::{\n@@ -1155,12 +1157,11 @@ impl State<ClientConnectionData> for ExpectCertificateVerify<'_> {\n }\n \n fn emit_compressed_certificate_tls13(\n-    transcript: &mut HandshakeHash,\n+    flight: &mut HandshakeFlightTls13<'_>,\n     certkey: &CertifiedKey,\n     auth_context: Option<Vec<u8>>,\n     compressor: &dyn compress::CertCompressor,\n     config: &ClientConfig,\n-    common: &mut CommonState,\n ) {\n     let mut cert_payload = CertificatePayloadTls13::new(certkey.cert.iter(), None);\n     cert_payload.context = PayloadU8::new(auth_context.clone().unwrap_or_default());\n@@ -1170,25 +1171,19 @@ fn emit_compressed_certificate_tls13(\n         .compression_for(compressor, &cert_payload)\n     {\n         Ok(compressed) => compressed,\n-        Err(_) => return emit_certificate_tls13(transcript, Some(certkey), auth_context, common),\n+        Err(_) => return emit_certificate_tls13(flight, Some(certkey), auth_context),\n     };\n \n-    let m = Message {\n-        version: ProtocolVersion::TLSv1_3,\n-        payload: MessagePayload::handshake(HandshakeMessagePayload {\n-            typ: HandshakeType::CompressedCertificate,\n-            payload: HandshakePayload::CompressedCertificate(compressed.compressed_cert_payload()),\n-        }),\n-    };\n-    transcript.add_message(&m);\n-    common.send_msg(m, true);\n+    flight.add(HandshakeMessagePayload {\n+        typ: HandshakeType::CompressedCertificate,\n+        payload: HandshakePayload::CompressedCertificate(compressed.compressed_cert_payload()),\n+    });\n }\n \n fn emit_certificate_tls13(\n-    transcript: &mut HandshakeHash,\n+    flight: &mut HandshakeFlightTls13<'_>,\n     certkey: Option<&CertifiedKey>,\n     auth_context: Option<Vec<u8>>,\n-    common: &mut CommonState,\n ) {\n     let certs = certkey\n         .map(|ck| ck.cert.as_ref())\n@@ -1196,58 +1191,36 @@ fn emit_certificate_tls13(\n     let mut cert_payload = CertificatePayloadTls13::new(certs.iter(), None);\n     cert_payload.context = PayloadU8::new(auth_context.unwrap_or_default());\n \n-    let m = Message {\n-        version: ProtocolVersion::TLSv1_3,\n-        payload: MessagePayload::handshake(HandshakeMessagePayload {\n-            typ: HandshakeType::Certificate,\n-            payload: HandshakePayload::CertificateTls13(cert_payload),\n-        }),\n-    };\n-    transcript.add_message(&m);\n-    common.send_msg(m, true);\n+    flight.add(HandshakeMessagePayload {\n+        typ: HandshakeType::Certificate,\n+        payload: HandshakePayload::CertificateTls13(cert_payload),\n+    });\n }\n \n fn emit_certverify_tls13(\n-    transcript: &mut HandshakeHash,\n+    flight: &mut HandshakeFlightTls13<'_>,\n     signer: &dyn Signer,\n-    common: &mut CommonState,\n ) -> Result<(), Error> {\n-    let message = construct_client_verify_message(&transcript.current_hash());\n+    let message = construct_client_verify_message(&flight.transcript.current_hash());\n \n     let scheme = signer.scheme();\n     let sig = signer.sign(&message)?;\n     let dss = DigitallySignedStruct::new(scheme, sig);\n \n-    let m = Message {\n-        version: ProtocolVersion::TLSv1_3,\n-        payload: MessagePayload::handshake(HandshakeMessagePayload {\n-            typ: HandshakeType::CertificateVerify,\n-            payload: HandshakePayload::CertificateVerify(dss),\n-        }),\n-    };\n-\n-    transcript.add_message(&m);\n-    common.send_msg(m, true);\n+    flight.add(HandshakeMessagePayload {\n+        typ: HandshakeType::CertificateVerify,\n+        payload: HandshakePayload::CertificateVerify(dss),\n+    });\n     Ok(())\n }\n \n-fn emit_finished_tls13(\n-    transcript: &mut HandshakeHash,\n-    verify_data: &crypto::hmac::Tag,\n-    common: &mut CommonState,\n-) {\n+fn emit_finished_tls13(flight: &mut HandshakeFlightTls13<'_>, verify_data: &crypto::hmac::Tag) {\n     let verify_data_payload = Payload::new(verify_data.as_ref());\n \n-    let m = Message {\n-        version: ProtocolVersion::TLSv1_3,\n-        payload: MessagePayload::handshake(HandshakeMessagePayload {\n-            typ: HandshakeType::Finished,\n-            payload: HandshakePayload::Finished(verify_data_payload),\n-        }),\n-    };\n-\n-    transcript.add_message(&m);\n-    common.send_msg(m, true);\n+    flight.add(HandshakeMessagePayload {\n+        typ: HandshakeType::Finished,\n+        payload: HandshakePayload::Finished(verify_data_payload),\n+    });\n }\n \n fn emit_end_of_early_data_tls13(transcript: &mut HandshakeHash, common: &mut CommonState) {\n@@ -1321,6 +1294,8 @@ impl State<ClientConnectionData> for ExpectFinished {\n                 .set_handshake_encrypter(cx.common);\n         }\n \n+        let mut flight = HandshakeFlightTls13::new(&mut st.transcript);\n+\n         /* Send our authentication/finished messages.  These are still encrypted\n          * with our handshake keys. */\n         if let Some(client_auth) = st.client_auth {\n@@ -1328,7 +1303,7 @@ impl State<ClientConnectionData> for ExpectFinished {\n                 ClientAuthDetails::Empty {\n                     auth_context_tls13: auth_context,\n                 } => {\n-                    emit_certificate_tls13(&mut st.transcript, None, auth_context, cx.common);\n+                    emit_certificate_tls13(&mut flight, None, auth_context);\n                 }\n                 ClientAuthDetails::Verify {\n                     auth_context_tls13: auth_context,\n@@ -1336,7 +1311,7 @@ impl State<ClientConnectionData> for ExpectFinished {\n                 } if cx.data.ech_status == EchStatus::Rejected => {\n                     // If ECH was offered, and rejected, we MUST respond with\n                     // an empty certificate message.\n-                    emit_certificate_tls13(&mut st.transcript, None, auth_context, cx.common);\n+                    emit_certificate_tls13(&mut flight, None, auth_context);\n                 }\n                 ClientAuthDetails::Verify {\n                     certkey,\n@@ -1346,22 +1321,16 @@ impl State<ClientConnectionData> for ExpectFinished {\n                 } => {\n                     if let Some(compressor) = compressor {\n                         emit_compressed_certificate_tls13(\n-                            &mut st.transcript,\n+                            &mut flight,\n                             &certkey,\n                             auth_context,\n                             compressor,\n                             &st.config,\n-                            cx.common,\n                         );\n                     } else {\n-                        emit_certificate_tls13(\n-                            &mut st.transcript,\n-                            Some(&certkey),\n-                            auth_context,\n-                            cx.common,\n-                        );\n+                        emit_certificate_tls13(&mut flight, Some(&certkey), auth_context);\n                     }\n-                    emit_certverify_tls13(&mut st.transcript, signer.as_ref(), cx.common)?;\n+                    emit_certverify_tls13(&mut flight, signer.as_ref())?;\n                 }\n             }\n         }\n@@ -1370,12 +1339,13 @@ impl State<ClientConnectionData> for ExpectFinished {\n             .key_schedule\n             .into_pre_finished_client_traffic(\n                 hash_after_handshake,\n-                st.transcript.current_hash(),\n+                flight.transcript.current_hash(),\n                 &*st.config.key_log,\n                 &st.randoms.client,\n             );\n \n-        emit_finished_tls13(&mut st.transcript, &verify_data, cx.common);\n+        emit_finished_tls13(&mut flight, &verify_data);\n+        flight.finish(cx.common);\n \n         /* We're now sure this server supports TLS1.3.  But if we run out of TLS1.3 tickets\n          * when connecting to it again, we definitely don't want to attempt a TLS1.2 resumption. */\ndiff --git a/rustls/src/common_state.rs b/rustls/src/common_state.rs\nindex 44b30644fbd..1939c802133 100644\n--- a/rustls/src/common_state.rs\n+++ b/rustls/src/common_state.rs\n@@ -6,12 +6,14 @@ use pki_types::CertificateDer;\n use crate::crypto::SupportedKxGroup;\n use crate::enums::{AlertDescription, ContentType, HandshakeType, ProtocolVersion};\n use crate::error::{Error, InvalidMessage, PeerMisbehaved};\n+use crate::hash_hs::HandshakeHash;\n use crate::log::{debug, error, warn};\n use crate::msgs::alert::AlertMessagePayload;\n use crate::msgs::base::Payload;\n+use crate::msgs::codec::Codec;\n use crate::msgs::enums::{AlertLevel, KeyUpdateRequest};\n use crate::msgs::fragmenter::MessageFragmenter;\n-use crate::msgs::handshake::CertificateChain;\n+use crate::msgs::handshake::{CertificateChain, HandshakeMessagePayload};\n use crate::msgs::message::{\n     Message, MessagePayload, OutboundChunks, OutboundOpaqueMessage, OutboundPlainMessage,\n     PlainMessage,\n@@ -432,7 +434,10 @@ impl CommonState {\n                     self.quic.alert = Some(alert.description);\n                 } else {\n                     debug_assert!(\n-                        matches!(m.payload, MessagePayload::Handshake { .. }),\n+                        matches!(\n+                            m.payload,\n+                            MessagePayload::Handshake { .. } | MessagePayload::HandshakeFlight(_)\n+                        ),\n                         \"QUIC uses TLS for the cryptographic handshake only\"\n                     );\n                     let mut bytes = Vec::new();\n@@ -974,5 +979,42 @@ impl KxState {\n     }\n }\n \n+pub(crate) struct HandshakeFlight<'a, const TLS13: bool> {\n+    pub(crate) transcript: &'a mut HandshakeHash,\n+    body: Vec<u8>,\n+}\n+\n+impl<'a, const TLS13: bool> HandshakeFlight<'a, TLS13> {\n+    pub(crate) fn new(transcript: &'a mut HandshakeHash) -> Self {\n+        Self {\n+            transcript,\n+            body: Vec::new(),\n+        }\n+    }\n+\n+    pub(crate) fn add(&mut self, hs: HandshakeMessagePayload<'_>) {\n+        let start_len = self.body.len();\n+        hs.encode(&mut self.body);\n+        self.transcript\n+            .add(&self.body[start_len..]);\n+    }\n+\n+    pub(crate) fn finish(self, common: &mut CommonState) {\n+        common.send_msg(\n+            Message {\n+                version: match TLS13 {\n+                    true => ProtocolVersion::TLSv1_3,\n+                    false => ProtocolVersion::TLSv1_2,\n+                },\n+                payload: MessagePayload::HandshakeFlight(Payload::new(self.body)),\n+            },\n+            TLS13,\n+        );\n+    }\n+}\n+\n+pub(crate) type HandshakeFlightTls12<'a> = HandshakeFlight<'a, false>;\n+pub(crate) type HandshakeFlightTls13<'a> = HandshakeFlight<'a, true>;\n+\n const DEFAULT_RECEIVED_PLAINTEXT_LIMIT: usize = 16 * 1024;\n pub(crate) const DEFAULT_BUFFER_LIMIT: usize = 64 * 1024;\ndiff --git a/rustls/src/hash_hs.rs b/rustls/src/hash_hs.rs\nindex 07be5ef5d79..0e802879137 100644\n--- a/rustls/src/hash_hs.rs\n+++ b/rustls/src/hash_hs.rs\n@@ -35,15 +35,15 @@ impl HandshakeHashBuffer {\n \n     /// Hash/buffer a handshake message.\n     pub(crate) fn add_message(&mut self, m: &Message<'_>) {\n-        if let MessagePayload::Handshake { encoded, .. } = &m.payload {\n-            self.buffer\n-                .extend_from_slice(encoded.bytes());\n-        }\n+        match &m.payload {\n+            MessagePayload::Handshake { encoded, .. } => self.add_raw(encoded.bytes()),\n+            MessagePayload::HandshakeFlight(payload) => self.add_raw(payload.bytes()),\n+            _ => {}\n+        };\n     }\n \n     /// Hash or buffer a byte slice.\n-    #[cfg(all(test, any(feature = \"ring\", feature = \"aws_lc_rs\")))]\n-    fn update_raw(&mut self, buf: &[u8]) {\n+    fn add_raw(&mut self, buf: &[u8]) {\n         self.buffer.extend_from_slice(buf);\n     }\n \n@@ -98,14 +98,20 @@ impl HandshakeHash {\n \n     /// Hash/buffer a handshake message.\n     pub(crate) fn add_message(&mut self, m: &Message<'_>) -> &mut Self {\n-        if let MessagePayload::Handshake { encoded, .. } = &m.payload {\n-            self.update_raw(encoded.bytes());\n+        match &m.payload {\n+            MessagePayload::Handshake { encoded, .. } => self.add_raw(encoded.bytes()),\n+            MessagePayload::HandshakeFlight(payload) => self.add_raw(payload.bytes()),\n+            _ => self,\n         }\n-        self\n+    }\n+\n+    /// Hash/buffer an encoded handshake message.\n+    pub(crate) fn add(&mut self, bytes: &[u8]) {\n+        self.add_raw(bytes);\n     }\n \n     /// Hash or buffer a byte slice.\n-    fn update_raw(&mut self, buf: &[u8]) -> &mut Self {\n+    fn add_raw(&mut self, buf: &[u8]) -> &mut Self {\n         self.ctx.update(buf);\n \n         if let Some(buffer) = &mut self.client_auth {\n@@ -145,7 +151,7 @@ impl HandshakeHash {\n         let old_handshake_hash_msg =\n             HandshakeMessagePayload::build_handshake_hash(old_hash.as_ref());\n \n-        self.update_raw(&old_handshake_hash_msg.get_encoding());\n+        self.add_raw(&old_handshake_hash_msg.get_encoding());\n     }\n \n     /// Get the current hash value.\n@@ -178,17 +184,21 @@ impl Clone for HandshakeHash {\n }\n \n test_for_each_provider! {\n-    use super::HandshakeHashBuffer;\n+    use super::*;\n+    use crate::msgs::handshake::{HandshakeMessagePayload, HandshakePayload};\n+    use crate::crypto::hash::Hash;\n+    use crate::msgs::base::Payload;\n+    use crate::enums::{ProtocolVersion, HandshakeType};\n     use provider::hash::SHA256;\n \n     #[test]\n     fn hashes_correctly() {\n         let mut hhb = HandshakeHashBuffer::new();\n-        hhb.update_raw(b\"hello\");\n+        hhb.add_raw(b\"hello\");\n         assert_eq!(hhb.buffer.len(), 5);\n         let mut hh = hhb.start_hash(&SHA256);\n         assert!(hh.client_auth.is_none());\n-        hh.update_raw(b\"world\");\n+        hh.add_raw(b\"world\");\n         let h = hh.current_hash();\n         let h = h.as_ref();\n         assert_eq!(h[0], 0x93);\n@@ -197,12 +207,50 @@ test_for_each_provider! {\n         assert_eq!(h[3], 0x5c);\n     }\n \n+    #[test]\n+    fn hashes_message_types() {\n+        // handshake protocol encoding of 0x0e 00 00 00\n+        let server_hello_done_message = Message {\n+            version: ProtocolVersion::TLSv1_2,\n+            payload: MessagePayload::handshake(HandshakeMessagePayload {\n+                        typ: HandshakeType::ServerHelloDone,\n+                        payload: HandshakePayload::ServerHelloDone,\n+                        }) };\n+\n+        let app_data_ignored = Message {\n+            version: ProtocolVersion::TLSv1_3,\n+            payload: MessagePayload::ApplicationData(Payload::Borrowed(b\"hello\")),\n+        };\n+\n+        let end_of_early_data_flight = Message {\n+            version: ProtocolVersion::TLSv1_3,\n+            payload: MessagePayload::HandshakeFlight(Payload::Borrowed(b\"\\x05\\x00\\x00\\x00\"))\n+        };\n+\n+        // buffered mode\n+        let mut hhb = HandshakeHashBuffer::new();\n+        hhb.add_message(&server_hello_done_message);\n+        hhb.add_message(&app_data_ignored);\n+        hhb.add_message(&end_of_early_data_flight);\n+        assert_eq!(hhb.start_hash(&SHA256).current_hash().as_ref(),\n+                   SHA256.hash(b\"\\x0e\\x00\\x00\\x00\\x05\\x00\\x00\\x00\").as_ref());\n+\n+        // non-buffered mode\n+        let mut hh = HandshakeHashBuffer::new().start_hash(&SHA256);\n+        hh.add_message(&server_hello_done_message);\n+        hh.add_message(&app_data_ignored);\n+        hh.add_message(&end_of_early_data_flight);\n+        assert_eq!(hh.current_hash().as_ref(),\n+                   SHA256.hash(b\"\\x0e\\x00\\x00\\x00\\x05\\x00\\x00\\x00\").as_ref());\n+\n+    }\n+\n     #[cfg(feature = \"tls12\")]\n     #[test]\n     fn buffers_correctly() {\n         let mut hhb = HandshakeHashBuffer::new();\n         hhb.set_client_auth_enabled();\n-        hhb.update_raw(b\"hello\");\n+        hhb.add_raw(b\"hello\");\n         assert_eq!(hhb.buffer.len(), 5);\n         let mut hh = hhb.start_hash(&SHA256);\n         assert_eq!(\n@@ -211,7 +259,7 @@ test_for_each_provider! {\n                 .map(|buf| buf.len()),\n             Some(5)\n         );\n-        hh.update_raw(b\"world\");\n+        hh.add_raw(b\"world\");\n         assert_eq!(\n             hh.client_auth\n                 .as_ref()\n@@ -232,7 +280,7 @@ test_for_each_provider! {\n     fn abandon() {\n         let mut hhb = HandshakeHashBuffer::new();\n         hhb.set_client_auth_enabled();\n-        hhb.update_raw(b\"hello\");\n+        hhb.add_raw(b\"hello\");\n         assert_eq!(hhb.buffer.len(), 5);\n         let mut hh = hhb.start_hash(&SHA256);\n         assert_eq!(\n@@ -243,7 +291,7 @@ test_for_each_provider! {\n         );\n         hh.abandon_client_auth();\n         assert_eq!(hh.client_auth, None);\n-        hh.update_raw(b\"world\");\n+        hh.add_raw(b\"world\");\n         assert_eq!(hh.client_auth, None);\n         let h = hh.current_hash();\n         let h = h.as_ref();\n@@ -257,7 +305,7 @@ test_for_each_provider! {\n     fn clones_correctly() {\n         let mut hhb = HandshakeHashBuffer::new();\n         hhb.set_client_auth_enabled();\n-        hhb.update_raw(b\"hello\");\n+        hhb.add_raw(b\"hello\");\n         assert_eq!(hhb.buffer.len(), 5);\n \n         // Cloning the HHB should result in the same buffer and client auth state.\n@@ -266,7 +314,7 @@ test_for_each_provider! {\n         assert!(hhb_prime.client_auth_enabled);\n \n         // Updating the HHB clone shouldn't affect the original.\n-        hhb_prime.update_raw(b\"world\");\n+        hhb_prime.add_raw(b\"world\");\n         assert_eq!(hhb_prime.buffer.len(), 10);\n         assert_ne!(hhb.buffer, hhb_prime.buffer);\n \n@@ -281,7 +329,7 @@ test_for_each_provider! {\n         assert_eq!(hh_hash, hh_prime_hash);\n \n         // Updating the HH clone shouldn't affect the original.\n-        hh_prime.update_raw(b\"goodbye\");\n+        hh_prime.add_raw(b\"goodbye\");\n         assert_eq!(hh.current_hash().as_ref(), hh_hash);\n         assert_ne!(hh_prime.current_hash().as_ref(), hh_hash);\n     }\ndiff --git a/rustls/src/msgs/message/mod.rs b/rustls/src/msgs/message/mod.rs\nindex 2614aeee66c..aeb35c850c1 100644\n--- a/rustls/src/msgs/message/mod.rs\n+++ b/rustls/src/msgs/message/mod.rs\n@@ -19,10 +19,13 @@ pub use outbound::{OutboundChunks, OutboundOpaqueMessage, OutboundPlainMessage,\n #[derive(Debug)]\n pub enum MessagePayload<'a> {\n     Alert(AlertMessagePayload),\n+    // one handshake message, parsed\n     Handshake {\n         parsed: HandshakeMessagePayload<'a>,\n         encoded: Payload<'a>,\n     },\n+    // (potentially) multiple handshake messages, unparsed\n+    HandshakeFlight(Payload<'a>),\n     ChangeCipherSpec(ChangeCipherSpecPayload),\n     ApplicationData(Payload<'a>),\n }\n@@ -32,6 +35,7 @@ impl<'a> MessagePayload<'a> {\n         match self {\n             Self::Alert(x) => x.encode(bytes),\n             Self::Handshake { encoded, .. } => bytes.extend(encoded.bytes()),\n+            Self::HandshakeFlight(x) => bytes.extend(x.bytes()),\n             Self::ChangeCipherSpec(x) => x.encode(bytes),\n             Self::ApplicationData(x) => x.encode(bytes),\n         }\n@@ -69,7 +73,7 @@ impl<'a> MessagePayload<'a> {\n     pub fn content_type(&self) -> ContentType {\n         match self {\n             Self::Alert(_) => ContentType::Alert,\n-            Self::Handshake { .. } => ContentType::Handshake,\n+            Self::Handshake { .. } | Self::HandshakeFlight(_) => ContentType::Handshake,\n             Self::ChangeCipherSpec(_) => ContentType::ChangeCipherSpec,\n             Self::ApplicationData(_) => ContentType::ApplicationData,\n         }\n@@ -83,6 +87,7 @@ impl<'a> MessagePayload<'a> {\n                 parsed: parsed.into_owned(),\n                 encoded: encoded.into_owned(),\n             },\n+            HandshakeFlight(x) => HandshakeFlight(x.into_owned()),\n             ChangeCipherSpec(x) => ChangeCipherSpec(x),\n             ApplicationData(x) => ApplicationData(x.into_owned()),\n         }\ndiff --git a/rustls/src/server/tls12.rs b/rustls/src/server/tls12.rs\nindex dfddf241377..954c71d2661 100644\n--- a/rustls/src/server/tls12.rs\n+++ b/rustls/src/server/tls12.rs\n@@ -12,7 +12,7 @@ use super::common::ActiveCertifiedKey;\n use super::hs::{self, ServerContext};\n use super::server_conn::{ProducesTickets, ServerConfig, ServerConnectionData};\n use crate::check::inappropriate_message;\n-use crate::common_state::{CommonState, HandshakeKind, Side, State};\n+use crate::common_state::{CommonState, HandshakeFlightTls12, HandshakeKind, Side, State};\n use crate::conn::ConnectionRandoms;\n use crate::crypto::ActiveKeyExchange;\n use crate::enums::{AlertDescription, ContentType, HandshakeType, ProtocolVersion};\n@@ -194,9 +194,11 @@ mod client_hello {\n             cx.common.kx_state = KxState::Start(selected_kxg);\n             cx.common.handshake_kind = Some(HandshakeKind::Full);\n \n+            let mut flight = HandshakeFlightTls12::new(&mut self.transcript);\n+\n             self.send_ticket = emit_server_hello(\n+                &mut flight,\n                 &self.config,\n-                &mut self.transcript,\n                 cx,\n                 self.session_id,\n                 self.suite,\n@@ -207,20 +209,21 @@ mod client_hello {\n                 &self.randoms,\n                 self.extra_exts,\n             )?;\n-            emit_certificate(&mut self.transcript, cx.common, server_key.get_cert());\n+            emit_certificate(&mut flight, server_key.get_cert());\n             if let Some(ocsp_response) = ocsp_response {\n-                emit_cert_status(&mut self.transcript, cx.common, ocsp_response);\n+                emit_cert_status(&mut flight, ocsp_response);\n             }\n             let server_kx = emit_server_kx(\n-                &mut self.transcript,\n-                cx.common,\n+                &mut flight,\n                 sigschemes,\n                 selected_kxg,\n                 server_key.get_key(),\n                 &self.randoms,\n             )?;\n-            let doing_client_auth = emit_certificate_req(&self.config, &mut self.transcript, cx)?;\n-            emit_server_hello_done(&mut self.transcript, cx.common);\n+            let doing_client_auth = emit_certificate_req(&mut flight, &self.config)?;\n+            emit_server_hello_done(&mut flight);\n+\n+            flight.finish(cx.common);\n \n             if doing_client_auth {\n                 Ok(Box::new(ExpectCertificate {\n@@ -265,9 +268,10 @@ mod client_hello {\n             }\n \n             self.session_id = *id;\n+            let mut flight = HandshakeFlightTls12::new(&mut self.transcript);\n             self.send_ticket = emit_server_hello(\n+                &mut flight,\n                 &self.config,\n-                &mut self.transcript,\n                 cx,\n                 self.session_id,\n                 self.suite,\n@@ -278,6 +282,7 @@ mod client_hello {\n                 &self.randoms,\n                 self.extra_exts,\n             )?;\n+            flight.finish(cx.common);\n \n             let secrets = ConnectionSecrets::new_resume(\n                 self.randoms,\n@@ -325,8 +330,8 @@ mod client_hello {\n     }\n \n     fn emit_server_hello(\n+        flight: &mut HandshakeFlightTls12<'_>,\n         config: &ServerConfig,\n-        transcript: &mut HandshakeHash,\n         cx: &mut ServerContext<'_>,\n         session_id: SessionId,\n         suite: &'static Tls12CipherSuite,\n@@ -341,60 +346,42 @@ mod client_hello {\n         ep.process_common(config, cx, ocsp_response, hello, resumedata, extra_exts)?;\n         ep.process_tls12(config, hello, using_ems);\n \n-        let sh = Message {\n-            version: ProtocolVersion::TLSv1_2,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::ServerHello,\n-                payload: HandshakePayload::ServerHello(ServerHelloPayload {\n-                    legacy_version: ProtocolVersion::TLSv1_2,\n-                    random: Random::from(randoms.server),\n-                    session_id,\n-                    cipher_suite: suite.common.suite,\n-                    compression_method: Compression::Null,\n-                    extensions: ep.exts,\n-                }),\n+        let sh = HandshakeMessagePayload {\n+            typ: HandshakeType::ServerHello,\n+            payload: HandshakePayload::ServerHello(ServerHelloPayload {\n+                legacy_version: ProtocolVersion::TLSv1_2,\n+                random: Random::from(randoms.server),\n+                session_id,\n+                cipher_suite: suite.common.suite,\n+                compression_method: Compression::Null,\n+                extensions: ep.exts,\n             }),\n         };\n-\n         trace!(\"sending server hello {:?}\", sh);\n-        transcript.add_message(&sh);\n-        cx.common.send_msg(sh, false);\n+        flight.add(sh);\n+\n         Ok(ep.send_ticket)\n     }\n \n     fn emit_certificate(\n-        transcript: &mut HandshakeHash,\n-        common: &mut CommonState,\n+        flight: &mut HandshakeFlightTls12<'_>,\n         cert_chain: &[CertificateDer<'static>],\n     ) {\n-        let c = Message {\n-            version: ProtocolVersion::TLSv1_2,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::Certificate,\n-                payload: HandshakePayload::Certificate(CertificateChain(cert_chain.to_vec())),\n-            }),\n-        };\n-\n-        transcript.add_message(&c);\n-        common.send_msg(c, false);\n+        flight.add(HandshakeMessagePayload {\n+            typ: HandshakeType::Certificate,\n+            payload: HandshakePayload::Certificate(CertificateChain(cert_chain.to_vec())),\n+        });\n     }\n \n-    fn emit_cert_status(transcript: &mut HandshakeHash, common: &mut CommonState, ocsp: &[u8]) {\n-        let c = Message {\n-            version: ProtocolVersion::TLSv1_2,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::CertificateStatus,\n-                payload: HandshakePayload::CertificateStatus(CertificateStatus::new(ocsp)),\n-            }),\n-        };\n-\n-        transcript.add_message(&c);\n-        common.send_msg(c, false);\n+    fn emit_cert_status(flight: &mut HandshakeFlightTls12<'_>, ocsp: &[u8]) {\n+        flight.add(HandshakeMessagePayload {\n+            typ: HandshakeType::CertificateStatus,\n+            payload: HandshakePayload::CertificateStatus(CertificateStatus::new(ocsp)),\n+        });\n     }\n \n     fn emit_server_kx(\n-        transcript: &mut HandshakeHash,\n-        common: &mut CommonState,\n+        flight: &mut HandshakeFlightTls12<'_>,\n         sigschemes: Vec<SignatureScheme>,\n         selected_group: &'static dyn SupportedKxGroup,\n         signing_key: &dyn sign::SigningKey,\n@@ -419,23 +406,16 @@ mod client_hello {\n             dss: DigitallySignedStruct::new(sigscheme, sig),\n         });\n \n-        let m = Message {\n-            version: ProtocolVersion::TLSv1_2,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::ServerKeyExchange,\n-                payload: HandshakePayload::ServerKeyExchange(skx),\n-            }),\n-        };\n-\n-        transcript.add_message(&m);\n-        common.send_msg(m, false);\n+        flight.add(HandshakeMessagePayload {\n+            typ: HandshakeType::ServerKeyExchange,\n+            payload: HandshakePayload::ServerKeyExchange(skx),\n+        });\n         Ok(kx)\n     }\n \n     fn emit_certificate_req(\n+        flight: &mut HandshakeFlightTls12<'_>,\n         config: &ServerConfig,\n-        transcript: &mut HandshakeHash,\n-        cx: &mut ServerContext<'_>,\n     ) -> Result<bool, Error> {\n         let client_auth = &config.verifier;\n \n@@ -459,31 +439,21 @@ mod client_hello {\n             canames: names,\n         };\n \n-        let m = Message {\n-            version: ProtocolVersion::TLSv1_2,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::CertificateRequest,\n-                payload: HandshakePayload::CertificateRequest(cr),\n-            }),\n+        let creq = HandshakeMessagePayload {\n+            typ: HandshakeType::CertificateRequest,\n+            payload: HandshakePayload::CertificateRequest(cr),\n         };\n \n-        trace!(\"Sending CertificateRequest {:?}\", m);\n-        transcript.add_message(&m);\n-        cx.common.send_msg(m, false);\n+        trace!(\"Sending CertificateRequest {:?}\", creq);\n+        flight.add(creq);\n         Ok(true)\n     }\n \n-    fn emit_server_hello_done(transcript: &mut HandshakeHash, common: &mut CommonState) {\n-        let m = Message {\n-            version: ProtocolVersion::TLSv1_2,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::ServerHelloDone,\n-                payload: HandshakePayload::ServerHelloDone,\n-            }),\n-        };\n-\n-        transcript.add_message(&m);\n-        common.send_msg(m, false);\n+    fn emit_server_hello_done(flight: &mut HandshakeFlightTls12<'_>) {\n+        flight.add(HandshakeMessagePayload {\n+            typ: HandshakeType::ServerHelloDone,\n+            payload: HandshakePayload::ServerHelloDone,\n+        });\n     }\n }\n \ndiff --git a/rustls/src/server/tls13.rs b/rustls/src/server/tls13.rs\nindex 919cca361fa..656186ad7f3 100644\n--- a/rustls/src/server/tls13.rs\n+++ b/rustls/src/server/tls13.rs\n@@ -10,7 +10,9 @@ use subtle::ConstantTimeEq;\n use super::hs::{self, HandshakeHashOrBuffer, ServerContext};\n use super::server_conn::ServerConnectionData;\n use crate::check::{inappropriate_handshake_message, inappropriate_message};\n-use crate::common_state::{CommonState, HandshakeKind, Protocol, Side, State};\n+use crate::common_state::{\n+    CommonState, HandshakeFlightTls13, HandshakeKind, Protocol, Side, State,\n+};\n use crate::conn::ConnectionRandoms;\n use crate::enums::{AlertDescription, ContentType, HandshakeType, ProtocolVersion};\n use crate::error::{Error, InvalidMessage, PeerIncompatible, PeerMisbehaved};\n@@ -355,8 +357,9 @@ mod client_hello {\n             }\n \n             let mut ocsp_response = server_key.get_ocsp();\n+            let mut flight = HandshakeFlightTls13::new(&mut self.transcript);\n             let doing_early_data = emit_encrypted_extensions(\n-                &mut self.transcript,\n+                &mut flight,\n                 self.suite,\n                 cx,\n                 &mut ocsp_response,\n@@ -367,28 +370,21 @@ mod client_hello {\n             )?;\n \n             let doing_client_auth = if full_handshake {\n-                let client_auth =\n-                    emit_certificate_req_tls13(&mut self.transcript, cx, &self.config)?;\n+                let client_auth = emit_certificate_req_tls13(&mut flight, &self.config)?;\n \n                 if let Some(compressor) = cert_compressor {\n                     emit_compressed_certificate_tls13(\n-                        &mut self.transcript,\n-                        cx.common,\n+                        &mut flight,\n                         &self.config,\n                         server_key.get_cert(),\n                         ocsp_response,\n                         compressor,\n                     );\n                 } else {\n-                    emit_certificate_tls13(\n-                        &mut self.transcript,\n-                        cx.common,\n-                        server_key.get_cert(),\n-                        ocsp_response,\n-                    );\n+                    emit_certificate_tls13(&mut flight, server_key.get_cert(), ocsp_response);\n                 }\n                 emit_certificate_verify_tls13(\n-                    &mut self.transcript,\n+                    &mut flight,\n                     cx.common,\n                     server_key.get_key(),\n                     &sigschemes_ext,\n@@ -421,13 +417,8 @@ mod client_hello {\n             }\n \n             cx.common.check_aligned_handshake()?;\n-            let key_schedule_traffic = emit_finished_tls13(\n-                &mut self.transcript,\n-                &self.randoms,\n-                cx,\n-                key_schedule,\n-                &self.config,\n-            );\n+            let key_schedule_traffic =\n+                emit_finished_tls13(flight, &self.randoms, cx, key_schedule, &self.config);\n \n             if !doing_client_auth && self.config.send_half_rtt_data {\n                 // Application data can be sent immediately after Finished, in one\n@@ -671,7 +662,7 @@ mod client_hello {\n     }\n \n     fn emit_encrypted_extensions(\n-        transcript: &mut HandshakeHash,\n+        flight: &mut HandshakeFlightTls13<'_>,\n         suite: &'static Tls13CipherSuite,\n         cx: &mut ServerContext<'_>,\n         ocsp_response: &mut Option<&[u8]>,\n@@ -688,23 +679,18 @@ mod client_hello {\n             ep.exts.push(ServerExtension::EarlyData);\n         }\n \n-        let ee = Message {\n-            version: ProtocolVersion::TLSv1_3,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::EncryptedExtensions,\n-                payload: HandshakePayload::EncryptedExtensions(ep.exts),\n-            }),\n+        let ee = HandshakeMessagePayload {\n+            typ: HandshakeType::EncryptedExtensions,\n+            payload: HandshakePayload::EncryptedExtensions(ep.exts),\n         };\n \n         trace!(\"sending encrypted extensions {:?}\", ee);\n-        transcript.add_message(&ee);\n-        cx.common.send_msg(ee, true);\n+        flight.add(ee);\n         Ok(early_data)\n     }\n \n     fn emit_certificate_req_tls13(\n-        transcript: &mut HandshakeHash,\n-        cx: &mut ServerContext<'_>,\n+        flight: &mut HandshakeFlightTls13<'_>,\n         config: &ServerConfig,\n     ) -> Result<bool, Error> {\n         if !config.verifier.offer_client_auth() {\n@@ -739,43 +725,35 @@ mod client_hello {\n                 .push(CertReqExtension::AuthorityNames(authorities.to_vec()));\n         }\n \n-        let m = Message {\n-            version: ProtocolVersion::TLSv1_3,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::CertificateRequest,\n-                payload: HandshakePayload::CertificateRequestTls13(cr),\n-            }),\n+        let creq = HandshakeMessagePayload {\n+            typ: HandshakeType::CertificateRequest,\n+            payload: HandshakePayload::CertificateRequestTls13(cr),\n         };\n \n-        trace!(\"Sending CertificateRequest {:?}\", m);\n-        transcript.add_message(&m);\n-        cx.common.send_msg(m, true);\n+        trace!(\"Sending CertificateRequest {:?}\", creq);\n+        flight.add(creq);\n         Ok(true)\n     }\n \n     fn emit_certificate_tls13(\n-        transcript: &mut HandshakeHash,\n-        common: &mut CommonState,\n+        flight: &mut HandshakeFlightTls13<'_>,\n         cert_chain: &[CertificateDer<'static>],\n         ocsp_response: Option<&[u8]>,\n     ) {\n-        let cert_body = CertificatePayloadTls13::new(cert_chain.iter(), ocsp_response);\n-        let c = Message {\n-            version: ProtocolVersion::TLSv1_3,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::Certificate,\n-                payload: HandshakePayload::CertificateTls13(cert_body),\n-            }),\n+        let cert = HandshakeMessagePayload {\n+            typ: HandshakeType::Certificate,\n+            payload: HandshakePayload::CertificateTls13(CertificatePayloadTls13::new(\n+                cert_chain.iter(),\n+                ocsp_response,\n+            )),\n         };\n \n-        trace!(\"sending certificate {:?}\", c);\n-        transcript.add_message(&c);\n-        common.send_msg(c, true);\n+        trace!(\"sending certificate {:?}\", cert);\n+        flight.add(cert);\n     }\n \n     fn emit_compressed_certificate_tls13(\n-        transcript: &mut HandshakeHash,\n-        common: &mut CommonState,\n+        flight: &mut HandshakeFlightTls13<'_>,\n         config: &ServerConfig,\n         cert_chain: &[CertificateDer<'static>],\n         ocsp_response: Option<&[u8]>,\n@@ -788,29 +766,25 @@ mod client_hello {\n             .compression_for(cert_compressor, &payload)\n         {\n             Ok(entry) => entry,\n-            Err(_) => return emit_certificate_tls13(transcript, common, cert_chain, ocsp_response),\n+            Err(_) => return emit_certificate_tls13(flight, cert_chain, ocsp_response),\n         };\n \n-        let c = Message {\n-            version: ProtocolVersion::TLSv1_3,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::CompressedCertificate,\n-                payload: HandshakePayload::CompressedCertificate(entry.compressed_cert_payload()),\n-            }),\n+        let c = HandshakeMessagePayload {\n+            typ: HandshakeType::CompressedCertificate,\n+            payload: HandshakePayload::CompressedCertificate(entry.compressed_cert_payload()),\n         };\n \n         trace!(\"sending compressed certificate {:?}\", c);\n-        transcript.add_message(&c);\n-        common.send_msg(c, true);\n+        flight.add(c);\n     }\n \n     fn emit_certificate_verify_tls13(\n-        transcript: &mut HandshakeHash,\n+        flight: &mut HandshakeFlightTls13<'_>,\n         common: &mut CommonState,\n         signing_key: &dyn sign::SigningKey,\n         schemes: &[SignatureScheme],\n     ) -> Result<(), Error> {\n-        let message = construct_server_verify_message(&transcript.current_hash());\n+        let message = construct_server_verify_message(&flight.transcript.current_hash());\n \n         let signer = signing_key\n             .choose_scheme(schemes)\n@@ -826,43 +800,36 @@ mod client_hello {\n \n         let cv = DigitallySignedStruct::new(scheme, sig);\n \n-        let m = Message {\n-            version: ProtocolVersion::TLSv1_3,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::CertificateVerify,\n-                payload: HandshakePayload::CertificateVerify(cv),\n-            }),\n+        let cv = HandshakeMessagePayload {\n+            typ: HandshakeType::CertificateVerify,\n+            payload: HandshakePayload::CertificateVerify(cv),\n         };\n \n-        trace!(\"sending certificate-verify {:?}\", m);\n-        transcript.add_message(&m);\n-        common.send_msg(m, true);\n+        trace!(\"sending certificate-verify {:?}\", cv);\n+        flight.add(cv);\n         Ok(())\n     }\n \n     fn emit_finished_tls13(\n-        transcript: &mut HandshakeHash,\n+        mut flight: HandshakeFlightTls13<'_>,\n         randoms: &ConnectionRandoms,\n         cx: &mut ServerContext<'_>,\n         key_schedule: KeyScheduleHandshake,\n         config: &ServerConfig,\n     ) -> KeyScheduleTrafficWithClientFinishedPending {\n-        let handshake_hash = transcript.current_hash();\n+        let handshake_hash = flight.transcript.current_hash();\n         let verify_data = key_schedule.sign_server_finish(&handshake_hash);\n         let verify_data_payload = Payload::new(verify_data.as_ref());\n \n-        let m = Message {\n-            version: ProtocolVersion::TLSv1_3,\n-            payload: MessagePayload::handshake(HandshakeMessagePayload {\n-                typ: HandshakeType::Finished,\n-                payload: HandshakePayload::Finished(verify_data_payload),\n-            }),\n+        let fin = HandshakeMessagePayload {\n+            typ: HandshakeType::Finished,\n+            payload: HandshakePayload::Finished(verify_data_payload),\n         };\n \n-        trace!(\"sending finished {:?}\", m);\n-        transcript.add_message(&m);\n-        let hash_at_server_fin = transcript.current_hash();\n-        cx.common.send_msg(m, true);\n+        trace!(\"sending finished {:?}\", fin);\n+        flight.add(fin);\n+        let hash_at_server_fin = flight.transcript.current_hash();\n+        flight.finish(cx.common);\n \n         // Now move to application data keys.  Read key change is deferred until\n         // the Finish message is received & validated.\n", "instance_id": "rustls__rustls-2120", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the goal of packing multiple handshake messages into a single TLS message to reduce encryption overhead and improve performance. It identifies the current issue (each handshake message is sent in its own TLS message) and the desired outcome (combining messages to reduce overhead). However, it lacks critical details such as specific constraints, expected input/output formats for the messages, and any explicit mention of edge cases or performance benchmarks to achieve. Additionally, there are no examples or detailed requirements about how the packing should be implemented or what specific handshake messages are targeted. This leaves minor ambiguities that could affect the implementation approach, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated as Hard (0.75) due to several factors. First, the scope of code changes is significant, spanning multiple files (`tls13.rs`, `tls12.rs`, `common_state.rs`, `hash_hs.rs`, `message/mod.rs`) and involving core components of the TLS handshake process in the `rustls` library. The modifications require a deep understanding of the codebase architecture, particularly how handshake messages are constructed, encoded, and transmitted. Second, the changes introduce a new abstraction (`HandshakeFlight`) to batch messages, which impacts the system's message handling logic and requires careful integration with existing transcript and encryption mechanisms. Third, the problem demands knowledge of advanced Rust concepts (e.g., generics with const parameters, lifetime management) and domain-specific knowledge of TLS protocols (e.g., handshake message formats, encryption overhead). Fourth, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest potential complexities in handling message ordering, partial failures during batching, and ensuring compatibility with different TLS versions (TLS 1.2 and 1.3). Overall, this task requires substantial expertise in both Rust and TLS, along with careful consideration of performance and correctness, justifying a difficulty score of 0.75.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "new feature:  Add Support for Presigned Deletes in OpenDAL Python Bindings\n### Feature Description\n\nProposed Functionality:\r\n\r\n- Allow generating a `presigned` **delete** URL for objects in supported storage backends.\r\n- Define a configurable expiration time for the `presigned` URL.\r\n- Ensure compatibility with storage services that natively support presigned delete operations (e.g., AWS S3, Azure Blob Storage).\r\n- Provide clear error handling for backends that do not support presigned deletes.\n\n### Problem and Solution\n\nProblem:\r\nCurrently we have support for presigned Reads and Writes with openDAL python bindings, but presigned support for DELETE operations are not yet supported.  Related discussion: https://github.com/apache/opendal/discussions/5423\r\nSolution:\r\nWould like to presigned Delete support to be added to openDAL python bindings with the introduction of this feature. \n\n### Additional Context\n\n_No response_\n\n### Are you willing to contribute to the development of this feature?\n\n- [X] Yes, I am willing to contribute to the development of this feature.\n", "patch": "diff --git a/core/src/raw/ops.rs b/core/src/raw/ops.rs\nindex 12f4a3a0535..b76dbaff127 100644\n--- a/core/src/raw/ops.rs\n+++ b/core/src/raw/ops.rs\n@@ -273,6 +273,8 @@ pub enum PresignOperation {\n     Read(OpRead),\n     /// Presign a write operation.\n     Write(OpWrite),\n+    /// Presign a delete operation.\n+    Delete(OpDelete),\n }\n \n impl From<OpStat> for PresignOperation {\n@@ -293,6 +295,12 @@ impl From<OpWrite> for PresignOperation {\n     }\n }\n \n+impl From<OpDelete> for PresignOperation {\n+    fn from(v: OpDelete) -> Self {\n+        Self::Delete(v)\n+    }\n+}\n+\n /// Args for `read` operation.\n #[derive(Debug, Clone, Default)]\n pub struct OpRead {\ndiff --git a/core/src/services/azblob/backend.rs b/core/src/services/azblob/backend.rs\nindex f2adce968ae..863a586a2b5 100644\n--- a/core/src/services/azblob/backend.rs\n+++ b/core/src/services/azblob/backend.rs\n@@ -634,18 +634,24 @@ impl Access for AzblobBackend {\n     }\n \n     async fn presign(&self, path: &str, args: OpPresign) -> Result<RpPresign> {\n-        let mut req = match args.operation() {\n-            PresignOperation::Stat(v) => self.core.azblob_head_blob_request(path, v)?,\n+        let req = match args.operation() {\n+            PresignOperation::Stat(v) => self.core.azblob_head_blob_request(path, v),\n             PresignOperation::Read(v) => {\n                 self.core\n-                    .azblob_get_blob_request(path, BytesRange::default(), v)?\n+                    .azblob_get_blob_request(path, BytesRange::default(), v)\n             }\n             PresignOperation::Write(_) => {\n                 self.core\n-                    .azblob_put_blob_request(path, None, &OpWrite::default(), Buffer::new())?\n+                    .azblob_put_blob_request(path, None, &OpWrite::default(), Buffer::new())\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         };\n \n+        let mut req = req?;\n+\n         self.core.sign_query(&mut req).await?;\n \n         let (parts, _) = req.into_parts();\ndiff --git a/core/src/services/b2/backend.rs b/core/src/services/b2/backend.rs\nindex e8f4c25130a..d30caf166e6 100644\n--- a/core/src/services/b2/backend.rs\n+++ b/core/src/services/b2/backend.rs\n@@ -471,6 +471,10 @@ impl Access for B2Backend {\n                     parts.headers,\n                 )))\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         }\n     }\n }\ndiff --git a/core/src/services/cos/backend.rs b/core/src/services/cos/backend.rs\nindex b0542a33a34..ae41b343c43 100644\n--- a/core/src/services/cos/backend.rs\n+++ b/core/src/services/cos/backend.rs\n@@ -418,17 +418,22 @@ impl Access for CosBackend {\n     }\n \n     async fn presign(&self, path: &str, args: OpPresign) -> Result<RpPresign> {\n-        let mut req = match args.operation() {\n-            PresignOperation::Stat(v) => self.core.cos_head_object_request(path, v)?,\n+        let req = match args.operation() {\n+            PresignOperation::Stat(v) => self.core.cos_head_object_request(path, v),\n             PresignOperation::Read(v) => {\n                 self.core\n-                    .cos_get_object_request(path, BytesRange::default(), v)?\n+                    .cos_get_object_request(path, BytesRange::default(), v)\n             }\n             PresignOperation::Write(v) => {\n                 self.core\n-                    .cos_put_object_request(path, None, v, Buffer::new())?\n+                    .cos_put_object_request(path, None, v, Buffer::new())\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         };\n+        let mut req = req?;\n         self.core.sign_query(&mut req, args.expire()).await?;\n \n         // We don't need this request anymore, consume it directly.\ndiff --git a/core/src/services/gcs/backend.rs b/core/src/services/gcs/backend.rs\nindex fa2c0156d62..f055cbe0126 100644\n--- a/core/src/services/gcs/backend.rs\n+++ b/core/src/services/gcs/backend.rs\n@@ -514,15 +514,19 @@ impl Access for GcsBackend {\n \n     async fn presign(&self, path: &str, args: OpPresign) -> Result<RpPresign> {\n         // We will not send this request out, just for signing.\n-        let mut req = match args.operation() {\n-            PresignOperation::Stat(v) => self.core.gcs_head_object_xml_request(path, v)?,\n-            PresignOperation::Read(v) => self.core.gcs_get_object_xml_request(path, v)?,\n+        let req = match args.operation() {\n+            PresignOperation::Stat(v) => self.core.gcs_head_object_xml_request(path, v),\n+            PresignOperation::Read(v) => self.core.gcs_get_object_xml_request(path, v),\n             PresignOperation::Write(v) => {\n                 self.core\n-                    .gcs_insert_object_xml_request(path, v, Buffer::new())?\n+                    .gcs_insert_object_xml_request(path, v, Buffer::new())\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         };\n-\n+        let mut req = req?;\n         self.core.sign_query(&mut req, args.expire())?;\n \n         // We don't need this request anymore, consume it directly.\ndiff --git a/core/src/services/obs/backend.rs b/core/src/services/obs/backend.rs\nindex 8aaa99741a9..33ceb922f9f 100644\n--- a/core/src/services/obs/backend.rs\n+++ b/core/src/services/obs/backend.rs\n@@ -410,17 +410,22 @@ impl Access for ObsBackend {\n     }\n \n     async fn presign(&self, path: &str, args: OpPresign) -> Result<RpPresign> {\n-        let mut req = match args.operation() {\n-            PresignOperation::Stat(v) => self.core.obs_head_object_request(path, v)?,\n+        let req = match args.operation() {\n+            PresignOperation::Stat(v) => self.core.obs_head_object_request(path, v),\n             PresignOperation::Read(v) => {\n                 self.core\n-                    .obs_get_object_request(path, BytesRange::default(), v)?\n+                    .obs_get_object_request(path, BytesRange::default(), v)\n             }\n             PresignOperation::Write(v) => {\n                 self.core\n-                    .obs_put_object_request(path, None, v, Buffer::new())?\n+                    .obs_put_object_request(path, None, v, Buffer::new())\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         };\n+        let mut req = req?;\n         self.core.sign_query(&mut req, args.expire()).await?;\n \n         // We don't need this request anymore, consume it directly.\ndiff --git a/core/src/services/oss/backend.rs b/core/src/services/oss/backend.rs\nindex 727e62d72d2..a0a6b1d2e39 100644\n--- a/core/src/services/oss/backend.rs\n+++ b/core/src/services/oss/backend.rs\n@@ -619,14 +619,19 @@ impl Access for OssBackend {\n \n     async fn presign(&self, path: &str, args: OpPresign) -> Result<RpPresign> {\n         // We will not send this request out, just for signing.\n-        let mut req = match args.operation() {\n-            PresignOperation::Stat(v) => self.core.oss_head_object_request(path, true, v)?,\n-            PresignOperation::Read(v) => self.core.oss_get_object_request(path, true, v)?,\n+        let req = match args.operation() {\n+            PresignOperation::Stat(v) => self.core.oss_head_object_request(path, true, v),\n+            PresignOperation::Read(v) => self.core.oss_get_object_request(path, true, v),\n             PresignOperation::Write(v) => {\n                 self.core\n-                    .oss_put_object_request(path, None, v, Buffer::new(), true)?\n+                    .oss_put_object_request(path, None, v, Buffer::new(), true)\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         };\n+        let mut req = req?;\n \n         self.core.sign_query(&mut req, args.expire()).await?;\n \ndiff --git a/core/src/services/s3/backend.rs b/core/src/services/s3/backend.rs\nindex 609db520eb6..cf34524a3d0 100644\n--- a/core/src/services/s3/backend.rs\n+++ b/core/src/services/s3/backend.rs\n@@ -1099,19 +1099,23 @@ impl Access for S3Backend {\n \n     async fn presign(&self, path: &str, args: OpPresign) -> Result<RpPresign> {\n         let (expire, op) = args.into_parts();\n-\n         // We will not send this request out, just for signing.\n-        let mut req = match op {\n-            PresignOperation::Stat(v) => self.core.s3_head_object_request(path, v)?,\n+        let req = match op {\n+            PresignOperation::Stat(v) => self.core.s3_head_object_request(path, v),\n             PresignOperation::Read(v) => {\n                 self.core\n-                    .s3_get_object_request(path, BytesRange::default(), &v)?\n+                    .s3_get_object_request(path, BytesRange::default(), &v)\n             }\n             PresignOperation::Write(_) => {\n                 self.core\n-                    .s3_put_object_request(path, None, &OpWrite::default(), Buffer::new())?\n+                    .s3_put_object_request(path, None, &OpWrite::default(), Buffer::new())\n             }\n+            PresignOperation::Delete(_) => Err(Error::new(\n+                ErrorKind::Unsupported,\n+                \"operation is not supported\",\n+            )),\n         };\n+        let mut req = req?;\n \n         self.core.sign_query(&mut req, expire).await?;\n \ndiff --git a/core/src/types/capability.rs b/core/src/types/capability.rs\nindex c74fe3628f1..ca280ce5ee2 100644\n--- a/core/src/types/capability.rs\n+++ b/core/src/types/capability.rs\n@@ -227,6 +227,8 @@ pub struct Capability {\n     pub presign_stat: bool,\n     /// Indicates if presigned URLs for write operations are supported.\n     pub presign_write: bool,\n+    /// Indicates if presigned URLs for delete operations are supported.\n+    pub presign_delete: bool,\n \n     /// Indicate if the operator supports shared access.\n     pub shared: bool,\ndiff --git a/core/src/types/operator/operator.rs b/core/src/types/operator/operator.rs\nindex 9f23f4ed9d2..c0183465614 100644\n--- a/core/src/types/operator/operator.rs\n+++ b/core/src/types/operator/operator.rs\n@@ -1964,4 +1964,61 @@ impl Operator {\n             },\n         )\n     }\n+\n+    /// Presign an operation for delete.\n+    ///\n+    /// # Notes\n+    ///\n+    /// ## Extra Options\n+    ///\n+    /// `presign_delete` is a wrapper of [`Self::presign_delete_with`] without any options.\n+    ///\n+    /// # Example\n+    ///\n+    /// ```\n+    /// use std::time::Duration;\n+    ///\n+    /// use anyhow::Result;\n+    /// use opendal::Operator;\n+    ///\n+    /// async fn test(op: Operator) -> Result<()> {\n+    ///     let signed_req = op\n+    ///         .presign_delete(\"test.txt\", Duration::from_secs(3600))\n+    ///         .await?;\n+    ///     Ok(())\n+    /// }\n+    /// ```\n+    ///\n+    /// - `signed_req.method()`: `DELETE`\n+    /// - `signed_req.uri()`: `https://s3.amazonaws.com/examplebucket/test.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=access_key_id/20130721/us-east-1/s3/aws4_request&X-Amz-Date=20130721T201207Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=<signature-value>`\n+    /// - `signed_req.headers()`: `{ \"host\": \"s3.amazonaws.com\" }`\n+    ///\n+    /// We can delete file as this file via `curl` or other tools without credential:\n+    ///\n+    /// ```shell\n+    /// curl -X DELETE \"https://s3.amazonaws.com/examplebucket/test.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=access_key_id/20130721/us-east-1/s3/aws4_request&X-Amz-Date=20130721T201207Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=<signature-value>\"\n+    /// ```\n+    pub async fn presign_delete(&self, path: &str, expire: Duration) -> Result<PresignedRequest> {\n+        self.presign_delete_with(path, expire).await\n+    }\n+\n+    /// Presign an operation for delete without extra options.\n+    pub fn presign_delete_with(\n+        &self,\n+        path: &str,\n+        expire: Duration,\n+    ) -> FuturePresignDelete<impl Future<Output = Result<PresignedRequest>>> {\n+        let path = normalize_path(path);\n+\n+        OperatorFuture::new(\n+            self.inner().clone(),\n+            path,\n+            (OpDelete::default(), expire),\n+            |inner, path, (args, dur)| async move {\n+                let op = OpPresign::new(args, dur);\n+                let rp = inner.presign(&path, op).await?;\n+                Ok(rp.into_presigned_request())\n+            },\n+        )\n+    }\n }\ndiff --git a/core/src/types/operator/operator_futures.rs b/core/src/types/operator/operator_futures.rs\nindex b75e5153ce7..c70481720f2 100644\n--- a/core/src/types/operator/operator_futures.rs\n+++ b/core/src/types/operator/operator_futures.rs\n@@ -152,6 +152,13 @@ impl<F: Future<Output = Result<PresignedRequest>>> FuturePresignStat<F> {\n     }\n }\n \n+/// Future that generated by [`Operator::presign_delete_with`].\n+///\n+/// Users can add more options by public functions provided by this struct.\n+pub type FuturePresignDelete<F> = OperatorFuture<(OpDelete, Duration), PresignedRequest, F>;\n+\n+impl<F: Future<Output = Result<PresignedRequest>>> FuturePresignDelete<F> {}\n+\n /// Future that generated by [`Operator::presign_read_with`].\n ///\n /// Users can add more options by public functions provided by this struct.\n", "instance_id": "apache__opendal-5647", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding support for presigned DELETE operations in the OpenDAL Python bindings. It specifies the desired functionality, such as generating presigned URLs for delete operations with configurable expiration times and ensuring compatibility with certain storage backends (e.g., AWS S3, Azure Blob Storage). It also mentions the need for error handling for unsupported backends, which is a critical detail. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats for the presigned delete operation (e.g., what the URL structure should look like or how expiration time is specified in the API). Additionally, there are no examples of usage or specific edge cases mentioned (e.g., handling invalid expiration times or backend-specific quirks). While a related discussion link is provided, it is not embedded with sufficient context to clarify these gaps directly in the statement. Thus, while the overall intent is clear, these minor omissions prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, as it spans multiple files and modules within the OpenDAL core library, affecting various storage backend implementations (e.g., S3, GCS, Azure Blob, etc.). The changes involve adding a new variant to the `PresignOperation` enum, updating capability structs, and implementing presigned delete functionality in the `Operator` struct with associated future types. While the actual code changes per file are relatively small (e.g., adding error handling for unsupported operations in most backends), the breadth of files touched requires a good understanding of the codebase's architecture and how different storage services handle presigned operations. Second, the technical concepts involved include Rust's type system (enums, traits, and futures), asynchronous programming, and domain-specific knowledge of cloud storage APIs and their presigning mechanisms. Understanding how to integrate a new operation type into an existing framework without breaking compatibility is non-trivial. Third, while the problem statement mentions error handling for unsupported backends (which is reflected in the code changes as returning `Unsupported` errors), it does not specify other potential edge cases like invalid paths, expiration time constraints, or backend-specific limitations, which the developer must anticipate and handle. Finally, the impact on the system's architecture is moderate, as it extends an existing feature (presigning) rather than introducing a completely new paradigm, but it still requires careful integration to ensure consistency across backends. Given these factors\u2014a broad but not overly complex scope, moderate technical depth, and some unaddressed edge cases\u2014I assign a difficulty score of 0.65, placing it in the lower end of the \"Hard\" range (0.6-0.8).", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Unexpected warning about a component being neither used nor exported\n### Bug Description\n\nI get a warning in my slint code that I do not expect:\r\n\r\n```\r\nComponent is neither used nor exported [Ln 1, Col 1]\r\n```\n\n### Reproducible Code (if applicable)\n\n```slint\ncomponent Foo /* Foo 1 */ { }\r\n\r\nexport component FooBaz {\r\n    Foo /* <- TEST_ME_1 */ { }\r\n}\r\n\r\ncomponent Foo /* Foo 2 */ { }\r\n\r\nexport component Baz {\r\n    Foo /* <- TEST_ME_2 */ { }\r\n}\n```\n\n\n### Environment Details\n\n- Slint Version: master\r\n- Platform/OS: Linux\r\n- Programming Language: Not applicable\r\n- Backend/Renderer: Not applicable\r\n\n\n### Product Impact\n\nTiny :-)\n", "patch": "diff --git a/internal/compiler/object_tree.rs b/internal/compiler/object_tree.rs\nindex feae872f3cf..87be05f9493 100644\n--- a/internal/compiler/object_tree.rs\n+++ b/internal/compiler/object_tree.rs\n@@ -86,7 +86,9 @@ impl Document {\n              diag: &mut BuildDiagnostics,\n              local_registry: &mut TypeRegister| {\n                 let compo = Component::from_node(n, diag, local_registry);\n-                local_registry.add(compo.clone());\n+                if !local_registry.add(compo.clone()) {\n+                    diag.push_warning(format!(\"Component '{}' is replacing a previously defined component with the same name\", compo.id), &syntax_nodes::Component::from(compo.node.clone().unwrap()).DeclaredIdentifier());\n+                }\n                 inner_components.push(compo);\n             };\n         let process_struct = |n: syntax_nodes::StructDeclaration,\n@@ -102,7 +104,14 @@ impl Document {\n                 parser::identifier_text(&n.DeclaredIdentifier()),\n             );\n             assert!(matches!(ty, Type::Struct(_)));\n-            local_registry.insert_type(ty.clone());\n+            if !local_registry.insert_type(ty.clone()) {\n+                diag.push_warning(\n+                    format!(\n+                        \"Struct '{ty}' is replacing a previously defined type with the same name\"\n+                    ),\n+                    &n.DeclaredIdentifier(),\n+                );\n+            }\n             inner_types.push(ty);\n         };\n         let process_enum = |n: syntax_nodes::EnumDeclaration,\n@@ -132,9 +141,17 @@ impl Document {\n                     }\n                 })\n                 .collect();\n-            let en = Enumeration { name: name.clone(), values, default_value: 0, node: Some(n) };\n+            let en =\n+                Enumeration { name: name.clone(), values, default_value: 0, node: Some(n.clone()) };\n             let ty = Type::Enumeration(Rc::new(en));\n-            local_registry.insert_type_with_name(ty.clone(), name);\n+            if !local_registry.insert_type_with_name(ty.clone(), name.clone()) {\n+                diag.push_warning(\n+                    format!(\n+                        \"Enum '{name}' is replacing a previously defined type with the same name\"\n+                    ),\n+                    &n.DeclaredIdentifier(),\n+                );\n+            }\n             inner_types.push(ty);\n         };\n \n@@ -226,9 +243,7 @@ impl Document {\n             if local_compo.is_global() {\n                 continue;\n             }\n-            // First ref count is in the type registry, the second one in inner_components. Any use of the element\n-            // would have resulted in another strong reference.\n-            if Rc::strong_count(local_compo) == 2 {\n+            if !local_compo.used.get() {\n                 diag.push_warning(\n                     \"Component is neither used nor exported\".into(),\n                     &local_compo.node,\n@@ -377,6 +392,9 @@ pub struct Component {\n     /// if it is a global singleton and exported.\n     pub exported_global_names: RefCell<Vec<ExportedName>>,\n \n+    /// True if this component is used as a sub-component by at least one other component.\n+    pub used: Cell<bool>,\n+\n     /// The list of properties (name and type) declared as private in the component.\n     /// This is used to issue better error in the generated code if the property is used.\n     pub private_properties: RefCell<Vec<(SmolStr, Type)>>,\n@@ -962,6 +980,9 @@ impl Element {\n         };\n         // This isn't truly qualified yet, the enclosing component is added at the end of Component::from_node\n         let qualified_id = (!id.is_empty()).then(|| id.clone());\n+        if let ElementType::Component(c) = &base_type {\n+            c.used.set(true);\n+        }\n         let type_name = base_type\n             .type_name()\n             .filter(|_| base_type != tr.empty_type())\ndiff --git a/internal/compiler/passes/inlining.rs b/internal/compiler/passes/inlining.rs\nindex 052f72ec0bc..0d0a57b8e61 100644\n--- a/internal/compiler/passes/inlining.rs\n+++ b/internal/compiler/passes/inlining.rs\n@@ -403,6 +403,7 @@ fn duplicate_sub_component(\n         popup_windows: Default::default(),\n         timers: component_to_duplicate.timers.clone(),\n         exported_global_names: component_to_duplicate.exported_global_names.clone(),\n+        used: component_to_duplicate.used.clone(),\n         private_properties: Default::default(),\n         inherits_popup_window: core::cell::Cell::new(false),\n     };\ndiff --git a/internal/compiler/typeloader.rs b/internal/compiler/typeloader.rs\nindex 54a64d5f28f..ab39547f587 100644\n--- a/internal/compiler/typeloader.rs\n+++ b/internal/compiler/typeloader.rs\n@@ -350,6 +350,7 @@ impl Snapshotter {\n                 exported_global_names: RefCell::new(\n                     component.exported_global_names.borrow().clone(),\n                 ),\n+                used: component.used.clone(),\n                 init_code: RefCell::new(component.init_code.borrow().clone()),\n                 inherits_popup_window: std::cell::Cell::new(component.inherits_popup_window.get()),\n                 optimized_elements,\n@@ -1455,7 +1456,7 @@ impl TypeLoader {\n                 itertools::Either::Right(ty) => registry_to_populate\n                     .borrow_mut()\n                     .insert_type_with_name(ty, import_name.internal_name),\n-            }\n+            };\n         }\n     }\n \ndiff --git a/internal/compiler/typeregister.rs b/internal/compiler/typeregister.rs\nindex 72d5428ffff..27b37388f2c 100644\n--- a/internal/compiler/typeregister.rs\n+++ b/internal/compiler/typeregister.rs\n@@ -356,12 +356,17 @@ impl TypeRegister {\n         }\n     }\n \n-    /// FIXME: same as 'add' ?\n-    pub fn insert_type(&mut self, t: Type) {\n-        self.types.insert(t.to_smolstr(), t);\n+    /// Insert a type into the type register with its builtin type name.\n+    ///\n+    /// Returns false if a it replaced an existing type.\n+    pub fn insert_type(&mut self, t: Type) -> bool {\n+        self.types.insert(t.to_smolstr(), t).is_none()\n     }\n-    pub fn insert_type_with_name(&mut self, t: Type, name: SmolStr) {\n-        self.types.insert(name, t);\n+    /// Insert a type into the type register with a specified name.\n+    ///\n+    /// Returns false if a it replaced an existing type.\n+    pub fn insert_type_with_name(&mut self, t: Type, name: SmolStr) -> bool {\n+        self.types.insert(name, t).is_none()\n     }\n \n     fn builtin_internal() -> Self {\n@@ -642,12 +647,18 @@ impl TypeRegister {\n         self.lookup(qualified[0].as_ref())\n     }\n \n-    pub fn add(&mut self, comp: Rc<Component>) {\n-        self.add_with_name(comp.id.clone(), comp);\n+    /// Add the component with it's defined name\n+    ///\n+    /// Returns false if there was already an element with the same name\n+    pub fn add(&mut self, comp: Rc<Component>) -> bool {\n+        self.add_with_name(comp.id.clone(), comp)\n     }\n \n-    pub fn add_with_name(&mut self, name: SmolStr, comp: Rc<Component>) {\n-        self.elements.insert(name, ElementType::Component(comp));\n+    /// Add the component with a specified name\n+    ///\n+    /// Returns false if there was already an element with the same name\n+    pub fn add_with_name(&mut self, name: SmolStr, comp: Rc<Component>) -> bool {\n+        self.elements.insert(name, ElementType::Component(comp)).is_none()\n     }\n \n     pub fn add_builtin(&mut self, builtin: Rc<BuiltinElement>) {\n", "instance_id": "slint-ui__slint-7261", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an unexpected warning in Slint code about a component being neither used nor exported. It provides a reproducible code snippet that demonstrates the issue, along with environment details such as the Slint version and platform. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior (i.e., under what conditions the warning should or should not appear) beyond stating the warning is unexpected. Additionally, it lacks discussion of potential edge cases or specific constraints that might influence the solution. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (`object_tree.rs`, `inlining.rs`, `typeloader.rs`, `typeregister.rs`) within the Slint compiler codebase, indicating a need to understand interactions between different modules. The changes involve modifying how components and types are registered and tracked for usage, which requires a moderate understanding of the internal architecture of the Slint compiler, particularly around type registration and component lifecycle management. \n\nThe technical concepts involved include Rust's ownership model (e.g., `Rc`, `Cell`, `RefCell`), component tracking logic, and diagnostic reporting mechanisms. These are not overly complex for a senior engineer but do require familiarity with Rust's memory management and the specific domain of UI compiler internals. The code changes are not trivial; they introduce a new `used` field to track component usage and modify type registration to return a boolean indicating whether a type was replaced, along with issuing warnings for duplicates. This suggests a moderate level of complexity in ensuring correctness across the codebase.\n\nRegarding edge cases and error handling, the problem statement does not explicitly mention specific scenarios to handle, but the code changes imply a need to correctly manage component redefinition and usage tracking, which could have subtle edge cases (e.g., nested components, re-exported components). The impact on the system's architecture is limited, as the changes are mostly additive and focused on improving diagnostics rather than altering core functionality. Overall, this problem requires a solid understanding of the codebase and careful implementation across multiple files, placing it in the medium difficulty range at 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Popups should expose their accessibility tree\n### Bug Description\n\nWhen a popup element is shown, its accessibility tree doesn't seem to be merged with the parent one. I would expect that the root element of the popup would be added as a child of the element which contains the popup.\r\n\r\nThis is not even a case of keyboard focus not moving properly, the child elements of any popup are not part of the accessibility tree, even if they are configured to be.\n\n### Reproducible Code (if applicable)\n\n```slint\nThe issue can easily be reproduced with the todo example:\r\n\r\n1. Open Accessibility Insights for Windows\r\n2. Run the todo example: `cargo run -p todo`\r\n3. Click on the Close button of the example window to make the confirmation popup appear\r\n4. Observe in Accessibility Insights that the content of the popup cannot be found\n```\n\n\n### Environment Details\n\n- Slint Version: commit d6f83a2af663de2128104fe4241e8b12f308db8d\r\n- Platform/OS: Windows 11, Windows 10\r\n- Programming Language: Rust\r\n- Backend/Renderer: winit\r\n\n\n### Product Impact\n\nI'm working on supporting combo boxes in AccessKit. Not being able to interact with the list of possible choices that appear when expanding the combo box is a blocker.\n", "patch": "diff --git a/internal/backends/winit/accesskit.rs b/internal/backends/winit/accesskit.rs\nindex ded7c676670..8fc5f134da3 100644\n--- a/internal/backends/winit/accesskit.rs\n+++ b/internal/backends/winit/accesskit.rs\n@@ -13,8 +13,8 @@ use i_slint_core::accessibility::{\n use i_slint_core::api::Window;\n use i_slint_core::item_tree::{ItemTreeRc, ItemTreeRef, ItemTreeWeak};\n use i_slint_core::items::{ItemRc, WindowItem};\n-use i_slint_core::lengths::ScaleFactor;\n-use i_slint_core::window::WindowInner;\n+use i_slint_core::lengths::{LogicalPoint, ScaleFactor};\n+use i_slint_core::window::{PopupWindowLocation, WindowInner};\n use i_slint_core::SharedString;\n use i_slint_core::{properties::PropertyTracker, window::WindowAdapter};\n \n@@ -195,7 +195,11 @@ impl AccessKitAdapter {\n                         let scale_factor = ScaleFactor::new(window.scale_factor());\n                         let item = self.nodes.item_rc_for_node_id(cached_node.id)?;\n \n-                        let mut node = self.nodes.build_node_without_children(&item, scale_factor);\n+                        let mut node = self.nodes.build_node_without_children(\n+                            &item,\n+                            scale_factor,\n+                            Default::default(),\n+                        );\n \n                         node.set_children(cached_node.children.clone());\n \n@@ -226,6 +230,18 @@ impl AccessKitAdapter {\n     }\n }\n \n+fn accessible_parent_for_item_rc(mut item: ItemRc) -> ItemRc {\n+    while !item.is_accessible() {\n+        if let Some(parent) = item.parent_item() {\n+            item = parent;\n+        } else {\n+            break;\n+        }\n+    }\n+\n+    item\n+}\n+\n struct NodeCollection {\n     next_component_id: u32,\n     components_by_id: HashMap<u32, ItemTreeWeak>,\n@@ -235,19 +251,7 @@ struct NodeCollection {\n }\n \n impl NodeCollection {\n-    fn accessible_parent_for_item_rc(&self, mut item: ItemRc) -> ItemRc {\n-        while !item.is_accessible() {\n-            if let Some(parent) = item.parent_item() {\n-                item = parent;\n-            } else {\n-                break;\n-            }\n-        }\n-\n-        item\n-    }\n-\n-    fn focus_node(&self, window_adapter_weak: &Weak<WinitWindowAdapter>) -> NodeId {\n+    fn focus_node(&mut self, window_adapter_weak: &Weak<WinitWindowAdapter>) -> NodeId {\n         window_adapter_weak\n             .upgrade()\n             .filter(|window_adapter| {\n@@ -260,7 +264,7 @@ impl NodeCollection {\n                     .borrow()\n                     .upgrade()\n                     .map(|focus_item| {\n-                        let parent = self.accessible_parent_for_item_rc(focus_item);\n+                        let parent = accessible_parent_for_item_rc(focus_item);\n                         parent\n                             .accessible_string_property(AccessibleStringProperty::DelegateFocus)\n                             .and_then(|s| s.parse::<usize>().ok())\n@@ -274,7 +278,7 @@ impl NodeCollection {\n                             .try_component()\n                             .map(|component_rc| ItemRc::new(component_rc, 0))\n                     })\n-                    .and_then(|focus_item| self.find_node_id_by_item_rc(focus_item))\n+                    .map(|focus_item| self.find_node_id_by_item_rc(focus_item))\n             })\n             .unwrap_or_else(|| self.root_node_id)\n     }\n@@ -286,47 +290,76 @@ impl NodeCollection {\n         Some(ItemRc::new(component, index))\n     }\n \n-    fn find_node_id_by_item_rc(&self, mut item: ItemRc) -> Option<NodeId> {\n-        item = self.accessible_parent_for_item_rc(item);\n+    fn find_node_id_by_item_rc(&mut self, mut item: ItemRc) -> NodeId {\n+        item = accessible_parent_for_item_rc(item);\n \n         self.encode_item_node_id(&item)\n     }\n \n-    fn encode_item_node_id(&self, item: &ItemRc) -> Option<NodeId> {\n+    fn encode_item_node_id(&mut self, item: &ItemRc) -> NodeId {\n         let component = item.item_tree();\n         let component_ptr = ItemTreeRef::as_ptr(ItemTreeRc::borrow(component));\n-        let component_id = *(self.component_ids.get(&component_ptr)?);\n+        let component_id = match self.component_ids.get(&component_ptr) {\n+            Some(&component_id) => component_id,\n+            None => {\n+                let component_id = self.next_component_id;\n+                self.next_component_id += 1;\n+                self.component_ids.insert(component_ptr, component_id);\n+                self.components_by_id.insert(component_id, ItemTreeRc::downgrade(component));\n+                component_id\n+            }\n+        };\n+\n         let index = item.index();\n-        Some(NodeId((component_id as u64) << u32::BITS | (index as u64 & u32::MAX as u64)))\n+        NodeId((component_id as u64) << u32::BITS | (index as u64 & u32::MAX as u64))\n     }\n \n     fn build_node_for_item_recursively(\n         &mut self,\n         item: ItemRc,\n         nodes: &mut Vec<(NodeId, Node)>,\n+        popups: &[AccessiblePopup],\n         scale_factor: ScaleFactor,\n+        window_position: LogicalPoint,\n     ) -> NodeId {\n         let tracker = Box::pin(PropertyTracker::default());\n \n-        let mut node =\n-            tracker.as_ref().evaluate(|| self.build_node_without_children(&item, scale_factor));\n+        let mut node = tracker\n+            .as_ref()\n+            .evaluate(|| self.build_node_without_children(&item, scale_factor, window_position));\n+\n+        let id = self.encode_item_node_id(&item);\n+\n+        let popup_child = popups.iter().find_map(|popup| {\n+            if popup.parent_node != id {\n+                return None;\n+            }\n+\n+            let popup_item = ItemRc::new(popup.component.clone(), 0);\n+            Some(self.build_node_for_item_recursively(\n+                popup_item,\n+                nodes,\n+                popups,\n+                scale_factor,\n+                popup.location,\n+            ))\n+        });\n \n         let children = i_slint_core::accessibility::accessible_descendents(&item)\n-            .map(|child| self.build_node_for_item_recursively(child, nodes, scale_factor))\n+            .map(|child| {\n+                self.build_node_for_item_recursively(\n+                    child,\n+                    nodes,\n+                    popups,\n+                    scale_factor,\n+                    window_position,\n+                )\n+            })\n+            .chain(popup_child)\n             .collect::<Vec<NodeId>>();\n \n         node.set_children(children.clone());\n \n-        let component = item.item_tree();\n-        let component_ptr = ItemTreeRef::as_ptr(ItemTreeRc::borrow(component));\n-        if !self.component_ids.contains_key(&component_ptr) {\n-            let component_id = self.next_component_id;\n-            self.next_component_id += 1;\n-            self.component_ids.insert(component_ptr, component_id);\n-            self.components_by_id.insert(component_id, ItemTreeRc::downgrade(component));\n-        }\n-\n-        let id = self.encode_item_node_id(&item).unwrap();\n         self.all_nodes.push(CachedNode { id, children, tracker });\n \n         nodes.push((id, node));\n@@ -351,6 +384,25 @@ impl NodeCollection {\n \n         let root_item = ItemRc::new(window_inner.component(), 0);\n \n+        let popups = window_inner\n+            .active_popups()\n+            .iter()\n+            .filter_map(|popup| {\n+                let PopupWindowLocation::ChildWindow(location) = popup.location else {\n+                    return None;\n+                };\n+\n+                let parent_item = accessible_parent_for_item_rc(popup.parent_item.upgrade()?);\n+                let parent_node = self.encode_item_node_id(if parent_item.is_accessible() {\n+                    &parent_item\n+                } else {\n+                    &root_item\n+                });\n+\n+                Some(AccessiblePopup { location, parent_node, component: popup.component.clone() })\n+            })\n+            .collect::<Vec<_>>();\n+\n         self.all_nodes.clear();\n         let mut nodes = Vec::new();\n \n@@ -358,7 +410,9 @@ impl NodeCollection {\n             self.build_node_for_item_recursively(\n                 root_item,\n                 &mut nodes,\n+                &popups,\n                 ScaleFactor::new(window.scale_factor()),\n+                Default::default(),\n             )\n         });\n         self.root_node_id = root_id;\n@@ -370,7 +424,12 @@ impl NodeCollection {\n         }\n     }\n \n-    fn build_node_without_children(&self, item: &ItemRc, scale_factor: ScaleFactor) -> Node {\n+    fn build_node_without_children(\n+        &self,\n+        item: &ItemRc,\n+        scale_factor: ScaleFactor,\n+        window_position: LogicalPoint,\n+    ) -> Node {\n         let is_checkable = item\n             .accessible_string_property(AccessibleStringProperty::Checkable)\n             .is_some_and(|x| x == \"true\");\n@@ -425,7 +484,7 @@ impl NodeCollection {\n         }\n \n         let geometry = item.geometry();\n-        let absolute_origin = item.map_to_window(geometry.origin);\n+        let absolute_origin = item.map_to_window(geometry.origin) + window_position.to_vector();\n         let physical_origin = (absolute_origin * scale_factor).cast::<f64>();\n         let physical_size = (geometry.size * scale_factor).cast::<f64>();\n         node.set_bounds(accesskit::Rect {\n@@ -595,3 +654,9 @@ impl DeferredAccessKitAction {\n         }\n     }\n }\n+\n+struct AccessiblePopup {\n+    location: LogicalPoint,\n+    parent_node: NodeId,\n+    component: ItemTreeRc,\n+}\ndiff --git a/internal/core/window.rs b/internal/core/window.rs\nindex cbe14c10266..4d359842ebe 100644\n--- a/internal/core/window.rs\n+++ b/internal/core/window.rs\n@@ -373,7 +373,8 @@ impl crate::properties::PropertyDirtyHandler for WindowRedrawTracker {\n }\n \n /// This enum describes the different ways a popup can be rendered by the back-end.\n-enum PopupWindowLocation {\n+#[derive(Clone)]\n+pub enum PopupWindowLocation {\n     /// The popup is rendered in its own top-level window that is know to the windowing system.\n     TopLevel(Rc<dyn WindowAdapter>),\n     /// The popup is rendered as an embedded child window at the given position.\n@@ -382,17 +383,20 @@ enum PopupWindowLocation {\n \n /// This structure defines a graphical element that is designed to pop up from the surrounding\n /// UI content, for example to show a context menu.\n-struct PopupWindow {\n+#[derive(Clone)]\n+pub struct PopupWindow {\n     /// The ID of the associated popup.\n     popup_id: NonZeroU32,\n     /// The location defines where the pop up is rendered.\n-    location: PopupWindowLocation,\n+    pub location: PopupWindowLocation,\n     /// The component that is responsible for providing the popup content.\n-    component: ItemTreeRc,\n+    pub component: ItemTreeRc,\n     /// Defines the close behaviour of the popup.\n     close_policy: PopupClosePolicy,\n     /// the item that had the focus in the parent window when the popup was opened\n     focus_item_in_parent: ItemWeak,\n+    /// The item from where the Popup was invoked from\n+    pub parent_item: ItemWeak,\n }\n \n #[pin_project::pin_project]\n@@ -552,6 +556,11 @@ impl WindowInner {\n         self.component.borrow().upgrade()\n     }\n \n+    /// Returns a slice of the active poppups.\n+    pub fn active_popups(&self) -> core::cell::Ref<'_, [PopupWindow]> {\n+        core::cell::Ref::map(self.active_popups.borrow(), |v| v.as_slice())\n+    }\n+\n     /// Receive a mouse event and pass it to the items of the component to\n     /// change their state.\n     ///\n@@ -1103,6 +1112,7 @@ impl WindowInner {\n             component: popup_componentrc.clone(),\n             close_policy,\n             focus_item_in_parent: focus_item,\n+            parent_item: parent_item.downgrade(),\n         });\n \n         popup_id\n", "instance_id": "slint-ui__slint-7265", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: popups in a UI framework (Slint) do not properly integrate their accessibility tree with the parent window's tree, which is critical for accessibility tools like Accessibility Insights for Windows. The goal is evident\u2014ensure that popup elements are accessible as part of the parent tree. The statement includes a reproducible example using the \"todo\" example, environment details (Slint version, OS, backend), and the impact on supporting combo boxes in AccessKit. However, there are minor ambiguities: the problem does not explicitly define what \"merging with the parent tree\" entails in technical terms (e.g., specific accessibility API expectations or standards like ARIA roles). Additionally, edge cases or specific constraints (e.g., types of popups, nested popups, or performance implications) are not mentioned, which could affect the solution's scope. Overall, it is clear enough to understand the intent but lacks some finer details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes involves multiple files (`accesskit.rs` and `window.rs`) and touches core accessibility logic in a UI framework, requiring a deep understanding of the Slint codebase's architecture, particularly how popups and accessibility trees are managed. The changes are not trivial\u2014they involve modifying data structures (e.g., adding `AccessiblePopup`), adjusting how nodes are built recursively with popup integration, and handling window positioning logic, which impacts the rendering and accessibility hierarchy. Second, the technical concepts required include Rust's ownership and borrowing patterns, familiarity with accessibility APIs (AccessKit), and domain-specific knowledge of UI rendering and popup mechanics (e.g., `PopupWindowLocation`, `LogicalPoint`). Third, while edge cases are not explicitly mentioned in the problem statement, the code changes suggest potential complexities like handling multiple active popups, nested structures, or incorrect parent-child relationships in the accessibility tree, which could require additional error handling or validation. Finally, the impact on the system is significant as it affects a core feature (accessibility), which is critical for user interaction and compliance with standards. However, it does not reach the \"Very Hard\" range (0.8-1.0) as it does not appear to involve system-level redesign or extremely intricate algorithms\u2014just a focused but complex feature enhancement. Thus, a score of 0.65 reflects the challenging nature of understanding and modifying the accessibility integration while balancing potential edge cases and system impact.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Qt backend: Escape does not close PopupWindow\n### Bug Description\n\nThe escape key should close a PopupWindow. It does with winit and WASM, but nothing happens with the Qt backend.\n\n### Reproducible Code (if applicable)\n\n```slint\nexport component AppWindow inherits Window {\r\n    width: 100px;\r\n    height: 100px;\r\n\r\n    popup := PopupWindow {\r\n        Rectangle {\r\n            background: yellow;\r\n        }\r\n\r\n        x: 10px;\r\n        y: 10px;\r\n        height: 50px;\r\n        width: 50px;\r\n    }\r\n\r\n\r\n    TouchArea {\r\n        clicked => {\r\n            popup.show();\r\n        }\r\n    }\r\n}\n```\n\n\n### Environment Details\n\n- Slint Version: 1.9.1\r\n- Platform/OS: Linux\r\n- Backend/Renderer: Qt\r\n\n\n### Product Impact\n\n_No response_\n", "patch": "diff --git a/internal/core/window.rs b/internal/core/window.rs\nindex b61207858dc..ce4fa58ec89 100644\n--- a/internal/core/window.rs\n+++ b/internal/core/window.rs\n@@ -746,7 +746,18 @@ impl WindowInner {\n             && event.text.starts_with(key_codes::Escape)\n         {\n             // Closes top most popup on esc key pressed when policy is not no-auto-close\n-            let close_on_escape = if let Some(popup) = self.active_popups.borrow().last() {\n+\n+            // Try to get the parent window in case `self` is the popup itself\n+            let mut adapter = self.window_adapter();\n+            let item_tree = self.component();\n+            let mut a = None;\n+            ItemTreeRc::borrow_pin(&item_tree).as_ref().window_adapter(false, &mut a);\n+            if let Some(a) = a {\n+                adapter = a;\n+            }\n+            let window = WindowInner::from_pub(adapter.window());\n+\n+            let close_on_escape = if let Some(popup) = window.active_popups.borrow().last() {\n                 popup.close_policy == PopupClosePolicy::CloseOnClick\n                     || popup.close_policy == PopupClosePolicy::CloseOnClickOutside\n             } else {\n@@ -754,7 +765,7 @@ impl WindowInner {\n             };\n \n             if close_on_escape {\n-                self.close_top_popup();\n+                window.close_top_popup();\n             }\n         }\n         crate::properties::ChangeTracker::run_change_handlers();\n", "instance_id": "slint-ui__slint-7356", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the escape key does not close a PopupWindow in the Qt backend, while it works as expected in other backends (winit and WASM). The goal is evident\u2014fix the behavior for the Qt backend to match the others. A reproducible code snippet is provided, which helps in understanding the context of a PopupWindow. Environment details such as Slint version, platform, and backend are also specified, which aids in scoping the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the expected behavior beyond \"closing the PopupWindow\" (e.g., should it close only the topmost popup or all popups?). Additionally, edge cases or specific constraints (e.g., behavior when multiple popups are open or when certain close policies are set) are not discussed. These omissions prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows that the fix is localized to a single file (`window.rs`) and involves a relatively small change (about 10-15 lines of code). The modification does not impact the broader system architecture but focuses on adjusting the logic for handling the escape key to close a popup. It requires identifying the correct window context (whether the current window is a popup or parent) and updating the reference to ensure the correct window's popup is closed. This is a straightforward bug fix without widespread impact on the codebase.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of Rust (specifically, borrowing and ownership with `Rc` and `RefCell` as seen in `ItemTreeRc` and `borrow_pin`), familiarity with the Slint framework's window and popup handling mechanisms, and knowledge of event handling logic. These concepts are not overly complex for someone with moderate experience in Rust or GUI frameworks. No advanced algorithms, design patterns, or domain-specific knowledge beyond GUI event handling are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change hints at considerations like ensuring the correct window context (popup vs. parent window) and respecting the popup's close policy. The diff shows logic to handle whether a popup exists and whether it should close based on its policy. However, these edge cases are relatively simple and do not introduce significant complexity. No extensive error handling modifications are required beyond what's already in the diff.\n\n4. **Overall Complexity**: The fix involves understanding a specific part of the codebase (window and popup management) and making a targeted change to correct the behavior for the Qt backend. It does not require deep architectural knowledge or complex refactoring. The primary challenge lies in correctly navigating the window adapter and popup references, which is a moderate but not overly difficult task.\n\nGiven these factors, a difficulty score of 0.35 is appropriate. It is slightly above the lower end of the \"Easy\" range due to the need to understand the Slint framework's window handling and ensure the correct context is used, but it remains a relatively simple bug fix that does not demand advanced skills or extensive codebase knowledge.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "feat(cli): better error message when failed to resolve non explicit CJS module from ESM\n# Summary\r\n\r\npart of \r\n- https://github.com/denoland/deno/issues/26652\r\n\r\nI think it covers almost use cases, while it's not as strict as the [Node.js implementation](https://github.com/nodejs/node/blob/c5a13605cbbf780345ddfdcb2900ff5ba249fac1/lib/internal/modules/esm/resolve.js#L1018-L1030).\r\nAlthough this is a somewhat complex configuration, it is not expected to have any performance impact on runtime.\r\n\r\n# Changes\r\n\r\n- app.tsx\r\n\r\n```typescript\r\nimport ReactDomServer from 'npm:react-dom@16.8.5/server'\r\nconsole.log(ReactDomServer);\r\n```\r\n\r\nbefore\r\n```bash\r\n% deno run app.tsx\r\nerror: Unable to load /path/to/Library/Caches/deno/npm/registry.npmjs.org/react-dom/16.8.5/server imported from file:///path/to/app.tsx\r\n\r\nCaused by:\r\n    No such file or directory (os error 2)\r\n```\r\n\r\nafter\r\n```bash\r\n% /path/to/deno/target/debug/deno run app.tsx\r\nerror: Unable to load /path/to/Library/Caches/deno/npm/registry.npmjs.org/react-dom/16.8.5/server imported from file:///path/to/app.tsx\r\n\r\nCaused by:\r\n    No such file or directory (os error 2)\r\n\r\n    info: The module that you are trying to import seems to be a CommonJS.\r\n          However it is not listed in the exports of package.json.\r\n          So it cannot be statically resolved when imported from ESM.\r\n          Consider to include an extension of the module.\r\n    hint: Did you mean to import \"react-dom/server.js\"?\r\n```\r\n\r\n\n", "patch": "diff --git a/resolvers/node/errors.rs b/resolvers/node/errors.rs\nindex 85ab5ca444deee..ff80294c046be2 100644\n--- a/resolvers/node/errors.rs\n+++ b/resolvers/node/errors.rs\n@@ -198,6 +198,7 @@ impl NodeJsErrorCoded for PackageSubpathResolveError {\n       PackageSubpathResolveErrorKind::PkgJsonLoad(e) => e.code(),\n       PackageSubpathResolveErrorKind::Exports(e) => e.code(),\n       PackageSubpathResolveErrorKind::LegacyResolve(e) => e.code(),\n+      PackageSubpathResolveErrorKind::FinalizeResolution(e) => e.code(),\n     }\n   }\n }\n@@ -216,6 +217,9 @@ pub enum PackageSubpathResolveErrorKind {\n   #[class(inherit)]\n   #[error(transparent)]\n   LegacyResolve(LegacyResolveError),\n+  #[class(inherit)]\n+  #[error(transparent)]\n+  FinalizeResolution(#[from] FinalizeResolutionError),\n }\n \n #[derive(Debug, Error, JsError)]\n@@ -551,16 +555,18 @@ impl NodeJsErrorCoded for FinalizeResolutionError {\n #[derive(Debug, Error, JsError)]\n #[class(generic)]\n #[error(\n-  \"[{}] Cannot find {} '{}'{}\",\n+  \"[{}] Cannot find {} '{}'{}{}\",\n   self.code(),\n   typ,\n   specifier,\n-  maybe_referrer.as_ref().map(|referrer| format!(\" imported from '{}'\", referrer)).unwrap_or_default()\n+  maybe_referrer.as_ref().map(|referrer| format!(\" imported from '{}'\", referrer)).unwrap_or_default(),\n+  suggested_ext.as_ref().map(|m| format!(\"\\nDid you mean to import with the \\\".{}\\\" extension?\", m)).unwrap_or_default()\n )]\n pub struct ModuleNotFoundError {\n   pub specifier: UrlOrPath,\n   pub maybe_referrer: Option<UrlOrPath>,\n   pub typ: &'static str,\n+  pub suggested_ext: Option<&'static str>,\n }\n \n impl NodeJsErrorCoded for ModuleNotFoundError {\n@@ -572,14 +578,16 @@ impl NodeJsErrorCoded for ModuleNotFoundError {\n #[derive(Debug, Error, JsError)]\n #[class(generic)]\n #[error(\n-  \"[{}] Directory import '{}' is not supported resolving ES modules{}\",\n+  \"[{}] Directory import '{}' is not supported resolving ES modules{}{}\",\n   self.code(),\n   dir_url,\n   maybe_referrer.as_ref().map(|referrer| format!(\" imported from '{}'\", referrer)).unwrap_or_default(),\n+  suggested_file_name.map(|file_name| format!(\"\\nDid you mean to import {file_name} within the directory?\")).unwrap_or_default(),\n )]\n pub struct UnsupportedDirImportError {\n   pub dir_url: UrlOrPath,\n   pub maybe_referrer: Option<UrlOrPath>,\n+  pub suggested_file_name: Option<&'static str>,\n }\n \n impl NodeJsErrorCoded for UnsupportedDirImportError {\ndiff --git a/resolvers/node/resolution.rs b/resolvers/node/resolution.rs\nindex 481e4d679f581f..ea982df17e52b0 100644\n--- a/resolvers/node/resolution.rs\n+++ b/resolvers/node/resolution.rs\n@@ -9,6 +9,7 @@ use anyhow::bail;\n use anyhow::Error as AnyError;\n use deno_media_type::MediaType;\n use deno_package_json::PackageJson;\n+use deno_path_util::url_to_file_path;\n use serde_json::Map;\n use serde_json::Value;\n use sys_traits::FileType;\n@@ -143,6 +144,39 @@ impl NodeResolution {\n   }\n }\n \n+struct LocalPath {\n+  path: PathBuf,\n+  known_exists: bool,\n+}\n+\n+enum LocalUrlOrPath {\n+  Url(Url),\n+  Path(LocalPath),\n+}\n+\n+impl LocalUrlOrPath {\n+  pub fn into_url_or_path(self) -> UrlOrPath {\n+    match self {\n+      LocalUrlOrPath::Url(url) => UrlOrPath::Url(url),\n+      LocalUrlOrPath::Path(local_path) => UrlOrPath::Path(local_path.path),\n+    }\n+  }\n+}\n+\n+/// This struct helps ensure we remember to probe for\n+/// declaration files and to prevent accidentally probing\n+/// multiple times.\n+struct MaybeTypesResolvedUrl(LocalUrlOrPath);\n+\n+/// Kind of method that resolution suceeded with.\n+enum ResolvedMethod {\n+  Url,\n+  RelativeOrAbsolute,\n+  PackageImports,\n+  PackageExports,\n+  PackageSubPath,\n+}\n+\n #[allow(clippy::disallowed_types)]\n pub type NodeResolverRc<\n   TInNpmPackageChecker,\n@@ -262,7 +296,7 @@ impl<\n       .conditions_from_resolution_mode\n       .resolve(resolution_mode);\n     let referrer = UrlOrPathRef::from_url(referrer);\n-    let url = self.module_resolve(\n+    let (url, resolved_kind) = self.module_resolve(\n       specifier,\n       &referrer,\n       resolution_mode,\n@@ -270,19 +304,8 @@ impl<\n       resolution_kind,\n     )?;\n \n-    let url = if resolution_kind.is_types() && url.is_file() {\n-      let file_path = url.into_path()?;\n-      self.path_to_declaration_path(\n-        file_path,\n-        Some(&referrer),\n-        resolution_mode,\n-        conditions,\n-      )?\n-    } else {\n-      url\n-    };\n-\n-    let url_or_path = self.finalize_resolution(url, Some(&referrer))?;\n+    let url_or_path =\n+      self.finalize_resolution(url, resolved_kind, Some(&referrer))?;\n     let resolve_response = NodeResolution::Module(url_or_path);\n     // TODO(bartlomieju): skipped checking errors for commonJS resolution and\n     // \"preserveSymlinksMain\"/\"preserveSymlinks\" options.\n@@ -296,34 +319,50 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, NodeResolveError> {\n+  ) -> Result<(MaybeTypesResolvedUrl, ResolvedMethod), NodeResolveError> {\n     if should_be_treated_as_relative_or_absolute_path(specifier) {\n       let referrer_url = referrer.url()?;\n-      Ok(UrlOrPath::Url(\n-        node_join_url(referrer_url, specifier).map_err(|err| {\n-          NodeResolveRelativeJoinError {\n-            path: specifier.to_string(),\n-            base: referrer_url.clone(),\n-            source: err,\n-          }\n-        })?,\n-      ))\n+      let url = node_join_url(referrer_url, specifier).map_err(|err| {\n+        NodeResolveRelativeJoinError {\n+          path: specifier.to_string(),\n+          base: referrer_url.clone(),\n+          source: err,\n+        }\n+      })?;\n+      let url = self.maybe_resolve_types(\n+        LocalUrlOrPath::Url(url),\n+        Some(referrer),\n+        resolution_mode,\n+        conditions,\n+        resolution_kind,\n+      )?;\n+      Ok((url, ResolvedMethod::RelativeOrAbsolute))\n     } else if specifier.starts_with('#') {\n       let pkg_config = self\n         .pkg_json_resolver\n         .get_closest_package_json(referrer.path()?)\n         .map_err(PackageImportsResolveErrorKind::ClosestPkgJson)\n         .map_err(|err| PackageImportsResolveError(Box::new(err)))?;\n-      Ok(self.package_imports_resolve(\n-        specifier,\n+      Ok((\n+        self.package_imports_resolve_internal(\n+          specifier,\n+          Some(referrer),\n+          resolution_mode,\n+          pkg_config.as_deref(),\n+          conditions,\n+          resolution_kind,\n+        )?,\n+        ResolvedMethod::PackageImports,\n+      ))\n+    } else if let Ok(url) = Url::parse(specifier) {\n+      let url_or_path = self.maybe_resolve_types(\n+        LocalUrlOrPath::Url(url),\n         Some(referrer),\n         resolution_mode,\n-        pkg_config.as_deref(),\n         conditions,\n         resolution_kind,\n-      )?)\n-    } else if let Ok(resolved) = Url::parse(specifier) {\n-      Ok(UrlOrPath::Url(resolved))\n+      )?;\n+      Ok((url_or_path, ResolvedMethod::Url))\n     } else {\n       Ok(self.package_resolve(\n         specifier,\n@@ -337,19 +376,21 @@ impl<\n \n   fn finalize_resolution(\n     &self,\n-    resolved: UrlOrPath,\n+    resolved: MaybeTypesResolvedUrl,\n+    resolved_method: ResolvedMethod,\n     maybe_referrer: Option<&UrlOrPathRef>,\n   ) -> Result<UrlOrPath, FinalizeResolutionError> {\n     let encoded_sep_re = lazy_regex::regex!(r\"%2F|%2C\");\n \n+    let resolved = resolved.0;\n     let text = match &resolved {\n-      UrlOrPath::Url(url) => Cow::Borrowed(url.as_str()),\n-      UrlOrPath::Path(path_buf) => path_buf.to_string_lossy(),\n+      LocalUrlOrPath::Url(url) => Cow::Borrowed(url.as_str()),\n+      LocalUrlOrPath::Path(LocalPath { path, .. }) => path.to_string_lossy(),\n     };\n     if encoded_sep_re.is_match(&text) {\n       return Err(\n         errors::InvalidModuleSpecifierError {\n-          request: resolved.to_string(),\n+          request: text.into_owned(),\n           reason: Cow::Borrowed(\n             \"must not include encoded \\\"/\\\" or \\\"\\\\\\\\\\\" characters\",\n           ),\n@@ -363,11 +404,23 @@ impl<\n       );\n     }\n \n-    if resolved.is_node_url() {\n-      return Ok(resolved);\n-    }\n-\n-    let path = resolved.into_path()?;\n+    let (path, maybe_url) = match resolved {\n+      LocalUrlOrPath::Url(url) => {\n+        if url.scheme() == \"file\" {\n+          (url_to_file_path(&url)?, Some(url))\n+        } else {\n+          return Ok(UrlOrPath::Url(url));\n+        }\n+      }\n+      LocalUrlOrPath::Path(LocalPath { path, known_exists }) => {\n+        if known_exists {\n+          // no need to do the finalization checks\n+          return Ok(UrlOrPath::Path(path));\n+        } else {\n+          (path, None)\n+        }\n+      }\n+    };\n \n     // TODO(bartlomieju): currently not supported\n     // if (getOptionValue('--experimental-specifier-resolution') === 'node') {\n@@ -376,24 +429,38 @@ impl<\n \n     let p_str = path.to_str().unwrap();\n     let path = if p_str.ends_with('/') {\n-      // todo(dsherret): don't allocate a new string here\n-      PathBuf::from(p_str[p_str.len() - 1..].to_string())\n+      PathBuf::from(&p_str[p_str.len() - 1..])\n     } else {\n       path\n     };\n \n     let maybe_file_type = self.sys.fs_metadata(&path).map(|m| m.file_type());\n     match maybe_file_type {\n-      Ok(FileType::Dir) => Err(\n-        UnsupportedDirImportError {\n-          dir_url: UrlOrPath::Path(path),\n-          maybe_referrer: maybe_referrer.map(|r| r.display()),\n-        }\n-        .into(),\n-      ),\n-      Ok(FileType::File) => Ok(UrlOrPath::Path(path)),\n+      Ok(FileType::Dir) => {\n+        let suggested_file_name = [\"index.mjs\", \"index.js\", \"index.cjs\"]\n+          .into_iter()\n+          .find(|e| self.sys.fs_is_file_no_err(path.join(e)));\n+        Err(\n+          UnsupportedDirImportError {\n+            dir_url: UrlOrPath::Path(path),\n+            maybe_referrer: maybe_referrer.map(|r| r.display()),\n+            suggested_file_name,\n+          }\n+          .into(),\n+        )\n+      }\n+      Ok(FileType::File) => {\n+        // prefer returning the url to avoid re-allocating in the CLI crate\n+        Ok(\n+          maybe_url\n+            .map(UrlOrPath::Url)\n+            .unwrap_or(UrlOrPath::Path(path)),\n+        )\n+      }\n       _ => Err(\n         ModuleNotFoundError {\n+          suggested_ext: self\n+            .module_not_found_ext_suggestion(&path, resolved_method),\n           specifier: UrlOrPath::Path(path),\n           maybe_referrer: maybe_referrer.map(|r| r.display()),\n           typ: \"module\",\n@@ -403,6 +470,34 @@ impl<\n     }\n   }\n \n+  fn module_not_found_ext_suggestion(\n+    &self,\n+    path: &Path,\n+    resolved_method: ResolvedMethod,\n+  ) -> Option<&'static str> {\n+    fn should_probe(path: &Path, resolved_method: ResolvedMethod) -> bool {\n+      if MediaType::from_path(path) != MediaType::Unknown {\n+        return false;\n+      }\n+      match resolved_method {\n+        ResolvedMethod::Url\n+        | ResolvedMethod::RelativeOrAbsolute\n+        | ResolvedMethod::PackageSubPath => true,\n+        ResolvedMethod::PackageImports | ResolvedMethod::PackageExports => {\n+          false\n+        }\n+      }\n+    }\n+\n+    if should_probe(path, resolved_method) {\n+      [\"js\", \"mjs\", \"cjs\"]\n+        .into_iter()\n+        .find(|ext| self.sys.fs_is_file_no_err(with_known_extension(path, ext)))\n+    } else {\n+      None\n+    }\n+  }\n+\n   pub fn resolve_package_subpath_from_deno_module(\n     &self,\n     package_dir: &Path,\n@@ -417,7 +512,7 @@ impl<\n       .map(|s| format!(\"./{s}\"))\n       .unwrap_or_else(|| \".\".to_string());\n     let maybe_referrer = maybe_referrer.map(UrlOrPathRef::from_url);\n-    let resolved_url = self.resolve_package_dir_subpath(\n+    let (resolved_url, resolved_method) = self.resolve_package_dir_subpath(\n       package_dir,\n       &package_subpath,\n       maybe_referrer.as_ref(),\n@@ -427,9 +522,14 @@ impl<\n         .resolve(resolution_mode),\n       resolution_kind,\n     )?;\n+    let url_or_path = self.finalize_resolution(\n+      resolved_url,\n+      resolved_method,\n+      maybe_referrer.as_ref(),\n+    )?;\n     // TODO(bartlomieju): skipped checking errors for commonJS resolution and\n     // \"preserveSymlinksMain\"/\"preserveSymlinks\" options.\n-    Ok(resolved_url)\n+    Ok(url_or_path)\n   }\n \n   pub fn resolve_binary_commands(\n@@ -495,14 +595,48 @@ impl<\n       .resolve_package_folder_from_package(specifier, referrer)\n   }\n \n+  fn maybe_resolve_types(\n+    &self,\n+    url: LocalUrlOrPath,\n+    maybe_referrer: Option<&UrlOrPathRef>,\n+    resolution_mode: ResolutionMode,\n+    conditions: &[&str],\n+    resolution_kind: NodeResolutionKind,\n+  ) -> Result<MaybeTypesResolvedUrl, TypesNotFoundError> {\n+    if resolution_kind.is_types() {\n+      let file_path = match url {\n+        LocalUrlOrPath::Url(url) => {\n+          match deno_path_util::url_to_file_path(&url) {\n+            Ok(path) => LocalPath {\n+              path,\n+              known_exists: false,\n+            },\n+            Err(_) => {\n+              return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Url(url)));\n+            }\n+          }\n+        }\n+        LocalUrlOrPath::Path(path) => path,\n+      };\n+      self.path_to_declaration_path(\n+        file_path,\n+        maybe_referrer,\n+        resolution_mode,\n+        conditions,\n+      )\n+    } else {\n+      Ok(MaybeTypesResolvedUrl(url))\n+    }\n+  }\n+\n   /// Checks if the resolved file has a corresponding declaration file.\n   fn path_to_declaration_path(\n     &self,\n-    path: PathBuf,\n+    local_path: LocalPath,\n     maybe_referrer: Option<&UrlOrPathRef>,\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n-  ) -> Result<UrlOrPath, TypesNotFoundError> {\n+  ) -> Result<MaybeTypesResolvedUrl, TypesNotFoundError> {\n     fn probe_extensions<TSys: FsMetadata>(\n       sys: &TSys,\n       path: &Path,\n@@ -551,43 +685,49 @@ impl<\n       None\n     }\n \n-    let media_type = MediaType::from_path(&path);\n+    let media_type = MediaType::from_path(&local_path.path);\n     if media_type.is_declaration() {\n-      return Ok(UrlOrPath::Path(path));\n+      return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Path(local_path)));\n     }\n     if let Some(path) =\n-      probe_extensions(&self.sys, &path, media_type, resolution_mode)\n+      probe_extensions(&self.sys, &local_path.path, media_type, resolution_mode)\n     {\n-      return Ok(UrlOrPath::Path(path));\n+      return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Path(LocalPath {\n+        path,\n+        known_exists: true,\n+      })));\n     }\n-    if self.sys.fs_is_dir_no_err(&path) {\n+    if self.sys.fs_is_dir_no_err(&local_path.path) {\n       let resolution_result = self.resolve_package_dir_subpath(\n-        &path,\n+        &local_path.path,\n         /* sub path */ \".\",\n         maybe_referrer,\n         resolution_mode,\n         conditions,\n         NodeResolutionKind::Types,\n       );\n-      if let Ok(resolution) = resolution_result {\n-        return Ok(resolution);\n+      if let Ok((url_or_path, _)) = resolution_result {\n+        return Ok(url_or_path);\n       }\n-      let index_path = path.join(\"index.js\");\n+      let index_path = local_path.path.join(\"index.js\");\n       if let Some(path) = probe_extensions(\n         &self.sys,\n         &index_path,\n         MediaType::from_path(&index_path),\n         resolution_mode,\n       ) {\n-        return Ok(UrlOrPath::Path(path));\n+        return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Path(LocalPath {\n+          path,\n+          known_exists: true,\n+        })));\n       }\n     }\n     // allow resolving .ts-like or .css files for types resolution\n     if media_type.is_typed() || media_type == MediaType::Css {\n-      return Ok(UrlOrPath::Path(path));\n+      return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Path(local_path)));\n     }\n     Err(TypesNotFoundError(Box::new(TypesNotFoundErrorData {\n-      code_specifier: UrlOrPathRef::from_path(&path).display(),\n+      code_specifier: UrlOrPathRef::from_path(&local_path.path).display(),\n       maybe_referrer: maybe_referrer.map(|r| r.display()),\n     })))\n   }\n@@ -602,6 +742,28 @@ impl<\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n   ) -> Result<UrlOrPath, PackageImportsResolveError> {\n+    self\n+      .package_imports_resolve_internal(\n+        name,\n+        maybe_referrer,\n+        resolution_mode,\n+        referrer_pkg_json,\n+        conditions,\n+        resolution_kind,\n+      )\n+      .map(|url| url.0.into_url_or_path())\n+  }\n+\n+  #[allow(clippy::too_many_arguments)]\n+  fn package_imports_resolve_internal(\n+    &self,\n+    name: &str,\n+    maybe_referrer: Option<&UrlOrPathRef>,\n+    resolution_mode: ResolutionMode,\n+    referrer_pkg_json: Option<&PackageJson>,\n+    conditions: &[&str],\n+    resolution_kind: NodeResolutionKind,\n+  ) -> Result<MaybeTypesResolvedUrl, PackageImportsResolveError> {\n     if name == \"#\" || name.starts_with(\"#/\") || name.ends_with('/') {\n       let reason = \"is not a valid internal imports specifier name\";\n       return Err(\n@@ -703,7 +865,7 @@ impl<\n     internal: bool,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, PackageTargetResolveError> {\n+  ) -> Result<MaybeTypesResolvedUrl, PackageTargetResolveError> {\n     if !subpath.is_empty() && !pattern && !target.ends_with('/') {\n       return Err(\n         InvalidPackageTargetError {\n@@ -727,7 +889,7 @@ impl<\n             if get_module_name_from_builtin_node_module_specifier(&url)\n               .is_some()\n             {\n-              return Ok(UrlOrPath::Url(url));\n+              return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Url(url)));\n             }\n           }\n           Err(_) => {\n@@ -745,7 +907,7 @@ impl<\n               conditions,\n               resolution_kind,\n             ) {\n-              Ok(url) => Ok(url),\n+              Ok((url, _)) => Ok(url),\n               Err(err) => match err.code() {\n                 NodeJsErrorCode::ERR_INVALID_FILE_URL_PATH\n                 | NodeJsErrorCode::ERR_INVALID_MODULE_SPECIFIER\n@@ -781,9 +943,9 @@ impl<\n                   .is_built_in_node_module_checker\n                   .is_builtin_node_module(target)\n                 {\n-                  Ok(UrlOrPath::Url(\n+                  Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Url(\n                     Url::parse(&format!(\"node:{}\", target)).unwrap(),\n-                  ))\n+                  )))\n                 } else {\n                   Err(err)\n                 }\n@@ -829,10 +991,12 @@ impl<\n         .into(),\n       );\n     }\n-    if subpath.is_empty() {\n-      return Ok(UrlOrPath::Path(resolved_path));\n-    }\n-    if invalid_segment_re.is_match(subpath) {\n+    let path = if subpath.is_empty() {\n+      LocalPath {\n+        path: resolved_path,\n+        known_exists: false,\n+      }\n+    } else if invalid_segment_re.is_match(subpath) {\n       let request = if pattern {\n         match_.replace('*', subpath)\n       } else {\n@@ -847,14 +1011,27 @@ impl<\n         )\n         .into(),\n       );\n-    }\n-    if pattern {\n+    } else if pattern {\n       let resolved_path_str = resolved_path.to_string_lossy();\n       let replaced = pattern_re\n         .replace(&resolved_path_str, |_caps: &regex::Captures| subpath);\n-      return Ok(UrlOrPath::Path(PathBuf::from(replaced.to_string())));\n-    }\n-    Ok(UrlOrPath::Path(resolved_path.join(subpath).clean()))\n+      LocalPath {\n+        path: PathBuf::from(replaced.as_ref()),\n+        known_exists: false,\n+      }\n+    } else {\n+      LocalPath {\n+        path: resolved_path.join(subpath).clean(),\n+        known_exists: false,\n+      }\n+    };\n+    Ok(self.maybe_resolve_types(\n+      LocalUrlOrPath::Path(path),\n+      maybe_referrer,\n+      resolution_mode,\n+      conditions,\n+      resolution_kind,\n+    )?)\n   }\n \n   #[allow(clippy::too_many_arguments)]\n@@ -870,7 +1047,7 @@ impl<\n     internal: bool,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<Option<UrlOrPath>, PackageTargetResolveError> {\n+  ) -> Result<Option<MaybeTypesResolvedUrl>, PackageTargetResolveError> {\n     let result = self.resolve_package_target_inner(\n       package_json_path,\n       target,\n@@ -926,7 +1103,7 @@ impl<\n     internal: bool,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<Option<UrlOrPath>, PackageTargetResolveError> {\n+  ) -> Result<Option<MaybeTypesResolvedUrl>, PackageTargetResolveError> {\n     if let Some(target) = target.as_str() {\n       let url_or_path = self.resolve_package_target_string(\n         target,\n@@ -940,17 +1117,7 @@ impl<\n         conditions,\n         resolution_kind,\n       )?;\n-      if resolution_kind.is_types() && url_or_path.is_file() {\n-        let path = url_or_path.into_path()?;\n-        return Ok(Some(self.path_to_declaration_path(\n-          path,\n-          maybe_referrer,\n-          resolution_mode,\n-          conditions,\n-        )?));\n-      } else {\n-        return Ok(Some(url_or_path));\n-      }\n+      return Ok(Some(url_or_path));\n     } else if let Some(target_arr) = target.as_array() {\n       if target_arr.is_empty() {\n         return Ok(None);\n@@ -1051,6 +1218,30 @@ impl<\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n   ) -> Result<UrlOrPath, PackageExportsResolveError> {\n+    self\n+      .package_exports_resolve_internal(\n+        package_json_path,\n+        package_subpath,\n+        package_exports,\n+        maybe_referrer,\n+        resolution_mode,\n+        conditions,\n+        resolution_kind,\n+      )\n+      .map(|url| url.0.into_url_or_path())\n+  }\n+\n+  #[allow(clippy::too_many_arguments)]\n+  fn package_exports_resolve_internal(\n+    &self,\n+    package_json_path: &Path,\n+    package_subpath: &str,\n+    package_exports: &Map<String, Value>,\n+    maybe_referrer: Option<&UrlOrPathRef>,\n+    resolution_mode: ResolutionMode,\n+    conditions: &[&str],\n+    resolution_kind: NodeResolutionKind,\n+  ) -> Result<MaybeTypesResolvedUrl, PackageExportsResolveError> {\n     if let Some(target) = package_exports.get(package_subpath) {\n       if package_subpath.find('*').is_none() && !package_subpath.ends_with('/')\n       {\n@@ -1156,14 +1347,14 @@ impl<\n     )\n   }\n \n-  pub(super) fn package_resolve(\n+  fn package_resolve(\n     &self,\n     specifier: &str,\n     referrer: &UrlOrPathRef,\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, PackageResolveError> {\n+  ) -> Result<(MaybeTypesResolvedUrl, ResolvedMethod), PackageResolveError> {\n     let (package_name, package_subpath, _is_scoped) =\n       parse_npm_pkg_name(specifier, referrer)?;\n \n@@ -1175,7 +1366,7 @@ impl<\n       if package_config.name.as_deref() == Some(package_name) {\n         if let Some(exports) = &package_config.exports {\n           return self\n-            .package_exports_resolve(\n+            .package_exports_resolve_internal(\n               &package_config.path,\n               &package_subpath,\n               exports,\n@@ -1184,6 +1375,7 @@ impl<\n               conditions,\n               resolution_kind,\n             )\n+            .map(|url| (url, ResolvedMethod::PackageExports))\n             .map_err(|err| err.into());\n         }\n       }\n@@ -1208,7 +1400,7 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, PackageResolveError> {\n+  ) -> Result<(MaybeTypesResolvedUrl, ResolvedMethod), PackageResolveError> {\n     let result = self.resolve_package_subpath_for_package_inner(\n       package_name,\n       package_subpath,\n@@ -1243,7 +1435,7 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, PackageResolveError> {\n+  ) -> Result<(MaybeTypesResolvedUrl, ResolvedMethod), PackageResolveError> {\n     let package_dir_path = self\n       .npm_pkg_folder_resolver\n       .resolve_package_folder_from_package(package_name, referrer)?;\n@@ -1283,7 +1475,8 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, PackageSubpathResolveError> {\n+  ) -> Result<(MaybeTypesResolvedUrl, ResolvedMethod), PackageSubpathResolveError>\n+  {\n     let package_json_path = package_dir_path.join(\"package.json\");\n     match self\n       .pkg_json_resolver\n@@ -1306,6 +1499,7 @@ impl<\n           conditions,\n           resolution_kind,\n         )\n+        .map(|url| (url, ResolvedMethod::PackageSubPath))\n         .map_err(|err| {\n           PackageSubpathResolveErrorKind::LegacyResolve(err).into()\n         }),\n@@ -1321,9 +1515,10 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, PackageSubpathResolveError> {\n+  ) -> Result<(MaybeTypesResolvedUrl, ResolvedMethod), PackageSubpathResolveError>\n+  {\n     if let Some(exports) = &package_json.exports {\n-      let result = self.package_exports_resolve(\n+      let result = self.package_exports_resolve_internal(\n         &package_json.path,\n         package_subpath,\n         exports,\n@@ -1333,7 +1528,7 @@ impl<\n         resolution_kind,\n       );\n       match result {\n-        Ok(found) => return Ok(found),\n+        Ok(found) => return Ok((found, ResolvedMethod::PackageExports)),\n         Err(exports_err) => {\n           if resolution_kind.is_types() && package_subpath == \".\" {\n             return self\n@@ -1344,6 +1539,7 @@ impl<\n                 conditions,\n                 resolution_kind,\n               )\n+              .map(|url| (url, ResolvedMethod::PackageSubPath))\n               .map_err(|err| {\n                 PackageSubpathResolveErrorKind::LegacyResolve(err).into()\n               });\n@@ -1364,6 +1560,7 @@ impl<\n           conditions,\n           resolution_kind,\n         )\n+        .map(|url| (url, ResolvedMethod::PackageSubPath))\n         .map_err(|err| {\n           PackageSubpathResolveErrorKind::LegacyResolve(err).into()\n         });\n@@ -1378,6 +1575,7 @@ impl<\n         conditions,\n         resolution_kind,\n       )\n+      .map(|url| (url, ResolvedMethod::PackageSubPath))\n       .map_err(|err| {\n         PackageSubpathResolveErrorKind::LegacyResolve(err.into()).into()\n       })\n@@ -1391,19 +1589,19 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, TypesNotFoundError> {\n+  ) -> Result<MaybeTypesResolvedUrl, TypesNotFoundError> {\n     assert_ne!(package_subpath, \".\");\n     let file_path = directory.join(package_subpath);\n-    if resolution_kind.is_types() {\n-      Ok(self.path_to_declaration_path(\n-        file_path,\n-        referrer,\n-        resolution_mode,\n-        conditions,\n-      )?)\n-    } else {\n-      Ok(UrlOrPath::Path(file_path))\n-    }\n+    self.maybe_resolve_types(\n+      LocalUrlOrPath::Path(LocalPath {\n+        path: file_path,\n+        known_exists: false,\n+      }),\n+      referrer,\n+      resolution_mode,\n+      conditions,\n+      resolution_kind,\n+    )\n   }\n \n   fn resolve_package_subpath_no_pkg_json(\n@@ -1414,16 +1612,14 @@ impl<\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, LegacyResolveError> {\n+  ) -> Result<MaybeTypesResolvedUrl, LegacyResolveError> {\n     if package_subpath == \".\" {\n-      self\n-        .legacy_index_resolve(\n-          directory,\n-          maybe_referrer,\n-          resolution_mode,\n-          resolution_kind,\n-        )\n-        .map(UrlOrPath::Path)\n+      self.legacy_index_resolve(\n+        directory,\n+        maybe_referrer,\n+        resolution_mode,\n+        resolution_kind,\n+      )\n     } else {\n       self\n         .resolve_subpath_exact(\n@@ -1438,14 +1634,14 @@ impl<\n     }\n   }\n \n-  pub(super) fn legacy_main_resolve(\n+  fn legacy_main_resolve(\n     &self,\n     package_json: &PackageJson,\n     maybe_referrer: Option<&UrlOrPathRef>,\n     resolution_mode: ResolutionMode,\n     conditions: &[&str],\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<UrlOrPath, LegacyResolveError> {\n+  ) -> Result<MaybeTypesResolvedUrl, LegacyResolveError> {\n     let pkg_json_kind = match resolution_mode {\n       ResolutionMode::Require => deno_package_json::NodeModuleKind::Cjs,\n       ResolutionMode::Import => deno_package_json::NodeModuleKind::Esm,\n@@ -1459,7 +1655,10 @@ impl<\n           if let Some(main) = package_json.main(pkg_json_kind) {\n             let main = package_json.path.parent().unwrap().join(main).clean();\n             let decl_path_result = self.path_to_declaration_path(\n-              main,\n+              LocalPath {\n+                path: main,\n+                known_exists: false,\n+              },\n               maybe_referrer,\n               resolution_mode,\n               conditions,\n@@ -1479,7 +1678,16 @@ impl<\n     if let Some(main) = maybe_main {\n       let guess = package_json.path.parent().unwrap().join(main).clean();\n       if self.sys.fs_is_file_no_err(&guess) {\n-        return Ok(UrlOrPath::Path(guess));\n+        return Ok(self.maybe_resolve_types(\n+          LocalUrlOrPath::Path(LocalPath {\n+            path: guess,\n+            known_exists: true,\n+          }),\n+          maybe_referrer,\n+          resolution_mode,\n+          conditions,\n+          resolution_kind,\n+        )?);\n       }\n \n       // todo(dsherret): investigate exactly how node and typescript handles this\n@@ -1509,19 +1717,20 @@ impl<\n           .clean();\n         if self.sys.fs_is_file_no_err(&guess) {\n           // TODO(bartlomieju): emitLegacyIndexDeprecation()\n-          return Ok(UrlOrPath::Path(guess));\n+          return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Path(LocalPath {\n+            path: guess,\n+            known_exists: true,\n+          })));\n         }\n       }\n     }\n \n-    self\n-      .legacy_index_resolve(\n-        package_json.path.parent().unwrap(),\n-        maybe_referrer,\n-        resolution_mode,\n-        resolution_kind,\n-      )\n-      .map(UrlOrPath::Path)\n+    self.legacy_index_resolve(\n+      package_json.path.parent().unwrap(),\n+      maybe_referrer,\n+      resolution_mode,\n+      resolution_kind,\n+    )\n   }\n \n   fn legacy_index_resolve(\n@@ -1530,7 +1739,7 @@ impl<\n     maybe_referrer: Option<&UrlOrPathRef>,\n     resolution_mode: ResolutionMode,\n     resolution_kind: NodeResolutionKind,\n-  ) -> Result<PathBuf, LegacyResolveError> {\n+  ) -> Result<MaybeTypesResolvedUrl, LegacyResolveError> {\n     let index_file_names = if resolution_kind.is_types() {\n       // todo(dsherret): investigate exactly how typescript does this\n       match resolution_mode {\n@@ -1546,7 +1755,10 @@ impl<\n       let guess = directory.join(index_file_name).clean();\n       if self.sys.fs_is_file_no_err(&guess) {\n         // TODO(bartlomieju): emitLegacyIndexDeprecation()\n-        return Ok(guess);\n+        return Ok(MaybeTypesResolvedUrl(LocalUrlOrPath::Path(LocalPath {\n+          path: guess,\n+          known_exists: true,\n+        })));\n       }\n     }\n \n@@ -1565,6 +1777,7 @@ impl<\n           specifier: UrlOrPath::Path(directory.join(\"index.js\")),\n           typ: \"module\",\n           maybe_referrer: maybe_referrer.map(|r| r.display()),\n+          suggested_ext: None,\n         }\n         .into(),\n       )\n", "instance_id": "denoland__deno-27905", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to improve error messaging for resolving CommonJS modules from ESM in the Deno CLI. It provides a summary of the goal, links to a related issue for context, and includes before-and-after examples of the error output, which helps in understanding the desired outcome. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the full scope of \"non-explicit CJS modules\" or provide detailed constraints on what constitutes a valid resolution or suggestion. Additionally, edge cases or specific scenarios where the error message should or should not appear are not mentioned. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, involving modifications across multiple files (`errors.rs` and `resolution.rs`) in the node resolution logic of Deno, a complex runtime. The changes impact core functionality related to module resolution, requiring a deep understanding of Deno's architecture, particularly how it handles ESM and CommonJS interoperability. Second, the technical concepts involved are moderately advanced, including Rust's error handling mechanisms, module resolution algorithms, and file system interactions. The code changes also introduce new logic for suggesting file extensions and handling directory imports, which adds complexity. Third, the modifications require careful consideration of edge cases, such as invalid specifiers, missing files, and directory imports, as evidenced by the addition of suggestion logic in error messages. While the problem does not involve system-level considerations or extremely intricate domain-specific knowledge, it demands a solid grasp of the codebase and thoughtful implementation to avoid introducing bugs or performance issues. A score of 0.65 reflects the challenge of navigating and modifying a critical part of the system with moderate-to-high complexity.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "add virtual host domain to logging\n### Search for duplicate feature request\n\n- [X] I already searched, and this feature request or improvement is not a duplicate.\n\n### Feature scope\n\nImprove existing functionality\n\n### Feature request related to a problem\n\n_No response_\n\n### Describe the solution you'd like\n\nI have installed sws and it works fine for virtual hosts domains. \r\nConfig is like so:\r\n`\r\n[[advanced.virtual-hosts]]\r\nhost = \"www.hanscees.com\"\r\nroot = \"/var/hanscees\"\r\n` \r\n\r\nHowever, in the logging it is not discernable what domain the logs belong to. \r\n`\r\nweb-1  | 2024-03-01T22:03:40.848154Z  INFO static_web_server::handler: incoming request: method=GET uri=/img/slide-3.jpg remote_addr=[::ffff:192.168.0.2]:43484\r\n`\r\nPlease consider adding something to the logging so it becomes clear what domain was involved\n\n### Describe alternatives you've considered\n\nnon\n\n### Build target\n\nAll targets\n\n### Additional context\n\n_No response_\n", "patch": "diff --git a/src/handler.rs b/src/handler.rs\nindex 3c113c51..88ffa556 100644\n--- a/src/handler.rs\n+++ b/src/handler.rs\n@@ -235,7 +235,7 @@ impl RequestHandler {\n             if let Some(advanced) = &self.opts.advanced_opts {\n                 // If the \"Host\" header matches any virtual_host, change the root directory\n                 if let Some(root) =\n-                    virtual_hosts::get_real_root(req.headers(), advanced.virtual_hosts.as_deref())\n+                    virtual_hosts::get_real_root(req, advanced.virtual_hosts.as_deref())\n                 {\n                     base_path = root;\n                 }\ndiff --git a/src/lib.rs b/src/lib.rs\nindex d6947849..900e0051 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -194,7 +194,7 @@ pub mod static_files;\n #[cfg_attr(docsrs, doc(cfg(feature = \"http2\")))]\n pub mod tls;\n pub mod transport;\n-pub mod virtual_hosts;\n+pub(crate) mod virtual_hosts;\n #[cfg(windows)]\n #[cfg_attr(docsrs, doc(cfg(windows)))]\n pub mod winservice;\ndiff --git a/src/virtual_hosts.rs b/src/virtual_hosts.rs\nindex 3fc1acd6..c700c5a8 100644\n--- a/src/virtual_hosts.rs\n+++ b/src/virtual_hosts.rs\n@@ -6,21 +6,28 @@\n //! Module that allows to determine a virtual hostname.\n //!\n \n-use headers::HeaderMap;\n use hyper::header::HOST;\n+use hyper::Request;\n use std::path::PathBuf;\n \n use crate::settings::VirtualHosts;\n \n /// It returns different root directory if the \"Host\" header matches a virtual hostname.\n-pub fn get_real_root<'a>(\n-    headers: &HeaderMap,\n+pub(crate) fn get_real_root<'a, T>(\n+    req: &mut Request<T>,\n     vhosts_opts: Option<&'a [VirtualHosts]>,\n ) -> Option<&'a PathBuf> {\n     if let Some(vhosts) = vhosts_opts {\n-        if let Ok(host_str) = headers.get(HOST)?.to_str() {\n+        if let Ok(host_str) = req.headers().get(HOST)?.to_str() {\n             for vhost in vhosts {\n                 if vhost.host == host_str {\n+                    tracing::info!(\n+                        \"virtual host matched: vhost={} vhost_root={} method={} uri={}\",\n+                        vhost.host,\n+                        vhost.root.display(),\n+                        req.method(),\n+                        req.uri(),\n+                    );\n                     return Some(&vhost.root);\n                 }\n             }\n", "instance_id": "static-web-server__static-web-server-442", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent: the user wants to add virtual host domain information to the logging output of a web server application (sws) to distinguish which domain a log entry pertains to. The goal is explicitly stated, and a configuration example is provided to contextualize the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the exact format or content of the desired log output (e.g., should it include just the domain name or additional metadata?). Additionally, edge cases or constraints, such as handling invalid or missing \"Host\" headers, are not mentioned. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes are relatively localized, affecting only a few files (`handler.rs`, `lib.rs`, and `virtual_hosts.rs`). The modifications involve updating a function signature, changing visibility of a module, and adding a logging statement using the `tracing` crate. The changes do not impact the broader system architecture and are limited to enhancing an existing feature (logging). The amount of code change is minimal, focusing on a single logical addition.\n\n2. **Technical Concepts Involved:** Solving this problem requires basic familiarity with Rust, particularly with the `hyper` crate for handling HTTP requests and the `tracing` crate for logging. The concepts involved\u2014accessing request headers, matching strings, and emitting log messages\u2014are straightforward and do not require advanced language features, complex algorithms, or design patterns. Understanding the virtual host configuration and its integration into the request handling logic is necessary but not overly complex.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, such as requests without a \"Host\" header or malformed headers. The provided code changes handle the happy path (successful header retrieval and matching) but do not add new error handling logic beyond what is already present (e.g., using `?` for error propagation). While a more robust solution might consider additional edge cases, the current scope of the change does not demand complex error handling.\n\n4. **Overall Complexity:** The task involves understanding a small part of the codebase (request handling and virtual host logic) and making a simple modification to include additional logging information. It does not require deep architectural changes or extensive debugging of interactions between modules. The problem is a straightforward feature enhancement that a developer with intermediate Rust experience could tackle with minimal effort.\n\nGiven these considerations, a difficulty score of 0.30 reflects the simplicity of the task while acknowledging the need for some understanding of the codebase and relevant libraries.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fix the null handling for `to_char` function\n### Describe the bug\n\nCurrenlty, if we input a null value to `to_char`, we will get an empty string instead of a null value.\n```\n> select to_char(NULL, '%Y-%m-%d %H:%M:%S') is null;\n+-------------------------------------------------+\n| to_char(NULL,Utf8(\"%Y-%m-%d %H:%M:%S\")) IS NULL |\n+-------------------------------------------------+\n| false                                           |\n+-------------------------------------------------+\n1 row(s) fetched. \nElapsed 0.001 seconds.\n\n> select to_char(NULL, '%Y-%m-%d %H:%M:%S');\n+-----------------------------------------+\n| to_char(NULL,Utf8(\"%Y-%m-%d %H:%M:%S\")) |\n+-----------------------------------------+\n|                                         |\n+-----------------------------------------+\n1 row(s) fetched. \nElapsed 0.001 seconds.\n```\n\nHowever, the postgres behavior is returing a null value if got a null input.\n```\npsql=# select to_char(null::timestamp, '%Y-%m-%d %H:%M:%S') is null;\n ?column? \n----------\n t\n(1 row)\n\npsql=# select to_char(null::timestamp, '%Y-%m-%d %H:%M:%S');\n to_char \n---------\n \n(1 row)\n```\n\nI think we should follow the behavior of postgres.\n\n### To Reproduce\n\nExecute the SQL mentioned above.\n\n### Expected behavior\n\n`to_char` should return a null if got a null input.\n\n### Additional context\n\n_No response_\n", "patch": "diff --git a/datafusion/functions/src/datetime/to_char.rs b/datafusion/functions/src/datetime/to_char.rs\nindex a9630ea4f9ba..034bbb705070 100644\n--- a/datafusion/functions/src/datetime/to_char.rs\n+++ b/datafusion/functions/src/datetime/to_char.rs\n@@ -212,6 +212,14 @@ fn _to_char_scalar(\n     let is_scalar_expression = matches!(&expression, ColumnarValue::Scalar(_));\n     let array = expression.into_array(1)?;\n \n+    // fix https://github.com/apache/datafusion/issues/14884\n+    // If the input date/time is null, return a null Utf8 result.\n+    if array.is_null(0) {\n+        return Ok(match is_scalar_expression {\n+            true => ColumnarValue::Scalar(ScalarValue::Utf8(None)),\n+            false => ColumnarValue::Array(new_null_array(&Utf8, array.len())),\n+        });\n+    }\n     if format.is_none() {\n         if is_scalar_expression {\n             return Ok(ColumnarValue::Scalar(ScalarValue::Utf8(None)));\n@@ -252,6 +260,13 @@ fn _to_char_array(args: &[ColumnarValue]) -> Result<ColumnarValue> {\n     let data_type = arrays[0].data_type();\n \n     for idx in 0..arrays[0].len() {\n+        // fix https://github.com/apache/datafusion/issues/14884\n+        // If the date/time value is null, push None.\n+        if arrays[0].is_null(idx) {\n+            results.push(None);\n+            continue;\n+        }\n+\n         let format = if format_array.is_null(idx) {\n             None\n         } else {\n@@ -663,4 +678,31 @@ mod tests {\n             \"Execution error: Format for `to_char` must be non-null Utf8, received Timestamp(Nanosecond, None)\"\n         );\n     }\n+\n+    #[test]\n+    fn test_to_char_input_none_array() {\n+        let date_array = Arc::new(Date32Array::from(vec![Some(18506), None])) as ArrayRef;\n+        let format_array =\n+            StringArray::from(vec![\"%Y-%m-%d\".to_string(), \"%Y-%m-%d\".to_string()]);\n+        let args = datafusion_expr::ScalarFunctionArgs {\n+            args: vec![\n+                ColumnarValue::Array(date_array),\n+                ColumnarValue::Array(Arc::new(format_array) as ArrayRef),\n+            ],\n+            number_rows: 2,\n+            return_type: &DataType::Utf8,\n+        };\n+        let result = ToCharFunc::new()\n+            .invoke_with_args(args)\n+            .expect(\"Expected no error\");\n+        if let ColumnarValue::Array(result) = result {\n+            let result = result.as_any().downcast_ref::<StringArray>().unwrap();\n+            assert_eq!(result.len(), 2);\n+            // The first element is valid, second is null.\n+            assert!(!result.is_null(0));\n+            assert!(result.is_null(1));\n+        } else {\n+            panic!(\"Expected an array value\");\n+        }\n+    }\n }\n", "instance_id": "apache__datafusion-14908", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-defined. The goal is explicitly stated: to fix the behavior of the `to_char` function to return a null value when given a null input, aligning with PostgreSQL's behavior. The input and output expectations are clearly demonstrated through SQL query examples for both the current buggy behavior and the expected behavior. The bug is reproducible with provided steps, and the context of aligning with PostgreSQL behavior adds clarity to the intent. There are no significant ambiguities, and the problem description includes detailed examples and expected outcomes, making it easy to understand the issue and the desired fix. All critical details, such as the nature of the bug and the target behavior, are provided, leaving no room for misinterpretation.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are localized to a single file (`to_char.rs`) and involve modifications to two functions (`_to_char_scalar` and `_to_char_array`). The changes are minimal, consisting of adding null checks before processing the input and returning a null result when the input is null. There is no impact on the broader system architecture or interactions with other modules, and the amount of code change is small (a few lines of conditional logic and a new test case). This keeps the scope narrow and manageable.\n\n2. **Number of Technical Concepts**: The solution requires understanding basic Rust concepts such as handling null values in arrays (`is_null`), working with `ColumnarValue` types (scalar vs. array), and returning appropriate null results using the DataFusion library's constructs. These are straightforward concepts for anyone familiar with Rust and the DataFusion framework. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic database function behavior are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem focuses on a specific edge case\u2014handling null inputs\u2014but this is explicitly mentioned in the problem statement and directly addressed in the code changes. The modification involves simple null checks, and no additional complex error handling or unforeseen edge cases are introduced. The provided test case (`test_to_char_input_none_array`) ensures the fix works as expected for null inputs in an array context, covering the primary concern.\n\n4. **Overall Complexity**: The logic to fix the bug is simple\u2014check if the input is null and return a null result if it is. The changes do not require deep understanding of the codebase beyond the immediate function context, and the impact is limited to the behavior of `to_char`. While familiarity with DataFusion's data types and array handling is necessary, this is not a significant barrier for a developer with moderate experience in Rust or database systems.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the fix, the limited scope of changes, and the straightforward nature of the technical concepts involved. It is a bug fix that requires understanding some code logic and making simple modifications, fitting well within the \"Easy\" range.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Sample history and frequency charts ignore sub-millisecond samples\nThe sample history and frequency charts ignore sub-millisecond samples.  This leads to \"gaps\" in the sample history chart and missing frequency buckets in the frequency chart.\r\n\r\n## Sample History Chart\r\n\r\nFor the sample history, the logic is:\r\n\r\n```rust\r\nif s.as_secs_f64() > 0_f64 {\r\n    Some((s.as_secs_f64() * 1000_f64) as u64)\r\n} else {\r\n    None\r\n}\r\n```\r\n\r\nIf the sample duration is < 1ms (1000us) then the above evaluated to 0.\r\n\r\nExamples:\r\n\r\n```\r\nmicros: 1000, millis: 1, nanos: 1000000, secs: 0.001,    (secs * 1000_f64) as u64: 1\r\nmicros: 999,  millis: 0, nanos: 999000,  secs: 0.000999, (secs * 1000_f64) as u64: 0\r\nmicros: 1,    millis: 0, nanos: 1000,    secs: 0.000001, (secs * 1000_f64) as u64: 0\r\n```\r\n\r\nFor this chart we do not care about the absolute value of the sample, only the relative value (vs all other values).  Therefore, to support a resolution down to 1us, we can use `s.as_micros()` instead.\r\n\r\nWe should also replace `s.as_secs_f64() > 0_f64` with `!s.is_zero()`.\r\n\r\n## Frequency Chart\r\n\r\nFor the frequency chart the logic is:\r\n\r\n```rust\r\nif sample.as_millis() > 0 {\r\n    ... snip ...\r\n}\r\n```\r\n\r\nIf the sample duration is < 1ms (1000us) then the above check evaluates to false.\r\n\r\nFor this chart it is ok for a bucket to exist for zero (samples of duration between 0us and 999us), therefore we should replace `sample.as_millis() > 0` with `!s.is_zero()`.\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 37399c8d..cc7a9eed 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -40,6 +40,8 @@ to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n   `--tui-preserve-screen` ([#1375](https://github.com/fujiapple852/trippy/issues/1375))\n - Config item `tui-address-mode` does not accept `ip` ([#1327](https://github.com/fujiapple852/trippy/issues/1327))\n - Icmp extension mode not shown in Tui settings ([#1289](https://github.com/fujiapple852/trippy/issues/1289))\n+- Sample history and frequency charts ignore sub-millisecond\n+  samples ([#1398](https://github.com/fujiapple852/trippy/issues/1398))\n \n ## [0.11.0] - 2024-08-11\n \ndiff --git a/RELEASES.md b/RELEASES.md\nindex 21568bde..d48597b4 100644\n--- a/RELEASES.md\n+++ b/RELEASES.md\n@@ -319,6 +319,9 @@ The previous release of Trippy introduced a bug ([#1290](https://github.com/fuji\n caused reverse DNS lookups to be enqueued multiple times when the `dns-ttl` expired, potentially leading to the hostname\n being displayed as `Timeout: xxx` for a brief period.\n \n+A long standing bug ([#1398](https://github.com/fujiapple852/trippy/issues/1398)) which caused the TUI sample history\n+and frequency charts to ignore sub-millisecond samples has been fixed.\n+\n This release fixes a bug ([#1287](https://github.com/fujiapple852/trippy/issues/1287)) that caused a tracer panic when\n parsing certain ICMP extensions with malformed lengths.\n \ndiff --git a/crates/trippy-tui/src/frontend/render/histogram.rs b/crates/trippy-tui/src/frontend/render/histogram.rs\nindex 43203818..ccbc1047 100644\n--- a/crates/trippy-tui/src/frontend/render/histogram.rs\n+++ b/crates/trippy-tui/src/frontend/render/histogram.rs\n@@ -43,7 +43,7 @@ fn sample_frequency(samples: &[Duration]) -> Vec<(String, u64)> {\n     let sample_count = samples.len();\n     let mut count_by_duration: BTreeMap<u128, u64> = BTreeMap::new();\n     for sample in samples {\n-        if sample.as_millis() > 0 {\n+        if !sample.is_zero() {\n             *count_by_duration.entry(sample.as_millis()).or_default() += 1;\n         }\n     }\ndiff --git a/crates/trippy-tui/src/frontend/render/history.rs b/crates/trippy-tui/src/frontend/render/history.rs\nindex a8620f25..8534a8f9 100644\n--- a/crates/trippy-tui/src/frontend/render/history.rs\n+++ b/crates/trippy-tui/src/frontend/render/history.rs\n@@ -13,10 +13,10 @@ pub fn render(f: &mut Frame<'_>, app: &TuiApp, rect: Rect) {\n         .iter()\n         .take(rect.width as usize)\n         .map(|s| {\n-            if s.as_secs_f64() > 0_f64 {\n-                Some((s.as_secs_f64() * 1000_f64) as u64)\n-            } else {\n+            if s.is_zero() {\n                 None\n+            } else {\n+                Some(s.as_micros() as u64)\n             }\n         })\n         .collect::<Vec<_>>();\n", "instance_id": "fujiapple852__trippy-1399", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly identifies the issue with sub-millisecond samples being ignored in both the sample history and frequency charts, leading to gaps and missing buckets. The goal is explicitly stated, and the logic behind the current behavior is explained with detailed code snippets and examples that illustrate the problem (e.g., conversion of durations to milliseconds and the resulting loss of precision). The proposed solution is also clearly outlined, with specific changes suggested for both charts (e.g., using `s.as_micros()` and replacing conditions with `!s.is_zero()`). Constraints and requirements are implicitly clear from the context, and there are no significant ambiguities. The inclusion of examples and the direct mapping to code changes further enhance the clarity. Therefore, this problem statement earns a score of 3 for being valid, clear, and detailed.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The modifications are localized to two specific files (`histogram.rs` and `history.rs`) within the `trippy-tui` crate, affecting only small, targeted sections of code. The changes involve updating conditional checks and duration conversion methods, which are straightforward and do not impact the broader system architecture or require understanding complex interactions between modules. The amount of code change is minimal, as shown in the diff (a few lines per file).\n\n2. **Number of Technical Concepts:** The problem requires basic familiarity with Rust's `Duration` type and its associated methods (`as_secs_f64()`, `as_micros()`, `as_millis()`, `is_zero()`), which are part of the standard library and not particularly complex. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond understanding how durations are handled and represented in Rust. The concepts involved are fundamental and accessible to developers with basic to intermediate Rust experience.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond the core issue of sub-millisecond samples being ignored. The code changes do not introduce new error handling logic or require significant consideration of edge cases, as the replacement of conditions (`!s.is_zero()`) and the use of `as_micros()` are robust for the intended purpose. The focus is on precision rather than handling exceptional scenarios, keeping the complexity low.\n\n4. **Overall Complexity:** The logic behind the fix is simple\u2014adjusting how durations are evaluated and represented to include sub-millisecond values. There are no performance implications or architectural considerations mentioned or implied in the changes. The task is essentially a bug fix that involves understanding a small piece of logic and making targeted updates.\n\nGiven these factors, a difficulty score of 0.25 is appropriate. It reflects a problem that is easy to solve with basic Rust knowledge and minimal codebase exploration, requiring only simple modifications to existing logic without broader impact or complex considerations.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Provide the `Path` of the library in the `Context`\n### Describe the feature\n\nThe main motivation for this functionality comes from the Python bindings: in order to load the Python script of a Node, the `pyo3` library requires a `Path`. To provide this path the `configuration` section of a Node was modified by Zenoh-Flow, adding a `python-script` field.\r\n\r\nProviding the path of the library in the `Context` is a much cleaner approach.\r\n\r\nAdditionally, we believe this information could be useful to application developers.\n", "patch": "diff --git a/zenoh-flow-nodes/src/context.rs b/zenoh-flow-nodes/src/context.rs\nindex 1c37b2fa..b8136882 100644\n--- a/zenoh-flow-nodes/src/context.rs\n+++ b/zenoh-flow-nodes/src/context.rs\n@@ -12,7 +12,7 @@\n //   ZettaScale Zenoh Team, <zenoh@zettascale.tech>\n //\n \n-use std::sync::Arc;\n+use std::{path::PathBuf, sync::Arc};\n \n use zenoh_flow_commons::{InstanceId, RuntimeId};\n \n@@ -27,15 +27,22 @@ pub struct Context {\n     pub(crate) flow_name: Arc<str>,\n     pub(crate) instance_id: InstanceId,\n     pub(crate) runtime_id: RuntimeId,\n+    pub(crate) library_path: Arc<PathBuf>,\n }\n \n impl Context {\n     /// Creates a new node `Context`.\n-    pub fn new(flow_name: Arc<str>, instance_id: InstanceId, runtime_id: RuntimeId) -> Self {\n+    pub fn new(\n+        flow_name: Arc<str>,\n+        instance_id: InstanceId,\n+        runtime_id: RuntimeId,\n+        library_path: Arc<PathBuf>,\n+    ) -> Self {\n         Self {\n             flow_name,\n             instance_id,\n             runtime_id,\n+            library_path,\n         }\n     }\n \n@@ -58,4 +65,11 @@ impl Context {\n     pub fn runtime_id(&self) -> &RuntimeId {\n         &self.runtime_id\n     }\n+\n+    /// Returns the path of the library loaded by the Zenoh-Flow runtime.\n+    ///\n+    /// The path is local to the machine where the Zenoh-Flow runtime is running.\n+    pub fn library_path(&self) -> &PathBuf {\n+        &self.library_path\n+    }\n }\ndiff --git a/zenoh-flow-runtime/src/loader/mod.rs b/zenoh-flow-runtime/src/loader/mod.rs\nindex 7e746b22..30618eee 100644\n--- a/zenoh-flow-runtime/src/loader/mod.rs\n+++ b/zenoh-flow-runtime/src/loader/mod.rs\n@@ -142,7 +142,7 @@ pub(crate) fn try_get_constructor<N>(\n #[derive(Default)]\n pub(crate) struct Loader {\n     pub(crate) extensions: Extensions,\n-    pub(crate) libraries: HashMap<Url, Arc<Library>>,\n+    pub(crate) libraries: HashMap<Url, (Arc<PathBuf>, Arc<Library>)>,\n }\n \n impl Deref for Loader {\n@@ -191,7 +191,7 @@ impl Loader {\n     pub(crate) fn remove_unused_libraries(&mut self) {\n         let number_libraries = self.libraries.len();\n         self.libraries\n-            .retain(|_, library| Arc::strong_count(library) > 1);\n+            .retain(|_, (_, library)| Arc::strong_count(library) > 1);\n         tracing::trace!(\n             \"Removed {} unused libraries.\",\n             number_libraries - self.libraries.len()\n@@ -217,12 +217,13 @@ impl Loader {\n         &mut self,\n         url: &Url,\n         node_symbol: &NodeSymbol,\n-    ) -> Result<(C, Arc<Library>)> {\n-        if let Some(library) = self.libraries.get(url) {\n-            return try_get_constructor(library.clone(), node_symbol);\n+    ) -> Result<(C, Arc<PathBuf>, Arc<Library>)> {\n+        if let Some((path, library)) = self.libraries.get(url) {\n+            let (constructor, library) = try_get_constructor::<C>(library.clone(), node_symbol)?;\n+            return Ok((constructor, path.clone(), library));\n         }\n \n-        let library = Arc::new(match url.scheme() {\n+        let (path, library) = match url.scheme() {\n             \"file\" => self\n                 .try_load_library_from_uri(url.path(), node_symbol)\n                 .context(format!(\"Failed to load library from file:\\n{}\", url.path()))?,\n@@ -231,12 +232,13 @@ impl Loader {\n                 url.scheme(),\n                 url\n             ),\n-        });\n+        };\n \n         let (constructor, library) = try_get_constructor::<C>(library, node_symbol)?;\n-        self.libraries.insert(url.clone(), library.clone());\n+        self.libraries\n+            .insert(url.clone(), (path.clone(), library.clone()));\n \n-        Ok((constructor, library))\n+        Ok((constructor, path, library))\n     }\n \n     /// Given the string representation of a path, attempts to load a library.\n@@ -253,14 +255,21 @@ impl Loader {\n         &self,\n         path: &str,\n         node_symbol: &NodeSymbol,\n-    ) -> Result<Library> {\n-        let path_buf = PathBuf::from_str(path)\n+    ) -> Result<(Arc<PathBuf>, Arc<Library>)> {\n+        let library_path = PathBuf::from_str(path)\n             .context(format!(\"Failed to convert path to a `PathBuf`:\\n{}\", path))?;\n \n-        let library_path = match path_buf.extension().and_then(|ext| ext.to_str()) {\n+        // The `rust_library_path`, exposing the symbols Zenoh-Flow will look for, is not always the same as the\n+        // `library_path`!\n+        //\n+        // For instance, for the Python extension:\n+        // - the `library_path` points to the location of the Python script,\n+        // - the `rust_library_path` points to the location of the Rust shared library that will then load the Python\n+        //   script.\n+        let rust_library_path = match library_path.extension().and_then(|ext| ext.to_str()) {\n             Some(extension) => {\n                 if extension == std::env::consts::DLL_EXTENSION {\n-                    &path_buf\n+                    &library_path\n                 } else {\n                     self.extensions\n                         .get_library_path(extension, node_symbol)\n@@ -268,28 +277,28 @@ impl Loader {\n                             anyhow!(\n                                 \"Cannot load library, no extension found for files of type < {} > :\\n{}\",\n                                 extension,\n-                                path_buf.display()\n+                                library_path.display()\n                             )\n                         })?\n                 }\n             }\n             None => bail!(\n                 \"Cannot load library, missing file extension:\\n{}\",\n-                path_buf.display()\n+                library_path.display()\n             ),\n         };\n \n-        let library_path = std::fs::canonicalize(library_path).context(format!(\n+        let rust_library_path = std::fs::canonicalize(rust_library_path).context(format!(\n             \"Failed to canonicalize path (did you put an absolute path?):\\n{}\",\n-            path_buf.display()\n+            library_path.display()\n         ))?;\n \n         #[cfg(any(target_family = \"unix\", target_family = \"windows\"))]\n-        Ok(unsafe {\n-            Library::new(&library_path).context(format!(\n+        Ok((Arc::new(library_path), unsafe {\n+            Arc::new(Library::new(&rust_library_path).context(format!(\n                 \"libloading::Library::new failed:\\n{}\",\n-                library_path.display()\n-            ))?\n-        })\n+                rust_library_path.display()\n+            ))?)\n+        }))\n     }\n }\ndiff --git a/zenoh-flow-runtime/src/runtime/load.rs b/zenoh-flow-runtime/src/runtime/load.rs\nindex 9e201021..f6670c9f 100644\n--- a/zenoh-flow-runtime/src/runtime/load.rs\n+++ b/zenoh-flow-runtime/src/runtime/load.rs\n@@ -33,6 +33,7 @@ use crate::InstanceState;\n use crate::{instance::DataFlowInstance, runners::Runner};\n \n use std::collections::HashMap;\n+use std::path::PathBuf;\n use std::sync::Arc;\n \n use anyhow::{bail, Context as _};\n@@ -115,41 +116,24 @@ impl Runtime {\n                 Err(e) => break 'load Err(e),\n             };\n \n-            let context = Context::new(\n-                data_flow.name().clone(),\n-                data_flow.instance_id().clone(),\n-                self.runtime_id.clone(),\n-            );\n-\n             runners.extend(\n-                match self\n-                    .try_load_operators(data_flow, &mut channels, context.clone())\n-                    .await\n-                {\n+                match self.try_load_operators(data_flow, &mut channels).await {\n                     Ok(operators) => operators,\n                     Err(e) => break 'load Err(e),\n                 },\n             );\n \n             runners.extend(\n-                match self\n-                    .try_load_sources(data_flow, &mut channels, context.clone())\n-                    .await\n-                {\n+                match self.try_load_sources(data_flow, &mut channels).await {\n                     Ok(sources) => sources,\n                     Err(e) => break 'load Err(e),\n                 },\n             );\n \n-            runners.extend(\n-                match self\n-                    .try_load_sinks(data_flow, &mut channels, context.clone())\n-                    .await\n-                {\n-                    Ok(sinks) => sinks,\n-                    Err(e) => break 'load Err(e),\n-                },\n-            );\n+            runners.extend(match self.try_load_sinks(data_flow, &mut channels).await {\n+                Ok(sinks) => sinks,\n+                Err(e) => break 'load Err(e),\n+            });\n \n             #[cfg(feature = \"zenoh\")]\n             {\n@@ -263,7 +247,6 @@ The problematic link is:\n         &self,\n         record: &DataFlowRecord,\n         channels: &mut Channels,\n-        context: Context,\n     ) -> Result<HashMap<NodeId, Runner>> {\n         let mut runners = HashMap::default();\n         let assigned_nodes = match record.mapping().get(&self.runtime_id) {\n@@ -284,9 +267,17 @@ The channels for the Inputs and Outputs of Operator < {} > were not created.\n                 &operator_id\n             ))?;\n \n-            let (constructor, library) = self\n+            let (constructor, path, library) = self\n                 .try_load_constructor::<OperatorFn>(&operator.library, &NodeSymbol::Operator)\n                 .await?;\n+\n+            let context = Context::new(\n+                record.name().clone(),\n+                record.instance_id().clone(),\n+                self.runtime_id.clone(),\n+                path,\n+            );\n+\n             let operator_node = (constructor)(\n                 context.clone(),\n                 operator.configuration.clone(),\n@@ -319,7 +310,6 @@ The channels for the Inputs and Outputs of Operator < {} > were not created.\n         &self,\n         record: &DataFlowRecord,\n         channels: &mut Channels,\n-        context: Context,\n     ) -> Result<HashMap<NodeId, Runner>> {\n         let mut runners = HashMap::default();\n         let assigned_nodes = match record.mapping().get(&self.runtime_id) {\n@@ -342,9 +332,17 @@ The channels for the Outputs of Source < {} > were not created.\n \n             let runner = match &source.source {\n                 SourceVariant::Library(uri) => {\n-                    let (constructor, library) = self\n+                    let (constructor, path, library) = self\n                         .try_load_constructor::<SourceFn>(uri, &NodeSymbol::Source)\n                         .await?;\n+\n+                    let context = Context::new(\n+                        record.name().clone(),\n+                        record.instance_id().clone(),\n+                        self.runtime_id.clone(),\n+                        path,\n+                    );\n+\n                     let source_node =\n                         (constructor)(context.clone(), source.configuration.clone(), outputs)\n                             .await?;\n@@ -391,7 +389,6 @@ Maybe change the features in the Cargo.toml?\n         &self,\n         record: &DataFlowRecord,\n         channels: &mut Channels,\n-        context: Context,\n     ) -> Result<HashMap<NodeId, Runner>> {\n         let mut runners = HashMap::default();\n         let assigned_nodes = match record.mapping().get(&self.runtime_id) {\n@@ -414,9 +411,17 @@ The channels for the Inputs of Sink < {} > were not created.\n \n             let runner = match &sink.sink {\n                 SinkVariant::Library(uri) => {\n-                    let (constructor, library) = self\n+                    let (constructor, library_path, library) = self\n                         .try_load_constructor::<SinkFn>(uri, &NodeSymbol::Sink)\n                         .await?;\n+\n+                    let context = Context::new(\n+                        record.name().clone(),\n+                        record.instance_id().clone(),\n+                        self.runtime_id.clone(),\n+                        library_path,\n+                    );\n+\n                     let sink_node =\n                         (constructor)(context.clone(), sink.configuration.clone(), inputs).await?;\n \n@@ -569,7 +574,7 @@ The channels for the Inputs of Connector Sender < {} > were not created.\n         &self,\n         url: &Url,\n         node_symbol: &NodeSymbol,\n-    ) -> Result<(C, Arc<Library>)> {\n+    ) -> Result<(C, Arc<PathBuf>, Arc<Library>)> {\n         let mut loader_write_guard = self.loader.lock().await;\n         loader_write_guard.try_load_constructor::<C>(url, node_symbol)\n     }\n", "instance_id": "eclipse-zenoh-flow__zenoh-flow-247", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to provide the `Path` of the library in the `Context` for cleaner integration, particularly with Python bindings using `pyo3`. It outlines the motivation and the general goal of the feature. However, it lacks critical details such as specific input/output expectations, constraints, or examples of usage. Additionally, there is no mention of potential edge cases or specific requirements for how the library path should be handled or validated. While the intent is understandable, these missing details prevent it from being comprehensive, resulting in a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes spans multiple files (`context.rs`, `loader/mod.rs`, and `runtime/load.rs`), requiring modifications to the `Context` structure, library loading logic, and runtime initialization. This indicates a need to understand interactions between different parts of the codebase, such as how libraries are loaded and how context is passed to various components (e.g., operators, sources, sinks). Second, the number of technical concepts involved includes familiarity with Rust's `std::path::PathBuf`, `Arc` for thread-safe reference counting, and the dynamic library loading mechanism (`libloading`). While these concepts are not overly complex for an experienced Rust developer, they do require a solid understanding of Rust's ownership model and concurrency primitives. Third, the changes do not appear to significantly impact the system's architecture but do involve a moderate amount of code modification (adding fields, updating function signatures, and adjusting data structures like `HashMap`). Lastly, the problem statement does not explicitly mention edge cases or error handling requirements, but the code changes suggest some implicit handling (e.g., canonicalizing paths and potential errors in library loading), which adds a slight layer of complexity. Overall, this task requires understanding multiple concepts and making coordinated changes across files, but it does not involve deep architectural refactoring or advanced domain-specific knowledge, justifying a difficulty score of 0.45.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Unfocused Wezterm window content doesn't get drawn until \"damage\" occurs\n### What Operating System(s) are you seeing this problem on?\n\nLinux X11\n\n### Which Wayland compositor or X11 Window manager(s) are you using?\n\nXfce4 4.18 / Xorg 21.1.13\n\n### WezTerm version\n\n20240509-223025-91a16e52\n\n### Did you try the latest nightly build to see if the issue is better (or worse!) than your current version?\n\nYes, and I updated the version box above to show the version of the nightly that I tried\n\n### Describe the bug\n\nWhen switching to a virtual desktop unfocused Wezterm windows don't get drawn until some sort of \"damage\" (mouse over, focusing, etc.) occurs.\r\n\r\n![wezterm-unfocused](https://github.com/wez/wezterm/assets/4973300/4f098bb2-409e-43fb-bb33-32d7f33ae429)\r\n\r\nThis happens with or without an active compositor.\r\nDefault or front_end = \"OpenGL\" makes no difference either\r\n\n\n### To Reproduce\n\nSwitch away and back to a virtual desktop that has unfocused Wezterm windows on it.\n\n### Configuration\n\nno config\r\n\r\nTested with:  WEZTERM_CONFIG_FILE=/dev/null wezterm\n\n### Expected Behavior\n\nWindows should get drawn as soon as they become visible\n\n### Logs\n\n_No response_\n\n### Anything else?\n\nCommit 0184e88 (29. April 2024) works fine. This issue started happening after that release.\n", "patch": "diff --git a/window/src/os/x11/connection.rs b/window/src/os/x11/connection.rs\nindex 0a11d75d583..3f078b82f08 100644\n--- a/window/src/os/x11/connection.rs\n+++ b/window/src/os/x11/connection.rs\n@@ -70,6 +70,7 @@ pub struct XConnection {\n     pub atom_net_active_window: Atom,\n     pub(crate) xrm: RefCell<HashMap<String, String>>,\n     pub(crate) windows: RefCell<HashMap<xcb::x::Window, Arc<Mutex<XWindowInner>>>>,\n+    pub(crate) child_to_parent_id: RefCell<HashMap<xcb::x::Window, xcb::x::Window>>,\n     should_terminate: RefCell<bool>,\n     pub(crate) visual: xcb::x::Visualtype,\n     pub(crate) depth: u8,\n@@ -578,6 +579,10 @@ impl XConnection {\n         self.windows.borrow().get(&window_id).map(Arc::clone)\n     }\n \n+    fn parent_id_by_child_id(&self, child_id: xcb::x::Window) -> Option<xcb::x::Window> {\n+        self.child_to_parent_id.borrow().get(&child_id).copied()\n+    }\n+\n     fn dispatch_pending_events(&self) -> anyhow::Result<()> {\n         for window in self.windows.borrow().values() {\n             let mut inner = window.lock().unwrap();\n@@ -595,6 +600,11 @@ impl XConnection {\n         if let Some(window) = self.window_by_id(window_id) {\n             let mut inner = window.lock().unwrap();\n             inner.dispatch_event(event)?;\n+        } else if let Some(parent_id) = self.parent_id_by_child_id(window_id) {\n+            if let Some(window) = self.window_by_id(parent_id) {\n+                let mut inner = window.lock().unwrap();\n+                inner.dispatch_event(event)?;\n+            }\n         }\n         Ok(())\n     }\n@@ -804,6 +814,7 @@ impl XConnection {\n             atom_xsel_data,\n             atom_targets,\n             windows: RefCell::new(HashMap::new()),\n+            child_to_parent_id: RefCell::new(HashMap::new()),\n             should_terminate: RefCell::new(false),\n             depth,\n             visual,\ndiff --git a/window/src/os/x11/window.rs b/window/src/os/x11/window.rs\nindex 8b434c32a89..58ec4f8936f 100644\n--- a/window/src/os/x11/window.rs\n+++ b/window/src/os/x11/window.rs\n@@ -792,6 +792,7 @@ impl XWindowInner {\n             Event::X(xcb::x::Event::DestroyNotify(_)) => {\n                 self.events.dispatch(WindowEvent::Destroyed);\n                 conn.windows.borrow_mut().remove(&self.window_id);\n+                conn.child_to_parent_id.borrow_mut().remove(&self.child_id);\n             }\n             Event::X(xcb::x::Event::SelectionClear(e)) => {\n                 self.selection_clear(e)?;\n@@ -1414,8 +1415,7 @@ impl XWindow {\n                     xcb::x::Cw::BackPixel(0), // transparent background\n                     xcb::x::Cw::BorderPixel(screen.black_pixel()),\n                     xcb::x::Cw::EventMask(\n-                        xcb::x::EventMask::EXPOSURE\n-                            | xcb::x::EventMask::FOCUS_CHANGE\n+                        xcb::x::EventMask::FOCUS_CHANGE\n                             | xcb::x::EventMask::KEY_PRESS\n                             | xcb::x::EventMask::BUTTON_PRESS\n                             | xcb::x::EventMask::BUTTON_RELEASE\n@@ -1449,6 +1449,7 @@ impl XWindow {\n                     xcb::x::Cw::BackPixel(0), // transparent background\n                     xcb::x::Cw::BorderPixel(screen.black_pixel()),\n                     xcb::x::Cw::BitGravity(xcb::x::Gravity::NorthWest),\n+                    xcb::x::Cw::EventMask(xcb::x::EventMask::EXPOSURE),\n                     xcb::x::Cw::Colormap(color_map_id),\n                 ],\n             })\n@@ -1538,6 +1539,9 @@ impl XWindow {\n         let window_handle = Window::X11(XWindow::from_id(window_id));\n \n         conn.windows.borrow_mut().insert(window_id, window);\n+        conn.child_to_parent_id\n+            .borrow_mut()\n+            .insert(child_id, window_id);\n \n         window_handle.set_title(name);\n         // Before we map the window, flush to ensure that all of the other properties\n@@ -1581,6 +1585,10 @@ impl XWindowInner {\n         // Drop impl, and that cannot succeed after we've\n         // destroyed the window at the X11 level.\n         self.conn().windows.borrow_mut().remove(&self.window_id);\n+        self.conn()\n+            .child_to_parent_id\n+            .borrow_mut()\n+            .remove(&self.child_id);\n \n         // Unmap the window first: calling DestroyWindow here may race\n         // with some requests made either by EGL or the IME, but I haven't\n", "instance_id": "wezterm__wezterm-5408", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: unfocused Wezterm windows on Linux X11 with Xfce4 do not get drawn until some \"damage\" (like mouse over or focusing) occurs. It provides specific details about the operating system, window manager, Wezterm version, and even a visual example via an image link. Steps to reproduce are provided, and the expected behavior is explicitly stated. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether this issue affects all unfocused windows or only specific ones under certain conditions (e.g., specific virtual desktops or window states). Additionally, while it mentions a commit where the issue started, it lacks deeper context about what might have changed in the codebase to cause this regression. Edge cases or specific configurations that might exacerbate the issue are also not mentioned. Overall, the statement is valid and clear but misses some minor details that could aid in fully understanding the scope of the problem.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes involves modifications across multiple files (`connection.rs` and `window.rs`) in the X11-specific part of the Wezterm codebase, indicating a need to understand interactions between window management and event handling in an X11 environment. The changes introduce a mapping between child and parent window IDs to handle events for child windows, which suggests a moderate architectural impact on how events are dispatched. Second, the number of technical concepts required to solve this includes a deep understanding of X11 window management (e.g., event masks like EXPOSURE, window hierarchies), Rust's concurrency model (e.g., `RefCell`, `Arc`, `Mutex`), and the specific behavior of Wezterm's rendering and event loop. These concepts are moderately complex, especially for someone unfamiliar with low-level windowing systems. Third, the problem likely involves handling edge cases, such as ensuring that event dispatching to parent windows does not introduce unintended side effects or performance issues, though these are not explicitly mentioned in the problem statement. Finally, the fact that this is a regression introduced after a specific commit implies a need for debugging and understanding historical changes in the codebase, adding to the complexity. While not at the extreme end of difficulty (e.g., requiring system-level redesign or advanced domain knowledge beyond X11), this problem demands a solid grasp of the Wezterm architecture and X11 intricacies, justifying a score of 0.65.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Implement system window menu\nThe system window menu normally shows as the context menu of title-bars, also shows when clicking in the window icon (Windows).\r\n\r\nAdd a  `WINDOWS` method and view API and  to cause this menu to show on demand. Winit provides the [`show_window_menu`] method.\r\n\r\n\r\n[`show_window_menu`]: https://docs.rs/winit/latest/winit/window/struct.Window.html#method.show_window_menu\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 20219a344..574721d59 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,5 +1,8 @@\n # Unpublished\n \n+* **Breaking** Add `open_title_bar_context_menu` to view API.\n+    - Note that this is only a breaking change for direct dependents of `zng-view-api`.\n+* Add `OPEN_TITLE_BAR_CONTEXT_MENU_CMD` to window API.\n \n # 0.4.0\n \n@@ -26,7 +29,7 @@\n * Fix `AnsiText!` not resetting style.\n * `Markdown!` widget now uses `AnsiText!` for ```console code block.\n * Fix `auto_size` not using the min/max_size constraints.\n-* **Braking:** Change return type of `SCROLL.vertical_offset`, `SCROLL.horizontal_offset` and `SCROLL.zoom_scale`.\n+* **Breaking:** Change return type of `SCROLL.vertical_offset`, `SCROLL.horizontal_offset` and `SCROLL.zoom_scale`.\n     - Changed only from `ReadOnlyContextVar<Factor>` to `ContextVar<Factor>` so it has minimal impact.\n * Add `vertical_offset`, `horizontal_offset` and `zoom_scale` properties in `Scroll!`.\n     - Users should prefer using scroll commands over these properties, but they are useful for implementing features\ndiff --git a/crates/zng-app/src/view_process.rs b/crates/zng-app/src/view_process.rs\nindex fa8cc7320..e07f359cb 100644\n--- a/crates/zng-app/src/view_process.rs\n+++ b/crates/zng-app/src/view_process.rs\n@@ -777,6 +777,11 @@ impl ViewWindow {\n         self.0.call(|id, p| p.set_focus_indicator(id, indicator))\n     }\n \n+    /// Open system title bar context menu.\n+    pub fn open_title_bar_context_menu(&self, position: DipPoint) -> Result<()> {\n+        self.0.call(|id, p| p.open_title_bar_context_menu(id, position))\n+    }\n+\n     /// Shows a native message dialog for the window.\n     ///\n     /// The window is not interactive while the dialog is visible and the dialog may be modal in the view-process.\ndiff --git a/crates/zng-ext-window/src/cmd.rs b/crates/zng-ext-window/src/cmd.rs\nindex 0467a06f9..354be9321 100644\n--- a/crates/zng-ext-window/src/cmd.rs\n+++ b/crates/zng-ext-window/src/cmd.rs\n@@ -73,6 +73,17 @@ command! {\n     ///\n     /// If any IME preview is active close it without committing.\n     pub static CANCEL_IME_CMD;\n+\n+    /// Represents the window **open title bar context menu** action.\n+    ///\n+    /// # Parameter\n+    ///\n+    /// This command supports an optional parameter, it can be a [`DipPoint`] or [`PxPoint`] that defines\n+    /// the menu position.\n+    ///\n+    /// [`DipPoint`]: zng_layout::unit::DipPoint\n+    /// [`PxPoint`]: zng_layout::unit::PxPoint\n+    pub static OPEN_TITLE_BAR_CONTEXT_MENU_CMD;\n }\n \n pub(super) struct WindowCommands {\n@@ -93,7 +104,6 @@ impl WindowCommands {\n             restore_handle: RESTORE_CMD.scoped(window_id).subscribe(false),\n             fullscreen_handle: FULLSCREEN_CMD.scoped(window_id).subscribe(true),\n             exclusive_handle: EXCLUSIVE_FULLSCREEN_CMD.scoped(window_id).subscribe(true),\n-\n             close_handle: CLOSE_CMD.scoped(window_id).subscribe(true),\n         }\n     }\ndiff --git a/crates/zng-ext-window/src/control.rs b/crates/zng-ext-window/src/control.rs\nindex a2ab2abcc..96bbf6fae 100644\n--- a/crates/zng-ext-window/src/control.rs\n+++ b/crates/zng-ext-window/src/control.rs\n@@ -30,8 +30,8 @@ use zng_ext_image::{ImageRenderArgs, ImageSource, ImageVar, Img, IMAGES};\n use zng_layout::{\n     context::{LayoutMetrics, LayoutPassId, DIRECTION_VAR, LAYOUT},\n     unit::{\n-        Dip, DipRect, DipSize, DipToPx, Factor, FactorUnits, Layout1d, Layout2d, Length, Ppi, Px, PxConstraints, PxPoint, PxRect, PxSize,\n-        PxToDip, PxVector, TimeUnits,\n+        Dip, DipPoint, DipRect, DipSize, DipToPx, Factor, FactorUnits, Layout1d, Layout2d, Length, Ppi, Px, PxConstraints, PxPoint, PxRect,\n+        PxSize, PxToDip, PxVector, TimeUnits,\n     },\n };\n use zng_var::{AnyVar, ReadOnlyArcVar, Var, VarHandle, VarHandles};\n@@ -106,6 +106,7 @@ struct HeadedCtrl {\n     render_access_update: Option<WidgetInfoTree>, // previous info tree\n     ime_info: Option<ImeInfo>,\n     cancel_ime_handle: CommandHandle,\n+    open_title_menu_handle: CommandHandle,\n }\n impl HeadedCtrl {\n     pub fn new(vars: &WindowVars, commands: WindowCommands, content: WindowRoot) -> Self {\n@@ -136,6 +137,7 @@ impl HeadedCtrl {\n             render_access_update: None,\n             ime_info: None,\n             cancel_ime_handle: CommandHandle::dummy(),\n+            open_title_menu_handle: CommandHandle::dummy(),\n         }\n     }\n \n@@ -671,6 +673,7 @@ impl HeadedCtrl {\n \n                 self.window = Some(args.window.clone());\n                 self.cancel_ime_handle = super::cmd::CANCEL_IME_CMD.scoped(WINDOW.id()).subscribe(true);\n+                self.open_title_menu_handle = super::cmd::OPEN_TITLE_BAR_CONTEXT_MENU_CMD.scoped(WINDOW.id()).subscribe(true);\n \n                 self.vars.0.render_mode.set(args.data.render_mode);\n                 self.vars.state().set(args.data.state.state);\n@@ -768,6 +771,7 @@ impl HeadedCtrl {\n \n                     self.window = None;\n                     self.cancel_ime_handle = CommandHandle::dummy();\n+                    self.open_title_menu_handle = CommandHandle::dummy();\n                     self.waiting_view = false;\n                     self.delayed_view_updates = vec![];\n                     self.respawned = true;\n@@ -789,7 +793,23 @@ impl HeadedCtrl {\n             if let Some(w) = &self.window {\n                 let _ = w.set_ime_area(None);\n             }\n-        }\n+        } else if let Some(args) = super::cmd::OPEN_TITLE_BAR_CONTEXT_MENU_CMD.scoped(WINDOW.id()).on(update) {\n+            let pos = args.handle_enabled(&self.open_title_menu_handle, |args| {\n+                let pos = if let Some(p) = args.param::<DipPoint>() {\n+                    *p\n+                } else if let Some(p) = args.param::<PxPoint>() {\n+                    p.to_dip(self.vars.scale_factor().get())\n+                } else {\n+                    DipPoint::splat(Dip::new(24))\n+                };\n+                pos\n+            });\n+            if let Some(pos) = pos {\n+                self.view_task(Box::new(move |w| {\n+                    let _ = w.unwrap().open_title_bar_context_menu(pos);\n+                }));\n+            }\n+        };\n     }\n \n     pub fn ui_event(&mut self, update: &EventUpdate) {\n@@ -1289,6 +1309,7 @@ impl HeadedCtrl {\n         self.content.close();\n         self.window = None;\n         self.cancel_ime_handle = CommandHandle::dummy();\n+        self.cancel_ime_handle = CommandHandle::dummy();\n     }\n \n     fn view_task(&mut self, task: Box<dyn FnOnce(Option<&ViewWindow>) + Send>) {\ndiff --git a/crates/zng-view-api/src/lib.rs b/crates/zng-view-api/src/lib.rs\nindex 289fcec4d..cbcae8b27 100644\n--- a/crates/zng-view-api/src/lib.rs\n+++ b/crates/zng-view-api/src/lib.rs\n@@ -58,7 +58,7 @@ use font::{FontFaceId, FontId, FontOptions, FontVariationName};\n use image::{ImageId, ImageMaskMode, ImageRequest, ImageTextureId};\n use ipc::{IpcBytes, IpcBytesReceiver};\n use window::WindowId;\n-use zng_unit::{DipRect, DipSize, Factor, Px, PxRect};\n+use zng_unit::{DipPoint, DipRect, DipSize, Factor, Px, PxRect};\n \n /// Packaged API request.\n #[derive(Debug)]\n@@ -339,6 +339,9 @@ declare_api! {\n     /// This request can steal focus from other apps disrupting the user, be careful with it.\n     pub fn focus_window(&mut self, id: WindowId);\n \n+    /// Open the system title bar context menu.\n+    pub fn open_title_bar_context_menu(&mut self, id: WindowId, position: DipPoint);\n+\n     /// Cache an image resource.\n     ///\n     /// The image is decoded asynchronously, the events [`Event::ImageMetadataLoaded`], [`Event::ImageLoaded`]\ndiff --git a/crates/zng-view/src/lib.rs b/crates/zng-view/src/lib.rs\nindex 219c323c9..81c6fcbdf 100644\n--- a/crates/zng-view/src/lib.rs\n+++ b/crates/zng-view/src/lib.rs\n@@ -1626,6 +1626,10 @@ impl Api for App {\n         }\n     }\n \n+    fn open_title_bar_context_menu(&mut self, id: WindowId, position: DipPoint) {\n+        self.with_window(id, |w| w.open_title_bar_context_menu(position), || ())\n+    }\n+\n     fn set_cursor(&mut self, id: WindowId, icon: Option<CursorIcon>) {\n         self.with_window(id, |w| w.set_cursor(icon), || ())\n     }\ndiff --git a/crates/zng-view/src/window.rs b/crates/zng-view/src/window.rs\nindex 603a6c7ec..7e3cb08d9 100644\n--- a/crates/zng-view/src/window.rs\n+++ b/crates/zng-view/src/window.rs\n@@ -1098,6 +1098,11 @@ impl Window {\n         true\n     }\n \n+    /// Open windows title bar context menu.\n+    pub fn open_title_bar_context_menu(&self, pos: DipPoint) {\n+        self.window.show_window_menu(pos.to_winit())\n+    }\n+\n     fn apply_state(&mut self, new_state: WindowStateAll, force: bool) {\n         if self.state.chrome_visible != new_state.chrome_visible {\n             self.window.set_decorations(new_state.chrome_visible);\ndiff --git a/examples/window.rs b/examples/window.rs\nindex 5fdfe14cb..827198f6e 100644\n--- a/examples/window.rs\n+++ b/examples/window.rs\n@@ -68,6 +68,11 @@ async fn main_window() -> window::WindowRoot {\n         background_color = color_scheme_map(colors::BLACK, colors::WHITE);\n         padding = 4;\n         corner_radius = (0, 0, 5, 5);\n+        gesture::on_context_click = hn!(|args: &gesture::ClickArgs| {\n+            if let Some(p) = args.position() {\n+                window::cmd::OPEN_TITLE_BAR_CONTEXT_MENU_CMD.scoped(WINDOW.id()).notify_param(p);\n+            }\n+        });\n     };\n     LAYERS.insert(LayerIndex::TOP_MOST, custom_chrome);\n \n", "instance_id": "zng-ui__zng-145", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to implement a system window menu feature using the `show_window_menu` method provided by the Winit library. It specifies the goal of adding a method to display the context menu of title bars on demand and mentions the relevant API. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input format for the position parameter (though it can be inferred from the code changes as a `DipPoint`). Additionally, there are no mentions of specific constraints, edge cases, or platform-specific behaviors that might affect the implementation. While the intent is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files and modules (e.g., `zng-app`, `zng-ext-window`, `zng-view-api`, etc.), requiring an understanding of the interactions between different parts of the codebase, such as the window control logic, command handling, and API integration. However, the actual changes are relatively straightforward, involving the addition of a new command (`OPEN_TITLE_BAR_CONTEXT_MENU_CMD`), updating the view API, and integrating with the Winit library's `show_window_menu` method. The number of technical concepts involved is moderate, including familiarity with Rust, command patterns, unit conversions (e.g., `DipPoint` to Winit-compatible units), and event handling. Edge cases, such as invalid position parameters or platform-specific behavior of the context menu, are not explicitly mentioned in the problem statement but are handled minimally in the code (e.g., default position if none provided). The changes do not significantly impact the system's architecture, and the overall complexity of the logic is manageable for someone with intermediate experience in Rust and GUI frameworks. Therefore, a score of 0.45 reflects a medium difficulty level, requiring understanding of multiple concepts and cross-file modifications but not posing significant architectural or algorithmic challenges.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`Default` trait implementation for `Opt` shouldn\u2019t have side-effects\n## Describe the bug\r\n\r\nAs things are right now, `Opt::default()` on `pingora_core::server::configuration::Opt` is implemented as an alias for `Opt::parse()`:\r\n\r\nhttps://github.com/cloudflare/pingora/blob/216d8e9d92c75d46cba78da9b167fb41dec91ad7/pingora-core/src/server/configuration/mod.rs#L168-L172\r\n\r\nThis can cause unexpected side-effects if someone (like me) gets some Pingora options by other means and merely wants to fill up the data. Depending on the command line parameters the application might even shut down.\r\n\r\n## Pingora info\r\n\r\n**Pingora version**: 0.2.0 or [216d8e9](https://github.com/cloudflare/pingora/commit/216d8e9d92c75d46cba78da9b167fb41dec91ad7)\r\n\r\n## Steps to reproduce\r\n\r\nConsider the following application:\r\n\r\n```rust\r\nuse pingora_core::server::{configuration::{Opt, ServerConf}, Server};\r\n\r\nfn main() {\r\n    let server = Server::new_with_opt_and_conf(\r\n        Opt {\r\n            daemon: true,\r\n            ..Default::default()\r\n        },\r\n        ServerConf::default(),\r\n    );\r\n    println!(\"Server created\");\r\n}\r\n```\r\n\r\nRun it using `cargo run -- -h` or `cargo run -- --blub` command.\r\n\r\n## Expected results\r\n\r\nThe application should be able to create a server regardless of its command line flags and print: \u201cServer created\u201d\r\n\r\n## Observed results\r\n\r\nUsing the first command, the application prints command line help which doesn\u2019t apply to the application and exits. Using the first command, it produces the error message \u201cerror: Found argument '--blub' which wasn't expected, or isn't valid in this context\u201d and exits. In both cases the message is never printed.\r\n\r\n## Additional context\r\n\r\nThe `Default` trait implementation can be derived automatically here, it will set all the boolean flags to `false` and the configuration path to `None`. If a trait-less alias for `Opt::parse()` is required, it can be provided under a dedicated and unambiguous name like `Opt::parse_command_line()`.\n", "patch": "diff --git a/.bleep b/.bleep\nindex 84aa6357c..cee92a797 100644\n--- a/.bleep\n+++ b/.bleep\n@@ -1,1 +1,1 @@\n-f70d8b77a4085cbe11b9559317f6d6e7e49914db\n\\ No newline at end of file\n+361d88592075f7f98f581b139d0349f1b70190a2\n\\ No newline at end of file\ndiff --git a/docs/quick_start.md b/docs/quick_start.md\nindex 5099f7ec0..8a3b796b4 100644\n--- a/docs/quick_start.md\n+++ b/docs/quick_start.md\n@@ -237,7 +237,7 @@ take advantage of with single-line change.\n \n ```rust\n fn main() {\n-    let mut my_server = Server::new(Some(Opt::default())).unwrap();\n+    let mut my_server = Server::new(Some(Opt::parse_args())).unwrap();\n     ...\n }\n ```\ndiff --git a/pingora-cache/src/lib.rs b/pingora-cache/src/lib.rs\nindex 563727cfd..6ba057e30 100644\n--- a/pingora-cache/src/lib.rs\n+++ b/pingora-cache/src/lib.rs\n@@ -44,7 +44,7 @@ pub use key::CacheKey;\n use lock::{CacheLock, LockStatus, Locked};\n pub use memory::MemCache;\n pub use meta::{CacheMeta, CacheMetaDefaults};\n-pub use storage::{HitHandler, MissHandler, Storage};\n+pub use storage::{HitHandler, MissHandler, PurgeType, Storage};\n pub use variance::VarianceBuilder;\n \n pub mod prelude {}\n@@ -658,7 +658,10 @@ impl HttpCache {\n                     let handle = span.handle();\n                     for item in evicted {\n                         // TODO: warn/log the error\n-                        let _ = inner.storage.purge(&item, &handle).await;\n+                        let _ = inner\n+                            .storage\n+                            .purge(&item, PurgeType::Eviction, &handle)\n+                            .await;\n                     }\n                 }\n                 inner.traces.finish_miss_span();\n@@ -1063,7 +1066,10 @@ impl HttpCache {\n                 let inner = self.inner_mut();\n                 let mut span = inner.traces.child(\"purge\");\n                 let key = inner.key.as_ref().unwrap().to_compact();\n-                let result = inner.storage.purge(&key, &span.handle()).await;\n+                let result = inner\n+                    .storage\n+                    .purge(&key, PurgeType::Invalidation, &span.handle())\n+                    .await;\n                 // FIXME: also need to remove from eviction manager\n                 span.set_tag(|| trace::Tag::new(\"purged\", matches!(result, Ok(true))));\n                 result\ndiff --git a/pingora-cache/src/memory.rs b/pingora-cache/src/memory.rs\nindex 3863b2bd3..dec8f1ad3 100644\n--- a/pingora-cache/src/memory.rs\n+++ b/pingora-cache/src/memory.rs\n@@ -306,7 +306,12 @@ impl Storage for MemCache {\n         Ok(Box::new(miss_handler))\n     }\n \n-    async fn purge(&'static self, key: &CompactCacheKey, _trace: &SpanHandle) -> Result<bool> {\n+    async fn purge(\n+        &'static self,\n+        key: &CompactCacheKey,\n+        _type: PurgeType,\n+        _trace: &SpanHandle,\n+    ) -> Result<bool> {\n         // This usually purges the primary key because, without a lookup, the variance key is usually\n         // empty\n         let hash = key.combined();\n@@ -525,7 +530,9 @@ mod test {\n \n         assert!(cache.temp.read().contains_key(&hash));\n \n-        let result = cache.purge(&key, &Span::inactive().handle()).await;\n+        let result = cache\n+            .purge(&key, PurgeType::Invalidation, &Span::inactive().handle())\n+            .await;\n         assert!(result.is_ok());\n \n         assert!(!cache.temp.read().contains_key(&hash));\n@@ -551,7 +558,9 @@ mod test {\n \n         assert!(cache.cached.read().contains_key(&hash));\n \n-        let result = cache.purge(&key, &Span::inactive().handle()).await;\n+        let result = cache\n+            .purge(&key, PurgeType::Invalidation, &Span::inactive().handle())\n+            .await;\n         assert!(result.is_ok());\n \n         assert!(!cache.cached.read().contains_key(&hash));\ndiff --git a/pingora-cache/src/put.rs b/pingora-cache/src/put.rs\nindex e29cfadfb..e1041e00f 100644\n--- a/pingora-cache/src/put.rs\n+++ b/pingora-cache/src/put.rs\n@@ -119,7 +119,7 @@ impl<C: CachePut> CachePutCtx<C> {\n                 .handle();\n             for item in evicted {\n                 // TODO: warn/log the error\n-                let _ = self.storage.purge(&item, &trace).await;\n+                let _ = self.storage.purge(&item, PurgeType::Eviction, &trace).await;\n             }\n         }\n \ndiff --git a/pingora-cache/src/storage.rs b/pingora-cache/src/storage.rs\nindex c6365c7cb..0d4e1041b 100644\n--- a/pingora-cache/src/storage.rs\n+++ b/pingora-cache/src/storage.rs\n@@ -22,6 +22,15 @@ use async_trait::async_trait;\n use pingora_error::Result;\n use std::any::Any;\n \n+/// The reason a purge() is called\n+#[derive(Debug, Clone, Copy)]\n+pub enum PurgeType {\n+    // For eviction because the cache storage is full\n+    Eviction,\n+    // For cache invalidation\n+    Invalidation,\n+}\n+\n /// Cache storage interface\n #[async_trait]\n pub trait Storage {\n@@ -45,7 +54,12 @@ pub trait Storage {\n     /// Delete the cached asset for the given key\n     ///\n     /// [CompactCacheKey] is used here because it is how eviction managers store the keys\n-    async fn purge(&'static self, key: &CompactCacheKey, trace: &SpanHandle) -> Result<bool>;\n+    async fn purge(\n+        &'static self,\n+        key: &CompactCacheKey,\n+        purge_type: PurgeType,\n+        trace: &SpanHandle,\n+    ) -> Result<bool>;\n \n     /// Update cache header and metadata for the already stored asset.\n     async fn update_meta(\ndiff --git a/pingora-core/src/connectors/http/v1.rs b/pingora-core/src/connectors/http/v1.rs\nindex 7958a0939..66f7bbba7 100644\n--- a/pingora-core/src/connectors/http/v1.rs\n+++ b/pingora-core/src/connectors/http/v1.rs\n@@ -51,10 +51,11 @@ impl Connector {\n \n     pub async fn release_http_session<P: Peer + Send + Sync + 'static>(\n         &self,\n-        session: HttpSession,\n+        mut session: HttpSession,\n         peer: &P,\n         idle_timeout: Option<Duration>,\n     ) {\n+        session.respect_keepalive();\n         if let Some(stream) = session.reuse().await {\n             self.transport\n                 .release_stream(stream, peer.reuse_hash(), idle_timeout);\ndiff --git a/pingora-core/src/connectors/http/v2.rs b/pingora-core/src/connectors/http/v2.rs\nindex 2c11ede17..a0829431a 100644\n--- a/pingora-core/src/connectors/http/v2.rs\n+++ b/pingora-core/src/connectors/http/v2.rs\n@@ -102,6 +102,10 @@ impl ConnectionRef {\n         &self.0.digest\n     }\n \n+    pub fn digest_mut(&mut self) -> Option<&mut Digest> {\n+        Arc::get_mut(&mut self.0).map(|inner| &mut inner.digest)\n+    }\n+\n     pub fn ping_timedout(&self) -> bool {\n         self.0.ping_timeout_occurred.load(Ordering::Relaxed)\n     }\ndiff --git a/pingora-core/src/listeners/tls.rs b/pingora-core/src/listeners/tls.rs\nindex 655346fd8..1dd63d462 100644\n--- a/pingora-core/src/listeners/tls.rs\n+++ b/pingora-core/src/listeners/tls.rs\n@@ -38,6 +38,15 @@ pub struct TlsSettings {\n     callbacks: Option<TlsAcceptCallbacks>,\n }\n \n+impl From<SslAcceptorBuilder> for TlsSettings {\n+    fn from(settings: SslAcceptorBuilder) -> Self {\n+        TlsSettings {\n+            accept_builder: settings,\n+            callbacks: None,\n+        }\n+    }\n+}\n+\n impl Deref for TlsSettings {\n     type Target = SslAcceptorBuilder;\n \ndiff --git a/pingora-core/src/modules/http/mod.rs b/pingora-core/src/modules/http/mod.rs\nindex 31c5f9a21..02a3e0c39 100644\n--- a/pingora-core/src/modules/http/mod.rs\n+++ b/pingora-core/src/modules/http/mod.rs\n@@ -65,7 +65,7 @@ pub trait HttpModule {\n     fn as_any_mut(&mut self) -> &mut dyn Any;\n }\n \n-type Module = Box<dyn HttpModule + 'static + Send + Sync>;\n+pub type Module = Box<dyn HttpModule + 'static + Send + Sync>;\n \n /// Trait to init the http module ctx for each request\n pub trait HttpModuleBuilder {\ndiff --git a/pingora-core/src/protocols/http/client.rs b/pingora-core/src/protocols/http/client.rs\nindex 5c44b6f2d..4d3044dcd 100644\n--- a/pingora-core/src/protocols/http/client.rs\n+++ b/pingora-core/src/protocols/http/client.rs\n@@ -89,7 +89,9 @@ impl HttpSession {\n \n     /// Set the write timeout for writing header and body.\n     ///\n-    /// The timeout is per write operation, not on the overall time writing the entire request\n+    /// The timeout is per write operation, not on the overall time writing the entire request.\n+    ///\n+    /// This is a noop for h2.\n     pub fn set_write_timeout(&mut self, timeout: Duration) {\n         match self {\n             HttpSession::H1(h1) => h1.write_timeout = Some(timeout),\n@@ -151,7 +153,7 @@ impl HttpSession {\n     /// Return the [Digest] of the connection\n     ///\n     /// For reused connection, the timing in the digest will reflect its initial handshakes\n-    /// The caller should check if the connection is reused to avoid misuse of the timing field\n+    /// The caller should check if the connection is reused to avoid misuse of the timing field.\n     pub fn digest(&self) -> Option<&Digest> {\n         match self {\n             Self::H1(s) => Some(s.digest()),\n@@ -159,6 +161,16 @@ impl HttpSession {\n         }\n     }\n \n+    /// Return a mutable [Digest] reference for the connection, see [`digest`] for more details.\n+    ///\n+    /// Will return `None` if this is an H2 session and multiple streams are open.\n+    pub fn digest_mut(&mut self) -> Option<&mut Digest> {\n+        match self {\n+            Self::H1(s) => Some(s.digest_mut()),\n+            Self::H2(s) => s.digest_mut(),\n+        }\n+    }\n+\n     /// Return the server (peer) address of the connection.\n     pub fn server_addr(&self) -> Option<&SocketAddr> {\n         match self {\ndiff --git a/pingora-core/src/protocols/http/server.rs b/pingora-core/src/protocols/http/server.rs\nindex d4113b08b..c6479e7a0 100644\n--- a/pingora-core/src/protocols/http/server.rs\n+++ b/pingora-core/src/protocols/http/server.rs\n@@ -25,6 +25,7 @@ use http::{header::AsHeaderName, HeaderMap};\n use log::error;\n use pingora_error::Result;\n use pingora_http::{RequestHeader, ResponseHeader};\n+use std::time::Duration;\n \n /// HTTP server session object for both HTTP/1.x and HTTP/2\n pub enum Session {\n@@ -188,6 +189,35 @@ impl Session {\n         }\n     }\n \n+    /// Sets the downstream write timeout. This will trigger if we're unable\n+    /// to write to the stream after `duration`. If a `min_send_rate` is\n+    /// configured then the `min_send_rate` calculated timeout has higher priority.\n+    ///\n+    /// This is a noop for h2.\n+    pub fn set_write_timeout(&mut self, timeout: Duration) {\n+        match self {\n+            Self::H1(s) => s.set_write_timeout(timeout),\n+            Self::H2(_) => {}\n+        }\n+    }\n+\n+    /// Sets the minimum downstream send rate in bytes per second. This\n+    /// is used to calculate a write timeout in seconds based on the size\n+    /// of the buffer being written. If a `min_send_rate` is configured it\n+    /// has higher priority over a set `write_timeout`. The minimum send\n+    /// rate must be greater than zero.\n+    ///\n+    /// Calculated write timeout is guaranteed to be at least 1s if `min_send_rate`\n+    /// is greater than zero, a send rate of zero is a noop.\n+    ///\n+    /// This is a noop for h2.\n+    pub fn set_min_send_rate(&mut self, rate: usize) {\n+        match self {\n+            Self::H1(s) => s.set_min_send_rate(rate),\n+            Self::H2(_) => {}\n+        }\n+    }\n+\n     /// Return a digest of the request including the method, path and Host header\n     // TODO: make this use a `Formatter`\n     pub fn request_summary(&self) -> String {\n@@ -357,7 +387,7 @@ impl Session {\n         }\n     }\n \n-    /// Return the digest for the session.\n+    /// Return the [Digest] for the connection.\n     pub fn digest(&self) -> Option<&Digest> {\n         match self {\n             Self::H1(s) => Some(s.digest()),\n@@ -365,6 +395,16 @@ impl Session {\n         }\n     }\n \n+    /// Return a mutable [Digest] reference for the connection.\n+    ///\n+    /// Will return `None` if multiple H2 streams are open.\n+    pub fn digest_mut(&mut self) -> Option<&mut Digest> {\n+        match self {\n+            Self::H1(s) => Some(s.digest_mut()),\n+            Self::H2(s) => s.digest_mut(),\n+        }\n+    }\n+\n     /// Return the client (peer) address of the connection.\n     pub fn client_addr(&self) -> Option<&SocketAddr> {\n         match self {\ndiff --git a/pingora-core/src/protocols/http/v1/client.rs b/pingora-core/src/protocols/http/v1/client.rs\nindex 7a72ed40b..8c2ab14ed 100644\n--- a/pingora-core/src/protocols/http/v1/client.rs\n+++ b/pingora-core/src/protocols/http/v1/client.rs\n@@ -631,10 +631,19 @@ impl HttpSession {\n         // TODO: support h1 trailer\n     }\n \n+    /// Return the [Digest] of the connection\n+    ///\n+    /// For reused connection, the timing in the digest will reflect its initial handshakes\n+    /// The caller should check if the connection is reused to avoid misuse the timing field.\n     pub fn digest(&self) -> &Digest {\n         &self.digest\n     }\n \n+    /// Return a mutable [Digest] reference for the connection, see [`digest`] for more details.\n+    pub fn digest_mut(&mut self) -> &mut Digest {\n+        &mut self.digest\n+    }\n+\n     /// Return the server (peer) address recorded in the connection digest.\n     pub fn server_addr(&self) -> Option<&SocketAddr> {\n         self.digest()\ndiff --git a/pingora-core/src/protocols/http/v1/server.rs b/pingora-core/src/protocols/http/v1/server.rs\nindex 909381c41..82a93a20f 100644\n--- a/pingora-core/src/protocols/http/v1/server.rs\n+++ b/pingora-core/src/protocols/http/v1/server.rs\n@@ -72,6 +72,8 @@ pub struct HttpSession {\n     upgraded: bool,\n     /// Digest to track underlying connection metrics\n     digest: Box<Digest>,\n+    /// Minimum send rate to the client\n+    min_send_rate: Option<usize>,\n }\n \n impl HttpSession {\n@@ -106,6 +108,7 @@ impl HttpSession {\n             retry_buffer: None,\n             upgraded: false,\n             digest,\n+            min_send_rate: None,\n         }\n     }\n \n@@ -511,6 +514,18 @@ impl HttpSession {\n         is_buf_keepalive(self.get_header(header::CONNECTION))\n     }\n \n+    // calculate write timeout from min_send_rate if set, otherwise return write_timeout\n+    fn write_timeout(&self, buf_len: usize) -> Option<Duration> {\n+        let Some(min_send_rate) = self.min_send_rate.filter(|r| *r > 0) else {\n+            return self.write_timeout;\n+        };\n+\n+        // min timeout is 1s\n+        let ms = (buf_len.max(min_send_rate) as f64 / min_send_rate as f64) * 1000.0;\n+        // truncates unrealistically large values (we'll be out of memory before this happens)\n+        Some(Duration::from_millis(ms as u64))\n+    }\n+\n     /// Apply keepalive settings according to the client\n     /// For HTTP 1.1, assume keepalive as long as there is no `Connection: Close` request header.\n     /// For HTTP 1.0, only keepalive if there is an explicit header `Connection: keep-alive`.\n@@ -579,7 +594,7 @@ impl HttpSession {\n     /// to be written, e.g., writing more bytes than what the `Content-Length` header suggests\n     pub async fn write_body(&mut self, buf: &[u8]) -> Result<Option<usize>> {\n         // TODO: check if the response header is written\n-        match self.write_timeout {\n+        match self.write_timeout(buf.len()) {\n             Some(t) => match timeout(t, self.do_write_body(buf)).await {\n                 Ok(res) => res,\n                 Err(_) => Error::e_explain(WriteTimedout, format!(\"writing body, timeout: {t:?}\")),\n@@ -588,7 +603,7 @@ impl HttpSession {\n         }\n     }\n \n-    async fn write_body_buf(&mut self) -> Result<Option<usize>> {\n+    async fn do_write_body_buf(&mut self) -> Result<Option<usize>> {\n         // Don't flush empty chunks, they are considered end of body for chunks\n         if self.body_write_buf.is_empty() {\n             return Ok(None);\n@@ -609,6 +624,16 @@ impl HttpSession {\n         written\n     }\n \n+    async fn write_body_buf(&mut self) -> Result<Option<usize>> {\n+        match self.write_timeout(self.body_write_buf.len()) {\n+            Some(t) => match timeout(t, self.do_write_body_buf()).await {\n+                Ok(res) => res,\n+                Err(_) => Error::e_explain(WriteTimedout, format!(\"writing body, timeout: {t:?}\")),\n+            },\n+            None => self.do_write_body_buf().await,\n+        }\n+    }\n+\n     fn maybe_force_close_body_reader(&mut self) {\n         if self.upgraded && !self.body_reader.body_done() {\n             // response is done, reset the request body to close\n@@ -778,11 +803,37 @@ impl HttpSession {\n         }\n     }\n \n+    /// Sets the downstream write timeout. This will trigger if we're unable\n+    /// to write to the stream after `duration`. If a `min_send_rate` is\n+    /// configured then the `min_send_rate` calculated timeout has higher priority.\n+    pub fn set_write_timeout(&mut self, timeout: Duration) {\n+        self.write_timeout = Some(timeout);\n+    }\n+\n+    /// Sets the minimum downstream send rate in bytes per second. This\n+    /// is used to calculate a write timeout in seconds based on the size\n+    /// of the buffer being written. If a `min_send_rate` is configured it\n+    /// has higher priority over a set `write_timeout`. The minimum send\n+    /// rate must be greater than zero.\n+    ///\n+    /// Calculated write timeout is guaranteed to be at least 1s if `min_send_rate`\n+    /// is greater than zero, a send rate of zero is a noop.\n+    pub fn set_min_send_rate(&mut self, min_send_rate: usize) {\n+        if min_send_rate > 0 {\n+            self.min_send_rate = Some(min_send_rate);\n+        }\n+    }\n+\n     /// Return the [Digest] of the connection.\n     pub fn digest(&self) -> &Digest {\n         &self.digest\n     }\n \n+    /// Return a mutable [Digest] reference for the connection.\n+    pub fn digest_mut(&mut self) -> &mut Digest {\n+        &mut self.digest\n+    }\n+\n     /// Return the client (peer) address of the underlying connection.\n     pub fn client_addr(&self) -> Option<&SocketAddr> {\n         self.digest()\n@@ -1583,6 +1634,30 @@ mod tests_stream {\n         assert!(written.is_none());\n     }\n \n+    #[tokio::test]\n+    #[should_panic(expected = \"There is still data left to write.\")]\n+    async fn test_write_body_buf_write_timeout() {\n+        let wire1 = b\"HTTP/1.1 200 OK\\r\\nContent-Length: 3\\r\\n\\r\\n\";\n+        let wire2 = b\"abc\";\n+        let mock_io = Builder::new()\n+            .write(wire1)\n+            .wait(Duration::from_millis(500))\n+            .write(wire2)\n+            .build();\n+        let mut http_stream = HttpSession::new(Box::new(mock_io));\n+        http_stream.write_timeout = Some(Duration::from_millis(100));\n+        let mut new_response = ResponseHeader::build(StatusCode::OK, None).unwrap();\n+        new_response.append_header(\"Content-Length\", \"3\").unwrap();\n+        http_stream.update_resp_headers = false;\n+        http_stream\n+            .write_response_header_ref(&new_response)\n+            .await\n+            .unwrap();\n+        http_stream.body_write_buf = BytesMut::from(&b\"abc\"[..]);\n+        let res = http_stream.write_body_buf().await;\n+        assert_eq!(res.unwrap_err().etype(), &WriteTimedout);\n+    }\n+\n     #[tokio::test]\n     async fn test_write_continue_resp() {\n         let wire = b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\";\n@@ -1610,6 +1685,48 @@ mod tests_stream {\n         response.set_version(http::Version::HTTP_11);\n         assert!(!is_upgrade_resp(&response));\n     }\n+\n+    #[test]\n+    fn test_get_write_timeout() {\n+        let mut http_stream = HttpSession::new(Box::new(Builder::new().build()));\n+        let expected = Duration::from_secs(5);\n+\n+        http_stream.set_write_timeout(expected);\n+        assert_eq!(Some(expected), http_stream.write_timeout(50));\n+    }\n+\n+    #[test]\n+    fn test_get_write_timeout_none() {\n+        let http_stream = HttpSession::new(Box::new(Builder::new().build()));\n+        assert!(http_stream.write_timeout(50).is_none());\n+    }\n+\n+    #[test]\n+    fn test_get_write_timeout_min_send_rate_zero_noop() {\n+        let mut http_stream = HttpSession::new(Box::new(Builder::new().build()));\n+        http_stream.set_min_send_rate(0);\n+        assert!(http_stream.write_timeout(50).is_none());\n+    }\n+\n+    #[test]\n+    fn test_get_write_timeout_min_send_rate_overrides_write_timeout() {\n+        let mut http_stream = HttpSession::new(Box::new(Builder::new().build()));\n+        let expected = Duration::from_millis(29800);\n+\n+        http_stream.set_write_timeout(Duration::from_secs(60));\n+        http_stream.set_min_send_rate(5000);\n+\n+        assert_eq!(Some(expected), http_stream.write_timeout(149000));\n+    }\n+\n+    #[test]\n+    fn test_get_write_timeout_min_send_rate_max_zero_buf() {\n+        let mut http_stream = HttpSession::new(Box::new(Builder::new().build()));\n+        let expected = Duration::from_secs(1);\n+\n+        http_stream.set_min_send_rate(1);\n+        assert_eq!(Some(expected), http_stream.write_timeout(0));\n+    }\n }\n \n #[cfg(test)]\ndiff --git a/pingora-core/src/protocols/http/v2/client.rs b/pingora-core/src/protocols/http/v2/client.rs\nindex 15f86d08e..86a3fe3da 100644\n--- a/pingora-core/src/protocols/http/v2/client.rs\n+++ b/pingora-core/src/protocols/http/v2/client.rs\n@@ -305,11 +305,18 @@ impl Http2Session {\n     /// Return the [Digest] of the connection\n     ///\n     /// For reused connection, the timing in the digest will reflect its initial handshakes\n-    /// The caller should check if the connection is reused to avoid misuse the timing field\n+    /// The caller should check if the connection is reused to avoid misuse the timing field.\n     pub fn digest(&self) -> Option<&Digest> {\n         Some(self.conn.digest())\n     }\n \n+    /// Return a mutable [Digest] reference for the connection, see [`digest`] for more details.\n+    ///\n+    /// Will return `None` if multiple H2 streams are open.\n+    pub fn digest_mut(&mut self) -> Option<&mut Digest> {\n+        self.conn.digest_mut()\n+    }\n+\n     /// Return the server (peer) address recorded in the connection digest.\n     pub fn server_addr(&self) -> Option<&SocketAddr> {\n         self.conn\ndiff --git a/pingora-core/src/protocols/http/v2/server.rs b/pingora-core/src/protocols/http/v2/server.rs\nindex 57238d107..f9072d6b6 100644\n--- a/pingora-core/src/protocols/http/v2/server.rs\n+++ b/pingora-core/src/protocols/http/v2/server.rs\n@@ -454,6 +454,11 @@ impl HttpSession {\n         Some(&self.digest)\n     }\n \n+    /// Return a mutable [Digest] reference for the connection.\n+    pub fn digest_mut(&mut self) -> Option<&mut Digest> {\n+        Arc::get_mut(&mut self.digest)\n+    }\n+\n     /// Return the server (local) address recorded in the connection digest.\n     pub fn server_addr(&self) -> Option<&SocketAddr> {\n         self.digest.socket_digest.as_ref().map(|d| d.local_addr())?\ndiff --git a/pingora-core/src/protocols/l4/socket.rs b/pingora-core/src/protocols/l4/socket.rs\nindex 186fbecb3..93d334ed5 100644\n--- a/pingora-core/src/protocols/l4/socket.rs\n+++ b/pingora-core/src/protocols/l4/socket.rs\n@@ -15,6 +15,7 @@\n //! Generic socket type\n \n use crate::{Error, OrErr};\n+use log::warn;\n use nix::sys::socket::{getpeername, getsockname, SockaddrStorage};\n use std::cmp::Ordering;\n use std::hash::{Hash, Hasher};\n@@ -174,14 +175,23 @@ impl std::str::FromStr for SocketAddr {\n     type Err = Box<Error>;\n \n     // This is very basic parsing logic, it might treat invalid IP:PORT str as UDS path\n-    // TODO: require UDS to have some prefix\n     fn from_str(s: &str) -> Result<Self, Self::Err> {\n-        match StdSockAddr::from_str(s) {\n-            Ok(addr) => Ok(SocketAddr::Inet(addr)),\n-            Err(_) => {\n-                let uds_socket = StdUnixSockAddr::from_pathname(s)\n-                    .or_err(crate::BindError, \"invalid UDS path\")?;\n-                Ok(SocketAddr::Unix(uds_socket))\n+        if s.starts_with(\"unix:\") {\n+            // format unix:/tmp/server.socket\n+            let path = s.trim_start_matches(\"unix:\");\n+            let uds_socket = StdUnixSockAddr::from_pathname(path)\n+                .or_err(crate::BindError, \"invalid UDS path\")?;\n+            Ok(SocketAddr::Unix(uds_socket))\n+        } else {\n+            match StdSockAddr::from_str(s) {\n+                Ok(addr) => Ok(SocketAddr::Inet(addr)),\n+                Err(_) => {\n+                    // Try to parse as UDS for backward compatibility\n+                    let uds_socket = StdUnixSockAddr::from_pathname(s)\n+                        .or_err(crate::BindError, \"invalid UDS path\")?;\n+                    warn!(\"Raw Unix domain socket path support will be deprecated, add 'unix:' prefix instead\");\n+                    Ok(SocketAddr::Unix(uds_socket))\n+                }\n             }\n         }\n     }\n@@ -246,4 +256,10 @@ mod test {\n         let uds: SocketAddr = \"/tmp/my.sock\".parse().unwrap();\n         assert!(uds.as_unix().is_some());\n     }\n+\n+    #[test]\n+    fn parse_uds_with_prefix() {\n+        let uds: SocketAddr = \"unix:/tmp/my.sock\".parse().unwrap();\n+        assert!(uds.as_unix().is_some());\n+    }\n }\ndiff --git a/pingora-core/src/server/configuration/mod.rs b/pingora-core/src/server/configuration/mod.rs\nindex c118c84ac..5174d4072 100644\n--- a/pingora-core/src/server/configuration/mod.rs\n+++ b/pingora-core/src/server/configuration/mod.rs\n@@ -119,7 +119,7 @@ impl Default for ServerConf {\n /// Command-line options\n ///\n /// Call `Opt::from_args()` to build this object from the process's command line arguments.\n-#[derive(Parser, Debug)]\n+#[derive(Parser, Debug, Default)]\n #[clap(name = \"basic\", long_about = None)]\n pub struct Opt {\n     /// Whether this server should try to upgrade from a running old server\n@@ -163,15 +163,6 @@ pub struct Opt {\n     pub conf: Option<String>,\n }\n \n-/// Create the default instance of Opt based on the current command-line args.\n-/// This is equivalent to running `Opt::parse` but does not require the\n-/// caller to have included the `clap::Parser`\n-impl Default for Opt {\n-    fn default() -> Self {\n-        Opt::parse()\n-    }\n-}\n-\n impl ServerConf {\n     // Does not has to be async until we want runtime reload\n     pub fn load_from_yaml<P>(path: P) -> Result<Self>\n@@ -236,6 +227,15 @@ impl ServerConf {\n     }\n }\n \n+/// Create an instance of Opt by parsing the current command-line args.\n+/// This is equivalent to running `Opt::parse` but does not require the\n+/// caller to have included the `clap::Parser`\n+impl Opt {\n+    pub fn parse_args() -> Self {\n+        Opt::parse()\n+    }\n+}\n+\n #[cfg(test)]\n mod tests {\n     use super::*;\ndiff --git a/pingora-header-serde/src/lib.rs b/pingora-header-serde/src/lib.rs\nindex 73b9b2929..e78897a78 100644\n--- a/pingora-header-serde/src/lib.rs\n+++ b/pingora-header-serde/src/lib.rs\n@@ -131,7 +131,7 @@ fn resp_header_to_buf(resp: &ResponseHeader, buf: &mut Vec<u8>) -> usize {\n }\n \n // Should match pingora http1 setting\n-const MAX_HEADERS: usize = 160;\n+const MAX_HEADERS: usize = 256;\n \n #[inline]\n fn buf_to_http_header(buf: &[u8]) -> Result<ResponseHeader> {\ndiff --git a/pingora-memory-cache/src/lib.rs b/pingora-memory-cache/src/lib.rs\nindex 5e0254bd9..b6e78bda0 100644\n--- a/pingora-memory-cache/src/lib.rs\n+++ b/pingora-memory-cache/src/lib.rs\n@@ -45,6 +45,14 @@ impl CacheStatus {\n             Self::LockHit => \"lock_hit\",\n         }\n     }\n+\n+    /// Returns whether this status represents a cache hit.\n+    pub fn is_hit(&self) -> bool {\n+        match self {\n+            CacheStatus::Hit | CacheStatus::LockHit => true,\n+            CacheStatus::Miss | CacheStatus::Expired => false,\n+        }\n+    }\n }\n \n #[derive(Debug, Clone)]\ndiff --git a/pingora-openssl/src/lib.rs b/pingora-openssl/src/lib.rs\nindex 25f7cb715..7d52d683d 100644\n--- a/pingora-openssl/src/lib.rs\n+++ b/pingora-openssl/src/lib.rs\n@@ -25,6 +25,7 @@ pub use tokio_openssl as tokio_ssl;\n pub mod ext;\n \n // export commonly used libs\n+pub use ssl_lib::dh;\n pub use ssl_lib::error;\n pub use ssl_lib::hash;\n pub use ssl_lib::nid;\ndiff --git a/pingora-proxy/src/lib.rs b/pingora-proxy/src/lib.rs\nindex 0add56aba..99e7d6d06 100644\n--- a/pingora-proxy/src/lib.rs\n+++ b/pingora-proxy/src/lib.rs\n@@ -110,6 +110,14 @@ impl<SV> HttpProxy<SV> {\n         }\n     }\n \n+    fn handle_init_modules(&mut self)\n+    where\n+        SV: ProxyHttp,\n+    {\n+        self.inner\n+            .init_downstream_modules(&mut self.downstream_modules);\n+    }\n+\n     async fn handle_new_request(\n         &self,\n         mut downstream_session: Box<HttpSession>,\n@@ -739,7 +747,10 @@ use pingora_core::services::listening::Service;\n /// Create a [Service] from the user implemented [ProxyHttp].\n ///\n /// The returned [Service] can be hosted by a [pingora_core::server::Server] directly.\n-pub fn http_proxy_service<SV>(conf: &Arc<ServerConf>, inner: SV) -> Service<HttpProxy<SV>> {\n+pub fn http_proxy_service<SV>(conf: &Arc<ServerConf>, inner: SV) -> Service<HttpProxy<SV>>\n+where\n+    SV: ProxyHttp,\n+{\n     http_proxy_service_with_name(conf, inner, \"Pingora HTTP Proxy Service\")\n }\n \n@@ -750,11 +761,11 @@ pub fn http_proxy_service_with_name<SV>(\n     conf: &Arc<ServerConf>,\n     inner: SV,\n     name: &str,\n-) -> Service<HttpProxy<SV>> {\n+) -> Service<HttpProxy<SV>>\n+where\n+    SV: ProxyHttp,\n+{\n     let mut proxy = HttpProxy::new(inner, conf.clone());\n-    // Add disabled downstream compression module by default\n-    proxy\n-        .downstream_modules\n-        .add_module(ResponseCompressionBuilder::enable(0));\n+    proxy.handle_init_modules();\n     Service::new(name.to_string(), proxy)\n }\ndiff --git a/pingora-proxy/src/proxy_h1.rs b/pingora-proxy/src/proxy_h1.rs\nindex feac860c5..de49c03c4 100644\n--- a/pingora-proxy/src/proxy_h1.rs\n+++ b/pingora-proxy/src/proxy_h1.rs\n@@ -98,10 +98,7 @@ impl<SV> HttpProxy<SV> {\n         );\n \n         match ret {\n-            Ok((_first, _second)) => {\n-                client_session.respect_keepalive();\n-                (true, true, None)\n-            }\n+            Ok((_first, _second)) => (true, true, None),\n             Err(e) => (false, false, Some(e)),\n         }\n     }\ndiff --git a/pingora-proxy/src/proxy_trait.rs b/pingora-proxy/src/proxy_trait.rs\nindex 6f429ac3e..2d99567eb 100644\n--- a/pingora-proxy/src/proxy_trait.rs\n+++ b/pingora-proxy/src/proxy_trait.rs\n@@ -40,6 +40,17 @@ pub trait ProxyHttp {\n         ctx: &mut Self::CTX,\n     ) -> Result<Box<HttpPeer>>;\n \n+    /// Set up downstream modules.\n+    ///\n+    /// In this phase, users can add or configure [HttpModules] before the server starts up.\n+    ///\n+    /// In the default implementation of this method, [ResponseCompressionBuilder] is added\n+    /// and disabled.\n+    fn init_downstream_modules(&self, modules: &mut HttpModules) {\n+        // Add disabled downstream compression module by default\n+        modules.add_module(ResponseCompressionBuilder::enable(0));\n+    }\n+\n     /// Handle the incoming request.\n     ///\n     /// In this phase, users can parse, validate, rate limit, perform access control and/or\ndiff --git a/tinyufo/src/estimation.rs b/tinyufo/src/estimation.rs\nindex 6f165ac0e..11b9d923d 100644\n--- a/tinyufo/src/estimation.rs\n+++ b/tinyufo/src/estimation.rs\n@@ -36,23 +36,36 @@ impl Estimator {\n \n     fn optimal(items: usize) -> Self {\n         let (slots, hashes) = Self::optimal_paras(items);\n-        Self::new(hashes, slots)\n+        Self::new(hashes, slots, RandomState::new)\n     }\n \n     fn compact(items: usize) -> Self {\n         let (slots, hashes) = Self::optimal_paras(items / 100);\n-        Self::new(hashes, slots)\n+        Self::new(hashes, slots, RandomState::new)\n     }\n \n-    /// Create a new `Estimator` with the given amount of hashes and columns (slots).\n-    pub fn new(hashes: usize, slots: usize) -> Self {\n+    #[cfg(test)]\n+    fn seeded(items: usize) -> Self {\n+        let (slots, hashes) = Self::optimal_paras(items);\n+        Self::new(hashes, slots, || RandomState::with_seeds(2, 3, 4, 5))\n+    }\n+\n+    #[cfg(test)]\n+    fn seeded_compact(items: usize) -> Self {\n+        let (slots, hashes) = Self::optimal_paras(items / 100);\n+        Self::new(hashes, slots, || RandomState::with_seeds(2, 3, 4, 5))\n+    }\n+\n+    /// Create a new `Estimator` with the given amount of hashes and columns (slots) using\n+    /// the given random source.\n+    pub fn new(hashes: usize, slots: usize, random: impl Fn() -> RandomState) -> Self {\n         let mut estimator = Vec::with_capacity(hashes);\n         for _ in 0..hashes {\n             let mut slot = Vec::with_capacity(slots);\n             for _ in 0..slots {\n                 slot.push(AtomicU8::new(0));\n             }\n-            estimator.push((slot.into_boxed_slice(), RandomState::new()));\n+            estimator.push((slot.into_boxed_slice(), random()));\n         }\n \n         Estimator {\n@@ -161,6 +174,26 @@ impl TinyLfu {\n             window_limit: cache_size * 8,\n         }\n     }\n+\n+    #[cfg(test)]\n+    pub fn new_seeded(cache_size: usize) -> Self {\n+        Self {\n+            estimator: Estimator::seeded(cache_size),\n+            window_counter: Default::default(),\n+            // 8x: just a heuristic to balance the memory usage and accuracy\n+            window_limit: cache_size * 8,\n+        }\n+    }\n+\n+    #[cfg(test)]\n+    pub fn new_compact_seeded(cache_size: usize) -> Self {\n+        Self {\n+            estimator: Estimator::seeded_compact(cache_size),\n+            window_counter: Default::default(),\n+            // 8x: just a heuristic to balance the memory usage and accuracy\n+            window_limit: cache_size * 8,\n+        }\n+    }\n }\n \n #[cfg(test)]\ndiff --git a/tinyufo/src/lib.rs b/tinyufo/src/lib.rs\nindex 805373e81..eb4cbaf48 100644\n--- a/tinyufo/src/lib.rs\n+++ b/tinyufo/src/lib.rs\n@@ -473,7 +473,9 @@ mod tests {\n \n     #[test]\n     fn test_evict_from_small() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -496,7 +498,9 @@ mod tests {\n \n     #[test]\n     fn test_evict_from_small_to_main() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -510,20 +514,30 @@ mod tests {\n         assert_eq!(cache.peek_queue(2), Some(SMALL));\n         assert_eq!(cache.peek_queue(3), Some(SMALL));\n \n-        let evicted = cache.put(4, 4, 1);\n+        let evicted = cache.put(4, 4, 2);\n         assert_eq!(evicted.len(), 1);\n-        assert_eq!(evicted[0].data, 2);\n+        assert_eq!(evicted[0].weight, 2);\n \n         assert_eq!(cache.peek_queue(1), Some(MAIN));\n-        // 2 is evicted because 1 is in main\n-        assert_eq!(cache.peek_queue(2), None);\n-        assert_eq!(cache.peek_queue(3), Some(SMALL));\n-        assert_eq!(cache.peek_queue(4), Some(SMALL));\n+        // either 2, 3, or 4 was evicted. Check evicted for which.\n+        let mut remaining = vec![2, 3, 4];\n+        remaining.remove(\n+            remaining\n+                .iter()\n+                .position(|x| *x == evicted[0].data)\n+                .unwrap(),\n+        );\n+        assert_eq!(cache.peek_queue(evicted[0].key), None);\n+        for k in remaining {\n+            assert_eq!(cache.peek_queue(k), Some(SMALL));\n+        }\n     }\n \n     #[test]\n     fn test_evict_reentry() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -555,7 +569,9 @@ mod tests {\n \n     #[test]\n     fn test_evict_entry_denied() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -583,7 +599,9 @@ mod tests {\n \n     #[test]\n     fn test_force_put() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -612,7 +630,9 @@ mod tests {\n \n     #[test]\n     fn test_evict_from_main() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -649,7 +669,9 @@ mod tests {\n \n     #[test]\n     fn test_evict_from_small_compact() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_compact_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n@@ -672,7 +694,9 @@ mod tests {\n \n     #[test]\n     fn test_evict_from_small_to_main_compact() {\n-        let cache = TinyUfo::new(5, 5);\n+        let mut cache = TinyUfo::new(5, 5);\n+        cache.random_status = RandomState::with_seeds(2, 3, 4, 5);\n+        cache.queues.estimator = TinyLfu::new_compact_seeded(5);\n \n         cache.put(1, 1, 1);\n         cache.put(2, 2, 2);\n", "instance_id": "cloudflare__pingora-310", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the bug in the `Default` trait implementation for `Opt` in the `pingora-core` library, where `Opt::default()` inadvertently parses command-line arguments, leading to unexpected side effects such as application shutdown or help message display. The goal is explicit: to remove side effects from the `Default` implementation by decoupling it from command-line parsing. The statement includes detailed steps to reproduce the issue, expected versus observed results, and additional context suggesting a solution (deriving `Default` automatically and providing a separate method for parsing command-line arguments). There are no significant ambiguities, and the problem is supported by a concrete code example and a reference to the specific line in the codebase. All critical details, including the impact of the bug, are well-articulated, making this a clear and actionable issue report.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The core issue involves modifying the `Default` trait implementation for the `Opt` struct to avoid side effects by not parsing command-line arguments automatically. This requires a straightforward change in the `pingora-core/src/server/configuration/mod.rs` file to derive `Default` automatically or set default values explicitly, and to introduce a separate method (e.g., `Opt::parse_args()`) for command-line parsing. The scope of the code change is limited to a single module, with minimal impact on the broader codebase architecture, as seen in the provided diff where only a few lines are altered to achieve the fix. The technical concepts involved are basic\u2014understanding Rust's `Default` trait and the `clap` library for argument parsing, which are standard and not complex. There are no significant edge cases or error handling requirements mentioned beyond ensuring that the default values are sensible (e.g., setting boolean flags to `false` and optional fields to `None`). The additional changes in the diff (e.g., updates to cache handling, HTTP session management) appear unrelated to the core issue described and do not factor into this difficulty assessment for the specific problem of `Opt::default()`. Overall, this task requires minimal effort and a basic understanding of Rust, making it an easy fix for a developer familiar with the language.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Reduce node logs\n#### Summary\r\nThis is the data logged with a \"standard\" configuration\r\n\r\n```\r\n{\"timestamp\":\"2024-12-07T09:36:29.091866Z\",\"level\":\"INFO\",\"event\":\"block accepted\",\"height\":397287,\"iter\":0,\"hash\":\"0486ca867b62bb4b...dcf694dd038d5bbc\",\"txs\":0,\"state_hash\":\"62f430dfe6b3d110...89935dc1aa57d3d7\",\"fsv_bitset\":3939320110216175,\"ssv_bitset\":56566594122515,\"block_time\":10,\"generator\":\"rSGm1CeKgLT7SeUW\",\"dur_ms\":\"15\",\"label\":\"Attested(1)\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.092082Z\",\"level\":\"INFO\",\"event\":\"spawn consensus\",\"id\":162452,\"round\":397288,\"timeout\":\"{Validation: 7s, Ratification: 7s, Proposal: 7s}\",\"all\":282,\"eligible\":257,\"target\":\"node::chain::consensus\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.092165Z\",\"level\":\"INFO\",\"event\":\"consensus canceled\",\"round\":397287,\"target\":\"node::chain\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.336954Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"5094a17ecb21c136...9a07ffc0f3a5404c\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.337232Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"516f4756f48b3fb5...ae860419ae01595c\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.606090Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"913fca928ac897c9...2162009a8089401d\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.622206Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"e40c106274818399...cc3a3b43d42a8dc6\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.661839Z\",\"level\":\"INFO\",\"event\":\"Step completed\",\"step\":\"Proposal\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"0486ca867b62bb4b...dcf694dd038d5bbc\\\", round: 397288, iteration: 0 }\",\"target\":\"dusk_consensus::execution_ctx\",\"spans\":[{\"iter\":0,\"name\":\"Proposal\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:29.661906Z\",\"level\":\"INFO\",\"event\":\"next_slot\",\"delay\":\"8s\",\"target\":\"dusk_consensus::proposal::step\",\"spans\":[{\"iter\":0,\"name\":\"Proposal\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:29.844112Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"52d45ce2d2549d45...00650d565b546635\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.864249Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"475399f89e5ef766...11070e7ca0aa192a\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.082281Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"824984f7d0399ce1...31ed5a95b090e2a3\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.082736Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"eb98aa0d7372ef64...64f6f71961465e79\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.803192Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"fd997bc4c4815469...dead9267daaa6841\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.808207Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"ababdca100c66c98...9897fd15bca089b3\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.924375Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"9284db6876a87d04...a8c64e0914a60f11\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.924899Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"500a60ea968e359d...00ff62bc9c3792f2\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:30.925310Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"f6d94bf28afbafea...49e9d99894b3e51f\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:31.123727Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"6761cb0ed536a84e...1d4864c9ea38c546\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:31.416311Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"692eb5cbc7095e36...5caac868c75e6d91\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:31.417034Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"9ea957bfbaa86ddd...84f1a1b651a6a32f\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:31.417518Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"2bdf68878d0d06c8...f5c6a35f0ff13aed\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:31.523560Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"ce0d775d76d1c6a6...85bda132eb503fa1\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:31.524405Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"e92c241dd101910e...839a3e611bcddaf9\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:32.026841Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"d51ad07518873476...48f9d2f485044ddb\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:33.288709Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"be6b2b838151e87a...185fc0a7c5f9016a\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:35.898733Z\",\"level\":\"WARN\",\"event\":\"msg discarded\",\"topic\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"c6997a868380691d...aed1e860dbff1166\\\", round: 397287, iteration: 0 }\",\"ray_id\":\"e8573c165e04eaa6...52256a66e69c3fbf\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:37.974683Z\",\"level\":\"INFO\",\"event\":\"quorum reached\",\"vote\":\"Vote: Valid(af8ccbdef824ae11...6244f0ed36da00b8)\",\"iter\":0,\"step\":\"Validation\",\"total\":43,\"target\":43,\"bitset\":342593699032318,\"step\":1,\"signature\":\"b9971978b9210e3a...0b895ef4fe009963\",\"target\":\"dusk_consensus::aggregator\",\"spans\":[{\"iter\":0,\"name\":\"Validation\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:37.974751Z\",\"level\":\"INFO\",\"event\":\"quorum reached\",\"vote\":\"Vote: Valid(af8ccbdef824ae11...6244f0ed36da00b8)\",\"target\":\"dusk_consensus::validation::handler\",\"spans\":[{\"iter\":0,\"name\":\"Validation\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:37.974762Z\",\"level\":\"INFO\",\"event\":\"Step completed\",\"step\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"0486ca867b62bb4b...dcf694dd038d5bbc\\\", round: 397288, iteration: 0 }\",\"target\":\"dusk_consensus::execution_ctx\",\"spans\":[{\"iter\":0,\"name\":\"Validation\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:39.391946Z\",\"level\":\"INFO\",\"event\":\"New block\",\"src\":\"Quorum msg\",\"height\":397288,\"iter\":0,\"hash\":\"af8ccbdef824ae11...6244f0ed36da00b8\",\"target\":\"node::chain::fsm\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.439804Z\",\"level\":\"INFO\",\"message\":\"Received accept request\",\"target\":\"rusk::node::vm\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.451738Z\",\"level\":\"INFO\",\"event\":\"block confirmed\",\"src\":\"rolling_finality\",\"current_height\":397288,\"height\":397287,\"confirmed_after\":1,\"hash\":\"0486ca867b62bb4b...dcf694dd038d5bbc\",\"label\":\"Attested(1)\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.451808Z\",\"level\":\"INFO\",\"event\":\"block finalized\",\"src\":\"rolling_finality\",\"current_height\":397288,\"height\":397287,\"finalized_after\":1,\"hash\":\"0486ca867b62bb4b...dcf694dd038d5bbc\",\"state_root\":\"62f430dfe6b3d110...89935dc1aa57d3d7\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.451900Z\",\"level\":\"INFO\",\"message\":\"Received finalize request\",\"target\":\"rusk::node::vm\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.457040Z\",\"level\":\"INFO\",\"event\":\"block accepted\",\"height\":397288,\"iter\":0,\"hash\":\"af8ccbdef824ae11...6244f0ed36da00b8\",\"txs\":0,\"state_hash\":\"5ff39cc775234f66...6c772f856d2471be\",\"fsv_bitset\":342906162089214,\"ssv_bitset\":786645673959343,\"block_time\":10,\"generator\":\"stEepsCQ13uvpoDy\",\"dur_ms\":\"17\",\"label\":\"Attested(1)\",\"target\":\"node::chain::acceptor\"}\r\n\r\n```\r\n\r\n\r\n#### Possible solution design or implementation\r\n- \"WARN msg discarded\" should not be a WARN, since that the msg is related to a past round message\r\n- \"quorum reached\" is duplicated\r\n- \"Spawn consensus\" and \"Consensus canceled\" are not so useful (consider moving them to debug\"\r\n- \"Step completed\" is misleading, since there is a \"next slot\" after it (consider moving the \"Step completed\" outside the event_loop, more precisely after \"await_phase!\" macro)\r\n\r\nWith the following changed, the logs for a single empty block are the following (still too much btw)\r\n\r\n```\r\n{\"timestamp\":\"2024-12-07T09:36:29.091866Z\",\"level\":\"INFO\",\"event\":\"block accepted\",\"height\":397287,\"iter\":0,\"hash\":\"0486ca867b62bb4b...dcf694dd038d5bbc\",\"txs\":0,\"state_hash\":\"62f430dfe6b3d110...89935dc1aa57d3d7\",\"fsv_bitset\":3939320110216175,\"ssv_bitset\":56566594122515,\"block_time\":10,\"generator\":\"rSGm1CeKgLT7SeUW\",\"dur_ms\":\"15\",\"label\":\"Attested(1)\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.092082Z\",\"level\":\"INFO\",\"event\":\"spawn consensus\",\"id\":162452,\"round\":397288,\"timeout\":\"{Validation: 7s, Ratification: 7s, Proposal: 7s}\",\"all\":282,\"eligible\":257,\"target\":\"node::chain::consensus\"}\r\n{\"timestamp\":\"2024-12-07T09:36:29.661839Z\",\"level\":\"INFO\",\"event\":\"Step completed\",\"step\":\"Proposal\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"0486ca867b62bb4b...dcf694dd038d5bbc\\\", round: 397288, iteration: 0 }\",\"target\":\"dusk_consensus::execution_ctx\",\"spans\":[{\"iter\":0,\"name\":\"Proposal\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:37.974683Z\",\"level\":\"INFO\",\"event\":\"quorum reached\",\"vote\":\"Vote: Valid(af8ccbdef824ae11...6244f0ed36da00b8)\",\"iter\":0,\"step\":\"Validation\",\"total\":43,\"target\":43,\"bitset\":342593699032318,\"step\":1,\"signature\":\"b9971978b9210e3a...0b895ef4fe009963\",\"target\":\"dusk_consensus::aggregator\",\"spans\":[{\"iter\":0,\"name\":\"Validation\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:37.974762Z\",\"level\":\"INFO\",\"event\":\"Step completed\",\"step\":\"Validation\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"0486ca867b62bb4b...dcf694dd038d5bbc\\\", round: 397288, iteration: 0 }\",\"target\":\"dusk_consensus::execution_ctx\",\"spans\":[{\"iter\":0,\"name\":\"Validation\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:39.391946Z\",\"level\":\"INFO\",\"event\":\"New block\",\"src\":\"Quorum msg\",\"height\":397288,\"iter\":0,\"hash\":\"af8ccbdef824ae11...6244f0ed36da00b8\",\"target\":\"node::chain::fsm\"}\r\n{\"timestamp\":\"2024-12-07T09:36:37.974762Z\",\"level\":\"INFO\",\"event\":\"Step completed\",\"step\":\"Ratification\",\"info\":\"ConsensusHeader { prev_block_hash: \\\"0486ca867b62bb4b...dcf694dd038d5bbc\\\", round: 397288, iteration: 0 }\",\"target\":\"dusk_consensus::execution_ctx\",\"spans\":[{\"iter\":0,\"name\":\"Ratification\",\"pk\":\"y42cbkejKvFwwmAG\",\"round\":397288,\"name\":\"main\"}]}\r\n{\"timestamp\":\"2024-12-07T09:36:39.439804Z\",\"level\":\"INFO\",\"message\":\"Received accept request\",\"target\":\"rusk::node::vm\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.451738Z\",\"level\":\"INFO\",\"event\":\"block confirmed\",\"src\":\"rolling_finality\",\"current_height\":397288,\"height\":397287,\"confirmed_after\":1,\"hash\":\"0486ca867b62bb4b...dcf694dd038d5bbc\",\"label\":\"Attested(1)\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.451808Z\",\"level\":\"INFO\",\"event\":\"block finalized\",\"src\":\"rolling_finality\",\"current_height\":397288,\"height\":397287,\"finalized_after\":1,\"hash\":\"0486ca867b62bb4b...dcf694dd038d5bbc\",\"state_root\":\"62f430dfe6b3d110...89935dc1aa57d3d7\",\"target\":\"node::chain::acceptor\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.451900Z\",\"level\":\"INFO\",\"message\":\"Received finalize request\",\"target\":\"rusk::node::vm\"}\r\n{\"timestamp\":\"2024-12-07T09:36:39.457040Z\",\"level\":\"INFO\",\"event\":\"block accepted\",\"height\":397288,\"iter\":0,\"hash\":\"af8ccbdef824ae11...6244f0ed36da00b8\",\"txs\":0,\"state_hash\":\"5ff39cc775234f66...6c772f856d2471be\",\"fsv_bitset\":342906162089214,\"ssv_bitset\":786645673959343,\"block_time\":10,\"generator\":\"stEepsCQ13uvpoDy\",\"dur_ms\":\"17\",\"label\":\"Attested(1)\",\"target\":\"node::chain::acceptor\"}\r\n```\r\n\n", "patch": "diff --git a/consensus/src/proposal/step.rs b/consensus/src/proposal/step.rs\nindex 91d2a93720..380dbaef4a 100644\n--- a/consensus/src/proposal/step.rs\n+++ b/consensus/src/proposal/step.rs\n@@ -12,7 +12,7 @@ use node_data::get_current_timestamp;\n use node_data::ledger::IterationsInfo;\n use node_data::message::Message;\n use tokio::sync::Mutex;\n-use tracing::{debug, error, info};\n+use tracing::{debug, error};\n \n use crate::commons::Database;\n use crate::config;\n@@ -137,7 +137,7 @@ impl<T: Operations + 'static, D: Database> ProposalStep<T, D> {\n     /// Waits until the next slot is reached\n     async fn wait_until_next_slot(tip_timestamp: u64) {\n         if let Some(delay) = Self::next_slot_in(tip_timestamp) {\n-            info!(event = \"next_slot\", ?delay);\n+            debug!(event = \"Wait next block slot for validation\", ?delay);\n             tokio::time::sleep(delay).await;\n         }\n     }\ndiff --git a/consensus/src/validation/handler.rs b/consensus/src/validation/handler.rs\nindex 600800ecb6..e3fbeb407a 100644\n--- a/consensus/src/validation/handler.rs\n+++ b/consensus/src/validation/handler.rs\n@@ -205,7 +205,6 @@ impl<D: Database> MsgHandler for ValidationHandler<D> {\n                     return Err(ConsensusError::InvalidVote(vote));\n                 }\n             };\n-            info!(event = \"quorum reached\", ?vote);\n \n             let vrmsg = self\n                 .build_validation_result(sv, vote, quorum_type, &p.header())\ndiff --git a/node/src/chain.rs b/node/src/chain.rs\nindex dfe4e1119b..f092ccf2d7 100644\n--- a/node/src/chain.rs\n+++ b/node/src/chain.rs\n@@ -130,7 +130,7 @@ impl<N: Network, DB: database::DB, VM: vm::VMExecution>\n                 recv = result_chan.recv() => {\n                     match recv? {\n                         Err(ConsensusError::Canceled(round)) => {\n-                            info!(event = \"consensus canceled\", round);\n+                            debug!(event = \"consensus canceled\", round);\n                         }\n                         Err(err) => {\n                             // Internal consensus execution has terminated with an error\ndiff --git a/node/src/chain/acceptor.rs b/node/src/chain/acceptor.rs\nindex 79b6fd6847..3585b66feb 100644\n--- a/node/src/chain/acceptor.rs\n+++ b/node/src/chain/acceptor.rs\n@@ -355,20 +355,36 @@ impl<DB: database::DB, VM: vm::VMExecution, N: Network> Acceptor<N, DB, VM> {\n             | Payload::Validation(_)\n             | Payload::Ratification(_)\n             | Payload::ValidationQuorum(_) => {\n-                // Process consensus msg only if they are for the current round\n-                // or at most 10 rounds in the future\n                 let msg_round = msg.header.round;\n-                if msg_round > tip_height\n-                    && msg_round <= (tip_height + MAX_ROUND_DISTANCE)\n-                {\n-                    consensus_task.main_inbound.try_send(msg);\n-                } else {\n-                    warn!(\n-                      event = \"msg discarded\",\n-                      topic = ?msg.topic(),\n-                      info = ?msg.header,\n-                      ray_id = msg.ray_id()\n-                    );\n+\n+                match msg_round {\n+                    // Discard messages from the past\n+                    r if r <= tip_height => {\n+                        debug!(\n+                          event = \"Consensus msg discarded\",\n+                          reason = \"past round\",\n+                          topic = ?msg.topic(),\n+                          info = ?msg.header,\n+                          ray_id = msg.ray_id()\n+                        );\n+                    }\n+\n+                    // Discard messages too far from the future\n+                    r if r > tip_height + MAX_ROUND_DISTANCE => {\n+                        warn!(\n+                          event = \"Consensus msg discarded\",\n+                          reason = \"too far in the future\",\n+                          topic = ?msg.topic(),\n+                          info = ?msg.header,\n+                          ray_id = msg.ray_id()\n+                        );\n+                    }\n+\n+                    _ => {\n+                        // Process consensus msg only if they are for the\n+                        // current round or at most 10 rounds in the future\n+                        consensus_task.main_inbound.try_send(msg);\n+                    }\n                 }\n             }\n \ndiff --git a/node/src/chain/consensus.rs b/node/src/chain/consensus.rs\nindex e4e6eee572..b3212f3341 100644\n--- a/node/src/chain/consensus.rs\n+++ b/node/src/chain/consensus.rs\n@@ -130,7 +130,7 @@ impl Task {\n \n         let (all_num, eligible_num) = current.get_provisioners_info(ru.round);\n \n-        info!(\n+        debug!(\n             event = \"spawn consensus\",\n             id = self.task_id,\n             round = ru.round,\n", "instance_id": "dusk-network__rusk-3236", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to reduce node logs by adjusting the verbosity and relevance of log messages in a blockchain consensus system. It provides a detailed log output example and identifies specific log messages to modify (e.g., changing \"WARN msg discarded\" to a lower level, removing duplicates like \"quorum reached\", and adjusting the placement of \"Step completed\"). Additionally, it offers a possible solution design with specific suggestions. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the desired log levels for all cases or provide clear criteria for what constitutes \"too much\" logging beyond subjective observation. Edge cases, such as how to handle logs under high load or in error conditions, are not mentioned. Constraints or performance implications of logging changes are also absent. Despite these gaps, the overall goal and approach are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the relatively straightforward nature of the required changes, which primarily involve modifying log levels and message handling in a Rust-based codebase. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The changes span multiple files (e.g., `proposal/step.rs`, `validation/handler.rs`, `chain.rs`, `chain/acceptor.rs`, `chain/consensus.rs`), but they are localized to specific log statements. The modifications are mostly about changing log levels (e.g., `info!` to `debug!`), removing duplicate logs, and adding conditional logic for message discarding with appropriate log levels. The overall amount of code change is small, and there is no significant impact on the system's architecture.\n\n2. **Number of Technical Concepts**: The problem requires basic familiarity with Rust's `tracing` crate for logging, understanding of log levels (INFO, WARN, DEBUG), and minimal domain knowledge of blockchain consensus (e.g., rounds, quorum). These concepts are not particularly complex for a developer with moderate experience in Rust or logging systems. No advanced algorithms, design patterns, or deep system-level knowledge are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes introduce conditional logic for handling consensus messages based on round numbers (e.g., discarding past or far-future messages). This requires minor consideration of edge cases, such as ensuring logs are still useful for debugging without being overly verbose. However, the complexity of these edge cases is low, and no significant error handling modifications are needed beyond adjusting log levels.\n\n4. **Overall Complexity**: The task involves understanding the intent behind log messages and making targeted changes to improve clarity and reduce noise. While it requires navigating multiple files and understanding the context of consensus rounds, the logic is not intricate, and the impact is limited to logging behavior rather than core functionality.\n\nGiven these factors, a difficulty score of 0.35 reflects an Easy problem that requires some code logic understanding and simple modifications across a few files. It is slightly above the lower end of the Easy range due to the need to coordinate changes across multiple modules and ensure the logging adjustments do not obscure critical debugging information.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}