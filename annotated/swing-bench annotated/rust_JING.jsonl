{"problem_statement": "Deduplicate fontdb types\n`resvg` optionally depends on `fontdb`. Meanwhile,\r\n\r\n- `usvg::FontStyle` is identical to `fontdb::Style`\r\n- `usvg::FontStretch` is identical to `fontdb::Stretch`\r\n\r\nWorse, there isn't even a conversion method between the two. This is a pain when implementing a `usvg::FontResolver`.\n", "patch": "diff --git a/crates/usvg/src/tree/text.rs b/crates/usvg/src/tree/text.rs\nindex 64903fa6..f488ec79 100644\n--- a/crates/usvg/src/tree/text.rs\n+++ b/crates/usvg/src/tree/text.rs\n@@ -33,6 +33,40 @@ impl Default for FontStretch {\n     }\n }\n \n+#[cfg(feature = \"text\")]\n+impl From<fontdb::Stretch> for FontStretch {\n+    fn from(stretch: fontdb::Stretch) -> Self {\n+        match stretch {\n+            fontdb::Stretch::UltraCondensed => FontStretch::UltraCondensed,\n+            fontdb::Stretch::ExtraCondensed => FontStretch::ExtraCondensed,\n+            fontdb::Stretch::Condensed => FontStretch::Condensed,\n+            fontdb::Stretch::SemiCondensed => FontStretch::SemiCondensed,\n+            fontdb::Stretch::Normal => FontStretch::Normal,\n+            fontdb::Stretch::SemiExpanded => FontStretch::SemiExpanded,\n+            fontdb::Stretch::Expanded => FontStretch::Expanded,\n+            fontdb::Stretch::ExtraExpanded => FontStretch::ExtraExpanded,\n+            fontdb::Stretch::UltraExpanded => FontStretch::UltraExpanded,\n+        }\n+    }\n+}\n+\n+#[cfg(feature = \"text\")]\n+impl From<FontStretch> for fontdb::Stretch {\n+    fn from(stretch: FontStretch) -> Self {\n+        match stretch {\n+            FontStretch::UltraCondensed => fontdb::Stretch::UltraCondensed,\n+            FontStretch::ExtraCondensed => fontdb::Stretch::ExtraCondensed,\n+            FontStretch::Condensed => fontdb::Stretch::Condensed,\n+            FontStretch::SemiCondensed => fontdb::Stretch::SemiCondensed,\n+            FontStretch::Normal => fontdb::Stretch::Normal,\n+            FontStretch::SemiExpanded => fontdb::Stretch::SemiExpanded,\n+            FontStretch::Expanded => fontdb::Stretch::Expanded,\n+            FontStretch::ExtraExpanded => fontdb::Stretch::ExtraExpanded,\n+            FontStretch::UltraExpanded => fontdb::Stretch::UltraExpanded,\n+        }\n+    }\n+}\n+\n /// A font style property.\n #[derive(Clone, Copy, PartialEq, Eq, Debug, Hash)]\n pub enum FontStyle {\n@@ -51,6 +85,28 @@ impl Default for FontStyle {\n     }\n }\n \n+#[cfg(feature = \"text\")]\n+impl From<fontdb::Style> for FontStyle {\n+    fn from(style: fontdb::Style) -> Self {\n+        match style {\n+            fontdb::Style::Normal => FontStyle::Normal,\n+            fontdb::Style::Italic => FontStyle::Italic,\n+            fontdb::Style::Oblique => FontStyle::Oblique,\n+        }\n+    }\n+}\n+\n+#[cfg(feature = \"text\")]\n+impl From<FontStyle> for fontdb::Style {\n+    fn from(style: FontStyle) -> Self {\n+        match style {\n+            FontStyle::Normal => fontdb::Style::Normal,\n+            FontStyle::Italic => fontdb::Style::Italic,\n+            FontStyle::Oblique => fontdb::Style::Oblique,\n+        }\n+    }\n+}\n+\n /// Text font properties.\n #[derive(Clone, Eq, PartialEq, Hash, Debug)]\n pub struct Font {\n", "instance_id": "linebender__resvg-813", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: there is a duplication of types between `usvg` and `fontdb` (specifically `FontStyle` and `FontStretch`), and there is no conversion mechanism between them, which creates friction when implementing a `usvg::FontResolver`. The goal of deduplicating or providing conversion is implied, and the code changes align with this interpretation by adding conversion implementations. However, the statement lacks explicit details on the expected solution (e.g., whether to fully deduplicate by removing one set of types or just add conversions), and there are no mentions of constraints, edge cases, or specific use cases for the `FontResolver`. Additionally, it does not clarify if there are broader implications or dependencies in the codebase that need to be considered. Despite these minor ambiguities, the intent is understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The code changes are straightforward and localized to a single file (`crates/usvg/src/tree/text.rs`), involving the addition of `From` trait implementations for bidirectional conversion between `usvg` and `fontdb` types (`FontStyle` and `FontStretch`). The scope of the change is minimal, with no impact on the broader architecture or interactions between multiple modules. The technical concepts required are basic: understanding Rust's trait system and how to implement conversions using `From`. No complex algorithms, design patterns, or domain-specific knowledge are needed beyond familiarity with the Rust language. There are no explicit edge cases or error handling requirements mentioned in the problem statement, and the code changes do not introduce any error handling logic, as the mappings are direct and one-to-one. The overall effort is minimal, akin to adding a simple utility feature, making this a relatively easy task for a developer with basic to intermediate Rust experience.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Century cutoff for %y undocumented\nIf I run\r\n```rust\r\nuse chrono::NaiveDate;\r\n\r\n\r\nfn main() {\r\n    let res = NaiveDate::parse_from_str(\"01-01-69\", \"%d-%m-%y\");\r\n    println!(\"res: {:?}\", res);\r\n    let res = NaiveDate::parse_from_str(\"01-01-70\", \"%d-%m-%y\");\r\n    println!(\"res: {:?}\", res);\r\n}\r\n```\r\nI get\r\n```\r\nres: Ok(2069-01-01)\r\nres: Ok(1970-01-01)\r\n```\r\n\r\nIt seems that 70 is the cutoff, but the docs don't mention it https://docs.rs/chrono/latest/chrono/format/strftime/index.html#fn1\n", "patch": "diff --git a/src/format/strftime.rs b/src/format/strftime.rs\nindex 555e36f9c0..e446c68c02 100644\n--- a/src/format/strftime.rs\n+++ b/src/format/strftime.rs\n@@ -99,6 +99,8 @@ Notes:\n \n [^1]: `%C`, `%y`:\n    This is floor division, so 100 BCE (year number -99) will print `-1` and `99` respectively.\n+   For `%y`, values greater or equal to 70 are interpreted as being in the 20th century,\n+   values smaller than 70 in the 21st century.\n \n [^2]: `%U`:\n    Week 1 starts with the first Sunday in that year.\n", "instance_id": "chronotope__chrono-1598", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the behavior of the `%y` format specifier in the `chrono` crate for parsing two-digit years is not documented, and the user has observed a cutoff at 70 (interpreting years >=70 as 20th century and <70 as 21st century). The goal of documenting this behavior is implied, and the provided code example effectively demonstrates the issue. However, there are minor ambiguities: the problem statement does not explicitly state whether the task is to update the documentation, confirm the cutoff logic, or modify the behavior. Additionally, edge cases (e.g., behavior for negative years or invalid inputs) are not mentioned, and there is no discussion of whether this cutoff is intentional or a bug. Despite these minor gaps, the intent and issue are understandable with the provided context and code output.", "difficulty_explanation": "The difficulty of this task is very low, as it involves a straightforward documentation update rather than complex code changes. The provided diff shows a minimal modification to a single file (`strftime.rs`), adding just two lines to clarify the century cutoff behavior for the `%y` format specifier. There is no need to understand intricate codebase interactions, implement algorithms, or handle edge cases in the code itself\u2014 the task is purely about documenting an existing behavior. No advanced technical concepts, language features, or domain-specific knowledge are required beyond basic familiarity with Rust documentation practices. The scope is extremely limited, with no impact on the system's architecture or functionality. This places the difficulty in the \"very easy\" range, as it is a simple, localized change akin to fixing a typo or adding a clarifying note.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug(`forge`): `forge coverage --report summary / lcov` doesn't mark empty try/blocks as covered\n### Component\n\nForge\n\n### Have you ensured that all of these are up to date?\n\n- [x] Foundry\n- [x] Foundryup\n\n### What version of Foundry are you on?\n\nforge Version: 1.0.0-nightly Commit SHA: 97e2ebbb7f74189b65943ae0b8537415d709608c Build Timestamp: 2025-02-19T00:21:36.666480000Z (1739924496) Build Profile: maxperf\n\n### What version of Foundryup are you on?\n\nfoundryup: installed - chisel Version: 1.0.0-nightly Commit SHA: 97e2ebbb7f74189b65943ae0b8537415d709608c Build Timestamp: 2025-02-19T00:21:36.666480000Z (1739924496)\n\n### What command(s) is the bug in?\n\n`forge coverage --report summary` and `forge coverage --report lcov`\n\n### Operating System\n\nmacOS (Apple Silicon)\n\n### Describe the bug\n\nCode blocks with empty try/catch blocks like `try contract.call() {} catch {}` are marked uncovered statements in `forge coverage --report summary` even when there are tests covering cases where `contract.call()` succeeds or reverts. \n\nBut when those blocks aren't empty, e.g. `try fn.call() {emit Event()} catch {emit Event()}`, coverage reports all statements are covered as expected.\n\nA code example can be found [here](https://github.com/garyghayrat/forge-coverage-bug). You can see the effect on the coverage summary by commenting out `try` and `catch` statements and running `forge coverage --report summary`.\n\nThe example is kind of silly, but this try/catch pattern is often used in production code like [here](https://github.com/uniswapfoundation/UniStaker/blob/887d7dc0c1db3f17227d13af4d8a791a66912d42/src/UniStaker.sol#L306).\n", "patch": "diff --git a/crates/evm/coverage/src/analysis.rs b/crates/evm/coverage/src/analysis.rs\nindex 89374668031a..37bd4d381e53 100644\n--- a/crates/evm/coverage/src/analysis.rs\n+++ b/crates/evm/coverage/src/analysis.rs\n@@ -290,8 +290,8 @@ impl<'a> ContractVisitor<'a> {\n \n                 Ok(())\n             }\n-            // Try-catch statement. Coverage is reported for expression, for each clause and their\n-            // bodies (if any).\n+            // Try-catch statement. Coverage is reported as branches for catch clauses with\n+            // statements.\n             NodeType::TryStatement => {\n                 self.visit_expression(\n                     &node\n@@ -299,20 +299,53 @@ impl<'a> ContractVisitor<'a> {\n                         .ok_or_else(|| eyre::eyre!(\"try statement had no call\"))?,\n                 )?;\n \n-                // Add coverage for each Try-catch clause.\n-                for clause in node\n+                let branch_id = self.branch_id;\n+                self.branch_id += 1;\n+\n+                let mut clauses = node\n                     .attribute::<Vec<Node>>(\"clauses\")\n-                    .ok_or_else(|| eyre::eyre!(\"try statement had no clause\"))?\n-                {\n-                    // Add coverage for clause statement.\n-                    self.push_item_kind(CoverageItemKind::Statement, &clause.src);\n-                    self.visit_statement(&clause)?;\n-\n-                    // Add coverage for clause body only if it is not empty.\n-                    if let Some(block) = clause.attribute::<Node>(\"block\") {\n-                        if has_statements(&block) {\n-                            self.push_item_kind(CoverageItemKind::Statement, &block.src);\n-                            self.visit_block(&block)?;\n+                    .ok_or_else(|| eyre::eyre!(\"try statement had no clauses\"))?;\n+\n+                let try_block = clauses\n+                    .remove(0)\n+                    .attribute::<Node>(\"block\")\n+                    .ok_or_else(|| eyre::eyre!(\"try statement had no block\"))?;\n+                // Add branch with path id 0 for try (first clause).\n+                self.push_item_kind(\n+                    CoverageItemKind::Branch { branch_id, path_id: 0, is_first_opcode: true },\n+                    &ast::LowFidelitySourceLocation {\n+                        start: node.src.start,\n+                        length: try_block\n+                            .src\n+                            .length\n+                            .map(|length| try_block.src.start + length - node.src.start),\n+                        index: node.src.index,\n+                    },\n+                );\n+                self.visit_block(&try_block)?;\n+\n+                let mut path_id = 1;\n+                for clause in clauses {\n+                    if let Some(catch_block) = clause.attribute::<Node>(\"block\") {\n+                        if has_statements(&catch_block) {\n+                            // Add catch branch if it has statements.\n+                            self.push_item_kind(\n+                                CoverageItemKind::Branch {\n+                                    branch_id,\n+                                    path_id,\n+                                    is_first_opcode: true,\n+                                },\n+                                &catch_block.src,\n+                            );\n+                            self.visit_block(&catch_block)?;\n+                            // Increment path id for next branch.\n+                            path_id += 1;\n+                        } else if clause.attribute::<Node>(\"parameters\").is_some() {\n+                            // Add coverage for clause with parameters and empty statements.\n+                            // (`catch (bytes memory reason) {}`).\n+                            // Catch all clause without statements is ignored (`catch {}`).\n+                            self.push_item_kind(CoverageItemKind::Statement, &clause.src);\n+                            self.visit_statement(&clause)?;\n                         }\n                     }\n                 }\n", "instance_id": "foundry-rs__foundry-9978", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `forge coverage` tool not marking empty `try/catch` blocks as covered, even when tests execute those paths. It provides specific commands (`forge coverage --report summary` and `forge coverage --report lcov`), a reproducible example on GitHub, and a real-world context with a reference to production code. The goal is evident: to fix the coverage reporting for empty `try/catch` blocks. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior for all edge cases (e.g., nested `try/catch` blocks or partial coverage within clauses). Additionally, the problem statement does not specify constraints or performance expectations for the fix. Thus, while the core issue is well-defined, some minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`crates/evm/coverage/src/analysis.rs`) and a specific part of the coverage analysis logic for `try/catch` statements. The diff shows a moderate amount of code change (around 30-40 lines), which involves restructuring how coverage items are tracked for branches and statements in `try/catch` blocks. Second, the technical concepts required include an understanding of Rust (specifically, working with AST-like structures and visitor patterns), familiarity with coverage analysis logic, and the ability to handle conditional branching for different `try/catch` scenarios. These concepts are not overly complex but require a solid grasp of the existing codebase's design. Third, the problem involves handling specific edge cases, such as distinguishing between empty and non-empty `catch` blocks and handling `catch` clauses with parameters but no statements, which adds some complexity to the logic. However, the changes do not appear to impact the broader system architecture or require extensive refactoring. Overall, this problem requires understanding multiple concepts and making moderately complex modifications, but it does not reach the level of deep architectural changes or advanced domain-specific knowledge, justifying a difficulty score of 0.55.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug(`cast`): 4byte-calldata should correctly handle just function selector without additional calldata\n### Component\n\nCast\n\n### Have you ensured that all of these are up to date?\n\n- [x] Foundry\n- [x] Foundryup\n\n### What version of Foundry are you on?\n\nforge Version: 1.0.0-stable\n\n### What version of Foundryup are you on?\n\nN/A\n\n### What command(s) is the bug in?\n\ncast 4byte-decode\n\n### Operating System\n\nmacOS (Apple Silicon)\n\n### Describe the bug\n\n```\ncast 4byte-decode 0x79ba5097\n```\n\nThis should return [acceptOwnership](https://openchain.xyz/signatures?query=0x79ba5097) but instead errors.\n", "patch": "diff --git a/crates/cast/bin/main.rs b/crates/cast/bin/main.rs\nindex c5be05df987b..8342f7b2d6bd 100644\n--- a/crates/cast/bin/main.rs\n+++ b/crates/cast/bin/main.rs\n@@ -518,8 +518,21 @@ async fn main_args(args: CastArgs) -> Result<()> {\n                 sh_println!(\"{sig}\")?\n             }\n         }\n+\n         CastSubcommand::FourByteCalldata { calldata } => {\n             let calldata = stdin::unwrap_line(calldata)?;\n+\n+            if calldata.len() == 10 {\n+                let sigs = decode_function_selector(&calldata).await?;\n+                if sigs.is_empty() {\n+                    eyre::bail!(\"No matching function signatures found for calldata `{calldata}`\");\n+                }\n+                for sig in sigs {\n+                    sh_println!(\"{sig}\")?\n+                }\n+                return Ok(());\n+            }\n+\n             let sigs = decode_calldata(&calldata).await?;\n             sigs.iter().enumerate().for_each(|(i, sig)| {\n                 let _ = sh_println!(\"{}) \\\"{sig}\\\"\", i + 1);\n@@ -537,6 +550,7 @@ async fn main_args(args: CastArgs) -> Result<()> {\n             let tokens = SimpleCast::calldata_decode(sig, &calldata, true)?;\n             print_tokens(&tokens);\n         }\n+\n         CastSubcommand::FourByteEvent { topic } => {\n             let topic = stdin::unwrap_line(topic)?;\n             let sigs = decode_event_topic(&topic).await?;\n", "instance_id": "foundry-rs__foundry-9952", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the bug: the `cast 4byte-decode` command fails to handle a specific input case (a function selector without additional calldata) and errors out instead of returning the expected result (e.g., \"acceptOwnership\"). The goal is evident\u2014fix the command to correctly decode a 4-byte function selector. The input and expected output are implied through the example (`0x79ba5097` should return a function signature). However, there are minor ambiguities: the problem does not explicitly define the expected behavior for all edge cases (e.g., invalid or malformed input of length 10), nor does it specify if there are other related scenarios to handle. Additionally, there are no detailed constraints or requirements beyond the single example. While the issue is valid and understandable, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to several factors. First, the scope of the code change is limited to a single file (`main.rs`) and a specific function handling the `FourByteCalldata` subcommand. The modification involves adding a conditional check for a specific input length (10 characters, representing a 4-byte selector) and invoking an existing function (`decode_function_selector`) to handle it, which is a straightforward logic adjustment. The amount of code change is small, with only about 13 lines added, and it does not impact the broader system architecture or require understanding complex interactions across modules. Second, the technical concepts involved are relatively basic: familiarity with Rust syntax, async/await patterns (already used in the codebase), and the existing decoding logic. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic Ethereum function selectors are required. Third, while the problem statement does not explicitly mention edge cases, the code change includes a simple error message for when no matching signatures are found, indicating minimal error handling complexity. Overall, this is a simple bug fix requiring moderate understanding of the local code context and basic modifications, justifying a score of 0.25.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Async client is not notified of (unexpected) websocket transport close\n## Problem\r\n\r\nUsing the async client without reconnect, when the transport websocket closes properly but without an engineio message indicating the close, the client doesn't fire an event and as such the user of the client doesn't notice the closure.\r\n\r\nThis has the problem that we can't easily react to the closure and keep thinking that we are connected.\r\n\r\n## Steps to reproduce\r\n\r\n1. Run a server like this:\r\n\r\n    ```javascript\r\n    const server = require('http').createServer();\r\n    const io = require('socket.io')(server);\r\n\r\n    console.log('Started');\r\n    var callback = client => {\r\n        console.log('Connected!');\r\n\r\n        // Close underlying websocket connection\r\n        client.client.conn.close();\r\n    };\r\n    io.on('connection', callback);\r\n    // the socket.io client runs on port 4201\r\n    server.listen(4200);\r\n    ```\r\n\r\n1. Connect with a Rust client like the following. Make sure to set `reconnect` to false. To get some more logging, you can run `export RUST_LOG=\"info,tungstenite::protocol=trace,tungstenite::protocol::frame::frame=debug,rust_socketio=trace,rust_engineio=trace\"` first.\r\n\r\n    ```rust\r\n    use futures_util::FutureExt;\r\n    use log::{error, info};\r\n    use rust_socketio::{asynchronous::ClientBuilder, Event, TransportType,};\r\n    use tokio::signal::{self,unix::{signal, SignalKind}};\r\n\r\n    #[tokio::main]\r\n    async fn main() {\r\n        env_logger::Builder::new().parse_default_env().init();\r\n\r\n        info!(\"Starting\");\r\n\r\n        // get a socket that is connected to the admin namespace\r\n        let socket = ClientBuilder::new(\"http://localhost:4200/\")\r\n            .transport_type(TransportType::Websocket)\r\n            .reconnect(false)\r\n            .on(Event::Connect, |_payload, _client| {{ async move { info!(\"Connected!\"); }.boxed() }})\r\n            .on(Event::Close, |_payload, _client| {{ async move { info!(\"Connection closed!\"); }.boxed() }})\r\n            .on(Event::Error, |err, _| { async move { error!(\"Error: {:#?}\", err) }.boxed() })\r\n            .connect()\r\n            .await\r\n            .expect(\"Connection failed\");\r\n\r\n        let mut sigterm = signal(SignalKind::terminate()).expect(\"unable to create sigterm signal\");\r\n        tokio::select! {\r\n            _ = signal::ctrl_c() => { info!(\"Received SIGINT, shutting down.\"); },\r\n            _ = sigterm.recv() => { info!(\"Received SIGTERM, shutting down.\"); },\r\n        }\r\n\r\n        socket.disconnect().await.expect(\"Disconnect failed\");\r\n    }\r\n    ```\r\n\r\n1. Notice that the tungstenite websocket closes by receiving an sending an `opecode: CLOSE` frame, but the client isn't notified.\r\n    \r\n## Reason\r\n\r\nWhen the async client connects to the server, it spawns a new thread to handle the arriving messages asynchronously, which is immediately detached with no chance of awaiting its completion.\r\n\r\nThis may happen if the underlying tungstenite websocket shuts down ('Received close frame: None') but there are no engineio/socketio close frames. Hence, since the stream terminates, the message handling task stops without a Close or Error event being fired.\r\n\r\nhttps://github.com/1c3t3a/rust-socketio/blob/64d2a133560aa0b31b2263a820645f982e7276b3/socketio/src/asynchronous/client/client.rs#L151-L159\r\n\r\nThis was kind of introduced by me in a9a81e9 when I added handling for the stream closure to avoid the panic. As it says in the commit message *However, the program which uses the socket.io client currently isn't notified of the unexpected closure.*, so time to clean that up :)\r\n\r\n## Potential Solution\r\n\r\nLooking at the official JavaScript [`disconnect` specification](https://socket.io/docs/v4/client-socket-instance/#disconnect) there are reasons specified for the disconnection which include a `transport close`.\r\n\r\nThus, once we disconnect for an unknown reason (e.g. the transport closed), we could fire a `CLOSE` event to signal the closure to the user.\r\n\r\nAlternatively, we could also fire this event only after the loop terminates, before the thread itself terminates.\r\n\r\nSee #449 for an implementation of that and feel free to discuss the proposed changes.\n", "patch": "diff --git a/ci/socket-io.js b/ci/socket-io.js\nindex 37649712..8f609b33 100644\n--- a/ci/socket-io.js\n+++ b/ci/socket-io.js\n@@ -36,6 +36,14 @@ var callback = client => {\n             ack(Buffer.from([1, 2, 3]));\n         }\n     });\n+\n+    // This event allows the test framework to arbitrarily close the underlying connection\n+    client.on('close_transport', data => {\n+        console.log(['close_transport', 'Request to close transport received'])\n+        // Close underlying websocket connection\n+        client.client.conn.close();\n+    })\n+\n     client.emit('Hello from the message event!');\n     client.emit('test', 'Hello from the test event!');\n     client.emit(Buffer.from([4, 5, 6]));\ndiff --git a/socketio/src/asynchronous/client/client.rs b/socketio/src/asynchronous/client/client.rs\nindex 67feb7db..da1d5f09 100644\n--- a/socketio/src/asynchronous/client/client.rs\n+++ b/socketio/src/asynchronous/client/client.rs\n@@ -2,7 +2,7 @@ use std::{ops::DerefMut, pin::Pin, sync::Arc};\n \n use backoff::{backoff::Backoff, ExponentialBackoffBuilder};\n use futures_util::{future::BoxFuture, stream, Stream, StreamExt};\n-use log::trace;\n+use log::{error, trace};\n use rand::{thread_rng, Rng};\n use serde_json::Value;\n use tokio::{\n@@ -19,7 +19,7 @@ use crate::{\n     asynchronous::socket::Socket as InnerSocket,\n     error::{Error, Result},\n     packet::{Packet, PacketId},\n-    Event, Payload,\n+    CloseReason, Event, Payload,\n };\n \n #[derive(Default)]\n@@ -162,7 +162,21 @@ impl Client {\n                 drop(stream);\n \n                 let should_reconnect = match *(client_clone.disconnect_reason.read().await) {\n-                    DisconnectReason::Unknown => reconnect,\n+                    DisconnectReason::Unknown => {\n+                        // If we disconnected for an unknown reason, the client might not have noticed\n+                        // the closure yet. Hence, fire a transport close event to notify it.\n+                        // We don't need to do that in the other cases, since proper server close\n+                        // and manual client close are handled explicitly.\n+                        if let Some(err) = client_clone\n+                            .callback(&Event::Close, CloseReason::TransportClose.as_str())\n+                            .await\n+                            .err()\n+                        {\n+                            error!(\"Error while notifying client of transport close: {err}\")\n+                        }\n+\n+                        reconnect\n+                    }\n                     DisconnectReason::Manual => false,\n                     DisconnectReason::Server => reconnect_on_disconnect,\n                 };\n@@ -510,7 +524,8 @@ impl Client {\n                 }\n                 PacketId::Disconnect => {\n                     *(self.disconnect_reason.write().await) = DisconnectReason::Server;\n-                    self.callback(&Event::Close, \"\").await?;\n+                    self.callback(&Event::Close, CloseReason::IOServerDisconnect.as_str())\n+                        .await?;\n                 }\n                 PacketId::ConnectError => {\n                     self.callback(\n@@ -590,7 +605,7 @@ mod test {\n         },\n         error::Result,\n         packet::{Packet, PacketId},\n-        Payload, TransportType,\n+        CloseReason, Event, Payload, TransportType,\n     };\n \n     #[tokio::test]\n@@ -926,6 +941,50 @@ mod test {\n         Ok(())\n     }\n \n+    #[tokio::test]\n+    async fn socket_io_transport_close() -> Result<()> {\n+        let url = crate::test::socket_io_server();\n+\n+        let (tx, mut rx) = mpsc::channel(1);\n+\n+        let notify = Arc::new(tokio::sync::Notify::new());\n+        let notify_clone = notify.clone();\n+\n+        let socket = ClientBuilder::new(url)\n+            .on(Event::Connect, move |_, _| {\n+                let cl = notify_clone.clone();\n+                async move {\n+                    cl.notify_one();\n+                }\n+                .boxed()\n+            })\n+            .on(Event::Close, move |payload, _| {\n+                let clone_tx = tx.clone();\n+                async move { clone_tx.send(payload).await.unwrap() }.boxed()\n+            })\n+            .connect()\n+            .await?;\n+\n+        // Wait until socket is connected\n+        let connect_timeout = timeout(Duration::from_secs(1), notify.notified()).await;\n+        assert!(connect_timeout.is_ok());\n+\n+        // Instruct server to close transport\n+        let result = socket.emit(\"close_transport\", Payload::from(\"\")).await;\n+        assert!(result.is_ok());\n+\n+        // Wait for Event::Close\n+        let rx_timeout = timeout(Duration::from_secs(1), rx.recv()).await;\n+        assert!(rx_timeout.is_ok());\n+\n+        assert_eq!(\n+            rx_timeout.unwrap(),\n+            Some(Payload::from(CloseReason::TransportClose.as_str()))\n+        );\n+\n+        Ok(())\n+    }\n+\n     #[tokio::test]\n     async fn socketio_polling_integration() -> Result<()> {\n         let url = crate::test::socket_io_server();\ndiff --git a/socketio/src/client/raw_client.rs b/socketio/src/client/raw_client.rs\nindex 0686683f..865a9ec6 100644\n--- a/socketio/src/client/raw_client.rs\n+++ b/socketio/src/client/raw_client.rs\n@@ -1,7 +1,7 @@\n use super::callback::Callback;\n use crate::packet::{Packet, PacketId};\n use crate::Error;\n-pub(crate) use crate::{event::Event, payload::Payload};\n+pub(crate) use crate::{event::CloseReason, event::Event, payload::Payload};\n use rand::{thread_rng, Rng};\n use serde_json::Value;\n \n@@ -149,7 +149,7 @@ impl RawClient {\n         let _ = self.socket.send(disconnect_packet);\n         self.socket.disconnect()?;\n \n-        let _ = self.callback(&Event::Close, \"\"); // trigger on_close\n+        let _ = self.callback(&Event::Close, CloseReason::IOClientDisconnect.as_str()); // trigger on_close\n         Ok(())\n     }\n \n@@ -372,7 +372,7 @@ impl RawClient {\n                     self.callback(&Event::Connect, \"\")?;\n                 }\n                 PacketId::Disconnect => {\n-                    self.callback(&Event::Close, \"\")?;\n+                    self.callback(&Event::Close, CloseReason::IOServerDisconnect.as_str())?;\n                 }\n                 PacketId::ConnectError => {\n                     self.callback(\ndiff --git a/socketio/src/event.rs b/socketio/src/event.rs\nindex 2d2ed1f9..ef1c65a8 100644\n--- a/socketio/src/event.rs\n+++ b/socketio/src/event.rs\n@@ -57,3 +57,38 @@ impl Display for Event {\n         f.write_str(self.as_str())\n     }\n }\n+\n+/// A `CloseReason` is the payload of the [`Event::Close`] and specifies the reason for\n+/// why it was fired.\n+/// These are aligned with the official Socket.IO disconnect reasons, see\n+/// https://socket.io/docs/v4/client-socket-instance/#disconnect\n+#[derive(Debug, PartialEq, PartialOrd, Clone, Eq, Hash)]\n+pub enum CloseReason {\n+    IOServerDisconnect,\n+    IOClientDisconnect,\n+    TransportClose,\n+}\n+\n+impl CloseReason {\n+    pub fn as_str(&self) -> &str {\n+        match self {\n+            // Inspired by https://github.com/socketio/socket.io/blob/d0fc72042068e7eaef448941add617f05e1ec236/packages/socket.io-client/lib/socket.ts#L865\n+            CloseReason::IOServerDisconnect => \"io server disconnect\",\n+            // Inspired by https://github.com/socketio/socket.io/blob/d0fc72042068e7eaef448941add617f05e1ec236/packages/socket.io-client/lib/socket.ts#L911\n+            CloseReason::IOClientDisconnect => \"io client disconnect\",\n+            CloseReason::TransportClose => \"transport close\",\n+        }\n+    }\n+}\n+\n+impl From<CloseReason> for String {\n+    fn from(event: CloseReason) -> Self {\n+        Self::from(event.as_str())\n+    }\n+}\n+\n+impl Display for CloseReason {\n+    fn fmt(&self, f: &mut Formatter<'_>) -> FmtResult {\n+        f.write_str(self.as_str())\n+    }\n+}\ndiff --git a/socketio/src/lib.rs b/socketio/src/lib.rs\nindex b913eb4d..1445cc82 100644\n--- a/socketio/src/lib.rs\n+++ b/socketio/src/lib.rs\n@@ -191,7 +191,7 @@ pub mod asynchronous;\n \n pub use error::Error;\n \n-pub use {event::Event, payload::Payload};\n+pub use {event::CloseReason, event::Event, payload::Payload};\n \n pub use client::{ClientBuilder, RawClient, TransportType};\n \n", "instance_id": "1c3t3a__rust-socketio-449", "clarity": 3, "difficulty": 0.55, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly defines the issue: the async client in a Rust-based Socket.IO implementation fails to notify users of an unexpected WebSocket transport closure when there is no corresponding Engine.IO message. The goal is explicit\u2014to ensure the client fires a close event in such scenarios. The input and output expectations are indirectly but clearly conveyed through the provided reproduction steps and code snippets. The statement includes detailed steps to reproduce the issue, complete with server and client code examples, environment setup instructions, and logs to observe the behavior. Constraints, such as disabling reconnection, are specified. Additionally, the reason for the issue is explained with reference to specific code lines and past commits, and a potential solution is suggested with a link to official documentation for alignment. There are no significant ambiguities, and the inclusion of a proposed implementation discussion further enhances clarity. The only minor omission is a lack of explicit mention of specific edge cases beyond the primary scenario, but the context and reproduction steps cover the critical details needed to understand and address the problem.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, involving modifications across a few files (e.g., `client.rs`, `raw_client.rs`, `event.rs`) in the Rust Socket.IO library, but not requiring a full architectural overhaul. The changes primarily focus on adding a new `CloseReason` enum to categorize disconnection reasons and updating the event firing logic to handle transport closures explicitly, which impacts the client notification mechanism. Second, the technical concepts involved include asynchronous programming in Rust (using `tokio` and `futures_util`), WebSocket communication (via `tungstenite`), and understanding the Socket.IO protocol's disconnection behavior as per its official specification. These concepts require a solid grasp of async Rust and event-driven programming, but they are not extraordinarily complex for an experienced developer. Third, the problem necessitates handling a specific edge case\u2014unexpected transport closure without an Engine.IO message\u2014which the code changes address by firing a `Close` event with a `TransportClose` reason. Additional error handling logic is introduced to log errors during notification, but the edge cases are not overly intricate. Finally, the changes do not significantly impact the broader system architecture or performance, as they are localized to the client disconnection handling. Overall, this problem requires understanding multiple concepts and making targeted, moderately complex modifications, justifying a score of 0.55, slightly above the medium threshold due to the need for async Rust expertise and protocol-specific knowledge.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`oxipng` keeps iCCP chunk when making PNGs greyscale, resulting in warnings from libpng\n1. Get a greyscale PNG encoded using RGB and saved with an `iCCP` chunk. For example, the below image:\n\n![Image](https://github.com/user-attachments/assets/552cdfd4-9565-4fe4-bcd2-d310e9199be1)\n\n2. Open that image with a program that uses libpng (for example, `cwebp` or `cjxl`). No warnings will show.\n3. Run `oxipng` on that image.\n4. Open again the image with a problem that uses libpng. Now a warning will show:\n\n```\nlibpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n```\n\nMost programs will not be bothered by this warning, but some will (I encountered this because [`cjpegli` refuses to compress an image if it encounters this error](https://github.com/google/jpegli/blob/9e21f7e38283fe0837be763e02d97b7f4988831f/third_party/apngdis/dec.cc#L1107-L1109) (I have verified that replacing this `return` with a `continue` solves my problem). Though, strangely, despite [having the same code](https://github.com/libjxl/libjxl/blob/b3fc5f67b21c52a3a0cd24242ffabde4e79c018d/lib/extras/dec/apng.cc#L1107-L1109) `cjxl` only prints the warning and does not error, and I\u2019m not sure why).\n\nI\u2019m not sure what the best solution is here. Maybe libpng is wrong, or `cjpegli` is wrong.\n", "patch": "diff --git a/src/headers.rs b/src/headers.rs\nindex cf447ed0..2bfbda11 100644\n--- a/src/headers.rs\n+++ b/src/headers.rs\n@@ -1,5 +1,5 @@\n use indexmap::IndexSet;\n-use log::warn;\n+use log::{debug, trace, warn};\n use rgb::{RGB16, RGBA8};\n \n use crate::{\n@@ -8,7 +8,7 @@ use crate::{\n     display_chunks::DISPLAY_CHUNKS,\n     error::PngError,\n     interlace::Interlacing,\n-    Deflaters, PngResult,\n+    Deflaters, Options, PngResult,\n };\n \n #[derive(Debug, Clone)]\n@@ -314,3 +314,97 @@ pub fn srgb_rendering_intent(icc_data: &[u8]) -> Option<u8> {\n         _ => None,\n     }\n }\n+\n+/// Process aux chunks and potentially adjust options before optimizing\n+pub fn preprocess_chunks(aux_chunks: &mut Vec<Chunk>, opts: &mut Options) {\n+    let has_srgb = aux_chunks.iter().any(|c| &c.name == b\"sRGB\");\n+    // Grayscale conversion should not be performed if the image is not in the sRGB colorspace\n+    // An sRGB profile would need to be stripped on conversion, so disallow if stripping is disabled\n+    let mut allow_grayscale = !has_srgb || opts.strip != StripChunks::None;\n+\n+    if let Some(iccp_idx) = aux_chunks.iter().position(|c| &c.name == b\"iCCP\") {\n+        allow_grayscale = false;\n+        // See if we can replace an iCCP chunk with an sRGB chunk\n+        let may_replace_iccp = opts.strip != StripChunks::None && opts.strip.keep(b\"sRGB\");\n+        if may_replace_iccp && has_srgb {\n+            // Files aren't supposed to have both chunks, so we chose to honor sRGB\n+            trace!(\"Removing iCCP chunk due to conflict with sRGB chunk\");\n+            aux_chunks.remove(iccp_idx);\n+            allow_grayscale = true;\n+        } else if let Some(icc) = extract_icc(&aux_chunks[iccp_idx]) {\n+            let intent = if may_replace_iccp {\n+                srgb_rendering_intent(&icc)\n+            } else {\n+                None\n+            };\n+            // sRGB-like profile can be replaced with an sRGB chunk with the same rendering intent\n+            if let Some(intent) = intent {\n+                trace!(\"Replacing iCCP chunk with equivalent sRGB chunk\");\n+                aux_chunks[iccp_idx] = Chunk {\n+                    name: *b\"sRGB\",\n+                    data: vec![intent],\n+                };\n+                allow_grayscale = true;\n+            } else if opts.idat_recoding {\n+                // Try recompressing the profile\n+                let cur_len = aux_chunks[iccp_idx].data.len();\n+                if let Ok(iccp) = make_iccp(&icc, opts.deflate, Some(cur_len - 1)) {\n+                    debug!(\n+                        \"Recompressed iCCP chunk: {} ({} bytes decrease)\",\n+                        iccp.data.len(),\n+                        cur_len - iccp.data.len()\n+                    );\n+                    aux_chunks[iccp_idx] = iccp;\n+                }\n+            }\n+        }\n+    }\n+\n+    if !allow_grayscale && opts.grayscale_reduction {\n+        debug!(\"Disabling grayscale reduction due to presence of sRGB or iCCP chunk\");\n+        opts.grayscale_reduction = false;\n+    }\n+\n+    // Check for APNG by presence of acTL chunk\n+    if aux_chunks.iter().any(|c| &c.name == b\"acTL\") {\n+        warn!(\"APNG detected, disabling all reductions\");\n+        opts.interlace = None;\n+        opts.bit_depth_reduction = false;\n+        opts.color_type_reduction = false;\n+        opts.palette_reduction = false;\n+        opts.grayscale_reduction = false;\n+    }\n+}\n+\n+/// Perform cleanup of certain aux chunks after optimization has been completed\n+pub fn postprocess_chunks(aux_chunks: &mut Vec<Chunk>, ihdr: &IhdrData, orig_ihdr: &IhdrData) {\n+    // If the depth/color type has changed, some chunks may be invalid and should be dropped\n+    // While these could potentially be converted, they have no known use case today and are\n+    // generally more trouble than they're worth\n+    if orig_ihdr.bit_depth != ihdr.bit_depth || orig_ihdr.color_type != ihdr.color_type {\n+        aux_chunks.retain(|c| {\n+            let invalid = &c.name == b\"bKGD\" || &c.name == b\"sBIT\" || &c.name == b\"hIST\";\n+            if invalid {\n+                warn!(\n+                    \"Removing {} chunk as it no longer matches the image data\",\n+                    std::str::from_utf8(&c.name).unwrap()\n+                );\n+            }\n+            !invalid\n+        });\n+    }\n+\n+    // Remove any sRGB or iCCP chunks if the image was converted to or from grayscale\n+    if orig_ihdr.color_type.is_gray() != ihdr.color_type.is_gray() {\n+        aux_chunks.retain(|c| {\n+            let invalid = &c.name == b\"sRGB\" || &c.name == b\"iCCP\";\n+            if invalid {\n+                trace!(\n+                    \"Removing {} chunk as it no longer matches the color type\",\n+                    std::str::from_utf8(&c.name).unwrap()\n+                );\n+            }\n+            !invalid\n+        });\n+    }\n+}\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 3546cc05..10c85d09 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -26,7 +26,6 @@ extern crate rayon;\n mod rayon;\n \n use std::{\n-    borrow::Cow,\n     fs::{File, Metadata},\n     io::{stdin, stdout, BufWriter, Read, Write},\n     path::Path,\n@@ -155,23 +154,27 @@ impl RawImage {\n \n     /// Create an optimized png from the raw image data using the options provided\n     pub fn create_optimized_png(&self, opts: &Options) -> PngResult<Vec<u8>> {\n+        let mut opts = opts.to_owned();\n+        let mut aux_chunks: Vec<_> = self\n+            .aux_chunks\n+            .iter()\n+            .filter(|c| opts.strip.keep(&c.name))\n+            .cloned()\n+            .collect();\n+        preprocess_chunks(&mut aux_chunks, &mut opts);\n+\n         let deadline = Arc::new(Deadline::new(opts.timeout));\n-        let Some(result) = optimize_raw(self.png.clone(), opts, deadline, None) else {\n+        let Some(result) = optimize_raw(self.png.clone(), &opts, deadline, None) else {\n             return Err(PngError::new(\"Failed to optimize input data\"));\n         };\n \n         let mut png = PngData {\n             raw: result.image,\n             idat_data: result.idat_data,\n-            aux_chunks: self\n-                .aux_chunks\n-                .iter()\n-                .filter(|c| opts.strip.keep(&c.name))\n-                .cloned()\n-                .collect(),\n+            aux_chunks,\n             frames: Vec::new(),\n         };\n-        postprocess_chunks(&mut png, opts, &self.png.ihdr);\n+        postprocess_chunks(&mut png.aux_chunks, &png.raw.ihdr, &self.png.ihdr);\n \n         Ok(png.output())\n     }\n@@ -346,19 +349,9 @@ fn optimize_png(\n     debug!(\"    IDAT size = {} bytes\", idat_original_size);\n     debug!(\"    File size = {} bytes\", file_original_size);\n \n-    // Check for APNG by presence of acTL chunk\n-    let opts = if png.aux_chunks.iter().any(|c| &c.name == b\"acTL\") {\n-        warn!(\"APNG detected, disabling all reductions\");\n-        let mut opts = opts.to_owned();\n-        opts.interlace = None;\n-        opts.bit_depth_reduction = false;\n-        opts.color_type_reduction = false;\n-        opts.palette_reduction = false;\n-        opts.grayscale_reduction = false;\n-        Cow::Owned(opts)\n-    } else {\n-        Cow::Borrowed(opts)\n-    };\n+    let mut opts = opts.to_owned();\n+    preprocess_chunks(&mut png.aux_chunks, &mut opts);\n+\n     let max_size = if opts.force {\n         None\n     } else {\n@@ -368,10 +361,9 @@ fn optimize_png(\n         png.raw = result.image;\n         png.idat_data = result.idat_data;\n         recompress_frames(png, &opts, deadline, result.filter)?;\n+        postprocess_chunks(&mut png.aux_chunks, &png.raw.ihdr, &raw.ihdr);\n     }\n \n-    postprocess_chunks(png, &opts, &raw.ihdr);\n-\n     let output = png.output();\n \n     if idat_original_size >= png.idat_data.len() {\n@@ -619,61 +611,6 @@ fn report_format(prefix: &str, png: &PngImage) {\n     );\n }\n \n-/// Perform cleanup of certain chunks from the `PngData` object, after optimization has been completed\n-fn postprocess_chunks(png: &mut PngData, opts: &Options, orig_ihdr: &IhdrData) {\n-    if let Some(iccp_idx) = png.aux_chunks.iter().position(|c| &c.name == b\"iCCP\") {\n-        // See if we can replace an iCCP chunk with an sRGB chunk\n-        let may_replace_iccp = opts.strip != StripChunks::None && opts.strip.keep(b\"sRGB\");\n-        if may_replace_iccp && png.aux_chunks.iter().any(|c| &c.name == b\"sRGB\") {\n-            // Files aren't supposed to have both chunks, so we chose to honor sRGB\n-            trace!(\"Removing iCCP chunk due to conflict with sRGB chunk\");\n-            png.aux_chunks.remove(iccp_idx);\n-        } else if let Some(icc) = extract_icc(&png.aux_chunks[iccp_idx]) {\n-            let intent = if may_replace_iccp {\n-                srgb_rendering_intent(&icc)\n-            } else {\n-                None\n-            };\n-            // sRGB-like profile can be replaced with an sRGB chunk with the same rendering intent\n-            if let Some(intent) = intent {\n-                trace!(\"Replacing iCCP chunk with equivalent sRGB chunk\");\n-                png.aux_chunks[iccp_idx] = Chunk {\n-                    name: *b\"sRGB\",\n-                    data: vec![intent],\n-                };\n-            } else if opts.idat_recoding {\n-                // Try recompressing the profile\n-                let cur_len = png.aux_chunks[iccp_idx].data.len();\n-                if let Ok(iccp) = make_iccp(&icc, opts.deflate, Some(cur_len - 1)) {\n-                    debug!(\n-                        \"Recompressed iCCP chunk: {} ({} bytes decrease)\",\n-                        iccp.data.len(),\n-                        cur_len - iccp.data.len()\n-                    );\n-                    png.aux_chunks[iccp_idx] = iccp;\n-                }\n-            }\n-        }\n-    }\n-\n-    // If the depth/color type has changed, some chunks may be invalid and should be dropped\n-    // While these could potentially be converted, they have no known use case today and are\n-    // generally more trouble than they're worth\n-    let ihdr = &png.raw.ihdr;\n-    if orig_ihdr.bit_depth != ihdr.bit_depth || orig_ihdr.color_type != ihdr.color_type {\n-        png.aux_chunks.retain(|c| {\n-            let invalid = &c.name == b\"bKGD\" || &c.name == b\"sBIT\" || &c.name == b\"hIST\";\n-            if invalid {\n-                warn!(\n-                    \"Removing {} chunk as it no longer matches the image data\",\n-                    std::str::from_utf8(&c.name).unwrap()\n-                );\n-            }\n-            !invalid\n-        });\n-    }\n-}\n-\n /// Recompress the additional frames of an APNG\n fn recompress_frames(\n     png: &mut PngData,\n", "instance_id": "shssoichiro__oxipng-682", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: `oxipng` retains the iCCP chunk when converting PNG images to grayscale, which causes warnings or errors in programs using libpng (e.g., `cjpegli`). The statement provides a reproducible scenario with steps to observe the issue, including an example image and specific tools to test with. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or desired outcome\u2014should `oxipng` strip the iCCP chunk, modify it, or handle it differently? Additionally, while it mentions the behavior of different tools (`cjpegli` vs `cjxl`), it lacks clarity on whether this inconsistency is part of the problem to solve or just context. Edge cases, such as images with multiple conflicting chunks or non-standard iCCP profiles, are not addressed. Overall, the statement is valid and mostly clear but lacks precision in defining the goal and constraints.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving modifications across two files (`headers.rs` and `lib.rs`) and the addition of significant logic (around 100 lines of new code). The changes require understanding and manipulating PNG chunk data (e.g., iCCP, sRGB) and their interactions with image properties like color type and grayscale conversion, which adds complexity. Second, the technical concepts involved include familiarity with PNG file structure, chunk processing, and image encoding standards, as well as Rust-specific features like ownership and borrowing (evident in the use of mutable references and vector operations). Third, the problem requires handling specific edge cases, such as conflicts between sRGB and iCCP chunks, and decisions on whether to strip or replace chunks based on user options, which are addressed in the code changes. However, the changes do not significantly impact the overall architecture of the system, nor do they involve advanced algorithms or performance-critical optimizations. The problem does not require deep domain-specific knowledge beyond PNG specifications, which are well-documented. Therefore, I rate this as a medium difficulty task (0.55), as it involves understanding multiple concepts and making targeted, non-trivial modifications, but it remains within the scope of a moderately experienced developer familiar with image processing or file formats.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "GPU indexing does not work with payload\nI try to use GPU indexing on small index with payload\n\n## Current Behavior\nQdrant panicked after starting indexation of payload field indexes\n\n## Steps to Reproduce\n1. create new collection with default qdrant config + gpu.indexing = true\n2. create payload indexes\n3. upload data thought qdrant-client(rust). 2000000 vectors D=512\n\nconfig with  optimizer status\n\n```json\n{\n  \"result\": {\n    \"status\": \"red\",\n    \"optimizer_status\": {\n      \"error\": \"Service internal error: Optimization task panicked: removal index (is 0) should be < len (is 0)\"\n    },\n    \"indexed_vectors_count\": 0,\n    \"points_count\": 207041,\n    \"segments_count\": 8,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 512,\n          \"distance\": \"Cosine\",\n          \"on_disk\": true\n        },\n        \"shard_number\": 1,\n        \"replication_factor\": 1,\n        \"write_consistency_factor\": 1,\n        \"on_disk_payload\": true\n      },\n      \"hnsw_config\": {\n        \"m\": 32,\n        \"ef_construct\": 128,\n        \"full_scan_threshold\": 10000,\n        \"max_indexing_threads\": 0,\n        \"on_disk\": true\n      },\n      \"optimizer_config\": {\n        \"deleted_threshold\": 0.2,\n        \"vacuum_min_vector_number\": 1000,\n        \"default_segment_number\": 0,\n        \"max_segment_size\": null,\n        \"memmap_threshold\": 10000,\n        \"indexing_threshold\": 20000,\n        \"flush_interval_sec\": 5,\n        \"max_optimization_threads\": null\n      },\n      \"wal_config\": {\n        \"wal_capacity_mb\": 32,\n        \"wal_segments_ahead\": 0\n      },\n      \"quantization_config\": {\n        \"scalar\": {\n          \"type\": \"int8\",\n          \"quantile\": 0.99,\n          \"always_ram\": true\n        }\n      },\n      \"strict_mode_config\": {\n        \"enabled\": false\n      }\n    },\n    \"payload_schema\": {\n      \"c1\": {\n        \"data_type\": \"integer\",\n        \"params\": {\n          \"type\": \"integer\",\n          \"lookup\": true,\n          \"range\": true,\n          \"is_principal\": null,\n          \"on_disk\": false\n        },\n        \"points\": 12252\n      },\n      \"p1\": {\n        \"data_type\": \"datetime\",\n        \"params\": {\n          \"type\": \"datetime\",\n          \"is_principal\": null,\n          \"on_disk\": false\n        },\n        \"points\": 79797\n      },\n      \"t1\": {\n        \"data_type\": \"keyword\",\n        \"params\": {\n          \"type\": \"keyword\",\n          \"on_disk\": false\n        },\n        \"points\": 79797\n      },\n      \"f1\": {\n        \"data_type\": \"integer\",\n        \"params\": {\n          \"type\": \"integer\",\n          \"lookup\": true,\n          \"range\": true,\n          \"is_principal\": null,\n          \"on_disk\": false\n        },\n        \"points\": 38570\n      },\n      \"a1\": {\n        \"data_type\": \"keyword\",\n        \"params\": {\n          \"type\": \"keyword\",\n          \"on_disk\": false\n        },\n        \"points\": 79797\n      },\n      \"p2\": {\n        \"data_type\": \"keyword\",\n        \"params\": {\n          \"type\": \"keyword\",\n          \"on_disk\": false\n        },\n        \"points\": 79797\n      },\n      \"l1\": {\n        \"data_type\": \"integer\",\n        \"params\": {\n          \"type\": \"integer\",\n          \"lookup\": true,\n          \"range\": true,\n          \"is_principal\": null,\n          \"on_disk\": false\n        },\n        \"points\": 0\n      }\n    }\n  },\n  \"status\": \"ok\",\n  \"time\": 0.00009899\n}\n```\n\nlog of qdrant. i built from source\n\n```log\n2025-01-17T12:46:17.247549Z DEBUG segment::index::hnsw_index::hnsw: building HNSW for 19927 vectors with 1 CPUs and GPU    \n2025-01-17T12:46:17.284228Z DEBUG segment::index::hnsw_index::gpu::shader_builder: Shader compilation took: 7.949189ms    \n2025-01-17T12:46:17.325421Z DEBUG segment::index::hnsw_index::gpu::shader_builder: Shader compilation took: 41.158017ms    \n2025-01-17T12:46:17.555323Z DEBUG segment::index::hnsw_index::gpu::gpu_graph_builder: Starting GPU level 4    \n2025-01-17T12:46:17.559169Z DEBUG segment::index::hnsw_index::gpu::gpu_graph_builder: Starting GPU level 3    \n2025-01-17T12:46:17.562265Z DEBUG segment::index::hnsw_index::gpu::gpu_graph_builder: Starting GPU level 2    \n2025-01-17T12:46:17.567253Z DEBUG segment::index::hnsw_index::gpu::gpu_graph_builder: Starting GPU level 1    \n2025-01-17T12:46:17.598774Z DEBUG segment::index::hnsw_index::gpu::gpu_graph_builder: Starting GPU level 0    \n2025-01-17T12:46:17.601496Z DEBUG collection::update_handler: Optimizing segments: [0, 7]    \n2025-01-17T12:46:17.724459Z DEBUG segment::index::hnsw_index::gpu::gpu_insert_context: Gpu graph patches time: 138.817831ms, count 48, avg 2.892038ms    \n2025-01-17T12:46:17.724470Z DEBUG segment::index::hnsw_index::gpu::gpu_insert_context: Gpu graph update entries time: 21.479809ms, count 152, avg 141.314\u00b5s    \n2025-01-17T12:46:17.725137Z DEBUG segment::index::hnsw_index::hnsw: Finish main graph in time 476.098102ms    \n2025-01-17T12:46:17.725383Z DEBUG segment::index::hnsw_index::hnsw: building additional index for field f1   \n2025-01-17T12:46:17.725404Z DEBUG segment::index::hnsw_index::hnsw: building additional index for field a1    \n2025-01-17T12:46:17.725474Z DEBUG segment::index::hnsw_index::hnsw: building additional index for field c1    \n2025-01-17T12:46:17.734271Z DEBUG segment::index::hnsw_index::gpu::shader_builder: Shader compilation took: 7.796939ms    \n2025-01-17T12:46:17.775174Z DEBUG segment::index::hnsw_index::gpu::shader_builder: Shader compilation took: 40.871079ms    \n2025-01-17T12:46:17.981914Z DEBUG segment::index::hnsw_index::gpu::gpu_graph_builder: Starting GPU level 0    \n2025-01-17T12:46:17.998076Z DEBUG segment::index::hnsw_index::gpu::gpu_insert_context: Gpu graph patches time: 9.925018ms, count 7, avg 1.417859ms    \n2025-01-17T12:46:17.998090Z DEBUG segment::index::hnsw_index::gpu::gpu_insert_context: Gpu graph update entries time: 0ns, count 0, avg 0ns    \n2025-01-17T12:46:18.000249Z DEBUG segment::index::hnsw_index::hnsw: building additional index for field l1    \n2025-01-17T12:46:18.000479Z ERROR qdrant::startup: Panic backtrace: \n   0: qdrant::startup::setup_panic_hook::{{closure}}\n   1: std::panicking::rust_panic_with_hook\n   2: std::panicking::begin_panic_handler::{{closure}}\n   3: std::sys::backtrace::__rust_end_short_backtrace\n   4: rust_begin_unwind\n   5: core::panicking::panic_fmt\n   6: alloc::vec::Vec<T,A>::remove::assert_failed\n   7: segment::index::hnsw_index::hnsw::HNSWIndex::build_index\n   8: segment::index::hnsw_index::hnsw::HNSWIndex::open\n   9: segment::segment_constructor::segment_constructor_base::create_vector_index\n  10: segment::segment_constructor::segment_builder::SegmentBuilder::build\n  11: collection::collection_manager::optimizers::segment_optimizer::SegmentOptimizer::build_new_segment\n  12: collection::collection_manager::optimizers::segment_optimizer::SegmentOptimizer::optimize\n  13: tokio::runtime::task::raw::poll\n  14: std::sys::backtrace::__rust_begin_short_backtrace\n  15: core::ops::function::FnOnce::call_once{{vtable.shim}}\n  16: std::sys::pal::unix::thread::Thread::new::thread_start\n  17: start_thread\n             at ./nptl/pthread_create.c:447:8\n  18: clone3\n             at ./misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78\n    \n2025-01-17T12:46:18.000492Z ERROR qdrant::startup: Panic occurred in file lib/segment/src/index/hnsw_index/gpu/batched_points.rs at line 48: removal index (is 0) should be < len (is 0)    \n2025-01-17T12:46:18.603939Z  WARN collection::update_handler: Optimization task panicked, collection may be in unstable state: removal index (is 0) should be < len (is 0)\n```\n\nqdrant version: 1.13.0\nOS: Ubuntu 24.04\nGPU: NVIDIA\n\n", "patch": "diff --git a/lib/gpu/src/context.rs b/lib/gpu/src/context.rs\nindex 0c32833a880..3784ad09b37 100644\n--- a/lib/gpu/src/context.rs\n+++ b/lib/gpu/src/context.rs\n@@ -169,6 +169,10 @@ impl Context {\n         dst_offset: usize,\n         size: usize,\n     ) -> GpuResult<()> {\n+        if size == 0 {\n+            return Ok(());\n+        }\n+\n         if self.vk_command_buffer == vk::CommandBuffer::null() {\n             self.init_command_buffer()?;\n         }\n@@ -201,6 +205,10 @@ impl Context {\n \n     /// Clear buffer with zeros command. It records command to run it on GPU after `run` call.\n     pub fn clear_buffer(&mut self, buffer: Arc<Buffer>) -> GpuResult<()> {\n+        if buffer.size() == 0 {\n+            return Ok(());\n+        }\n+\n         if buffer.size() % std::mem::size_of::<u32>() != 0 {\n             return Err(GpuError::OutOfBounds(\n                 \"Buffer size must be a multiple of `uint32` size to clear it\".to_string(),\ndiff --git a/lib/segment/src/index/hnsw_index/gpu/batched_points.rs b/lib/segment/src/index/hnsw_index/gpu/batched_points.rs\nindex 4c6e4b5924c..d90c07c89fa 100644\n--- a/lib/segment/src/index/hnsw_index/gpu/batched_points.rs\n+++ b/lib/segment/src/index/hnsw_index/gpu/batched_points.rs\n@@ -27,7 +27,7 @@ pub struct Batch<'a> {\n pub struct BatchedPoints {\n     points: Vec<PointLinkingData>,\n     batches: Vec<Range<usize>>,\n-    first_point_id: PointOffsetType,\n+    first_point_id: Option<PointOffsetType>,\n     levels_count: usize,\n     remap: Vec<PointOffsetType>,\n }\n@@ -45,7 +45,11 @@ impl BatchedPoints {\n             remap[*id as usize] = remapped_id as PointOffsetType;\n         }\n \n-        let first_point_id = ids.remove(0);\n+        let first_point_id = if !ids.is_empty() {\n+            Some(ids.remove(0))\n+        } else {\n+            None\n+        };\n \n         let batches = Self::build_initial_batches(&level_fn, &ids, groups_count);\n \n@@ -57,7 +61,7 @@ impl BatchedPoints {\n                 points.push(PointLinkingData {\n                     point_id,\n                     level,\n-                    entry: first_point_id.into(),\n+                    entry: first_point_id.unwrap_or_default().into(),\n                 });\n             }\n         }\n@@ -66,12 +70,14 @@ impl BatchedPoints {\n             points,\n             batches,\n             first_point_id,\n-            levels_count: level_fn(first_point_id) + 1,\n+            levels_count: first_point_id\n+                .map(|first_point_id| level_fn(first_point_id) + 1)\n+                .unwrap_or_default(),\n             remap,\n         })\n     }\n \n-    pub fn first_point_id(&self) -> PointOffsetType {\n+    pub fn first_point_id(&self) -> Option<PointOffsetType> {\n         self.first_point_id\n     }\n \ndiff --git a/lib/segment/src/index/hnsw_index/gpu/gpu_graph_builder.rs b/lib/segment/src/index/hnsw_index/gpu/gpu_graph_builder.rs\nindex a24703e6a37..1dc4101f451 100644\n--- a/lib/segment/src/index/hnsw_index/gpu/gpu_graph_builder.rs\n+++ b/lib/segment/src/index/hnsw_index/gpu/gpu_graph_builder.rs\n@@ -57,18 +57,6 @@ pub fn build_hnsw_on_gpu<'a>(\n         groups_count,\n     )?;\n \n-    // Create all GPU resources.\n-    let mut gpu_search_context = GpuInsertContext::new(\n-        gpu_vector_storage,\n-        groups_count,\n-        batched_points.remap(),\n-        m,\n-        m0,\n-        ef,\n-        exact,\n-        1..MAX_VISITED_FLAGS_FACTOR,\n-    )?;\n-\n     let graph_layers_builder =\n         create_graph_layers_builder(&batched_points, num_vectors, m, m0, ef, entry_points_num);\n \n@@ -90,9 +78,31 @@ pub fn build_hnsw_on_gpu<'a>(\n         }\n     }\n \n+    // Check if all points are linked on CPU.\n+    // If there are no batches left, we can return result before gpu resources creation.\n+    if batched_points\n+        .iter_batches(cpu_linked_points_count)\n+        .next()\n+        .is_none()\n+    {\n+        return Ok(graph_layers_builder);\n+    }\n+\n+    // Create all GPU resources.\n+    let mut gpu_search_context = GpuInsertContext::new(\n+        gpu_vector_storage,\n+        groups_count,\n+        batched_points.remap(),\n+        m,\n+        m0,\n+        ef,\n+        exact,\n+        1..MAX_VISITED_FLAGS_FACTOR,\n+    )?;\n+\n     // Build all levels on GPU level by level.\n     for level in (0..batched_points.levels_count()).rev() {\n-        log::debug!(\"Starting GPU level {}\", level,);\n+        log::trace!(\"Starting GPU level {level}\");\n \n         gpu_search_context.upload_links(level, &graph_layers_builder, stopped)?;\n         build_level_on_gpu(\n@@ -236,4 +246,26 @@ mod tests {\n \n         check_graph_layers_builders_quality(graph_layers_builder, test, top, ef, 0.8)\n     }\n+\n+    #[test]\n+    fn test_gpu_empty_hnsw() {\n+        let _ = env_logger::builder()\n+            .is_test(true)\n+            .filter_level(log::LevelFilter::Trace)\n+            .try_init();\n+\n+        let num_vectors = 0;\n+        let dim = 64;\n+        let m = 8;\n+        let m0 = 16;\n+        let ef = 32;\n+        let groups_count = 4;\n+        let searches_count = 20;\n+        let min_cpu_linked_points_count = 64;\n+\n+        let test = create_gpu_graph_test_data(num_vectors, dim, m, m0, ef, searches_count);\n+        let graph_layers_builder =\n+            build_gpu_graph(&test, groups_count, min_cpu_linked_points_count, false);\n+        assert!(graph_layers_builder.links_layers().is_empty());\n+    }\n }\ndiff --git a/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_multivectors.rs b/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_multivectors.rs\nindex 397b98ec934..7b323750b66 100644\n--- a/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_multivectors.rs\n+++ b/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_multivectors.rs\n@@ -129,7 +129,7 @@ impl GpuMultivectors {\n             device.clone(),\n             \"Multivector offsets buffer\",\n             gpu::BufferType::Storage,\n-            multivectors_count * std::mem::size_of::<GpuMultivectorOffset>(),\n+            std::cmp::max(multivectors_count, 1) * std::mem::size_of::<GpuMultivectorOffset>(),\n         )?;\n         let offsets_staging_buffer = gpu::Buffer::new(\n             device.clone(),\ndiff --git a/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_quantization.rs b/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_quantization.rs\nindex 0c8d7ec7e05..d205fa4474f 100644\n--- a/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_quantization.rs\n+++ b/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/gpu_quantization.rs\n@@ -230,7 +230,7 @@ impl GpuScalarQuantization {\n             device.clone(),\n             \"SQ offsets buffer\",\n             gpu::BufferType::Storage,\n-            quantized_storage.vectors_count() * std::mem::size_of::<f32>(),\n+            std::cmp::max(quantized_storage.vectors_count(), 1) * std::mem::size_of::<f32>(),\n         )?;\n \n         let sq_offsets_staging_buffer = gpu::Buffer::new(\n@@ -349,7 +349,9 @@ impl GpuProductQuantization {\n             device.clone(),\n             \"PQ vector division buffer\",\n             gpu::BufferType::Storage,\n-            quantized_storage.get_metadata().vector_division.len() * std::mem::size_of::<u32>() * 2,\n+            std::cmp::max(quantized_storage.get_metadata().vector_division.len(), 1)\n+                * std::mem::size_of::<u32>()\n+                * 2,\n         )?;\n         let vector_division_staging_buffer = gpu::Buffer::new(\n             device.clone(),\ndiff --git a/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/mod.rs b/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/mod.rs\nindex 288aa7eb6f1..997ee66a526 100644\n--- a/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/mod.rs\n+++ b/lib/segment/src/index/hnsw_index/gpu/gpu_vector_storage/mod.rs\n@@ -137,13 +137,6 @@ impl GpuVectorStorage {\n         force_half_precision: bool,\n         stopped: &AtomicBool,\n     ) -> OperationResult<Self> {\n-        // GPU buffers should not be empty.\n-        // Check that we have enough vectors to store at least one vector in each buffer.\n-        if vector_storage.total_vector_count() < STORAGES_COUNT {\n-            return Err(OperationError::service_error(\n-                \"Vectors count is less than `STORAGES_COUNT`\",\n-            ));\n-        }\n         if let Some(quantized_storage) = quantized_storage {\n             Self::new_quantized(\n                 device,\n@@ -557,7 +550,7 @@ impl GpuVectorStorage {\n                     device.clone(),\n                     \"Vector storage buffer\",\n                     gpu::BufferType::Storage,\n-                    points_in_storage_count * gpu_vector_size,\n+                    std::cmp::max(points_in_storage_count, 1) * gpu_vector_size,\n                 )\n             })\n             .collect::<gpu::GpuResult<Vec<_>>>()?;\n@@ -576,7 +569,7 @@ impl GpuVectorStorage {\n             device.clone(),\n             \"Vector storage upload staging buffer\",\n             gpu::BufferType::CpuToGpu,\n-            upload_points_count * gpu_vector_size,\n+            std::cmp::max(upload_points_count, 1) * gpu_vector_size,\n         )?;\n         // fill staging buffer with zeros\n         let zero_vector = vec![TElement::default(); gpu_vector_capacity];\ndiff --git a/lib/segment/src/index/hnsw_index/gpu/mod.rs b/lib/segment/src/index/hnsw_index/gpu/mod.rs\nindex 45e999735a7..22dc942fa0f 100644\n--- a/lib/segment/src/index/hnsw_index/gpu/mod.rs\n+++ b/lib/segment/src/index/hnsw_index/gpu/mod.rs\n@@ -71,25 +71,20 @@ fn create_graph_layers_builder(\n     // mark all vectors as ready\n     graph_layers_builder.clear_ready_list();\n \n-    // set first entry point\n-    graph_layers_builder.set_levels(\n-        batched_points.first_point_id(),\n-        batched_points.levels_count() - 1,\n-    );\n-    graph_layers_builder.get_entry_points().new_point(\n-        batched_points.first_point_id(),\n-        batched_points.levels_count() - 1,\n-        |_| true,\n-    );\n-\n-    // set levels\n-    graph_layers_builder.set_levels(\n-        batched_points.first_point_id(),\n-        batched_points.levels_count() - 1,\n-    );\n-    for batch in batched_points.iter_batches(0) {\n-        for linking_point in batch.points {\n-            graph_layers_builder.set_levels(linking_point.point_id, batch.level);\n+    if let Some(first_point_id) = batched_points.first_point_id() {\n+        // set first entry point\n+        graph_layers_builder.get_entry_points().new_point(\n+            first_point_id,\n+            batched_points.levels_count() - 1,\n+            |_| true,\n+        );\n+\n+        // set levels\n+        graph_layers_builder.set_levels(first_point_id, batched_points.levels_count() - 1);\n+        for batch in batched_points.iter_batches(0) {\n+            for linking_point in batch.points {\n+                graph_layers_builder.set_levels(linking_point.point_id, batch.level);\n+            }\n         }\n     }\n \n", "instance_id": "qdrant__qdrant-5847", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: GPU indexing in Qdrant panics when used with payload field indexes. It provides detailed steps to reproduce the issue, configuration details, and logs that pinpoint the error (\"removal index (is 0) should be < len (is 0)\"). This gives a good starting point for understanding the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior (e.g., how GPU indexing should handle payload indexes), nor does it specify constraints or edge cases beyond the provided configuration. Additionally, while the logs and configuration are helpful, there is no mention of specific performance or compatibility requirements for the GPU or payload data. Overall, the statement is valid and mostly clear but lacks some finer details about expected outcomes and edge scenarios.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes spans multiple files and modules within the GPU indexing and vector storage components of Qdrant, indicating a need to understand interactions between different parts of the codebase (e.g., `batched_points.rs`, `gpu_graph_builder.rs`, `gpu_vector_storage`). The changes are not trivial; they involve handling edge cases like empty buffers or zero-sized data structures, which are critical to prevent panics as seen in the logs. Second, the technical concepts required are advanced, including GPU programming (likely Vulkan, given the context), HNSW (Hierarchical Navigable Small World) graph indexing, and quantization techniques for vector storage. These are domain-specific and require a deep understanding of both the algorithms and the GPU implementation details. Third, the error handling and edge cases addressed in the code changes (e.g., checking for zero-sized buffers, handling empty point lists) are non-trivial, as they directly impact the stability of the system during indexing operations. Finally, while the changes do not appear to alter the core architecture, they touch critical low-level components that could affect performance and correctness if not handled properly. The problem does not reach the \"Very Hard\" category (0.8-1.0) because it does not involve system-level redesign or extremely intricate distributed systems logic, but it still demands significant expertise in GPU computing and indexing algorithms, justifying a score of 0.75.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug: stack overflow on closures with outer params\n# Bug Report\n\n**Cairo version:**\n```\n\u276f scarb --version\nscarb 2.9.2+nightly-2025-02-01 (3a752f93d 2025-02-01)\ncairo: 2.9.2 (99f052811)\nsierra: 1.6.0\n```\nThe following code:\n```\nfn main() {\n    let outer_var = 42;\n    let closure_annotated = |i: u32| -> u32 { i + outer_var };\n    println!(\"closure_annotated: {}\", closure_annotated(1));\n}\n```\n\nfails `scarb cairo-run` with:\n\n```\nthread 'scarb compile closures-665rku3l74rb0' has overflowed its stack\nfatal runtime error: stack overflow\nerror: `scarb metadata` exited with error\n```\n", "patch": "diff --git a/crates/cairo-lang-semantic/src/types.rs b/crates/cairo-lang-semantic/src/types.rs\nindex 392695c053d..71faba8790d 100644\n--- a/crates/cairo-lang-semantic/src/types.rs\n+++ b/crates/cairo-lang-semantic/src/types.rs\n@@ -10,7 +10,7 @@ use cairo_lang_syntax::attribute::consts::MUST_USE_ATTR;\n use cairo_lang_syntax::node::ids::SyntaxStablePtrId;\n use cairo_lang_syntax::node::{TypedStablePtr, TypedSyntaxNode, ast};\n use cairo_lang_utils::{Intern, LookupIntern, OptionFrom, define_short_id, try_extract_matches};\n-use itertools::Itertools;\n+use itertools::{Itertools, chain};\n use num_bigint::BigInt;\n use num_traits::Zero;\n use sha3::{Digest, Keccak256};\n@@ -946,7 +946,7 @@ pub fn priv_type_is_var_free(db: &dyn SemanticGroup, ty: TypeId) -> bool {\n         // a var free ImplType needs to be rewritten if has impl bounds constraints.\n         TypeLongId::ImplType(_) => false,\n         TypeLongId::Closure(closure) => {\n-            closure.param_tys.iter().all(|param| param.is_var_free(db))\n+            chain!(&closure.captured_types, &closure.param_tys).all(|param| param.is_var_free(db))\n                 && closure.ret_ty.is_var_free(db)\n         }\n     }\n", "instance_id": "starkware-libs__cairo-7255", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a stack overflow occurs when running a specific piece of code involving closures with outer parameters in the Cairo language. It provides the exact code snippet that triggers the bug, the version of the software, and the error message output. However, there are minor ambiguities and missing details. For instance, it does not explicitly state the expected behavior or the root cause of the stack overflow (e.g., whether it's due to infinite recursion in type checking or another issue). Additionally, there are no mentions of specific edge cases or constraints beyond the provided example. While the issue is reproducible and the goal (fixing the stack overflow) is implied, the lack of deeper context or detailed requirements prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code change is relatively small, confined to a single file (`types.rs`) and a specific function (`priv_type_is_var_free`). The modification itself is minimal, involving the addition of `closure.captured_types` to the check for variable-free types using the `chain!` macro from the `itertools` crate. However, the conceptual depth required to understand and fix this issue is significant. Solving this requires a deep understanding of the Cairo language's type system, particularly how closures and captured variables are handled during semantic analysis. It also demands familiarity with the internal representation of types (`TypeLongId::Closure`) and the logic behind variable-free type checking, which is a non-trivial concept. Additionally, the bug's impact (stack overflow) suggests a potentially recursive or cyclic issue in the type system, which could be challenging to debug without extensive knowledge of the codebase's architecture. While the code change does not directly address complex edge cases or error handling, identifying the root cause likely involved considering various scenarios for closure type handling. Overall, this problem requires a strong grasp of language internals and type theory, pushing it into the harder spectrum, though not at the extreme end due to the localized nature of the fix.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`--get-repos-no-status` typo in help.rs\nhttps://github.com/eza-community/eza/blob/ab57f463440f1441691616cbf6761f66b7949591/src/options/help.rs#L95-L99\r\n\r\nShouldn't it be `--git-repos-no-status`?\n", "patch": "diff --git a/src/options/help.rs b/src/options/help.rs\nindex 4bcea8695..f23a2dd45 100644\n--- a/src/options/help.rs\n+++ b/src/options/help.rs\n@@ -96,7 +96,7 @@ static GIT_VIEW_HELP: &str = \"  \\\n   --no-git                   suppress Git status (always overrides --git,\n                              --git-repos, --git-repos-no-status)\n   --git-repos                list root of git-tree status\n-  --get-repos-no-status      list each git-repos branch name (much faster)\n+  --git-repos-no-status      list each git-repos branch name (much faster)\n     \";\n static EXTENDED_HELP: &str = \"  \\\n   -@, --extended             list each file's extended attributes and sizes\";\n", "instance_id": "eza-community__eza-1212", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a potential typo in the help text of a command-line option within the `help.rs` file. The goal is straightforward\u2014correcting the typo from `--get-repos-no-status` to `--git-repos-no-status`. The statement includes a direct link to the relevant lines of code in the GitHub repository, which aids in understanding the context. However, it lacks explicit confirmation of whether this is indeed a typo or if there is an underlying reason for the current naming (e.g., intentional design or compatibility). Additionally, there are no examples or further context about the expected behavior of the flag or its usage, which could be helpful for someone unfamiliar with the project. Thus, while the problem is valid and mostly clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a simple textual correction in a single line of a help string within one file (`help.rs`). The scope of the code change is minimal, requiring no understanding of the broader codebase, interactions between modules, or architectural impact. The technical concepts involved are negligible\u2014basic text editing with no need for knowledge of specific language features, libraries, algorithms, or domain-specific logic beyond familiarity with source code files. There are no edge cases or error handling requirements mentioned or implied, as this is purely a documentation fix within the help text. The effort is trivial, akin to fixing a typo, and thus I assign a difficulty score of 0.1, placing it in the \"Very Easy\" range (0.0-0.2).", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Unintended input when text editor gains focus\n### Is your issue REALLY a bug?\n\n- [X] My issue is indeed a bug!\n- [X] I am not crazy! I will not fill out this form just to ask a question or request a feature. Pinky promise.\n\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\nThis is a bit of niche one. I use the Awesome tiling window manager on Linux. I have `Super+j` and `Super+k` bound to cycle back/forward through windows. When the focus reaches an Iced application with a text input a j or k is typed in the message box. This doesn't happen with any other applications on my system. I first observed the behaviour in the [Halloy IRC client](https://github.com/squidowl/halloy) but it happens with the `editor` example too.\r\n\r\n### Steps to Reproduce\r\n\r\n- Run Awesome WM\r\n- Cycle through applications with `Super+j` until the Iced editor example is reached\r\n- Observe a `j` or `k` is typed.\r\n\r\nVideo of behaviour:\r\n\r\n\r\nhttps://github.com/user-attachments/assets/a6e6b32e-0a2d-4ccb-a6cc-08afe7930d52\r\n\r\n\r\n\r\n### Environment\r\n\r\nOS: Arch Linux, kernel 6.9.7\r\nWM: Awesome\r\nDisplay Server: X11\r\nGPU: AMD Radeon RX 6700 XT\n\n### What is the expected behavior?\n\nExpected behaviour is that when an iced application is focused via a keyboard shortcut no text is typed.\n\n### Version\n\nmaster\n\n### Operating System\n\nLinux\n\n### Do you have any log output?\n\n_No response_\n", "patch": "diff --git a/winit/src/conversion.rs b/winit/src/conversion.rs\nindex 5d0f834817..8e6f7aaef2 100644\n--- a/winit/src/conversion.rs\n+++ b/winit/src/conversion.rs\n@@ -191,6 +191,8 @@ pub fn window_event(\n                 }))\n             }\n         },\n+        // Ignore keyboard presses/releases during window focus/unfocus\n+        WindowEvent::KeyboardInput { is_synthetic, .. } if is_synthetic => None,\n         WindowEvent::KeyboardInput { event, .. } => Some(Event::Keyboard({\n             let key = {\n                 #[cfg(not(target_arch = \"wasm32\"))]\n", "instance_id": "iced-rs__iced-2649", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: unintended text input occurs in an Iced application when it gains focus via keyboard shortcuts in the Awesome tiling window manager on Linux. The goal (preventing unintended input on focus) and the context (specific to Iced applications under certain conditions) are well-defined. Steps to reproduce are provided, along with a video for visual reference, which adds clarity. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly discuss potential edge cases (e.g., behavior with other window managers or input methods) or constraints (e.g., whether the fix should maintain compatibility with other platforms). Additionally, while the expected behavior is stated, there is no discussion of potential trade-offs or side effects of ignoring input during focus events. Overall, the statement is valid and clear but lacks comprehensive coverage of edge cases and broader implications.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). The code change provided is minimal, involving a single line addition in the `conversion.rs` file to ignore synthetic keyboard input events during window focus/unfocus. This suggests that the solution requires only a basic understanding of the `winit` library's event handling mechanism and the specific `WindowEvent::KeyboardInput` variant. The scope of the change is limited to a single file and does not appear to impact the broader architecture of the codebase or require deep knowledge of interactions between modules. The technical concepts involved are straightforward: familiarity with Rust syntax, event handling in GUI frameworks, and the concept of synthetic events. There are no complex algorithms, design patterns, or domain-specific knowledge required. Regarding edge cases, the problem statement does not explicitly mention any beyond the specific scenario described, and the code change does not introduce significant error handling complexity. However, a developer might need to consider whether ignoring synthetic events could affect other legitimate use cases, though this is a minor concern not explicitly raised in the problem. Overall, this is a simple bug fix with limited scope and complexity.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Does MSRV apply to clap_complete's unstable-dynamic feature?\n### Please complete the following tasks\n\n- [X] I have searched the [discussions](https://github.com/clap-rs/clap/discussions)\n- [X] I have searched the [open](https://github.com/clap-rs/clap/issues) and [rejected](https://github.com/clap-rs/clap/issues?q=is%3Aissue+label%3AS-wont-fix+is%3Aclosed) issues\n\n### Rust Version\n\n1.74.0\n\n### Clap Version\n\nmaster\n\n### Minimal reproducible code\n\nN/A\n\n### Steps to reproduce the bug with the above code\n\nin the clap repo:\r\n```sh\r\nRUSTUP_TOOLCHAIN=1.74.0 cargo check --workspace --all-features\r\n```\n\n### Actual Behaviour\n\n```\r\nerror[E0599]: no method named `split_at_checked` found for reference `&str` in the current scope\r\n   --> clap_complete/src/engine/complete.rs:395:10\r\n    |\r\n394 |       let (prefix, value) = value\r\n    |  ___________________________-\r\n395 | |         .split_at_checked(pos + delimiter.len_utf8())\r\n    | |         -^^^^^^^^^^^^^^^^ help: there is a method with a similar name: `split_at`\r\n    | |_________|\r\n    |\r\n```\n\n### Expected Behaviour\n\nShould clap_complete compile with the MSRV and `unstable-dynamic` turned on? Or is the unstable feature exempt from the MSRV policy?\n\n### Additional Context\n\nThe MSRV policy is 2 minor versions, behind, so 1.80 (which is required for `split_at_checked`) is within that policy. So it would be possible to bump the MSRV to fix it, if desired.\r\n\r\nIf `unstable-dynamic` should be covered by the MSRV policy, I think this could be checked in CI by extending this line in the Makefile:\r\n```\r\n_FEATURES_full = --features \"deprecated derive cargo env unicode string wrap_help unstable-ext unstable-dynamic\"\r\n```\r\n\r\nMSRV violations are also caught by clippy since 1.78:\r\n\r\n```sh\r\ncargo clippy --workspace --all-features\r\n```\r\n\r\n```\r\nwarning: current MSRV (Minimum Supported Rust Version) is `1.74.0` but this item is stable since `1.80.0`\r\n   --> clap_complete/src/engine/complete.rs:395:10\r\n    |\r\n395 |         .split_at_checked(pos + delimiter.len_utf8())\r\n    |          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |\r\n    = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#incompatible_msrv\r\n    = note: `#[warn(clippy::incompatible_msrv)]` on by default\r\n```\r\n\r\n(plus two unrelated clippy warnings in the `unstable-dynamic` code)\r\n\r\nExentding the Makefile as suggested would also surface these three clippy warnings as errors.\n\n### Debug Output\n\n_No response_\n", "patch": "diff --git a/clap_complete/src/engine/complete.rs b/clap_complete/src/engine/complete.rs\nindex c5ff943dbc5..9907749970d 100644\n--- a/clap_complete/src/engine/complete.rs\n+++ b/clap_complete/src/engine/complete.rs\n@@ -391,9 +391,7 @@ fn rsplit_delimiter<'s, 'o>(\n     let delimiter = delimiter?;\n     let value = value.ok()?;\n     let pos = value.rfind(delimiter)?;\n-    let (prefix, value) = value\n-        .split_at_checked(pos + delimiter.len_utf8())\n-        .expect(\"since delimiter was found, it is within bounds\");\n+    let (prefix, value) = value.split_at(pos + delimiter.len_utf8());\n     Some((Some(prefix), Ok(value)))\n }\n \ndiff --git a/clap_complete/src/engine/custom.rs b/clap_complete/src/engine/custom.rs\nindex db0eb2a17c9..a81f0b02848 100644\n--- a/clap_complete/src/engine/custom.rs\n+++ b/clap_complete/src/engine/custom.rs\n@@ -213,6 +213,7 @@ where\n /// ```\n pub struct PathCompleter {\n     current_dir: Option<std::path::PathBuf>,\n+    #[allow(clippy::type_complexity)]\n     filter: Option<Box<dyn Fn(&std::path::Path) -> bool + Send + Sync>>,\n     stdio: bool,\n }\ndiff --git a/clap_complete/src/env/mod.rs b/clap_complete/src/env/mod.rs\nindex ea33d8d68a3..2c5049bdb87 100644\n--- a/clap_complete/src/env/mod.rs\n+++ b/clap_complete/src/env/mod.rs\n@@ -250,15 +250,16 @@ impl<'s, F: Fn() -> clap::Command> CompleteEnv<'s, F> {\n         let name = name.to_string_lossy();\n \n         let shell = self.shells.completer(&name).ok_or_else(|| {\n-            let shells = self\n-                .shells\n-                .names()\n-                .enumerate()\n-                .map(|(i, name)| {\n-                    let prefix = if i == 0 { \"\" } else { \", \" };\n-                    format!(\"{prefix}`{name}`\")\n-                })\n-                .collect::<String>();\n+            let shells =\n+                self.shells\n+                    .names()\n+                    .enumerate()\n+                    .fold(String::new(), |mut seed, (i, name)| {\n+                        use std::fmt::Write as _;\n+                        let prefix = if i == 0 { \"\" } else { \", \" };\n+                        let _ = write!(&mut seed, \"{prefix}`{name}`\");\n+                        seed\n+                    });\n             std::io::Error::new(\n                 std::io::ErrorKind::Other,\n                 format!(\"unknown shell `{name}`, expected one of {shells}\"),\n", "instance_id": "clap-rs__clap-5805", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a compilation error in the `clap_complete` crate when using the `unstable-dynamic` feature with a specific Rust version (MSRV 1.74.0). The goal is to determine whether the `unstable-dynamic` feature should adhere to the MSRV policy or be exempt, and the error is well-documented with a specific code snippet and error message. Additionally, the context about the MSRV policy and potential CI checks is helpful. However, there are minor ambiguities: the problem does not explicitly state the desired resolution (e.g., should the code be modified to support MSRV, or should the MSRV be bumped?). It also lacks clarity on whether there are specific constraints or expectations for handling this feature in relation to the MSRV policy. Edge cases or broader implications of changing the MSRV or modifying the code are not discussed. Overall, while the issue is valid and mostly clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are minimal and localized to three files (`complete.rs`, `custom.rs`, and `env/mod.rs`). The primary fix in `complete.rs` involves replacing `split_at_checked` with `split_at`, a straightforward API substitution to address an MSRV compatibility issue. The other changes in `custom.rs` and `env/mod.rs` are minor, addressing unrelated Clippy warnings or formatting improvements (e.g., using `fold` instead of `map` for string building). These changes do not impact the broader architecture of the system and are limited to specific functions.\n\n2. **Number of Technical Concepts**: The problem requires basic knowledge of Rust's string handling APIs (`split_at` vs. `split_at_checked`) and an understanding of MSRV (Minimum Supported Rust Version) policies. It also involves familiarity with Rust's toolchain (e.g., `cargo check`) and potentially CI configurations (Makefile updates). These concepts are not particularly complex for a developer with moderate Rust experience, as they involve standard library usage and version compatibility rather than advanced algorithms or design patterns.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code change in `complete.rs` removes a safety check (`split_at_checked`) in favor of `split_at`, which could theoretically introduce risks if the index is out of bounds. However, the original code includes an `expect` assertion that the delimiter position is valid, mitigating this concern. No additional error handling logic is required beyond this change, and the scope of edge cases appears minimal.\n\n4. **Overall Complexity**: While the problem involves a policy decision (whether `unstable-dynamic` should adhere to MSRV), the technical implementation is simple. The hardest part might be understanding the broader context of MSRV policies and deciding on a long-term solution (e.g., bumping MSRV or adding CI checks), but the provided code changes themselves are trivial. This keeps the difficulty low, as it primarily requires basic code modification and minimal cross-module understanding.\n\nIn summary, this problem is easy to solve with the provided diff, requiring only surface-level changes and basic Rust knowledge. The score of 0.25 reflects the simplicity of the fix while acknowledging the minor context of MSRV policy and CI integration that a developer might need to consider.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Open above and below are broken in multi-range selections\n### Summary\r\n\r\n`open_above` and `open_below` are both broken on the secondary ranges of a multi-range selection. The secondary cursor ends up on a line above where it should be for both commands. e.g.,\r\n\r\n```toml\r\n[workspace]\r\nresolver = \"2\"\r\nmembers = [\r\n  \"#[|h]#elix-core\",\r\n  \"#(|h)#elix-view\",\r\n  \"#(|h)#elix-term\",\r\n  \"helix-tui\",\r\n  \"helix-lsp-types\",\r\n  \"helix-lsp\",\r\n  \"helix-event\",\r\n  \"helix-dap\",\r\n  \"helix-loader\",\r\n  \"helix-vcs\",\r\n  \"helix-parsec\",\r\n  \"helix-stdx\",\r\n  \"xtask\",\r\n]\r\n```\r\n\r\nPressing `o` becomes:\r\n\r\n```toml\r\n[workspace]\r\nresolver = \"2\"\r\nmembers = [\r\n  \"helix-core\",\r\n  #[|]#\r\n  \"helix-view\",#(|)#\r\n\r\n  \"helix-ter#(|m)#\",\r\n\r\n  \"helix-tui\",\r\n  \"helix-lsp-types\",\r\n  \"helix-lsp\",\r\n  \"helix-event\",\r\n  \"helix-dap\",\r\n  \"helix-loader\",\r\n  \"helix-vcs\",\r\n  \"helix-parsec\",\r\n  \"helix-stdx\",\r\n  \"xtask\",\r\n]\r\n```\r\n\r\nand pressing `O` becomes:\r\n\r\n```toml\r\n[workspace]\r\nresolver = \"2\"\r\nmembers = [\r\n  #[|]#\r\n  \"helix-core\",#(|)#\r\n\r\n  \"helix-vie#(|w)#\",\r\n\r\n  \"helix-term\",\r\n  \"helix-tui\",\r\n  \"helix-lsp-types\",\r\n  \"helix-lsp\",\r\n  \"helix-event\",\r\n  \"helix-dap\",\r\n  \"helix-loader\",\r\n  \"helix-vcs\",\r\n  \"helix-parsec\",\r\n  \"helix-stdx\",\r\n  \"xtask\",\r\n]\r\n```\r\n\r\nFrom a bisect, it looks like the breakage happened in https://github.com/helix-editor/helix/pull/12215\n", "patch": "diff --git a/helix-term/src/commands.rs b/helix-term/src/commands.rs\nindex 3b90648742dd..755a7dc077bb 100644\n--- a/helix-term/src/commands.rs\n+++ b/helix-term/src/commands.rs\n@@ -3474,6 +3474,7 @@ fn open(cx: &mut Context, open: Open) {\n     let text = doc.text().slice(..);\n     let contents = doc.text();\n     let selection = doc.selection(view.id);\n+    let mut offs = 0;\n \n     let mut ranges = SmallVec::with_capacity(selection.len());\n \n@@ -3550,7 +3551,7 @@ fn open(cx: &mut Context, open: Open) {\n         let text = text.repeat(count);\n \n         // calculate new selection ranges\n-        let pos = above_next_line_end_index + above_next_line_end_width;\n+        let pos = offs + above_next_line_end_index + above_next_line_end_width;\n         let comment_len = continue_comment_token\n             .map(|token| token.len() + 1) // `+ 1` for the extra space added\n             .unwrap_or_default();\n@@ -3563,6 +3564,9 @@ fn open(cx: &mut Context, open: Open) {\n             ));\n         }\n \n+        // update the offset for the next range\n+        offs += text.chars().count();\n+\n         (\n             above_next_line_end_index,\n             above_next_line_end_index,\n", "instance_id": "helix-editor__helix-12465", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `open_above` and `open_below` commands in the context of multi-range selections in the Helix editor. It provides a specific example using a TOML file to illustrate the incorrect behavior of cursor positioning after executing the commands `o` and `O`. The summary effectively communicates the goal of fixing the cursor placement issue in secondary ranges. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected correct behavior for cursor positioning beyond the examples, nor does it mention specific constraints or edge cases (e.g., behavior with empty lines, different file formats, or varying selection sizes). Additionally, while it references a specific pull request as the source of the breakage, it lacks deeper context about the codebase or the exact nature of the regression introduced. Overall, the statement is valid and clear but misses some minor details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code changes is relatively small and localized to a single file (`helix-term/src/commands.rs`) and a specific function (`open`). The diff provided shows a minimal change involving the addition of an offset variable (`offs`) to adjust cursor positioning for multi-range selections, indicating a straightforward bug fix rather than a complex feature addition or architectural change. Second, the technical concepts required to solve this are not overly advanced; they involve understanding text manipulation, cursor positioning logic, and iteration over selection ranges in the context of a text editor, which are moderately complex but well within the grasp of a developer familiar with Rust and text editor internals. Third, while the problem statement does not explicitly mention edge cases, the nature of multi-range selections implies potential challenges like handling varying line lengths, empty selections, or non-uniform text structures, but the provided fix does not address these, suggesting they may not be critical to the core issue. Finally, the impact on the codebase appears minimal, with no evidence of architectural changes or cross-module dependencies. Overall, this problem requires understanding some code logic and making a simple modification, aligning with a difficulty score of 0.35.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Docs: The usearch.h exact_search documentation doesn't match the function arguments\n### Describe the bug\n\nThe usearch.h exact_search documentation doesn't match the function arguments\r\n\r\n```\r\n/**\r\n *  @brief Multi-threaded exact nearest neighbors search for equi-dimensional vectors.\r\n *  @param[in] dataset Pointer to the first scalar of the dataset matrix.\r\n *  @param[in] queries Pointer to the first scalar of the queries matrix.\r\n *  @param[in] dataset_size Number of vectors in the `dataset`.\r\n *  @param[in] queries_size Number of vectors in the `queries` set.\r\n *  @param[in] dataset_stride Number of bytes between starts of consecutive vectors in `dataset`.\r\n *  @param[in] queries_stride Number of bytes between starts of consecutive vectors in `queries`.\r\n *  @param[in] scalar_kind The scalar type used in the vectors.\r\n *  @param[in] dimensions The number of dimensions in each vector.\r\n *  @param[in] metric_kind The metric kind used for distance calculation between vectors.\r\n *  @param[in] count Upper bound on the number of neighbors to search, the \"k\" in \"kANN\".\r\n *  @param[in] threads Upper bound for the number of CPU threads to use.\r\n *  @param[out] keys Output buffer for up to `count` nearest neighbors keys.\r\n *  @param[out] distances Output buffer for up to `count` distances to nearest neighbors.\r\n *  @param[out] error Pointer to a string where the error message will be stored, if an error occurs.\r\n *  @return Number of found matches.\r\n */\r\nUSEARCH_EXPORT void usearch_exact_search(                            //\r\n    void const* dataset, size_t dataset_size, size_t dataset_stride, //\r\n    void const* queries, size_t queries_size, size_t queries_stride, //\r\n    usearch_scalar_kind_t scalar_kind, size_t dimensions,            //\r\n    usearch_metric_kind_t metric_kind, size_t count, size_t threads, //\r\n    usearch_key_t* keys, size_t keys_stride,                         //\r\n    usearch_distance_t* distances, size_t distances_stride,          //\r\n    usearch_error_t* error);\r\n```\n\n### Steps to reproduce\n\n-\n\n### Expected behavior\n\n-\n\n### USearch version\n\nmain-dev\n\n### Operating System\n\nUbuntu 24\n\n### Hardware architecture\n\nx86\n\n### Which interface are you using?\n\nC++ implementation\n\n### Contact Details\n\n_No response_\n\n### Are you open to being tagged as a contributor?\n\n- [X] I am open to being mentioned in the project `.git` history as a contributor\n\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct\n", "patch": "diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml\nindex 144eb476..c128da8a 100644\n--- a/.github/workflows/release.yml\n+++ b/.github/workflows/release.yml\n@@ -26,11 +26,6 @@ jobs:\n         with:\n           fetch-depth: 0\n           persist-credentials: false\n-      - name: Set up Cargo\n-        uses: actions-rs/toolchain@v1\n-        with:\n-          toolchain: stable\n-          override: true\n       - name: Run TinySemVer\n         uses: ashvardanian/tinysemver@v2.0.1\n         with:\n@@ -40,6 +35,7 @@ jobs:\n             package.json:\"version\": \"(\\d+\\.\\d+\\.\\d+)\"\n             CITATION.cff:^version: (\\d+\\.\\d+\\.\\d+)\n             Cargo.toml:^version = \"(\\d+\\.\\d+\\.\\d+)\"\n+            Cargo.lock:^version = \"(\\d+\\.\\d+\\.\\d+)\"\n             wasmer.toml:^version = \"(\\d+\\.\\d+\\.\\d+)\"\n             conanfile.py:version = \"(\\d+\\.\\d+\\.\\d+)\"\n             java/README.md:<version>(\\d+\\.\\d+\\.\\d+)</version>\ndiff --git a/Cargo.lock b/Cargo.lock\nindex 36d1079d..74494b1c 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -135,7 +135,7 @@ checksum = \"68f5e5f3158ecfd4b8ff6fe086db7c8467a2dfdac97fe420f2b7c4aa97af66d6\"\n \n [[package]]\n name = \"usearch\"\n-version = \"2.13.0\"\n+version = \"2.13.1\"\n dependencies = [\n  \"cxx\",\n  \"cxx-build\",\ndiff --git a/README.md b/README.md\nindex f54cd780..2dcc11ce 100644\n--- a/README.md\n+++ b/README.md\n@@ -400,7 +400,7 @@ def search(query: str) -> np.ndarray:\n server.run()\n ```\n \n-Similar experiences can also be imlemented in other languages and on the client side, removing the network latency.\n+Similar experiences can also be implemented in other languages and on the client side, removing the network latency.\n For Swift and iOS, check out the [`ashvardanian/SwiftSemanticSearch`](https://github.com/ashvardanian/SwiftSemanticSearch) repository.\n \n <table>\ndiff --git a/c/usearch.h b/c/usearch.h\nindex 0acce530..743d49cc 100644\n--- a/c/usearch.h\n+++ b/c/usearch.h\n@@ -434,7 +434,7 @@ USEARCH_EXPORT usearch_distance_t usearch_distance(       //\n     usearch_metric_kind_t metric_kind, usearch_error_t* error);\n \n /**\n- *  @brief Multi-threaded exact nearest neighbors search for equi-dimensional vectors.\n+ *  @brief Multi-threaded many-to-many exact nearest neighbors search for equi-dimensional vectors.\n  *  @param[in] dataset Pointer to the first scalar of the dataset matrix.\n  *  @param[in] queries Pointer to the first scalar of the queries matrix.\n  *  @param[in] dataset_size Number of vectors in the `dataset`.\n@@ -446,10 +446,13 @@ USEARCH_EXPORT usearch_distance_t usearch_distance(       //\n  *  @param[in] metric_kind The metric kind used for distance calculation between vectors.\n  *  @param[in] count Upper bound on the number of neighbors to search, the \"k\" in \"kANN\".\n  *  @param[in] threads Upper bound for the number of CPU threads to use.\n- *  @param[out] keys Output buffer for up to `count` nearest neighbors keys.\n- *  @param[out] distances Output buffer for up to `count` distances to nearest neighbors.\n+ *  @param[out] keys Output matrix for `queries_size * count` nearest neighbors keys. Each row of the\n+ *              matrix must be contiguous in memory, but different rows can be separated by `keys_stride` bytes.\n+ *  @param[in] keys_stride Number of bytes between starts of consecutive rows od scalars in `keys`.\n+ *  @param[out] distances Output matrix for `queries_size * count` distances to nearest neighbors. Each row of the\n+ *              matrix must be contiguous in memory, but different rows can be separated by `keys_stride` bytes.\n+ *  @param[in] distances_stride Number of bytes between starts of consecutive rows od scalars in `distances`.\n  *  @param[out] error Pointer to a string where the error message will be stored, if an error occurs.\n- *  @return Number of found matches.\n  */\n USEARCH_EXPORT void usearch_exact_search(                            //\n     void const* dataset, size_t dataset_size, size_t dataset_stride, //\ndiff --git a/java/cloud/unum/usearch/Index.java b/java/cloud/unum/usearch/Index.java\nindex c6de074f..173c1762 100644\n--- a/java/cloud/unum/usearch/Index.java\n+++ b/java/cloud/unum/usearch/Index.java\n@@ -176,7 +176,7 @@ public int[] search(float vector[], long count) {\n    *\n    * @param key key to lookup.\n    * @return the contents of the vector.\n-   * @throws linkIllegalArgumentException} is key is not available.\n+   * @throws IllegalArgumentException is key is not available.\n    */\n   public float[] get(int key) {\n     if (c_ptr == 0) {\ndiff --git a/java/cloud/unum/usearch/cloud_unum_usearch_Index.cpp b/java/cloud/unum/usearch/cloud_unum_usearch_Index.cpp\nindex 995b7343..cf96521d 100644\n--- a/java/cloud/unum/usearch/cloud_unum_usearch_Index.cpp\n+++ b/java/cloud/unum/usearch/cloud_unum_usearch_Index.cpp\n@@ -60,7 +60,8 @@ JNIEXPORT jlong JNICALL Java_cloud_unum_usearch_Index_c_1create( //\n     return result;\n }\n \n-JNIEXPORT jlong JNICALL Java_cloud_unum_usearch_Index_c_1createFromFile(JNIEnv *env, jclass, jstring path, jboolean view) {\n+JNIEXPORT jlong JNICALL Java_cloud_unum_usearch_Index_c_1createFromFile(JNIEnv* env, jclass, jstring path,\n+                                                                        jboolean view) {\n     char const* path_cstr = env->GetStringUTFChars(path, 0);\n     index_dense_t::state_result_t make_result = index_dense_t::make(path_cstr, view);\n     env->ReleaseStringUTFChars(path, path_cstr);\n@@ -157,9 +158,8 @@ JNIEXPORT void JNICALL Java_cloud_unum_usearch_Index_c_1add( //\n     (*env).ReleaseFloatArrayElements(vector, vector_data, 0);\n }\n \n-JNIEXPORT jfloatArray JNICALL Java_cloud_unum_usearch_Index_c_1get(\n-    JNIEnv *env, jclass, jlong c_ptr, jint key) {\n-    \n+JNIEXPORT jfloatArray JNICALL Java_cloud_unum_usearch_Index_c_1get(JNIEnv* env, jclass, jlong c_ptr, jint key) {\n+\n     auto index = reinterpret_cast<index_dense_t*>(c_ptr);\n     size_t dim = index->dimensions();\n     std::unique_ptr<jfloat[]> vector(new jfloat[dim]);\n@@ -170,7 +170,7 @@ JNIEXPORT jfloatArray JNICALL Java_cloud_unum_usearch_Index_c_1get(\n         }\n     }\n     jfloatArray jvector = env->NewFloatArray(dim);\n-    if (jvector == nullptr) {  // out of memory\n+    if (jvector == nullptr) { // out of memory\n         return nullptr;\n     }\n     env->SetFloatArrayRegion(jvector, 0, dim, vector.get());\ndiff --git a/rust/lib.cpp b/rust/lib.cpp\nindex 60e7a2e9..b480a596 100644\n--- a/rust/lib.cpp\n+++ b/rust/lib.cpp\n@@ -177,5 +177,9 @@ std::unique_ptr<NativeIndex> new_native_index(IndexOptions const& options) {\n         throw std::invalid_argument(\"Unsupported metric or scalar type\");\n     index_dense_config_t config(options.connectivity, options.expansion_add, options.expansion_search);\n     config.multi = options.multi;\n-    return wrap(index_t::make(metric, config));\n+    index_t index = index_t::make(metric, config);\n+    // In Rust we have the luxury of returning a `Result` type even for the constructor.\n+    // So let's pre-reserve the maximal number of threads and return the error if it fails.\n+    index.reserve(index_limits_t{});\n+    return wrap(std::move(index));\n }\ndiff --git a/rust/lib.hpp b/rust/lib.hpp\nindex a810d347..ba7f3fe4 100644\n--- a/rust/lib.hpp\n+++ b/rust/lib.hpp\n@@ -1,7 +1,7 @@\n #pragma once\n #include \"rust/cxx.h\"\n \n-// We don't have to forward decalre all of those:\n+// We don't have to forward declare all of those:\n struct Matches;\n struct IndexOptions;\n enum class MetricKind;\n", "instance_id": "unum-cloud__usearch-461", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a mismatch between the documentation of the `usearch_exact_search` function in `usearch.h` and its actual arguments. The goal is to update the documentation to reflect the correct function signature and behavior, specifically regarding the output buffers for keys and distances. The provided code changes align with this goal by updating the comments to describe the matrix structure of the output and the stride parameters. However, the problem statement lacks critical details such as specific examples of the mismatch (beyond the provided code snippet), expected behavior beyond \"matching documentation,\" and any potential edge cases or constraints related to the function's usage. Additionally, the \"Steps to Reproduce\" and \"Expected Behavior\" sections are empty, which reduces the comprehensiveness of the description. Despite these minor shortcomings, the intent and scope of the issue are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range (Very Easy). The primary task involves updating the documentation in `usearch.h` to accurately describe the function arguments and output structure for `usearch_exact_search`, which is a straightforward modification of comments. The code changes provided are minimal and focused, affecting only a few lines in the header file to clarify the matrix layout and stride parameters for the output buffers. Beyond this, there are unrelated changes in other files (e.g., version updates in `Cargo.lock`, typo fixes in `README.md`, and minor code formatting in Java and Rust files), but these appear to be incidental and not directly tied to the core issue described in the problem statement. The task requires no deep understanding of the codebase architecture, complex logic, or advanced technical concepts beyond basic familiarity with C-style function signatures and documentation conventions. There are no significant edge cases or error handling requirements mentioned or implied in the problem statement or code changes related to the documentation fix. The scope of impactful changes is limited to a single file (`usearch.h`), and the overall amount of relevant code modification is trivial. Therefore, a difficulty score of 0.15 is appropriate, reflecting a very easy task with minimal cognitive or technical load.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Manually impl `PartialEq` for `Reverts`\nIn reth's invalid block hooks, we first compare the invalid block's `BundleState` to a re-executed version, then use `serde_json` to serialize `BundleState` and save it as JSON. Because the reverts are unsorted, this leads to equivalent `BundleState`s being seen as not equal. Instead, there should be a comparison API for `Reverts` that returns `true` if the reverts have the same contents, regardless of ordering.\n", "patch": "diff --git a/crates/bytecode/src/bytecode.rs b/crates/bytecode/src/bytecode.rs\nindex f1a4a58ae2..d7fa73b46e 100644\n--- a/crates/bytecode/src/bytecode.rs\n+++ b/crates/bytecode/src/bytecode.rs\n@@ -8,7 +8,7 @@ use primitives::{keccak256, Address, Bytes, B256, KECCAK_EMPTY};\n use std::sync::Arc;\n \n /// State of the [`Bytecode`] analysis.\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub enum Bytecode {\n     /// No analysis has been performed.\ndiff --git a/crates/bytecode/src/eip7702.rs b/crates/bytecode/src/eip7702.rs\nindex 191a0c33da..88a1ca5319 100644\n--- a/crates/bytecode/src/eip7702.rs\n+++ b/crates/bytecode/src/eip7702.rs\n@@ -14,7 +14,7 @@ pub const EIP7702_VERSION: u8 = 0;\n ///\n /// Format of EIP-7702 bytecode consist of:\n /// 0xEF00 (MAGIC) + 0x00 (VERSION) + 20 bytes of address.\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct Eip7702Bytecode {\n     pub delegated_address: Address,\ndiff --git a/crates/bytecode/src/eof.rs b/crates/bytecode/src/eof.rs\nindex 6aa7a00597..ce9279b162 100644\n--- a/crates/bytecode/src/eof.rs\n+++ b/crates/bytecode/src/eof.rs\n@@ -27,7 +27,7 @@ pub static EOF_MAGIC_BYTES: Bytes = bytes!(\"ef00\");\n /// EVM Object Format (EOF) container.\n ///\n /// It consists of a header, body and the raw original bytes.\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct Eof {\n     pub header: EofHeader,\ndiff --git a/crates/bytecode/src/eof/body.rs b/crates/bytecode/src/eof/body.rs\nindex 66ccd9707c..9b0423820b 100644\n--- a/crates/bytecode/src/eof/body.rs\n+++ b/crates/bytecode/src/eof/body.rs\n@@ -7,7 +7,7 @@ use std::vec::Vec;\n /// Contains types, code, container and data sections.\n ///\n /// Can be used to create a new EOF container using the [`into_eof`](EofBody::into_eof) method.\n-#[derive(Clone, Debug, Default, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, Default, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct EofBody {\n     pub types_section: Vec<TypesSection>,\ndiff --git a/crates/bytecode/src/eof/header.rs b/crates/bytecode/src/eof/header.rs\nindex a1226678cf..bbe3e364c0 100644\n--- a/crates/bytecode/src/eof/header.rs\n+++ b/crates/bytecode/src/eof/header.rs\n@@ -5,7 +5,7 @@ use super::{\n use std::vec::Vec;\n \n /// EOF Header containing\n-#[derive(Clone, Debug, Default, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, Default, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct EofHeader {\n     /// Size of EOF types section.\ndiff --git a/crates/bytecode/src/eof/types_section.rs b/crates/bytecode/src/eof/types_section.rs\nindex c2c6c475db..87f828321c 100644\n--- a/crates/bytecode/src/eof/types_section.rs\n+++ b/crates/bytecode/src/eof/types_section.rs\n@@ -8,7 +8,7 @@ use std::vec::Vec;\n const EOF_NON_RETURNING_FUNCTION: u8 = 0x80;\n \n /// Types section that contains stack information for matching code section.\n-#[derive(Debug, Clone, Default, Hash, PartialEq, Eq, Copy)]\n+#[derive(Debug, Clone, Default, Hash, PartialEq, Eq, Copy, PartialOrd, Ord)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct TypesSection {\n     /// inputs - 1 byte - `0x00-0x7F`\ndiff --git a/crates/bytecode/src/legacy/analyzed.rs b/crates/bytecode/src/legacy/analyzed.rs\nindex 1a8f08511c..e16dfc8a64 100644\n--- a/crates/bytecode/src/legacy/analyzed.rs\n+++ b/crates/bytecode/src/legacy/analyzed.rs\n@@ -4,7 +4,7 @@ use primitives::Bytes;\n use std::sync::Arc;\n \n // Legacy analyzed\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct LegacyAnalyzedBytecode {\n     /// Bytecode with 32 zero bytes padding.\ndiff --git a/crates/bytecode/src/legacy/jump_map.rs b/crates/bytecode/src/legacy/jump_map.rs\nindex bd791646fa..32069c0f0c 100644\n--- a/crates/bytecode/src/legacy/jump_map.rs\n+++ b/crates/bytecode/src/legacy/jump_map.rs\n@@ -3,7 +3,7 @@ use primitives::hex;\n use std::{fmt::Debug, sync::Arc};\n \n /// A map of valid `jump` destinations.\n-#[derive(Clone, Default, PartialEq, Eq, Hash)]\n+#[derive(Clone, Default, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct JumpTable(pub Arc<BitVec<u8>>);\n \ndiff --git a/crates/bytecode/src/legacy/raw.rs b/crates/bytecode/src/legacy/raw.rs\nindex c12252cb2c..b55ea2845a 100644\n--- a/crates/bytecode/src/legacy/raw.rs\n+++ b/crates/bytecode/src/legacy/raw.rs\n@@ -5,7 +5,7 @@ use core::ops::Deref;\n use primitives::Bytes;\n use std::{sync::Arc, vec::Vec};\n \n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, PartialEq, Eq, Hash, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct LegacyRawBytecode(pub Bytes);\n \ndiff --git a/crates/database/src/states/account_status.rs b/crates/database/src/states/account_status.rs\nindex 6f0d8e7f78..4de97c8824 100644\n--- a/crates/database/src/states/account_status.rs\n+++ b/crates/database/src/states/account_status.rs\n@@ -14,7 +14,7 @@\n /// - `Destroyed`: the account has been destroyed.\n /// - `DestroyedChanged`: the account has been destroyed and then modified.\n /// - `DestroyedAgain`: the account has been destroyed again.\n-#[derive(Clone, Copy, Default, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Copy, Default, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub enum AccountStatus {\n     #[default]\ndiff --git a/crates/database/src/states/reverts.rs b/crates/database/src/states/reverts.rs\nindex 3c14154cdc..259739259e 100644\n--- a/crates/database/src/states/reverts.rs\n+++ b/crates/database/src/states/reverts.rs\n@@ -4,11 +4,13 @@ use super::{\n };\n use core::ops::{Deref, DerefMut};\n use primitives::{Address, HashMap, U256};\n+use std::cmp::Ordering;\n+\n use state::AccountInfo;\n use std::vec::Vec;\n \n /// Contains reverts of multiple account in multiple transitions (Transitions as a block).\n-#[derive(Clone, Debug, Default, PartialEq, Eq)]\n+#[derive(Clone, Debug, Default, Eq)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct Reverts(Vec<Vec<(Address, AccountRevert)>>);\n \n@@ -81,6 +83,36 @@ impl Reverts {\n         state_reverts\n     }\n \n+    /// Compare two Reverts instances, ignoring the order of elements\n+    pub fn content_eq(&self, other: &Self) -> bool {\n+        if self.0.len() != other.0.len() {\n+            return false;\n+        }\n+\n+        for (self_transition, other_transition) in self.0.iter().zip(other.0.iter()) {\n+            if self_transition.len() != other_transition.len() {\n+                return false;\n+            }\n+\n+            let mut self_transition = self_transition.clone();\n+            let mut other_transition = other_transition.clone();\n+            // Sort both transitions\n+            self_transition.sort_by(|(addr1, revert1), (addr2, revert2)| {\n+                addr1.cmp(addr2).then_with(|| revert1.cmp(revert2))\n+            });\n+            other_transition.sort_by(|(addr1, revert1), (addr2, revert2)| {\n+                addr1.cmp(addr2).then_with(|| revert1.cmp(revert2))\n+            });\n+\n+            // Compare sorted transitions\n+            if self_transition != other_transition {\n+                return false;\n+            }\n+        }\n+\n+        true\n+    }\n+\n     /// Consume reverts and create [`PlainStateReverts`].\n     ///\n     /// Note that account are sorted by address.\n@@ -90,6 +122,12 @@ impl Reverts {\n     }\n }\n \n+impl PartialEq for Reverts {\n+    fn eq(&self, other: &Self) -> bool {\n+        self.content_eq(other)\n+    }\n+}\n+\n /// Assumption is that Revert can return full state from any future state to any past state.\n ///\n /// It is created when new account state is applied to old account state.\n@@ -198,9 +236,55 @@ impl AccountRevert {\n     }\n }\n \n+/// Implements partial ordering for AccountRevert\n+impl PartialOrd for AccountRevert {\n+    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {\n+        Some(self.cmp(other))\n+    }\n+}\n+\n+/// Implements total ordering for AccountRevert\n+impl Ord for AccountRevert {\n+    fn cmp(&self, other: &Self) -> Ordering {\n+        // First compare accounts\n+        if let Some(ord) = self.account.partial_cmp(&other.account) {\n+            if ord != Ordering::Equal {\n+                return ord;\n+            }\n+        }\n+\n+        // Convert HashMaps to sorted vectors for comparison\n+        let mut self_storage: Vec<_> = self.storage.iter().collect();\n+        let mut other_storage: Vec<_> = other.storage.iter().collect();\n+\n+        // Sort by key and then by value\n+        self_storage.sort_by(|(k1, v1), (k2, v2)| k1.cmp(k2).then_with(|| v1.cmp(v2)));\n+        other_storage.sort_by(|(k1, v1), (k2, v2)| k1.cmp(k2).then_with(|| v1.cmp(v2)));\n+\n+        // Compare each element\n+        for (self_entry, other_entry) in self_storage.iter().zip(other_storage.iter()) {\n+            let key_ord = self_entry.0.cmp(other_entry.0);\n+            if key_ord != Ordering::Equal {\n+                return key_ord;\n+            }\n+            let value_ord = self_entry.1.cmp(other_entry.1);\n+            if value_ord != Ordering::Equal {\n+                return value_ord;\n+            }\n+        }\n+\n+        // If one vector is longer than the other, or if all elements are equal\n+        self_storage\n+            .len()\n+            .cmp(&other_storage.len())\n+            .then_with(|| self.previous_status.cmp(&other.previous_status))\n+            .then_with(|| self.wipe_storage.cmp(&other.wipe_storage))\n+    }\n+}\n+\n /// Depending on previous state of account info this\n /// will tell us what to do on revert.\n-#[derive(Clone, Default, Debug, PartialEq, Eq, Hash)]\n+#[derive(Clone, Default, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub enum AccountInfoRevert {\n     #[default]\n@@ -219,7 +303,7 @@ pub enum AccountInfoRevert {\n ///\n /// Note: It is completely different state if Storage is Zero or Some or if Storage was\n /// Destroyed. Because if it is destroyed, previous values can be found in database or it can be zero.\n-#[derive(Clone, Debug, Copy, PartialEq, Eq, Hash)]\n+#[derive(Clone, Debug, Copy, PartialEq, Eq, Hash, PartialOrd, Ord)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub enum RevertToSlot {\n     Some(U256),\ndiff --git a/crates/state/src/account_info.rs b/crates/state/src/account_info.rs\nindex 0dbdd75c63..18c2824415 100644\n--- a/crates/state/src/account_info.rs\n+++ b/crates/state/src/account_info.rs\n@@ -3,7 +3,7 @@ use core::hash::{Hash, Hasher};\n use primitives::{B256, KECCAK_EMPTY, U256};\n \n /// AccountInfo account information.\n-#[derive(Clone, Debug, Eq)]\n+#[derive(Clone, Debug, Eq, Ord, PartialOrd)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct AccountInfo {\n     /// Account balance.\n", "instance_id": "bluealloy__revm-1827", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal: to implement a custom `PartialEq` for the `Reverts` struct to ensure equivalence comparison regardless of the order of elements. It provides context about the issue (unsorted reverts causing inequality in JSON serialization) and specifies the desired outcome (a comparison API for `Reverts`). However, there are minor ambiguities and missing details. For instance, it does not explicitly mention whether the comparison should handle nested structures or specific edge cases (e.g., empty reverts, duplicate entries). Additionally, the problem statement lacks examples of input and expected output for clarity on how equivalence should be determined. While the intent is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, primarily focused on a single struct (`Reverts`) and its associated types in the `crates/database/src/states/reverts.rs` file, with additional changes to derive `Ord` and `PartialOrd` traits across multiple related files for sorting purposes. This requires understanding and modifying interactions between related data structures (`AccountRevert`, `AccountInfo`, etc.) to ensure proper ordering and comparison logic. Second, the technical concepts involved include Rust's trait system (implementing `PartialEq`, `Ord`, and `PartialOrd`), sorting algorithms for unordered collections, and handling nested data structures (e.g., comparing `HashMap`s by converting to sorted vectors). These concepts are not overly complex for an experienced Rust developer but require careful implementation to avoid performance pitfalls or incorrect comparisons. Third, the problem introduces some edge case considerations, such as ensuring equivalence for different orderings of reverts and handling nested comparisons, though these are not explicitly detailed in the problem statement. Finally, the changes do not significantly impact the broader system architecture but are localized to the state management logic. Overall, this task requires a moderate level of understanding and effort, fitting within the 0.4-0.6 range, with a score of 0.45 reflecting the balance of focused but non-trivial modifications.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc1b Unexpected error `The parser is no longer progressing`\n### Environment information\r\n\r\n```block\r\nCLI:\r\n  Version:                      0.0.0 (self compiled 4848994e4507d80e56ea07c40ff41a7b303918cd)\r\n  Color support:                true\r\n\r\nPlatform:\r\n  CPU Architecture:             x86_64\r\n  OS:                           linux\r\n\r\nEnvironment:\r\n  BIOME_LOG_PATH:               unset\r\n  BIOME_LOG_PREFIX_NAME:        unset\r\n  BIOME_CONFIG_PATH:            unset\r\n  NO_COLOR:                     unset\r\n  TERM:                         \"xterm-256color\"\r\n  JS_RUNTIME_VERSION:           unset\r\n  JS_RUNTIME_NAME:              unset\r\n  NODE_PACKAGE_MANAGER:         unset\r\n\r\nBiome Configuration:\r\n  Status:                       unset\r\n\r\nWorkspace:\r\n  Open Documents:               0\r\n```\r\n\r\n\r\n### What happened?\r\n\r\nFile content(at the bottom should be attached raw, not formatted file - github removes some non-printable characters, so copying from here may not work):\r\n```\r\nr={\r\n=\r\n):\r\n```\r\n\r\ncommand\r\n```\r\ntimeout -v 150 biome lint TEST___FILE.ts\r\n```\r\n\r\ncause this\r\n```\r\nBiome encountered an unexpected error\r\n\r\nThis is a bug in Biome, not an error in your code, and we would appreciate it if you could report it to https://github.com/biomejs/biome/issues/ along with the following information to help us fixing the issue:\r\n\r\nSource Location: /home/runner/work/Automated-Fuzzer/Automated-Fuzzer/biome-main/crates/biome_parser/src/lib.rs:535:9\r\nThread Name: biome::worker_0\r\nMessage: The parser is no longer progressing. Stuck at ')' R_PAREN:6..7\r\n\r\n\r\n##### Automatic Fuzzer note, output status \"None\", output signal \"Some(6)\"\r\n```\r\n\r\n[compressed.zip](https://github.com/user-attachments/files/17892202/compressed.zip)\r\n\r\n\r\n### Expected result\r\n\r\nNo error\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Biome's Code of Conduct\n", "patch": "diff --git a/.changeset/gracefully-handle-stalled-parser.md b/.changeset/gracefully-handle-stalled-parser.md\nnew file mode 100644\nindex 000000000000..335d4af18235\n--- /dev/null\n+++ b/.changeset/gracefully-handle-stalled-parser.md\n@@ -0,0 +1,8 @@\n+---\n+\"@biomejs/biome\": patch\n+---\n+Our JavaScript parser can now gracefully handle situations where we detect the\n+parser to have stalled, such as in\n+[#4622](https://github.com/biomejs/biome/issues/4622). This means we don't fail\n+with an assertion anymore, but invalid code can trigger a graceful diagnostic\n+in such cases.\ndiff --git a/crates/biome_js_parser/src/syntax/pattern.rs b/crates/biome_js_parser/src/syntax/pattern.rs\nindex fc4ae027abf2..b6ca498757cd 100644\n--- a/crates/biome_js_parser/src/syntax/pattern.rs\n+++ b/crates/biome_js_parser/src/syntax/pattern.rs\n@@ -156,7 +156,11 @@ pub(crate) trait ParseObjectPattern {\n         let mut progress = ParserProgress::default();\n \n         while !p.at(T!['}']) {\n-            progress.assert_progressing(p);\n+            if !progress.has_progressed(p) {\n+                let diagnostic = Self::expected_property_pattern_error(p, p.cur_range());\n+                p.error(diagnostic);\n+                break;\n+            }\n \n             if p.at(T![,]) {\n                 // missing element\ndiff --git a/crates/biome_parser/src/lib.rs b/crates/biome_parser/src/lib.rs\nindex 0e79ee868151..4603dc8721d1 100644\n--- a/crates/biome_parser/src/lib.rs\n+++ b/crates/biome_parser/src/lib.rs\n@@ -510,16 +510,21 @@ pub trait Parser: Sized {\n pub struct ParserProgress(Option<TextSize>);\n \n impl ParserProgress {\n-    /// Returns true if the current parser position is passed this position\n+    /// Returns true if the current parser position is passed this position,\n+    /// and updates the progress.\n     #[inline]\n-    pub fn has_progressed<P>(&self, p: &P) -> bool\n+    pub fn has_progressed<P>(&mut self, p: &P) -> bool\n     where\n         P: Parser,\n     {\n-        match self.0 {\n+        let has_progressed = match self.0 {\n             None => true,\n             Some(pos) => pos < p.source().position(),\n-        }\n+        };\n+\n+        self.0 = Some(p.source().position());\n+\n+        has_progressed\n     }\n \n     /// Asserts that the parsing is still making progress.\n@@ -539,8 +544,6 @@ impl ParserProgress {\n             p.cur(),\n             p.cur_range(),\n         );\n-\n-        self.0 = Some(p.source().position());\n     }\n }\n \n", "instance_id": "biomejs__biome-5198", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a parser in the Biome JavaScript tool stalls and throws an unexpected error (\"The parser is no longer progressing\") when processing specific malformed input. The goal of preventing this error is implied, and the provided environment details, file content, and command to reproduce the issue add clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior beyond \"no error,\" nor does it specify whether the parser should recover gracefully or reject the input with a diagnostic message (though the code changes suggest the latter). Additionally, edge cases or constraints on the input format are not mentioned, which could be critical for a parser-related issue. Despite these minor gaps, the issue is reproducible with the provided information, and the intent is reasonably clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively focused, affecting only a couple of files (`biome_parser/src/lib.rs` and `biome_js_parser/src/syntax/pattern.rs`) and involving a small number of lines. The changes modify the parser's progress tracking logic to handle stalled states gracefully by emitting a diagnostic instead of asserting or failing. This requires understanding specific technical concepts, such as parser design, token progression tracking, and error handling in Rust, which are moderately complex but not overly advanced. The problem does not appear to impact the broader system architecture significantly, as it is confined to the parser's behavior. However, it does require careful consideration of edge cases, such as various forms of malformed input that could stall the parser, though these are not explicitly detailed in the problem statement. Solving this also demands familiarity with the Biome project's parser internals and Rust's ownership and borrowing rules to ensure the changes are safe and effective. Overall, this problem requires a moderate depth of understanding and targeted modifications, justifying a difficulty score of 0.55, leaning toward the higher end of the medium range due to the parser-specific knowledge required.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc1b `lint/a11y/noAutofocus` contradicts with MDN recommendations for <dialog>\n### Environment information\n\n```block\nCLI:\r\n  Version:                      1.9.4\r\n  Color support:                true\r\n\r\nPlatform:\r\n  CPU Architecture:             x86_64\r\n  OS:                           linux\r\n\r\nEnvironment:\r\n  BIOME_LOG_PATH:               unset\r\n  BIOME_LOG_PREFIX_NAME:        unset\r\n  BIOME_CONFIG_PATH:            unset\r\n  NO_COLOR:                     unset\r\n  TERM:                         \"xterm-256color\"\r\n  JS_RUNTIME_VERSION:           \"v20.11.1\"\r\n  JS_RUNTIME_NAME:              \"node\"\r\n  NODE_PACKAGE_MANAGER:         unset\r\n\r\nBiome Configuration:\r\n  Status:                       Loaded successfully\r\n  Formatter disabled:           false\r\n  Linter disabled:              false\r\n  Organize imports disabled:    false\r\n  VCS disabled:                 false\r\n\r\nWorkspace:\r\n  Open Documents:               0\n```\n\n\n### What happened?\n\nA `lint/a11y/noAutofocus` contradicts with MDN recommendations for <dialog>:\r\n\r\n> The [autofocus](https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/autofocus) attribute should be added to the element the user is expected to interact with immediately upon opening a modal dialog. If no other element involves more immediate interaction, it is recommended to add autofocus to the close button inside the dialog, or the dialog itself if the user is expected to click/activate it to dismiss.\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog\r\n\r\n[Code example](https://biomejs.dev/playground/?indentStyle=space&code=ZQB4AHAAbwByAHQAIABjAG8AbgBzAHQAIABBAHMAawBQAGEAcwBzAHcAbwByAGQARABpAGEAbABvAGcAIAA9ACAAKAApACAAPQA%2BACAAewAKACAAIABjAG8AbgBzAHQAIABpAGQAIAA9ACAAdQBzAGUASQBkACgAKQA7AAoAIAAgAGMAbwBuAHMAdAAgAFsAcABhAHMAcwB3AG8AcgBkACwAIABzAGUAdABQAGEAcwBzAHcAbwByAGQAXQAgAD0AIAB1AHMAZQBTAHQAYQB0AGUAKAAiACIAKQA7AAoACgAgACAAYwBvAG4AcwB0ACAAaABhAG4AZABsAGUAQwBoAGEAbgBnAGUAIAA9ACAAdQBzAGUAQwBhAGwAbABiAGEAYwBrACgAKABlAHYAZQBuAHQAKQAgAD0APgAgAHsACgAgACAAIAAgAHMAZQB0AFAAYQBzAHMAdwBvAHIAZAAoAGUAdgBlAG4AdAAuAHQAYQByAGcAZQB0AC4AdgBhAGwAdQBlACkAOwAKACAAIAB9ACwAIABbAF0AKQA7AAoACgAgACAAcgBlAHQAdQByAG4AIAAoAAoAIAAgACAAIAA8AGQAaQBhAGwAbwBnACAAbwBwAGUAbgA%2BAAoAIAAgACAAIAAgACAAPABmAG8AcgBtAD4ACgAgACAAIAAgACAAIAAgACAAPABsAGEAYgBlAGwAIABoAHQAbQBsAEYAbwByAD0AewBpAGQAfQA%2BAAoAIAAgACAAIAAgACAAIAAgACAAIABFAG4AdABlAHIAIABwAGEAcwBzAHcAbwByAGQACgAgACAAIAAgACAAIAAgACAAPAAvAGwAYQBiAGUAbAA%2BAAoAIAAgACAAIAAgACAAIAAgADwAaQBuAHAAdQB0AAoAIAAgACAAIAAgACAAIAAgACAAIAB0AHkAcABlAD0AIgBwAGEAcwBzAHcAbwByAGQAIgAKACAAIAAgACAAIAAgACAAIAAgACAAdgBhAGwAdQBlAD0AewBwAGEAcwBzAHcAbwByAGQAfQAKACAAIAAgACAAIAAgACAAIAAgACAAbwBuAEMAaABhAG4AZwBlAD0AewBoAGEAbgBkAGwAZQBDAGgAYQBuAGcAZQB9AAoAIAAgACAAIAAgACAAIAAgACAAIABhAHUAdABvAEYAbwBjAHUAcwAKACAAIAAgACAAIAAgACAAIAAgACAAYQB1AHQAbwBDAG8AbQBwAGwAZQB0AGUAPQAiAG8AZgBmACIACgAgACAAIAAgACAAIAAgACAAIAAgAGkAZAA9AHsAaQBkAH0ACgAgACAAIAAgACAAIAAgACAALwA%2BAAoAIAAgACAAIAAgACAAPAAvAGYAbwByAG0APgAKACAAIAAgACAAPAAvAGQAaQBhAGwAbwBnAD4ACgAgACAAKQA7AAoAfQA7AAoA)\r\n\n\n### Expected result\n\nIt should permit `autoFocus` when it is used inside a dialog.\n\n### Code of Conduct\n\n- [X] I agree to follow Biome's Code of Conduct\n", "patch": "diff --git a/crates/biome_js_analyze/src/lint/a11y/no_autofocus.rs b/crates/biome_js_analyze/src/lint/a11y/no_autofocus.rs\nindex 9b52a095cfa6..6867213b97cc 100644\n--- a/crates/biome_js_analyze/src/lint/a11y/no_autofocus.rs\n+++ b/crates/biome_js_analyze/src/lint/a11y/no_autofocus.rs\n@@ -1,15 +1,19 @@\n use crate::JsRuleAction;\n use biome_analyze::{\n-    context::RuleContext, declare_lint_rule, Ast, FixKind, Rule, RuleDiagnostic, RuleSource,\n+    context::RuleContext, declare_lint_rule, FixKind, Phases, QueryMatch, Queryable, Rule,\n+    RuleDiagnostic, RuleSource, Visitor,\n };\n use biome_console::markup;\n-use biome_js_syntax::{jsx_ext::AnyJsxElement, JsxAttribute};\n-use biome_rowan::{AstNode, BatchMutationExt};\n+use biome_js_syntax::{jsx_ext::AnyJsxElement, JsLanguage, JsSyntaxKind, JsxAttribute};\n+use biome_rowan::{AstNode, BatchMutationExt, WalkEvent};\n+use biome_string_case::StrOnlyExtension;\n \n declare_lint_rule! {\n     /// Enforce that autoFocus prop is not used on elements.\n     ///\n     /// Autofocusing elements can cause usability issues for sighted and non-sighted users, alike.\n+    /// But the autofocus attribute should be added to the element the user is expected to\n+    /// interact with immediately upon opening a modal dialog or popover.\n     ///\n     /// ## Examples\n     ///\n@@ -50,10 +54,21 @@ declare_lint_rule! {\n     /// <MyComponent autoFocus={true} />\n     ///```\n     ///\n+    /// ```jsx\n+    /// // `autoFocus` prop in element has `popover` attribute is valid\n+    /// <div popover><input autoFocus /></div>\n+    /// ```\n+    ///\n+    /// ```jsx\n+    /// // `autoFocus` prop in `dialog` is valid\n+    /// <dialog><input autoFocus /></dialog>\n+    /// ```\n+    ///\n     /// ## Resources\n     ///\n     /// - [WHATWG HTML Standard, The autofocus attribute](https://html.spec.whatwg.org/multipage/interaction.html#attr-fe-autofocus)\n     /// - [The accessibility of HTML 5 autofocus](https://brucelawson.co.uk/2009/the-accessibility-of-html-5-autofocus/)\n+    /// - [MDN Web Docs, HTMLElement: autofocus property](https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/autofocus)\n     ///\n     pub NoAutofocus {\n         version: \"1.0.0\",\n@@ -65,8 +80,113 @@ declare_lint_rule! {\n     }\n }\n \n+fn find_kept_autofocus_mark(element: &AnyJsxElement) -> bool {\n+    // the check for no_autofocus can be ignored\n+    // 1. inside the `dialog` element\n+    // 2. inside the element with the popover attribute\n+\n+    let is_dialog_element = match element.name_value_token() {\n+        Some(syntax_token) => {\n+            let tag_name = String::from(syntax_token.text_trimmed());\n+            tag_name.to_lowercase_cow() == \"dialog\"\n+        }\n+        None => false,\n+    };\n+\n+    let has_popover_attr = element.has_truthy_attribute(\"popover\");\n+\n+    is_dialog_element || has_popover_attr\n+}\n+\n+#[derive(Default)]\n+struct ValidAutofocusVisitor {\n+    stack: Vec<(AnyJsxElement, bool)>,\n+}\n+\n+impl Visitor for ValidAutofocusVisitor {\n+    type Language = JsLanguage;\n+\n+    fn visit(\n+        &mut self,\n+        event: &biome_rowan::WalkEvent<biome_rowan::SyntaxNode<Self::Language>>,\n+        mut ctx: biome_analyze::VisitorContext<Self::Language>,\n+    ) {\n+        match event {\n+            WalkEvent::Enter(node) => {\n+                let kind = node.kind();\n+\n+                match kind {\n+                    JsSyntaxKind::JSX_OPENING_ELEMENT => {\n+                        let element = AnyJsxElement::unwrap_cast(node.clone());\n+\n+                        let is_hold = match self.stack.last() {\n+                            None => false,\n+                            Some((_, value)) => *value,\n+                        };\n+\n+                        if is_hold {\n+                            self.stack.push((element.clone(), true));\n+                        } else {\n+                            let next_hold = find_kept_autofocus_mark(&element);\n+                            self.stack.push((element.clone(), next_hold));\n+                        }\n+\n+                        ctx.match_query(ValidAutofocus(element));\n+                    }\n+                    JsSyntaxKind::JSX_SELF_CLOSING_ELEMENT => {\n+                        let element = AnyJsxElement::unwrap_cast(node.clone());\n+\n+                        let is_hold = match self.stack.last() {\n+                            None => false,\n+                            Some((_, value)) => *value,\n+                        };\n+\n+                        if !is_hold {\n+                            ctx.match_query(ValidAutofocus(element));\n+                        }\n+                    }\n+                    JsSyntaxKind::JSX_CLOSING_ELEMENT => {\n+                        self.stack.pop();\n+                    }\n+                    _ => {}\n+                }\n+            }\n+            WalkEvent::Leave(_) => {}\n+        };\n+    }\n+}\n+\n+pub struct ValidAutofocus(AnyJsxElement);\n+\n+impl QueryMatch for ValidAutofocus {\n+    fn text_range(&self) -> biome_rowan::TextRange {\n+        self.0.range()\n+    }\n+}\n+\n+impl Queryable for ValidAutofocus {\n+    type Input = Self;\n+\n+    type Output = AnyJsxElement;\n+\n+    type Language = JsLanguage;\n+\n+    type Services = ();\n+\n+    fn build_visitor(\n+        analyzer: &mut impl biome_analyze::AddVisitor<Self::Language>,\n+        _: &<Self::Language as biome_rowan::Language>::Root,\n+    ) {\n+        analyzer.add_visitor(Phases::Syntax, ValidAutofocusVisitor::default);\n+    }\n+\n+    fn unwrap_match(_: &biome_analyze::ServiceBag, query: &Self::Input) -> Self::Output {\n+        query.0.clone()\n+    }\n+}\n+\n impl Rule for NoAutofocus {\n-    type Query = Ast<AnyJsxElement>;\n+    type Query = ValidAutofocus;\n     type State = JsxAttribute;\n     type Signals = Option<Self::State>;\n     type Options = ();\n", "instance_id": "biomejs__biome-4475", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `lint/a11y/noAutofocus` rule in the BiomeJS tool contradicts MDN recommendations for the `<dialog>` element by not allowing the `autoFocus` attribute, which should be permitted in specific contexts like modal dialogs or elements with the `popover` attribute. The goal is explicitly stated\u2014to permit `autoFocus` inside a `<dialog>` or elements with `popover`. The statement includes relevant environment information, a code example, and references to MDN documentation, which adds to its clarity. However, it lacks explicit mention of specific edge cases or constraints (e.g., whether `autoFocus` should be allowed only on certain child elements within a `<dialog>` or if there are nested scenarios to consider). Additionally, the expected behavior for complex JSX structures or interactions with other lint rules is not detailed. Thus, while the problem is valid and mostly clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is limited to a single file (`no_autofocus.rs`) within the BiomeJS linting framework, focusing on modifying the `NoAutofocus` rule. However, the changes are non-trivial, involving the addition of a custom visitor (`ValidAutofocusVisitor`) to traverse the JSX syntax tree and logic to detect whether an element is within a `<dialog>` or has a `popover` attribute. This requires understanding Rust's syntax tree manipulation, the BiomeJS analyzer framework (e.g., `Visitor`, `Queryable`, `Phases`), and JSX-specific handling, which collectively involve multiple technical concepts of moderate complexity. The code changes are significant, adding over 100 lines of logic to handle the new behavior, including state management with a stack to track nested elements. While the problem does not impact the broader system architecture, it requires careful handling of edge cases like nested elements or malformed JSX, though these are not explicitly mentioned in the problem statement. Overall, the task demands a solid understanding of Rust, syntax tree traversal, and the specific linting framework, but it does not reach the level of hard or very hard due to its localized scope and lack of system-wide impact or advanced domain-specific challenges. A score of 0.55 reflects this medium difficulty, leaning slightly above the midpoint due to the complexity of the visitor pattern implementation.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udc85 `useImportType` fails to remove inline `type` imports when the group also uses a `type` import\n### Environment information\n\n```bash\nCLI:\r\n  Version:                      1.9.3\r\n  Color support:                true\r\n\r\nPlatform:\r\n  CPU Architecture:             x86_64\r\n  OS:                           linux\r\n\r\nEnvironment:\r\n  BIOME_LOG_PATH:               unset\r\n  BIOME_LOG_PREFIX_NAME:        unset\r\n  BIOME_CONFIG_PATH:            unset\r\n  NO_COLOR:                     unset\r\n  TERM:                         \"xterm-256color\"\r\n  JS_RUNTIME_VERSION:           \"v20.17.0\"\r\n  JS_RUNTIME_NAME:              \"node\"\r\n  NODE_PACKAGE_MANAGER:         \"pnpm/9.11.0\"\r\n\r\nBiome Configuration:\r\n  Status:                       Loaded successfully\r\n  Formatter disabled:           false\r\n  Linter disabled:              false\r\n  Organize imports disabled:    false\r\n  VCS disabled:                 false\r\n\r\nLinter:\r\n  JavaScript enabled:           true\r\n  JSON enabled:                 true\r\n  CSS enabled:                  true\r\n  GraphQL enabled:              false\r\n  Recommended:                  true\r\n  All:                          false\r\n  Enabled rules:\r\n  performance/noDelete\r\n  suspicious/noCatchAssign\r\n  suspicious/noUnsafeNegation\r\n  complexity/useLiteralKeys\r\n  suspicious/noClassAssign\r\n  style/useImportType\r\n  complexity/noMultipleSpacesInRegularExpressionLiterals\r\n  a11y/useValidLang\r\n  complexity/noUselessEmptyExport\r\n  suspicious/useNamespaceKeyword\r\n  suspicious/useValidTypeof\r\n  a11y/useValidAriaRole\r\n  correctness/noConstantCondition\r\n  a11y/useAriaActivedescendantWithTabindex\r\n  suspicious/noAssignInExpressions\r\n  style/useDefaultParameterLast\r\n  complexity/noEmptyTypeParameters\r\n  correctness/noConstructorReturn\r\n  style/useSelfClosingElements\r\n  suspicious/noDuplicateParameters\r\n  suspicious/noDuplicateSelectorsKeyframeBlock\r\n  correctness/noUnknownProperty\r\n  style/useTemplate\r\n  correctness/noUnusedLabels\r\n  complexity/noUselessTernary\r\n  correctness/noUnreachableSuper\r\n  suspicious/noCompareNegZero\r\n  suspicious/noExplicitAny\r\n  correctness/noSwitchDeclarations\r\n  a11y/noAutofocus\r\n  correctness/noUnsafeOptionalChaining\r\n  correctness/noConstAssign\r\n  suspicious/noControlCharactersInRegex\r\n  complexity/noUselessTypeConstraint\r\n  style/noVar\r\n  suspicious/noDoubleEquals\r\n  suspicious/noRedundantUseStrict\r\n  style/useLiteralEnumMembers\r\n  suspicious/noGlobalIsNan\r\n  suspicious/noEmptyInterface\r\n  suspicious/noConstEnum\r\n  suspicious/noMisleadingCharacterClass\r\n  correctness/noPrecisionLoss\r\n  a11y/noLabelWithoutControl\r\n  suspicious/noRedeclare\r\n  correctness/noStringCaseMismatch\r\n  correctness/noSetterReturn\r\n  correctness/noInvalidConstructorSuper\r\n  suspicious/noImplicitAnyLet\r\n  suspicious/noFallthroughSwitchClause\r\n  suspicious/noUnsafeDeclarationMerging\r\n  complexity/noUselessThisAlias\r\n  correctness/noUnreachable\r\n  a11y/useKeyWithClickEvents\r\n  suspicious/noDuplicateObjectKeys\r\n  complexity/noThisInStatic\r\n  complexity/useOptionalChain\r\n  correctness/noInnerDeclarations\r\n  style/noParameterAssign\r\n  suspicious/noDuplicateCase\r\n  a11y/useValidAnchor\r\n  complexity/useRegexLiterals\r\n  correctness/noSelfAssign\r\n  correctness/noInvalidBuiltinInstantiation\r\n  style/noUselessElse\r\n  style/useShorthandFunctionType\r\n  suspicious/noShadowRestrictedNames\r\n  correctness/noInvalidDirectionInLinearGradient\r\n  suspicious/noImportantInKeyframe\r\n  a11y/useMediaCaption\r\n  complexity/noUselessLabel\r\n  complexity/noUselessCatch\r\n  correctness/noUnsafeFinally\r\n  a11y/useAriaPropsForRole\r\n  correctness/noNonoctalDecimalEscape\r\n  style/useEnumInitializers\r\n  a11y/useHtmlLang\r\n  suspicious/noDuplicateTestHooks\r\n  complexity/noStaticOnlyClass\r\n  style/useWhile\r\n  complexity/useArrowFunction\r\n  style/noInferrableTypes\r\n  a11y/noNoninteractiveTabindex\r\n  complexity/useSimpleNumberKeys\r\n  correctness/useYield\r\n  a11y/noInteractiveElementToNoninteractiveRole\r\n  style/useNumericLiterals\r\n  correctness/noUnnecessaryContinue\r\n  suspicious/noApproximativeNumericConstant\r\n  suspicious/noImportAssign\r\n  suspicious/noLabelVar\r\n  correctness/noGlobalObjectCalls\r\n  suspicious/useDefaultSwitchClauseLast\r\n  correctness/noUnknownUnit\r\n  a11y/useAltText\r\n  correctness/noEmptyCharacterClassInRegex\r\n  suspicious/noSparseArray\r\n  a11y/useIframeTitle\r\n  complexity/noBannedTypes\r\n  a11y/noSvgWithoutTitle\r\n  correctness/noVoidElementsWithChildren\r\n  style/useAsConstAssertion\r\n  correctness/useJsxKeyInIterable\r\n  style/useExportType\r\n  complexity/noUselessLoneBlockStatements\r\n  suspicious/noDebugger\r\n  a11y/useValidAriaValues\r\n  style/noArguments\r\n  suspicious/noCommentText\r\n  a11y/useFocusableInteractive\r\n  correctness/noUnmatchableAnbSelector\r\n  suspicious/noGlobalAssign\r\n  suspicious/noDuplicateJsxProps\r\n  suspicious/noMisleadingInstantiator\r\n  a11y/noPositiveTabindex\r\n  correctness/noEmptyPattern\r\n  complexity/noExcessiveNestedTestSuites\r\n  security/noDangerouslySetInnerHtmlWithChildren\r\n  a11y/useKeyWithMouseEvents\r\n  suspicious/noExtraNonNullAssertion\r\n  suspicious/noPrototypeBuiltins\r\n  correctness/noRenderReturnValue\r\n  correctness/useExhaustiveDependencies\r\n  security/noGlobalEval\r\n  style/noNonNullAssertion\r\n  a11y/noRedundantRoles\r\n  complexity/useFlatMap\r\n  correctness/useIsNan\r\n  style/useConst\r\n  suspicious/noGlobalIsFinite\r\n  suspicious/noSelfCompare\r\n  suspicious/noShorthandPropertyOverrides\r\n  suspicious/noAsyncPromiseExecutor\r\n  suspicious/noDuplicateFontNames\r\n  suspicious/noThenProperty\r\n  suspicious/useGetterReturn\r\n  security/noDangerouslySetInnerHtml\r\n  style/useNodejsImportProtocol\r\n  a11y/noDistractingElements\r\n  suspicious/noArrayIndexKey\r\n  complexity/noWith\r\n  suspicious/noDuplicateClassMembers\r\n  complexity/noExtraBooleanCast\r\n  performance/noAccumulatingSpread\r\n  a11y/useValidAriaProps\r\n  a11y/noRedundantAlt\r\n  correctness/noChildrenProp\r\n  correctness/noUnknownFunction\r\n  correctness/noInvalidPositionAtImportRule\r\n  suspicious/noConfusingLabels\r\n  suspicious/noSuspiciousSemicolonInJsx\r\n  suspicious/noConfusingVoidType\r\n  suspicious/noFocusedTests\r\n  a11y/useButtonType\r\n  a11y/useSemanticElements\r\n  a11y/noAriaUnsupportedElements\r\n  correctness/noInvalidGridAreas\r\n  correctness/noFlatMapIdentity\r\n  a11y/noBlankTarget\r\n  a11y/useHeadingContent\r\n  correctness/useValidForDirection\r\n  correctness/noVoidTypeReturn\r\n  correctness/noInvalidUseBeforeDeclaration\r\n  a11y/noAriaHiddenOnFocusable\r\n  a11y/useGenericFontNames\r\n  correctness/noUnknownMediaFeatureName\r\n  a11y/useAnchorContent\r\n  complexity/noUselessRename\r\n  style/useNumberNamespace\r\n  complexity/noUselessConstructor\r\n  a11y/noAccessKey\r\n  style/useExponentiationOperator\r\n  style/noUnusedTemplateLiteral\r\n  complexity/noUselessSwitchCase\r\n  style/useSingleVarDeclarator\r\n  suspicious/noExportsInTest\r\n  a11y/noNoninteractiveElementToInteractiveRole\r\n  style/noCommaOperator\r\n  suspicious/noDuplicateAtImportRules\r\n  suspicious/useIsArray\r\n  a11y/noHeaderScope\r\n  complexity/noUselessFragments\r\n  suspicious/noMisrefactoredShorthandAssign\r\n  suspicious/noEmptyBlock\r\n  complexity/noForEach\r\n  correctness/noUnusedImports\r\n  suspicious/noFunctionAssign\r\n\r\nWorkspace:\r\n  Open Documents:               0\n```\n\n\n### Rule name\n\nuseImportType\n\n### Playground link\n\nhttps://biomejs.dev/playground/?code=aQBtAHAAbwByAHQAIAB0AHkAcABlACAAewAgAHQAeQBwAGUAIABGAG8AbwAsACAAdAB5AHAAZQAgAEIAYQByACAAfQAgAGYAcgBvAG0AIAAiAHMAbwBtAGUAdwBoAGUAcgBlACIAOwAKAAoAZQB4AHAAbwByAHQAIABjAG8AbgBzAHQAIABmAG8AbwA6ACAARgBvAG8AIAA9ACAAMQA7AAoAZQB4AHAAbwByAHQAIABjAG8AbgBzAHQAIABiAGEAcgA6ACAAQgBhAHIAIAA9ACAAMgA7AAoA\n\n### Expected result\n\nIt should remove the inline `type` keywords.\n\n### Code of Conduct\n\n- [X] I agree to follow Biome's Code of Conduct\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex ac4765b877f8..b9b867f1e308 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -33,6 +33,8 @@ our [guidelines for writing a good changelog entry](https://github.com/biomejs/b\n \n ### Linter\n \n+#### Bug Fixes\n+\n - Biome no longer crashes when it encounters a string that contain a multibyte character ([#4181](https://github.com/biomejs/biome/issues/4181)).\n \n   This fixes a regression introduced in Biome 1.9.3\n@@ -49,6 +51,17 @@ our [guidelines for writing a good changelog entry](https://github.com/biomejs/b\n \n - Fix [#4041](https://github.com/biomejs/biome/issues/4041). Now the rule `useSortedClasses` won't be triggered if `className` is composed only by inlined variables. Contributed by @ematipico\n \n+- [useImportType](https://biomejs.dev/linter/rules/use-import-type/) now reports useless inline type qualifiers ([#4178](https://github.com/biomejs/biome/issues/4178)).\n+\n+  The following fix is now proposed:\n+\n+  ```diff\n+  - import type { type A, B } from \"\";\n+  + import type { A, B } from \"\";\n+  ```\n+\n+  Contributed by @Conaclos\n+\n ### Parser\n \n #### Bug Fixes\ndiff --git a/crates/biome_js_analyze/src/lint/style/use_import_type.rs b/crates/biome_js_analyze/src/lint/style/use_import_type.rs\nindex 678fec49aa97..29b77ce026d5 100644\n--- a/crates/biome_js_analyze/src/lint/style/use_import_type.rs\n+++ b/crates/biome_js_analyze/src/lint/style/use_import_type.rs\n@@ -136,10 +136,8 @@ impl Rule for UseImportType {\n         if import_clause.assertion().is_some() {\n             return None;\n         }\n-        if import_clause.type_token().is_some() ||\n-            // Import attributes and type-only imports are not compatible.\n-            import_clause.assertion().is_some()\n-        {\n+        // Import attributes and type-only imports are not compatible.\n+        if import_clause.assertion().is_some() {\n             return None;\n         }\n         let model = ctx.model();\n@@ -157,7 +155,7 @@ impl Rule for UseImportType {\n                 };\n                 match clause.specifier().ok()? {\n                     AnyJsCombinedSpecifier::JsNamedImportSpecifiers(named_specifiers) => {\n-                        match named_import_type_fix(model, &named_specifiers) {\n+                        match named_import_type_fix(model, &named_specifiers, false) {\n                             Some(NamedImportTypeFix::UseImportType(specifiers)) => {\n                                 if is_default_used_as_type {\n                                     Some(ImportTypeFix::UseImportType)\n@@ -180,6 +178,10 @@ impl Rule for UseImportType {\n                                     Some(ImportTypeFix::AddInlineTypeQualifiers(specifiers))\n                                 }\n                             }\n+                            Some(NamedImportTypeFix::RemoveInlineTypeQualifiers(_)) => {\n+                                // Should not be reached because we pass `false` to `named_import_type_fix`.\n+                                None\n+                            }\n                             None => is_default_used_as_type\n                                 .then_some(ImportTypeFix::ExtractDefaultImportType(vec![])),\n                         }\n@@ -206,6 +208,9 @@ impl Rule for UseImportType {\n                 }\n             }\n             AnyJsImportClause::JsImportDefaultClause(clause) => {\n+                if clause.type_token().is_some() {\n+                    return None;\n+                }\n                 let default_binding = clause.default_specifier().ok()?.local_name().ok()?;\n                 let default_binding = default_binding.as_js_identifier_binding()?;\n                 if ctx.jsx_runtime() == JsxRuntime::ReactClassic\n@@ -217,14 +222,24 @@ impl Rule for UseImportType {\n                 is_only_used_as_type(model, default_binding).then_some(ImportTypeFix::UseImportType)\n             }\n             AnyJsImportClause::JsImportNamedClause(clause) => {\n-                match named_import_type_fix(model, &clause.named_specifiers().ok()?)? {\n+                match named_import_type_fix(\n+                    model,\n+                    &clause.named_specifiers().ok()?,\n+                    clause.type_token().is_some(),\n+                )? {\n                     NamedImportTypeFix::UseImportType(_) => Some(ImportTypeFix::UseImportType),\n                     NamedImportTypeFix::AddInlineTypeQualifiers(specifiers) => {\n                         Some(ImportTypeFix::AddInlineTypeQualifiers(specifiers))\n                     }\n+                    NamedImportTypeFix::RemoveInlineTypeQualifiers(type_tokens) => {\n+                        Some(ImportTypeFix::RemoveTypeQualifiers(type_tokens))\n+                    }\n                 }\n             }\n             AnyJsImportClause::JsImportNamespaceClause(clause) => {\n+                if clause.type_token().is_some() {\n+                    return None;\n+                }\n                 let namespace_binding = clause.namespace_specifier().ok()?.local_name().ok()?;\n                 let namespace_binding = namespace_binding.as_js_identifier_binding()?;\n                 if ctx.jsx_runtime() == JsxRuntime::ReactClassic\n@@ -300,6 +315,20 @@ impl Rule for UseImportType {\n                 }\n                 diagnostic\n             }\n+            ImportTypeFix::RemoveTypeQualifiers(type_tokens) => {\n+                let mut diagnostic = RuleDiagnostic::new(\n+                    rule_category!(),\n+                    import.import_clause().ok()?.type_token()?.text_trimmed_range(),\n+                    \"The import has this type qualifier that makes all inline type qualifiers useless.\",\n+                );\n+                for type_token in type_tokens {\n+                    diagnostic = diagnostic.detail(\n+                        type_token.text_trimmed_range(),\n+                        \"This inline type qualifier is useless.\",\n+                    )\n+                }\n+                return Some(diagnostic);\n+            }\n         };\n         Some(diagnostic.note(markup! {\n             \"Importing the types with \"<Emphasis>\"import type\"</Emphasis>\" ensures that they are removed by the transpilers and avoids loading unnecessary modules.\"\n@@ -527,6 +556,11 @@ impl Rule for UseImportType {\n                     mutation.replace_node(specifier.clone(), new_specifier);\n                 }\n             }\n+            ImportTypeFix::RemoveTypeQualifiers(type_tokens) => {\n+                for type_token in type_tokens {\n+                    mutation.remove_token(type_token.clone());\n+                }\n+            }\n         }\n         Some(JsRuleAction::new(\n             ActionCategory::QuickFix,\n@@ -543,6 +577,7 @@ pub enum ImportTypeFix {\n     ExtractDefaultImportType(Vec<AnyJsNamedImportSpecifier>),\n     ExtractCombinedImportType,\n     AddInlineTypeQualifiers(Vec<AnyJsNamedImportSpecifier>),\n+    RemoveTypeQualifiers(Vec<JsSyntaxToken>),\n }\n \n /// Returns `true` if all references of `binding` are only used as a type.\n@@ -564,50 +599,71 @@ fn is_only_used_as_type(model: &SemanticModel, binding: &JsIdentifierBinding) ->\n pub enum NamedImportTypeFix {\n     UseImportType(Vec<AnyJsNamedImportSpecifier>),\n     AddInlineTypeQualifiers(Vec<AnyJsNamedImportSpecifier>),\n+    RemoveInlineTypeQualifiers(Vec<JsSyntaxToken>),\n }\n \n fn named_import_type_fix(\n     model: &SemanticModel,\n     named_specifiers: &JsNamedImportSpecifiers,\n+    has_type_token: bool,\n ) -> Option<NamedImportTypeFix> {\n     let specifiers = named_specifiers.specifiers();\n     if specifiers.is_empty() {\n         return None;\n     };\n-    let mut imports_only_types = true;\n-    let mut specifiers_requiring_type_marker = Vec::with_capacity(specifiers.len());\n-    for specifier in specifiers.iter() {\n-        let Ok(specifier) = specifier else {\n-            imports_only_types = false;\n-            continue;\n-        };\n-        if specifier.type_token().is_none() {\n-            if specifier\n-                .local_name()\n-                .and_then(|local_name| {\n-                    Some(is_only_used_as_type(\n-                        model,\n-                        local_name.as_js_identifier_binding()?,\n-                    ))\n-                })\n-                .unwrap_or(false)\n-            {\n-                specifiers_requiring_type_marker.push(specifier);\n-            } else {\n-                imports_only_types = false;\n+    if has_type_token {\n+        let mut useless_type_tokens = Vec::with_capacity(specifiers.len());\n+        for specifier in specifiers.iter() {\n+            let Ok(specifier) = specifier else {\n+                continue;\n+            };\n+            if let Some(type_token) = specifier.type_token() {\n+                useless_type_tokens.push(type_token);\n             }\n         }\n-    }\n-    if imports_only_types {\n-        Some(NamedImportTypeFix::UseImportType(\n-            specifiers_requiring_type_marker,\n-        ))\n-    } else if specifiers_requiring_type_marker.is_empty() {\n-        None\n+        if useless_type_tokens.is_empty() {\n+            None\n+        } else {\n+            Some(NamedImportTypeFix::RemoveInlineTypeQualifiers(\n+                useless_type_tokens,\n+            ))\n+        }\n     } else {\n-        Some(NamedImportTypeFix::AddInlineTypeQualifiers(\n-            specifiers_requiring_type_marker,\n-        ))\n+        let mut imports_only_types = true;\n+        let mut specifiers_requiring_type_marker = Vec::with_capacity(specifiers.len());\n+        for specifier in specifiers.iter() {\n+            let Ok(specifier) = specifier else {\n+                imports_only_types = false;\n+                continue;\n+            };\n+            if specifier.type_token().is_none() {\n+                if specifier\n+                    .local_name()\n+                    .and_then(|local_name| {\n+                        Some(is_only_used_as_type(\n+                            model,\n+                            local_name.as_js_identifier_binding()?,\n+                        ))\n+                    })\n+                    .unwrap_or(false)\n+                {\n+                    specifiers_requiring_type_marker.push(specifier);\n+                } else {\n+                    imports_only_types = false;\n+                }\n+            }\n+        }\n+        if imports_only_types {\n+            Some(NamedImportTypeFix::UseImportType(\n+                specifiers_requiring_type_marker,\n+            ))\n+        } else if specifiers_requiring_type_marker.is_empty() {\n+            None\n+        } else {\n+            Some(NamedImportTypeFix::AddInlineTypeQualifiers(\n+                specifiers_requiring_type_marker,\n+            ))\n+        }\n     }\n }\n \n", "instance_id": "biomejs__biome-4201", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `useImportType` rule fails to remove inline `type` keywords in imports when the group also uses a `type` import. It provides a specific example in the form of a playground link and expected behavior (removing inline `type` keywords). Additionally, the environment information and rule context are well-documented. However, there are minor ambiguities, such as the lack of explicit mention of edge cases or constraints (e.g., how this rule interacts with other import types or complex import structures). While the goal is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`use_import_type.rs`) within the Biome linter project, with modifications to the logic for handling inline `type` qualifiers in imports. The changes involve understanding and modifying the existing rule logic, which requires familiarity with Rust, the Biome project's internal AST (Abstract Syntax Tree) representation, and semantic analysis via the `SemanticModel`. The technical concepts involved include parsing import clauses, handling token-based syntax modifications, and ensuring compatibility with existing rules, which adds moderate complexity. Additionally, the code changes address a specific bug, requiring careful handling of conditions like `has_type_token` to avoid introducing new issues, though explicit edge cases are not extensively detailed in the problem statement. The impact on the codebase is limited to the linter's style rule, without broader architectural changes. Overall, this task requires a solid understanding of a few specific concepts and careful implementation, but it does not demand deep architectural refactoring or advanced domain-specific knowledge, placing it in the medium difficulty range at 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udcce Implement `useCollapsedIf` - `clippy/collapsible_if`, `unicorn/no-lonely-if`\n### Description\n\nImplement [clippy/collapsible_if](https://rust-lang.github.io/rust-clippy/master/#/collapsible_if) and [unicorn/no-lonely-if](https://github.com/sindresorhus/eslint-plugin-unicorn/blob/main/docs/rules/no-lonely-if.md).\r\n\r\n**Want to contribute?** Lets you know you are interested! We will assign you to the issue to prevent several people to work on the same issue. Don't worry, we can unassign you later if you are no longer interested in the issue! Read our [contributing guide](https://github.com/biomejs/biome/blob/main/CONTRIBUTING.md) and [analyzer contributing guide](https://github.com/biomejs/biome/blob/main/crates/biome_analyze/CONTRIBUTING.md).\r\n\r\nThe implementer could take some inspirations from the implementation of existing rules such as `useCollapsedElseIf`.\n", "patch": "diff --git a/crates/biome_cli/src/execute/migrate/eslint_any_rule_to_biome.rs b/crates/biome_cli/src/execute/migrate/eslint_any_rule_to_biome.rs\nindex 439041656c45..b5c5dd51781f 100644\n--- a/crates/biome_cli/src/execute/migrate/eslint_any_rule_to_biome.rs\n+++ b/crates/biome_cli/src/execute/migrate/eslint_any_rule_to_biome.rs\n@@ -1543,6 +1543,14 @@ pub(crate) fn migrate_eslint_any_rule(\n             let rule = group.use_is_array.get_or_insert(Default::default());\n             rule.set_level(rule_severity.into());\n         }\n+        \"unicorn/no-lonely-if\" => {\n+            if !options.include_nursery {\n+                return false;\n+            }\n+            let group = rules.nursery.get_or_insert_with(Default::default);\n+            let rule = group.use_collapsed_if.get_or_insert(Default::default());\n+            rule.set_level(rule_severity.into());\n+        }\n         \"unicorn/no-static-only-class\" => {\n             let group = rules.complexity.get_or_insert_with(Default::default);\n             let rule = group.no_static_only_class.get_or_insert(Default::default());\ndiff --git a/crates/biome_configuration/src/analyzer/linter/rules.rs b/crates/biome_configuration/src/analyzer/linter/rules.rs\nindex f57c2e14820a..76de7ff26e0a 100644\n--- a/crates/biome_configuration/src/analyzer/linter/rules.rs\n+++ b/crates/biome_configuration/src/analyzer/linter/rules.rs\n@@ -3395,6 +3395,9 @@ pub struct Nursery {\n     #[doc = \"Use at() instead of integer index access.\"]\n     #[serde(skip_serializing_if = \"Option::is_none\")]\n     pub use_at_index: Option<RuleFixConfiguration<biome_js_analyze::options::UseAtIndex>>,\n+    #[doc = \"Enforce using single if instead of nested if clauses.\"]\n+    #[serde(skip_serializing_if = \"Option::is_none\")]\n+    pub use_collapsed_if: Option<RuleFixConfiguration<biome_js_analyze::options::UseCollapsedIf>>,\n     #[doc = \"Enforce declaring components only within modules that export React Components exclusively.\"]\n     #[serde(skip_serializing_if = \"Option::is_none\")]\n     pub use_component_export_only_modules:\n@@ -3488,6 +3491,7 @@ impl Nursery {\n         \"useAdjacentOverloadSignatures\",\n         \"useAriaPropsSupportedByRole\",\n         \"useAtIndex\",\n+        \"useCollapsedIf\",\n         \"useComponentExportOnlyModules\",\n         \"useConsistentCurlyBraces\",\n         \"useConsistentMemberAccessibility\",\n@@ -3528,9 +3532,9 @@ impl Nursery {\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[27]),\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[28]),\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[32]),\n-        RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[36]),\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[37]),\n-        RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[42]),\n+        RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[38]),\n+        RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[43]),\n     ];\n     const ALL_RULES_AS_FILTERS: &'static [RuleFilter<'static>] = &[\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[0]),\n@@ -3578,6 +3582,7 @@ impl Nursery {\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[42]),\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[43]),\n         RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[44]),\n+        RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[45]),\n     ];\n     #[doc = r\" Retrieves the recommended rules\"]\n     pub(crate) fn is_recommended_true(&self) -> bool {\n@@ -3764,61 +3769,66 @@ impl Nursery {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[33]));\n             }\n         }\n-        if let Some(rule) = self.use_component_export_only_modules.as_ref() {\n+        if let Some(rule) = self.use_collapsed_if.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[34]));\n             }\n         }\n-        if let Some(rule) = self.use_consistent_curly_braces.as_ref() {\n+        if let Some(rule) = self.use_component_export_only_modules.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[35]));\n             }\n         }\n-        if let Some(rule) = self.use_consistent_member_accessibility.as_ref() {\n+        if let Some(rule) = self.use_consistent_curly_braces.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[36]));\n             }\n         }\n-        if let Some(rule) = self.use_deprecated_reason.as_ref() {\n+        if let Some(rule) = self.use_consistent_member_accessibility.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[37]));\n             }\n         }\n-        if let Some(rule) = self.use_explicit_type.as_ref() {\n+        if let Some(rule) = self.use_deprecated_reason.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[38]));\n             }\n         }\n-        if let Some(rule) = self.use_guard_for_in.as_ref() {\n+        if let Some(rule) = self.use_explicit_type.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[39]));\n             }\n         }\n-        if let Some(rule) = self.use_import_restrictions.as_ref() {\n+        if let Some(rule) = self.use_guard_for_in.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[40]));\n             }\n         }\n-        if let Some(rule) = self.use_sorted_classes.as_ref() {\n+        if let Some(rule) = self.use_import_restrictions.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[41]));\n             }\n         }\n-        if let Some(rule) = self.use_strict_mode.as_ref() {\n+        if let Some(rule) = self.use_sorted_classes.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[42]));\n             }\n         }\n-        if let Some(rule) = self.use_trim_start_end.as_ref() {\n+        if let Some(rule) = self.use_strict_mode.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[43]));\n             }\n         }\n-        if let Some(rule) = self.use_valid_autocomplete.as_ref() {\n+        if let Some(rule) = self.use_trim_start_end.as_ref() {\n             if rule.is_enabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[44]));\n             }\n         }\n+        if let Some(rule) = self.use_valid_autocomplete.as_ref() {\n+            if rule.is_enabled() {\n+                index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[45]));\n+            }\n+        }\n         index_set\n     }\n     pub(crate) fn get_disabled_rules(&self) -> FxHashSet<RuleFilter<'static>> {\n@@ -3993,61 +4003,66 @@ impl Nursery {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[33]));\n             }\n         }\n-        if let Some(rule) = self.use_component_export_only_modules.as_ref() {\n+        if let Some(rule) = self.use_collapsed_if.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[34]));\n             }\n         }\n-        if let Some(rule) = self.use_consistent_curly_braces.as_ref() {\n+        if let Some(rule) = self.use_component_export_only_modules.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[35]));\n             }\n         }\n-        if let Some(rule) = self.use_consistent_member_accessibility.as_ref() {\n+        if let Some(rule) = self.use_consistent_curly_braces.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[36]));\n             }\n         }\n-        if let Some(rule) = self.use_deprecated_reason.as_ref() {\n+        if let Some(rule) = self.use_consistent_member_accessibility.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[37]));\n             }\n         }\n-        if let Some(rule) = self.use_explicit_type.as_ref() {\n+        if let Some(rule) = self.use_deprecated_reason.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[38]));\n             }\n         }\n-        if let Some(rule) = self.use_guard_for_in.as_ref() {\n+        if let Some(rule) = self.use_explicit_type.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[39]));\n             }\n         }\n-        if let Some(rule) = self.use_import_restrictions.as_ref() {\n+        if let Some(rule) = self.use_guard_for_in.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[40]));\n             }\n         }\n-        if let Some(rule) = self.use_sorted_classes.as_ref() {\n+        if let Some(rule) = self.use_import_restrictions.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[41]));\n             }\n         }\n-        if let Some(rule) = self.use_strict_mode.as_ref() {\n+        if let Some(rule) = self.use_sorted_classes.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[42]));\n             }\n         }\n-        if let Some(rule) = self.use_trim_start_end.as_ref() {\n+        if let Some(rule) = self.use_strict_mode.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[43]));\n             }\n         }\n-        if let Some(rule) = self.use_valid_autocomplete.as_ref() {\n+        if let Some(rule) = self.use_trim_start_end.as_ref() {\n             if rule.is_disabled() {\n                 index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[44]));\n             }\n         }\n+        if let Some(rule) = self.use_valid_autocomplete.as_ref() {\n+            if rule.is_disabled() {\n+                index_set.insert(RuleFilter::Rule(Self::GROUP_NAME, Self::GROUP_RULES[45]));\n+            }\n+        }\n         index_set\n     }\n     #[doc = r\" Checks if, given a rule name, matches one of the rules contained in this category\"]\n@@ -4220,6 +4235,10 @@ impl Nursery {\n                 .use_at_index\n                 .as_ref()\n                 .map(|conf| (conf.level(), conf.get_options())),\n+            \"useCollapsedIf\" => self\n+                .use_collapsed_if\n+                .as_ref()\n+                .map(|conf| (conf.level(), conf.get_options())),\n             \"useComponentExportOnlyModules\" => self\n                 .use_component_export_only_modules\n                 .as_ref()\ndiff --git a/crates/biome_diagnostics_categories/src/categories.rs b/crates/biome_diagnostics_categories/src/categories.rs\nindex 4d03ac67332e..7a5f6e5341dc 100644\n--- a/crates/biome_diagnostics_categories/src/categories.rs\n+++ b/crates/biome_diagnostics_categories/src/categories.rs\n@@ -188,6 +188,7 @@ define_categories! {\n     \"lint/nursery/useAriaPropsSupportedByRole\": \"https://biomejs.dev/linter/rules/use-aria-props-supported-by-role\",\n     \"lint/nursery/useAtIndex\": \"https://biomejs.dev/linter/rules/use-at-index\",\n     \"lint/nursery/useBiomeSuppressionComment\": \"https://biomejs.dev/linter/rules/use-biome-suppression-comment\",\n+    \"lint/nursery/useCollapsedIf\": \"https://biomejs.dev/linter/rules/use-collapsed-if\",\n     \"lint/nursery/useComponentExportOnlyModules\": \"https://biomejs.dev/linter/rules/use-components-only-module\",\n     \"lint/nursery/useConsistentCurlyBraces\": \"https://biomejs.dev/linter/rules/use-consistent-curly-braces\",\n     \"lint/nursery/useConsistentMemberAccessibility\": \"https://biomejs.dev/linter/rules/use-consistent-member-accessibility\",\ndiff --git a/crates/biome_js_analyze/src/lint/nursery.rs b/crates/biome_js_analyze/src/lint/nursery.rs\nindex 4a3341d01aca..4660799ace74 100644\n--- a/crates/biome_js_analyze/src/lint/nursery.rs\n+++ b/crates/biome_js_analyze/src/lint/nursery.rs\n@@ -27,6 +27,7 @@ pub mod no_useless_string_raw;\n pub mod use_adjacent_overload_signatures;\n pub mod use_aria_props_supported_by_role;\n pub mod use_at_index;\n+pub mod use_collapsed_if;\n pub mod use_component_export_only_modules;\n pub mod use_consistent_curly_braces;\n pub mod use_consistent_member_accessibility;\n@@ -67,6 +68,7 @@ declare_lint_group! {\n             self :: use_adjacent_overload_signatures :: UseAdjacentOverloadSignatures ,\n             self :: use_aria_props_supported_by_role :: UseAriaPropsSupportedByRole ,\n             self :: use_at_index :: UseAtIndex ,\n+            self :: use_collapsed_if :: UseCollapsedIf ,\n             self :: use_component_export_only_modules :: UseComponentExportOnlyModules ,\n             self :: use_consistent_curly_braces :: UseConsistentCurlyBraces ,\n             self :: use_consistent_member_accessibility :: UseConsistentMemberAccessibility ,\ndiff --git a/crates/biome_js_analyze/src/lint/nursery/use_collapsed_if.rs b/crates/biome_js_analyze/src/lint/nursery/use_collapsed_if.rs\nnew file mode 100644\nindex 000000000000..ced96dd5292e\n--- /dev/null\n+++ b/crates/biome_js_analyze/src/lint/nursery/use_collapsed_if.rs\n@@ -0,0 +1,198 @@\n+use biome_analyze::{\n+    context::RuleContext, declare_lint_rule, ActionCategory, Ast, FixKind, Rule, RuleDiagnostic,\n+    RuleSource,\n+};\n+use biome_console::markup;\n+use biome_js_factory::make::{js_logical_expression, parenthesized, token};\n+use biome_js_syntax::parentheses::NeedsParentheses;\n+use biome_js_syntax::{AnyJsStatement, JsIfStatement, T};\n+use biome_rowan::{AstNode, AstNodeList, BatchMutationExt};\n+\n+use crate::JsRuleAction;\n+\n+declare_lint_rule! {\n+    /// Enforce using single `if` instead of nested `if` clauses.\n+    ///\n+    /// If an `if (b)` statement is the only statement in an `if (a)` block, it is often clearer to use an `if (a && b)` form.\n+    ///\n+    /// ## Examples\n+    ///\n+    /// ### Invalid\n+    ///\n+    /// ```js,expect_diagnostic\n+    /// if (condition) {\n+    ///     if (anotherCondition) {\n+    ///         // ...\n+    ///     }\n+    /// }\n+    /// ```\n+    ///\n+    /// ```js,expect_diagnostic\n+    /// if (condition) {\n+    ///     // Comment\n+    ///     if (anotherCondition) {\n+    ///         // ...\n+    ///     }\n+    /// }\n+    /// ```\n+    ///\n+    /// ### Valid\n+    ///\n+    /// ```js\n+    /// if (condition && anotherCondition) {\n+    ///     // ...\n+    /// }\n+    /// ```\n+    ///\n+    /// ```js\n+    /// if (condition) {\n+    ///     if (anotherCondition) {\n+    ///         // ...\n+    ///     }\n+    ///     doSomething();\n+    /// }\n+    /// ```\n+    ///\n+    /// ```js\n+    /// if (condition) {\n+    ///     if (anotherCondition) {\n+    ///         // ...\n+    ///     } else {\n+    ///         // ...\n+    ///     }\n+    /// }\n+    /// ```\n+    ///\n+    pub UseCollapsedIf {\n+        version: \"next\",\n+        name: \"useCollapsedIf\",\n+        language: \"js\",\n+        sources: &[\n+            RuleSource::EslintUnicorn(\"no-lonely-if\"),\n+            RuleSource::Clippy(\"collapsible_if\")\n+        ],\n+        recommended: false,\n+        fix_kind: FixKind::Safe,\n+    }\n+}\n+\n+pub struct RuleState {\n+    parent_if_statement: JsIfStatement,\n+    child_if_statement: JsIfStatement,\n+}\n+\n+impl Rule for UseCollapsedIf {\n+    type Query = Ast<JsIfStatement>;\n+    type State = RuleState;\n+    type Signals = Option<Self::State>;\n+    type Options = ();\n+\n+    fn run(ctx: &RuleContext<Self>) -> Self::Signals {\n+        let if_stmt = ctx.query();\n+        let consequent = if_stmt.consequent().ok()?;\n+\n+        let child_if_statement = match consequent {\n+            // If `consequent` is a `JsBlockStatement` and the block contains only one\n+            // `JsIfStatement`, the child `if` statement should be merged.\n+            AnyJsStatement::JsBlockStatement(parent_block_statement) => {\n+                let statements = parent_block_statement.statements();\n+                if statements.len() != 1 {\n+                    return None;\n+                }\n+\n+                let AnyJsStatement::JsIfStatement(child_if_statement) = statements.first()? else {\n+                    return None;\n+                };\n+\n+                Some(child_if_statement)\n+            }\n+            // If `consequent` is a `JsIfStatement` without any block, it should be merged.\n+            AnyJsStatement::JsIfStatement(child_if_statement) => Some(child_if_statement),\n+            _ => None,\n+        }?;\n+\n+        // It cannot be merged if the child `if` statement has any else clause(s).\n+        if child_if_statement.else_clause().is_some() {\n+            return None;\n+        }\n+\n+        Some(RuleState {\n+            parent_if_statement: if_stmt.clone(),\n+            child_if_statement,\n+        })\n+    }\n+\n+    fn diagnostic(_ctx: &RuleContext<Self>, state: &Self::State) -> Option<RuleDiagnostic> {\n+        Some(RuleDiagnostic::new(\n+            rule_category!(),\n+            state.child_if_statement.syntax().text_range(),\n+            markup! {\n+                \"This \"<Emphasis>\"if\"</Emphasis>\" statement can be collapsed into another \"<Emphasis>\"if\"</Emphasis>\" statement.\"\n+            },\n+        ))\n+    }\n+\n+    fn action(ctx: &RuleContext<Self>, state: &Self::State) -> Option<JsRuleAction> {\n+        let RuleState {\n+            parent_if_statement,\n+            child_if_statement,\n+        } = state;\n+\n+        let parent_consequent = parent_if_statement.consequent().ok()?;\n+        let parent_test = parent_if_statement.test().ok()?;\n+        let child_consequent = child_if_statement.consequent().ok()?;\n+        let child_test = child_if_statement.test().ok()?;\n+\n+        let parent_has_comments = match &parent_consequent {\n+            AnyJsStatement::JsBlockStatement(block_stmt) => {\n+                block_stmt.l_curly_token().ok()?.has_trailing_comments()\n+                    || block_stmt.r_curly_token().ok()?.has_leading_comments()\n+            }\n+            _ => false,\n+        };\n+\n+        let has_comments = parent_has_comments\n+            || child_if_statement.syntax().has_comments_direct()\n+            || child_if_statement\n+                .r_paren_token()\n+                .ok()?\n+                .has_trailing_comments();\n+        if has_comments {\n+            return None;\n+        }\n+\n+        let mut expr =\n+            js_logical_expression(parent_test.clone(), token(T![&&]), child_test.clone());\n+\n+        // Parenthesize arms of the `&&` expression if needed\n+        let left = expr.left().ok()?;\n+        if left.needs_parentheses() {\n+            expr = expr.with_left(parenthesized(left).into());\n+        }\n+\n+        let right = expr.right().ok()?;\n+        if right.needs_parentheses() {\n+            expr = expr.with_right(parenthesized(right).into());\n+        }\n+\n+        // If the inner `if` statement has no block and the statement does not end with semicolon,\n+        // it cannot be fixed automatically because that will break the ASI rule.\n+        if !matches!(&child_consequent, AnyJsStatement::JsBlockStatement(_)) {\n+            let last_token = child_consequent.syntax().last_token()?;\n+            if last_token.kind() != T![;] {\n+                return None;\n+            }\n+        }\n+\n+        let mut mutation = ctx.root().begin();\n+        mutation.replace_node(parent_test, expr.into());\n+        mutation.replace_node(parent_consequent, child_consequent);\n+\n+        Some(JsRuleAction::new(\n+            ActionCategory::QuickFix,\n+            ctx.metadata().applicability(),\n+            markup! { \"Use collapsed \"<Emphasis>\"if\"</Emphasis>\" instead.\" }.to_owned(),\n+            mutation,\n+        ))\n+    }\n+}\ndiff --git a/crates/biome_js_analyze/src/options.rs b/crates/biome_js_analyze/src/options.rs\nindex 93ab2a833591..fc0bd18366de 100644\n--- a/crates/biome_js_analyze/src/options.rs\n+++ b/crates/biome_js_analyze/src/options.rs\n@@ -302,6 +302,8 @@ pub type UseButtonType =\n     <lint::a11y::use_button_type::UseButtonType as biome_analyze::Rule>::Options;\n pub type UseCollapsedElseIf =\n     <lint::style::use_collapsed_else_if::UseCollapsedElseIf as biome_analyze::Rule>::Options;\n+pub type UseCollapsedIf =\n+    <lint::nursery::use_collapsed_if::UseCollapsedIf as biome_analyze::Rule>::Options;\n pub type UseComponentExportOnlyModules = < lint :: nursery :: use_component_export_only_modules :: UseComponentExportOnlyModules as biome_analyze :: Rule > :: Options ;\n pub type UseConsistentArrayType = < lint :: style :: use_consistent_array_type :: UseConsistentArrayType as biome_analyze :: Rule > :: Options ;\n pub type UseConsistentBuiltinInstantiation = < lint :: style :: use_consistent_builtin_instantiation :: UseConsistentBuiltinInstantiation as biome_analyze :: Rule > :: Options ;\ndiff --git a/packages/@biomejs/backend-jsonrpc/src/workspace.ts b/packages/@biomejs/backend-jsonrpc/src/workspace.ts\nindex a38557656a37..a5030b84d2d0 100644\n--- a/packages/@biomejs/backend-jsonrpc/src/workspace.ts\n+++ b/packages/@biomejs/backend-jsonrpc/src/workspace.ts\n@@ -1358,6 +1358,10 @@ export interface Nursery {\n \t * Use at() instead of integer index access.\n \t */\n \tuseAtIndex?: RuleFixConfiguration_for_Null;\n+\t/**\n+\t * Enforce using single if instead of nested if clauses.\n+\t */\n+\tuseCollapsedIf?: RuleFixConfiguration_for_Null;\n \t/**\n \t * Enforce declaring components only within modules that export React Components exclusively.\n \t */\n@@ -2956,6 +2960,7 @@ export type Category =\n \t| \"lint/nursery/useAriaPropsSupportedByRole\"\n \t| \"lint/nursery/useAtIndex\"\n \t| \"lint/nursery/useBiomeSuppressionComment\"\n+\t| \"lint/nursery/useCollapsedIf\"\n \t| \"lint/nursery/useComponentExportOnlyModules\"\n \t| \"lint/nursery/useConsistentCurlyBraces\"\n \t| \"lint/nursery/useConsistentMemberAccessibility\"\ndiff --git a/packages/@biomejs/biome/configuration_schema.json b/packages/@biomejs/biome/configuration_schema.json\nindex 82f908c7d177..419e88edbaa9 100644\n--- a/packages/@biomejs/biome/configuration_schema.json\n+++ b/packages/@biomejs/biome/configuration_schema.json\n@@ -2327,6 +2327,13 @@\n \t\t\t\t\t\t{ \"type\": \"null\" }\n \t\t\t\t\t]\n \t\t\t\t},\n+\t\t\t\t\"useCollapsedIf\": {\n+\t\t\t\t\t\"description\": \"Enforce using single if instead of nested if clauses.\",\n+\t\t\t\t\t\"anyOf\": [\n+\t\t\t\t\t\t{ \"$ref\": \"#/definitions/RuleFixConfiguration\" },\n+\t\t\t\t\t\t{ \"type\": \"null\" }\n+\t\t\t\t\t]\n+\t\t\t\t},\n \t\t\t\t\"useComponentExportOnlyModules\": {\n \t\t\t\t\t\"description\": \"Enforce declaring components only within modules that export React Components exclusively.\",\n \t\t\t\t\t\"anyOf\": [\n", "instance_id": "biomejs__biome-4179", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to implement a linter rule for collapsing nested `if` statements into a single `if` condition using logical operators, inspired by existing rules like `clippy/collapsible_if` and `unicorn/no-lonely-if`. It provides references to the original rules and links to contributing guides, which help in understanding the context. The goal is evident: to enforce a coding style that avoids unnecessary nested `if` statements. However, the problem statement lacks specific details about edge cases, constraints, or exact expectations for the implementation (e.g., how to handle comments, specific syntax variations, or performance considerations). While the code changes provide examples of valid and invalid cases, the problem description itself does not explicitly outline these, leaving minor ambiguities that could be clarified with more detailed requirements or test cases. Hence, it is rated as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving multiple files (configuration, rule definitions, and implementation logic) within the BiomeJS codebase, a linter/analyzer framework for JavaScript. The changes include adding a new rule (`useCollapsedIf`) with logic to detect and fix nested `if` statements, updating configuration schemas, and mapping ESLint rules to Biome rules. This requires understanding the interaction between different modules, such as rule declaration, diagnostics, and configuration handling.\n\nSecond, the number of technical concepts involved is notable. The implementer needs to be familiar with Rust (the language used in the codebase), specifically with procedural macros for rule declaration (`declare_lint_rule!`), AST manipulation using `biome_js_syntax`, and mutation of syntax trees for quick fixes. Additionally, knowledge of linter design patterns (e.g., rule context, diagnostics, and actions) and JavaScript syntax nuances (e.g., handling parentheses, ASI rules) is required. While these concepts are not extremely advanced, they demand a solid understanding of both Rust and linter internals.\n\nThird, the problem involves handling edge cases and error conditions, as seen in the implementation. The code checks for comments, else clauses, and ASI (Automatic Semicolon Insertion) issues to ensure safe fixes, which adds complexity to the logic. However, these edge cases are not overly intricate compared to more systemic challenges.\n\nFinally, the impact on the system's architecture is minimal; this is an additive feature (a new rule) rather than a refactoring of core components. The amount of code change is moderate, with a significant portion being boilerplate updates to configuration and rule lists, while the core logic in `use_collapsed_if.rs` is focused and contained.\n\nGiven these considerations, I assign a difficulty score of 0.55, placing it in the medium range. It requires understanding multiple concepts and making targeted changes across files, but it does not demand deep architectural changes or highly advanced technical expertise beyond what a moderately experienced Rust developer familiar with linter tools could handle.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add default non-root user to UI Dockerfile\nWe do this for the gateway's Dockerfile for security purposes:\n\n```Dockerfile\n...\n\nFROM base AS gateway\n\nRUN useradd -m -s /bin/bash gateway\n\nUSER gateway\n\n...\n```\n", "patch": "diff --git a/ui/Dockerfile b/ui/Dockerfile\nindex 3623f6ed4..fcdd3cb50 100644\n--- a/ui/Dockerfile\n+++ b/ui/Dockerfile\n@@ -1,26 +1,57 @@\n+# ========== base ==========\n+\n FROM node:20-alpine AS base\n+\n RUN npm install -g pnpm\n \n+# ========== development-dependencies-env ==========\n+\n FROM base AS development-dependencies-env\n+\n COPY . /app\n+\n WORKDIR /app\n+\n RUN pnpm install --frozen-lockfile\n \n+# ========== production-dependencies-env ==========\n+\n FROM base AS production-dependencies-env\n+\n COPY ./package.json pnpm-lock.yaml /app/\n+\n WORKDIR /app\n+\n RUN pnpm install --frozen-lockfile --prod\n \n+# ========== build-env ==========\n+\n FROM base AS build-env\n+\n COPY . /app/\n+\n COPY --from=development-dependencies-env /app/node_modules /app/node_modules\n+\n WORKDIR /app\n+\n RUN pnpm run build\n \n-FROM base\n+# ========== ui ==========\n+\n+FROM base AS ui\n+\n+RUN adduser -D -s /bin/sh ui\n+\n+USER ui\n+\n COPY ./package.json pnpm-lock.yaml /app/\n+\n COPY --from=production-dependencies-env /app/node_modules /app/node_modules\n+\n COPY --from=build-env /app/build /app/build\n+\n WORKDIR /app\n+\n EXPOSE 4000\n+\n CMD [\"pnpm\", \"run\", \"start\"]\n", "instance_id": "tensorzero__tensorzero-904", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to add a default non-root user to the UI Dockerfile for security purposes, mirroring the approach used in the gateway Dockerfile. It provides a snippet of the desired change, which helps in understanding the goal. However, it lacks explicit details about the specific user permissions, potential constraints, or reasons behind choosing specific user settings (e.g., why `/bin/sh` over `/bin/bash` for the UI user). Additionally, there are no mentions of edge cases or potential issues that might arise from running as a non-root user in the context of the UI application. While the intent is clear, these minor missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range. The change involves a straightforward modification to a single file, the UI Dockerfile, by adding a non-root user and switching to it using standard Docker commands (`adduser` and `USER`). The scope of the code change is minimal, with no impact on the broader codebase or system architecture. It does not require understanding complex interactions between modules or deep knowledge of the application logic. The technical concepts involved are basic Docker user management, which is a fundamental skill for anyone familiar with containerization. There are no significant edge cases or error handling requirements mentioned or implied in the problem statement or code changes, as the modification is isolated to the Dockerfile setup. Overall, this is a very easy task that requires only basic knowledge and minimal effort to implement.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Require a \"Function\" selection before \"Metric\" selection in SFT UI and filter irrelevant metrics\n1) The \"Variant\" dropdown is disabled until we select a function. The same behavior should apply to \"Metric\".\r\n\r\n2) Remove any metrics that have zero relevant feedback from the dropdown (i.e. Curated Inferences = 0). This might require a more advanced ClickHouse query.\n", "patch": "diff --git a/ui/app/routes.ts b/ui/app/routes.ts\nindex b2139f463..6efd70394 100644\n--- a/ui/app/routes.ts\n+++ b/ui/app/routes.ts\n@@ -10,6 +10,10 @@ export default [\n     \"api/curated_inferences/count\",\n     \"routes/api/curated_inferences/count.route.ts\",\n   ),\n+  route(\n+    \"api/function/:function_name/feedback_counts\",\n+    \"routes/api/function/$function_name/feedback_counts.route.ts\",\n+  ),\n   route(\n     \"observability/inferences\",\n     \"routes/observability/inferences/route.tsx\",\ndiff --git a/ui/app/routes/api/function/$function_name/feedback_counts.route.ts b/ui/app/routes/api/function/$function_name/feedback_counts.route.ts\nnew file mode 100644\nindex 000000000..d76280b92\n--- /dev/null\n+++ b/ui/app/routes/api/function/$function_name/feedback_counts.route.ts\n@@ -0,0 +1,37 @@\n+import type { LoaderFunctionArgs } from \"react-router\";\n+import { getConfig } from \"~/utils/config/index.server\";\n+import { getInferenceTableName } from \"~/utils/clickhouse/common\";\n+import {\n+  queryMetricsWithFeedback,\n+  type MetricsWithFeedbackData,\n+} from \"~/utils/clickhouse/feedback\";\n+\n+export async function loader({\n+  params,\n+}: LoaderFunctionArgs): Promise<Response> {\n+  const functionName = params.function_name;\n+\n+  if (!functionName) {\n+    return Response.json({ metrics: [] } as MetricsWithFeedbackData);\n+  }\n+\n+  try {\n+    const config = await getConfig();\n+    const functionConfig = config.functions[functionName];\n+    const inferenceTable = getInferenceTableName(functionConfig);\n+\n+    const result = await queryMetricsWithFeedback({\n+      function_name: functionName,\n+      inference_table: inferenceTable,\n+      metrics: config.metrics,\n+    });\n+\n+    return Response.json(result);\n+  } catch (error) {\n+    console.error(\"Error fetching metrics with feedback:\", error);\n+    throw new Response(\"Error fetching metrics with feedback\", {\n+      status: 500,\n+      statusText: \"Failed to fetch metrics with feedback\",\n+    });\n+  }\n+}\ndiff --git a/ui/app/routes/optimization/supervised-fine-tuning/MetricSelector.tsx b/ui/app/routes/optimization/supervised-fine-tuning/MetricSelector.tsx\nindex 03d4da33a..df6c04954 100644\n--- a/ui/app/routes/optimization/supervised-fine-tuning/MetricSelector.tsx\n+++ b/ui/app/routes/optimization/supervised-fine-tuning/MetricSelector.tsx\n@@ -1,4 +1,4 @@\n-import type { Control } from \"react-hook-form\";\n+import { useWatch, useFormContext, type Control } from \"react-hook-form\";\n import type { SFTFormValues } from \"./types\";\n import { Config } from \"~/utils/config\";\n import { FormField, FormItem, FormLabel } from \"~/components/ui/form\";\n@@ -12,6 +12,10 @@ import {\n import { Skeleton } from \"~/components/ui/skeleton\";\n import { Input } from \"~/components/ui/input\";\n import { MetricBadges } from \"~/components/metric/MetricBadges\";\n+import { useEffect, useMemo } from \"react\";\n+import { useFetcher } from \"react-router\";\n+import type { MetricsWithFeedbackData } from \"~/utils/clickhouse/feedback\";\n+import { Badge } from \"~/components/ui/badge\";\n \n type MetricSelectorProps = {\n   control: Control<SFTFormValues>;\n@@ -26,6 +30,37 @@ export function MetricSelector({\n   curatedInferenceCount,\n   config,\n }: MetricSelectorProps) {\n+  const metricsFetcher = useFetcher<MetricsWithFeedbackData>();\n+  const { getValues, setValue } = useFormContext<SFTFormValues>();\n+\n+  const functionValue = useWatch({\n+    control,\n+    name: \"function\",\n+  });\n+\n+  useEffect(() => {\n+    if (functionValue) {\n+      metricsFetcher.load(\n+        `/api/function/${encodeURIComponent(functionValue)}/feedback_counts`,\n+      );\n+    }\n+  }, [functionValue]);\n+\n+  const validMetrics = useMemo(() => {\n+    if (!metricsFetcher.data) return new Set<string>();\n+    return new Set(metricsFetcher.data.metrics.map((m) => m.metric_name));\n+  }, [metricsFetcher.data]);\n+\n+  const isLoading = metricsFetcher.state === \"loading\";\n+\n+  // Reset metric value if the selected function does not have the previously selected metric\n+  useEffect(() => {\n+    const metricValue = getValues(\"metric\");\n+    if (functionValue && metricValue && !validMetrics.has(metricValue)) {\n+      setValue(\"metric\", null);\n+    }\n+  }, [functionValue, validMetrics, getValues, setValue]);\n+\n   return (\n     <FormField\n       control={control}\n@@ -41,9 +76,14 @@ export function MetricSelector({\n                   field.onChange(metricValue);\n                 }}\n                 value={field.value ?? \"none\"}\n+                disabled={!functionValue || isLoading}\n               >\n                 <SelectTrigger>\n-                  <SelectValue placeholder=\"Select a metric\" />\n+                  <SelectValue\n+                    placeholder={\n+                      isLoading ? \"Loading metrics...\" : \"Select a metric\"\n+                    }\n+                  />\n                 </SelectTrigger>\n                 <SelectContent>\n                   <SelectItem value=\"none\">\n@@ -51,16 +91,29 @@ export function MetricSelector({\n                       <span>None</span>\n                     </div>\n                   </SelectItem>\n-                  {Object.entries(config.metrics).map(([name, metric]) => (\n-                    <SelectItem key={name} value={name}>\n-                      <div className=\"flex w-full items-center justify-between\">\n-                        <span>{name}</span>\n-                        <div className=\"ml-2\">\n-                          <MetricBadges metric={metric} />\n-                        </div>\n-                      </div>\n-                    </SelectItem>\n-                  ))}\n+                  {Object.entries(config.metrics)\n+                    .filter(([name]) => validMetrics.has(name))\n+                    .map(([name, metric]) => {\n+                      const metricFeedback = metricsFetcher.data?.metrics.find(\n+                        (m) => m.metric_name === name,\n+                      );\n+\n+                      return (\n+                        <SelectItem key={name} value={name}>\n+                          <div className=\"flex w-full items-center justify-between\">\n+                            <span>{name}</span>\n+                            <div className=\"ml-2 flex items-center gap-2\">\n+                              {metricFeedback && (\n+                                <Badge className=\"bg-gray-200 text-gray-800 dark:bg-gray-900 dark:text-gray-300\">\n+                                  Count: {metricFeedback.feedback_count}\n+                                </Badge>\n+                              )}\n+                              <MetricBadges metric={metric} />\n+                            </div>\n+                          </div>\n+                        </SelectItem>\n+                      );\n+                    })}\n                 </SelectContent>\n               </Select>\n \ndiff --git a/ui/app/utils/clickhouse/curation.ts b/ui/app/utils/clickhouse/curation.ts\nindex 93497719b..417aebab0 100644\n--- a/ui/app/utils/clickhouse/curation.ts\n+++ b/ui/app/utils/clickhouse/curation.ts\n@@ -11,7 +11,9 @@ import {\n import { clickhouseClient } from \"./common\";\n import type { FunctionConfig } from \"../config/function\";\n \n-function getInferenceJoinKey(metric_config: MetricConfig): InferenceJoinKey {\n+export function getInferenceJoinKey(\n+  metric_config: MetricConfig,\n+): InferenceJoinKey {\n   if (\"level\" in metric_config) {\n     switch (metric_config.level) {\n       case \"inference\":\ndiff --git a/ui/app/utils/clickhouse/feedback.ts b/ui/app/utils/clickhouse/feedback.ts\nindex 5021bb615..c15a4e286 100644\n--- a/ui/app/utils/clickhouse/feedback.ts\n+++ b/ui/app/utils/clickhouse/feedback.ts\n@@ -1,7 +1,9 @@\n import { z } from \"zod\";\n import type { TableBounds } from \"./common\";\n import { data } from \"react-router\";\n-import { clickhouseClient } from \"./common\";\n+import { clickhouseClient, InferenceJoinKey } from \"./common\";\n+import type { MetricConfig } from \"~/utils/config/metric\";\n+import { getInferenceJoinKey } from \"~/utils/clickhouse/curation\";\n \n export const booleanMetricFeedbackRowSchema = z.object({\n   type: z.literal(\"boolean\"),\n@@ -742,3 +744,175 @@ export async function countFeedbackByTargetId(\n     booleanMetrics + commentFeedback + demonstrationFeedback + floatMetrics\n   );\n }\n+\n+export const metricsWithFeedbackRowSchema = z\n+  .object({\n+    function_name: z.string(),\n+    metric_name: z.string(),\n+    metric_type: z.enum([\"boolean\", \"float\", \"demonstration\"]),\n+    feedback_count: z.number(),\n+  })\n+  .strict();\n+\n+export const metricsWithFeedbackDataSchema = z\n+  .object({\n+    metrics: z.array(metricsWithFeedbackRowSchema),\n+  })\n+  .strict();\n+\n+export type MetricsWithFeedbackRow = z.infer<\n+  typeof metricsWithFeedbackRowSchema\n+>;\n+export type MetricsWithFeedbackData = z.infer<\n+  typeof metricsWithFeedbackDataSchema\n+>;\n+\n+export async function queryMetricsWithFeedback(params: {\n+  function_name: string;\n+  inference_table: string;\n+  metrics: Record<string, MetricConfig>;\n+}): Promise<MetricsWithFeedbackData> {\n+  const { function_name, inference_table, metrics } = params;\n+\n+  const inferenceMetrics = Object.entries(metrics)\n+    .filter(([, metric]) => {\n+      try {\n+        return getInferenceJoinKey(metric) === InferenceJoinKey.ID;\n+      } catch {\n+        return false;\n+      }\n+    })\n+    .map(([name]) => name);\n+\n+  const episodeMetrics = Object.entries(metrics)\n+    .filter(([, metric]) => {\n+      try {\n+        return getInferenceJoinKey(metric) === InferenceJoinKey.EPISODE_ID;\n+      } catch {\n+        return false;\n+      }\n+    })\n+    .map(([name]) => name);\n+\n+  const idInClause =\n+    inferenceMetrics.length > 0\n+      ? `IN ('${inferenceMetrics.join(\"','\")}')`\n+      : `= ''`;\n+\n+  const episodeIdInClause =\n+    episodeMetrics.length > 0 ? `IN ('${episodeMetrics.join(\"','\")}')` : `= ''`;\n+\n+  const query = `\n+    WITH\n+    boolean_inference_metrics AS (\n+      SELECT\n+        i.function_name,\n+        bmf.metric_name,\n+        'boolean' as metric_type,\n+        COUNT(DISTINCT i.id) as feedback_count\n+      FROM tensorzero.${inference_table} i\n+      JOIN tensorzero.BooleanMetricFeedback bmf ON bmf.target_id = i.id\n+      WHERE i.function_name = {function_name:String}\n+        AND bmf.metric_name ${idInClause}\n+      GROUP BY i.function_name, bmf.metric_name\n+      HAVING feedback_count > 0\n+    ),\n+\n+    boolean_episode_metrics AS (\n+      SELECT\n+        i.function_name,\n+        bmf.metric_name,\n+        'boolean' as metric_type,\n+        COUNT(DISTINCT i.id) as feedback_count\n+      FROM tensorzero.${inference_table} i\n+      JOIN tensorzero.BooleanMetricFeedback bmf ON bmf.target_id = i.episode_id\n+      WHERE i.function_name = {function_name:String}\n+        AND bmf.metric_name ${episodeIdInClause}\n+      GROUP BY i.function_name, bmf.metric_name\n+      HAVING feedback_count > 0\n+    ),\n+\n+    float_inference_metrics AS (\n+      SELECT\n+        i.function_name,\n+        fmf.metric_name,\n+        'float' as metric_type,\n+        COUNT(DISTINCT i.id) as feedback_count\n+      FROM tensorzero.${inference_table} i\n+      JOIN tensorzero.FloatMetricFeedback fmf ON fmf.target_id = i.id\n+      WHERE i.function_name = {function_name:String}\n+        AND fmf.metric_name ${idInClause}\n+      GROUP BY i.function_name, fmf.metric_name\n+      HAVING feedback_count > 0\n+    ),\n+\n+    float_episode_metrics AS (\n+      SELECT\n+        i.function_name,\n+        fmf.metric_name,\n+        'float' as metric_type,\n+        COUNT(DISTINCT i.id) as feedback_count\n+      FROM tensorzero.${inference_table} i\n+      JOIN tensorzero.FloatMetricFeedback fmf ON fmf.target_id = i.episode_id\n+      WHERE i.function_name = {function_name:String}\n+        AND fmf.metric_name ${episodeIdInClause}\n+      GROUP BY i.function_name, fmf.metric_name\n+      HAVING feedback_count > 0\n+    ),\n+\n+    demo_metrics AS (\n+      SELECT\n+        i.function_name,\n+        'demonstration' as metric_name,\n+        'demonstration' as metric_type,\n+        COUNT(DISTINCT i.id) as feedback_count\n+      FROM tensorzero.${inference_table} i\n+      JOIN tensorzero.DemonstrationFeedback df ON df.inference_id = i.id\n+      WHERE i.function_name = {function_name:String}\n+      GROUP BY i.function_name\n+      HAVING feedback_count > 0\n+    )\n+\n+    SELECT\n+      function_name,\n+      metric_name,\n+      metric_type,\n+      toString(feedback_count) as feedback_count\n+    FROM (\n+      SELECT * FROM boolean_inference_metrics\n+      UNION ALL\n+      SELECT * FROM boolean_episode_metrics\n+      UNION ALL\n+      SELECT * FROM float_inference_metrics\n+      UNION ALL\n+      SELECT * FROM float_episode_metrics\n+      UNION ALL\n+      SELECT * FROM demo_metrics\n+    )\n+    ORDER BY metric_type, metric_name`;\n+\n+  try {\n+    const resultSet = await clickhouseClient.query({\n+      query,\n+      format: \"JSONEachRow\",\n+      query_params: { function_name },\n+    });\n+\n+    const rawMetrics = (await resultSet.json()) as Array<{\n+      function_name: string;\n+      metric_name: string;\n+      metric_type: \"boolean\" | \"float\" | \"demonstration\";\n+      feedback_count: string;\n+    }>;\n+\n+    const validMetrics = rawMetrics.map((metric) => ({\n+      ...metric,\n+      feedback_count: Number(metric.feedback_count),\n+    }));\n+\n+    return { metrics: validMetrics };\n+  } catch (error) {\n+    console.error(\"Error fetching metrics with feedback:\", error);\n+    throw data(\"Error fetching metrics with feedback\", { status: 500 });\n+  }\n+}\n", "instance_id": "tensorzero__tensorzero-770", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent: it requires enforcing a dependency between \"Function\" and \"Metric\" selections in a UI (disabling \"Metric\" until \"Function\" is selected) and filtering out metrics with zero relevant feedback. The goals are understandable, and the expected behavior is outlined. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes \"relevant feedback\" beyond a mention of \"Curated Inferences = 0,\" nor does it specify how the UI should handle cases where a previously selected metric becomes invalid after a function change. Additionally, there are no examples or edge cases provided (e.g., what happens if no metrics have feedback for a selected function?). While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files and modules, including adding a new API route, implementing a complex ClickHouse query for feedback counts, and modifying the UI logic to handle dynamic metric filtering and selection dependencies. This requires understanding interactions between backend data fetching (ClickHouse queries), API design, and frontend React components (using hooks like useEffect and useMemo). Second, the technical concepts involved include database querying (ClickHouse with joins and unions), React state management, and API integration with React Router's useFetcher. The ClickHouse query is moderately complex, as it handles multiple metric types and feedback counts with filtering logic. Third, while edge cases are not explicitly mentioned in the problem statement, the code changes address some implicitly (e.g., resetting the metric selection if it becomes invalid for a new function), and error handling is implemented for API failures. However, the problem does not appear to impact core system architecture or require advanced domain-specific knowledge beyond typical full-stack development skills. Overall, this problem requires a solid understanding of multiple concepts and careful implementation across several files, justifying a score of 0.55 in the medium difficulty range.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`stor update` fails with single-quote (apostrophe) in string (but `stor insert` works)\n### Describe the bug\n\nAttempting to use a string in the `stor update --update-record` with an embedded single quote fails.  However, inserting that same record into a row entry works fine.\n\n### How to reproduce\n\n```nushell\nstor reset   # Optional\nstor create --table-name test_table --columns { id: int, value: str }\n# => \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n# => \u2502 test_table \u2502 [list 0 items] \u2502\n# => \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n# We can insert new row with a string with a single quote just fine\nstor insert -t test_table --data-record {\n  id: 1\n  value: \"Doesn't fail\"\n}\n# => \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n# => \u2502            \u2502 \u256d\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n# => \u2502 test_table \u2502 \u2502 # \u2502 id \u2502    value     \u2502 \u2502\n# => \u2502            \u2502 \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n# => \u2502            \u2502 \u2502 0 \u2502  1 \u2502 Doesn't fail \u2502 \u2502\n# => \u2502            \u2502 \u2570\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n# => \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n# We can update the row as long as it doesn't have a single quote\nstor update -t test_table --where-clause 'id = 1' --update-record {\n  id: 1\n  value: \"This also works\"\n}\n# => \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n# => \u2502            \u2502 \u256d\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n# => \u2502 test_table \u2502 \u2502 # \u2502 id \u2502      value      \u2502 \u2502\n# => \u2502            \u2502 \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n# => \u2502            \u2502 \u2502 0 \u2502  1 \u2502 This also works \u2502 \u2502\n# => \u2502            \u2502 \u2570\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n# => \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n# But an update with a single-quote in the string will fail\nstor update -t test_table --where-clause 'id = 1' --update-record {\n  id: 1\n  value: \"But this doesn't work\"\n}\n# => Error:\n# =>   \u00d7 Failed to open SQLite connection in memory from update\n# =>    \u256d\u2500[source:1:1]\n# =>  1 \u2502 nu\n# =>    \u00b7 \u25b2\n# =>    \u00b7 \u2570\u2500\u2500 near \"t\": syntax error in UPDATE test_table SET id = 1, value = 'But this doesn't work' WHERE id = 1 at offset 54\n# =>    \u2570\u2500\u2500\u2500\u2500\n```\n\nNote also that the span is incorrect - It points to a span start of 0 apparently, meaning it loads `view files | get 0` (a.k.a. the `source` entry)\n\n### Expected behavior\n\nRecord is updated properly.\n\n### Configuration\n\n| key                | value                                    |\n| ------------------ | ---------------------------------------- |\n| version            | 0.101.1                                  |\n| major              | 0                                        |\n| minor              | 101                                      |\n| patch              | 1                                        |\n| branch             | main                                     |\n| commit_hash        | 299453ecb7bff2be8fbb3ca6402ce743a1060d03 |\n| build_os           | linux-x86_64                             |\n| build_target       | x86_64-unknown-linux-gnu                 |\n| rust_version       | rustc 1.82.0 (f6e511eec 2024-10-15)      |\n| rust_channel       | 1.82.0-x86_64-unknown-linux-gnu          |\n| cargo_version      | cargo 1.82.0 (8f40fc59f 2024-08-21)      |\n| build_time         | 2025-01-24 09:22:03 -05:00               |\n| build_rust_channel | release                                  |\n| allocator          | mimalloc                                 |\n| features           | default, sqlite, trash                   |\n| installed_plugins  | formats 0.101.0, gstat 0.101.1           |\n", "patch": "diff --git a/crates/nu-command/src/stor/insert.rs b/crates/nu-command/src/stor/insert.rs\nindex 94b5bdf940130..4a2ebf492893b 100644\n--- a/crates/nu-command/src/stor/insert.rs\n+++ b/crates/nu-command/src/stor/insert.rs\n@@ -161,27 +161,23 @@ fn process(\n     let new_table_name = table_name.unwrap_or(\"table\".into());\n \n     if let Ok(conn) = db.open_connection() {\n-        let mut create_stmt = format!(\"INSERT INTO {} ( \", new_table_name);\n+        let mut create_stmt = format!(\"INSERT INTO {} (\", new_table_name);\n+        let mut column_placeholders: Vec<String> = Vec::new();\n+\n         let cols = record.columns();\n         cols.for_each(|col| {\n-            create_stmt.push_str(&format!(\"{}, \", col));\n+            column_placeholders.push(col.to_string());\n         });\n-        if create_stmt.ends_with(\", \") {\n-            create_stmt.pop();\n-            create_stmt.pop();\n-        }\n+\n+        create_stmt.push_str(&column_placeholders.join(\", \"));\n \n         // Values are set as placeholders.\n-        create_stmt.push_str(\") VALUES ( \");\n+        create_stmt.push_str(\") VALUES (\");\n+        let mut value_placeholders: Vec<String> = Vec::new();\n         for (index, _) in record.columns().enumerate() {\n-            create_stmt.push_str(&format!(\"?{}, \", index + 1));\n+            value_placeholders.push(format!(\"?{}\", index + 1));\n         }\n-\n-        if create_stmt.ends_with(\", \") {\n-            create_stmt.pop();\n-            create_stmt.pop();\n-        }\n-\n+        create_stmt.push_str(&value_placeholders.join(\", \"));\n         create_stmt.push(')');\n \n         // dbg!(&create_stmt);\ndiff --git a/crates/nu-command/src/stor/update.rs b/crates/nu-command/src/stor/update.rs\nindex a158e0f7d063d..8a77f41d51071 100644\n--- a/crates/nu-command/src/stor/update.rs\n+++ b/crates/nu-command/src/stor/update.rs\n@@ -1,6 +1,7 @@\n-use crate::database::{SQLiteDatabase, MEMORY_DB};\n+use crate::database::{values_to_sql, SQLiteDatabase, MEMORY_DB};\n use nu_engine::command_prelude::*;\n use nu_protocol::Signals;\n+use rusqlite::params_from_iter;\n \n #[derive(Clone)]\n pub struct StorUpdate;\n@@ -163,36 +164,12 @@ fn process(\n         let mut update_stmt = format!(\"UPDATE {} \", new_table_name);\n \n         update_stmt.push_str(\"SET \");\n-        let vals = record.iter();\n-        vals.for_each(|(key, val)| match val {\n-            Value::Int { val, .. } => {\n-                update_stmt.push_str(&format!(\"{} = {}, \", key, val));\n-            }\n-            Value::Float { val, .. } => {\n-                update_stmt.push_str(&format!(\"{} = {}, \", key, val));\n-            }\n-            Value::String { val, .. } => {\n-                update_stmt.push_str(&format!(\"{} = '{}', \", key, val));\n-            }\n-            Value::Date { val, .. } => {\n-                update_stmt.push_str(&format!(\"{} = '{}', \", key, val));\n-            }\n-            Value::Bool { val, .. } => {\n-                update_stmt.push_str(&format!(\"{} = {}, \", key, val));\n-            }\n-            _ => {\n-                // return Err(ShellError::UnsupportedInput {\n-                //     msg: format!(\"{} is not a valid datepart, expected one of year, month, day, hour, minute, second, millisecond, microsecond, nanosecond\", part.item),\n-                //     input: \"value originates from here\".to_string(),\n-                //     msg_span: span,\n-                //     input_span: val.span(),\n-                // });\n-            }\n-        });\n-        if update_stmt.ends_with(\", \") {\n-            update_stmt.pop();\n-            update_stmt.pop();\n+        let mut placeholders: Vec<String> = Vec::new();\n+\n+        for (index, (key, _)) in record.iter().enumerate() {\n+            placeholders.push(format!(\"{} = ?{}\", key, index + 1));\n         }\n+        update_stmt.push_str(&placeholders.join(\", \"));\n \n         // Yup, this is a bit janky, but I'm not sure a better way to do this without having\n         // --and and --or flags as well as supporting ==, !=, <>, is null, is not null, etc.\n@@ -202,7 +179,10 @@ fn process(\n         }\n         // dbg!(&update_stmt);\n \n-        conn.execute(&update_stmt, [])\n+        // Get the params from the passed values\n+        let params = values_to_sql(record.values().cloned())?;\n+\n+        conn.execute(&update_stmt, params_from_iter(params))\n             .map_err(|err| ShellError::GenericError {\n                 error: \"Failed to open SQLite connection in memory from update\".into(),\n                 msg: err.to_string(),\n", "instance_id": "nushell__nushell-14921", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear, earning a score of 2. The goal is well-defined: to fix a bug in the `stor update` command where strings with single quotes cause a failure, while `stor insert` works fine. The input, output, and expected behavior are demonstrated through detailed reproduction steps and examples in the form of command-line interactions. The error message and its incorrect span are also mentioned, providing additional context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases beyond single quotes (e.g., other special characters or SQL injection concerns). Additionally, while the reproduction steps are clear, there is no mention of constraints or specific requirements for the fix (e.g., performance considerations or compatibility with other features). These minor gaps prevent it from being comprehensive, but the statement is still actionable and understandable.", "difficulty_explanation": "I assign a difficulty score of 0.35, placing this problem in the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are relatively localized, primarily affecting the `stor update` module and partially the `stor insert` module for consistency in implementation style. The modifications involve refactoring how SQL statements are constructed in the `update.rs` file to use parameterized queries instead of direct string interpolation, which is a straightforward change. The diff shows a moderate amount of code change (around 30-40 lines), but it does not impact the broader system architecture or require extensive cross-module understanding. The changes are mostly self-contained within the SQL query-building logic.\n\n2. **Number of Technical Concepts**: The solution requires understanding a few key concepts: SQL query construction, parameterized queries to prevent injection (using `rusqlite::params_from_iter`), and basic Rust syntax for string manipulation and iteration. These are not particularly complex for a developer familiar with Rust and database interactions. The problem does not involve advanced algorithms, design patterns, or domain-specific knowledge beyond basic SQLite usage.\n\n3. **Potential Edge Cases and Error Handling**: The primary edge case addressed is handling single quotes in strings, which the fix resolves by using parameterized queries. The problem statement does not mention other edge cases (e.g., other special characters, empty strings, or large data inputs), and the code changes do not introduce additional error handling beyond what is necessary for the SQL execution. The complexity of edge cases is low, as parameterized queries inherently handle most escaping issues.\n\n4. **Overall Complexity**: The fix involves a simple but important bug resolution\u2014moving from unsafe string concatenation to parameterized queries. While this requires understanding the risk of SQL injection and the correct use of the `rusqlite` library, it is not a deep or intricate change. It is a standard best practice in database programming and does not require advanced debugging or architectural redesign.\n\nGiven these factors, the problem is on the higher end of \"Easy\" due to the need to understand parameterized queries and ensure correctness in SQL statement construction. However, it does not reach \"Medium\" difficulty (0.4-0.6) because it lacks complexity in terms of codebase impact, advanced concepts, or extensive edge case handling. A developer with basic to intermediate Rust and database experience should be able to implement this fix with minimal effort.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Re-export winit\nThere are a lot of public exposed APIs that accept and return winit types (ie):\r\n\r\nhttps://github.com/glium/glium/blob/1676f0de6dddfa294f834613372f5096a429acd8/src/backend/glutin/mod.rs#L333\r\n\r\nAlso, most of the tutorials make heavy use of the winit APIs.\r\n\r\nCurrently this requires users to figure out which version of winit is currently being used, and make sure their winit dependency stays in sync with the glium dependency, otherwise it will be a breaking change if your API expects a 0.29 EventLoop when the user is still using a 0.28 EventLoop.\r\n\r\nIt seems like you are aware of this, and are bumping semver minor to signify pre-1.0.0 breaking changes whenever you bump the winit dependency to a breaking change.\r\n\r\nSince that is already a requirement on your library, you might as well re-export winit explicitly and change the tutorials to recommend using the re-export.\n", "patch": "diff --git a/book/tuto-01-getting-started.md b/book/tuto-01-getting-started.md\nindex d01de87748..41d3eba362 100644\n--- a/book/tuto-01-getting-started.md\n+++ b/book/tuto-01-getting-started.md\n@@ -38,9 +38,11 @@ Initializing a simple OpenGL window with the default winit/glutin backend can be\n \n This will open a new window, register it with the given event_loop and create a (glutin) OpenGL context and glium Display while finally returning both the window and display to you.\n \n+The following examples require the `simple_window_builder` feature, and provide a re-export of winit to prevent version mismatches.\n+\n ```rust\n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new().build().expect(\"event loop building\");\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new().build().expect(\"event loop building\");\n     let (_window, display) = glium::backend::glutin::SimpleWindowBuilder::new().build(&event_loop);\n }\n ```\n@@ -50,8 +52,8 @@ If you try to run this example with `cargo run` you'll encounter a problem: as s\n ```rust\n let _ = event_loop.run(move |event, window_target| {\n     match event {\n-        winit::event::Event::WindowEvent { event, .. } => match event {\n-\t    winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+        glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+\t    glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n \t    _ => (),\n         },\n         _ => (),\n@@ -100,7 +102,7 @@ Here is our full program:\n use glium::Surface;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new().build().expect(\"event loop building\");\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new().build().expect(\"event loop building\");\n     let (_window, display) = glium::backend::glutin::SimpleWindowBuilder::new().build(&event_loop);\n \n     let mut frame = display.draw();\n@@ -109,8 +111,8 @@ fn main() {\n \n     let _ = event_loop.run(move |event, window_target| {\n         match event {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-\t        winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+\t        glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n \t        _ => (),\n             },\n             _ => (),\ndiff --git a/book/tuto-03-animated-triangle.md b/book/tuto-03-animated-triangle.md\nindex c54b5494f2..c685fbe05b 100644\n--- a/book/tuto-03-animated-triangle.md\n+++ b/book/tuto-03-animated-triangle.md\n@@ -7,9 +7,9 @@ So far we have only ever rendered a single frame and then waited for the program\n ```rust\n let _ = event_loop.run(move |event, window_target| {\n     match event {\n-        winit::event::Event::WindowEvent { event, .. } => match event {\n-            winit::event::WindowEvent::CloseRequested => window_target.exit(),\n-            winit::event::WindowEvent::RedrawRequested => {\n+        glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+            glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::WindowEvent::RedrawRequested => {\n                 // Move the draw code here!\n             },\n             _ => (),\n@@ -24,14 +24,14 @@ What exactly triggers this event is platform specific, but in order to draw our\n ```rust\n let _ = event_loop.run(move |event, window_target| {\n     match event {\n-        winit::event::Event::WindowEvent { event, .. } => match event {\n-            winit::event::WindowEvent::CloseRequested => window_target.exit(),\n-            winit::event::WindowEvent::RedrawRequested => {\n+        glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+            glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::WindowEvent::RedrawRequested => {\n                 // Move the draw code here!\n             },\n             _ => (),\n         },\n-        winit::event::Event::AboutToWait => {\n+        glium::winit::event::Event::AboutToWait => {\n             window.request_redraw();\n         },\n         _ => (),\n@@ -47,17 +47,17 @@ While we are working on our event_loop there is one more event that we should ha\n let mut t: f32 = 0.0;\n let _ = event_loop.run(move |event, window_target| {\n     match event {\n-        winit::event::Event::WindowEvent { event, .. } => match event {\n-            winit::event::WindowEvent::CloseRequested => window_target.exit(),\n-            winit::event::WindowEvent::Resized(window_size) => {\n+        glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+            glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::WindowEvent::Resized(window_size) => {\n                 display.resize(window_size.into());\n             },\n-            winit::event::WindowEvent::RedrawRequested => {\n+            glium::winit::event::WindowEvent::RedrawRequested => {\n                 // Move the draw code here!\n             },\n             _ => (),\n         },\n-        winit::event::Event::AboutToWait => {\n+        glium::winit::event::Event::AboutToWait => {\n             window.request_redraw();\n         },\n         _ => (),\n@@ -75,12 +75,12 @@ Our first approach will be to create a variable named `t` which represents the s\n let mut t: f32 = 0.0;\n let _ = event_loop.run(move |event, window_target| {\n     match event {\n-        winit::event::Event::WindowEvent { event, .. } => match event {\n-            winit::event::WindowEvent::CloseRequested => window_target.exit(),\n-            winit::event::WindowEvent::Resized(window_size) => {\n+        glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+            glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::WindowEvent::Resized(window_size) => {\n                 display.resize(window_size.into());\n             },\n-            winit::event::WindowEvent::RedrawRequested => {\n+            glium::winit::event::WindowEvent::RedrawRequested => {\n                 // We update `t`\n                 t += 0.02;\n                 // We use the sine of t as an offset, this way we get a nice smooth animation\n@@ -101,7 +101,7 @@ let _ = event_loop.run(move |event, window_target| {\n             },\n             _ => (),\n         },\n-        winit::event::Event::AboutToWait => {\n+        glium::winit::event::Event::AboutToWait => {\n             window.request_redraw();\n         },\n         _ => (),\n@@ -127,12 +127,12 @@ Let's remove the two `let`'s that redefine our shape and vertex_buffer from our\n let mut t: f32 = 0.0;\n let _ = event_loop.run(move |event, window_target| {\n     match event {\n-        winit::event::Event::WindowEvent { event, .. } => match event {\n-            winit::event::WindowEvent::CloseRequested => window_target.exit(),\n-            winit::event::WindowEvent::Resized(window_size) => {\n+        glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+            glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::WindowEvent::Resized(window_size) => {\n                 display.resize(window_size.into());\n             },\n-\t    winit::event::WindowEvent::RedrawRequested => {\n+\t    glium::winit::event::WindowEvent::RedrawRequested => {\n \t        // We update `t`\n \t        t += 0.02;\n                 // We use the sine of t as an offset, this way we get a nice smooth animation\n@@ -146,7 +146,7 @@ let _ = event_loop.run(move |event, window_target| {\n \t    },\n             _ => (),\n         },\n-        winit::event::Event::AboutToWait => {\n+        glium::winit::event::Event::AboutToWait => {\n             window.request_redraw();\n         },\n         _ => (),\ndiff --git a/examples/displacement_mapping.rs b/examples/displacement_mapping.rs\nindex 94affa7ec4..73ba9299bf 100644\n--- a/examples/displacement_mapping.rs\n+++ b/examples/displacement_mapping.rs\n@@ -260,7 +260,7 @@ impl ApplicationContext for Application {\n         frame.finish().unwrap();\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n         self.camera.process_input(&event);\n     }\n \ndiff --git a/examples/fullscreen.rs b/examples/fullscreen.rs\nindex 1b76dcd8a2..39a297010e 100644\n--- a/examples/fullscreen.rs\n+++ b/examples/fullscreen.rs\n@@ -8,7 +8,7 @@ use glium::index::PrimitiveType;\n use glium::{Display, Frame, Surface};\n use glutin::surface::WindowSurface;\n use support::{ApplicationContext, State};\n-use winit::{window::Fullscreen, keyboard::{PhysicalKey, KeyCode}};\n+use glium::winit::{window::Fullscreen, keyboard::{PhysicalKey, KeyCode}};\n \n mod support;\n \n@@ -29,7 +29,7 @@ struct Application {\n }\n \n impl Application {\n-    fn toggle_fullscreen(&mut self, window: &winit::window::Window) {\n+    fn toggle_fullscreen(&mut self, window: &glium::winit::window::Window) {\n         if self.fullscreen {\n             window.set_fullscreen(None);\n             self.fullscreen = false;\n@@ -123,10 +123,10 @@ impl ApplicationContext for Application {\n         frame.finish().unwrap();\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, window: &glium::winit::window::Window) {\n         match event {\n-            winit::event::WindowEvent::KeyboardInput { event, .. } => match event.state {\n-                winit::event::ElementState::Pressed => match event.physical_key {\n+            glium::winit::event::WindowEvent::KeyboardInput { event, .. } => match event.state {\n+                glium::winit::event::ElementState::Pressed => match event.physical_key {\n                     PhysicalKey::Code(KeyCode::Enter) => {\n                         self.toggle_fullscreen(window);\n                     }\ndiff --git a/examples/fxaa.rs b/examples/fxaa.rs\nindex 7c6a414f62..0cb76a33dc 100644\n--- a/examples/fxaa.rs\n+++ b/examples/fxaa.rs\n@@ -1,12 +1,12 @@\n #[macro_use]\n extern crate glium;\n \n-use std::{cell::RefCell};\n+use std::cell::RefCell;\n \n use glium::{Display, Surface, framebuffer::SimpleFrameBuffer};\n use glutin::surface::WindowSurface;\n use support::{ApplicationContext, State};\n-use winit::keyboard::{PhysicalKey, KeyCode};\n+use glium::winit::keyboard::{PhysicalKey, KeyCode};\n \n mod support;\n \n@@ -275,12 +275,12 @@ impl ApplicationContext for Application {\n         target.finish().unwrap();\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n         self.camera.process_input(&event);\n \n         match event {\n-            winit::event::WindowEvent::KeyboardInput { event, .. } => match event.state {\n-                winit::event::ElementState::Pressed => match event.physical_key {\n+            glium::winit::event::WindowEvent::KeyboardInput { event, .. } => match event.state {\n+                glium::winit::event::ElementState::Pressed => match event.physical_key {\n                     PhysicalKey::Code(KeyCode::Space) => {\n                         self.fxaa_enabled = !self.fxaa_enabled;\n                         println!(\"FXAA is now {}\", if self.fxaa_enabled { \"enabled\" } else { \"disabled\" });\ndiff --git a/examples/instancing.rs b/examples/instancing.rs\nindex 9912bb7910..2d5b63fd00 100644\n--- a/examples/instancing.rs\n+++ b/examples/instancing.rs\n@@ -131,7 +131,7 @@ impl ApplicationContext for Application {\n         frame.finish().unwrap();\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n         self.camera.process_input(&event);\n     }\n \ndiff --git a/examples/manual-creation.rs b/examples/manual-creation.rs\nindex 75c3146005..643ba0c2fe 100644\n--- a/examples/manual-creation.rs\n+++ b/examples/manual-creation.rs\n@@ -17,8 +17,8 @@ There are three concepts in play:\n \n */\n \n-use winit::event_loop::EventLoopBuilder;\n-use winit::window::WindowBuilder;\n+use glium::winit::event_loop::EventLoopBuilder;\n+use glium::winit::window::WindowBuilder;\n use glium::Surface;\n use glutin::surface::WindowSurface;\n use glutin::config::ConfigTemplateBuilder;\n@@ -158,8 +158,8 @@ fn main() {\n     // the window is still available\n     event_loop.run(|event, window_target| {\n         match event {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n                 _ => (),\n             },\n             _ => (),\ndiff --git a/examples/multiple_windows.rs b/examples/multiple_windows.rs\nindex af1e51ecb0..ae45663cc4 100644\n--- a/examples/multiple_windows.rs\n+++ b/examples/multiple_windows.rs\n@@ -6,7 +6,7 @@ use std::collections::HashMap;\n use glium::{Surface, Display};\n use glutin::surface::WindowSurface;\n use support::{ApplicationContext, State};\n-use winit::{window::WindowId, event_loop::EventLoopBuilder};\n+use glium::winit::{window::WindowId, event_loop::EventLoopBuilder};\n \n mod support;\n \n@@ -46,20 +46,20 @@ fn main() {\n         match event {\n             // The Resumed/Suspended events are mostly for Android compatiblity since the context can get lost there at any point.\n             // For convenience's sake the Resumed event is also delivered on other platforms on program startup.\n-            winit::event::Event::Resumed => {\n+            glium::winit::event::Event::Resumed => {\n                 // On startup we create the main window and insert it into the HashMap just like subsequent sub windows.\n                 let window:State<Application> = State::new(window_target, true);\n                 windows.insert(window.window.id(), window);\n             },\n-            winit::event::Event::Suspended => {\n+            glium::winit::event::Event::Suspended => {\n                 windows.clear();\n             },\n-            winit::event::Event::WindowEvent { event, window_id, .. } => {\n+            glium::winit::event::Event::WindowEvent { event, window_id, .. } => {\n                 if let Some(state) = windows.get_mut(&window_id) {\n                     match event {\n-                        winit::event::WindowEvent::MouseInput {\n-                            state: winit::event::ElementState::Released,\n-                            button: winit::event::MouseButton::Left,\n+                        glium::winit::event::WindowEvent::MouseInput {\n+                            state: glium::winit::event::ElementState::Released,\n+                            button: glium::winit::event::MouseButton::Left,\n                             ..\n                         } => {\n                             // This is the main part, where we actually create the new window, enumerate it and\n@@ -74,11 +74,11 @@ fn main() {\n                                 windows.remove(&window_id);\n                             }\n                         },\n-                        winit::event::WindowEvent::RedrawRequested => {\n+                        glium::winit::event::WindowEvent::RedrawRequested => {\n                             state.context.update();\n                             state.context.draw_frame(&state.display);\n                         }\n-                        winit::event::WindowEvent::CloseRequested => {\n+                        glium::winit::event::WindowEvent::CloseRequested => {\n                             if state.context.id == 1 {\n                                 window_target.exit();\n                             } else {\n@@ -87,7 +87,7 @@ fn main() {\n                         },\n                         // These two aren't necessary for this particular example, but forgetting especially the Resized\n                         // event could lead to stretched images being rendered if you decide to render something more interesting based on this example.\n-                        winit::event::WindowEvent::Resized(new_size) => {\n+                        glium::winit::event::WindowEvent::Resized(new_size) => {\n                             state.display.resize(new_size.into());\n                         },\n                         ev => {\ndiff --git a/examples/picking.rs b/examples/picking.rs\nindex 606242e125..d3264fdfc5 100644\n--- a/examples/picking.rs\n+++ b/examples/picking.rs\n@@ -258,9 +258,9 @@ impl ApplicationContext for Application {\n         }\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n         match event {\n-            winit::event::WindowEvent::CursorMoved { position, .. } => {\n+            glium::winit::event::WindowEvent::CursorMoved { position, .. } => {\n                 self.cursor_position = Some(position.cast::<i32>().into());\n             }\n             ev => self.camera.process_input(ev),\ndiff --git a/examples/screenshot-asynchronous.rs b/examples/screenshot-asynchronous.rs\nindex 64f6c86d9a..86b62bc77a 100644\n--- a/examples/screenshot-asynchronous.rs\n+++ b/examples/screenshot-asynchronous.rs\n@@ -7,7 +7,7 @@ use glium::index::PrimitiveType;\n use glium::{Display, Surface};\n use glutin::surface::WindowSurface;\n use support::{ApplicationContext, State};\n-use winit::keyboard::{PhysicalKey, KeyCode};\n+use glium::winit::keyboard::{PhysicalKey, KeyCode};\n \n mod screenshot {\n     use glium::Surface;\n@@ -269,9 +269,9 @@ impl ApplicationContext for Application {\n         }\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n-        if let winit::event::WindowEvent::KeyboardInput { event, ..} = event {\n-            if let winit::event::ElementState::Pressed = event.state {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n+        if let glium::winit::event::WindowEvent::KeyboardInput { event, ..} = event {\n+            if let glium::winit::event::ElementState::Pressed = event.state {\n                 if let PhysicalKey::Code(KeyCode::KeyS) = event.physical_key {\n                     self.take_screenshot = true;\n                 }\n@@ -282,4 +282,4 @@ impl ApplicationContext for Application {\n \n fn main() {\n     State::<Application>::run_loop();\n-}\n\\ No newline at end of file\n+}\ndiff --git a/examples/shadow_mapping.rs b/examples/shadow_mapping.rs\nindex 09d61542ed..cc912a1e0f 100644\n--- a/examples/shadow_mapping.rs\n+++ b/examples/shadow_mapping.rs\n@@ -2,7 +2,7 @@\n extern crate glium;\n \n use cgmath::SquareMatrix;\n-use winit::keyboard::{PhysicalKey, KeyCode};\n+use glium::winit::keyboard::{PhysicalKey, KeyCode};\n use std::time::Instant;\n use glium::{Surface, Display, VertexBuffer, IndexBuffer, Program, texture::DepthTexture2d};\n use glutin::surface::WindowSurface;\n@@ -275,9 +275,9 @@ impl ApplicationContext for Application {\n         }\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n         match event {\n-            winit::event::WindowEvent::KeyboardInput { event, .. } => if event.state == winit::event::ElementState::Pressed {\n+            glium::winit::event::WindowEvent::KeyboardInput { event, .. } => if event.state == glium::winit::event::ElementState::Pressed {\n                 if let PhysicalKey::Code(code) = event.physical_key {\n                     match code {\n                         KeyCode::KeyC => self.camera_rotating = !self.camera_rotating,\ndiff --git a/examples/spirv/main.rs b/examples/spirv/main.rs\nindex e570373d3a..2039894be6 100644\n--- a/examples/spirv/main.rs\n+++ b/examples/spirv/main.rs\n@@ -16,10 +16,10 @@ use raw_window_handle::HasRawWindowHandle;\n \n fn main() {\n     // We start by creating the main EventLoop\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n-    let window_builder = winit::window::WindowBuilder::new().with_title(\"Glium SPIR-V example\");\n+    let window_builder = glium::winit::window::WindowBuilder::new().with_title(\"Glium SPIR-V example\");\n     let config_template_builder = glutin::config::ConfigTemplateBuilder::new();\n     let display_builder = glutin_winit::DisplayBuilder::new().with_window_builder(Some(window_builder));\n \ndiff --git a/examples/support/camera.rs b/examples/support/camera.rs\nindex 94ff610659..f429bec7be 100644\n--- a/examples/support/camera.rs\n+++ b/examples/support/camera.rs\n@@ -148,12 +148,12 @@ impl CameraState {\n         }\n     }\n \n-    pub fn process_input(&mut self, event: &winit::event::WindowEvent) {\n-        use winit::keyboard::{PhysicalKey, KeyCode};\n-        let winit::event::WindowEvent::KeyboardInput { event, .. } = event else {\n+    pub fn process_input(&mut self, event: &glium::winit::event::WindowEvent) {\n+        use glium::winit::keyboard::{PhysicalKey, KeyCode};\n+        let glium::winit::event::WindowEvent::KeyboardInput { event, .. } = event else {\n             return\n         };\n-        let pressed = event.state == winit::event::ElementState::Pressed;\n+        let pressed = event.state == glium::winit::event::ElementState::Pressed;\n         match &event.physical_key {\n             PhysicalKey::Code(KeyCode::ArrowUp) => self.moving_up = pressed,\n             PhysicalKey::Code(KeyCode::ArrowDown) => self.moving_down = pressed,\ndiff --git a/examples/support/mod.rs b/examples/support/mod.rs\nindex 1a3aed3de9..b1b3065ba7 100644\n--- a/examples/support/mod.rs\n+++ b/examples/support/mod.rs\n@@ -88,22 +88,22 @@ pub trait ApplicationContext {\n     fn draw_frame(&mut self, _display: &Display<WindowSurface>) { }\n     fn new(display: &Display<WindowSurface>) -> Self;\n     fn update(&mut self) { }\n-    fn handle_window_event(&mut self, _event: &winit::event::WindowEvent, _window: &winit::window::Window) { }\n+    fn handle_window_event(&mut self, _event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) { }\n     const WINDOW_TITLE:&'static str;\n }\n \n pub struct State<T> {\n     pub display: glium::Display<WindowSurface>,\n-    pub window: winit::window::Window,\n+    pub window: glium::winit::window::Window,\n     pub context: T,\n }\n \n impl<T: ApplicationContext + 'static> State<T> {\n     pub fn new<W>(\n-        event_loop: &winit::event_loop::EventLoopWindowTarget<W>,\n+        event_loop: &glium::winit::event_loop::EventLoopWindowTarget<W>,\n         visible: bool,\n     ) -> Self {\n-        let window_builder = winit::window::WindowBuilder::new().with_title(T::WINDOW_TITLE).with_visible(visible);\n+        let window_builder = glium::winit::window::WindowBuilder::new().with_title(T::WINDOW_TITLE).with_visible(visible);\n         let config_template_builder = glutin::config::ConfigTemplateBuilder::new();\n         let display_builder = glutin_winit::DisplayBuilder::new().with_window_builder(Some(window_builder));\n \n@@ -150,7 +150,7 @@ impl<T: ApplicationContext + 'static> State<T> {\n \n     pub fn from_display_window(\n         display: glium::Display<WindowSurface>,\n-        window: winit::window::Window,\n+        window: glium::winit::window::Window,\n     ) -> Self {\n         let context = T::new(&display);\n         Self {\n@@ -162,7 +162,7 @@ impl<T: ApplicationContext + 'static> State<T> {\n \n     /// Start the event_loop and keep rendering frames until the program is closed\n     pub fn run_loop() {\n-        let event_loop = winit::event_loop::EventLoopBuilder::new()\n+        let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n             .build()\n             .expect(\"event loop building\");\n         let mut state: Option<State<T>> = None;\n@@ -171,24 +171,24 @@ impl<T: ApplicationContext + 'static> State<T> {\n             match event {\n                 // The Resumed/Suspended events are mostly for Android compatiblity since the context can get lost there at any point.\n                 // For convenience's sake the Resumed event is also delivered on other platforms on program startup.\n-                winit::event::Event::Resumed => {\n+                glium::winit::event::Event::Resumed => {\n                     state = Some(State::new(window_target, true));\n                 },\n-                winit::event::Event::Suspended => state = None,\n+                glium::winit::event::Event::Suspended => state = None,\n                 // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n                 // For applications that only change due to user input you could remove this handler.\n-                winit::event::Event::AboutToWait => {\n+                glium::winit::event::Event::AboutToWait => {\n                     if let Some(state) = &state {\n                         state.window.request_redraw();\n                     }\n                 }\n-                winit::event::Event::WindowEvent { event, .. } => match event {\n-                    winit::event::WindowEvent::Resized(new_size) => {\n+                glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                    glium::winit::event::WindowEvent::Resized(new_size) => {\n                         if let Some(state) = &state {\n                             state.display.resize(new_size.into());\n                         }\n                     },\n-                    winit::event::WindowEvent::RedrawRequested => {\n+                    glium::winit::event::WindowEvent::RedrawRequested => {\n                         if let Some(state) = &mut state {\n                             state.context.update();\n                             state.context.draw_frame(&state.display);\n@@ -196,10 +196,10 @@ impl<T: ApplicationContext + 'static> State<T> {\n                     },\n                     // Exit the event loop when requested (by closing the window for example) or when\n                     // pressing the Esc key.\n-                    winit::event::WindowEvent::CloseRequested\n-                    | winit::event::WindowEvent::KeyboardInput { event: winit::event::KeyEvent {\n-                        state: winit::event::ElementState::Pressed,\n-                        logical_key: winit::keyboard::Key::Named(winit::keyboard::NamedKey::Escape),\n+                    glium::winit::event::WindowEvent::CloseRequested\n+                    | glium::winit::event::WindowEvent::KeyboardInput { event: glium::winit::event::KeyEvent {\n+                        state: glium::winit::event::ElementState::Pressed,\n+                        logical_key: glium::winit::keyboard::Key::Named(glium::winit::keyboard::NamedKey::Escape),\n                         ..\n                     }, ..} => {\n                         window_target.exit()\n@@ -219,7 +219,7 @@ impl<T: ApplicationContext + 'static> State<T> {\n \n     /// Create a context and draw a single frame\n     pub fn run_once(visible: bool) {\n-        let event_loop = winit::event_loop::EventLoopBuilder::new()\n+        let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n             .build()\n             .expect(\"event loop building\");\n         let mut state:State<T> = State::new(&event_loop, visible);\ndiff --git a/examples/teapot.rs b/examples/teapot.rs\nindex e5237ab452..83768d5f4c 100644\n--- a/examples/teapot.rs\n+++ b/examples/teapot.rs\n@@ -164,7 +164,7 @@ impl ApplicationContext for Application {\n         frame.finish().unwrap();\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n         self.camera.process_input(&event);\n     }\n \ndiff --git a/examples/tessellation.rs b/examples/tessellation.rs\nindex e3429943db..dc16d3a45a 100644\n--- a/examples/tessellation.rs\n+++ b/examples/tessellation.rs\n@@ -189,11 +189,11 @@ impl ApplicationContext for Application {\n         frame.finish().unwrap();\n     }\n \n-    fn handle_window_event(&mut self, event: &winit::event::WindowEvent, _window: &winit::window::Window) {\n-        use winit::keyboard::{PhysicalKey, KeyCode};\n+    fn handle_window_event(&mut self, event: &glium::winit::event::WindowEvent, _window: &glium::winit::window::Window) {\n+        use glium::winit::keyboard::{PhysicalKey, KeyCode};\n         match event {\n-            winit::event::WindowEvent::KeyboardInput { event, .. } => match event.state {\n-                winit::event::ElementState::Pressed => match event.physical_key {\n+            glium::winit::event::WindowEvent::KeyboardInput { event, .. } => match event.state {\n+                glium::winit::event::ElementState::Pressed => match event.physical_key {\n                     PhysicalKey::Code(KeyCode::ArrowUp) => {\n                         self.tess_level += 1;\n                         println!(\"New tessellation level: {}\", self.tess_level);\ndiff --git a/examples/tutorial-01.rs b/examples/tutorial-01.rs\nindex 2a673a8b08..2a8223b2a7 100644\n--- a/examples/tutorial-01.rs\n+++ b/examples/tutorial-01.rs\n@@ -3,7 +3,7 @@ use glium::Surface;\n fn main() {\n     // We start by creating the EventLoop, this can only be done once per process.\n     // This also needs to happen on the main thread to make the program portable.\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (_window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -20,9 +20,9 @@ fn main() {\n     // Now we wait until the program is closed\n     event_loop.run(move |event, window_target| {\n         match event {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n                 // This event is sent by the OS when you close the Window, or request the program to quit via the taskbar.\n-                winit::event::WindowEvent::CloseRequested => window_target.exit(),\n+                glium::winit::event::WindowEvent::CloseRequested => window_target.exit(),\n                 _ => (),\n             },\n             _ => (),\ndiff --git a/examples/tutorial-02.rs b/examples/tutorial-02.rs\nindex 58af8bc0c1..01f1809434 100644\n--- a/examples/tutorial-02.rs\n+++ b/examples/tutorial-02.rs\n@@ -5,7 +5,7 @@ use glium::Surface;\n fn main() {\n     // We start by creating the EventLoop, this can only be done once per process.\n     // This also needs to happen on the main thread to make the program portable.\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (_window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -55,8 +55,8 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 _ => (),\ndiff --git a/examples/tutorial-03.rs b/examples/tutorial-03.rs\nindex 04f2094657..d76fca3d86 100644\n--- a/examples/tutorial-03.rs\n+++ b/examples/tutorial-03.rs\n@@ -3,7 +3,7 @@ extern crate glium;\n use glium::Surface;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -51,12 +51,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     // first we update `t`\n                     t += 0.02;\n                     let x_off = t.sin() * 0.5;\n@@ -70,14 +70,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a RedrawEventsCleared event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-04.rs b/examples/tutorial-04.rs\nindex b8c41dc861..ea70b31326 100644\n--- a/examples/tutorial-04.rs\n+++ b/examples/tutorial-04.rs\n@@ -3,7 +3,7 @@ extern crate glium;\n use glium::Surface;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -50,12 +50,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     // we update `t`\n                     t += 0.02;\n                     let x = t.sin() * 0.5;\n@@ -78,14 +78,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-05.rs b/examples/tutorial-05.rs\nindex 6b9efdad64..8f990c890b 100644\n--- a/examples/tutorial-05.rs\n+++ b/examples/tutorial-05.rs\n@@ -3,7 +3,7 @@ extern crate glium;\n use glium::Surface;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -56,12 +56,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     // we update `t`\n                     t += 0.02;\n                     let x = t.sin() * 0.5;\n@@ -84,14 +84,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-06.rs b/examples/tutorial-06.rs\nindex 0f6dbbc955..010de83780 100644\n--- a/examples/tutorial-06.rs\n+++ b/examples/tutorial-06.rs\n@@ -3,7 +3,7 @@ extern crate glium;\n use glium::Surface;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -67,12 +67,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     // we update `t`\n                     t += 0.02;\n                     let x = t.sin() * 0.5;\n@@ -96,14 +96,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-07.rs b/examples/tutorial-07.rs\nindex 26cad4fb6c..016b4d7634 100644\n--- a/examples/tutorial-07.rs\n+++ b/examples/tutorial-07.rs\n@@ -6,7 +6,7 @@ use glium::Surface;\n mod teapot;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -46,12 +46,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color(0.0, 0.0, 1.0, 1.0);\n \n@@ -68,14 +68,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-08.rs b/examples/tutorial-08.rs\nindex fb5e36ea66..1d4e81776a 100644\n--- a/examples/tutorial-08.rs\n+++ b/examples/tutorial-08.rs\n@@ -6,7 +6,7 @@ use glium::Surface;\n mod teapot;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -54,12 +54,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color(0.0, 0.0, 1.0, 1.0);\n \n@@ -79,14 +79,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-09.rs b/examples/tutorial-09.rs\nindex 49dc0e24c9..227a42c02c 100644\n--- a/examples/tutorial-09.rs\n+++ b/examples/tutorial-09.rs\n@@ -6,7 +6,7 @@ use glium::Surface;\n mod teapot;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -54,12 +54,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color_and_depth((0.0, 0.0, 1.0, 1.0), 1.0);\n \n@@ -87,14 +87,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-10.rs b/examples/tutorial-10.rs\nindex 711ce507ec..3a65979599 100644\n--- a/examples/tutorial-10.rs\n+++ b/examples/tutorial-10.rs\n@@ -6,7 +6,7 @@ use glium::Surface;\n mod teapot;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -55,12 +55,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color_and_depth((0.0, 0.0, 1.0, 1.0), 1.0);\n \n@@ -107,14 +107,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-12.rs b/examples/tutorial-12.rs\nindex 3f5aa403c1..d4616521e2 100644\n--- a/examples/tutorial-12.rs\n+++ b/examples/tutorial-12.rs\n@@ -9,7 +9,7 @@ mod support;\n use support::view_matrix;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -60,12 +60,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color_and_depth((0.0, 0.0, 1.0, 1.0), 1.0);\n \n@@ -115,14 +115,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-13.rs b/examples/tutorial-13.rs\nindex 4027c911e5..4ece9b977f 100644\n--- a/examples/tutorial-13.rs\n+++ b/examples/tutorial-13.rs\n@@ -9,7 +9,7 @@ mod support;\n use support::view_matrix;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -72,12 +72,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color_and_depth((0.0, 0.0, 1.0, 1.0), 1.0);\n \n@@ -127,14 +127,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/examples/tutorial-14.rs b/examples/tutorial-14.rs\nindex 05dcb09e6b..02d8ea4d4d 100644\n--- a/examples/tutorial-14.rs\n+++ b/examples/tutorial-14.rs\n@@ -6,7 +6,7 @@ mod support;\n use support::view_matrix;\n \n fn main() {\n-    let event_loop = winit::event_loop::EventLoopBuilder::new()\n+    let event_loop = glium::winit::event_loop::EventLoopBuilder::new()\n         .build()\n         .expect(\"event loop building\");\n     let (window, display) = glium::backend::glutin::SimpleWindowBuilder::new()\n@@ -119,12 +119,12 @@ fn main() {\n \n     event_loop.run(move |ev, window_target| {\n         match ev {\n-            winit::event::Event::WindowEvent { event, .. } => match event {\n-                winit::event::WindowEvent::CloseRequested => {\n+            glium::winit::event::Event::WindowEvent { event, .. } => match event {\n+                glium::winit::event::WindowEvent::CloseRequested => {\n                     window_target.exit();\n                 },\n                 // We now need to render everyting in response to a RedrawRequested event due to the animation\n-                winit::event::WindowEvent::RedrawRequested => {\n+                glium::winit::event::WindowEvent::RedrawRequested => {\n                     let mut target = display.draw();\n                     target.clear_color_and_depth((0.0, 0.0, 1.0, 1.0), 1.0);\n \n@@ -177,14 +177,14 @@ fn main() {\n                 },\n                 // Because glium doesn't know about windows we need to resize the display\n                 // when the window's size has changed.\n-                winit::event::WindowEvent::Resized(window_size) => {\n+                glium::winit::event::WindowEvent::Resized(window_size) => {\n                     display.resize(window_size.into());\n                 },\n                 _ => (),\n             },\n             // By requesting a redraw in response to a AboutToWait event we get continuous rendering.\n             // For applications that only change due to user input you could remove this handler.\n-            winit::event::Event::AboutToWait => {\n+            glium::winit::event::Event::AboutToWait => {\n                 window.request_redraw();\n             },\n             _ => (),\ndiff --git a/src/backend/mod.rs b/src/backend/mod.rs\nindex 2033ed5a5d..3aee8aa0ab 100644\n--- a/src/backend/mod.rs\n+++ b/src/backend/mod.rs\n@@ -29,6 +29,9 @@ pub use crate::context::ReleaseBehavior;\n #[cfg(feature = \"glutin\")]\n pub mod glutin;\n \n+#[cfg(feature = \"simple_window_builder\")]\n+pub use winit;\n+\n /// Trait for types that can be used as a backend for a glium context.\n ///\n /// This trait is unsafe, as you can get undefined behaviors or crashes if you don't implement\ndiff --git a/src/lib.rs b/src/lib.rs\nindex af8e9b9d96..439534dc95 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -12,7 +12,9 @@ The initialisation of a simple glium display occurs in two steps.\n \n ```no_run\n extern crate glium;\n-extern crate winit;\n+// Use the re-exported winit dependency to avoid version mismatches.\n+// Requires the `simple_window_builder` feature.\n+use glium::winit;\n \n fn main() {\n     // 1. The **winit::EventLoop** for handling events.\n@@ -108,6 +110,8 @@ result to the user.\n \n #[cfg(feature = \"glutin\")]\n pub use crate::backend::glutin::glutin;\n+#[cfg(feature = \"simple_window_builder\")]\n+pub use crate::backend::winit;\n pub use crate::context::{Capabilities, ExtensionsList, Profile, UuidError};\n pub use crate::draw_parameters::{Blend, BlendingFunction, LinearBlendingFactor, BackfaceCullingMode};\n pub use crate::draw_parameters::{Depth, DepthTest, PolygonMode, DrawParameters, StencilTest, StencilOperation};\n", "instance_id": "glium__glium-2115", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the goal of re-exporting the `winit` crate to prevent version mismatches between the `glium` library and user code. It provides context about the issue (version incompatibility leading to breaking changes) and suggests a solution (re-exporting `winit` and updating tutorials). The intent and motivation are evident, and the code changes align with the described goal. However, there are minor ambiguities: the problem statement does not explicitly define the scope of the re-export (e.g., which specific `winit` modules or types to expose) nor does it mention potential compatibility issues or constraints with different versions of `winit`. Additionally, it lacks discussion of edge cases, such as how to handle users who might still depend on a different `winit` version or potential downstream impacts on other dependencies. Despite these minor gaps, the overall intent and requirements are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the relatively straightforward nature of the changes required. The task involves re-exporting the `winit` crate within the `glium` library and updating various examples and tutorials to use this re-exported version instead of directly referencing `winit`. The code changes are extensive in terms of the number of files affected (multiple tutorial files, examples, and library source files), but the modifications themselves are repetitive and simple\u2014primarily search-and-replace operations to update namespace paths (e.g., changing `winit::` to `glium::winit::`). The scope of changes is limited to documentation and example code, with minimal impact on the core library architecture, as the re-export is a surface-level change in the library's public API (adding `pub use winit;` in the backend module). \n\nFrom a technical perspective, the problem requires basic familiarity with Rust's module system and re-exporting mechanisms, which are not complex concepts for an experienced developer. No advanced algorithms, design patterns, or domain-specific knowledge (beyond Rust dependency management) are needed. There are also no significant edge cases or error handling requirements mentioned or implied in the problem statement or code changes, as the task is primarily about namespace adjustments rather than functional logic. The main challenge lies in ensuring consistency across the numerous files, but this is more tedious than technically demanding. Therefore, a difficulty score of 0.30 is appropriate, reflecting an easy task that requires understanding of basic code logic and making simple, widespread modifications.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`cwd` gets messed up when using `g<Space>` with `../`\n### What system are you running Yazi on?\n\nLinux Wayland\n\n### What terminal are you running Yazi in?\n\nkitty 0.35.2\n\n### `yazi --debug` output\n\n<details>\r\n\r\n```sh\r\n##### \u2193\u2193\u2193 Paste the output here: \u2193\u2193\u2193 #####\r\n\r\nYazi\r\n    Version: 0.3.0 (d2ebadb6 2024-08-03)\r\n    Debug  : false\r\n    OS     : linux-x86_64 (unix)\r\n\r\nYa\r\n    Version: 0.3.0\r\n\r\nEmulator\r\n    Emulator.via_env: (\"xterm-kitty\", \"\")\r\n    Emulator.via_csi: Ok(Kitty)\r\n    Emulator.detect : Kitty\r\n\r\nAdapter\r\n    Adapter.matches: Kitty\r\n\r\nDesktop\r\n    XDG_SESSION_TYPE: Some(\"wayland\")\r\n    WAYLAND_DISPLAY : Some(\"wayland-0\")\r\n    DISPLAY         : Some(\":0\")\r\n\r\nSSH\r\n    shared.in_ssh_connection: false\r\n\r\nWSL\r\n    /proc/sys/fs/binfmt_misc/WSLInterop: false\r\n\r\nVariables\r\n    SHELL              : Some(\"/usr/bin/fish\")\r\n    EDITOR             : Some(\"nvim\")\r\n    YAZI_FILE_ONE      : None\r\n    YAZI_CONFIG_HOME   : None\r\n    ZELLIJ_SESSION_NAME: None\r\n\r\nText Opener\r\n    default: Some(Opener { run: \"${EDITOR:-vi} \\\"$@\\\"\", block: true, orphan: false, desc: \"$EDITOR\", for_: None, spread: true })\r\n    block  : Some(Opener { run: \"${EDITOR:-vi} \\\"$@\\\"\", block: true, orphan: false, desc: \"$EDITOR\", for_: None, spread: true })\r\n\r\ntmux\r\n    TMUX   : false\r\n    Version: 3.4\r\n\r\nDependencies\r\n    file             : 5.45\r\n    ueberzugpp       : 2.9.6\r\n    ffmpegthumbnailer: 2.2.2\r\n    magick           : 7.1.1-36\r\n    fzf              : 0.54.3\r\n    fd               : 10.1.0\r\n    rg               : 14.1.0\r\n    chafa            : 1.14.1\r\n    zoxide           : 0.9.4\r\n    7z               : 17.05\r\n    7zz              : No such file or directory (os error 2)\r\n    jq               : 1.7.1\r\n\r\n\r\n--------------------------------------------------\r\nWhen reporting a bug, please also upload the `yazi.log` log file - only upload the most recent content by time.\r\nYou can find it in the \"/home/tlss/.local/state/yazi\" directory.\r\n```\r\n\r\n</details>\r\n\n\n### Did you try the latest nightly build to see if the problem got fixed?\n\nYes, and I updated the debug information above (`yazi --debug`) to the nightly that I tried\n\n### Describe the bug\n\nWhen using `g<Space>` with `../xxx/xxx`, the `cwd` in the header is appended directly.\r\n\r\n![screenshot_20240806_010056](https://github.com/user-attachments/assets/18af8a0b-31b2-435e-acff-a668e16f1760)\r\n\r\n![screenshot_20240806_010858](https://github.com/user-attachments/assets/867b6099-4ccc-4157-930f-3fa1cd95061d)\r\n\n\n### Minimal reproducer\n\n1. Open yazi\r\n2. Press `g<Space>`\r\n3. Input `../` then press `<Enter>`\r\n4. Check the `cwd` in the header\n\n### Anything else?\n\n_No response_\n", "patch": "diff --git a/yazi-shared/src/fs/path.rs b/yazi-shared/src/fs/path.rs\nindex 4aa318e69..ddaebda8b 100644\n--- a/yazi-shared/src/fs/path.rs\n+++ b/yazi-shared/src/fs/path.rs\n@@ -1,9 +1,8 @@\n-use std::{borrow::Cow, env, ffi::OsString, path::{Component, Path, PathBuf, MAIN_SEPARATOR}};\n+use std::{borrow::Cow, env, ffi::OsString, path::{Component, Path, PathBuf}};\n \n use super::maybe_exists;\n use crate::fs::Url;\n \n-#[inline]\n pub fn current_cwd() -> Option<PathBuf> {\n \tenv::var_os(\"PWD\")\n \t\t.map(PathBuf::from)\n@@ -11,6 +10,29 @@ pub fn current_cwd() -> Option<PathBuf> {\n \t\t.or_else(|| env::current_dir().ok())\n }\n \n+pub fn clean_path(path: &Path) -> PathBuf {\n+\tlet mut out = vec![];\n+\tfor c in path.components() {\n+\t\tmatch c {\n+\t\t\tComponent::CurDir => {}\n+\t\t\tComponent::ParentDir => match out.last() {\n+\t\t\t\tSome(Component::RootDir) => {}\n+\t\t\t\tSome(Component::Normal(_)) => _ = out.pop(),\n+\t\t\t\tNone\n+\t\t\t\t| Some(Component::CurDir)\n+\t\t\t\t| Some(Component::ParentDir)\n+\t\t\t\t| Some(Component::Prefix(_)) => out.push(c),\n+\t\t\t},\n+\t\t\tc => out.push(c),\n+\t\t}\n+\t}\n+\n+\tif out.is_empty() { PathBuf::from(\".\") } else { out.iter().collect() }\n+}\n+\n+#[inline]\n+pub fn expand_path(p: impl AsRef<Path>) -> PathBuf { _expand_path(p.as_ref()) }\n+\n fn _expand_path(p: &Path) -> PathBuf {\n \t// ${HOME} or $HOME\n \t#[cfg(unix)]\n@@ -35,24 +57,18 @@ fn _expand_path(p: &Path) -> PathBuf {\n \t\t}\n \t}\n \n-\tlet p = Path::new(s.as_ref());\n+\tlet p = clean_path(Path::new(s.as_ref()));\n \tif let Ok(rest) = p.strip_prefix(\"~\") {\n \t\treturn dirs::home_dir().unwrap_or_default().join(rest);\n \t}\n \n \tif p.is_absolute() {\n-\t\treturn p.to_path_buf();\n+\t\tp\n+\t} else if let Some(cwd) = current_cwd() {\n+\t\tcwd.join(&p)\n+\t} else {\n+\t\tp\n \t}\n-\tcurrent_cwd().map_or_else(|| p.to_path_buf(), |c| c.join(p))\n-}\n-\n-#[inline]\n-pub fn expand_path(p: impl AsRef<Path>) -> PathBuf { _expand_path(p.as_ref()) }\n-\n-#[inline]\n-pub fn ends_with_slash(p: &Path) -> bool {\n-\tlet b = p.as_os_str().as_encoded_bytes();\n-\tif let [.., last] = b { *last == MAIN_SEPARATOR as u8 } else { false }\n }\n \n // FIXME: should return a `std::io::Result` to handle errors such as\n", "instance_id": "sxyazi__yazi-1428", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the current working directory (`cwd`) in the header of the Yazi application gets incorrectly appended when using the `g<Space>` command with a path like `../xxx/xxx`. The statement includes a minimal reproducer and screenshots to illustrate the bug, which helps in understanding the issue. However, there are minor ambiguities and missing details. For instance, the expected behavior of the `cwd` display is not explicitly stated (e.g., should it resolve to an absolute path or remain relative?). Additionally, the problem statement does not mention specific edge cases or constraints, such as how deeply nested paths or symbolic links should be handled. While the issue is valid and mostly comprehensible, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are confined to a single file (`yazi-shared/src/fs/path.rs`) and focus on modifying the path handling logic. The changes introduce a new `clean_path` function to normalize paths by handling `.` and `..` components and adjust the `expand_path` function to use this normalization. The amount of code change is moderate (around 30 lines added or modified), and it does not appear to impact the broader system architecture or require modifications across multiple modules.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding basic file system path handling in Rust, specifically the `std::path` module and its `Component` enum for iterating over path components. The logic for normalizing paths (handling `..` and `.`) is straightforward and does not involve advanced algorithms, design patterns, or domain-specific knowledge beyond basic file system concepts. Familiarity with Rust's ownership and borrowing rules is necessary but not complex in this context.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest consideration of basic path normalization scenarios (e.g., consecutive `..` or `.` components, empty paths). The implementation returns a default path (`.`) for empty results, which handles one edge case. However, more complex edge cases like symbolic links, invalid paths, or platform-specific behaviors are not addressed in the code or problem statement. Error handling is minimal and does not appear to be a significant focus of the change.\n\n4. **Overall Complexity**: The task involves understanding and modifying a specific part of the codebase related to path resolution. While it requires some logic to handle path components correctly, it does not demand deep knowledge of the entire Yazi codebase or complex interactions between modules. The fix is relatively self-contained and does not introduce significant performance or architectural challenges.\n\nGiven these points, a difficulty score of 0.35 reflects an \"Easy\" problem that requires moderate understanding of path handling in Rust and straightforward code modifications, with minimal impact on the broader system and limited complexity in edge case handling.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Cannot `fd_seek` with `fdflags::append`\n### Describe the bug\r\n\r\nIf an fd was opened with `fdflags::append`, `fd_seek` with it does not work properly.\r\n\r\n```sh\r\nwasmer -vV; rustc -vV\r\nwasmer 4.4.0 (8b97cfe 2024-10-18)\r\nbinary: wasmer-cli\r\ncommit-hash: 8b97cfe3992ae3c2f0002f9955fcb23057f666a9\r\ncommit-date: 2024-10-18\r\nhost: x86_64-unknown-linux-gnu\r\ncompiler: singlepass,cranelift\r\nc_api backend: \r\nrustc 1.81.0 (eeb90cda1 2024-09-04)\r\nbinary: rustc\r\ncommit-hash: eeb90cda1969383f56a2637cbd3037bdf598841c\r\ncommit-date: 2024-09-04\r\nhost: x86_64-unknown-linux-gnu\r\nrelease: 1.81.0\r\nLLVM version: 18.1.7\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nCompile the snippet with crate `wasi` v0.11, and run:\r\n\r\n```\r\ncargo build --target wasm32-wasip1\r\nmkdir dir\r\nwasmer run --mapdir /dir:dir target/wasm32-wasip1/debug/repro.wasm\r\n```\r\n\r\n```rust\r\nfn main() {\r\n    unsafe {\r\n        let base_fd = 5;\r\n        let rights = wasi::RIGHTS_FD_WRITE | wasi::RIGHTS_FD_TELL | wasi::RIGHTS_FD_SEEK;\r\n        let fd = wasi::path_open(\r\n            base_fd,\r\n            0,\r\n            \"dir/file\",\r\n            wasi::OFLAGS_CREAT | wasi::OFLAGS_EXCL,\r\n            rights,\r\n            rights,\r\n            0,\r\n        )\r\n        .unwrap();\r\n        let fd_append = wasi::path_open(\r\n            base_fd,\r\n            0,\r\n            \"dir/file\",\r\n            wasi::OFLAGS_CREAT,\r\n            rights,\r\n            rights,\r\n            wasi::FDFLAGS_APPEND,\r\n        )\r\n        .unwrap();\r\n\r\n        assert_eq!(wasi::fd_seek(fd, 100, wasi::WHENCE_SET).unwrap(), 100);\r\n        assert_eq!(\r\n            wasi::fd_seek(fd_append, 100, wasi::WHENCE_SET).unwrap(),\r\n            100\r\n        );\r\n    }\r\n}\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe snippet should run successfully. The two seeks should return the same offset.\r\n\r\n### Actual behavior\r\n\r\nWasmer fails when running the snippet because the second `fd_seek` returns 0 as the offset.\r\n\r\n\n", "patch": "diff --git a/lib/wasix/src/syscalls/wasi/fd_seek.rs b/lib/wasix/src/syscalls/wasi/fd_seek.rs\nindex 527ed185dcd..65d60514933 100644\n--- a/lib/wasix/src/syscalls/wasi/fd_seek.rs\n+++ b/lib/wasix/src/syscalls/wasi/fd_seek.rs\n@@ -62,9 +62,6 @@ pub(crate) fn fd_seek_internal(\n     if !fd_entry.rights.contains(Rights::FD_SEEK) {\n         return Ok(Err(Errno::Access));\n     }\n-    if fd_entry.flags.contains(Fdflags::APPEND) {\n-        return Ok(Ok(fd_entry.offset.load(Ordering::Acquire)));\n-    }\n \n     // TODO: handle case if fd is a dir?\n     let new_offset = match whence {\ndiff --git a/lib/wasix/src/syscalls/wasi/fd_write.rs b/lib/wasix/src/syscalls/wasi/fd_write.rs\nindex b5f0d648a82..04954d6770e 100644\n--- a/lib/wasix/src/syscalls/wasi/fd_write.rs\n+++ b/lib/wasix/src/syscalls/wasi/fd_write.rs\n@@ -127,6 +127,7 @@ pub(crate) fn fd_write_internal<M: MemorySize>(\n     should_update_cursor: bool,\n     should_snapshot: bool,\n ) -> Result<Result<usize, Errno>, WasiError> {\n+    let mut offset = offset;\n     let mut env = ctx.data();\n     let state = env.state.clone();\n \n@@ -160,6 +161,12 @@ pub(crate) fn fd_write_internal<M: MemorySize>(\n                             async {\n                                 let mut handle = handle.write().unwrap();\n                                 if !is_stdio {\n+                                    if fd_entry.flags.contains(Fdflags::APPEND) {\n+                                        // `fdflags::append` means we need to seek to the end before writing.\n+                                        offset = handle.size();\n+                                        fd_entry.offset.store(offset, Ordering::Release);\n+                                    }\n+\n                                     handle\n                                         .seek(std::io::SeekFrom::Start(offset))\n                                         .await\n", "instance_id": "wasmerio__wasmer-5160", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to `fd_seek` not working correctly with `fdflags::append` in the Wasmer runtime for WASI. It provides a reproducible code snippet, expected behavior, actual behavior, and relevant version information, which helps in understanding the issue. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases (e.g., behavior with different file types or concurrent access) or constraints around the WASI environment that might affect the solution. Additionally, it lacks detail on the broader implications of the bug (e.g., does it affect other syscalls or specific use cases?). These missing details prevent it from being fully comprehensive, but the core issue is well-articulated.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, affecting two specific files (`fd_seek.rs` and `fd_write.rs`) and involving modifications to the logic around file descriptor handling in a WASI runtime. The changes are not extensive in terms of lines of code but require a precise understanding of how file offsets and flags like `APPEND` interact with seek and write operations. Second, the technical concepts involved include familiarity with WASI syscalls, file descriptor rights and flags, and asynchronous I/O handling in Rust, which are moderately complex for someone without prior experience in this domain. Third, the problem touches on edge cases implicitly (e.g., ensuring `APPEND` mode correctly updates offsets before writing), but these are not overly intricate. Finally, the changes do not appear to impact the broader system architecture significantly, as they are confined to syscall implementations. Overall, this problem requires a moderate level of expertise in Rust and WASI, along with careful handling of file offset logic, placing it at the lower end of the medium difficulty range (0.45).", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "R file type is not mapped\nBased on https://www.nerdfonts.com/cheat-sheet, R programming language file type should look like this:\r\n\r\n```bash\r\necho -e \"file_type_r should look like \\ue68a\"                                                                                                                                                                         \ue0b2 \u2714 \ue0b2 at 07:19:11 \uf017 \ue0b4\r\nfile_type_r should look like \ue68a\r\n```\r\n\r\nNerd font is correctly installed on my system (i,e, I see the icons in other terminal program, and the  `echo`, https://dystroy.org/broot/icons/, also work).\r\n\r\nHowever, they are not showing in Broot. I guess the mapping is not done.\r\n\r\n![image](https://github.com/Canop/broot/assets/4519221/0b2ddbba-c012-4825-a8ef-e190c46e5cb7)\r\n\n", "patch": "diff --git a/resources/icons/nerdfont/data/extension_to_icon_name_map.rs b/resources/icons/nerdfont/data/extension_to_icon_name_map.rs\nindex 7c9ad859..8664d90f 100644\n--- a/resources/icons/nerdfont/data/extension_to_icon_name_map.rs\n+++ b/resources/icons/nerdfont/data/extension_to_icon_name_map.rs\n@@ -374,6 +374,7 @@\n ( \"qt\"                      , \"file_type_video\"                   ),\n ( \"qvd\"                     , \"file_type_qlikview\"                ),\n ( \"qvw\"                     , \"file_type_qlikview\"                ),\n+( \"r\"                       , \"file_type_r\"                       ),\n ( \"ra\"                      , \"file_type_audio\"                   ),\n ( \"rake\"                    , \"file_type_rake\"                    ),\n ( \"rakefile\"                , \"file_type_rake\"                    ),\n", "instance_id": "Canop__broot-853", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent: the user is reporting that the R programming language file type icon is not displayed correctly in Broot, despite the Nerd Font being installed and working elsewhere. The goal of mapping the R file extension to the appropriate icon is implied, and a visual example of the expected icon is provided via the `echo` output. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly confirm whether this is a request for a feature addition or a bug fix, though the context suggests the former. Additionally, there are no explicit constraints or requirements beyond the icon mapping, and edge cases (e.g., case sensitivity of file extensions or handling of multiple extensions for R) are not mentioned. Despite these minor gaps, the issue is understandable with the provided context and screenshot.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward and minimal code change. The provided diff shows that the solution requires adding a single line to an existing mapping table in the `extension_to_icon_name_map.rs` file, associating the \"r\" extension with the \"file_type_r\" icon. This change is isolated to a single file and does not impact the broader architecture or logic of the codebase. No complex technical concepts, algorithms, or domain-specific knowledge are required beyond basic familiarity with Rust syntax and the structure of a static mapping table. There are no edge cases or error handling considerations mentioned in the problem statement or evident in the code change, as this is a simple static mapping addition. The task is essentially a trivial configuration update, fitting within the 0.0-0.2 range for very easy problems.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ergonomics of `alt()` taking tuple versus slice\nIs there a reason `alt()` cannot take a slice of child parsers?\r\n\r\nInstead, it takes a `List`, which is any of the 1- to 21-tuple implementations.\r\n\r\nThis is fine when the format for which you are trying to parse is known at compile-time/statically, but unusable when the format is only known at runtime.\r\n\r\nFor example, I'm writing a software-version bumper right now, and the user provides both their arbitrary format (e.g. `yyyy.patch`) and their current version (e.g. `2024.5`) as inputs to my package/program. First, I have a parser for the format string (which is static-defined), but then I need to create a new parser from its result for the actual version string, which is runtime-defined. Therefore, this second parser cannot be made from a tuple.\r\n\r\nI have been able to work around this by continually building a parser up in a \"reduce\" fashion, something like\r\n\r\n```rust\r\nparsers.reduce(|acc, p| acc.or(p))\r\n```\r\n\r\n(I believe that applying pairwise `or()` calls has the same effect as `alt()`, right?) Nonetheless, it's cumbersome, especially with the type annotations, for a newer Rust user like me: plenty of `Box`, `dyn`, lifetimes, `Sized`, etc.\r\n\r\nSo, I wonder why there isn't a version of alt that could take in a dynamically sized slice of child parsers.\r\n\r\n## Prerequisites\r\n\r\n- Rust version: `rustc 1.77.0-nightly (e51e98dde 2023-12-31)`\r\n- nom version : `7.1.3`\r\n- nom compilation features used: default ones\r\n\r\n\r\n\n", "patch": "diff --git a/src/branch/mod.rs b/src/branch/mod.rs\nindex 80c3d1f84..20205f595 100644\n--- a/src/branch/mod.rs\n+++ b/src/branch/mod.rs\n@@ -212,6 +212,44 @@ impl<\n   }\n }\n \n+impl<\n+    Input: Clone,\n+    Output,\n+    Error: ParseError<Input>,\n+    A: Parser<Input, Output = Output, Error = Error>,\n+  > Parser<Input> for Choice<&mut [A]>\n+{\n+  type Output = Output;\n+  type Error = Error;\n+\n+  #[inline]\n+  fn process<OM: crate::OutputMode>(\n+    &mut self,\n+    input: Input,\n+  ) -> crate::PResult<OM, Input, Self::Output, Self::Error> {\n+    let mut error = None;\n+\n+    for branch in self.parser.iter_mut() {\n+      match branch.process::<OM>(input.clone()) {\n+        Err(Err::Error(e)) => match error {\n+          None => error = Some(e),\n+          Some(err) => error = Some(OM::Error::combine(err, e, |e1, e2| e1.or(e2))),\n+        },\n+        res => return res,\n+      }\n+    }\n+\n+    match error {\n+      Some(e) => Err(Err::Error(OM::Error::map(e, |err| {\n+        Error::append(input, ErrorKind::Alt, err)\n+      }))),\n+      None => Err(Err::Error(OM::Error::bind(|| {\n+        Error::from_error_kind(input, ErrorKind::Alt)\n+      }))),\n+    }\n+  }\n+}\n+\n macro_rules! permutation_trait(\n   (\n     $name1:ident $ty1:ident $item1:ident\n", "instance_id": "rust-bakery__nom-1754", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the user's intent and the limitation they are facing with the `alt()` function in the `nom` parser combinator library. The goal is well-defined: to support a dynamically sized slice of child parsers instead of a fixed-size tuple. The user provides a practical example of their use case (software version bumper with runtime-defined formats), which helps in understanding the problem context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats for the proposed `alt()` variant, nor does it discuss potential constraints or edge cases (e.g., performance implications of using slices, memory management, or parser compatibility). Additionally, while the workaround using `reduce` and `or()` is mentioned, it lacks clarity on whether this fully addresses the issue or if there are other limitations. Overall, the statement is valid and clear but misses some critical details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code change, while seemingly localized to a single file (`branch/mod.rs`), involves adding a new implementation of the `Parser` trait for a slice-based `Choice` type. This requires a deep understanding of the `nom` library's internals, particularly how parser combinators like `alt()` are implemented and how errors are propagated and combined. Second, the technical concepts involved are moderately complex, including Rust's trait system, generics, lifetime management, and parser combinator design patterns. The code change also needs to handle error accumulation and combination logic, which adds to the complexity. Third, while the problem statement does not explicitly mention edge cases, the implementation must consider scenarios like empty slices, parser failures, and input cloning overhead, which could impact performance. Finally, this change could have broader implications for users of the library, requiring careful consideration of backward compatibility and API ergonomics. Although it does not involve a full architectural overhaul or system-level design, the need for a nuanced understanding of parser combinators and Rust's type system pushes this into the harder spectrum of medium-to-hard difficulty (0.6-0.8 range). I\u2019ve assigned a score of 0.65 to reflect the balance between the localized scope and the depth of knowledge required.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "New get_guild_role endpoint\nThe new endpoint is:\r\nGET `/guilds/{1}/roles/{2}` where `{1}` is a `GuildId` instance and `{2}` is a `RoleId` instance.\r\n\r\nAs well as the Http implementation, a helper method in the `GuildId` object could end up being useful (similar to the existing `roles` method). I can't imagine a reason to have a helper method in `RoleId`.\r\n\r\n[Discord API docs change commit](https://github.com/discord/discord-api-docs/commit/722480036365d610539ac92f37279d028a247563)\n", "patch": "diff --git a/src/http/client.rs b/src/http/client.rs\nindex b8a18cd08e3..04a9eb0c47a 100644\n--- a/src/http/client.rs\n+++ b/src/http/client.rs\n@@ -3772,6 +3772,29 @@ impl Http {\n         .await\n     }\n \n+    /// Retrieves a specific role in a [`Guild`].\n+    pub async fn get_guild_role(&self, guild_id: GuildId, role_id: RoleId) -> Result<Role> {\n+        let mut value: Value = self\n+            .fire(Request {\n+                body: None,\n+                multipart: None,\n+                headers: None,\n+                method: LightMethod::Get,\n+                route: Route::GuildRole {\n+                    guild_id,\n+                    role_id,\n+                },\n+                params: None,\n+            })\n+            .await?;\n+\n+        if let Some(map) = value.as_object_mut() {\n+            map.insert(\"guild_id\".to_string(), guild_id.get().into());\n+        }\n+\n+        from_value(value).map_err(From::from)\n+    }\n+\n     /// Retrieves a list of roles in a [`Guild`].\n     pub async fn get_guild_roles(&self, guild_id: GuildId) -> Result<Vec<Role>> {\n         let mut value: Value = self\ndiff --git a/src/model/guild/guild_id.rs b/src/model/guild/guild_id.rs\nindex a810c4ef243..3b4b67c5dd6 100644\n--- a/src/model/guild/guild_id.rs\n+++ b/src/model/guild/guild_id.rs\n@@ -899,6 +899,16 @@ impl GuildId {\n         builder.execute(cache_http, self).await\n     }\n \n+    /// Gets a specific role in the guild, by Id.\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returns [`Error::Http`] if the current user is not in the guild, or if the role does not\n+    /// exist.\n+    pub async fn role(self, http: impl AsRef<Http>, role_id: RoleId) -> Result<Role> {\n+        http.as_ref().get_guild_role(self, role_id).await\n+    }\n+\n     /// Gets all of the guild's roles over the REST API.\n     ///\n     /// # Errors\n", "instance_id": "serenity-rs__serenity-2946", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in defining the goal of adding a new endpoint `GET /guilds/{1}/roles/{2}` to retrieve a specific role in a guild, with placeholders `{1}` and `{2}` explicitly identified as `GuildId` and `RoleId` respectively. It also suggests adding a helper method in the `GuildId` object, which provides some direction for implementation. The reference to the Discord API docs change commit adds context for the requirement. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input/output formats beyond the endpoint structure, nor does it mention specific error conditions or edge cases to handle (e.g., invalid IDs, permissions issues). Additionally, the statement lacks examples or detailed constraints, which could help in understanding the full scope of the task. Despite these minor gaps, the intent and primary requirements are understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, affecting two files (`client.rs` and `guild_id.rs`). The modifications involve adding a new method `get_guild_role` in the `Http` client to handle the API request and a helper method `role` in the `GuildId` struct. The changes are straightforward, involving about 30-40 lines of code, and do not impact the broader system architecture or require extensive refactoring. The implementation mirrors existing patterns (e.g., the `get_guild_roles` method), reducing the cognitive load.\n\n2. **Technical Concepts Required:** The problem requires basic familiarity with Rust, particularly async/await syntax, HTTP client interactions, and JSON deserialization (using `from_value`). It also involves understanding the structure of the existing codebase, such as the `Route` enum and how IDs are handled. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic API client development are needed. The concepts are relatively simple for someone with moderate Rust experience.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes include basic error handling via the `Result` type. The documentation in the `GuildId::role` method mentions potential errors like the user not being in the guild or the role not existing, which are handled by the underlying HTTP client. No complex edge case logic or additional error handling beyond the standard pattern is required in the provided changes.\n\n4. **Overall Complexity:** The task involves understanding a small part of the codebase and replicating existing patterns to add a new endpoint. It does not require deep architectural changes or extensive debugging of interactions between modules. The primary challenge might be ensuring the API request format matches the Discord API specification, but this is mitigated by the provided commit reference and the simplicity of the GET request.\n\nGiven these factors, a difficulty score of 0.30 reflects an easy task that requires understanding some code logic and making simple function additions, suitable for a developer with basic to intermediate Rust skills and minimal familiarity with the codebase.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "XML deserialization of empty tags is incorrect\nAs a working example, I am going to reference the following area of code, but I believe it is possible to run into this in any area that the library is parsing an `Option<String>` field: https://github.com/CycloneDX/cyclonedx-rust-cargo/blob/2911287b2520a7ddab1782b48c35112279b1be17/cyclonedx-bom/src/specs/common/component.rs#L513-L517\r\n\r\nSetting context, I was attempting to swap out an ad-hoc CycloneDX parser with `cyclonedx-bom`. One of the tests for my ad-hoc parser was to ensure that the keycloak BOM found here loads: https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/keycloak-10.0.2/bom.xml. Now, it obviously doesn't because it is a v1.2 spec and `cyclonedx-bom` only supports v1.3-v1.5. But, looking through the spec and looking at the BOM, it seemed like it should load as a v1.3, so I changed the version locally. That seemed a reasonable test given that a diff between https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/laravel-7.12.0/bom.1.2.xml and https://github.com/CycloneDX/bom-examples/blob/c0436d86cd60693f01d19fe1aacfd01e70e17036/SBOM/laravel-7.12.0/bom.1.3.xml only added some properties and changed the spec version. \r\n\r\nHowever, when I attempted to load the modified keycloak BOM, I got the following error:\r\n\r\n```\r\n    Got unexpected XML element when reading {http://cyclonedx.org/schema/bom/1.3}description: Got unexpected element EndElement({http://cyclonedx.org/schema/bom/1.3}description)\r\n```\r\n\r\nAfter digging into the issue, what I discovered was that the keycloak BOM has several empty description tags in it:\r\n\r\n```xml\r\n<description />\r\n```\r\n\r\nA quick modification of the `laravel-7.12.0/bom.1.3.xml` file to modify one of the description tags the same way caused the same error to occur. I also attempted to just modify one of the description like the following and received the same error:\r\n\r\n```xml\r\n<description></description>\r\n```\r\n\r\nThe [v1.3 (XML)](https://cyclonedx.org/docs/1.3/xml/#type_component) and [v1.2 (XML)](https://cyclonedx.org/docs/1.2/xml/#type_component) specs \r\nboth define the description field as `<bom:description> xs:normalizedString </bom:description> [0..1]` and an `xs:normalizedString` _can_ be empty.\r\n\r\nAgain, there are multiple places just in the `Component::read_xml_element` where this could be an issue. My guess is that, since the `pom.xml` file that is used to generate the CycloneDX Maven BOMs can have empty strings for the description, that is why it is getting forwarded on to the BOM that way instead of stripping out the description tag completely. \r\n\r\nI could make an argument for handling it either by making an empty string become a `None` for the `Option<String>` OR by literally making it an `Some(String::new())`. I think that the latter is closer to the behavior that would happen with the JSON implementation of the spec, since an equivalent there would be:\r\n\r\n```json\r\n\"description\": \"\"\r\n```\r\n\r\nand that would be parsed as `Some(String::new())` by `serde`.\r\n\r\n\n", "patch": "diff --git a/cyclonedx-bom/src/xml.rs b/cyclonedx-bom/src/xml.rs\nindex 20541279..2cd8f193 100644\n--- a/cyclonedx-bom/src/xml.rs\n+++ b/cyclonedx-bom/src/xml.rs\n@@ -442,6 +442,17 @@ impl FromXmlType for f32 {\n     }\n }\n \n+/// Reads a simple String tag.\n+///\n+/// ```xml\n+/// <description>Content</description>\n+/// ```\n+/// &\n+/// ```xml\n+/// <description />\n+/// ```\n+///\n+/// are valid XML tags. The first returns the string \"Content\", the latter is an empty string.\n pub(crate) fn read_simple_tag<R: Read>(\n     event_reader: &mut EventReader<R>,\n     element: &OwnedName,\n@@ -449,13 +460,19 @@ pub(crate) fn read_simple_tag<R: Read>(\n     let element_display = element.to_string();\n     let content = event_reader\n         .next()\n-        .map_err(to_xml_read_error(&element_display))\n-        .and_then(inner_text_or_error(&element_display))?;\n-\n-    event_reader\n-        .next()\n-        .map_err(to_xml_read_error(&element_display))\n-        .and_then(closing_tag_or_error(element))?;\n+        .map_err(to_xml_read_error(&element_display))?;\n+\n+    let content = match content {\n+        reader::XmlEvent::EndElement { .. } => String::new(),\n+        reader::XmlEvent::Characters(content) | reader::XmlEvent::CData(content) => {\n+            event_reader\n+                .next()\n+                .map_err(to_xml_read_error(&element_display))\n+                .and_then(closing_tag_or_error(element))?;\n+            content\n+        }\n+        unexpected => return Err(unexpected_element_error(element, unexpected)),\n+    };\n \n     Ok(content)\n }\n", "instance_id": "CycloneDX__cyclonedx-rust-cargo-761", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "\nThe problem statement is mostly clear and provides a detailed description of the issue with XML deserialization of empty tags in the `cyclonedx-bom` library. It includes specific references to the codebase, example XML files, and links to relevant specifications (v1.2 and v1.3 of CycloneDX). The issue is well-contextualized with a reproducible error and a clear explanation of the expected behavior versus the actual behavior. Additionally, the problem statement discusses potential solutions (e.g., mapping empty tags to `None` or `Some(String::new())`) and ties the issue to broader implications in the library.\n\nHowever, there are minor ambiguities that prevent a perfect score. The problem statement does not explicitly define the desired outcome or preference for handling empty tags (e.g., whether `None` or `Some(String::new())` is the intended fix). It also lacks explicit mention of specific edge cases beyond empty tags (e.g., malformed XML or nested tags) that might need consideration. While examples are provided, they are external and require additional effort to fully replicate the issue. Overall, the statement is clear enough to understand the problem and start working on a solution, but it misses some finer details.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.35, placing it in the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Clarity and Complexity of the Problem Description**: The problem logic is relatively straightforward\u2014handling empty XML tags during deserialization. It does not involve complex algorithms or intricate domain-specific logic beyond understanding XML parsing. The issue is isolated to how empty tags are processed in the deserialization logic.\n\n2. **Scope and Depth of Code Changes**: The provided diff shows a modification to a single utility function (`read_simple_tag`) in the `xml.rs` file. The change is localized and does not impact multiple modules or the broader architecture of the system. The amount of code change is small (around 10-15 lines), focusing on adjusting the logic to handle empty tags by returning an empty string (`String::new()`) when an immediate `EndElement` event is encountered. There is no indication of systemic impact or need for refactoring other parts of the codebase.\n\n3. **Number of Technical Concepts**: Solving this requires a basic understanding of Rust, specifically working with XML parsing libraries (likely `xml-rs` based on the code structure). The concepts involved are relatively simple: handling XML events, pattern matching, and error propagation. No advanced language features, complex algorithms, or design patterns are necessary. Familiarity with the `cyclonedx-bom` library's structure is helpful but not critical, as the change is isolated to a utility function.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement focuses on empty tags (`<description />` and `<description></description>`), which are the primary edge cases addressed in the code change. The diff handles these cases by returning an empty string for self-closing tags. However, the problem statement does not mention other potential edge cases (e.g., malformed XML, unexpected nested tags, or whitespace-only content), and the code change does not introduce additional error handling beyond what already exists. The complexity of edge cases is low, as the fix is narrowly scoped to empty tags.\n\nOverall, this problem is easy because it involves a small, localized change to handle a specific XML parsing scenario. It requires basic familiarity with Rust and XML event parsing but does not demand deep knowledge of the codebase or advanced technical skills. The difficulty is slightly above the \"Very Easy\" range (0.0-0.2) due to the need to understand XML event handling and ensure the change does not inadvertently break other parsing logic, but it remains within the \"Easy\" category.\n", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "In Atuin UI interprets backspace as literal ^h\nOpening the Atuin UI and pressing the backspace key writes an h to the input. To delete the previous character I need to press ctrl+backspace.\r\n\r\nI assume it has to do with backspace, `^H`, being interpreted as the literal `^H` in some way, hence why ctrl \"cancels out\" the `^`.\r\n\r\nI can reproduce in:\r\n - [cool-retro-terminal](https://github.com/Swordfish90/cool-retro-term)\r\n - xterm\r\n \r\nOS: Arch linux\r\nAtuin version: 18.0.1\n", "patch": "diff --git a/atuin/src/command/client/search/interactive.rs b/atuin/src/command/client/search/interactive.rs\nindex f74be6433fa..a12479641eb 100644\n--- a/atuin/src/command/client/search/interactive.rs\n+++ b/atuin/src/command/client/search/interactive.rs\n@@ -379,6 +379,21 @@ impl State {\n             KeyCode::Backspace => {\n                 self.search.input.back();\n             }\n+            KeyCode::Char('h' | '?') if ctrl => {\n+                // Depending on the terminal, [Backspace] can be transmitted as\n+                // \\x08 or \\x7F.  Also, [Ctrl+Backspace] can be transmitted as\n+                // \\x08 or \\x7F or \\x1F.  On the other hand, [Ctrl+h] and\n+                // [Ctrl+?] are also transmitted as \\x08 or \\x7F by the\n+                // terminals.\n+                //\n+                // The crossterm library translates \\x08 and \\x7F to C-h and\n+                // Backspace, respectively.  With the extended keyboard\n+                // protocol enabled, crossterm can faithfully translate\n+                // [Ctrl+h] and [Ctrl+?] to C-h and C-?.  There is no perfect\n+                // solution, but we treat C-h and C-? the same as backspace to\n+                // suppress quirks as much as possible.\n+                self.search.input.back();\n+            }\n             KeyCode::Delete if ctrl => self\n                 .search\n                 .input\n", "instance_id": "atuinsh__atuin-1857", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: pressing the backspace key in the Atuin UI results in an 'h' being input instead of deleting a character, and Ctrl+Backspace is required to delete. The user provides context by mentioning specific terminals (cool-retro-term and xterm) and the OS (Arch Linux), along with the Atuin version (18.0.1). The assumption about backspace being interpreted as a literal '^H' and the role of Ctrl in canceling it out adds some helpful insight. However, the statement lacks critical details such as explicit input/output expectations, specific conditions under which the issue occurs (e.g., terminal configurations or settings), and whether there are other related behaviors or edge cases to consider. Additionally, there are no examples of expected versus actual behavior in a structured format. Thus, while the problem is valid and mostly clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the 'Easy' range (0.2-0.4) due to the following analysis across the specified factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is localized to a single file (`interactive.rs`) and specifically to the handling of key events within a `State` implementation. The modification involves adding a conditional block to handle `Ctrl+h` and `Ctrl+?` as backspace equivalents, which is a small and focused change (approximately 15 lines of code including comments). There is no indication of impact on the broader system architecture or interactions with other modules, keeping the scope limited.\n\n2. **Number of Technical Concepts:** Solving this requires understanding terminal input handling, specifically how different terminals encode keypresses like Backspace and Ctrl-modified keys (e.g., \\x08, \\x7F, \\x1F). It also involves familiarity with the `crossterm` library used for terminal input in Rust, particularly its key event translation behavior. While these concepts are not trivial, they are relatively niche and do not require advanced algorithms, design patterns, or deep domain-specific knowledge beyond terminal I/O. The solution leverages basic control flow and input manipulation logic.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code change and accompanying comment suggest awareness of terminal-specific quirks (e.g., different encodings for Backspace and Ctrl+Backspace). The solution attempts to mitigate these by treating `Ctrl+h` and `Ctrl+?` as backspace, indicating some consideration of variability in terminal behavior. However, no additional error handling logic is introduced, and the complexity of edge cases appears moderate, limited to input interpretation rather than broader system impacts.\n\n4. **Overall Complexity:** The problem requires understanding a specific bug related to terminal input and applying a targeted fix. It does not demand deep architectural changes, performance optimization, or complex refactoring. The primary challenge lies in correctly interpreting terminal key codes and ensuring compatibility across different terminal environments, which is a moderately specialized but not overly complex task for someone familiar with Rust and terminal libraries like `crossterm`.\n\nGiven these factors, a difficulty score of 0.35 is appropriate, reflecting an 'Easy' problem that requires some understanding of terminal input handling and a straightforward code modification, but does not pose significant challenges in terms of codebase impact or conceptual depth.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Automatically optimize batch size for indexed CSVs\nThere are several parallelized commands with a `--batch` size parameter (often with a default of 50k)\r\n\r\nIf a CSV is indexed, automatically optimize the `--batch` parameter so that its set to abs(row_count/num_cpus) + 1.\r\n\r\nIn this way, we can process the command using optimum number of parallelized passes.\r\n\r\nFor example, the 1 million row NYC 311 benchmark data with an index, will have a `batch` of 62,500 rows on a 16 core CPU (1,000,000 / 16 ) instead of the default 50k.\r\n\r\nCurrently, 1M rows is processed in 20 batches of 50k rows. With this approach, it's processed in 16 batches of 62.5k rows.\n", "patch": "diff --git a/src/cmd/apply.rs b/src/cmd/apply.rs\nindex 6048222ca..296fcae1f 100644\n--- a/src/cmd/apply.rs\n+++ b/src/cmd/apply.rs\n@@ -286,7 +286,9 @@ apply options:\n     -j, --jobs <arg>            The number of jobs to run in parallel.\n                                 When not set, the number of jobs is set to the number of CPUs detected.\n     -b, --batch <size>          The number of rows per batch to load into memory, before running in parallel.\n-                                Set to 0 to load all rows in one batch.\n+                                Automatically determined for CSV files with more than 50000 rows.\n+                                Set to 0 to load all rows in one batch. Set to 1 to force batch optimization\n+                                even for files with less than 50000 rows.\n                                 [default: 50000]\n \n Common options:\n@@ -566,32 +568,22 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n     #[allow(unused_assignments)]\n     let mut batch_record = csv::StringRecord::new();\n \n+    // set RAYON_NUM_THREADS\n+    let num_jobs = util::njobs(args.flag_jobs);\n+\n     // reuse batch buffers\n-    let batchsize: usize = if args.flag_batch == 0 {\n-        util::count_rows(&rconfig)? as usize\n-    } else {\n-        args.flag_batch\n-    };\n+    let batchsize = util::optimal_batch_size(&rconfig, args.flag_batch, num_jobs);\n     let mut batch = Vec::with_capacity(batchsize);\n     let mut batch_results = Vec::with_capacity(batchsize);\n \n-    // set RAYON_NUM_THREADS\n-    util::njobs(args.flag_jobs);\n-\n     // main loop to read CSV and construct batches for parallel processing.\n     // each batch is processed via Rayon parallel iterator.\n     // loop exits when batch is empty.\n     'batch_loop: loop {\n         for _ in 0..batchsize {\n             match rdr.read_record(&mut batch_record) {\n-                Ok(has_data) => {\n-                    if has_data {\n-                        batch.push(std::mem::take(&mut batch_record));\n-                    } else {\n-                        // nothing else to add to batch\n-                        break;\n-                    }\n-                },\n+                Ok(true) => batch.push(std::mem::take(&mut batch_record)),\n+                Ok(false) => break, // nothing else to add to batch\n                 Err(e) => {\n                     return fail_clierror!(\"Error reading file: {e}\");\n                 },\ndiff --git a/src/cmd/datefmt.rs b/src/cmd/datefmt.rs\nindex 55645d708..efdd5cfbf 100644\n--- a/src/cmd/datefmt.rs\n+++ b/src/cmd/datefmt.rs\n@@ -84,7 +84,9 @@ datefmt options:\n     -j, --jobs <arg>            The number of jobs to run in parallel.\n                                 When not set, the number of jobs is set to the number of CPUs detected.\n     -b, --batch <size>          The number of rows per batch to load into memory, before running in parallel.\n-                                Set to 0 to load all rows in one batch.\n+                                Automatically determined for CSV files with more than 50000 rows.\n+                                Set to 0 to load all rows in one batch. Set to 1 to force batch optimization\n+                                even for files with less than 50000 rows.\n                                 [default: 50000]\n \n Common options:\n@@ -252,12 +254,11 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n     #[allow(unused_assignments)]\n     let mut batch_record = csv::StringRecord::new();\n \n+    // set RAYON_NUM_THREADS\n+    let num_jobs = util::njobs(args.flag_jobs);\n+\n     // reuse batch buffers\n-    let batchsize: usize = if args.flag_batch == 0 {\n-        util::count_rows(&rconfig)? as usize\n-    } else {\n-        args.flag_batch\n-    };\n+    let batchsize = util::optimal_batch_size(&rconfig, args.flag_batch, num_jobs);\n     let mut batch = Vec::with_capacity(batchsize);\n     let mut batch_results = Vec::with_capacity(batchsize);\n \n@@ -316,23 +317,14 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n \n     let is_output_utc = output_tz == chrono_tz::UTC;\n \n-    // set RAYON_NUM_THREADS\n-    util::njobs(args.flag_jobs);\n-\n     // main loop to read CSV and construct batches for parallel processing.\n     // each batch is processed via Rayon parallel iterator.\n     // loop exits when batch is empty.\n     'batch_loop: loop {\n         for _ in 0..batchsize {\n             match rdr.read_record(&mut batch_record) {\n-                Ok(has_data) => {\n-                    if has_data {\n-                        batch.push(std::mem::take(&mut batch_record));\n-                    } else {\n-                        // nothing else to add to batch\n-                        break;\n-                    }\n-                },\n+                Ok(true) => batch.push(std::mem::take(&mut batch_record)),\n+                Ok(false) => break, // nothing else to add to batch\n                 Err(e) => {\n                     return fail_clierror!(\"Error reading file: {e}\");\n                 },\ndiff --git a/src/cmd/tojsonl.rs b/src/cmd/tojsonl.rs\nindex 53b9f58d6..fea6ad4fa 100644\n--- a/src/cmd/tojsonl.rs\n+++ b/src/cmd/tojsonl.rs\n@@ -27,8 +27,12 @@ Tojsonl options:\n                            When not set, the number of jobs is set to the\n                            number of CPUs detected.\n     -b, --batch <size>     The number of rows per batch to load into memory,\n-                           before running in parallel. Set to 0 to load all\n-                           rows in one batch. [default: 50000]                           \n+                           before running in parallel. Automatically determined\n+                           for CSV files with more than 50000 rows.\n+                           Set to 0 to load all rows in one batch.\n+                           Set to 1 to force batch optimization even for files with\n+                           less than 50000 rows.\n+                           [default: 50000]                           \n \n Common options:\n     -h, --help             Display this message\n@@ -255,32 +259,22 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n     #[allow(unused_assignments)]\n     let mut batch_record = csv::StringRecord::new();\n \n+    // set RAYON_NUM_THREADS\n+    let num_jobs = util::njobs(args.flag_jobs);\n+\n     // reuse batch buffers\n-    let batchsize: usize = if args.flag_batch == 0 {\n-        record_count as usize\n-    } else {\n-        args.flag_batch\n-    };\n+    let batchsize = util::optimal_batch_size(&conf, args.flag_batch, num_jobs);\n     let mut batch = Vec::with_capacity(batchsize);\n     let mut batch_results = Vec::with_capacity(batchsize);\n \n-    // set RAYON_NUM_THREADS\n-    util::njobs(args.flag_jobs);\n-\n     // main loop to read CSV and construct batches for parallel processing.\n     // each batch is processed via Rayon parallel iterator.\n     // loop exits when batch is empty.\n     'batch_loop: loop {\n         for _ in 0..batchsize {\n             match rdr.read_record(&mut batch_record) {\n-                Ok(has_data) => {\n-                    if has_data {\n-                        batch.push(batch_record.clone());\n-                    } else {\n-                        // nothing else to add to batch\n-                        break;\n-                    }\n-                },\n+                Ok(true) => batch.push(std::mem::take(&mut batch_record)),\n+                Ok(false) => break, // nothing else to add to batch\n                 Err(e) => {\n                     return fail_clierror!(\"Error reading file: {e}\");\n                 },\ndiff --git a/src/cmd/validate.rs b/src/cmd/validate.rs\nindex b3d8c251f..f658534d3 100644\n--- a/src/cmd/validate.rs\n+++ b/src/cmd/validate.rs\n@@ -106,8 +106,11 @@ Validate options:\n                                When not set, the number of jobs is set to the\n                                number of CPUs detected.\n     -b, --batch <size>         The number of rows per batch to load into memory,\n-                               before running in parallel. Set to 0 to load all rows in one batch.\n-                               [default: 50000]\n+                               before running in parallel. Automatically determined\n+                               for CSV files with more than 50000 rows.\n+                               Set to 0 to load all rows in one batch.\n+                               Set to 1 to force batch optimization even for files with\n+                               less than 50000 rows. [default: 50000]\n     --timeout <seconds>        Timeout for downloading json-schemas on URLs and for\n                                'dynamicEnum' lookups on URLs. [default: 30]\n \n@@ -647,21 +650,18 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n \n     // amortize memory allocation by reusing record\n     let mut record = csv::ByteRecord::with_capacity(500, header_len);\n+\n+    // set RAYON_NUM_THREADS\n+    let num_jobs = util::njobs(args.flag_jobs);\n+\n     // reuse batch buffer\n-    let batch_size = if args.flag_batch == 0 {\n-        util::count_rows(&rconfig)? as usize\n-    } else {\n-        args.flag_batch\n-    };\n+    let batch_size = util::optimal_batch_size(&rconfig, args.flag_batch, num_jobs);\n     let mut batch = Vec::with_capacity(batch_size);\n     let mut validation_results = Vec::with_capacity(batch_size);\n     let mut valid_flags: Vec<bool> = Vec::with_capacity(batch_size);\n     let mut validation_error_messages: Vec<String> = Vec::with_capacity(50);\n     let flag_trim = args.flag_trim;\n \n-    // set RAYON_NUM_THREADS\n-    util::njobs(args.flag_jobs);\n-\n     // amortize buffer allocation\n     let mut buffer = itoa::Buffer::new();\n \ndiff --git a/src/util.rs b/src/util.rs\nindex d38991dee..ead067bda 100644\n--- a/src/util.rs\n+++ b/src/util.rs\n@@ -46,6 +46,8 @@ macro_rules! regex_oncelock {\n // leave at least 20% of the available memory free\n const DEFAULT_FREEMEMORY_HEADROOM_PCT: u8 = 20;\n \n+const DEFAULT_BATCH_SIZE: usize = 50_000;\n+\n static ROW_COUNT: OnceLock<Option<u64>> = OnceLock::new();\n \n pub type ByteString = Vec<u8>;\n@@ -2207,6 +2209,31 @@ pub fn csv_to_jsonl(\n     Ok(writer.flush()?)\n }\n \n+/// get the optimal batch size\n+/// if batch_size is 0, return the number of rows in the CSV, effectively disabling batching\n+/// if batch_size is 1, force batch_size to be set to \"optimal_size\", even though\n+/// its not recommended (number of rows is too small for parallel processing)\n+/// if batch_size is equal to DEFAULT_BATCH_SIZE, return the optimal_size\n+/// failing everything above, return the requested batch_size\n+#[inline]\n+pub fn optimal_batch_size(rconfig: &Config, batch_size: usize, num_jobs: usize) -> usize {\n+    if batch_size < DEFAULT_BATCH_SIZE {\n+        return DEFAULT_BATCH_SIZE;\n+    }\n+\n+    let num_rows = count_rows(rconfig).unwrap_or(DEFAULT_BATCH_SIZE as u64) as usize;\n+    if batch_size == 0 {\n+        num_rows\n+    } else if (num_rows > DEFAULT_BATCH_SIZE && (batch_size == DEFAULT_BATCH_SIZE))\n+        || batch_size == 1\n+    {\n+        let optimal_size = (num_rows / num_jobs) + 1;\n+        optimal_size\n+    } else {\n+        batch_size\n+    }\n+}\n+\n // comment out for now as this is still WIP\n // pub fn create_json_record(\n //     no_headers: bool,\n", "instance_id": "dathere__qsv-2178", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of optimizing the batch size for indexed CSVs based on the number of rows and available CPU cores. It provides a concrete example (1M rows on a 16-core CPU resulting in a batch size of 62,500) and explains the intended outcome (reducing the number of batches for better parallelization). However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes an \"indexed CSV\" or how the system determines if a CSV is indexed. Additionally, the problem statement lacks clarity on edge cases, such as how to handle very small files, extremely large files, or systems with a single CPU core. Constraints on input size or CPU detection are also not mentioned. While the intent is understandable, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (apply.rs, datefmt.rs, tojsonl.rs, validate.rs, and util.rs), requiring consistent modifications to batch size logic and command-line argument handling. However, the changes are relatively localized to specific functions and do not appear to impact the broader system architecture significantly. The amount of code change is moderate, involving updates to argument descriptions, batch size calculations, and loop logic for reading CSV records. \n\nTechnically, the problem requires understanding Rust's parallel processing (via Rayon, as hinted by RAYON_NUM_THREADS), CSV parsing, and command-line argument handling. The logic for calculating the optimal batch size is straightforward (dividing row count by the number of CPU cores), but implementing it correctly requires familiarity with the existing codebase's configuration and utility functions (e.g., count_rows, njobs). The introduction of a new utility function (optimal_batch_size) adds a small layer of complexity in ensuring it integrates well with existing code.\n\nEdge cases and error handling add some difficulty. The code changes account for scenarios like forcing batch optimization for small files (batch size = 1) or disabling batching (batch size = 0), but the problem statement does not explicitly address potential issues like row count estimation failures, non-integer batch sizes, or CPU detection errors. Implementing robust error handling for these scenarios would require additional effort.\n\nOverall, this problem is of medium difficulty (0.45) because it involves understanding multiple parts of the codebase, making consistent changes across files, and handling some implicit edge cases, but it does not require deep architectural changes or advanced technical concepts beyond standard Rust programming and parallel processing basics.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug: histogram function with bad argument cause panic\n### Search before asking\n\n- [X] I had searched in the [issues](https://github.com/databendlabs/databend/issues) and found no similar issues.\n\n\n### Version\n\nmain\n\n### What's Wrong?\n\nfound by duckdb fuzz test\r\n```\r\npanicked at src/query/expression/src/values.rs:453:18:\r\ninternal error: entered unreachable code: is_positive() called on non-numeric scalar\r\n```\n\n### How to Reproduce?\n\n```sql\r\nroot@0.0.0.0:48000/default> CREATE TABLE obs (n Int64);\r\nroot@0.0.0.0:48000/default> SELECT histogram(n, [10, 20, 30, 40, 50]) FROM obs;\r\nerror: APIError: QueryFailed: [1104]internal error: entered unreachable code: is_positive() called on non-numeric scalar\r\n```\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n", "patch": "diff --git a/src/query/expression/src/type_check.rs b/src/query/expression/src/type_check.rs\nindex 1e338cea0f9c3..3f11e208c565f 100755\n--- a/src/query/expression/src/type_check.rs\n+++ b/src/query/expression/src/type_check.rs\n@@ -250,7 +250,11 @@ pub fn check_number<Index: ColumnIndex, T: Number>(\n             ErrorCode::InvalidArgument(format!(\"Expect {}, but got {}\", T::data_type(), origin_ty))\n                 .set_span(span)\n         }),\n-        _ => Err(ErrorCode::InvalidArgument(\"Need constant number\").set_span(span)),\n+        _ => Err(ErrorCode::InvalidArgument(format!(\n+            \"Need constant number, but got {}\",\n+            expr.sql_display()\n+        ))\n+        .set_span(span)),\n     }\n }\n \ndiff --git a/src/query/sql/src/planner/semantic/type_check.rs b/src/query/sql/src/planner/semantic/type_check.rs\nindex df2eac94a1ced..02789950c9939 100644\n--- a/src/query/sql/src/planner/semantic/type_check.rs\n+++ b/src/query/sql/src/planner/semantic/type_check.rs\n@@ -1729,19 +1729,14 @@ impl<'a> TypeChecker<'a> {\n             && arguments.len() == 2\n             && params.is_empty()\n         {\n-            let max_num_buckets = ConstantExpr::try_from(arguments[1].clone());\n-\n-            let is_positive_integer = match &max_num_buckets {\n-                Ok(v) => v.value.is_positive(),\n-                Err(_) => false,\n-            } && arg_types[1].is_integer();\n-            if !is_positive_integer {\n-                return Err(ErrorCode::SemanticError(\n-                    \"The max_num_buckets of `histogram` must be a constant positive int\",\n-                ));\n-            }\n+            let max_num_buckets: u64 = check_number(\n+                None,\n+                &FunctionContext::default(),\n+                &arguments[1].as_expr()?,\n+                &BUILTIN_FUNCTIONS,\n+            )?;\n \n-            vec![max_num_buckets.unwrap().value]\n+            vec![Scalar::Number(NumberScalar::UInt64(max_num_buckets))]\n         } else {\n             params\n         };\n", "instance_id": "databendlabs__databend-17342", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a panic occurs in the `histogram` function due to a bad argument, specifically when a non-numeric scalar is provided, leading to an unreachable code error. The reproduction steps are provided via SQL queries, which help in understanding the context of the bug. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input types or constraints for the `histogram` function arguments (e.g., what constitutes a valid input for `max_num_buckets`). Additionally, edge cases or other potential failure scenarios are not mentioned, which could impact the completeness of the solution. Despite these minor gaps, the issue is valid and the goal (fixing the panic) is evident from the description and reproduction steps.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, affecting two files (`type_check.rs` and `semantic/type_check.rs`). The modifications involve updating error messages for clarity and refactoring the type checking logic for the `histogram` function's argument (`max_num_buckets`). The changes do not impact the broader system architecture and are limited to specific functions, requiring minimal understanding of the overall codebase interactions. The amount of code change is small, with only a few lines modified or added.\n\n2. **Technical Concepts Involved:** Solving this issue requires a basic understanding of Rust's type system, error handling (using `ErrorCode`), and function argument validation. The concept of type checking and ensuring a constant positive integer for `max_num_buckets` is straightforward for someone familiar with Rust. No advanced algorithms, design patterns, or domain-specific knowledge (beyond SQL query processing) are needed. The use of `check_number` function and handling of scalar values is a moderate but not overly complex concept.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond the provided reproduction scenario (non-numeric scalar input). However, the code changes improve error messaging to make debugging easier, which indirectly addresses potential future issues. The error handling logic is simple\u2014returning a descriptive error instead of panicking\u2014and does not involve complex edge case management.\n\n4. **Overall Complexity:** The task involves understanding the cause of the panic (a type mismatch or invalid input) and applying a targeted fix by leveraging existing type checking utilities. This does not require deep architectural changes or advanced debugging skills, making it accessible to developers with intermediate Rust experience.\n\nA score of 0.35 reflects that while the problem is slightly more involved than a trivial fix (e.g., changing a constant), it remains within the realm of straightforward bug fixes that require understanding specific code logic and making targeted modifications.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "GELF codec should support compression\n### A note for the community\n\n<!-- Please keep this note for the community -->\r\n* Please vote on this issue by adding a \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment\r\n<!-- Thank you for keeping this note for the community -->\r\n\n\n### Use Cases\n\nCurrently, the GELF codec fails to decode compressed messages received via UDP. \r\n\r\n[GELF](https://go2docs.graylog.org/current/getting_in_log_data/gelf.html) supports compression:\r\n\r\n> When using UDP as transport layer, GELF messages can be sent uncompressed or compressed with either GZIP or ZLIB.\r\n> Graylog nodes automatically detect the compression type in the GELF magic byte header.\r\n> Decide if you want to trade a bit more CPU load for saving network bandwidth. GZIP is the protocol default.\r\n\r\n\n\n### Attempted Solutions\n\nIf I understand that correctly, decompression is just not  yet implemented.\r\n\n\n### Proposal\n\nThe[ Graylog GELF decoder](https://github.com/Graylog2/graylog2-server/blob/6dab8975f5d80c90e6346f1fcab30af11bcadf19/graylog2-server/src/main/java/org/graylog2/inputs/codecs/gelf/GELFMessage.java#L108) decides whether it is gzip, zlib, or uncompressed by looking a the magic numbers in the first bytes, then decompress accordingly. It also detects Chunking (#13292) this way.\n\n### References\n\n#13292\r\n#4868 \n\n### Version\n\n_No response_\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex b6a4bccabc306..baa3385573b5e 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -2225,6 +2225,7 @@ dependencies = [\n  \"csv-core\",\n  \"derivative\",\n  \"dyn-clone\",\n+ \"flate2\",\n  \"futures 0.3.31\",\n  \"indoc\",\n  \"influxdb-line-protocol\",\n@@ -3549,6 +3550,7 @@ dependencies = [\n  \"tempfile\",\n  \"tokio\",\n  \"tracing 0.1.40\",\n+ \"vector-common\",\n  \"vector-config\",\n  \"vector-config-common\",\n  \"vector-config-macros\",\n@@ -10968,6 +10970,7 @@ dependencies = [\n  \"typetag\",\n  \"url\",\n  \"uuid\",\n+ \"vector-common\",\n  \"vector-config\",\n  \"vector-config-common\",\n  \"vector-config-macros\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 299b5d2151cc0..3620d930541a8 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -137,6 +137,7 @@ cfg-if = { version = \"1.0.0\", default-features = false }\n chrono = { version = \"0.4.38\", default-features = false, features = [\"clock\", \"serde\"] }\n chrono-tz = { version = \"0.10.0\", default-features = false, features = [\"serde\"] }\n clap = { version = \"4.5.21\", default-features = false, features = [\"derive\", \"error-context\", \"env\", \"help\", \"std\", \"string\", \"usage\", \"wrap_help\"] }\n+flate2 = { version = \"1.0.35\", default-features = false, features = [\"default\"] }\n futures = { version = \"0.3.31\", default-features = false, features = [\"compat\", \"io-compat\", \"std\"], package = \"futures\" }\n glob = { version = \"0.3.1\", default-features = false }\n indexmap = { version = \"2.6.0\", default-features = false, features = [\"serde\", \"std\"] }\n@@ -179,6 +180,7 @@ dnsmsg-parser = { path = \"lib/dnsmsg-parser\", optional = true }\n fakedata = { path = \"lib/fakedata\", optional = true }\n portpicker = { path = \"lib/portpicker\" }\n tracing-limit = { path = \"lib/tracing-limit\" }\n+vector-common = { path = \"lib/vector-common\", default-features = false}\n vector-lib.workspace = true\n vector-config.workspace = true\n vector-config-common.workspace = true\n@@ -306,7 +308,7 @@ dyn-clone = { version = \"1.0.17\", default-features = false }\n encoding_rs = { version = \"0.8.35\", default-features = false, features = [\"serde\"] }\n enum_dispatch = { version = \"0.3.13\", default-features = false }\n exitcode = { version = \"1.1.2\", default-features = false }\n-flate2 = { version = \"1.0.35\", default-features = false, features = [\"default\"] }\n+flate2.workspace = true\n futures-util = { version = \"0.3.29\", default-features = false }\n glob.workspace = true\n governor = { version = \"0.7.0\", default-features = false, features = [\"dashmap\", \"jitter\", \"std\"], optional = true }\ndiff --git a/changelog.d/21153-chunked_gelf_decompression.feature.md b/changelog.d/21153-chunked_gelf_decompression.feature.md\nnew file mode 100644\nindex 0000000000000..388af4b86711a\n--- /dev/null\n+++ b/changelog.d/21153-chunked_gelf_decompression.feature.md\n@@ -0,0 +1,3 @@\n+Chunked gelf decoder now allows for payload decompression.\n+\n+authors: jorgehermo9\ndiff --git a/lib/codecs/Cargo.toml b/lib/codecs/Cargo.toml\nindex ece90dc2aafcb..f4ab9561f75a0 100644\n--- a/lib/codecs/Cargo.toml\n+++ b/lib/codecs/Cargo.toml\n@@ -16,6 +16,7 @@ chrono.workspace = true\n csv-core = { version = \"0.1.10\", default-features = false }\n derivative = { version = \"2\", default-features = false }\n dyn-clone = { version = \"1\", default-features = false }\n+flate2.workspace = true\n influxdb-line-protocol = { version = \"2\", default-features = false }\n lookup = { package = \"vector-lookup\", path = \"../vector-lookup\", default-features = false, features = [\"test\"] }\n memchr = { version = \"2\", default-features = false }\ndiff --git a/lib/codecs/src/decoding/framing/chunked_gelf.rs b/lib/codecs/src/decoding/framing/chunked_gelf.rs\nindex e29d304e972f4..bbddc457a3d3d 100644\n--- a/lib/codecs/src/decoding/framing/chunked_gelf.rs\n+++ b/lib/codecs/src/decoding/framing/chunked_gelf.rs\n@@ -1,17 +1,19 @@\n-use crate::{BytesDecoder, StreamDecodingError};\n-\n use super::{BoxedFramingError, FramingError};\n+use crate::{BytesDecoder, StreamDecodingError};\n use bytes::{Buf, Bytes, BytesMut};\n use derivative::Derivative;\n-use snafu::{ensure, Snafu};\n+use flate2::read::{MultiGzDecoder, ZlibDecoder};\n+use snafu::{ensure, ResultExt, Snafu};\n use std::any::Any;\n use std::collections::HashMap;\n+use std::io::Read;\n use std::sync::{Arc, Mutex};\n use std::time::Duration;\n use tokio;\n use tokio::task::JoinHandle;\n use tokio_util::codec::Decoder;\n-use tracing::{debug, warn};\n+use tracing::{debug, trace, warn};\n+use vector_common::constants::{GZIP_MAGIC, ZLIB_MAGIC};\n use vector_config::configurable_component;\n \n const GELF_MAGIC: &[u8] = &[0x1e, 0x0f];\n@@ -38,6 +40,7 @@ impl ChunkedGelfDecoderConfig {\n             self.chunked_gelf.timeout_secs,\n             self.chunked_gelf.pending_messages_limit,\n             self.chunked_gelf.max_length,\n+            self.chunked_gelf.decompression,\n         )\n     }\n }\n@@ -71,6 +74,37 @@ pub struct ChunkedGelfDecoderOptions {\n     /// The message's payload is the concatenation of all the chunks' payloads.\n     #[serde(default, skip_serializing_if = \"vector_core::serde::is_default\")]\n     pub max_length: Option<usize>,\n+\n+    /// Decompression configuration for GELF messages.\n+    #[serde(default, skip_serializing_if = \"vector_core::serde::is_default\")]\n+    pub decompression: ChunkedGelfDecompressionConfig,\n+}\n+\n+/// Decompression options for ChunkedGelfDecoder.\n+#[configurable_component]\n+#[derive(Clone, Copy, Debug, PartialEq, Eq, Derivative)]\n+#[derivative(Default)]\n+pub enum ChunkedGelfDecompressionConfig {\n+    /// Automatically detect the decompression method based on the magic bytes of the message.\n+    #[derivative(Default)]\n+    Auto,\n+    /// Use Gzip decompression.\n+    Gzip,\n+    /// Use Zlib decompression.\n+    Zlib,\n+    /// Do not decompress the message.\n+    None,\n+}\n+\n+impl ChunkedGelfDecompressionConfig {\n+    pub fn get_decompression(&self, data: &Bytes) -> ChunkedGelfDecompression {\n+        match self {\n+            Self::Auto => ChunkedGelfDecompression::from_magic(data),\n+            Self::Gzip => ChunkedGelfDecompression::Gzip,\n+            Self::Zlib => ChunkedGelfDecompression::Zlib,\n+            Self::None => ChunkedGelfDecompression::None,\n+        }\n+    }\n }\n \n #[derive(Debug)]\n@@ -128,7 +162,72 @@ impl MessageState {\n     }\n }\n \n-#[derive(Debug, Snafu, PartialEq, Eq)]\n+#[derive(Debug, PartialEq, Eq)]\n+pub enum ChunkedGelfDecompression {\n+    Gzip,\n+    Zlib,\n+    None,\n+}\n+\n+impl ChunkedGelfDecompression {\n+    pub fn from_magic(data: &Bytes) -> Self {\n+        if data.starts_with(GZIP_MAGIC) {\n+            trace!(\"Detected Gzip compression\");\n+            return Self::Gzip;\n+        }\n+\n+        if data.starts_with(ZLIB_MAGIC) {\n+            // Based on https://datatracker.ietf.org/doc/html/rfc1950#section-2.2\n+            if let Some([first_byte, second_byte]) = data.get(0..2) {\n+                if (*first_byte as u16 * 256 + *second_byte as u16) % 31 == 0 {\n+                    trace!(\"Detected Zlib compression\");\n+                    return Self::Zlib;\n+                }\n+            };\n+\n+            warn!(\n+                \"Detected Zlib magic bytes but the header is invalid: {:?}\",\n+                data.get(0..2)\n+            );\n+        };\n+\n+        trace!(\"No compression detected\",);\n+        Self::None\n+    }\n+\n+    pub fn decompress(&self, data: Bytes) -> Result<Bytes, ChunkedGelfDecompressionError> {\n+        let decompressed = match self {\n+            Self::Gzip => {\n+                let mut decoder = MultiGzDecoder::new(data.reader());\n+                let mut decompressed = Vec::new();\n+                decoder\n+                    .read_to_end(&mut decompressed)\n+                    .context(GzipDecompressionSnafu)?;\n+                Bytes::from(decompressed)\n+            }\n+            Self::Zlib => {\n+                let mut decoder = ZlibDecoder::new(data.reader());\n+                let mut decompressed = Vec::new();\n+                decoder\n+                    .read_to_end(&mut decompressed)\n+                    .context(ZlibDecompressionSnafu)?;\n+                Bytes::from(decompressed)\n+            }\n+            Self::None => data,\n+        };\n+        Ok(decompressed)\n+    }\n+}\n+\n+#[derive(Debug, Snafu)]\n+pub enum ChunkedGelfDecompressionError {\n+    #[snafu(display(\"Gzip decompression error: {source}\"))]\n+    GzipDecompression { source: std::io::Error },\n+    #[snafu(display(\"Zlib decompression error: {source}\"))]\n+    ZlibDecompression { source: std::io::Error },\n+}\n+\n+#[derive(Debug, Snafu)]\n pub enum ChunkedGelfDecoderError {\n     #[snafu(display(\"Invalid chunk header with less than 10 bytes: 0x{header:0x}\"))]\n     InvalidChunkHeader { header: Bytes },\n@@ -164,6 +263,10 @@ pub enum ChunkedGelfDecoderError {\n         length: usize,\n         max_length: usize,\n     },\n+    #[snafu(display(\"Error while decompressing message. {source}\"))]\n+    Decompression {\n+        source: ChunkedGelfDecompressionError,\n+    },\n }\n \n impl StreamDecodingError for ChunkedGelfDecoderError {\n@@ -188,6 +291,7 @@ pub struct ChunkedGelfDecoder {\n     // This limitation is due to the fact that the GELF format does not specify the length of the\n     // message, so we have to read all the bytes from the message (datagram)\n     bytes_decoder: BytesDecoder,\n+    decompression_config: ChunkedGelfDecompressionConfig,\n     state: Arc<Mutex<HashMap<u64, MessageState>>>,\n     timeout: Duration,\n     pending_messages_limit: Option<usize>,\n@@ -200,9 +304,11 @@ impl ChunkedGelfDecoder {\n         timeout_secs: f64,\n         pending_messages_limit: Option<usize>,\n         max_length: Option<usize>,\n+        decompression_config: ChunkedGelfDecompressionConfig,\n     ) -> Self {\n         Self {\n             bytes_decoder: BytesDecoder::new(),\n+            decompression_config,\n             state: Arc::new(Mutex::new(HashMap::new())),\n             timeout: Duration::from_secs_f64(timeout_secs),\n             pending_messages_limit,\n@@ -339,18 +445,38 @@ impl ChunkedGelfDecoder {\n         &mut self,\n         mut src: Bytes,\n     ) -> Result<Option<Bytes>, ChunkedGelfDecoderError> {\n-        if src.starts_with(GELF_MAGIC) {\n+        let message = if src.starts_with(GELF_MAGIC) {\n+            trace!(\"Received a chunked GELF message based on the magic bytes\");\n             src.advance(2);\n-            self.decode_chunk(src)\n+            self.decode_chunk(src)?\n         } else {\n-            Ok(Some(src))\n-        }\n+            trace!(\n+                \"Received an unchunked GELF message. First two bytes of message: {:?}\",\n+                &src[0..2]\n+            );\n+            Some(src)\n+        };\n+\n+        // We can have both chunked and unchunked messages that are compressed\n+        message\n+            .map(|message| {\n+                self.decompression_config\n+                    .get_decompression(&message)\n+                    .decompress(message)\n+                    .context(DecompressionSnafu)\n+            })\n+            .transpose()\n     }\n }\n \n impl Default for ChunkedGelfDecoder {\n     fn default() -> Self {\n-        Self::new(DEFAULT_TIMEOUT_SECS, None, None)\n+        Self::new(\n+            DEFAULT_TIMEOUT_SECS,\n+            None,\n+            None,\n+            ChunkedGelfDecompressionConfig::Auto,\n+        )\n     }\n }\n \n@@ -388,22 +514,59 @@ mod tests {\n \n     use super::*;\n     use bytes::{BufMut, BytesMut};\n+    use flate2::{write::GzEncoder, write::ZlibEncoder};\n     use rand::{rngs::SmallRng, seq::SliceRandom, SeedableRng};\n     use rstest::{fixture, rstest};\n+    use std::fmt::Write as FmtWrite;\n+    use std::io::Write as IoWrite;\n     use tracing_test::traced_test;\n \n+    pub enum Compression {\n+        Gzip,\n+        Zlib,\n+    }\n+\n+    impl Compression {\n+        pub fn compress(&self, payload: &impl AsRef<[u8]>) -> Bytes {\n+            self.compress_with_level(payload, flate2::Compression::default())\n+        }\n+\n+        pub fn compress_with_level(\n+            &self,\n+            payload: &impl AsRef<[u8]>,\n+            level: flate2::Compression,\n+        ) -> Bytes {\n+            match self {\n+                Compression::Gzip => {\n+                    let mut encoder = GzEncoder::new(Vec::new(), level);\n+                    encoder\n+                        .write_all(payload.as_ref())\n+                        .expect(\"failed to write to encoder\");\n+                    encoder.finish().expect(\"failed to finish encoder\").into()\n+                }\n+                Compression::Zlib => {\n+                    let mut encoder = ZlibEncoder::new(Vec::new(), level);\n+                    encoder\n+                        .write_all(payload.as_ref())\n+                        .expect(\"failed to write to encoder\");\n+                    encoder.finish().expect(\"failed to finish encoder\").into()\n+                }\n+            }\n+        }\n+    }\n+\n     fn create_chunk(\n         message_id: u64,\n         sequence_number: u8,\n         total_chunks: u8,\n-        payload: &str,\n+        payload: &impl AsRef<[u8]>,\n     ) -> BytesMut {\n         let mut chunk = BytesMut::new();\n         chunk.put_slice(GELF_MAGIC);\n         chunk.put_u64(message_id);\n         chunk.put_u8(sequence_number);\n         chunk.put_u8(total_chunks);\n-        chunk.extend_from_slice(payload.as_bytes());\n+        chunk.extend_from_slice(payload.as_ref());\n         chunk\n     }\n \n@@ -424,7 +587,7 @@ mod tests {\n             message_id,\n             first_sequence_number,\n             total_chunks,\n-            first_payload,\n+            &first_payload,\n         );\n \n         let second_sequence_number = 1u8;\n@@ -433,7 +596,7 @@ mod tests {\n             message_id,\n             second_sequence_number,\n             total_chunks,\n-            second_payload,\n+            &second_payload,\n         );\n \n         (\n@@ -453,7 +616,7 @@ mod tests {\n             message_id,\n             first_sequence_number,\n             total_chunks,\n-            first_payload,\n+            &first_payload,\n         );\n \n         let second_sequence_number = 1u8;\n@@ -462,7 +625,7 @@ mod tests {\n             message_id,\n             second_sequence_number,\n             total_chunks,\n-            second_payload,\n+            &second_payload,\n         );\n \n         let third_sequence_number = 2u8;\n@@ -471,7 +634,7 @@ mod tests {\n             message_id,\n             third_sequence_number,\n             total_chunks,\n-            third_payload,\n+            &third_payload,\n         );\n \n         (\n@@ -582,7 +745,7 @@ mod tests {\n                 first_message_id,\n                 sequence_number,\n                 total_chunks,\n-                first_payload,\n+                &first_payload,\n             )\n         });\n         let second_message_chunks = (0..total_chunks).map(|sequence_number| {\n@@ -590,7 +753,7 @@ mod tests {\n                 second_message_id,\n                 sequence_number,\n                 total_chunks,\n-                second_payload,\n+                &second_payload,\n             )\n         });\n         let expected_first_message = first_payload.repeat(total_chunks as usize);\n@@ -672,12 +835,10 @@ mod tests {\n \n         let error = frame.unwrap_err();\n         let downcasted_error = downcast_framing_error(&error);\n-        assert_eq!(\n-            *downcasted_error,\n-            ChunkedGelfDecoderError::InvalidChunkHeader {\n-                header: Bytes::from_static(&[0x12, 0x34])\n-            }\n-        );\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::InvalidChunkHeader { .. }\n+        ));\n     }\n \n     #[tokio::test]\n@@ -686,20 +847,20 @@ mod tests {\n         let sequence_number = 1u8;\n         let invalid_total_chunks = GELF_MAX_TOTAL_CHUNKS + 1;\n         let payload = \"foo\";\n-        let mut chunk = create_chunk(message_id, sequence_number, invalid_total_chunks, payload);\n+        let mut chunk = create_chunk(message_id, sequence_number, invalid_total_chunks, &payload);\n         let mut decoder = ChunkedGelfDecoder::default();\n \n         let frame = decoder.decode_eof(&mut chunk);\n         let error = frame.unwrap_err();\n         let downcasted_error = downcast_framing_error(&error);\n-        assert_eq!(\n-            *downcasted_error,\n+        assert!(matches!(\n+            downcasted_error,\n             ChunkedGelfDecoderError::InvalidTotalChunks {\n                 message_id: 1,\n                 sequence_number: 1,\n                 total_chunks: 129,\n             }\n-        );\n+        ));\n     }\n \n     #[tokio::test]\n@@ -708,20 +869,20 @@ mod tests {\n         let total_chunks = 2u8;\n         let invalid_sequence_number = total_chunks + 1;\n         let payload = \"foo\";\n-        let mut chunk = create_chunk(message_id, invalid_sequence_number, total_chunks, payload);\n+        let mut chunk = create_chunk(message_id, invalid_sequence_number, total_chunks, &payload);\n         let mut decoder = ChunkedGelfDecoder::default();\n \n         let frame = decoder.decode_eof(&mut chunk);\n         let error = frame.unwrap_err();\n         let downcasted_error = downcast_framing_error(&error);\n-        assert_eq!(\n-            *downcasted_error,\n+        assert!(matches!(\n+            downcasted_error,\n             ChunkedGelfDecoderError::InvalidSequenceNumber {\n                 message_id: 1,\n                 sequence_number: 3,\n                 total_chunks: 2,\n-            },\n-        );\n+            }\n+        ));\n     }\n \n     #[rstest]\n@@ -732,7 +893,10 @@ mod tests {\n     ) {\n         let (mut two_chunks, _) = two_chunks_message;\n         let (mut three_chunks, _) = three_chunks_message;\n-        let mut decoder = ChunkedGelfDecoder::new(DEFAULT_TIMEOUT_SECS, Some(1), None);\n+        let mut decoder = ChunkedGelfDecoder {\n+            pending_messages_limit: Some(1),\n+            ..Default::default()\n+        };\n \n         let frame = decoder.decode_eof(&mut two_chunks[0]).unwrap();\n         assert!(frame.is_none());\n@@ -741,14 +905,14 @@ mod tests {\n         let frame = decoder.decode_eof(&mut three_chunks[0]);\n         let error = frame.unwrap_err();\n         let downcasted_error = downcast_framing_error(&error);\n-        assert_eq!(\n-            *downcasted_error,\n+        assert!(matches!(\n+            downcasted_error,\n             ChunkedGelfDecoderError::PendingMessagesLimitReached {\n                 message_id: 2u64,\n                 sequence_number: 0u8,\n                 pending_messages_limit: 1,\n             }\n-        );\n+        ));\n         assert!(decoder.state.lock().unwrap().len() == 1);\n     }\n \n@@ -759,9 +923,9 @@ mod tests {\n         let sequence_number = 0u8;\n         let total_chunks = 2u8;\n         let payload = \"foo\";\n-        let mut first_chunk = create_chunk(message_id, sequence_number, total_chunks, payload);\n+        let mut first_chunk = create_chunk(message_id, sequence_number, total_chunks, &payload);\n         let mut second_chunk =\n-            create_chunk(message_id, sequence_number + 1, total_chunks + 1, payload);\n+            create_chunk(message_id, sequence_number + 1, total_chunks + 1, &payload);\n         let mut decoder = ChunkedGelfDecoder::default();\n \n         let frame = decoder.decode_eof(&mut first_chunk).unwrap();\n@@ -770,37 +934,40 @@ mod tests {\n         let frame = decoder.decode_eof(&mut second_chunk);\n         let error = frame.unwrap_err();\n         let downcasted_error = downcast_framing_error(&error);\n-        assert_eq!(\n-            *downcasted_error,\n+        assert!(matches!(\n+            downcasted_error,\n             ChunkedGelfDecoderError::TotalChunksMismatch {\n                 message_id: 1,\n                 sequence_number: 1,\n                 original_total_chunks: 2,\n                 received_total_chunks: 3,\n             }\n-        );\n+        ));\n     }\n \n     #[rstest]\n     #[tokio::test]\n     async fn decode_message_greater_than_max_length(two_chunks_message: ([BytesMut; 2], String)) {\n         let (mut chunks, _) = two_chunks_message;\n-        let mut decoder = ChunkedGelfDecoder::new(DEFAULT_TIMEOUT_SECS, None, Some(5));\n+        let mut decoder = ChunkedGelfDecoder {\n+            max_length: Some(5),\n+            ..Default::default()\n+        };\n \n         let frame = decoder.decode_eof(&mut chunks[0]).unwrap();\n         assert!(frame.is_none());\n         let frame = decoder.decode_eof(&mut chunks[1]);\n         let error = frame.unwrap_err();\n         let downcasted_error = downcast_framing_error(&error);\n-        assert_eq!(\n-            *downcasted_error,\n+        assert!(matches!(\n+            downcasted_error,\n             ChunkedGelfDecoderError::MaxLengthExceed {\n                 message_id: 1,\n                 sequence_number: 1,\n                 length: 6,\n                 max_length: 5,\n             }\n-        );\n+        ));\n         assert_eq!(decoder.state.lock().unwrap().len(), 0);\n     }\n \n@@ -818,4 +985,284 @@ mod tests {\n         assert!(frame.is_none());\n         assert!(logs_contain(\"Received a duplicate chunk. Ignoring it.\"));\n     }\n+\n+    #[tokio::test]\n+    #[rstest]\n+    #[case::gzip(Compression::Gzip)]\n+    #[case::zlib(Compression::Zlib)]\n+    async fn decode_compressed_unchunked_message(#[case] compression: Compression) {\n+        let payload = (0..100).fold(String::new(), |mut payload, n| {\n+            write!(payload, \"foo{n}\").unwrap();\n+            payload\n+        });\n+        let compressed_payload = compression.compress(&payload);\n+        let mut decoder = ChunkedGelfDecoder::default();\n+\n+        let frame = decoder\n+            .decode_eof(&mut compressed_payload.into())\n+            .expect(\"decoding should not fail\")\n+            .expect(\"decoding should return a frame\");\n+\n+        assert_eq!(frame, payload);\n+    }\n+\n+    #[tokio::test]\n+    #[rstest]\n+    #[case::gzip(Compression::Gzip)]\n+    #[case::zlib(Compression::Zlib)]\n+    async fn decode_compressed_chunked_message(#[case] compression: Compression) {\n+        let message_id = 1u64;\n+        let max_chunk_size = 5;\n+        let payload = (0..100).fold(String::new(), |mut payload, n| {\n+            write!(payload, \"foo{n}\").unwrap();\n+            payload\n+        });\n+        let compressed_payload = compression.compress(&payload);\n+        let total_chunks = compressed_payload.len().div_ceil(max_chunk_size) as u8;\n+        assert!(total_chunks < GELF_MAX_TOTAL_CHUNKS);\n+        let mut chunks = compressed_payload\n+            .chunks(max_chunk_size)\n+            .enumerate()\n+            .map(|(i, chunk)| create_chunk(message_id, i as u8, total_chunks, &chunk))\n+            .collect::<Vec<_>>();\n+        let (last_chunk, first_chunks) =\n+            chunks.split_last_mut().expect(\"chunks should not be empty\");\n+        let mut decoder = ChunkedGelfDecoder::default();\n+\n+        for chunk in first_chunks {\n+            let frame = decoder.decode_eof(chunk).expect(\"decoding should not fail\");\n+            assert!(frame.is_none());\n+        }\n+        let frame = decoder\n+            .decode_eof(last_chunk)\n+            .expect(\"decoding should not fail\")\n+            .expect(\"decoding should return a frame\");\n+\n+        assert_eq!(frame, payload);\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_malformed_gzip_message() {\n+        let mut compressed_payload = BytesMut::new();\n+        compressed_payload.extend(GZIP_MAGIC);\n+        compressed_payload.extend(&[0x12, 0x34, 0x56, 0x78]);\n+        let mut decoder = ChunkedGelfDecoder::default();\n+\n+        let error = decoder\n+            .decode_eof(&mut compressed_payload)\n+            .expect_err(\"decoding should fail\");\n+\n+        let downcasted_error = downcast_framing_error(&error);\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::Decompression {\n+                source: ChunkedGelfDecompressionError::GzipDecompression { .. }\n+            }\n+        ));\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_malformed_zlib_message() {\n+        let mut compressed_payload = BytesMut::new();\n+        compressed_payload.extend(ZLIB_MAGIC);\n+        compressed_payload.extend(&[0x9c, 0x12, 0x34, 0x56]);\n+        let mut decoder = ChunkedGelfDecoder::default();\n+\n+        let error = decoder\n+            .decode_eof(&mut compressed_payload)\n+            .expect_err(\"decoding should fail\");\n+\n+        let downcasted_error = downcast_framing_error(&error);\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::Decompression {\n+                source: ChunkedGelfDecompressionError::ZlibDecompression { .. }\n+            }\n+        ));\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_zlib_payload_with_zlib_decoder() {\n+        let payload = \"foo\";\n+        let compressed_payload = Compression::Zlib.compress(&payload);\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::Zlib,\n+            ..Default::default()\n+        };\n+\n+        let frame = decoder\n+            .decode_eof(&mut compressed_payload.into())\n+            .expect(\"decoding should not fail\")\n+            .expect(\"decoding should return a frame\");\n+\n+        assert_eq!(frame, payload);\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_gzip_payload_with_zlib_decoder() {\n+        let payload = \"foo\";\n+        let compressed_payload = Compression::Gzip.compress(&payload);\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::Zlib,\n+            ..Default::default()\n+        };\n+\n+        let error = decoder\n+            .decode_eof(&mut compressed_payload.into())\n+            .expect_err(\"decoding should fail\");\n+\n+        let downcasted_error = downcast_framing_error(&error);\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::Decompression {\n+                source: ChunkedGelfDecompressionError::ZlibDecompression { .. }\n+            }\n+        ));\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_uncompressed_payload_with_zlib_decoder() {\n+        let payload = \"foo\";\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::Zlib,\n+            ..Default::default()\n+        };\n+\n+        let error = decoder\n+            .decode_eof(&mut payload.into())\n+            .expect_err(\"decoding should fail\");\n+\n+        let downcasted_error = downcast_framing_error(&error);\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::Decompression {\n+                source: ChunkedGelfDecompressionError::ZlibDecompression { .. }\n+            }\n+        ));\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_gzip_payload_with_gzip_decoder() {\n+        let payload = \"foo\";\n+        let compressed_payload = Compression::Gzip.compress(&payload);\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::Gzip,\n+            ..Default::default()\n+        };\n+\n+        let frame = decoder\n+            .decode_eof(&mut compressed_payload.into())\n+            .expect(\"decoding should not fail\")\n+            .expect(\"decoding should return a frame\");\n+\n+        assert_eq!(frame, payload);\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_zlib_payload_with_gzip_decoder() {\n+        let payload = \"foo\";\n+        let compressed_payload = Compression::Zlib.compress(&payload);\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::Gzip,\n+            ..Default::default()\n+        };\n+\n+        let error = decoder\n+            .decode_eof(&mut compressed_payload.into())\n+            .expect_err(\"decoding should fail\");\n+\n+        let downcasted_error = downcast_framing_error(&error);\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::Decompression {\n+                source: ChunkedGelfDecompressionError::GzipDecompression { .. }\n+            }\n+        ));\n+    }\n+\n+    #[tokio::test]\n+    async fn decode_uncompressed_payload_with_gzip_decoder() {\n+        let payload = \"foo\";\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::Gzip,\n+            ..Default::default()\n+        };\n+\n+        let error = decoder\n+            .decode_eof(&mut payload.into())\n+            .expect_err(\"decoding should fail\");\n+\n+        let downcasted_error = downcast_framing_error(&error);\n+        assert!(matches!(\n+            downcasted_error,\n+            ChunkedGelfDecoderError::Decompression {\n+                source: ChunkedGelfDecompressionError::GzipDecompression { .. }\n+            }\n+        ));\n+    }\n+\n+    #[tokio::test]\n+    #[rstest]\n+    #[case::gzip(Compression::Gzip)]\n+    #[case::zlib(Compression::Zlib)]\n+    async fn decode_compressed_payload_with_no_decompression_decoder(\n+        #[case] compression: Compression,\n+    ) {\n+        let payload = \"foo\";\n+        let compressed_payload = compression.compress(&payload);\n+        let mut decoder = ChunkedGelfDecoder {\n+            decompression_config: ChunkedGelfDecompressionConfig::None,\n+            ..Default::default()\n+        };\n+\n+        let frame = decoder\n+            .decode_eof(&mut compressed_payload.clone().into())\n+            .expect(\"decoding should not fail\")\n+            .expect(\"decoding should return a frame\");\n+\n+        assert_eq!(frame, compressed_payload);\n+    }\n+\n+    #[test]\n+    fn detect_gzip_compression() {\n+        let payload = \"foo\";\n+\n+        for level in 0..=9 {\n+            let level = flate2::Compression::new(level);\n+            let compressed_payload = Compression::Gzip.compress_with_level(&payload, level);\n+            let actual = ChunkedGelfDecompression::from_magic(&compressed_payload);\n+            assert_eq!(\n+                actual,\n+                ChunkedGelfDecompression::Gzip,\n+                \"Failed for level {}\",\n+                level.level()\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn detect_zlib_compression() {\n+        let payload = \"foo\";\n+\n+        for level in 0..=9 {\n+            let level = flate2::Compression::new(level);\n+            let compressed_payload = Compression::Zlib.compress_with_level(&payload, level);\n+            let actual = ChunkedGelfDecompression::from_magic(&compressed_payload);\n+            assert_eq!(\n+                actual,\n+                ChunkedGelfDecompression::Zlib,\n+                \"Failed for level {}\",\n+                level.level()\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn detect_no_compression() {\n+        let payload = \"foo\";\n+\n+        let detected_compression = ChunkedGelfDecompression::from_magic(&payload.into());\n+\n+        assert_eq!(detected_compression, ChunkedGelfDecompression::None);\n+    }\n }\ndiff --git a/lib/file-source/Cargo.toml b/lib/file-source/Cargo.toml\nindex c942fda74bd91..284999113766d 100644\n--- a/lib/file-source/Cargo.toml\n+++ b/lib/file-source/Cargo.toml\n@@ -14,6 +14,7 @@ winapi = { version = \"0.3\", features = [\"winioctl\"] }\n crc = \"3.2.1\"\n glob.workspace = true\n scan_fmt = \"0.2.6\"\n+vector-common = { path = \"../vector-common\", default-features = false }\n vector-config = { path = \"../vector-config\", default-features = false }\n vector-config-common = { path = \"../vector-config-common\", default-features = false }\n vector-config-macros = { path = \"../vector-config-macros\", default-features = false }\ndiff --git a/lib/file-source/src/file_watcher/mod.rs b/lib/file-source/src/file_watcher/mod.rs\nindex 7ac50173ea2b7..80db2b9bc876c 100644\n--- a/lib/file-source/src/file_watcher/mod.rs\n+++ b/lib/file-source/src/file_watcher/mod.rs\n@@ -9,6 +9,7 @@ use bytes::{Bytes, BytesMut};\n use chrono::{DateTime, Utc};\n use flate2::bufread::MultiGzDecoder;\n use tracing::debug;\n+use vector_common::constants::GZIP_MAGIC;\n \n use crate::{\n     buffer::read_until_with_max_size, metadata_ext::PortableFileExt, FilePosition, ReadFrom,\n@@ -293,7 +294,7 @@ fn is_gzipped(r: &mut io::BufReader<fs::File>) -> io::Result<bool> {\n     let header_bytes = r.fill_buf()?;\n     // WARN: The paired `BufReader::consume` is not called intentionally. If we\n     // do we'll chop a decent part of the potential gzip stream off.\n-    Ok(header_bytes.starts_with(&[0x1f, 0x8b]))\n+    Ok(header_bytes.starts_with(GZIP_MAGIC))\n }\n \n fn null_reader() -> impl BufRead {\ndiff --git a/lib/vector-common/src/constants.rs b/lib/vector-common/src/constants.rs\nnew file mode 100644\nindex 0000000000000..e463a19461874\n--- /dev/null\n+++ b/lib/vector-common/src/constants.rs\n@@ -0,0 +1,2 @@\n+pub const GZIP_MAGIC: &[u8] = &[0x1f, 0x8b];\n+pub const ZLIB_MAGIC: &[u8] = &[0x78];\ndiff --git a/lib/vector-common/src/lib.rs b/lib/vector-common/src/lib.rs\nindex b795415c43267..3e6d179f13b95 100644\n--- a/lib/vector-common/src/lib.rs\n+++ b/lib/vector-common/src/lib.rs\n@@ -22,6 +22,8 @@ pub mod json_size;\n \n pub mod config;\n \n+pub mod constants;\n+\n #[cfg(feature = \"conversion\")]\n pub use vrl::compiler::TimeZone;\n \ndiff --git a/src/sources/aws_kinesis_firehose/handlers.rs b/src/sources/aws_kinesis_firehose/handlers.rs\nindex d623dc297842f..cd424d97a336a 100644\n--- a/src/sources/aws_kinesis_firehose/handlers.rs\n+++ b/src/sources/aws_kinesis_firehose/handlers.rs\n@@ -7,6 +7,7 @@ use flate2::read::MultiGzDecoder;\n use futures::StreamExt;\n use snafu::{ResultExt, Snafu};\n use tokio_util::codec::FramedRead;\n+use vector_common::constants::GZIP_MAGIC;\n use vector_lib::codecs::StreamDecodingError;\n use vector_lib::lookup::{metadata_path, path, PathPrefix};\n use vector_lib::{\n@@ -242,7 +243,7 @@ fn is_gzip(data: &[u8]) -> bool {\n     // deflate algorithm.\n     //\n     // Reference: https://datatracker.ietf.org/doc/html/rfc1952 Section 2.3\n-    data.len() >= 10 && &data[..3] == b\"\\x1f\\x8b\\x08\"\n+    data.starts_with(GZIP_MAGIC)\n }\n \n fn decode_gzip(data: &[u8]) -> std::io::Result<Bytes> {\ndiff --git a/website/cue/reference/components/sources/base/amqp.cue b/website/cue/reference/components/sources/base/amqp.cue\nindex 155fa0d536dd6..d0ebadf50da28 100644\n--- a/website/cue/reference/components/sources/base/amqp.cue\n+++ b/website/cue/reference/components/sources/base/amqp.cue\n@@ -335,6 +335,19 @@ base: components: sources: amqp: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/aws_kinesis_firehose.cue b/website/cue/reference/components/sources/base/aws_kinesis_firehose.cue\nindex 4313099b9bc49..f00dc9c67bee1 100644\n--- a/website/cue/reference/components/sources/base/aws_kinesis_firehose.cue\n+++ b/website/cue/reference/components/sources/base/aws_kinesis_firehose.cue\n@@ -333,6 +333,19 @@ base: components: sources: aws_kinesis_firehose: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/aws_s3.cue b/website/cue/reference/components/sources/base/aws_s3.cue\nindex 12d3322a216ce..807c2eaa2b1b1 100644\n--- a/website/cue/reference/components/sources/base/aws_s3.cue\n+++ b/website/cue/reference/components/sources/base/aws_s3.cue\n@@ -433,6 +433,19 @@ base: components: sources: aws_s3: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/aws_sqs.cue b/website/cue/reference/components/sources/base/aws_sqs.cue\nindex 19db1cf8830c9..39def33c7888c 100644\n--- a/website/cue/reference/components/sources/base/aws_sqs.cue\n+++ b/website/cue/reference/components/sources/base/aws_sqs.cue\n@@ -437,6 +437,19 @@ base: components: sources: aws_sqs: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/datadog_agent.cue b/website/cue/reference/components/sources/base/datadog_agent.cue\nindex 156aa609cd356..60bb9aa6e8b0c 100644\n--- a/website/cue/reference/components/sources/base/datadog_agent.cue\n+++ b/website/cue/reference/components/sources/base/datadog_agent.cue\n@@ -330,6 +330,19 @@ base: components: sources: datadog_agent: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/demo_logs.cue b/website/cue/reference/components/sources/base/demo_logs.cue\nindex b7539615c41bf..b7af9828fb75e 100644\n--- a/website/cue/reference/components/sources/base/demo_logs.cue\n+++ b/website/cue/reference/components/sources/base/demo_logs.cue\n@@ -326,6 +326,19 @@ base: components: sources: demo_logs: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/exec.cue b/website/cue/reference/components/sources/base/exec.cue\nindex b21f203dde81c..ad6297800984f 100644\n--- a/website/cue/reference/components/sources/base/exec.cue\n+++ b/website/cue/reference/components/sources/base/exec.cue\n@@ -314,6 +314,19 @@ base: components: sources: exec: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/file_descriptor.cue b/website/cue/reference/components/sources/base/file_descriptor.cue\nindex f368cc4eedcc0..23124afa6a9c6 100644\n--- a/website/cue/reference/components/sources/base/file_descriptor.cue\n+++ b/website/cue/reference/components/sources/base/file_descriptor.cue\n@@ -292,6 +292,19 @@ base: components: sources: file_descriptor: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/gcp_pubsub.cue b/website/cue/reference/components/sources/base/gcp_pubsub.cue\nindex 61187238c72f2..55b16cdfbca17 100644\n--- a/website/cue/reference/components/sources/base/gcp_pubsub.cue\n+++ b/website/cue/reference/components/sources/base/gcp_pubsub.cue\n@@ -369,6 +369,19 @@ base: components: sources: gcp_pubsub: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/heroku_logs.cue b/website/cue/reference/components/sources/base/heroku_logs.cue\nindex 876b6f3a51dde..418f61da96d33 100644\n--- a/website/cue/reference/components/sources/base/heroku_logs.cue\n+++ b/website/cue/reference/components/sources/base/heroku_logs.cue\n@@ -327,6 +327,19 @@ base: components: sources: heroku_logs: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/http.cue b/website/cue/reference/components/sources/base/http.cue\nindex e3edaf3fc03a3..d53ca9b9f93fd 100644\n--- a/website/cue/reference/components/sources/base/http.cue\n+++ b/website/cue/reference/components/sources/base/http.cue\n@@ -342,6 +342,19 @@ base: components: sources: http: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/http_client.cue b/website/cue/reference/components/sources/base/http_client.cue\nindex 3e3b44cef3ec3..609949072fba6 100644\n--- a/website/cue/reference/components/sources/base/http_client.cue\n+++ b/website/cue/reference/components/sources/base/http_client.cue\n@@ -330,6 +330,19 @@ base: components: sources: http_client: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/http_server.cue b/website/cue/reference/components/sources/base/http_server.cue\nindex abaafa5c7a7f8..543a97a42c96c 100644\n--- a/website/cue/reference/components/sources/base/http_server.cue\n+++ b/website/cue/reference/components/sources/base/http_server.cue\n@@ -342,6 +342,19 @@ base: components: sources: http_server: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/kafka.cue b/website/cue/reference/components/sources/base/kafka.cue\nindex b5906e7d3aa3f..6e96d91f5fb74 100644\n--- a/website/cue/reference/components/sources/base/kafka.cue\n+++ b/website/cue/reference/components/sources/base/kafka.cue\n@@ -363,6 +363,19 @@ base: components: sources: kafka: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/nats.cue b/website/cue/reference/components/sources/base/nats.cue\nindex 52b2cbc2b5dc2..40516215dec9d 100644\n--- a/website/cue/reference/components/sources/base/nats.cue\n+++ b/website/cue/reference/components/sources/base/nats.cue\n@@ -382,6 +382,19 @@ base: components: sources: nats: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/pulsar.cue b/website/cue/reference/components/sources/base/pulsar.cue\nindex 1f411bbb831ab..14452ffe67932 100644\n--- a/website/cue/reference/components/sources/base/pulsar.cue\n+++ b/website/cue/reference/components/sources/base/pulsar.cue\n@@ -393,6 +393,19 @@ base: components: sources: pulsar: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/redis.cue b/website/cue/reference/components/sources/base/redis.cue\nindex f93131f0be48f..d9f167d400733 100644\n--- a/website/cue/reference/components/sources/base/redis.cue\n+++ b/website/cue/reference/components/sources/base/redis.cue\n@@ -300,6 +300,19 @@ base: components: sources: redis: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/socket.cue b/website/cue/reference/components/sources/base/socket.cue\nindex eca545eb86d65..dc338ced7cd66 100644\n--- a/website/cue/reference/components/sources/base/socket.cue\n+++ b/website/cue/reference/components/sources/base/socket.cue\n@@ -302,6 +302,19 @@ base: components: sources: socket: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\ndiff --git a/website/cue/reference/components/sources/base/stdin.cue b/website/cue/reference/components/sources/base/stdin.cue\nindex 6fcd46ddbfa9f..ded73f44e9095 100644\n--- a/website/cue/reference/components/sources/base/stdin.cue\n+++ b/website/cue/reference/components/sources/base/stdin.cue\n@@ -285,6 +285,19 @@ base: components: sources: stdin: configuration: {\n \t\t\t\trelevant_when: \"method = \\\"chunked_gelf\\\"\"\n \t\t\t\trequired:      false\n \t\t\t\ttype: object: options: {\n+\t\t\t\t\tdecompression: {\n+\t\t\t\t\t\tdescription: \"Decompression configuration for GELF messages.\"\n+\t\t\t\t\t\trequired:    false\n+\t\t\t\t\t\ttype: string: {\n+\t\t\t\t\t\t\tdefault: \"Auto\"\n+\t\t\t\t\t\t\tenum: {\n+\t\t\t\t\t\t\t\tAuto: \"Automatically detect the decompression method based on the magic bytes of the message.\"\n+\t\t\t\t\t\t\t\tGzip: \"Use Gzip decompression.\"\n+\t\t\t\t\t\t\t\tNone: \"Do not decompress the message.\"\n+\t\t\t\t\t\t\t\tZlib: \"Use Zlib decompression.\"\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t\tmax_length: {\n \t\t\t\t\t\tdescription: \"\"\"\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe maximum length of a single GELF message, in bytes. Messages longer than this length will\n", "instance_id": "vectordotdev__vector-21816", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding compression support (GZIP and ZLIB) to the GELF codec for decoding compressed messages received via UDP. It references the GELF specification and provides context about the expected behavior (automatic detection of compression type via magic bytes). The proposal also mentions how Graylog handles decompression and chunking, which adds clarity to the expected implementation approach. However, there are minor ambiguities: the problem statement does not explicitly define the input/output formats for compressed messages beyond referencing magic bytes, nor does it specify edge cases or constraints (e.g., maximum message size after decompression, handling of malformed compressed data). Additionally, while references to related issues are provided, there are no concrete examples or test cases in the statement itself to validate the implementation. Thus, while the overall intent is clear, some critical details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, primarily focused on a single module (`chunked_gelf.rs`) with the addition of decompression logic using the `flate2` library, alongside minor updates to configuration files and documentation across multiple sources. This requires understanding and modifying the existing GELF chunking logic to integrate decompression, which involves interactions between message parsing and payload processing. Second, the technical concepts involved include familiarity with Rust's I/O handling, the `flate2` library for GZIP and ZLIB decompression, and magic byte detection for compression type inference, which are moderately complex but well-documented. Third, the implementation addresses edge cases such as malformed compressed data and invalid headers, with comprehensive test cases added for both chunked and unchunked messages under various compression scenarios. However, the changes do not significantly impact the broader system architecture, and the problem does not require advanced domain-specific knowledge beyond standard compression techniques. Overall, this task requires a solid understanding of multiple concepts and careful handling of edge cases, but it is not overly complex or system-wide in impact, justifying a difficulty score of 0.55 (Medium).", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Suggestion: Rename substrate-client-keystore to ac-keystore\nAll our crates are named with ac-.. , where ac is short for `api-client`, except for the substrate-client-keystore. \r\n\r\nac-keystore was still free at time of writing this issue. Before doing it, one should check if the crate name is still available: https://crates.io/search?q=ac-keystore\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex a22582d53..0ec68c28a 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -61,6 +61,22 @@ dependencies = [\n  \"substrate-api-client\",\n ]\n \n+[[package]]\n+name = \"ac-keystore\"\n+version = \"0.10.0\"\n+dependencies = [\n+ \"array-bytes\",\n+ \"async-trait\",\n+ \"parking_lot\",\n+ \"sc-keystore\",\n+ \"serde_json\",\n+ \"sp-application-crypto\",\n+ \"sp-core\",\n+ \"sp-keyring\",\n+ \"sp-keystore\",\n+ \"tempfile\",\n+]\n+\n [[package]]\n name = \"ac-node-api\"\n version = \"0.6.0\"\n@@ -140,11 +156,11 @@ dependencies = [\n name = \"ac-testing-sync\"\n version = \"0.4.0\"\n dependencies = [\n+ \"ac-keystore\",\n  \"sp-application-crypto\",\n  \"sp-core\",\n  \"sp-runtime\",\n  \"substrate-api-client\",\n- \"substrate-client-keystore\",\n ]\n \n [[package]]\n@@ -7341,22 +7357,6 @@ dependencies = [\n  \"zeroize\",\n ]\n \n-[[package]]\n-name = \"substrate-client-keystore\"\n-version = \"0.10.0\"\n-dependencies = [\n- \"array-bytes\",\n- \"async-trait\",\n- \"parking_lot\",\n- \"sc-keystore\",\n- \"serde_json\",\n- \"sp-application-crypto\",\n- \"sp-core\",\n- \"sp-keyring\",\n- \"sp-keystore\",\n- \"tempfile\",\n-]\n-\n [[package]]\n name = \"substrate-wasm-builder\"\n version = \"17.0.0\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex e7f72ab0c..fd0316e0b 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -14,7 +14,7 @@ categories = [\"no-std\", \"wasm\"]\n [workspace]\n members = [\n     \".\",\n-    \"client-keystore\",\n+    \"keystore\",\n     \"compose-macros\",\n     \"examples/async\",\n     \"examples/sync\",\ndiff --git a/client-keystore/Cargo.toml b/keystore/Cargo.toml\nsimilarity index 95%\nrename from client-keystore/Cargo.toml\nrename to keystore/Cargo.toml\nindex 35333c26f..a9cec8ac2 100644\n--- a/client-keystore/Cargo.toml\n+++ b/keystore/Cargo.toml\n@@ -1,5 +1,5 @@\n [package]\n-name = \"substrate-client-keystore\"\n+name = \"ac-keystore\"\n version = \"0.10.0\"\n authors = [\"Supercomputing Systems AG <info@scs.ch>\"]\n license = \"Apache-2.0\"\ndiff --git a/client-keystore/src/keystore_ext.rs b/keystore/src/keystore_ext.rs\nsimilarity index 100%\nrename from client-keystore/src/keystore_ext.rs\nrename to keystore/src/keystore_ext.rs\ndiff --git a/client-keystore/src/lib.rs b/keystore/src/lib.rs\nsimilarity index 100%\nrename from client-keystore/src/lib.rs\nrename to keystore/src/lib.rs\n", "instance_id": "scs__substrate-api-client-754", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to rename the crate \"substrate-client-keystore\" to \"ac-keystore\" for consistency with other crate naming conventions (using the \"ac-\" prefix for \"api-client\"). The goal is straightforward, and the suggestion to check the availability of the crate name on crates.io is a helpful detail. However, there are minor ambiguities or missing details. For instance, the problem statement does not explicitly mention whether the rename should include updating all references in the codebase or if there are specific downstream impacts to consider (e.g., breaking changes for users of the crate). Additionally, there are no instructions regarding versioning or publishing the renamed crate. While the intent is clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a straightforward rename of a crate and associated file paths. The scope of code changes is limited to updating the crate name in `Cargo.toml`, renaming the directory, and updating references in the `Cargo.lock` and other dependent modules. This does not require deep understanding of the codebase's architecture or complex logic, as the changes are mostly mechanical and can be performed with basic knowledge of Rust's build system (Cargo). No advanced technical concepts, algorithms, or domain-specific knowledge are required beyond familiarity with Rust crate naming conventions and file structure. There are also no significant edge cases or error handling considerations mentioned or implied in the problem statement or code changes. The primary risk is ensuring the new crate name is available on crates.io, which is a procedural step rather than a technical challenge. Overall, this is a very easy task that requires minimal effort and expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Linking error of minijinja-cabi/example on  Debian GNU/Linux 11 (bullseye) with GCC 10.2.1\n## Description\r\nThe C bindings (minijinja-cabi) example's Makefile has incorrect linking order which causes compilation failure on Debian systems using GCC because the GNU linker processes arguments from left to right. The current command places the library flag (-l) before the source file, causing \"undefined reference\" errors with the GNU linker.\r\n\r\n## Reproduction steps\r\n1. Run `make` in minijinja-cabi/example\r\n2. Observe linker errors for all minijinja functions (mj_env_new, mj_env_set_debug, etc.)\r\n\r\n## Additional helpful information:\r\n\r\nVersion of minijinja: commit cd2063c270122970f827bf2555da18697cddd272\r\nVersion of rustc: 1.81.0 (eeb90cda1 2024-09-04)\r\nOperating system and version: Debian GNU/Linux 11 (bullseye) with GCC 10.2.1\r\n\r\n## What did you expect\r\nThe example should compile successfully out of the box on all supported systems.\n", "patch": "diff --git a/minijinja-cabi/example/Makefile b/minijinja-cabi/example/Makefile\nindex 37c65f5c..60b9a4ef 100644\n--- a/minijinja-cabi/example/Makefile\n+++ b/minijinja-cabi/example/Makefile\n@@ -1,2 +1,2 @@\n hello: hello.c\n-\tcc -I../include -lminijinja_cabi -L../../target/release ./hello.c -o hello\n+\tcc -I../include ./hello.c -L../../target/release -lminijinja_cabi -o hello\n", "instance_id": "mitsuhiko__minijinja-613", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly describes the issue\u2014a linking error in the Makefile for the minijinja-cabi example on Debian GNU/Linux 11 with GCC 10.2.1. The description explains the root cause (incorrect linking order leading to \"undefined reference\" errors due to GNU linker behavior), provides reproduction steps, and includes relevant context such as the specific versions of minijinja, rustc, and the operating system. The expected outcome (successful compilation out of the box) is also explicitly stated. There are no significant ambiguities, and the problem is presented with sufficient detail to understand the goal and context, including the specific behavior of the GNU linker. While edge cases or broader implications are not discussed, they are not critical for this specific issue, making the statement fully clear and actionable.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue is a straightforward build configuration error in a Makefile, requiring only a simple reordering of arguments in a single line of code to fix the linking order. The code change is minimal, confined to a single file (Makefile), and does not impact the broader codebase or system architecture. It does not require deep understanding of the minijinja library, Rust, or C beyond basic knowledge of how linkers work and the importance of argument order in GCC. No complex algorithms, design patterns, or domain-specific knowledge are needed\u2014just familiarity with build tools and linker behavior, which is a basic concept for most developers. There are no edge cases or error handling considerations mentioned or required in the fix, as the change is purely syntactic. This makes the problem very easy to resolve, suitable for a beginner or intermediate developer with minimal experience in build systems.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Docs: broken links to backend api\n### Bug Description\n\nA couple of links in https://docs.slint.dev/latest/docs/slint/guide/language/coding/file/#component-libraries are broken and browsers ask system to open such urls externally.\r\n\r\n`with_library_paths` -> `slint-build-rust:struct.CompilerConfiguration#method.with_library_paths`\r\nThis should be https://docs.slint.dev/latest/docs/rust/slint_build/struct.CompilerConfiguration#method.with_library_paths\r\n\r\n`slint_target_sources` -> `slint-cpp:cmake_reference#slint-target-sources`\r\nThis should be https://docs.slint.dev/latest/docs/cpp/cmake_reference#slint-target-sources\r\n\n\n### Reproducible Code (if applicable)\n\n_No response_\n\n### Environment Details\n\n- Slint Version:\r\n- Platform/OS:\r\n- Programming Language:\r\n- Backend/Renderer:\r\n\n\n### Product Impact\n\n_No response_\n", "patch": "diff --git a/docs/astro/src/content/docs/guide/language/coding/file.mdx b/docs/astro/src/content/docs/guide/language/coding/file.mdx\nindex 66116a7da77..ca4fef20ad4 100644\n--- a/docs/astro/src/content/docs/guide/language/coding/file.mdx\n+++ b/docs/astro/src/content/docs/guide/language/coding/file.mdx\n@@ -338,9 +338,9 @@ The path to each library, as file or directory, must be defined separately at co\n Use one of the following methods to help the Slint compiler resolve libraries to the correct\n path on disk:\n \n-* When using Rust and `build.rs`, call [`with_library_paths`](slint-build-rust:struct.CompilerConfiguration#method.with_library_paths)\n+* When using Rust and `build.rs`, call [`with_library_paths`](https://slint.dev/docs/rust/slint_build/struct.CompilerConfiguration#method.with_library_paths)\n   to provide a mapping from library name to path.\n-* When using C++, use `LIBRARY_PATHS` with [`slint_target_sources`](slint-cpp:cmake_reference#slint-target-sources).\n+* When using C++, use `LIBRARY_PATHS` with [`slint_target_sources`](https://slint.dev/docs/cpp/cmake_reference#slint-target-sources).\n * When invoking the `slint-viewer` from the command line, pass `-Lmylibrary=/path/to/my/library` for each component\n   library.\n * When using the VS Code extension, configure the Slint extension's library path\n", "instance_id": "slint-ui__slint-7263", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: broken links in the documentation that need to be updated to point to the correct URLs. The specific links to be fixed and their correct destinations are provided, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the \"Environment Details\" section is incomplete, lacking information about the Slint version, platform, or programming language context, which could be relevant for understanding the broader impact or testing the fix. Additionally, there is no mention of whether these links are part of a larger documentation system that might require a more systematic approach to prevent similar issues in the future. Despite these minor gaps, the core issue and required action are well-defined, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is very low, as it involves a straightforward fix of updating two URLs in a single documentation file. The scope of the code change is minimal, confined to a single file (file.mdx) with only two lines modified, and does not impact the system's architecture or require understanding complex interactions within the codebase. No advanced technical concepts, algorithms, or domain-specific knowledge are needed beyond basic text editing in a markdown file. There are no edge cases or error handling requirements mentioned or implied in the problem statement or code changes. This task falls into the \"Very Easy\" category (0.0-0.2), as it is essentially a typo fix in documentation, requiring minimal effort and expertise. I assign a difficulty score of 0.1 to reflect the simplicity of the task while acknowledging that it still requires a basic understanding of where to make the change.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add rename-all=prefix:\nThis adds a new rename-all annotation which allows setting a specific prefix to all fields. For example:\r\n\r\n```rust\r\n/// cbindgen:rename-all=prefix:ERR_\r\n#[repr(i32)]\r\npub enum Error {\r\n   BAD = 0,\r\n   VERY_BAD = 1\r\n}\r\n```\r\n\r\nWhile in principle this can be added to the config file, it's primarily useful for overrides on a case-by-case basis.\n", "patch": "diff --git a/src/bindgen/ir/enumeration.rs b/src/bindgen/ir/enumeration.rs\nindex d6ec313a..8927b8be 100644\n--- a/src/bindgen/ir/enumeration.rs\n+++ b/src/bindgen/ir/enumeration.rs\n@@ -152,9 +152,10 @@ impl EnumVariant {\n             annotations.add_default(\"derive-ostream\", AnnotationValue::Bool(b));\n         }\n \n-        let body_rule = enum_annotations\n-            .parse_atom::<RenameRule>(\"rename-variant-name-fields\")\n-            .unwrap_or(config.enumeration.rename_variant_name_fields);\n+        let body_rule = enum_annotations.parse_atom::<RenameRule>(\"rename-variant-name-fields\");\n+        let body_rule = body_rule\n+            .as_ref()\n+            .unwrap_or(&config.enumeration.rename_variant_name_fields);\n \n         let body = match variant.fields {\n             syn::Fields::Unit => VariantBody::Empty(annotations),\n@@ -555,10 +556,10 @@ impl Item for Enum {\n             }\n         }\n \n-        let rules = self\n-            .annotations\n-            .parse_atom::<RenameRule>(\"rename-all\")\n-            .unwrap_or(config.enumeration.rename_variants);\n+        let rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+        let rules = rules\n+            .as_ref()\n+            .unwrap_or(&config.enumeration.rename_variants);\n \n         if let Some(r) = rules.not_none() {\n             self.variants = self\ndiff --git a/src/bindgen/ir/function.rs b/src/bindgen/ir/function.rs\nindex 79adfce9..f25d2f88 100644\n--- a/src/bindgen/ir/function.rs\n+++ b/src/bindgen/ir/function.rs\n@@ -156,10 +156,8 @@ impl Function {\n         self.ret.rename_for_config(config, &generic_params);\n \n         // Apply rename rules to argument names\n-        let rules = self\n-            .annotations\n-            .parse_atom::<RenameRule>(\"rename-all\")\n-            .unwrap_or(config.function.rename_args);\n+        let rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+        let rules = rules.as_ref().unwrap_or(&config.function.rename_args);\n \n         if let Some(r) = rules.not_none() {\n             let args = std::mem::take(&mut self.args);\ndiff --git a/src/bindgen/ir/structure.rs b/src/bindgen/ir/structure.rs\nindex 9b33a15c..d805d69c 100644\n--- a/src/bindgen/ir/structure.rs\n+++ b/src/bindgen/ir/structure.rs\n@@ -339,10 +339,10 @@ impl Item for Struct {\n         {\n             let names = self.fields.iter_mut().map(|field| &mut field.name);\n \n-            let field_rules = self\n-                .annotations\n-                .parse_atom::<RenameRule>(\"rename-all\")\n-                .unwrap_or(config.structure.rename_fields);\n+            let field_rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+            let field_rules = field_rules\n+                .as_ref()\n+                .unwrap_or(&config.structure.rename_fields);\n \n             if let Some(o) = self.annotations.list(\"field-names\") {\n                 for (dest, src) in names.zip(o) {\ndiff --git a/src/bindgen/ir/union.rs b/src/bindgen/ir/union.rs\nindex 410e21a2..9c425837 100644\n--- a/src/bindgen/ir/union.rs\n+++ b/src/bindgen/ir/union.rs\n@@ -168,10 +168,8 @@ impl Item for Union {\n             field.ty.rename_for_config(config, &self.generic_params);\n         }\n \n-        let rules = self\n-            .annotations\n-            .parse_atom::<RenameRule>(\"rename-all\")\n-            .unwrap_or(config.structure.rename_fields);\n+        let rules = self.annotations.parse_atom::<RenameRule>(\"rename-all\");\n+        let rules = rules.as_ref().unwrap_or(&config.structure.rename_fields);\n \n         if let Some(o) = self.annotations.list(\"field-names\") {\n             let mut overriden_fields = Vec::new();\ndiff --git a/src/bindgen/rename.rs b/src/bindgen/rename.rs\nindex f594939e..6f4928ef 100644\n--- a/src/bindgen/rename.rs\n+++ b/src/bindgen/rename.rs\n@@ -28,7 +28,7 @@ impl<'a> IdentifierType<'a> {\n }\n \n /// A rule to apply to an identifier when generating bindings.\n-#[derive(Debug, Clone, Copy, Default)]\n+#[derive(Debug, Clone, Default)]\n pub enum RenameRule {\n     /// Do not apply any renaming. The default.\n     #[default]\n@@ -50,10 +50,12 @@ pub enum RenameRule {\n     /// Converts the identifier to SCREAMING_SNAKE_CASE and prefixes enum variants\n     /// with the enum name.\n     QualifiedScreamingSnakeCase,\n+    /// Adds a given prefix\n+    Prefix(String),\n }\n \n impl RenameRule {\n-    pub(crate) fn not_none(self) -> Option<Self> {\n+    pub(crate) fn not_none(&self) -> Option<&Self> {\n         match self {\n             RenameRule::None => None,\n             other => Some(other),\n@@ -61,7 +63,7 @@ impl RenameRule {\n     }\n \n     /// Applies the rename rule to a string\n-    pub fn apply<'a>(self, text: &'a str, context: IdentifierType) -> Cow<'a, str> {\n+    pub fn apply<'a>(&self, text: &'a str, context: IdentifierType) -> Cow<'a, str> {\n         use heck::*;\n \n         if text.is_empty() {\n@@ -90,6 +92,7 @@ impl RenameRule {\n                 result.push_str(&RenameRule::ScreamingSnakeCase.apply(text, context));\n                 result\n             }\n+            RenameRule::Prefix(prefix) => prefix.to_owned() + text,\n         })\n     }\n }\n@@ -98,6 +101,9 @@ impl FromStr for RenameRule {\n     type Err = String;\n \n     fn from_str(s: &str) -> Result<RenameRule, Self::Err> {\n+        const PREFIX: &str = \"prefix:\";\n+        const PREFIX_LEN: usize = PREFIX.len();\n+\n         match s {\n             \"none\" => Ok(RenameRule::None),\n             \"None\" => Ok(RenameRule::None),\n@@ -132,6 +138,8 @@ impl FromStr for RenameRule {\n             \"QualifiedScreamingSnakeCase\" => Ok(RenameRule::QualifiedScreamingSnakeCase),\n             \"qualified_screaming_snake_case\" => Ok(RenameRule::QualifiedScreamingSnakeCase),\n \n+            s if s.starts_with(PREFIX) => Ok(RenameRule::Prefix(s[PREFIX_LEN..].to_string())),\n+\n             _ => Err(format!(\"Unrecognized RenameRule: '{}'.\", s)),\n         }\n     }\n", "instance_id": "mozilla__cbindgen-1021", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a new \"rename-all=prefix\" annotation to allow setting a specific prefix to fields in Rust code (e.g., for enum variants). It provides a concrete example of how the feature should work with the `ERR_` prefix for an enum. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention how the prefix should interact with existing rename rules or whether there are constraints on the prefix string (e.g., valid characters, length). Additionally, edge cases such as empty prefixes, duplicate prefixes, or conflicts with existing names are not addressed. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (`enumeration.rs`, `function.rs`, `structure.rs`, `union.rs`, and `rename.rs`), indicating a need to understand and modify different parts of the codebase consistently. However, the changes are relatively localized within each file, focusing on updating the handling of the `RenameRule` enum and its application logic. The amount of code change is moderate, involving both structural updates (adding a new variant to `RenameRule`) and functional updates (modifying how rules are parsed and applied). \n\nTechnically, the problem requires understanding Rust-specific concepts such as enums, string manipulation, and parsing logic, as well as familiarity with the `heck` crate for case conversion (though not directly modified here). It also involves understanding the existing annotation system and configuration structure of the codebase, which adds a layer of complexity. However, the concepts are not overly advanced, and the logic for applying a prefix is straightforward compared to more complex renaming rules already implemented.\n\nEdge cases and error handling are not explicitly mentioned in the problem statement, but the code changes do not introduce significant new error handling logic. Potential edge cases like empty prefixes or invalid prefix strings are not addressed in the provided diff, which might require additional consideration during implementation or review. The changes do not appear to impact the broader system architecture, as they are confined to the renaming logic.\n\nOverall, I rate this as a medium difficulty task (0.45) because it requires understanding multiple parts of the codebase and making consistent changes across files, but the technical concepts and logic involved are not particularly complex for an experienced developer familiar with Rust and code generation tools like `cbindgen`.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`Deno.bench` no longer accepts iteration params\nHi! It looks like the `warmup` & `n` params were removed from `Deno.bench` which could control the number of iterations that run. The last version to include them looks to be [`v1.20.6`](https://deno.land/manual@v1.20.6/tools/benchmarker#iterations-and-warmup-runs).\r\n\r\nWas there a reason for this or could they be re-added? Happy to submit a PR to add them back.\n", "patch": "diff --git a/cli/js/40_bench.js b/cli/js/40_bench.js\nindex fb0e86463d01a9..83f1384a500851 100644\n--- a/cli/js/40_bench.js\n+++ b/cli/js/40_bench.js\n@@ -16,13 +16,32 @@ const {\n   op_bench_now,\n } = core.ops;\n const {\n-  ArrayPrototypePush,\n+  Array,\n+  ArrayPrototypeSort,\n+  ArrayPrototypeSlice,\n   Error,\n+  MathMax,\n+  MathMin,\n   MathCeil,\n   SymbolToStringTag,\n   TypeError,\n } = primordials;\n \n+/**\n+ * @typedef {{\n+ *   id: number,\n+ *   name: string,\n+ *   fn: BenchFunction\n+ *   origin: string,\n+ *   ignore: boolean,\n+ *   only: boolean.\n+ *   n?: number;\n+ *   warmup?: number;\n+ *   sanitizeExit: boolean,\n+ *   permissions: PermissionOptions,\n+ * }} BenchDescription\n+ */\n+\n /** @type {number | null} */\n let currentBenchId = null;\n // These local variables are used to track time measurements at\n@@ -199,37 +218,43 @@ function benchStats(\n   min,\n   max,\n   all,\n+  allLength,\n ) {\n   return {\n     n,\n     min,\n     max,\n-    p75: all[MathCeil(n * (75 / 100)) - 1],\n-    p99: all[MathCeil(n * (99 / 100)) - 1],\n-    p995: all[MathCeil(n * (99.5 / 100)) - 1],\n-    p999: all[MathCeil(n * (99.9 / 100)) - 1],\n+    p75: all[MathCeil(allLength * (75 / 100)) - 1],\n+    p99: all[MathCeil(allLength * (99 / 100)) - 1],\n+    p995: all[MathCeil(allLength * (99.5 / 100)) - 1],\n+    p999: all[MathCeil(allLength * (99.9 / 100)) - 1],\n     avg: !highPrecision ? (avg / n) : MathCeil(avg / n),\n     highPrecision,\n     usedExplicitTimers,\n   };\n }\n \n-async function benchMeasure(timeBudget, fn, async, context) {\n+// reuse the same array across all benchmarks\n+// and cap the length so that we don't spend\n+// too much time sorting\n+const allMaxLength = 10_000_000;\n+let all = new Array(allMaxLength);\n+const lowPrecisionThresholdInNs = 1e4;\n+\n+async function benchMeasure(timeBudget, fn, desc, context) {\n   let n = 0;\n   let avg = 0;\n   let wavg = 0;\n   let usedExplicitTimers = false;\n-  const all = [];\n   let min = Infinity;\n   let max = -Infinity;\n-  const lowPrecisionThresholdInNs = 1e4;\n \n   // warmup step\n   let c = 0;\n-  let iterations = 20;\n+  let iterations = desc.warmup > 0 ? desc.warmup : 20;\n   let budget = 10 * 1e6;\n \n-  if (!async) {\n+  if (!desc.async) {\n     while (budget > 0 || iterations-- > 0) {\n       const t1 = benchNow();\n       fn(context);\n@@ -272,11 +297,11 @@ async function benchMeasure(timeBudget, fn, async, context) {\n   wavg /= c;\n \n   // measure step\n-  if (wavg > lowPrecisionThresholdInNs) {\n-    let iterations = 10;\n-    let budget = timeBudget * 1e6;\n+  iterations = desc.n > 0 ? desc.n : 10;\n+  budget = timeBudget * 1e6;\n \n-    if (!async) {\n+  if (wavg > lowPrecisionThresholdInNs) {\n+    if (!desc.async) {\n       while (budget > 0 || iterations-- > 0) {\n         const t1 = benchNow();\n         fn(context);\n@@ -292,10 +317,13 @@ async function benchMeasure(timeBudget, fn, async, context) {\n           currentBenchUserExplicitEnd = null;\n         }\n \n+        if (n < allMaxLength) {\n+          all[n] = measuredTime;\n+        }\n+\n         n++;\n         avg += measuredTime;\n         budget -= totalTime;\n-        ArrayPrototypePush(all, measuredTime);\n         if (measuredTime < min) min = measuredTime;\n         if (measuredTime > max) max = measuredTime;\n       }\n@@ -315,10 +343,13 @@ async function benchMeasure(timeBudget, fn, async, context) {\n           currentBenchUserExplicitEnd = null;\n         }\n \n+        if (n < allMaxLength) {\n+          all[n] = measuredTime;\n+        }\n+\n         n++;\n         avg += measuredTime;\n         budget -= totalTime;\n-        ArrayPrototypePush(all, measuredTime);\n         if (measuredTime < min) min = measuredTime;\n         if (measuredTime > max) max = measuredTime;\n       }\n@@ -326,10 +357,9 @@ async function benchMeasure(timeBudget, fn, async, context) {\n   } else {\n     context.start = function start() {};\n     context.end = function end() {};\n-    let iterations = 10;\n-    let budget = timeBudget * 1e6;\n+    iterations = MathMax(MathCeil(iterations / lowPrecisionThresholdInNs), 10);\n \n-    if (!async) {\n+    if (!desc.async) {\n       while (budget > 0 || iterations-- > 0) {\n         const t1 = benchNow();\n         for (let c = 0; c < lowPrecisionThresholdInNs; c++) {\n@@ -337,9 +367,12 @@ async function benchMeasure(timeBudget, fn, async, context) {\n         }\n         const iterationTime = (benchNow() - t1) / lowPrecisionThresholdInNs;\n \n+        if (n < allMaxLength) {\n+          all[n] = iterationTime;\n+        }\n+\n         n++;\n         avg += iterationTime;\n-        ArrayPrototypePush(all, iterationTime);\n         if (iterationTime < min) min = iterationTime;\n         if (iterationTime > max) max = iterationTime;\n         budget -= iterationTime * lowPrecisionThresholdInNs;\n@@ -354,9 +387,12 @@ async function benchMeasure(timeBudget, fn, async, context) {\n         }\n         const iterationTime = (benchNow() - t1) / lowPrecisionThresholdInNs;\n \n+        if (n < allMaxLength) {\n+          all[n] = iterationTime;\n+        }\n+\n         n++;\n         avg += iterationTime;\n-        ArrayPrototypePush(all, iterationTime);\n         if (iterationTime < min) min = iterationTime;\n         if (iterationTime > max) max = iterationTime;\n         budget -= iterationTime * lowPrecisionThresholdInNs;\n@@ -364,7 +400,10 @@ async function benchMeasure(timeBudget, fn, async, context) {\n     }\n   }\n \n-  all.sort(compareMeasurements);\n+  const allLength = MathMin(allMaxLength, n);\n+  const allSlice = ArrayPrototypeSlice(all, 0, allLength);\n+  ArrayPrototypeSort(allSlice, compareMeasurements);\n+\n   return benchStats(\n     n,\n     wavg > lowPrecisionThresholdInNs,\n@@ -372,7 +411,8 @@ async function benchMeasure(timeBudget, fn, async, context) {\n     avg,\n     min,\n     max,\n-    all,\n+    allSlice,\n+    allLength,\n   );\n }\n \n@@ -440,7 +480,7 @@ function wrapBenchmark(desc) {\n       const stats = await benchMeasure(\n         benchTimeInMs,\n         fn,\n-        desc.async,\n+        desc,\n         context,\n       );\n \ndiff --git a/cli/tsc/dts/lib.deno.ns.d.ts b/cli/tsc/dts/lib.deno.ns.d.ts\nindex a03b35a197295b..8765a82fcca061 100644\n--- a/cli/tsc/dts/lib.deno.ns.d.ts\n+++ b/cli/tsc/dts/lib.deno.ns.d.ts\n@@ -1242,6 +1242,10 @@ declare namespace Deno {\n     /** If at least one bench has `only` set to true, only run benches that have\n      * `only` set to `true` and fail the bench suite. */\n     only?: boolean;\n+    /** Number of iterations to perform. */\n+    n?: number;\n+    /** Number of warmups to do before running the benchmark. */\n+    warmup?: number;\n     /** Ensure the bench case does not prematurely cause the process to exit,\n      * for example via a call to {@linkcode Deno.exit}.\n      *\n", "instance_id": "denoland__deno-28123", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the removal of `warmup` and `n` parameters from `Deno.bench` in a newer version of Deno, with a reference to the last version where they were present. The intent to re-add these parameters is explicit, and the user expresses willingness to submit a PR. However, the statement lacks critical details such as the expected behavior of these parameters, specific use cases or constraints for their re-implementation, and any potential compatibility issues with the current codebase. Additionally, there are no examples or edge cases mentioned that might need to be considered when re-adding these features. While the goal is understandable, these missing details prevent it from being comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is relatively limited, primarily affecting a single file (`40_bench.js`) with modifications to the benchmarking logic, and a small update to the TypeScript definition file (`lib.deno.ns.d.ts`). The changes involve reintroducing the `warmup` and `n` parameters, adjusting the measurement logic to account for these parameters, and optimizing memory usage with a capped array for storing benchmark results. The technical concepts required are straightforward: understanding JavaScript/TypeScript, basic array operations, and benchmarking logic. No complex algorithms, design patterns, or deep architectural changes are needed, and the impact on the broader codebase appears minimal. However, there is some complexity in ensuring the benchmark measurements are accurate and performant with the new parameters, as well as handling potential edge cases like invalid or excessively large values for `warmup` and `n` (though these are not explicitly addressed in the problem statement or code changes). Error handling modifications are not significant in the provided diff. Overall, this task requires understanding specific code logic and making targeted modifications, justifying a difficulty score of 0.35, towards the higher end of the \"Easy\" category due to the need for precision in benchmarking logic.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "crux_http and crux_kv: more ergonomic `Debug` on operations\nThe http requests and key value requests debug print as a giant list of numbers, which is not a great DX when used in tests. It would be nicer if they \r\n\r\n1. attempted to parse as UTF-8\r\n2. otherwise printed as either numbers or hex pairs\r\n3. truncated to the first N characters (maybe 50?)\n", "patch": "diff --git a/crux_http/src/protocol.rs b/crux_http/src/protocol.rs\nindex 08a625a1..5c711cc9 100644\n--- a/crux_http/src/protocol.rs\n+++ b/crux_http/src/protocol.rs\n@@ -16,7 +16,7 @@ pub struct HttpHeader {\n     pub value: String,\n }\n \n-#[derive(Serialize, Deserialize, Default, Clone, Debug, PartialEq, Eq, Builder)]\n+#[derive(Serialize, Deserialize, Default, Clone, PartialEq, Eq, Builder)]\n #[builder(\n     custom_constructor,\n     build_fn(private, name = \"fallible_build\"),\n@@ -30,6 +30,30 @@ pub struct HttpRequest {\n     pub body: Vec<u8>,\n }\n \n+impl std::fmt::Debug for HttpRequest {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        let body_repr = if let Ok(s) = std::str::from_utf8(&self.body) {\n+            if s.len() < 50 {\n+                format!(\"\\\"{s}\\\"\")\n+            } else {\n+                format!(\"\\\"{}\\\"...\", &s[..50])\n+            }\n+        } else {\n+            format!(\"<binary data - {} bytes>\", self.body.len())\n+        };\n+        let mut builder = f.debug_struct(\"HttpReqeuest\");\n+        builder\n+            .field(\"method\", &self.method)\n+            .field(\"url\", &self.url);\n+        if !self.headers.is_empty() {\n+            builder.field(\"headers\", &self.headers);\n+        };\n+        builder\n+            .field(\"body\", &format_args!(\"{}\", body_repr))\n+            .finish()\n+    }\n+}\n+\n macro_rules! http_method {\n     ($name:ident, $method:expr) => {\n         pub fn $name(url: impl Into<String>) -> HttpRequestBuilder {\n@@ -267,4 +291,44 @@ mod tests {\n             }\n         );\n     }\n+\n+    #[test]\n+    fn test_http_request_debug_repr() {\n+        {\n+            // small\n+            let req = HttpRequest::post(\"http://example.com\")\n+                .header(\"foo\", \"bar\")\n+                .body(\"hello world!\")\n+                .build();\n+            let repr = format!(\"{req:?}\");\n+            assert_eq!(\n+                repr,\n+                r#\"HttpReqeuest { method: \"POST\", url: \"http://example.com\", headers: [HttpHeader { name: \"foo\", value: \"bar\" }], body: \"hello world!\" }\"#\n+            );\n+        }\n+\n+        {\n+            // big\n+            let req = HttpRequest::post(\"http://example.com\")\n+                .body(\"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz\")\n+                .build();\n+            let repr = format!(\"{req:?}\");\n+            assert_eq!(\n+                repr,\n+                r#\"HttpReqeuest { method: \"POST\", url: \"http://example.com\", body: \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvw\"... }\"#\n+            );\n+        }\n+\n+        {\n+            // binary\n+            let req = HttpRequest::post(\"http://example.com\")\n+                .body(vec![255, 254, 253, 252])\n+                .build();\n+            let repr = format!(\"{req:?}\");\n+            assert_eq!(\n+                repr,\n+                r#\"HttpReqeuest { method: \"POST\", url: \"http://example.com\", body: <binary data - 4 bytes> }\"#\n+            );\n+        }\n+    }\n }\ndiff --git a/crux_kv/src/lib.rs b/crux_kv/src/lib.rs\nindex 08d36ae5..fba72f38 100644\n--- a/crux_kv/src/lib.rs\n+++ b/crux_kv/src/lib.rs\n@@ -15,7 +15,7 @@ use error::KeyValueError;\n use value::Value;\n \n /// Supported operations\n-#[derive(Clone, Serialize, Deserialize, Debug, PartialEq, Eq)]\n+#[derive(Clone, Serialize, Deserialize, PartialEq, Eq)]\n pub enum KeyValueOperation {\n     /// Read bytes stored under a key\n     Get { key: String },\n@@ -40,6 +40,40 @@ pub enum KeyValueOperation {\n     },\n }\n \n+impl std::fmt::Debug for KeyValueOperation {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match self {\n+            KeyValueOperation::Get { key } => f.debug_struct(\"Get\").field(\"key\", key).finish(),\n+            KeyValueOperation::Set { key, value } => {\n+                let body_repr = if let Ok(s) = std::str::from_utf8(&value) {\n+                    if s.len() < 50 {\n+                        format!(\"\\\"{s}\\\"\")\n+                    } else {\n+                        format!(\"\\\"{}\\\"...\", &s[..50])\n+                    }\n+                } else {\n+                    format!(\"<binary data - {} bytes>\", value.len())\n+                };\n+                f.debug_struct(\"Set\")\n+                    .field(\"key\", key)\n+                    .field(\"value\", &format_args!(\"{}\", body_repr))\n+                    .finish()\n+            }\n+            KeyValueOperation::Delete { key } => {\n+                f.debug_struct(\"Delete\").field(\"key\", key).finish()\n+            }\n+            KeyValueOperation::Exists { key } => {\n+                f.debug_struct(\"Exists\").field(\"key\", key).finish()\n+            }\n+            KeyValueOperation::ListKeys { prefix, cursor } => f\n+                .debug_struct(\"ListKeys\")\n+                .field(\"prefix\", prefix)\n+                .field(\"cursor\", cursor)\n+                .finish(),\n+        }\n+    }\n+}\n+\n /// The result of an operation on the store.\n ///\n /// Note: we can't use `Result` and `Option` here because generics are not currently\n", "instance_id": "redbadger__crux-253", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to improve the debug output of HTTP and key-value operations for better developer experience (DX) during testing. The goals are outlined with three specific requirements: attempting UTF-8 parsing, falling back to numbers or hex pairs for non-UTF-8 data, and truncating long outputs to a certain length (suggested as 50 characters). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what \"numbers or hex pairs\" means in terms of formatting, nor does it finalize the truncation limit (it suggests \"maybe 50?\"). Additionally, there are no examples of expected debug output formats or mentions of specific edge cases to handle (e.g., empty bodies, very large binary data). While the intent is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the straightforward nature of the required changes and the limited scope of technical complexity. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The modifications are localized to two files (`crux_http/src/protocol.rs` and `crux_kv/src/lib.rs`) and involve implementing custom `Debug` trait implementations for specific structs (`HttpRequest` and `KeyValueOperation`). The changes do not impact the broader system architecture or require understanding complex interactions between modules. The amount of code added is moderate, primarily consisting of formatting logic for debug output, along with some test cases to validate the behavior. Overall, the scope is narrow and contained.\n\n2. **Number of Technical Concepts:** The solution requires basic familiarity with Rust's `Debug` trait and how to implement custom formatting using `std::fmt::Formatter`. It also involves simple string manipulation with `std::str::from_utf8` for UTF-8 parsing and basic conditional logic for truncation and fallback formatting. No advanced algorithms, design patterns, or domain-specific knowledge are needed. The concepts are fundamental and accessible to developers with basic-to-intermediate Rust experience.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes address a few implicitly, such as handling non-UTF-8 binary data, truncating long strings, and dealing with empty or small bodies. The error handling is minimal, relying on `std::str::from_utf8` which returns a `Result` that is handled via pattern matching. The edge cases are not particularly complex, and the implementation covers the obvious scenarios (small strings, large strings, binary data) as demonstrated in the test cases.\n\n4. **Overall Complexity:** The task is primarily about improving usability of debug output, which is a non-critical feature enhancement. It does not involve performance optimization, concurrency, or deep architectural changes. The provided test cases further reduce ambiguity by showing expected output formats, making the implementation straightforward.\n\nGiven these points, a difficulty score of 0.25 reflects the simplicity of the task. It requires understanding some code logic (e.g., how to format debug output in Rust) and making targeted modifications, but it does not demand deep expertise or complex problem-solving. This is a task that a junior-to-mid-level Rust developer could handle with minimal guidance.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incorrect `content-encoding` for `gzip` when file is already available + `compression-static`\n### Search for duplicate issues\n\n- [X] I already searched, and this issue is not a duplicate.\n\n### Issue scope\n\nDocker / Kubernetes\n\n### Describe the bug\n\nIf I run `SWS` with my \"frontend\" package, with both regular and pre-compressed files (`.gzip`), I see an unexpected `content-encoding` returned by `SWS`: `content-encoding: gzip, gzip`.\r\n\r\nAfter verification, the content is packaged twice, aka a `gzip` inside a `gzip`.\r\n\r\nThis generates issues with Apple based browsers, because they don't process `gzip` twice and so can't parse `js` or `css` after the first \"unpackaging\".\r\n\r\n\n\n### How to reproduce it\n\nFind attached files I have generated, with the unexpected result in `SWS`: \r\n\r\n[app-css.zip](https://github.com/user-attachments/files/16589058/app-css.zip)\r\n\r\nIf I run the `SWS` with `docker run -it --rm -p 8080:8080 ui-for-testing`, using the following configuration: \r\n\r\n```toml\r\n[general]\r\n\r\nhost = \"::\"\r\nport = 8080\r\nlog-level = \"info\"\r\n\r\ncache-control-headers = true\r\ncompression = true\r\ncompression-static = true\r\nsecurity-headers = true\r\ndirectory-listing = false\r\nredirect-trailing-slash = true\r\nignore-hidden-files = true\r\n```\r\n\r\nI get the following result when I run the following `curl` call: \r\n\r\n```sh\r\ncurl 'http://localhost:8080/app.min.css' \\\r\n      -H 'Accept: */*' \\\r\n      -H 'Accept-Encoding: gzip' \\\r\n      -v -o nul &| grep content-encoding\r\n< content-encoding: gzip, gzip\r\n```\r\n\r\nIf I set `compression-static = false`, the result is \"normal\", but I suspect the compression is done by `SWS` and doesn't leverage the pre-built `gzip` file.\r\n\r\n```\r\ncurl 'http://localhost:8080/app.min.css' \\\r\n      -H 'Accept: */*' \\\r\n      -H 'Accept-Encoding: gzip' \\\r\n      -v -o nul &| grep content-encoding\r\n< content-encoding: gzip\r\n```\n\n### Expected behavior\n\nI expect the system to leverage the pre-build `gzip` and returns a `content-type` with only `gzip` once. \r\n\r\n\n\n### Complementary information\n\nFrom the investigation I did, this bug has been introduced after `2.31.0`. \r\n\r\nIf I change my `Dockerfile` to `2.32.0`, I have the issue, but I don't have it with `2.31.0`, of course with the same config and files. \r\n\r\n\n\n### Build target\n\nDocker linux/amd64\n\n### Environment and specs\n\n- [x] **static-web-server:** [e.g. v2.32.0+]\r\n- [ ] **Rust:** [e.g. v1.78.0]\r\n- [x] **OS:** Distrolesss\r\n- [x] **Arch:** x86_64 (64-bit), ARM (32-bit), ARM64 (64-bit)\r\n- [x] **Docker:** 27.1.1\r\n- [x] **Client:** All (Safari, Chrome, Arc, `curl`)\r\n\n\n### Additional context\n\nThis issue has been found while developing on my application, [Podcast-Server](https://gitlab.com/davinkevin/Podcast-Server/), and you can find the code used by the UI [here](https://gitlab.com/davinkevin/Podcast-Server/-/tree/main/ui/src/docker?ref_type=heads)\n", "patch": "diff --git a/src/compression.rs b/src/compression.rs\nindex 69147783..babfe449 100644\n--- a/src/compression.rs\n+++ b/src/compression.rs\n@@ -98,6 +98,11 @@ pub(crate) fn post_process<T>(\n         return Ok(resp);\n     }\n \n+    let is_precompressed = resp.headers().get(CONTENT_ENCODING).is_some();\n+    if is_precompressed {\n+        return Ok(resp);\n+    }\n+\n     // Compression content encoding varies so use a `Vary` header\n     resp.headers_mut().insert(\n         hyper::header::VARY,\n", "instance_id": "static-web-server__static-web-server-471", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue, including the unexpected behavior of the `content-encoding` header being set to `gzip, gzip` when using pre-compressed files with the `compression-static` setting enabled in the static web server (SWS). It includes steps to reproduce the issue with a specific configuration and `curl` commands, as well as the expected behavior (single `gzip` encoding). Additionally, it provides context about the version where the bug was introduced and links to relevant codebases. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases beyond the Apple browser issue, nor does it clarify if there are other configurations or scenarios where this behavior might manifest differently. Constraints or side effects of disabling `compression-static` are also not fully explored, which could impact the solution's design. Overall, the statement is valid and clear but lacks some depth in edge case specification and broader implications.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal and localized to a single file (`compression.rs`) and a specific function (`post_process`). It involves adding a simple check to return early if the response already has a `CONTENT_ENCODING` header, preventing double compression. This change does not impact the broader system architecture or require modifications across multiple modules. The diff is small (a few lines), indicating a straightforward fix.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of HTTP headers (specifically `Content-Encoding`), conditional logic in Rust, and the behavior of the static web server's compression mechanism. No advanced algorithms, design patterns, or domain-specific knowledge beyond web server basics are needed. Familiarity with Rust's syntax and the `hyper` crate (used for HTTP handling) is sufficient, and these are not particularly complex for an experienced developer.\n\n3. **Edge Cases and Error Handling**: The problem statement mentions the Apple browser issue as a consequence of double compression, but the code change itself does not introduce new error handling or complex edge case logic. The fix is a simple guard clause that avoids further processing if the content is already compressed. While there might be implicit edge cases (e.g., ensuring the pre-compressed content is valid or handling other encoding types), these are not addressed in the provided diff and do not seem to be a primary concern for this specific fix.\n\n4. **Overall Complexity**: The problem requires understanding a specific bug in the compression logic and applying a targeted fix. It does not demand deep knowledge of the entire codebase or intricate interactions between components. The solution is a logical, contained modification that a developer with moderate experience in Rust and web servers could implement and test relatively quickly.\n\nA score of 0.35 reflects that this is slightly more involved than a trivial fix (e.g., changing a constant) due to the need to understand HTTP compression behavior and verify the impact of the change, but it remains on the easier side of the spectrum as it does not involve complex logic, extensive refactoring, or significant risk to the system's stability.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "minor: List of supported SQL dialects is out of date\nThe documentation/errors around what SQL dialects are supported are slightly out of date. Because we delegate to `sqlparser` and it doesn't expose a way to enumerate all options, it has to be updated manually once in a blue moon.\n", "patch": "diff --git a/datafusion-cli/src/exec.rs b/datafusion-cli/src/exec.rs\nindex d560dee987f5..a9e1e8efdfb6 100644\n--- a/datafusion-cli/src/exec.rs\n+++ b/datafusion-cli/src/exec.rs\n@@ -222,7 +222,7 @@ pub(super) async fn exec_and_print(\n         plan_datafusion_err!(\n             \"Unsupported SQL dialect: {dialect}. Available dialects: \\\n                  Generic, MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, \\\n-                 MsSQL, ClickHouse, BigQuery, Ansi.\"\n+                 MsSQL, ClickHouse, BigQuery, Ansi, DuckDB, Databricks.\"\n         )\n     })?;\n \n@@ -520,7 +520,7 @@ mod tests {\n             plan_datafusion_err!(\n                 \"Unsupported SQL dialect: {dialect}. Available dialects: \\\n                  Generic, MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, \\\n-                 MsSQL, ClickHouse, BigQuery, Ansi.\"\n+                 MsSQL, ClickHouse, BigQuery, Ansi, DuckDB, Databricks.\"\n             )\n         })?;\n         for location in locations {\ndiff --git a/datafusion/common/src/config.rs b/datafusion/common/src/config.rs\nindex 34c30abd5101..2ea5b3701550 100644\n--- a/datafusion/common/src/config.rs\n+++ b/datafusion/common/src/config.rs\n@@ -242,7 +242,7 @@ config_namespace! {\n         pub enable_options_value_normalization: bool, warn = \"`enable_options_value_normalization` is deprecated and ignored\", default = false\n \n         /// Configure the SQL dialect used by DataFusion's parser; supported values include: Generic,\n-        /// MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, MsSQL, ClickHouse, BigQuery, and Ansi.\n+        /// MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, MsSQL, ClickHouse, BigQuery, Ansi, DuckDB and Databricks.\n         pub dialect: String, default = \"generic\".to_string()\n         // no need to lowercase because `sqlparser::dialect_from_str`] is case-insensitive\n \ndiff --git a/datafusion/core/src/execution/session_state.rs b/datafusion/core/src/execution/session_state.rs\nindex f1abf30c0c54..bdaae4f6985b 100644\n--- a/datafusion/core/src/execution/session_state.rs\n+++ b/datafusion/core/src/execution/session_state.rs\n@@ -480,7 +480,7 @@ impl SessionState {\n             plan_datafusion_err!(\n                 \"Unsupported SQL dialect: {dialect}. Available dialects: \\\n                      Generic, MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, \\\n-                     MsSQL, ClickHouse, BigQuery, Ansi.\"\n+                     MsSQL, ClickHouse, BigQuery, Ansi, DuckDB, Databricks.\"\n             )\n         })?;\n         let mut statements = DFParser::parse_sql_with_dialect(sql, dialect.as_ref())?;\n@@ -518,7 +518,7 @@ impl SessionState {\n             plan_datafusion_err!(\n                 \"Unsupported SQL dialect: {dialect}. Available dialects: \\\n                          Generic, MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, \\\n-                         MsSQL, ClickHouse, BigQuery, Ansi.\"\n+                         MsSQL, ClickHouse, BigQuery, Ansi, DuckDB, Databricks.\"\n             )\n         })?;\n \ndiff --git a/docs/source/user-guide/configs.md b/docs/source/user-guide/configs.md\nindex a454a1777b64..c93bed963834 100644\n--- a/docs/source/user-guide/configs.md\n+++ b/docs/source/user-guide/configs.md\n@@ -125,6 +125,6 @@ Environment variables are read during `SessionConfig` initialisation so they mus\n | datafusion.sql_parser.parse_float_as_decimal                            | false                     | When set to true, SQL parser will parse float as decimal type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n | datafusion.sql_parser.enable_ident_normalization                        | true                      | When set to true, SQL parser will normalize ident (convert ident to lowercase when not quoted)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n | datafusion.sql_parser.enable_options_value_normalization                | false                     | When set to true, SQL parser will normalize options value (convert value to lowercase). Note that this option is ignored and will be removed in the future. All case-insensitive values are normalized automatically.                                                                                                                                                                                                                                                                                                                                                    |\n-| datafusion.sql_parser.dialect                                           | generic                   | Configure the SQL dialect used by DataFusion's parser; supported values include: Generic, MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, MsSQL, ClickHouse, BigQuery, and Ansi.                                                                                                                                                                                                                                                                                                                                                                                   |\n+| datafusion.sql_parser.dialect                                           | generic                   | Configure the SQL dialect used by DataFusion's parser; supported values include: Generic, MySQL, PostgreSQL, Hive, SQLite, Snowflake, Redshift, MsSQL, ClickHouse, BigQuery, Ansi, DuckDB and Databricks.                                                                                                                                                                                                                                                                                                                                                                |\n | datafusion.sql_parser.support_varchar_with_length                       | true                      | If true, permit lengths for `VARCHAR` such as `VARCHAR(20)`, but ignore the length. If false, error if a `VARCHAR` with a length is specified. The Arrow type system does not have a notion of maximum string length and thus DataFusion can not enforce such limits.                                                                                                                                                                                                                                                                                                    |\n | datafusion.sql_parser.collect_spans                                     | false                     | When set to true, the source locations relative to the original SQL query (i.e. [`Span`](sqlparser::tokenizer::Span)) will be collected and recorded in the logical plan nodes.                                                                                                                                                                                                                                                                                                                                                                                          |\n", "instance_id": "apache__datafusion-14893", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the list of supported SQL dialects in the documentation and error messages is outdated and needs to be updated manually due to limitations in the `sqlparser` library. The goal is straightforward\u2014update the list of supported dialects. However, the statement lacks specific details about which dialects are newly supported or deprecated, leaving it to the reader to infer this from the code changes. Additionally, there are no examples or references to specific commits or issues in the `sqlparser` library that triggered this update, which could have provided more context. Constraints or potential impacts of this change are also not mentioned. Despite these minor ambiguities, the intent and scope of the problem are understandable, especially when paired with the provided code changes.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves simple textual updates to error messages and documentation across a few files in the codebase. The changes are limited to replacing or adding strings in predefined lists of SQL dialects (e.g., adding \"DuckDB\" and \"Databricks\") in error messages and configuration comments. The scope of the code changes is minimal, affecting only a handful of lines in four files, with no impact on the system's architecture or logic. No deep understanding of the codebase, advanced programming concepts, algorithms, or domain-specific knowledge is required beyond basic familiarity with string literals and documentation. There are no edge cases or error handling considerations mentioned or implied in the problem or code changes, as this is purely a content update. Overall, this task is very easy, suitable for a beginner or someone with minimal experience in the codebase, and falls into the 0.0-0.2 range of difficulty.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Defaulting to features for a compliant CEL interpreter? \nWith the fix of #90, some \"standard\" features of CEL are now disabled by default. While I agree with the reason and think that providing this fine grained control over dependencies et al is real great to have in any crate, I wonder if such functionality now behind a feature flag, should be `default` and be an opt-out rather an opt-in. So that anyone grabbing the crate, gets a \"std compatible experience\" wrt CEL... more so here, as this would be a \"slow killer\", i.e. only becoming visible at runtime when actually executing some otherwise perfectly fine `Expression`. wdyt? cc/ @Caellian\r\n\r\n<details>\r\n\r\ni.e.\r\n```diff\r\ndiff --git a/interpreter/Cargo.toml b/interpreter/Cargo.toml\r\nindex fa70cce..c9fb30e 100644\r\n--- a/interpreter/Cargo.toml\r\n+++ b/interpreter/Cargo.toml\r\n@@ -28,6 +28,7 @@ name = \"runtime\"\r\n harness = false\r\n \r\n [features]\r\n+default = [\"regex\", \"chrono\"]\r\n json = [\"dep:base64\", \"dep:serde_json\"]\r\n regex = [\"dep:regex\"]\r\n chrono = [\"dep:chrono\"]\r\n```\r\n\r\n</details>\n", "patch": "diff --git a/example/Cargo.toml b/example/Cargo.toml\nindex a0720ef..5751eee 100644\n--- a/example/Cargo.toml\n+++ b/example/Cargo.toml\n@@ -3,48 +3,58 @@ name = \"example\"\n version = \"0.1.0\"\n edition = \"2021\"\n \n+[features]\n+axum = [\"dep:axum\", \"dep:tokio\", \"dep:thiserror\"]\n+json = [\"dep:serde_json\", \"cel-interpreter/json\"]\n+chrono = [\"dep:chrono\", \"cel-interpreter/chrono\"]\n+\n [dependencies]\n+cel-interpreter = { path = \"../interpreter\", default-features = false }\n+\n+chrono = { version = \"0.4\", optional = true }\n+\n+serde = { version = \"1.0\", features = [\"derive\"] }\n+serde_json = { version = \"1.0\", optional = true }\n+\n axum = { version = \"0.7.5\", default-features = false, features = [\n     \"http1\",\n     \"json\",\n     \"tokio\",\n-] }\n-cel-interpreter = { path = \"../interpreter\", features = [\"json\", \"chrono\", \"regex\"] }\n-chrono = \"0.4.26\"\n-serde = { version = \"1.0.196\", features = [\"derive\"] }\n-serde_json = \"1.0.124\"\n-thiserror = { version = \"1.0.61\", default-features = false }\n+], optional = true }\n tokio = { version = \"1.38.0\", default-features = false, features = [\n     \"macros\",\n     \"net\",\n     \"rt-multi-thread\",\n-] }\n+], optional = true }\n+thiserror = { version = \"1.0\", optional = true }\n \n [[bin]]\n-name = \"simple\"\n+name = \"example-simple\"\n path = \"src/simple.rs\"\n \n [[bin]]\n-name = \"variables\"\n+name = \"example-variables\"\n path = \"src/variables.rs\"\n \n [[bin]]\n-name = \"functions\"\n+name = \"example-functions\"\n path = \"src/functions.rs\"\n+required-features = [\"chrono\"]\n \n [[bin]]\n-name = \"threads\"\n+name = \"example-threads\"\n path = \"src/threads.rs\"\n \n [[bin]]\n-name = \"serde\"\n+name = \"example-serde\"\n path = \"src/serde.rs\"\n \n [[bin]]\n-name = \"axum\"\n+name = \"example-axum\"\n path = \"src/axum.rs\"\n+required-features = [\"axum\"]\n \n [[bin]]\n-name = \"json\"\n+name = \"example-json\"\n path = \"src/json.rs\"\n-\n+required-features = [\"json\"]\ndiff --git a/interpreter/Cargo.toml b/interpreter/Cargo.toml\nindex fa70cce..45a1379 100644\n--- a/interpreter/Cargo.toml\n+++ b/interpreter/Cargo.toml\n@@ -10,15 +10,18 @@ categories = [\"compilers\"]\n \n [dependencies]\n cel-parser = { path = \"../parser\", version = \"0.8.0\" }\n-thiserror = \"1.0.40\"\n-chrono = { version = \"0.4.26\", default-features = false, features = [\"alloc\"], optional = true }\n+\n nom = \"7.1.3\"\n-paste = \"1.0.14\"\n-serde = \"1.0.196\"\n+\n+chrono = { version = \"0.4\", default-features = false, features = [\"alloc\"], optional = true }\n regex = { version = \"1.10.5\", optional = true }\n-serde_json = { version = \"1.0.124\", optional = true }\n+serde = \"1.0\"\n+serde_json = { version = \"1.0\", optional = true }\n base64 = { version = \"0.22.1\", optional = true }\n \n+thiserror = \"1.0\"\n+paste = \"1.0\"\n+\n [dev-dependencies]\n criterion = { version = \"0.5.1\", features = [\"html_reports\"] }\n serde_bytes = \"0.11.14\"\n@@ -28,6 +31,7 @@ name = \"runtime\"\n harness = false\n \n [features]\n-json = [\"dep:base64\", \"dep:serde_json\"]\n+default = [\"regex\", \"chrono\"]\n+json = [\"dep:serde_json\", \"dep:base64\"]\n regex = [\"dep:regex\"]\n chrono = [\"dep:chrono\"]\ndiff --git a/interpreter/src/functions.rs b/interpreter/src/functions.rs\nindex a63cb38..457010f 100644\n--- a/interpreter/src/functions.rs\n+++ b/interpreter/src/functions.rs\n@@ -645,10 +645,7 @@ pub fn max(Arguments(args): Arguments) -> Result<Value> {\n #[cfg(test)]\n mod tests {\n     use crate::context::Context;\n-    use crate::testing::test_script;\n-    #[cfg(feature = \"regex\")]\n-    use crate::ExecutionError::FunctionError;\n-    use std::collections::HashMap;\n+    use crate::tests::test_script;\n \n     fn assert_script(input: &(&str, &str)) {\n         assert_eq!(test_script(input.1, None), Ok(true.into()), \"{}\", input.0);\n@@ -679,7 +676,7 @@ mod tests {\n \n         for (name, script) in tests {\n             let mut ctx = Context::default();\n-            ctx.add_variable_from_value(\"foo\", HashMap::from([(\"bar\", 1)]));\n+            ctx.add_variable_from_value(\"foo\", std::collections::HashMap::from([(\"bar\", 1)]));\n             assert_eq!(test_script(script, Some(ctx)), Ok(true.into()), \"{}\", name);\n         }\n     }\n@@ -943,7 +940,7 @@ mod tests {\n             test_script(\n                 \"'foobar'.matches('(foo') == true\", None),\n             Err(\n-                FunctionError {\n+                crate::ExecutionError::FunctionError {\n                     function: \"matches\".to_string(),\n                     message: \"'(foo' not a valid regex:\\nregex parse error:\\n    (foo\\n    ^\\nerror: unclosed group\".to_string()\n                 }\ndiff --git a/interpreter/src/lib.rs b/interpreter/src/lib.rs\nindex e307f55..b11ff47 100644\n--- a/interpreter/src/lib.rs\n+++ b/interpreter/src/lib.rs\n@@ -13,19 +13,19 @@ pub use cel_parser::Expression;\n pub use context::Context;\n pub use functions::FunctionContext;\n pub use objects::{ResolveResult, Value};\n-#[cfg(feature = \"chrono\")]\n-mod duration;\n pub mod functions;\n mod magic;\n pub mod objects;\n mod resolvers;\n+\n+#[cfg(feature = \"chrono\")]\n+mod duration;\n+\n mod ser;\n pub use ser::to_value;\n \n #[cfg(feature = \"json\")]\n mod json;\n-#[cfg(test)]\n-mod testing;\n \n use magic::FromContext;\n \n@@ -173,11 +173,16 @@ impl TryFrom<&str> for Program {\n mod tests {\n     use crate::context::Context;\n     use crate::objects::{ResolveResult, Value};\n-    use crate::testing::test_script;\n     use crate::{ExecutionError, Program};\n     use std::collections::HashMap;\n     use std::convert::TryInto;\n \n+    /// Tests the provided script and returns the result. An optional context can be provided.\n+    pub(crate) fn test_script(script: &str, ctx: Option<Context>) -> ResolveResult {\n+        let program = Program::compile(script).unwrap();\n+        program.execute(&ctx.unwrap_or_default())\n+    }\n+\n     #[test]\n     fn parse() {\n         Program::compile(\"1 + 1\").unwrap();\ndiff --git a/interpreter/src/objects.rs b/interpreter/src/objects.rs\nindex 50708d8..6c8ccc2 100644\n--- a/interpreter/src/objects.rs\n+++ b/interpreter/src/objects.rs\n@@ -1,15 +1,12 @@\n use crate::context::Context;\n use crate::functions::FunctionContext;\n-use crate::ser::SerializationError;\n-use crate::ExecutionError::NoSuchKey;\n-use crate::{to_value, ExecutionError};\n-use cel_parser::{ArithmeticOp, Atom, Expression, Member, RelationOp, UnaryOp};\n-use core::ops;\n-use serde::{Serialize, Serializer};\n+use crate::ExecutionError;\n+use cel_parser::ast::*;\n use std::cmp::Ordering;\n use std::collections::HashMap;\n use std::convert::{Infallible, TryFrom, TryInto};\n use std::fmt::{Display, Formatter};\n+use std::ops;\n use std::sync::Arc;\n \n #[derive(Debug, PartialEq, Clone)]\n@@ -84,10 +81,10 @@ impl From<u64> for Key {\n     }\n }\n \n-impl Serialize for Key {\n+impl serde::Serialize for Key {\n     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n     where\n-        S: Serializer,\n+        S: serde::Serializer,\n     {\n         match self {\n             Key::Int(v) => v.serialize(serializer),\n@@ -143,13 +140,12 @@ pub trait TryIntoValue {\n     fn try_into_value(self) -> Result<Value, Self::Error>;\n }\n \n-impl<T: Serialize> TryIntoValue for T {\n-    type Error = SerializationError;\n+impl<T: serde::Serialize> TryIntoValue for T {\n+    type Error = crate::ser::SerializationError;\n     fn try_into_value(self) -> Result<Value, Self::Error> {\n-        to_value(self)\n+        crate::ser::to_value(self)\n     }\n }\n-\n impl TryIntoValue for Value {\n     type Error = Infallible;\n     fn try_into_value(self) -> Result<Value, Self::Error> {\n@@ -629,7 +625,7 @@ impl<'a> Value {\n                 // give priority to the property. Maybe we can implement lookahead\n                 // to see if the next token is a function call?\n                 match (child, ctx.has_function(&***name)) {\n-                    (None, false) => NoSuchKey(name.clone()).into(),\n+                    (None, false) => ExecutionError::NoSuchKey(name.clone()).into(),\n                     (Some(child), _) => child.into(),\n                     (None, true) => Value::Function(name.clone(), Some(self.into())).into(),\n                 }\n@@ -960,4 +956,20 @@ mod tests {\n         let result = program.execute(&context);\n         assert_eq!(result.unwrap(), Value::Null);\n     }\n+\n+    #[test]\n+    fn reference_to_value() {\n+        let test = \"example\".to_string();\n+        let direct: Value = test.as_str().into();\n+        assert_eq!(direct, Value::String(Arc::new(String::from(\"example\"))));\n+\n+        let vec = vec![test.as_str()];\n+        let indirect: Value = vec.into();\n+        assert_eq!(\n+            indirect,\n+            Value::List(Arc::new(vec![Value::String(Arc::new(String::from(\n+                \"example\"\n+            )))]))\n+        );\n+    }\n }\n", "instance_id": "clarkmcc__cel-rust-109", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to discuss whether certain features of a CEL (Common Expression Language) interpreter should be enabled by default to provide a \"standard compatible experience.\" It raises a valid concern about usability and runtime surprises for users of the crate. The goal is to modify the default feature set in the Cargo.toml file to include \"regex\" and \"chrono\" as default features. However, the statement lacks specificity in certain areas, such as explicit mention of the expected behavior or impact of enabling these features by default. Additionally, there are no examples of what a \"std compatible experience\" entails or specific runtime errors that might occur if these features are not enabled. Constraints or trade-offs (e.g., performance or dependency size) of enabling these features by default are also not discussed. Despite these minor ambiguities, the intent and proposed solution (via the diff in the problem statement) are understandable, especially for someone familiar with Rust and Cargo feature flags.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The primary change involves modifying the `Cargo.toml` files to set certain features (\"regex\" and \"chrono\") as default for the `cel-interpreter` crate. This is a straightforward configuration change in a single line of the `interpreter/Cargo.toml` file. Additional changes in the `example/Cargo.toml` file involve restructuring dependencies to use optional features and updating binary names, which are also relatively simple and localized. The changes do not impact the broader system architecture or require deep modifications across multiple modules. The amount of code change is minimal, mostly configuration-related.\n\n2. **Number of Technical Concepts:** The problem requires basic knowledge of Rust's Cargo build system, specifically feature flags and dependency management. Understanding how feature flags control optional dependencies (like `regex` and `chrono`) and their impact on crate behavior is necessary but not complex for someone with moderate Rust experience. No advanced algorithms, design patterns, or domain-specific knowledge beyond Rust packaging are needed. The code changes also include minor refactoring in test modules and error handling (e.g., updating error type references), but these are trivial for a Rust developer.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases or error handling requirements related to enabling these features by default. However, the code changes indirectly address potential runtime issues (as hinted by the \"slow killer\" comment in the problem statement) by ensuring features are available by default. The changes in the `example/Cargo.toml` file introduce feature-specific binary requirements (e.g., `required-features`), which could lead to build-time errors if misconfigured, but these are straightforward to handle. No complex error handling logic is introduced or modified in the core codebase.\n\n4. **Overall Complexity:** The task is primarily a configuration adjustment with minimal coding effort. It requires understanding the intent behind feature flags and their impact on user experience, but it does not involve intricate logic, performance considerations, or deep architectural changes. The changes in the `interpreter/src` files (e.g., moving `test_script` function, updating error references) are minor and do not significantly increase the difficulty.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic modifications and a surface-level understanding of Rust's feature system. It is slightly above the \"Very Easy\" range due to the need to understand the context of feature flags and their implications on crate usability, but it remains an easy problem overall.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "sort: needs support for human-readable block size suffixes R and Q\nEnvironment: Ubuntu 20.04, uutils `main` branch (git commit 6a5f2aa334a0ac69c6450eff4a432c3035b57353), GNU coreutils v9.6.3-3189c-dirty\n\nSteps to reproduce:\n```\nprintf \"1Q\\n1R\\n\" | sort -h\n```\n\nWhat happens now: uutils `sort` outputs\n```\n1Q\n1R\n```\n\nWhat I expected to happen GNU `sort` outputs\n```\n1R\n1Q\n```\n\nNotes: this is causing a failure in the GNU test file `tests/sort/sort.pl`. Apparently `R` and `Q` are new suffixes for (very large) block sizes, see https://www.gnu.org/software/coreutils/manual/html_node/Block-size.html\n\n\n", "patch": "diff --git a/src/uu/sort/BENCHMARKING.md b/src/uu/sort/BENCHMARKING.md\nindex 0cc344c3118..355245b077b 100644\n--- a/src/uu/sort/BENCHMARKING.md\n+++ b/src/uu/sort/BENCHMARKING.md\n@@ -48,7 +48,7 @@ rand = \"0.8.3\"\n ```rust\n use rand::prelude::*;\n fn main() {\n-    let suffixes = ['k', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y'];\n+    let suffixes = ['k', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y', 'R', 'Q'];\n     let mut rng = thread_rng();\n     for _ in 0..100000 {\n         println!(\ndiff --git a/src/uu/sort/src/numeric_str_cmp.rs b/src/uu/sort/src/numeric_str_cmp.rs\nindex 54950f2dbfe..86cbddc6424 100644\n--- a/src/uu/sort/src/numeric_str_cmp.rs\n+++ b/src/uu/sort/src/numeric_str_cmp.rs\n@@ -82,7 +82,10 @@ impl NumInfo {\n             if Self::is_invalid_char(char, &mut had_decimal_pt, parse_settings) {\n                 return if let Some(start) = start {\n                     let has_si_unit = parse_settings.accept_si_units\n-                        && matches!(char, 'K' | 'k' | 'M' | 'G' | 'T' | 'P' | 'E' | 'Z' | 'Y');\n+                        && matches!(\n+                            char,\n+                            'K' | 'k' | 'M' | 'G' | 'T' | 'P' | 'E' | 'Z' | 'Y' | 'R' | 'Q'\n+                        );\n                     (\n                         Self { exponent, sign },\n                         start..if has_si_unit { idx + 1 } else { idx },\n@@ -176,6 +179,8 @@ fn get_unit(unit: Option<char>) -> u8 {\n             'E' => 6,\n             'Z' => 7,\n             'Y' => 8,\n+            'R' => 9,\n+            'Q' => 10,\n             _ => 0,\n         }\n     } else {\ndiff --git a/src/uu/sort/src/sort.rs b/src/uu/sort/src/sort.rs\nindex e6191410053..b9555974638 100644\n--- a/src/uu/sort/src/sort.rs\n+++ b/src/uu/sort/src/sort.rs\n@@ -289,7 +289,7 @@ impl GlobalSettings {\n         // GNU sort (8.32) invalid:  b, B, 1B,                         p, e, z, y\n         let size = Parser::default()\n             .with_allow_list(&[\n-                \"b\", \"k\", \"K\", \"m\", \"M\", \"g\", \"G\", \"t\", \"T\", \"P\", \"E\", \"Z\", \"Y\",\n+                \"b\", \"k\", \"K\", \"m\", \"M\", \"g\", \"G\", \"t\", \"T\", \"P\", \"E\", \"Z\", \"Y\", \"R\", \"Q\",\n             ])\n             .with_default_unit(\"K\")\n             .with_b_byte_count(true)\n@@ -535,8 +535,9 @@ impl<'a> Line<'a> {\n                     } else {\n                         // include a trailing si unit\n                         if selector.settings.mode == SortMode::HumanNumeric\n-                            && self.line[selection.end..initial_selection.end]\n-                                .starts_with(&['k', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y'][..])\n+                            && self.line[selection.end..initial_selection.end].starts_with(\n+                                &['k', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y', 'R', 'Q'][..],\n+                            )\n                         {\n                             selection.end += 1;\n                         }\n", "instance_id": "uutils__coreutils-7198", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear, providing a specific issue with the `sort` utility in the uutils project regarding the handling of human-readable block size suffixes 'R' and 'Q'. It includes steps to reproduce the issue, expected behavior compared to GNU `sort`, and a reference to relevant documentation. However, there are minor ambiguities: the problem does not explicitly define what 'R' and 'Q' represent in terms of numerical values (though it can be inferred from the context or documentation link), and it lacks detailed discussion of potential edge cases or constraints (e.g., how these suffixes interact with other options or invalid inputs). Overall, the goal and issue are clear, but some minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The issue requires adding support for two new block size suffixes ('R' and 'Q') in the `sort` utility, which involves straightforward modifications to existing code. Analyzing the code changes, the scope is limited to a few files (`numeric_str_cmp.rs`, `sort.rs`, and a benchmarking file), and the changes are mostly additive\u2014updating arrays and match conditions to include the new suffixes. The technical concepts involved are basic: understanding string parsing, comparison logic, and handling of SI units in Rust, which are not particularly complex for a developer familiar with the language. The changes do not impact the broader architecture of the codebase or require deep understanding of interactions between modules. Edge cases and error handling are not explicitly mentioned in the problem statement, and the code changes do not introduce significant new error handling logic beyond recognizing the new suffixes. Overall, this is a simple feature addition that requires minimal debugging or risk of introducing bugs, justifying a difficulty score of 0.25.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "empty hash fragment persists in link rendering\n**Problem**\n\nIf I pass an empty fragment in a link, like `blah.com/#` - Dioxus shouldn't necessarily render the `#`. Specifically, in internal links, you have no choice but to provide an empty fragment: \n\n```rust\npub enum BookRoute {\n    #[route(\"/./chapter_1#:section\")]\n    Chapter1 { section: Chapter1Section },\n    #[route(\"/./chapter_2#:section\")]\n    Chapter2 { section: Chapter2Section },\n    #[route(\"/./chapter_3#:section\")]\n    Chapter3 { section: Chapter3Section },\n}\n```\n\nEither we shouldn't store links with empty hashes *or* when rendering `Link {}` we just choose to trim off `#` from internal links.\n\nCreating hrefs with JS will lead to the `#` not being rendered if the hash is empty, so we should match that behavior.\n\n<img width=\"1230\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cb965713-cc7b-4231-ad27-9f6dafd85413\" />\n\nI think we just need to add a `trim_end_matches('#')` to https://github.com/DioxusLabs/dioxus/blob/c85018300b5ee4df3f715ec9507421b3892317a5/packages/router/src/components/link.rs#L163\n\n**Environment:**\n - Dioxus version:  <!-- e.g. v0.17, `master` -->\n - Rust version:    <!-- e.g. 1.43.0, `nightly` -->\n - OS info:         <!-- e.g. MacOS -->\n - App platform:    <!-- e.g. `web`, `desktop` -->\n\n**Questionnaire**\n<!-- If you feel up to the challenge, please uncomment one of the lines below: -->\n\n<!-- I'm interested in fixing this myself but don't know where to start -->\n<!-- I would like to fix and I have a solution -->\n<!-- I don't have time to fix this right now, but maybe later -->\n\n", "patch": "diff --git a/packages/router-macro/src/hash.rs b/packages/router-macro/src/hash.rs\nindex 2a16bc4e02..c199a0ac25 100644\n--- a/packages/router-macro/src/hash.rs\n+++ b/packages/router-macro/src/hash.rs\n@@ -25,7 +25,12 @@ impl HashFragment {\n     pub fn write(&self) -> TokenStream2 {\n         let ident = &self.ident;\n         quote! {\n-            write!(f, \"#{}\", #ident)?;\n+            {\n+                let __hash = #ident.to_string();\n+                if !__hash.is_empty() {\n+                    write!(f, \"#{}\", __hash)?;\n+                }\n+            }\n         }\n     }\n \n", "instance_id": "DioxusLabs__dioxus-3660", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: empty hash fragments in links (e.g., `blah.com/#`) should not render the `#` symbol, especially for internal links in the Dioxus framework. It provides context about the expected behavior (matching JavaScript's handling of empty hashes) and even points to a specific line of code where a potential fix could be applied. Additionally, it includes a code snippet showing the routing structure and an image for visual reference. However, there are minor ambiguities: the problem statement does not explicitly define what constitutes an \"empty hash\" beyond the example, nor does it discuss potential edge cases (e.g., what if the hash is programmatically set to empty at runtime?). Constraints or side effects of trimming the `#` are also not mentioned. Overall, while the goal is clear, some minor details are missing, which prevents it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`hash.rs`) and involves a small modification to the `write` method of `HashFragment`. The diff shows a simple conditional check to avoid rendering the `#` if the hash string is empty. There is no indication of widespread impact across the codebase or architectural changes. The amount of code change is minimal (a few lines).\n\n2. **Number of Technical Concepts**: The solution requires basic Rust knowledge, specifically string handling (`to_string()`, `is_empty()`), and familiarity with the `write!` macro for formatting output. No advanced language features, complex algorithms, design patterns, or domain-specific knowledge beyond the Dioxus router context are needed. Understanding the rendering logic for links in Dioxus is helpful but not overly complex.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond the empty hash scenario. The code change handles the primary case (empty hash) but does not introduce new error handling logic or address potential side effects (e.g., what if the hash contains only whitespace or special characters?). The complexity of edge cases appears minimal at this stage.\n\n4. **Overall Complexity**: The task involves a straightforward bug fix with a clear solution path (adding a conditional check before rendering the hash). It requires minimal understanding of the broader codebase beyond the specific function being modified. There are no performance or architectural considerations evident from the problem or the diff.\n\nGiven these factors, I assign a difficulty score of 0.25, as the problem requires understanding some code logic and making a simple modification, fitting within the \"Easy\" range (0.2-0.4). It does not involve complex interactions, advanced concepts, or significant risk of introducing bugs through the change.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Get rid of (serde_)y(a)ml\nserde_yaml is unmaintained, so trippy switched to the serde_yml fork, but it has a worrying code quality ([hardcoding `c_char` as `i8`](https://github.com/sebastienrousseau/libyml/pull/6)). YAML is a confusing format to begin with. So, it'd be nice if we could switch to TOML, which is already a dependency. Currently (almost) all usage of YAML is in test data. For example, \r\n\r\nhttps://github.com/fujiapple852/trippy/blob/e2c7726dc924d49c7a98461365f985e88076f394/crates/trippy-core/tests/resources/state/nat.yaml#L1-L22\r\n\r\nThis could be translated to:\r\n\r\n```toml\r\nlargest_ttl = 3\r\n\r\n[[rounds]]\r\nprobes = [\r\n  \"1 C 333 10.1.0.1 0 12340 80 43012 43012\",\r\n  \"2 C 777 10.1.0.2 1 12340 80 20544 20544\",\r\n  \"3 C 778 10.1.0.3 2 12340 80 20544 20544\"\r\n]\r\n\r\n[[rounds]]\r\nprobes = [\r\n  \"1 C 123 10.1.0.1 3 12340 80 43012 43012\",\r\n  \"2 C 788 10.1.0.2 4 12340 80 20544 20544\",\r\n  \"3 C 789 10.1.0.3 5 12340 80 20544 20544\"\r\n]\r\n\r\n[[rounds]]\r\nprobes = [\r\n  \"1 C 123 10.1.0.1 6 12340 80 43012 43012\",\r\n  \"2 C 780 10.1.0.2 7 12340 80 20544 20544\",\r\n  \"3 C 781 10.1.0.3 8 12340 80 20544 20544\"\r\n]\r\n\r\n[[expected.hops]]\r\nttl = 1\r\nlast_nat_status = \"no_nat\"\r\n\r\n[[expected.hops]]\r\nttl = 2\r\nlast_nat_status = \"nat\"\r\n\r\n[[expected.hops]]\r\nttl = 3\r\nlast_nat_status = \"no_nat\"\r\n```\r\n\r\nSome files use the tag feature of YAML e.g. `!SingleHost`, which presumably could be replaced with an optional key in TOML and some processing in the code.\r\n\r\nThe usage other than \"almost all\" is in i18n strings, introduced by rust-i18n, which is not packaged in Debian and \"polyfilled\" with an [ad-hoc patch](https://salsa.debian.org/debian/trippy/-/blob/debian/sid/debian/patches/ad-hoc-i18n.patch) to deliver the new functionality ASAP without seeing rust-i18n packages through NEW queue. Arguably, it could be replaced with [i18n-embed](https://lib.rs/crates/i18n-embed), which supports the Fluent translation system, as well as the old gettext system, both very popular and widely supported. It has built-in locale resolution through the [locale_config](https://lib.rs/crates/locale_config) crate. But this is probably too big a change, and a too abrupt one shortly after i18n is released.\r\n\r\nNote this is strongly influenced by my PoV as a downstream package maintainer, focusing on maintainability (serde_yml) and avoiding unnecessary dependencies (serde_yml, rust-i18n), which are not always the biggest concerns for upstream developers.\r\n\r\nI'm willing to implement the change if either is considered acceptable.\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex c54e49275..f5243dd95 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -1322,16 +1322,6 @@ version = \"0.0.4\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"64804cc6a5042d4f05379909ba25b503ec04e2c082151d62122d5dcaa274b961\"\n \n-[[package]]\n-name = \"libyml\"\n-version = \"0.0.5\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3302702afa434ffa30847a83305f0a69d6abd74293b6554c18ec85c7ef30c980\"\n-dependencies = [\n- \"anyhow\",\n- \"version_check\",\n-]\n-\n [[package]]\n name = \"linked-hash-map\"\n version = \"0.5.6\"\n@@ -1825,7 +1815,7 @@ dependencies = [\n  \"rust-i18n-support\",\n  \"serde\",\n  \"serde_json\",\n- \"serde_yml 0.0.11\",\n+ \"serde_yml\",\n  \"syn 2.0.90\",\n ]\n \n@@ -1846,7 +1836,7 @@ dependencies = [\n  \"regex\",\n  \"serde\",\n  \"serde_json\",\n- \"serde_yml 0.0.11\",\n+ \"serde_yml\",\n  \"siphasher\",\n  \"toml 0.7.8\",\n  \"triomphe\",\n@@ -1900,18 +1890,18 @@ checksum = \"94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49\"\n \n [[package]]\n name = \"serde\"\n-version = \"1.0.215\"\n+version = \"1.0.216\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"6513c1ad0b11a9376da888e3e0baa0077f1aed55c17f50e7b2397136129fb88f\"\n+checksum = \"0b9781016e935a97e8beecf0c933758c97a5520d32930e460142b4cd80c6338e\"\n dependencies = [\n  \"serde_derive\",\n ]\n \n [[package]]\n name = \"serde_derive\"\n-version = \"1.0.215\"\n+version = \"1.0.216\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"ad1e866f866923f252f05c889987993144fb74e722403468a4ebd70c3cd756c0\"\n+checksum = \"46f859dbbf73865c6627ed570e78961cd3ac92407a2d117204c49232485da55e\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -1970,7 +1960,7 @@ checksum = \"48e76bab63c3fd98d27c17f9cbce177f64a91f5e69ac04cafe04e1bb25d1dc3c\"\n dependencies = [\n  \"indexmap\",\n  \"itoa\",\n- \"libyml 0.0.4\",\n+ \"libyml\",\n  \"log\",\n  \"memchr\",\n  \"ryu\",\n@@ -1979,21 +1969,6 @@ dependencies = [\n  \"tempfile\",\n ]\n \n-[[package]]\n-name = \"serde_yml\"\n-version = \"0.0.12\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"59e2dd588bf1597a252c3b920e0143eb99b0f76e4e082f4c92ce34fbc9e71ddd\"\n-dependencies = [\n- \"indexmap\",\n- \"itoa\",\n- \"libyml 0.0.5\",\n- \"memchr\",\n- \"ryu\",\n- \"serde\",\n- \"version_check\",\n-]\n-\n [[package]]\n name = \"sharded-slab\"\n version = \"0.1.7\"\n@@ -2532,12 +2507,12 @@ dependencies = [\n  \"paste\",\n  \"rand\",\n  \"serde\",\n- \"serde_yml 0.0.12\",\n  \"socket2\",\n  \"test-case\",\n  \"thiserror 2.0.6\",\n  \"tokio\",\n  \"tokio-util\",\n+ \"toml 0.8.19\",\n  \"tracing\",\n  \"tracing-subscriber\",\n  \"trippy-packet\",\n@@ -2711,12 +2686,6 @@ version = \"0.1.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"830b7e5d4d90034032940e4ace0d9a9a057e7a45cd94e6c007832e39edb82f6d\"\n \n-[[package]]\n-name = \"version_check\"\n-version = \"0.9.5\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0b928f33d975fc6ad9f86c8f283853ad26bdd5b10b7f1542aa2fa15e2289105a\"\n-\n [[package]]\n name = \"walkdir\"\n version = \"2.5.0\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 32b7ab3f0..220350b67 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -65,7 +65,6 @@ rust-i18n = \"3.1.2\"\n serde = { version = \"1.0.201\", default-features = false }\n serde_json = { version = \"1.0.117\", default-features = false }\n serde_with = { version = \"3.11.0\", default-features = false, features = [\"macros\"] }\n-serde_yml = \"0.0.12\"\n socket2 = \"0.5.8\"\n strum = { version = \"0.26.3\", default-features = false }\n sys-locale = \"0.3.2\"\ndiff --git a/crates/trippy-core/Cargo.toml b/crates/trippy-core/Cargo.toml\nindex 358999179..8c2ab73d5 100644\n--- a/crates/trippy-core/Cargo.toml\n+++ b/crates/trippy-core/Cargo.toml\n@@ -49,10 +49,10 @@ ipnetwork.workspace = true\n mockall.workspace = true\n rand.workspace = true\n serde = { workspace = true, default-features = false, features = [ \"derive\" ] }\n-serde_yml.workspace = true\n test-case.workspace = true\n tokio-util.workspace = true\n tokio = { workspace = true, features = [ \"full\" ] }\n+toml.workspace = true\n tracing-subscriber = { workspace = true, default-features = false, features = [ \"env-filter\", \"fmt\" ] }\n \n # see https://github.com/meh/rust-tun/pull/74\n@@ -64,4 +64,4 @@ tun2 = { workspace = true, features = [ \"async\" ] }\n sim-tests = []\n \n [lints]\n-workspace = true\n\\ No newline at end of file\n+workspace = true\ndiff --git a/crates/trippy-core/src/state.rs b/crates/trippy-core/src/state.rs\nindex 619303a99..cbab1a9da 100644\n--- a/crates/trippy-core/src/state.rs\n+++ b/crates/trippy-core/src/state.rs\n@@ -891,18 +891,18 @@ mod tests {\n \n     macro_rules! file {\n         ($path:expr) => {{\n-            let yaml = include_str!(concat!(\"../tests/resources/state/\", $path));\n-            serde_yml::from_str(yaml).unwrap()\n+            let data = include_str!(concat!(\"../tests/resources/state/\", $path));\n+            toml::from_str(data).unwrap()\n         }};\n     }\n \n-    #[test_case(file!(\"full_mixed.yaml\"))]\n-    #[test_case(file!(\"full_completed.yaml\"))]\n-    #[test_case(file!(\"all_status.yaml\"))]\n-    #[test_case(file!(\"no_latency.yaml\"))]\n-    #[test_case(file!(\"nat.yaml\"))]\n-    #[test_case(file!(\"minimal.yaml\"))]\n-    #[test_case(file!(\"floss_bloss.yaml\"))]\n+    #[test_case(file!(\"full_mixed.toml\"))]\n+    #[test_case(file!(\"full_completed.toml\"))]\n+    #[test_case(file!(\"all_status.toml\"))]\n+    #[test_case(file!(\"no_latency.toml\"))]\n+    #[test_case(file!(\"nat.toml\"))]\n+    #[test_case(file!(\"minimal.toml\"))]\n+    #[test_case(file!(\"floss_bloss.toml\"))]\n     fn test_scenario(scenario: Scenario) {\n         let mut trace = State::new(StateConfig {\n             max_flows: 1,\ndiff --git a/crates/trippy-tui/locales/app.toml b/crates/trippy-tui/locales/app.toml\nnew file mode 100644\nindex 000000000..c68b0c453\n--- /dev/null\n+++ b/crates/trippy-tui/locales/app.toml\n@@ -0,0 +1,1308 @@\n+_version = 2\n+\n+[trippy]\n+en = \"trippy\"\n+fr = \"trippy\"\n+tr = \"trippy\"\n+it = \"trippy\"\n+pt = \"trippy\"\n+zh = \"trippy\"\n+sv = \"trippy\"\n+ru = \"trippy\"\n+es = \"trippy\"\n+de = \"trippy\"\n+\n+[auto]\n+en = \"auto\"\n+fr = \"automatique\"\n+tr = \"otomatik\"\n+it = \"auto\"\n+pt = \"autom\u00e1tico\"\n+zh = \"\u81ea\u52a8\"\n+sv = \"automatisk\"\n+ru = \"\u0430\u0432\u0442\u043e\"\n+es = \"autom\u00e1tico\"\n+de = \"auto\"\n+\n+[on]\n+en = \"on\"\n+fr = \"activ\u00e9\"\n+tr = \"a\u00e7\u0131k\"\n+it = \"on\"\n+pt = \"ligado\"\n+zh = \"\u5f00\"\n+sv = \"p\u00e5\"\n+ru = \"\u0432\u043a\u043b\"\n+es = \"activo\"\n+de = \"an\"\n+\n+[off]\n+en = \"off\"\n+fr = \"d\u00e9sactiv\u00e9\"\n+tr = \"kapal\u0131\"\n+it = \"off\"\n+pt = \"desligado\"\n+zh = \"\u5173\"\n+sv = \"av\"\n+ru = \"\u0432\u044b\u043a\u043b\"\n+es = \"inactivo\"\n+de = \"aus\"\n+\n+[yes]\n+en = \"Yes\"\n+fr = \"Oui\"\n+tr = \"Evet\"\n+it = \"S\u00ec\"\n+pt = \"Sim\"\n+zh = \"\u662f\"\n+sv = \"Ja\"\n+ru = \"\u0414\u0430\"\n+es = \"S\u00ed\"\n+de = \"Ja\"\n+\n+[no]\n+en = \"No\"\n+fr = \"Non\"\n+tr = \"Hay\u0131r\"\n+it = \"No\"\n+pt = \"N\u00e3o\"\n+zh = \"\u5426\"\n+sv = \"Nej\"\n+ru = \"\u041d\u0435\u0442\"\n+es = \"No\"\n+de = \"Nein\"\n+\n+[none]\n+en = \"none\"\n+fr = \"aucun\"\n+tr = \"hi\u00e7biri\"\n+it = \"nessuno\"\n+pt = \"nenhum\"\n+zh = \"\u65e0\"\n+sv = \"ingen\"\n+ru = \"\u043d\u0435\u0442\"\n+es = \"ninguno\"\n+de = \"keiner\"\n+\n+[hidden]\n+en = \"Hidden\"\n+fr = \"Cach\u00e9\"\n+tr = \"Gizli\"\n+it = \"Nascosto\"\n+pt = \"Oculto\"\n+zh = \"\u9690\u85cf\"\n+sv = \"Dold\"\n+ru = \"\u0421\u043a\u0440\u044b\u0442\u043e\"\n+es = \"Oculto\"\n+de = \"Versteckt\"\n+\n+[flow]\n+en = \"flow\"\n+fr = \"flux\"\n+tr = \"ak\u0131\u015f\"\n+it = \"flusso\"\n+pt = \"fluxo\"\n+zh = \"\u6d41\u91cf\"\n+sv = \"fl\u00f6de\"\n+ru = \"\u043f\u043e\u0442\u043e\u043a\"\n+es = \"flujo\"\n+de = \"fluss\"\n+\n+[flows]\n+en = \"flows\"\n+fr = \"flux\"\n+tr = \"ak\u0131\u015flar\"\n+it = \"flussi\"\n+pt = \"fluxos\"\n+zh = \"\u6d41\u91cf\"\n+sv = \"fl\u00f6den\"\n+ru = \"\u043f\u043e\u0442\u043e\u043a\u0438\"\n+es = \"flujos\"\n+de = \"fl\u00fcsse\"\n+\n+[target]\n+en = \"Target\"\n+fr = \"Cible\"\n+tr = \"Hedef\"\n+it = \"Target\"\n+pt = \"Alvo\"\n+zh = \"\u76ee\u6807\"\n+sv = \"M\u00e5l\"\n+ru = \"\u0426\u0435\u043b\u044c\"\n+es = \"Objetivo\"\n+de = \"Ziel\"\n+\n+[status]\n+en = \"Status\"\n+fr = \"Statut\"\n+tr = \"Durum\"\n+it = \"Stato\"\n+pt = \"Estado\"\n+zh = \"\u72b6\u6001\"\n+sv = \"Status\"\n+ru = \"\u0421\u0442\u0430\u0442\u0443\u0441\"\n+es = \"Estado\"\n+de = \"Status\"\n+\n+[details]\n+en = \"detail\"\n+fr = \"d\u00e9tail\"\n+tr = \"ayr\u0131nt\u0131lar\"\n+it = \"dettagli\"\n+pt = \"detalhe\"\n+zh = \"\u8be6\u60c5\"\n+sv = \"detaljer\"\n+ru = \"\u0434\u0435\u0442\u0430\u043b\u0438\"\n+es = \"detalles\"\n+de = \"detail\"\n+\n+[privileged]\n+en = \"privileged\"\n+fr = \"privil\u00e9gi\u00e9\"\n+tr = \"ayr\u0131cal\u0131kl\u0131\"\n+it = \"privilegiato\"\n+pt = \"privilegiado\"\n+zh = \"\u7279\u6743\"\n+sv = \"privilegierad\"\n+ru = \"\u043f\u0440\u0438\u0432\u0438\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439\"\n+es = \"privilegiado\"\n+de = \"privilegiert\"\n+\n+[unprivileged]\n+en = \"unprivileged\"\n+fr = \"non privil\u00e9gi\u00e9\"\n+tr = \"ayr\u0131cal\u0131ks\u0131z\"\n+it = \"non privilegiato\"\n+pt = \"n\u00e3o privilegiado\"\n+zh = \"\u975e\u7279\u6743\"\n+sv = \"oprivilegierad\"\n+ru = \"\u043d\u0435\u043f\u0440\u0438\u0432\u0438\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439\"\n+es = \"no privilegiado\"\n+de = \"unprivilegiert\"\n+\n+[privacy]\n+en = \"privacy\"\n+fr = \"confidentialit\u00e9\"\n+tr = \"gizlilik\"\n+it = \"privacy\"\n+pt = \"privacidade\"\n+zh = \"\u9690\u79c1\"\n+sv = \"integritet\"\n+ru = \"\u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\"\n+es = \"privacidad\"\n+de = \"datenschutz\"\n+\n+[na]\n+en = \"n/a\"\n+fr = \"non disponible\"\n+tr = \"yok\"\n+it = \"n/d\"\n+pt = \"n/d\"\n+zh = \"\u65e0\"\n+sv = \"ej tillg\u00e4nglig\"\n+ru = \"\u043d/\u0434\"\n+es = \"n/d\"\n+de = \"n/a\"\n+\n+[discovered]\n+en = \"discovered %{hop_count} hops\"\n+fr = \"%{hop_count} sauts d\u00e9couverts\"\n+tr = \"%{hop_count} atlanan ke\u015ffedildi\"\n+it = \"%{hop_count} salti trovati\"\n+pt = \"descobriu %{hop_count} saltos\"\n+zh = \"\u53d1\u73b0 %{hop_count} \u8df3\"\n+sv = \"uppt\u00e4ckte %{hop_count} hopp\"\n+ru = \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e %{hop_count} \u043f\u0440\u044b\u0436\u043a\u043e\u0432\"\n+es = \"se descubrieron %{hop_count} saltos\"\n+de = \"%{hop_count} gefundene hops\"\n+\n+[discovered_flows]\n+en = \"discovered %{hop_count} hops and %{flow_count} unique %{plural_flows}\"\n+fr = \"%{hop_count} sauts et %{flow_count} %{plural_flows} uniques d\u00e9couverts\"\n+tr = \"%{hop_count} atlanan ve %{flow_count} benzersiz %{plural_flows} ke\u015ffedildi\"\n+it = \"scoperti %{hop_count} salti e %{flow_count} %{plural_flows} unici\"\n+pt = \"descobriu %{hop_count} saltos e %{flow_count} %{plural_flows} \u00fanicos\"\n+zh = \"\u53d1\u73b0 %{hop_count} \u8df3\uff0c%{flow_count} \u4e2a\u552f\u4e00%{plural_flow}\"\n+sv = \"%{hop_count} hopp och unika %{flow_count} %{plural_flows} uppt\u00e4ckta\"\n+ru = \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e %{hop_count} \u043f\u0440\u044b\u0436\u043a\u043e\u0432 \u0438 %{flow_count} \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 %{plural_flows}\"\n+es = \"se descubrieron %{hop_count} saltos y %{flow_count} \u00fanicos %{plural_flows}\"\n+de = \"%{hop_count} gefundene hops und %{flow_count} eindeutige %{plural_flows}\"\n+\n+[unknown]\n+en = \"unknown\"\n+fr = \"inconnu\"\n+tr = \"bilinmeyen\"\n+it = \"sconosciuto\"\n+pt = \"desconhecido\"\n+zh = \"\u672a\u77e5\"\n+sv = \"ok\u00e4nd\"\n+ru = \"\u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\"\n+es = \"desconocido\"\n+de = \"unbekannt\"\n+\n+[icmp]\n+en = \"icmp\"\n+fr = \"icmp\"\n+tr = \"icmp\"\n+it = \"icmp\"\n+pt = \"icmp\"\n+zh = \"icmp\"\n+sv = \"icmp\"\n+ru = \"icmp\"\n+es = \"icmp\"\n+de = \"icmp\"\n+\n+[udp]\n+en = \"udp\"\n+fr = \"udp\"\n+tr = \"udp\"\n+it = \"udp\"\n+pt = \"udp\"\n+zh = \"udp\"\n+sv = \"udp\"\n+ru = \"udp\"\n+es = \"udp\"\n+de = \"udp\"\n+\n+[tcp]\n+en = \"tcp\"\n+fr = \"tcp\"\n+tr = \"tcp\"\n+it = \"tcp\"\n+pt = \"tcp\"\n+zh = \"tcp\"\n+sv = \"tcp\"\n+ru = \"tcp\"\n+es = \"tcp\"\n+de = \"tcp\"\n+\n+[status_failures]\n+en = \"%{failure_count} of %{total_probes} (%{failure_rate}%) probes failed\"\n+fr = \"%{failure_count} sur %{total_probes} (%{failure_rate}%) sondes ont \u00e9chou\u00e9\"\n+tr = \"%{failure_count} / %{total_probes} (%{failure_rate}%) sondaj\u0131n ba\u015far\u0131s\u0131z olmas\u0131\"\n+it = \"%{failure_count} di %{total_probes} (%{failure_rate}%) prove fallite\"\n+pt = \"%{failure_count} de %{total_probes} (%{failure_rate}%) sondas falharam\"\n+zh = \"%{failure_count} \u4e2a\u5931\u8d25\uff0c\u5171\u63a2\u6d4b\u5230 %{total_probes} \u4e2a\uff08%{failure_rate}%\uff09\"\n+sv = \"%{failure_count} av %{total_probes} (%{failure_rate}%) misslyckade prober\"\n+ru = \"%{failure_count} \u0438\u0437 %{total_probes} (%{failure_rate}%) \u0437\u043e\u043d\u0434\u043e\u0432 \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c\"\n+es = \"%{failure_count} de %{total_probes} (%{failure_rate}%) sondas fallaron\"\n+de = \"%{failure_count} von %{total_probes} (%{failure_rate}%) sonden sind fehlgeschlagen\"\n+\n+[status_failed]\n+en = \"Failed\"\n+fr = \"\u00c9chec\"\n+tr = \"Ba\u015far\u0131s\u0131z\"\n+it = \"Fallito\"\n+pt = \"Falhou\"\n+zh = \"\u5931\u8d25\"\n+sv = \"Misslyckades\"\n+ru = \"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c\"\n+es = \"Fallido\"\n+de = \"Fehlgeschlagen\"\n+\n+[status_running]\n+en = \"Running\"\n+fr = \"En cours\"\n+tr = \"\u00c7al\u0131\u015f\u0131yor\"\n+it = \"In esecuzione\"\n+pt = \"Executando\"\n+zh = \"\u8fd0\u884c\u4e2d\"\n+sv = \"K\u00f6r\"\n+ru = \"\u0417\u0430\u043f\u0443\u0449\u0435\u043d\"\n+es = \"En ejecuci\u00f3n\"\n+de = \"L\u00e4uft\"\n+\n+[status_frozen]\n+en = \"Frozen\"\n+fr = \"Gel\u00e9\"\n+tr = \"Dondurulmu\u015f\"\n+it = \"Congelato\"\n+pt = \"Congelado\"\n+zh = \"\u51bb\u7ed3\"\n+sv = \"Frusen\"\n+ru = \"\u0417\u0430\u043c\u043e\u0440\u043e\u0436\u0435\u043d\"\n+es = \"Congelado\"\n+de = \"Eingefroren\"\n+\n+[awaiting_data]\n+en = \"Awaiting data...\"\n+fr = \"En attente de donn\u00e9es...\"\n+tr = \"Veri bekleniyor...\"\n+it = \"In attesa di dati...\"\n+pt = \"Aguardando dados...\"\n+zh = \"\u7b49\u5f85\u6570\u636e\u2026\u2026\"\n+sv = \"V\u00e4ntar p\u00e5 data...\"\n+ru = \"\u041e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445...\"\n+es = \"Esperando datos...\"\n+de = \"Warten auf Daten...\"\n+\n+[header_help]\n+en = \"help\"\n+fr = \"aide\"\n+tr = \"yard\u0131m\"\n+it = \"aiuto\"\n+pt = \"ajuda\"\n+zh = \"\u5e2e\u52a9\"\n+sv = \"hj\u00e4lp\"\n+ru = \"\u043f\u043e\u043c\u043e\u0449\u044c\"\n+es = \"ayuda\"\n+de = \"hilfe\"\n+\n+[header_settings]\n+en = \"settings\"\n+fr = \"param\u00e8tres\"\n+tr = \"ayarlar\"\n+it = \"impostazioni\"\n+pt = \"configura\u00e7\u00f5es\"\n+zh = \"\u8bbe\u7f6e\"\n+sv = \"inst\u00e4llningar\"\n+ru = \"\u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\"\n+es = \"configuraciones\"\n+de = \"einstellungen\"\n+\n+[header_quit]\n+en = \"quit\"\n+fr = \"quitter\"\n+tr = \"c\u0131k\u0131\u015f\"\n+it = \"uscita\"\n+pt = \"sair\"\n+zh = \"\u9000\u51fa\"\n+sv = \"avsluta\"\n+ru = \"\u0412\u044b\u0445\u043e\u0434\"\n+es = \"salir\"\n+de = \"beenden\"\n+\n+[title_hops]\n+en = \"Hops\"\n+fr = \"Sauts\"\n+tr = \"Atlananlar\"\n+it = \"Salti\"\n+pt = \"Saltos\"\n+zh = \"\u8df3\"\n+sv = \"Hopp\"\n+ru = \"\u041f\u0440\u044b\u0436\u043a\u0438\"\n+es = \"Saltos\"\n+de = \"Hops\"\n+\n+[title_frequency]\n+en = \"Frequency\"\n+fr = \"Fr\u00e9quence\"\n+tr = \"S\u0131kl\u0131k\"\n+it = \"Frequenza\"\n+pt = \"Frequ\u00eancia\"\n+zh = \"\u9891\u7387\"\n+sv = \"Frekvens\"\n+ru = \"\u0427\u0430\u0441\u0442\u043e\u0442\u0430\"\n+es = \"Frecuencia\"\n+de = \"Frequenz\"\n+\n+[title_samples]\n+en = \"Samples\"\n+fr = \"\u00c9chantillons\"\n+tr = \"\u00d6rnekler\"\n+it = \"Campioni\"\n+pt = \"Amostras\"\n+zh = \"\u6837\u672c\"\n+sv = \"Prover\"\n+ru = \"\u041e\u0431\u0440\u0430\u0437\u0446\u044b\"\n+es = \"Muestras\"\n+de = \"Proben\"\n+\n+[title_traces]\n+en = \"Traces\"\n+fr = \"Traces\"\n+tr = \"\u0130zler\"\n+it = \"Tracce\"\n+pt = \"Rastreios\"\n+zh = \"\u8ddf\u8e2a\"\n+sv = \"Sp\u00e5r\"\n+ru = \"\u0421\u043b\u0435\u0434\u044b\"\n+es = \"Trazas\"\n+de = \"Spuren\"\n+\n+[title_flows]\n+en = \"Flows\"\n+fr = \"Flux\"\n+tr = \"Ak\u0131\u015flar\"\n+it = \"Flussi\"\n+pt = \"Fluxos\"\n+zh = \"\u6d41\u91cf\"\n+sv = \"Fl\u00f6den\"\n+ru = \"\u041f\u043e\u0442\u043e\u043a\u0438\"\n+es = \"Flujos\"\n+de = \"Fl\u00fcsse\"\n+\n+[title_map]\n+en = \"Map\"\n+fr = \"Carte\"\n+tr = \"Harita\"\n+it = \"Mappa\"\n+pt = \"Mapa\"\n+zh = \"\u5730\u56fe\"\n+sv = \"Karta\"\n+ru = \"\u041a\u0430\u0440\u0442\u0430\"\n+es = \"Mapa\"\n+de = \"Karte\"\n+\n+[title_help]\n+en = \"Help\"\n+fr = \"Aide\"\n+tr = \"Yard\u0131m\"\n+it = \"Aiuto\"\n+pt = \"Ajuda\"\n+zh = \"\u5e2e\u52a9\"\n+sv = \"Hj\u00e4lp\"\n+ru = \"\u041f\u043e\u043c\u043e\u0449\u044c\"\n+es = \"Ayuda\"\n+de = \"Hilfe\"\n+\n+[title_settings]\n+en = \"Settings\"\n+fr = \"Param\u00e8tres\"\n+tr = \"Ayarlar\"\n+it = \"Impostazioni\"\n+pt = \"Configura\u00e7\u00f5es\"\n+zh = \"\u8bbe\u7f6e\"\n+sv = \"Inst\u00e4llningar\"\n+ru = \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\"\n+es = \"Configuraciones\"\n+de = \"Einstellungen\"\n+\n+[bsod_failed]\n+en = \"Trippy Failed :(\"\n+fr = \"Trippy a \u00e9chou\u00e9 :(\"\n+tr = \"Trippy Ba\u015far\u0131s\u0131z :(\"\n+it = \"Trippy ha avuto un problema :(\"\n+pt = \"Trippy falhou :(\"\n+zh = \"Trippy \u5931\u8d25 :(\"\n+sv = \"Trippy misslyckades :(\"\n+ru = \"Trippy \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c :(\"\n+es = \"Trippy tuvo un problema :(\"\n+de = \"Trippy ist fehlgeschlagen :(\"\n+\n+[bsod_quit]\n+en = \"Press q to quit\"\n+fr = \"Appuyez sur q pour quitter\"\n+tr = \"\u00c7\u0131kmak i\u00e7in q tu\u015funa bas\u0131n\"\n+it = \"Premi q per uscire\"\n+pt = \"Pressione q para sair\"\n+zh = \"\u6309 q \u9000\u51fa\"\n+sv = \"Tryck p\u00e5 q f\u00f6r att avsluta\"\n+ru = \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 q \u0434\u043b\u044f \u0432\u044b\u0445\u043e\u0434\u0430\"\n+es = \"Presiona q para salir\"\n+de = \"Dr\u00fccken Sie q, um zu beenden\"\n+\n+[hop]\n+en = \"Hop\"\n+fr = \"Saut\"\n+tr = \"Atla\"\n+it = \"Salto\"\n+pt = \"Salto\"\n+zh = \"\u8df3\"\n+sv = \"Hopp\"\n+ru = \"\u041f\u0440\u044b\u0436\u043e\u043a\"\n+es = \"Salto\"\n+de = \"Hop\"\n+\n+[rtt]\n+en = \"RTT\"\n+fr = \"RTT\"\n+tr = \"RTT\"\n+it = \"RTT\"\n+pt = \"RTT\"\n+zh = \"\u5f80\u8fd4\u65f6\u95f4\"\n+sv = \"RTT\"\n+ru = \"RTT\"\n+es = \"RTT\"\n+de = \"RTT\"\n+\n+[title_chart]\n+en = \"Chart\"\n+fr = \"Graphique\"\n+tr = \"Grafik\"\n+it = \"Grafico\"\n+pt = \"Gr\u00e1fico\"\n+zh = \"\u56fe\u8868\"\n+sv = \"Diagram\"\n+ru = \"\u0414\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430\"\n+es = \"Gr\u00e1fico\"\n+de = \"Diagramm\"\n+\n+[samples]\n+en = \"Samples\"\n+fr = \"\u00c9chantillons\"\n+tr = \"\u00d6rnekler\"\n+it = \"Campioni\"\n+pt = \"Amostras\"\n+zh = \"\u6837\u672c\"\n+sv = \"Prover\"\n+ru = \"\u041e\u0431\u0440\u0430\u0437\u0446\u044b\"\n+es = \"Muestras\"\n+de = \"Proben\"\n+\n+[host]\n+en = \"Host\"\n+fr = \"H\u00f4te\"\n+tr = \"Ana bilgisayar\"\n+it = \"Host\"\n+pt = \"Host\"\n+zh = \"\u4e3b\u673a\"\n+sv = \"V\u00e4rd\"\n+ru = \"\u0425\u043e\u0441\u0442\"\n+es = \"Host\"\n+de = \"Host\"\n+\n+[no_response]\n+en = \"No response\"\n+fr = \"Pas de r\u00e9ponse\"\n+tr = \"Yan\u0131t yok\"\n+it = \"Nessuna risposta\"\n+pt = \"Sem resposta\"\n+zh = \"\u65e0\u54cd\u5e94\"\n+sv = \"Inget svar\"\n+ru = \"\u041d\u0435\u0442 \u043e\u0442\u0432\u0435\u0442\u0430\"\n+es = \"Sin respuesta\"\n+de = \"Keine Antwort\"\n+\n+[dns_failed]\n+en = \"Failed\"\n+fr = \"\u00c9chec\"\n+tr = \"Ba\u015far\u0131s\u0131z\"\n+it = \"Fallito\"\n+pt = \"Falhou\"\n+zh = \"\u5931\u8d25\"\n+sv = \"Misslyckades\"\n+ru = \"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c\"\n+es = \"Fallido\"\n+de = \"Fehlgeschlagen\"\n+\n+[dns_timeout]\n+en = \"Timeout\"\n+fr = \"D\u00e9lai d\u00e9pass\u00e9\"\n+tr = \"Zaman a\u015f\u0131m\u0131\"\n+it = \"Tempo scaduto\"\n+pt = \"Tempo esgotado\"\n+zh = \"\u8d85\u65f6\"\n+sv = \"Tidsgr\u00e4ns n\u00e5dd\"\n+ru = \"\u0422\u0430\u0439\u043c-\u0430\u0443\u0442\"\n+es = \"Tiempo de espera\"\n+de = \"Zeit\u00fcberschreitung\"\n+\n+[labels]\n+en = \"labels\"\n+fr = \"\u00e9tiquettes\"\n+tr = \"etiketler\"\n+it = \"etichette\"\n+pt = \"etiquetas\"\n+zh = \"\u6807\u7b7e\"\n+sv = \"etiketter\"\n+ru = \"\u043c\u0435\u0442\u043a\u0438\"\n+es = \"etiquetas\"\n+de = \"etiketten\"\n+\n+[not_enabled]\n+en = \"not enabled\"\n+fr = \"non activ\u00e9\"\n+tr = \"etkin de\u011fil\"\n+it = \"non abilitato\"\n+pt = \"n\u00e3o ativado\"\n+zh = \"\u672a\u542f\u7528\"\n+sv = \"ej aktiverad\"\n+ru = \"\u043d\u0435 \u0430\u043a\u0442\u0438\u0432\u0438\u0440\u043e\u0432\u0430\u043d\"\n+es = \"no habilitado\"\n+de = \"nicht aktiviert\"\n+\n+[not_found]\n+en = \"not found\"\n+fr = \"non trouv\u00e9\"\n+tr = \"bulunamad\u0131\"\n+it = \"non trovato\"\n+pt = \"n\u00e3o encontrado\"\n+zh = \"\u672a\u627e\u5230\"\n+sv = \"hittades inte\"\n+ru = \"\u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e\"\n+es = \"no encontrado\"\n+de = \"nicht gefunden\"\n+\n+[awaited]\n+en = \"awaited\"\n+fr = \"attendu\"\n+tr = \"beklenen\"\n+it = \"atteso\"\n+pt = \"aguardado\"\n+zh = \"\u7b49\u5f85\"\n+sv = \"v\u00e4ntade\"\n+ru = \"\u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u0439\"\n+es = \"esperado\"\n+de = \"erwartet\"\n+\n+[name]\n+en = \"Name\"\n+fr = \"Nom\"\n+tr = \"Ad\"\n+it = \"Nome\"\n+pt = \"Nome\"\n+zh = \"\u540d\u79f0\"\n+sv = \"Namn\"\n+ru = \"\u0418\u043c\u044f\"\n+es = \"Nombre\"\n+de = \"Name\"\n+\n+[info]\n+en = \"Info\"\n+fr = \"Information\"\n+tr = \"Bilgi\"\n+it = \"Info\"\n+pt = \"Informa\u00e7\u00e3o\"\n+zh = \"\u4fe1\u606f\"\n+sv = \"Information\"\n+ru = \"\u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\"\n+es = \"Informaci\u00f3n\"\n+de = \"Info\"\n+\n+[geo]\n+en = \"Geo\"\n+fr = \"G\u00e9o\"\n+tr = \"Co\u011frafi\"\n+it = \"Geo\"\n+pt = \"Geo\"\n+zh = \"\u5730\u7406\u5750\u6807\"\n+sv = \"Geo\"\n+ru = \"\u0413\u0435\u043e\"\n+es = \"Geo\"\n+de = \"Geo\"\n+\n+[pos]\n+en = \"Pos\"\n+fr = \"Pos\"\n+tr = \"Poz\"\n+it = \"Pos\"\n+pt = \"Pos\"\n+zh = \"\u4f4d\u7f6e\"\n+sv = \"Pos\"\n+ru = \"\u041f\u043e\u0437\"\n+es = \"Pos\"\n+de = \"Pos\"\n+\n+[ext]\n+en = \"Ext\"\n+fr = \"Ext\"\n+tr = \"Uzant\u0131\"\n+it = \"Est\"\n+pt = \"Ext\"\n+zh = \"\u6269\u5c55\"\n+sv = \"Ext\"\n+ru = \"\u0420\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u0435\"\n+es = \"Ext\"\n+de = \"Ext\"\n+\n+[help_tagline]\n+en = \"A network diagnostic tool\"\n+fr = \"Un outil de diagnostic r\u00e9seau\"\n+tr = \"Bir a\u011f analiz arac\u0131\"\n+it = \"Uno strumento diagnostico di rete\"\n+pt = \"Uma ferramenta de diagn\u00f3stico de rede\"\n+zh = \"\u7f51\u7edc\u8bca\u65ad\u5de5\u5177\"\n+sv = \"Ett n\u00e4tverksdiagnostikverktyg\"\n+ru = \"\u0418\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u043a\u0438 \u0441\u0435\u0442\u0438\"\n+es = \"Una herramienta de diagn\u00f3stico de red\"\n+de = \"Ein Netzwerkdiagnosetool\"\n+\n+[help_show_settings]\n+en = \"Press [%{key}] to show all settings\"\n+fr = \"Appuyez sur [%{key}] pour afficher tous les param\u00e8tres\"\n+tr = \"T\u00fcm ayarlar\u0131 g\u00f6rmek i\u00e7in [%{key}] tu\u015funa bas\u0131n\"\n+it = \"Premi [%{key}] per visualizzare tutte le impostazioni\"\n+pt = \"Pressione [%{key}] para mostrar todas as configura\u00e7\u00f5es\"\n+zh = \"\u6309 [%{key}] \u663e\u793a\u6240\u6709\u8bbe\u7f6e\"\n+sv = \"Tryck p\u00e5 [%{key}] f\u00f6r att visa alla inst\u00e4llningar\"\n+ru = \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{key}], \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\"\n+es = \"Presiona [%{key}] para mostrar todas las configuraciones\"\n+de = \"Dr\u00fccken Sie [%{key}], um alle Einstellungen anzuzeigen\"\n+\n+[help_show_bindings]\n+en = \"Press [%{key}] to show all bindings\"\n+fr = \"Appuyez sur [%{key}] pour afficher tous les raccourcis clavier\"\n+tr = \"T\u00fcm ba\u011flant\u0131lar\u0131 g\u00f6rmek i\u00e7in [%{key}] tu\u015funa bas\u0131n\"\n+it = \"Premi [%{key}] per visualizzare tutti i collegamenti\"\n+pt = \"Pressione [%{key}] para mostrar todos os atalhos\"\n+zh = \"\u6309 [%{key}] \u663e\u793a\u6240\u6709\u7ed1\u5b9a\"\n+sv = \"Tryck p\u00e5 [%{key}] f\u00f6r att visa alla kortkommando\"\n+ru = \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{key}], \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u043f\u0440\u0438\u0432\u044f\u0437\u043a\u0438\"\n+es = \"Presiona [%{key}] para mostrar todos los atajos\"\n+de = \"Dr\u00fccken Sie [%{key}], um alle Tastenbelegungen anzuzeigen\"\n+\n+[help_show_columns]\n+en = \"Press [%{key}] to show all columns\"\n+fr = \"Appuyez sur [%{key}] pour afficher toutes les colonnes\"\n+tr = \"T\u00fcm s\u00fctunlar\u0131 g\u00f6rmek i\u00e7in [%{key}] tu\u015funa bas\u0131n\"\n+it = \"Premi [%{key}] per visualizzare tutte le colonne\"\n+pt = \"Pressione [%{key}] para mostrar todas as colunas\"\n+zh = \"\u6309 [%{key}] \u663e\u793a\u6240\u6709\u5217\"\n+sv = \"Tryck p\u00e5 [%{key}] f\u00f6r att visa alla kolumner\"\n+ru = \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{key}], \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\"\n+es = \"Presiona [%{key}] para mostrar todas las columnas\"\n+de = \"Dr\u00fccken Sie [%{key}], um alle Spalten anzuzeigen\"\n+\n+[help_license]\n+en = \"Distributed under the Apache License 2.0\"\n+fr = \"Distribu\u00e9 sous licence Apache 2.0\"\n+tr = \"Apache Lisans\u0131 2.0 alt\u0131nda da\u011f\u0131t\u0131lm\u0131\u015ft\u0131r\"\n+it = \"Distribuito con licenza Apache 2.0\"\n+pt = \"Distribu\u00eddo sob a licen\u00e7a Apache 2.0\"\n+zh = \"\u4ee5 Apache-2.0 \u8bb8\u53ef\u5206\u53d1\"\n+sv = \"Distribueras under Apache License 2.0\"\n+ru = \"\u0420\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u043e\u0434 \u043b\u0438\u0446\u0435\u043d\u0437\u0438\u0435\u0439 Apache 2.0\"\n+es = \"Distribuido bajo la Licencia Apache 2.0\"\n+de = \"Verteilt unter der Apache-Lizenz 2.0\"\n+\n+[help_copyright]\n+en = \"Copyright 2022 Trippy Contributors\"\n+fr = \"Copyright 2022 Contributeurs de Trippy\"\n+tr = \"Telif Hakk\u0131 2022 - Trippy'ye Katk\u0131da Bulunanlar\"\n+it = \"Copyright 2022 - Collaboratori di Trippy\"\n+pt = \"Direitos autorais 2022 Colaboradores do Trippy\"\n+zh = \"\u7248\u6743\u6240\u6709 2022 Trippy \u8d21\u732e\u8005\"\n+sv = \"Upphovsr\u00e4tt 2022 Trippy-bidragsgivare\"\n+ru = \"\u0410\u0432\u0442\u043e\u0440\u0441\u043a\u043e\u0435 \u043f\u0440\u0430\u0432\u043e 2022 \u0423\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0438 Trippy\"\n+es = \"Derechos de autor 2022 Contribuidores de Trippy\"\n+de = \"Copyright 2022 Trippy Mitwirkende\"\n+\n+[geoip_not_enabled]\n+en = \"GeoIp not enabled\"\n+fr = \"GeoIp non activ\u00e9\"\n+tr = \"GeoIp etkin de\u011fil\"\n+it = \"GeoIp non abilitato\"\n+pt = \"GeoIp n\u00e3o ativado\"\n+zh = \"GeoIp \u672a\u542f\u7528\"\n+sv = \"GeoIp inte aktiverad\"\n+ru = \"GeoIp \u043d\u0435 \u0430\u043a\u0442\u0438\u0432\u0438\u0440\u043e\u0432\u0430\u043d\"\n+es = \"GeoIp no habilitado\"\n+de = \"GeoIp nicht aktiviert\"\n+\n+[geoip_no_data_for_hop]\n+en = \"No GeoIp data for hop\"\n+fr = \"Pas de donn\u00e9es GeoIp pour le saut\"\n+tr = \"Hop i\u00e7in GeoIp verisi yok\"\n+it = \"Nessun dato GeoIp per il salto\"\n+pt = \"Nenhum dado GeoIp para o salto\"\n+zh = \"\u65e0 GeoIp \u6570\u636e\"\n+sv = \"Inga GeoIp-data f\u00f6r hopp\"\n+ru = \"\u041d\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0445 GeoIp \u0434\u043b\u044f \u043f\u0440\u044b\u0436\u043a\u0430\"\n+es = \"No hay datos de GeoIp para el salto\"\n+de = \"Keine GeoIp-Daten f\u00fcr den Hop\"\n+\n+[geoip_multiple_data_for_hop]\n+en = \"Multiple GeoIp locations for hop\"\n+fr = \"Emplacements GeoIp multiples pour le saut\"\n+tr = \"Hop i\u00e7in birden fazla GeoIp konumu\"\n+it = \"Posizioni GeoIp multiple per il salto\"\n+pt = \"M\u00faltiplas localiza\u00e7\u00f5es GeoIp para o salto\"\n+zh = \"\u591a\u4e2a GeoIp \u4f4d\u7f6e\"\n+sv = \"Flera GeoIp-platser f\u00f6r hopp\"\n+ru = \"\u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043c\u0435\u0441\u0442\u043e\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0439 GeoIp \u0434\u043b\u044f \u043f\u0440\u044b\u0436\u043a\u0430\"\n+es = \"M\u00faltiples ubicaciones de GeoIp para el salto\"\n+de = \"Mehrere GeoIp-Standorte f\u00fcr den Hop\"\n+\n+[kilometer]\n+en = \"km\"\n+fr = \"km\"\n+tr = \"km\"\n+it = \"km\"\n+pt = \"km\"\n+zh = \"\u516c\u91cc\"\n+sv = \"km\"\n+ru = \"\u043a\u043c\"\n+es = \"km\"\n+de = \"km\"\n+\n+[settings_info]\n+en = \"Info\"\n+fr = \"Information\"\n+tr = \"Bilgi\"\n+it = \"Info\"\n+pt = \"Informa\u00e7\u00e3o\"\n+zh = \"\u4fe1\u606f\"\n+sv = \"Information\"\n+ru = \"\u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\"\n+es = \"Informaci\u00f3n\"\n+de = \"Info\"\n+\n+[settings_tab_tui_title]\n+en = \"Tui\"\n+fr = \"Tui\"\n+tr = \"Tui\"\n+it = \"Tui\"\n+pt = \"Tui\"\n+zh = \"\u7ec8\u7aef\u7528\u6237\u754c\u9762\"\n+sv = \"Tui\"\n+ru = \"Tui\"\n+es = \"Tui\"\n+de = \"Tui\"\n+\n+[settings_tab_trace_title]\n+en = \"Trace\"\n+fr = \"Tracer\"\n+tr = \"\u0130z\"\n+it = \"Traccia\"\n+pt = \"Rastrear\"\n+zh = \"\u8ddf\u8e2a\"\n+sv = \"Sp\u00e5ra\"\n+ru = \"\u0421\u043b\u0435\u0434\"\n+es = \"Rastrear\"\n+de = \"Trace\"\n+\n+[settings_tab_dns_title]\n+en = \"DNS\"\n+fr = \"DNS\"\n+tr = \"DNS\"\n+it = \"DNS\"\n+pt = \"DNS\"\n+zh = \"DNS\"\n+sv = \"DNS\"\n+ru = \"DNS\"\n+es = \"DNS\"\n+de = \"DNS\"\n+\n+[settings_tab_geoip_title]\n+en = \"GeoIp\"\n+fr = \"GeoIp\"\n+tr = \"GeoIp\"\n+it = \"GeoIp\"\n+pt = \"GeoIp\"\n+zh = \"GeoIp\"\n+sv = \"GeoIp\"\n+ru = \"GeoIp\"\n+es = \"GeoIp\"\n+de = \"GeoIp\"\n+\n+[settings_tab_bindings_title]\n+en = \"Bindings\"\n+fr = \"Raccourcis clavier\"\n+tr = \"Ba\u011flant\u0131lar\"\n+it = \"Collegamenti\"\n+pt = \"Atalhos\"\n+zh = \"\u7ed1\u5b9a\"\n+sv = \"Kortkommando\"\n+ru = \"\u041f\u0440\u0438\u0432\u044f\u0437\u043a\u0438\"\n+es = \"Atajos\"\n+de = \"Tastenbelegungen\"\n+\n+[settings_tab_theme_title]\n+en = \"Theme\"\n+fr = \"Th\u00e8me\"\n+tr = \"Tema\"\n+it = \"Tema\"\n+pt = \"Tema\"\n+zh = \"\u4e3b\u9898\"\n+sv = \"Tema\"\n+ru = \"\u0422\u0435\u043c\u0430\"\n+es = \"Tema\"\n+de = \"Darstellung\"\n+\n+[settings_tab_columns_title]\n+en = \"Columns\"\n+fr = \"Colonnes\"\n+tr = \"S\u00fctunlar\"\n+it = \"Colonne\"\n+pt = \"Colunas\"\n+zh = \"\u5217\"\n+es = \"Columnas\"\n+sv = \"Kolumner\"\n+ru = \"\u0421\u0442\u043e\u043b\u0431\u0446\u044b\"\n+de = \"Spalten\"\n+\n+[settings_tab_tui_desc]\n+en = \"Settings which control how data is displayed in this Tui\"\n+fr = \"Param\u00e8tres qui contr\u00f4lent la fa\u00e7on dont les donn\u00e9es sont affich\u00e9es dans ce Tui\"\n+tr = \"Aray\u00fczde verilerin nas\u0131l g\u00f6r\u00fcnt\u00fclendi\u011fini kontrol eden ayarlar\"\n+it = \"Impostazioni che controllano come i dati vengono visualizzati in questo Tui\"\n+pt = \"Configura\u00e7\u00f5es que controlam como os dados s\u00e3o exibidos neste Tui\"\n+zh = \"\u6570\u636e\u663e\u793a\u65b9\u5f0f\u8bbe\u7f6e\"\n+sv = \"Inst\u00e4llningar som styr hur data visas i detta Tui\"\n+ru = \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442, \u043a\u0430\u043a \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0442\u0441\u044f \u0432 \u044d\u0442\u043e\u043c Tui\"\n+es = \"Configuraciones que controlan c\u00f3mo se muestran los datos en este Tui\"\n+de = \"Einstellungen, die steuern, wie Daten in diesem Tui angezeigt werden\"\n+\n+[settings_tab_trace_desc]\n+en = \"Settings which control the tracing strategy\"\n+fr = \"Param\u00e8tres qui contr\u00f4lent la strat\u00e9gie de tra\u00e7age\"\n+tr = \"\u0130zleme stratejisini kontrol eden ayarlar\"\n+it = \"Impostazioni che controllano la strategia di tracciamento\"\n+pt = \"Configura\u00e7\u00f5es que controlam a estrat\u00e9gia de rastreamento\"\n+zh = \"\u8ddf\u8e2a\u7b56\u7565\u8bbe\u7f6e\"\n+sv = \"Inst\u00e4llningar som styr sp\u00e5rningsstrategin\"\n+ru = \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044e \u0442\u0440\u0430\u0441\u0441\u0438\u0440\u043e\u0432\u043a\u0438\"\n+es = \"Configuraciones que controlan la estrategia de rastreo\"\n+de = \"Einstellungen, die die Tracing-Strategie steuern\"\n+\n+[settings_tab_dns_desc]\n+en = \"Settings which control how DNS lookups are performed\"\n+fr = \"Param\u00e8tres qui contr\u00f4lent la fa\u00e7on dont les recherches DNS sont effectu\u00e9es\"\n+tr = \"DNS aramalar\u0131n\u0131n nas\u0131l yap\u0131ld\u0131\u011f\u0131n\u0131 kontrol eden ayarlar\"\n+it = \"Impostazioni che controllano come vengono eseguite le ricerche DNS\"\n+pt = \"Configura\u00e7\u00f5es que controlam como as pesquisas DNS s\u00e3o realizadas\"\n+zh = \"DNS \u67e5\u8be2\u8bbe\u7f6e\"\n+sv = \"Inst\u00e4llningar som styr hur DNS-uppslag utf\u00f6rs\"\n+ru = \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442, \u043a\u0430\u043a \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u044e\u0442\u0441\u044f DNS-\u0437\u0430\u043f\u0440\u043e\u0441\u044b\"\n+es = \"Configuraciones que controlan c\u00f3mo se realizan las b\u00fasquedas de DNS\"\n+de = \"Einstellungen, die steuern, wie DNS-Lookups durchgef\u00fchrt werden\"\n+\n+[settings_tab_geoip_desc]\n+en = \"Settings relating to GeoIp\"\n+fr = \"Param\u00e8tres relatifs \u00e0 GeoIp\"\n+tr = \"GeoIp ile ilgili ayarlar\"\n+it = \"Impostazioni relative a GeoIp\"\n+pt = \"Configura\u00e7\u00f5es relacionadas ao GeoIp\"\n+zh = \"GeoIp \u8bbe\u7f6e\"\n+sv = \"Inst\u00e4llningar som r\u00f6r GeoIp\"\n+ru = \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u0430\u0441\u0430\u044e\u0449\u0438\u0435\u0441\u044f GeoIp\"\n+es = \"Configuraciones relacionadas con GeoIp\"\n+de = \"Einstellungen im Zusammenhang mit GeoIp\"\n+\n+[settings_tab_bindings_desc]\n+en = \"Tui key bindings\"\n+fr = \"Raccourcis clavier Tui\"\n+tr = \"Tui tu\u015f ayarlar\u0131\"\n+it = \"Collegamenti chiave Tui\"\n+pt = \"Atalhos de teclado Tui\"\n+zh = \"\u6309\u952e\u7ed1\u5b9a\u8bbe\u7f6e\"\n+sv = \"Tui-kortkommando\"\n+ru = \"\u041f\u0440\u0438\u0432\u044f\u0437\u043a\u0438 \u043a\u043b\u0430\u0432\u0438\u0448 Tui\"\n+es = \"Atajos de teclado Tui\"\n+de = \"Tui-Tastenbelegungen\"\n+\n+[settings_tab_theme_desc]\n+en = \"Tui theme colors\"\n+fr = \"Couleurs du th\u00e8me Tui\"\n+tr = \"Tui tema renkleri\"\n+it = \"Colori del tema Tui\"\n+pt = \"Cores do tema Tui\"\n+zh = \"\u4e3b\u9898\u989c\u8272\u8bbe\u7f6e\"\n+sv = \"Tui-temaf\u00e4rger\"\n+ru = \"\u0426\u0432\u0435\u0442\u0430 \u0442\u0435\u043c\u044b Tui\"\n+es = \"Colores del tema Tui\"\n+de = \"Tui-Themefarben\"\n+\n+[settings_tab_columns_desc]\n+en = \"Tui table columns.  Press [%{c}] to toggle a column on or off and use the [%{d}] and [%{u}] keys to change the column order.\"\n+fr = \"Colonnes de table Tui. Appuyez sur [%{c}] pour activer ou d\u00e9sactiver une colonne et utilisez les touches [%{d}] et [%{u}] pour changer l'ordre des colonnes.\"\n+tr = \"Tui tablo s\u00fctunlar\u0131. Bir s\u00fctunu a\u00e7mak veya kapatmak i\u00e7in [%{c}] tu\u015funa bas\u0131n ve s\u00fctun s\u0131ras\u0131n\u0131 de\u011fi\u015ftirmek i\u00e7in [%{d}] ve [%{u}] tu\u015flar\u0131n\u0131 kullan\u0131n.\"\n+it = \"Colonne della tabella Tui. Premi [%{c}] per attivare o disattivare una colonna e usa i tasti [%{d}] e [%{u}] per cambiare l'ordine delle colonne.\"\n+pt = \"Colunas da tabela Tui. Pressione [%{c}] para ativar ou desativar uma coluna e use as teclas [%{d}] e [%{u}] para alterar a ordem das colunas.\"\n+zh = \"\u7ec8\u7aef\u7528\u6237\u754c\u9762\u8868\u683c\u5217\u3002\u6309 [%{c}] \u5207\u6362\u5217\u7684\u663e\u793a\u548c\u9690\u85cf\uff0c\u4f7f\u7528 [%{d}] \u548c [%{u}] \u952e\u66f4\u6539\u5217\u7684\u987a\u5e8f\u3002\"\n+sv = \"Tui-tabellkolumner. Tryck p\u00e5 [%{c}] f\u00f6r att sl\u00e5 p\u00e5 eller av en kolumn och anv\u00e4nd [%{d}] och [%{u}] f\u00f6r att \u00e4ndra kolumnordningen.\"\n+ru = \"\u0421\u0442\u043e\u043b\u0431\u0446\u044b \u0442\u0430\u0431\u043b\u0438\u0446\u044b Tui. \u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{c}], \u0447\u0442\u043e\u0431\u044b \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0438\u043b\u0438 \u043e\u0442\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0441\u0442\u043e\u043b\u0431\u0435\u0446, \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043a\u043b\u0430\u0432\u0438\u0448\u0438 [%{d}] \u0438 [%{u}], \u0447\u0442\u043e\u0431\u044b \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432.\"\n+es = \"Columnas de la tabla Tui. Presiona [%{c}] para activar o desactivar una columna y usa las teclas [%{d}] y [%{u}] para cambiar el orden de las columnas.\"\n+de = \"Tui-Tabellenspalten. Dr\u00fccken Sie [%{c}], um eine Spalte ein- oder auszuschalten, und verwenden Sie die Tasten [%{d}] und [%{u}], um die Spaltenreihenfolge zu \u00e4ndern.\"\n+\n+[settings_table_header_setting]\n+en = \"Setting\"\n+fr = \"Param\u00e8tres\"\n+tr = \"Ayar\"\n+it = \"Impostazione\"\n+pt = \"Configura\u00e7\u00e3o\"\n+zh = \"\u8bbe\u7f6e\"\n+sv = \"Inst\u00e4llning\"\n+ru = \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\"\n+es = \"Configuraci\u00f3n\"\n+de = \"Einstellung\"\n+\n+[settings_table_header_value]\n+en = \"Value\"\n+fr = \"Valeur\"\n+tr = \"De\u011fer\"\n+it = \"Valore\"\n+pt = \"Valor\"\n+zh = \"\u503c\"\n+sv = \"V\u00e4rde\"\n+ru = \"\u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435\"\n+es = \"Valor\"\n+de = \"Wert\"\n+\n+[column_host]\n+en = \"Host\"\n+fr = \"H\u00f4te\"\n+tr = \"Ana bilgisayar\"\n+it = \"Host\"\n+pt = \"Host\"\n+zh = \"\u4e3b\u673a\"\n+sv = \"V\u00e4rd\"\n+ru = \"\u0425\u043e\u0441\u0442\"\n+es = \"Host\"\n+de = \"Host\"\n+\n+[column_loss_pct]\n+en = \"Loss%\"\n+fr = \"% Perdus\"\n+tr = \"Kay\u0131p%\"\n+it = \"Persi%\"\n+pt = \"% Perdidos\"\n+zh = \"\u4e22\u5305\u7387\"\n+sv = \"F\u00f6rlust%\"\n+ru = \"\u041f\u043e\u0442\u0435\u0440\u0438%\"\n+es = \"% Perdidos\"\n+de = \"Verlust%\"\n+\n+[column_snd]\n+en = \"Snd\"\n+fr = \"Envoy\u00e9s\"\n+tr = \"G\u00f6nderilen\"\n+it = \"Snd\"\n+pt = \"Enviados\"\n+zh = \"\u53d1\u51fa\"\n+sv = \"Skickat\"\n+ru = \"\u041e\u0442\u043f\u0440\"\n+es = \"Enviados\"\n+de = \"Snd\"\n+\n+[column_recv]\n+en = \"Recv\"\n+fr = \"Re\u00e7us\"\n+tr = \"Al\u0131nan\"\n+it = \"Recv\"\n+pt = \"Recebidos\"\n+zh = \"\u63a5\u6536\"\n+sv = \"Mottagna\"\n+ru = \"\u041f\u043e\u043b\u0443\u0447\"\n+es = \"Recibidos\"\n+de = \"Recv\"\n+\n+[column_last]\n+en = \"Last\"\n+fr = \"Dernier\"\n+tr = \"Son\"\n+it = \"Ultimo\"\n+pt = \"\u00daltimo\"\n+zh = \"\u6700\u540e\"\n+sv = \"Senast\"\n+ru = \"\u041f\u043e\u0441\u043b\"\n+es = \"\u00daltimo\"\n+de = \"Letzte\"\n+\n+[column_avg]\n+en = \"Avg\"\n+fr = \"Moyenne\"\n+tr = \"Ort\"\n+it = \"Media\"\n+pt = \"M\u00e9dia\"\n+zh = \"\u5e73\u5747\"\n+sv = \"Genomsnitt\"\n+ru = \"\u0421\u0440\u0435\u0434\"\n+es = \"Prom\"\n+de = \"Durchschnitt\"\n+\n+[column_best]\n+en = \"Best\"\n+fr = \"Meilleur\"\n+tr = \"En iyi\"\n+it = \"Migliore\"\n+pt = \"Melhor\"\n+zh = \"\u6700\u4f73\"\n+sv = \"B\u00e4st\"\n+ru = \"\u041b\u0443\u0447\"\n+es = \"Mejor\"\n+de = \"Beste\"\n+\n+[column_wrst]\n+en = \"Wrst\"\n+fr = \"Pire\"\n+tr = \"En k\u00f6t\u00fc\"\n+it = \"Peggiore\"\n+pt = \"Pior\"\n+zh = \"\u6700\u5dee\"\n+sv = \"S\u00e4mst\"\n+ru = \"\u0425\u0443\u0434\"\n+es = \"Peor\"\n+de = \"Schlechteste\"\n+\n+[column_stdev]\n+en = \"StDev\"\n+fr = \"\u00c9cTyp\"\n+tr = \"StDev\"\n+it = \"StDev\"\n+pt = \"DesvPad\"\n+zh = \"\u6807\u51c6\u5dee\"\n+sv = \"StDev\"\n+ru = \"\u0421\u041a\u041e\"\n+es = \"DesvE\"\n+de = \"StdAbw\"\n+\n+[column_sts]\n+en = \"Sts\"\n+fr = \"\u00c9tat\"\n+tr = \"Sts\"\n+it = \"Stato\"\n+pt = \"Est\"\n+zh = \"\u72b6\u6001\"\n+sv = \"Sts\"\n+ru = \"\u0421\u0442\u0430\u0442\u0443\u0441\"\n+es = \"Est\"\n+de = \"Sts\"\n+\n+[column_jttr]\n+en = \"Jttr\"\n+fr = \"Gigue\"\n+tr = \"Jttr\"\n+it = \"Jttr\"\n+pt = \"Jttr\"\n+zh = \"\u6296\u52a8\"\n+sv = \"Jttr\"\n+ru = \"\u0414\u0436\u0438\u0442\u0440\"\n+es = \"Jttr\"\n+de = \"Jttr\"\n+\n+[column_javg]\n+en = \"Javg\"\n+fr = \"GigMoy\"\n+tr = \"Javg\"\n+it = \"Javg\"\n+pt = \"Javg\"\n+zh = \"\u5747\u6296\"\n+sv = \"Javg\"\n+ru = \"\u0421\u0440\u0435\u0434\u043d\u0414\u0436\u0438\u0442\u0440\"\n+es = \"PromJit\"\n+de = \"Javg\"\n+\n+[column_jmax]\n+en = \"Jmax\"\n+fr = \"GigMax\"\n+tr = \"Jmax\"\n+it = \"Jmax\"\n+pt = \"Jmax\"\n+zh = \"\u6700\u5927\u6296\"\n+sv = \"Jmax\"\n+ru = \"\u041c\u0430\u043a\u0441\u0414\u0436\u0438\u0442\u0440\"\n+es = \"JitMax\"\n+de = \"Jmax\"\n+\n+[column_jint]\n+en = \"Jint\"\n+fr = \"GigInt\"\n+tr = \"Jint\"\n+it = \"Jint\"\n+pt = \"Jint\"\n+zh = \"\u6296\u52a8\u95f4\u9694\"\n+sv = \"Jint\"\n+ru = \"\u0418\u043d\u0442\u0414\u0436\u0438\u0442\u0440\"\n+es = \"JitInt\"\n+de = \"Jint\"\n+\n+[column_sprt]\n+en = \"Sprt\"\n+fr = \"Psrc\"\n+tr = \"Sprt\"\n+it = \"Sprt\"\n+zh = \"\u6e90\u7aef\"\n+sv = \"Sprt\"\n+ru = \"\u0418\u0441\u0445\"\n+es = \"Sprt\"\n+de = \"Sprt\"\n+\n+[column_dprt]\n+en = \"Dprt\"\n+fr = \"Pdest\"\n+tr = \"Dprt\"\n+it = \"Dprt\"\n+pt = \"Dprt\"\n+zh = \"\u76ee\u6807\"\n+sv = \"Dprt\"\n+ru = \"\u041d\u0430\u0437\u043d\"\n+es = \"Dprt\"\n+de = \"Dprt\"\n+\n+[column_seq]\n+en = \"Seq\"\n+fr = \"Seq\"\n+tr = \"Seq\"\n+it = \"Seq\"\n+pt = \"Seq\"\n+zh = \"\u5e8f\u5217\"\n+sv = \"Seq\"\n+ru = \"\u041f\u043e\u0441\u043b\"\n+es = \"Seq\"\n+de = \"Seq\"\n+\n+[column_type]\n+en = \"Type\"\n+fr = \"Type\"\n+tr = \"Type\"\n+it = \"Tipo\"\n+pt = \"Tipo\"\n+zh = \"\u7c7b\u578b\"\n+sv = \"Typ\"\n+ru = \"\u0422\u0438\u043f\"\n+es = \"Tipo\"\n+de = \"Typ\"\n+\n+[column_code]\n+en = \"Code\"\n+fr = \"Code\"\n+tr = \"Code\"\n+it = \"Codice\"\n+pt = \"C\u00f3digo\"\n+zh = \"\u4ee3\u7801\"\n+sv = \"Kod\"\n+ru = \"\u041a\u043e\u0434\"\n+es = \"C\u00f3digo\"\n+de = \"Code\"\n+\n+[column_nat]\n+en = \"Nat\"\n+fr = \"Nat\"\n+tr = \"Nat\"\n+it = \"Nat\"\n+pt = \"Nat\"\n+zh = \"\u7f51\u7edc\u5730\u5740\u8f6c\u6362\"\n+sv = \"Nat\"\n+ru = \"Nat\"\n+es = \"Nat\"\n+de = \"Nat\"\n+\n+[column_fail]\n+en = \"Fail\"\n+fr = \"\u00c9chec\"\n+tr = \"Ba\u015far\u0131s\u0131z\"\n+it = \"Falliti\"\n+pt = \"Falha\"\n+zh = \"\u5931\u8d25\"\n+sv = \"Misslyckades\"\n+ru = \"\u041d\u0435\u0443\u0434\"\n+es = \"Fall\u00f3\"\n+de = \"Fehlgeschlagen\"\n+\n+[column_floss]\n+en = \"Floss\"\n+fr = \"Floss\"\n+tr = \"Floss\"\n+it = \"Floss\"\n+pt = \"Floss\"\n+zh = \"Floss\"\n+sv = \"Floss\"\n+ru = \"Floss\"\n+es = \"Floss\"\n+de = \"Floss\"\n+\n+[column_bloss]\n+en = \"Bloss\"\n+fr = \"Bloss\"\n+tr = \"Bloss\"\n+it = \"Bloss\"\n+pt = \"Bloss\"\n+zh = \"Bloss\"\n+sv = \"Bloss\"\n+ru = \"Bloss\"\n+es = \"Bloss\"\n+de = \"Bloss\"\n+\n+[column_floss_pct]\n+en = \"Floss%\"\n+fr = \"Floss%\"\n+tr = \"Floss%\"\n+it = \"Floss%\"\n+pt = \"Floss%\"\n+zh = \"Floss%\"\n+sv = \"Floss%\"\n+ru = \"Floss%\"\n+es = \"Floss%\"\n+de = \"Floss%\"\ndiff --git a/crates/trippy-tui/locales/app.yml b/crates/trippy-tui/locales/app.yml\ndeleted file mode 100644\nindex 714b4fcb9..000000000\n--- a/crates/trippy-tui/locales/app.yml\n+++ /dev/null\n@@ -1,1199 +0,0 @@\n-_version: 2\n-trippy:\n-  en: \"trippy\"\n-  fr: \"trippy\"\n-  tr: \"trippy\"\n-  it: \"trippy\"\n-  pt: \"trippy\"\n-  zh: \"trippy\"\n-  sv: \"trippy\"\n-  ru: \"trippy\"\n-  es: \"trippy\"\n-  de: \"trippy\"\n-auto:\n-  en: \"auto\"\n-  fr: \"automatique\"\n-  tr: \"otomatik\"\n-  it: \"auto\"\n-  pt: \"autom\u00e1tico\"\n-  zh: \"\u81ea\u52a8\"\n-  sv: \"automatisk\"\n-  ru: \"\u0430\u0432\u0442\u043e\"\n-  es: \"autom\u00e1tico\"\n-  de: \"auto\"\n-on:\n-  en: \"on\"\n-  fr: \"activ\u00e9\"\n-  tr: \"a\u00e7\u0131k\"\n-  it: \"on\"\n-  pt: \"ligado\"\n-  zh: \"\u5f00\"\n-  sv: \"p\u00e5\"\n-  ru: \"\u0432\u043a\u043b\"\n-  es: \"activo\"\n-  de: \"an\"\n-off:\n-  en: \"off\"\n-  fr: \"d\u00e9sactiv\u00e9\"\n-  tr: \"kapal\u0131\"\n-  it: \"off\"\n-  pt: \"desligado\"\n-  zh: \"\u5173\"\n-  sv: \"av\"\n-  ru: \"\u0432\u044b\u043a\u043b\"\n-  es: \"inactivo\"\n-  de: \"aus\"\n-yes:\n-  en: \"Yes\"\n-  fr: \"Oui\"\n-  tr: \"Evet\"\n-  it: \"S\u00ec\"\n-  pt: \"Sim\"\n-  zh: \"\u662f\"\n-  sv: \"Ja\"\n-  ru: \"\u0414\u0430\"\n-  es: \"S\u00ed\"\n-  de: \"Ja\"\n-no:\n-  en: \"No\"\n-  fr: \"Non\"\n-  tr: \"Hay\u0131r\"\n-  it: \"No\"\n-  pt: \"N\u00e3o\"\n-  zh: \"\u5426\"\n-  sv: \"Nej\"\n-  ru: \"\u041d\u0435\u0442\"\n-  es: \"No\"\n-  de: \"Nein\"\n-none:\n-  en: \"none\"\n-  fr: \"aucun\"\n-  tr: \"hi\u00e7biri\"\n-  it: \"nessuno\"\n-  pt: \"nenhum\"\n-  zh: \"\u65e0\"\n-  sv: \"ingen\"\n-  ru: \"\u043d\u0435\u0442\"\n-  es: \"ninguno\"\n-  de: \"keiner\"\n-hidden:\n-  en: \"Hidden\"\n-  fr: \"Cach\u00e9\"\n-  tr: \"Gizli\"\n-  it: \"Nascosto\"\n-  pt: \"Oculto\"\n-  zh: \"\u9690\u85cf\"\n-  sv: \"Dold\"\n-  ru: \"\u0421\u043a\u0440\u044b\u0442\u043e\"\n-  es: \"Oculto\"\n-  de: \"Versteckt\"\n-flow:\n-  en: \"flow\"\n-  fr: \"flux\"\n-  tr: \"ak\u0131\u015f\"\n-  it: \"flusso\"\n-  pt: \"fluxo\"\n-  zh: \"\u6d41\u91cf\"\n-  sv: \"fl\u00f6de\"\n-  ru: \"\u043f\u043e\u0442\u043e\u043a\"\n-  es: \"flujo\"\n-  de: \"fluss\"\n-flows:\n-  en: \"flows\"\n-  fr: \"flux\"\n-  tr: \"ak\u0131\u015flar\"\n-  it: \"flussi\"\n-  pt: \"fluxos\"\n-  zh: \"\u6d41\u91cf\"\n-  sv: \"fl\u00f6den\"\n-  ru: \"\u043f\u043e\u0442\u043e\u043a\u0438\"\n-  es: \"flujos\"\n-  de: \"fl\u00fcsse\"\n-target:\n-  en: \"Target\"\n-  fr: \"Cible\"\n-  tr: \"Hedef\"\n-  it: \"Target\"\n-  pt: \"Alvo\"\n-  zh: \"\u76ee\u6807\"\n-  sv: \"M\u00e5l\"\n-  ru: \"\u0426\u0435\u043b\u044c\"\n-  es: \"Objetivo\"\n-  de: \"Ziel\"\n-status:\n-  en: \"Status\"\n-  fr: \"Statut\"\n-  tr: \"Durum\"\n-  it: \"Stato\"\n-  pt: \"Estado\"\n-  zh: \"\u72b6\u6001\"\n-  sv: \"Status\"\n-  ru: \"\u0421\u0442\u0430\u0442\u0443\u0441\"\n-  es: \"Estado\"\n-  de: \"Status\"\n-details:\n-  en: \"detail\"\n-  fr: \"d\u00e9tail\"\n-  tr: \"ayr\u0131nt\u0131lar\"\n-  it: \"dettagli\"\n-  pt: \"detalhe\"\n-  zh: \"\u8be6\u60c5\"\n-  sv: \"detaljer\"\n-  ru: \"\u0434\u0435\u0442\u0430\u043b\u0438\"\n-  es: \"detalles\"\n-  de: \"detail\"\n-privileged:\n-  en: \"privileged\"\n-  fr: \"privil\u00e9gi\u00e9\"\n-  tr: \"ayr\u0131cal\u0131kl\u0131\"\n-  it: \"privilegiato\"\n-  pt: \"privilegiado\"\n-  zh: \"\u7279\u6743\"\n-  sv: \"privilegierad\"\n-  ru: \"\u043f\u0440\u0438\u0432\u0438\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439\"\n-  es: \"privilegiado\"\n-  de: \"privilegiert\"\n-unprivileged:\n-  en: \"unprivileged\"\n-  fr: \"non privil\u00e9gi\u00e9\"\n-  tr: \"ayr\u0131cal\u0131ks\u0131z\"\n-  it: \"non privilegiato\"\n-  pt: \"n\u00e3o privilegiado\"\n-  zh: \"\u975e\u7279\u6743\"\n-  sv: \"oprivilegierad\"\n-  ru: \"\u043d\u0435\u043f\u0440\u0438\u0432\u0438\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439\"\n-  es: \"no privilegiado\"\n-  de: \"unprivilegiert\"\n-privacy:\n-  en: \"privacy\"\n-  fr: \"confidentialit\u00e9\"\n-  tr: \"gizlilik\"\n-  it: \"privacy\"\n-  pt: \"privacidade\"\n-  zh: \"\u9690\u79c1\"\n-  sv: \"integritet\"\n-  ru: \"\u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\"\n-  es: \"privacidad\"\n-  de: \"datenschutz\"\n-na:\n-  en: \"n/a\"\n-  fr: \"non disponible\"\n-  tr: \"yok\"\n-  it: \"n/d\"\n-  pt: \"n/d\"\n-  zh: \"\u65e0\"\n-  sv: \"ej tillg\u00e4nglig\"\n-  ru: \"\u043d/\u0434\"\n-  es: \"n/d\"\n-  de: \"n/a\"\n-discovered:\n-  en: \"discovered %{hop_count} hops\"\n-  fr: \"%{hop_count} sauts d\u00e9couverts\"\n-  tr: \"%{hop_count} atlanan ke\u015ffedildi\"\n-  it: \"%{hop_count} salti trovati\"\n-  pt: \"descobriu %{hop_count} saltos\"\n-  zh: \"\u53d1\u73b0 %{hop_count} \u8df3\"\n-  sv: \"uppt\u00e4ckte %{hop_count} hopp\"\n-  ru: \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e %{hop_count} \u043f\u0440\u044b\u0436\u043a\u043e\u0432\"\n-  es: \"se descubrieron %{hop_count} saltos\"\n-  de: \"%{hop_count} gefundene hops\"\n-discovered_flows:\n-  en: \"discovered %{hop_count} hops and %{flow_count} unique %{plural_flows}\"\n-  fr: \"%{hop_count} sauts et %{flow_count} %{plural_flows} uniques d\u00e9couverts\"\n-  tr: \"%{hop_count} atlanan ve %{flow_count} benzersiz %{plural_flows} ke\u015ffedildi\"\n-  it: \"scoperti %{hop_count} salti e %{flow_count} %{plural_flows} unici\"\n-  pt: \"descobriu %{hop_count} saltos e %{flow_count} %{plural_flows} \u00fanicos\"\n-  zh: \"\u53d1\u73b0 %{hop_count} \u8df3\uff0c%{flow_count} \u4e2a\u552f\u4e00%{plural_flow}\"\n-  sv: \"%{hop_count} hopp och unika %{flow_count} %{plural_flows} uppt\u00e4ckta\"\n-  ru: \"\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043e %{hop_count} \u043f\u0440\u044b\u0436\u043a\u043e\u0432 \u0438 %{flow_count} \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 %{plural_flows}\"\n-  es: \"se descubrieron %{hop_count} saltos y %{flow_count} \u00fanicos %{plural_flows}\"\n-  de: \"%{hop_count} gefundene hops und %{flow_count} eindeutige %{plural_flows}\"\n-unknown:\n-  en: \"unknown\"\n-  fr: \"inconnu\"\n-  tr: \"bilinmeyen\"\n-  it: \"sconosciuto\"\n-  pt: \"desconhecido\"\n-  zh: \"\u672a\u77e5\"\n-  sv: \"ok\u00e4nd\"\n-  ru: \"\u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\"\n-  es: \"desconocido\"\n-  de: \"unbekannt\"\n-icmp:\n-  en: \"icmp\"\n-  fr: \"icmp\"\n-  tr: \"icmp\"\n-  it: \"icmp\"\n-  pt: \"icmp\"\n-  zh: \"icmp\"\n-  sv: \"icmp\"\n-  ru: \"icmp\"\n-  es: \"icmp\"\n-  de: \"icmp\"\n-udp:\n-  en: \"udp\"\n-  fr: \"udp\"\n-  tr: \"udp\"\n-  it: \"udp\"\n-  pt: \"udp\"\n-  zh: \"udp\"\n-  sv: \"udp\"\n-  ru: \"udp\"\n-  es: \"udp\"\n-  de: \"udp\"\n-tcp:\n-  en: \"tcp\"\n-  fr: \"tcp\"\n-  tr: \"tcp\"\n-  it: \"tcp\"\n-  pt: \"tcp\"\n-  zh: \"tcp\"\n-  sv: \"tcp\"\n-  ru: \"tcp\"\n-  es: \"tcp\"\n-  de: \"tcp\"\n-status_failures:\n-  en: \"%{failure_count} of %{total_probes} (%{failure_rate}%) probes failed\"\n-  fr: \"%{failure_count} sur %{total_probes} (%{failure_rate}%) sondes ont \u00e9chou\u00e9\"\n-  tr: \"%{failure_count} / %{total_probes} (%{failure_rate}%) sondaj\u0131n ba\u015far\u0131s\u0131z olmas\u0131\"\n-  it: \"%{failure_count} di %{total_probes} (%{failure_rate}%) prove fallite\"\n-  pt: \"%{failure_count} de %{total_probes} (%{failure_rate}%) sondas falharam\"\n-  zh: \"%{failure_count} \u4e2a\u5931\u8d25\uff0c\u5171\u63a2\u6d4b\u5230 %{total_probes} \u4e2a\uff08%{failure_rate}%\uff09\"\n-  sv: \"%{failure_count} av %{total_probes} (%{failure_rate}%) misslyckade prober\"\n-  ru: \"%{failure_count} \u0438\u0437 %{total_probes} (%{failure_rate}%) \u0437\u043e\u043d\u0434\u043e\u0432 \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c\"\n-  es: \"%{failure_count} de %{total_probes} (%{failure_rate}%) sondas fallaron\"\n-  de: \"%{failure_count} von %{total_probes} (%{failure_rate}%) sonden sind fehlgeschlagen\"\n-status_failed:\n-  en: \"Failed\"\n-  fr: \"\u00c9chec\"\n-  tr: \"Ba\u015far\u0131s\u0131z\"\n-  it: \"Fallito\"\n-  pt: \"Falhou\"\n-  zh: \"\u5931\u8d25\"\n-  sv: \"Misslyckades\"\n-  ru: \"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c\"\n-  es: \"Fallido\"\n-  de: \"Fehlgeschlagen\"\n-status_running:\n-  en: \"Running\"\n-  fr: \"En cours\"\n-  tr: \"\u00c7al\u0131\u015f\u0131yor\"\n-  it: \"In esecuzione\"\n-  pt: \"Executando\"\n-  zh: \"\u8fd0\u884c\u4e2d\"\n-  sv: \"K\u00f6r\"\n-  ru: \"\u0417\u0430\u043f\u0443\u0449\u0435\u043d\"\n-  es: \"En ejecuci\u00f3n\"\n-  de: \"L\u00e4uft\"\n-status_frozen:\n-  en: \"Frozen\"\n-  fr: \"Gel\u00e9\"\n-  tr: \"Dondurulmu\u015f\"\n-  it: \"Congelato\"\n-  pt: \"Congelado\"\n-  zh: \"\u51bb\u7ed3\"\n-  sv: \"Frusen\"\n-  ru: \"\u0417\u0430\u043c\u043e\u0440\u043e\u0436\u0435\u043d\"\n-  es: \"Congelado\"\n-  de: \"Eingefroren\"\n-awaiting_data:\n-  en: \"Awaiting data...\"\n-  fr: \"En attente de donn\u00e9es...\"\n-  tr: \"Veri bekleniyor...\"\n-  it: \"In attesa di dati...\"\n-  pt: \"Aguardando dados...\"\n-  zh: \"\u7b49\u5f85\u6570\u636e\u2026\u2026\"\n-  sv: \"V\u00e4ntar p\u00e5 data...\"\n-  ru: \"\u041e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445...\"\n-  es: \"Esperando datos...\"\n-  de: \"Warten auf Daten...\"\n-header_help:\n-  en: \"help\"\n-  fr: \"aide\"\n-  tr: \"yard\u0131m\"\n-  it: \"aiuto\"\n-  pt: \"ajuda\"\n-  zh: \"\u5e2e\u52a9\"\n-  sv: \"hj\u00e4lp\"\n-  ru: \"\u043f\u043e\u043c\u043e\u0449\u044c\"\n-  es: \"ayuda\"\n-  de: \"hilfe\"\n-header_settings:\n-  en: \"settings\"\n-  fr: \"param\u00e8tres\"\n-  tr: \"ayarlar\"\n-  it: \"impostazioni\"\n-  pt: \"configura\u00e7\u00f5es\"\n-  zh: \"\u8bbe\u7f6e\"\n-  sv: \"inst\u00e4llningar\"\n-  ru: \"\u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\"\n-  es: \"configuraciones\"\n-  de: \"einstellungen\"\n-header_quit:\n-  en: \"quit\"\n-  fr: \"quitter\"\n-  tr: \"c\u0131k\u0131\u015f\"\n-  it: \"uscita\"\n-  pt: \"sair\"\n-  zh: \"\u9000\u51fa\"\n-  sv: \"avsluta\"\n-  ru: \"\u0412\u044b\u0445\u043e\u0434\"\n-  es: \"salir\"\n-  de: \"beenden\"\n-title_hops:\n-  en: \"Hops\"\n-  fr: \"Sauts\"\n-  tr: \"Atlananlar\"\n-  it: \"Salti\"\n-  pt: \"Saltos\"\n-  zh: \"\u8df3\"\n-  sv: \"Hopp\"\n-  ru: \"\u041f\u0440\u044b\u0436\u043a\u0438\"\n-  es: \"Saltos\"\n-  de: \"Hops\"\n-title_frequency:\n-  en: \"Frequency\"\n-  fr: \"Fr\u00e9quence\"\n-  tr: \"S\u0131kl\u0131k\"\n-  it: \"Frequenza\"\n-  pt: \"Frequ\u00eancia\"\n-  zh: \"\u9891\u7387\"\n-  sv: \"Frekvens\"\n-  ru: \"\u0427\u0430\u0441\u0442\u043e\u0442\u0430\"\n-  es: \"Frecuencia\"\n-  de: \"Frequenz\"\n-title_samples:\n-  en: \"Samples\"\n-  fr: \"\u00c9chantillons\"\n-  tr: \"\u00d6rnekler\"\n-  it: \"Campioni\"\n-  pt: \"Amostras\"\n-  zh: \"\u6837\u672c\"\n-  sv: \"Prover\"\n-  ru: \"\u041e\u0431\u0440\u0430\u0437\u0446\u044b\"\n-  es: \"Muestras\"\n-  de: \"Proben\"\n-title_traces:\n-  en: \"Traces\"\n-  fr: \"Traces\"\n-  tr: \"\u0130zler\"\n-  it: \"Tracce\"\n-  pt: \"Rastreios\"\n-  zh: \"\u8ddf\u8e2a\"\n-  sv: \"Sp\u00e5r\"\n-  ru: \"\u0421\u043b\u0435\u0434\u044b\"\n-  es: \"Trazas\"\n-  de: \"Spuren\"\n-title_flows:\n-  en: \"Flows\"\n-  fr: \"Flux\"\n-  tr: \"Ak\u0131\u015flar\"\n-  it: \"Flussi\"\n-  pt: \"Fluxos\"\n-  zh: \"\u6d41\u91cf\"\n-  sv: \"Fl\u00f6den\"\n-  ru: \"\u041f\u043e\u0442\u043e\u043a\u0438\"\n-  es: \"Flujos\"\n-  de: \"Fl\u00fcsse\"\n-title_map:\n-  en: \"Map\"\n-  fr: \"Carte\"\n-  tr: \"Harita\"\n-  it: \"Mappa\"\n-  pt: \"Mapa\"\n-  zh: \"\u5730\u56fe\"\n-  sv: \"Karta\"\n-  ru: \"\u041a\u0430\u0440\u0442\u0430\"\n-  es: \"Mapa\"\n-  de: \"Karte\"\n-title_help:\n-  en: \"Help\"\n-  fr: \"Aide\"\n-  tr: \"Yard\u0131m\"\n-  it: \"Aiuto\"\n-  pt: \"Ajuda\"\n-  zh: \"\u5e2e\u52a9\"\n-  sv: \"Hj\u00e4lp\"\n-  ru: \"\u041f\u043e\u043c\u043e\u0449\u044c\"\n-  es: \"Ayuda\"\n-  de: \"Hilfe\"\n-title_settings:\n-  en: \"Settings\"\n-  fr: \"Param\u00e8tres\"\n-  tr: \"Ayarlar\"\n-  it: \"Impostazioni\"\n-  pt: \"Configura\u00e7\u00f5es\"\n-  zh: \"\u8bbe\u7f6e\"\n-  sv: \"Inst\u00e4llningar\"\n-  ru: \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\"\n-  es: \"Configuraciones\"\n-  de: \"Einstellungen\"\n-bsod_failed:\n-  en: \"Trippy Failed :(\"\n-  fr: \"Trippy a \u00e9chou\u00e9 :(\"\n-  tr: \"Trippy Ba\u015far\u0131s\u0131z :(\"\n-  it: \"Trippy ha avuto un problema :(\"\n-  pt: \"Trippy falhou :(\"\n-  zh: \"Trippy \u5931\u8d25 :(\"\n-  sv: \"Trippy misslyckades :(\"\n-  ru: \"Trippy \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c :(\"\n-  es: \"Trippy tuvo un problema :(\"\n-  de: \"Trippy ist fehlgeschlagen :(\"\n-bsod_quit:\n-  en: \"Press q to quit\"\n-  fr: \"Appuyez sur q pour quitter\"\n-  tr: \"\u00c7\u0131kmak i\u00e7in q tu\u015funa bas\u0131n\"\n-  it: \"Premi q per uscire\"\n-  pt: \"Pressione q para sair\"\n-  zh: \"\u6309 q \u9000\u51fa\"\n-  sv: \"Tryck p\u00e5 q f\u00f6r att avsluta\"\n-  ru: \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 q \u0434\u043b\u044f \u0432\u044b\u0445\u043e\u0434\u0430\"\n-  es: \"Presiona q para salir\"\n-  de: \"Dr\u00fccken Sie q, um zu beenden\"\n-hop:\n-  en: \"Hop\"\n-  fr: \"Saut\"\n-  tr: \"Atla\"\n-  it: \"Salto\"\n-  pt: \"Salto\"\n-  zh: \"\u8df3\"\n-  sv: \"Hopp\"\n-  ru: \"\u041f\u0440\u044b\u0436\u043e\u043a\"\n-  es: \"Salto\"\n-  de: \"Hop\"\n-rtt:\n-  en: \"RTT\"\n-  fr: \"RTT\"\n-  tr: \"RTT\"\n-  it: \"RTT\"\n-  pt: \"RTT\"\n-  zh: \"\u5f80\u8fd4\u65f6\u95f4\"\n-  sv: \"RTT\"\n-  ru: \"RTT\"\n-  es: \"RTT\"\n-  de: \"RTT\"\n-title_chart:\n-  en: \"Chart\"\n-  fr: \"Graphique\"\n-  tr: \"Grafik\"\n-  it: \"Grafico\"\n-  pt: \"Gr\u00e1fico\"\n-  zh: \"\u56fe\u8868\"\n-  sv: \"Diagram\"\n-  ru: \"\u0414\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430\"\n-  es: \"Gr\u00e1fico\"\n-  de: \"Diagramm\"\n-samples:\n-  en: \"Samples\"\n-  fr: \"\u00c9chantillons\"\n-  tr: \"\u00d6rnekler\"\n-  it: \"Campioni\"\n-  pt: \"Amostras\"\n-  zh: \"\u6837\u672c\"\n-  sv: \"Prover\"\n-  ru: \"\u041e\u0431\u0440\u0430\u0437\u0446\u044b\"\n-  es: \"Muestras\"\n-  de: \"Proben\"\n-host:\n-  en: \"Host\"\n-  fr: \"H\u00f4te\"\n-  tr: \"Ana bilgisayar\"\n-  it: \"Host\"\n-  pt: \"Host\"\n-  zh: \"\u4e3b\u673a\"\n-  sv: \"V\u00e4rd\"\n-  ru: \"\u0425\u043e\u0441\u0442\"\n-  es: \"Host\"\n-  de: \"Host\"\n-no_response:\n-  en: \"No response\"\n-  fr: \"Pas de r\u00e9ponse\"\n-  tr: \"Yan\u0131t yok\"\n-  it: \"Nessuna risposta\"\n-  pt: \"Sem resposta\"\n-  zh: \"\u65e0\u54cd\u5e94\"\n-  sv: \"Inget svar\"\n-  ru: \"\u041d\u0435\u0442 \u043e\u0442\u0432\u0435\u0442\u0430\"\n-  es: \"Sin respuesta\"\n-  de: \"Keine Antwort\"\n-dns_failed:\n-  en: \"Failed\"\n-  fr: \"\u00c9chec\"\n-  tr: \"Ba\u015far\u0131s\u0131z\"\n-  it: \"Fallito\"\n-  pt: \"Falhou\"\n-  zh: \"\u5931\u8d25\"\n-  sv: \"Misslyckades\"\n-  ru: \"\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c\"\n-  es: \"Fallido\"\n-  de: \"Fehlgeschlagen\"\n-dns_timeout:\n-  en: \"Timeout\"\n-  fr: \"D\u00e9lai d\u00e9pass\u00e9\"\n-  tr: \"Zaman a\u015f\u0131m\u0131\"\n-  it: \"Tempo scaduto\"\n-  pt: \"Tempo esgotado\"\n-  zh: \"\u8d85\u65f6\"\n-  sv: \"Tidsgr\u00e4ns n\u00e5dd\"\n-  ru: \"\u0422\u0430\u0439\u043c-\u0430\u0443\u0442\"\n-  es: \"Tiempo de espera\"\n-  de: \"Zeit\u00fcberschreitung\"\n-labels:\n-  en: \"labels\"\n-  fr: \"\u00e9tiquettes\"\n-  tr: \"etiketler\"\n-  it: \"etichette\"\n-  pt: \"etiquetas\"\n-  zh: \"\u6807\u7b7e\"\n-  sv: \"etiketter\"\n-  ru: \"\u043c\u0435\u0442\u043a\u0438\"\n-  es: \"etiquetas\"\n-  de: \"etiketten\"\n-not_enabled:\n-  en: \"not enabled\"\n-  fr: \"non activ\u00e9\"\n-  tr: \"etkin de\u011fil\"\n-  it: \"non abilitato\"\n-  pt: \"n\u00e3o ativado\"\n-  zh: \"\u672a\u542f\u7528\"\n-  sv: \"ej aktiverad\"\n-  ru: \"\u043d\u0435 \u0430\u043a\u0442\u0438\u0432\u0438\u0440\u043e\u0432\u0430\u043d\"\n-  es: \"no habilitado\"\n-  de: \"nicht aktiviert\"\n-not_found:\n-  en: \"not found\"\n-  fr: \"non trouv\u00e9\"\n-  tr: \"bulunamad\u0131\"\n-  it: \"non trovato\"\n-  pt: \"n\u00e3o encontrado\"\n-  zh: \"\u672a\u627e\u5230\"\n-  sv: \"hittades inte\"\n-  ru: \"\u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e\"\n-  es: \"no encontrado\"\n-  de: \"nicht gefunden\"\n-awaited:\n-  en: \"awaited\"\n-  fr: \"attendu\"\n-  tr: \"beklenen\"\n-  it: \"atteso\"\n-  pt: \"aguardado\"\n-  zh: \"\u7b49\u5f85\"\n-  sv: \"v\u00e4ntade\"\n-  ru: \"\u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u0439\"\n-  es: \"esperado\"\n-  de: \"erwartet\"\n-name:\n-  en: \"Name\"\n-  fr: \"Nom\"\n-  tr: \"Ad\"\n-  it: \"Nome\"\n-  pt: \"Nome\"\n-  zh: \"\u540d\u79f0\"\n-  sv: \"Namn\"\n-  ru: \"\u0418\u043c\u044f\"\n-  es: \"Nombre\"\n-  de: \"Name\"\n-info:\n-  en: \"Info\"\n-  fr: \"Information\"\n-  tr: \"Bilgi\"\n-  it: \"Info\"\n-  pt: \"Informa\u00e7\u00e3o\"\n-  zh: \"\u4fe1\u606f\"\n-  sv: \"Information\"\n-  ru: \"\u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\"\n-  es: \"Informaci\u00f3n\"\n-  de: \"Info\"\n-geo:\n-  en: \"Geo\"\n-  fr: \"G\u00e9o\"\n-  tr: \"Co\u011frafi\"\n-  it: \"Geo\"\n-  pt: \"Geo\"\n-  zh: \"\u5730\u7406\u5750\u6807\"\n-  sv: \"Geo\"\n-  ru: \"\u0413\u0435\u043e\"\n-  es: \"Geo\"\n-  de: \"Geo\"\n-pos:\n-  en: \"Pos\"\n-  fr: \"Pos\"\n-  tr: \"Poz\"\n-  it: \"Pos\"\n-  pt: \"Pos\"\n-  zh: \"\u4f4d\u7f6e\"\n-  sv: \"Pos\"\n-  ru: \"\u041f\u043e\u0437\"\n-  es: \"Pos\"\n-  de: \"Pos\"\n-ext:\n-  en: \"Ext\"\n-  fr: \"Ext\"\n-  tr: \"Uzant\u0131\"\n-  it: \"Est\"\n-  pt: \"Ext\"\n-  zh: \"\u6269\u5c55\"\n-  sv: \"Ext\"\n-  ru: \"\u0420\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u0435\"\n-  es: \"Ext\"\n-  de: \"Ext\"\n-help_tagline:\n-  en: \"A network diagnostic tool\"\n-  fr: \"Un outil de diagnostic r\u00e9seau\"\n-  tr: \"Bir a\u011f analiz arac\u0131\"\n-  it: \"Uno strumento diagnostico di rete\"\n-  pt: \"Uma ferramenta de diagn\u00f3stico de rede\"\n-  zh: \"\u7f51\u7edc\u8bca\u65ad\u5de5\u5177\"\n-  sv: \"Ett n\u00e4tverksdiagnostikverktyg\"\n-  ru: \"\u0418\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u043a\u0438 \u0441\u0435\u0442\u0438\"\n-  es: \"Una herramienta de diagn\u00f3stico de red\"\n-  de: \"Ein Netzwerkdiagnosetool\"\n-help_show_settings:\n-  en: \"Press [%{key}] to show all settings\"\n-  fr: \"Appuyez sur [%{key}] pour afficher tous les param\u00e8tres\"\n-  tr: \"T\u00fcm ayarlar\u0131 g\u00f6rmek i\u00e7in [%{key}] tu\u015funa bas\u0131n\"\n-  it: \"Premi [%{key}] per visualizzare tutte le impostazioni\"\n-  pt: \"Pressione [%{key}] para mostrar todas as configura\u00e7\u00f5es\"\n-  zh: \"\u6309 [%{key}] \u663e\u793a\u6240\u6709\u8bbe\u7f6e\"\n-  sv: \"Tryck p\u00e5 [%{key}] f\u00f6r att visa alla inst\u00e4llningar\"\n-  ru: \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{key}], \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\"\n-  es: \"Presiona [%{key}] para mostrar todas las configuraciones\"\n-  de: \"Dr\u00fccken Sie [%{key}], um alle Einstellungen anzuzeigen\"\n-help_show_bindings:\n-  en: \"Press [%{key}] to show all bindings\"\n-  fr: \"Appuyez sur [%{key}] pour afficher tous les raccourcis clavier\"\n-  tr: \"T\u00fcm ba\u011flant\u0131lar\u0131 g\u00f6rmek i\u00e7in [%{key}] tu\u015funa bas\u0131n\"\n-  it: \"Premi [%{key}] per visualizzare tutti i collegamenti\"\n-  pt: \"Pressione [%{key}] para mostrar todos os atalhos\"\n-  zh: \"\u6309 [%{key}] \u663e\u793a\u6240\u6709\u7ed1\u5b9a\"\n-  sv: \"Tryck p\u00e5 [%{key}] f\u00f6r att visa alla kortkommando\"\n-  ru: \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{key}], \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u043f\u0440\u0438\u0432\u044f\u0437\u043a\u0438\"\n-  es: \"Presiona [%{key}] para mostrar todos los atajos\"\n-  de: \"Dr\u00fccken Sie [%{key}], um alle Tastenbelegungen anzuzeigen\"\n-help_show_columns:\n-  en: \"Press [%{key}] to show all columns\"\n-  fr: \"Appuyez sur [%{key}] pour afficher toutes les colonnes\"\n-  tr: \"T\u00fcm s\u00fctunlar\u0131 g\u00f6rmek i\u00e7in [%{key}] tu\u015funa bas\u0131n\"\n-  it: \"Premi [%{key}] per visualizzare tutte le colonne\"\n-  pt: \"Pressione [%{key}] para mostrar todas as colunas\"\n-  zh: \"\u6309 [%{key}] \u663e\u793a\u6240\u6709\u5217\"\n-  sv: \"Tryck p\u00e5 [%{key}] f\u00f6r att visa alla kolumner\"\n-  ru: \"\u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{key}], \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0441\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\"\n-  es: \"Presiona [%{key}] para mostrar todas las columnas\"\n-  de: \"Dr\u00fccken Sie [%{key}], um alle Spalten anzuzeigen\"\n-help_license:\n-  en: \"Distributed under the Apache License 2.0\"\n-  fr: \"Distribu\u00e9 sous licence Apache 2.0\"\n-  tr: \"Apache Lisans\u0131 2.0 alt\u0131nda da\u011f\u0131t\u0131lm\u0131\u015ft\u0131r\"\n-  it: \"Distribuito con licenza Apache 2.0\"\n-  pt: \"Distribu\u00eddo sob a licen\u00e7a Apache 2.0\"\n-  zh: \"\u4ee5 Apache-2.0 \u8bb8\u53ef\u5206\u53d1\"\n-  sv: \"Distribueras under Apache License 2.0\"\n-  ru: \"\u0420\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u043e\u0434 \u043b\u0438\u0446\u0435\u043d\u0437\u0438\u0435\u0439 Apache 2.0\"\n-  es: \"Distribuido bajo la Licencia Apache 2.0\"\n-  de: \"Verteilt unter der Apache-Lizenz 2.0\"\n-help_copyright:\n-  en: \"Copyright 2022 Trippy Contributors\"\n-  fr: \"Copyright 2022 Contributeurs de Trippy\"\n-  tr: \"Telif Hakk\u0131 2022 - Trippy'ye Katk\u0131da Bulunanlar\"\n-  it: \"Copyright 2022 - Collaboratori di Trippy\"\n-  pt: \"Direitos autorais 2022 Colaboradores do Trippy\"\n-  zh: \"\u7248\u6743\u6240\u6709 2022 Trippy \u8d21\u732e\u8005\"\n-  sv: \"Upphovsr\u00e4tt 2022 Trippy-bidragsgivare\"\n-  ru: \"\u0410\u0432\u0442\u043e\u0440\u0441\u043a\u043e\u0435 \u043f\u0440\u0430\u0432\u043e 2022 \u0423\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0438 Trippy\"\n-  es: \"Derechos de autor 2022 Contribuidores de Trippy\"\n-  de: \"Copyright 2022 Trippy Mitwirkende\"\n-geoip_not_enabled:\n-  en: \"GeoIp not enabled\"\n-  fr: \"GeoIp non activ\u00e9\"\n-  tr: \"GeoIp etkin de\u011fil\"\n-  it: \"GeoIp non abilitato\"\n-  pt: \"GeoIp n\u00e3o ativado\"\n-  zh: \"GeoIp \u672a\u542f\u7528\"\n-  sv: \"GeoIp inte aktiverad\"\n-  ru: \"GeoIp \u043d\u0435 \u0430\u043a\u0442\u0438\u0432\u0438\u0440\u043e\u0432\u0430\u043d\"\n-  es: \"GeoIp no habilitado\"\n-  de: \"GeoIp nicht aktiviert\"\n-geoip_no_data_for_hop:\n-  en: \"No GeoIp data for hop\"\n-  fr: \"Pas de donn\u00e9es GeoIp pour le saut\"\n-  tr: \"Hop i\u00e7in GeoIp verisi yok\"\n-  it: \"Nessun dato GeoIp per il salto\"\n-  pt: \"Nenhum dado GeoIp para o salto\"\n-  zh: \"\u65e0 GeoIp \u6570\u636e\"\n-  sv: \"Inga GeoIp-data f\u00f6r hopp\"\n-  ru: \"\u041d\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0445 GeoIp \u0434\u043b\u044f \u043f\u0440\u044b\u0436\u043a\u0430\"\n-  es: \"No hay datos de GeoIp para el salto\"\n-  de: \"Keine GeoIp-Daten f\u00fcr den Hop\"\n-geoip_multiple_data_for_hop:\n-  en: \"Multiple GeoIp locations for hop\"\n-  fr: \"Emplacements GeoIp multiples pour le saut\"\n-  tr: \"Hop i\u00e7in birden fazla GeoIp konumu\"\n-  it: \"Posizioni GeoIp multiple per il salto\"\n-  pt: \"M\u00faltiplas localiza\u00e7\u00f5es GeoIp para o salto\"\n-  zh: \"\u591a\u4e2a GeoIp \u4f4d\u7f6e\"\n-  sv: \"Flera GeoIp-platser f\u00f6r hopp\"\n-  ru: \"\u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043c\u0435\u0441\u0442\u043e\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0439 GeoIp \u0434\u043b\u044f \u043f\u0440\u044b\u0436\u043a\u0430\"\n-  es: \"M\u00faltiples ubicaciones de GeoIp para el salto\"\n-  de: \"Mehrere GeoIp-Standorte f\u00fcr den Hop\"\n-kilometer:\n-  en: \"km\"\n-  fr: \"km\"\n-  tr: \"km\"\n-  it: \"km\"\n-  pt: \"km\"\n-  zh: \"\u516c\u91cc\"\n-  sv: \"km\"\n-  ru: \"\u043a\u043c\"\n-  es: \"km\"\n-  de: \"km\"\n-settings_info:\n-  en: \"Info\"\n-  fr: \"Information\"\n-  tr: \"Bilgi\"\n-  it: \"Info\"\n-  pt: \"Informa\u00e7\u00e3o\"\n-  zh: \"\u4fe1\u606f\"\n-  sv: \"Information\"\n-  ru: \"\u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\"\n-  es: \"Informaci\u00f3n\"\n-  de: \"Info\"\n-settings_tab_tui_title:\n-  en: \"Tui\"\n-  fr: \"Tui\"\n-  tr: \"Tui\"\n-  it: \"Tui\"\n-  pt: \"Tui\"\n-  zh: \"\u7ec8\u7aef\u7528\u6237\u754c\u9762\"\n-  sv: \"Tui\"\n-  ru: \"Tui\"\n-  es: \"Tui\"\n-  de: \"Tui\"\n-settings_tab_trace_title:\n-  en: \"Trace\"\n-  fr: \"Tracer\"\n-  tr: \"\u0130z\"\n-  it: \"Traccia\"\n-  pt: \"Rastrear\"\n-  zh: \"\u8ddf\u8e2a\"\n-  sv: \"Sp\u00e5ra\"\n-  ru: \"\u0421\u043b\u0435\u0434\"\n-  es: \"Rastrear\"\n-  de: \"Trace\"\n-settings_tab_dns_title:\n-  en: \"DNS\"\n-  fr: \"DNS\"\n-  tr: \"DNS\"\n-  it: \"DNS\"\n-  pt: \"DNS\"\n-  zh: \"DNS\"\n-  sv: \"DNS\"\n-  ru: \"DNS\"\n-  es: \"DNS\"\n-  de: \"DNS\"\n-settings_tab_geoip_title:\n-  en: \"GeoIp\"\n-  fr: \"GeoIp\"\n-  tr: \"GeoIp\"\n-  it: \"GeoIp\"\n-  pt: \"GeoIp\"\n-  zh: \"GeoIp\"\n-  sv: \"GeoIp\"\n-  ru: \"GeoIp\"\n-  es: \"GeoIp\"\n-  de: \"GeoIp\"\n-settings_tab_bindings_title:\n-  en: \"Bindings\"\n-  fr: \"Raccourcis clavier\"\n-  tr: \"Ba\u011flant\u0131lar\"\n-  it: \"Collegamenti\"\n-  pt: \"Atalhos\"\n-  zh: \"\u7ed1\u5b9a\"\n-  sv: \"Kortkommando\"\n-  ru: \"\u041f\u0440\u0438\u0432\u044f\u0437\u043a\u0438\"\n-  es: \"Atajos\"\n-  de: \"Tastenbelegungen\"\n-settings_tab_theme_title:\n-  en: \"Theme\"\n-  fr: \"Th\u00e8me\"\n-  tr: \"Tema\"\n-  it: \"Tema\"\n-  pt: \"Tema\"\n-  zh: \"\u4e3b\u9898\"\n-  sv: \"Tema\"\n-  ru: \"\u0422\u0435\u043c\u0430\"\n-  es: \"Tema\"\n-  de: \"Darstellung\"\n-settings_tab_columns_title:\n-  en: \"Columns\"\n-  fr: \"Colonnes\"\n-  tr: \"S\u00fctunlar\"\n-  it: \"Colonne\"\n-  pt: \"Colunas\"\n-  zh: \"\u5217\"\n-  es: \"Columnas\"\n-  sv: \"Kolumner\"\n-  ru: \"\u0421\u0442\u043e\u043b\u0431\u0446\u044b\"\n-  de: \"Spalten\"\n-settings_tab_tui_desc:\n-  en: \"Settings which control how data is displayed in this Tui\"\n-  fr: \"Param\u00e8tres qui contr\u00f4lent la fa\u00e7on dont les donn\u00e9es sont affich\u00e9es dans ce Tui\"\n-  tr: \"Aray\u00fczde verilerin nas\u0131l g\u00f6r\u00fcnt\u00fclendi\u011fini kontrol eden ayarlar\"\n-  it: \"Impostazioni che controllano come i dati vengono visualizzati in questo Tui\"\n-  pt: \"Configura\u00e7\u00f5es que controlam como os dados s\u00e3o exibidos neste Tui\"\n-  zh: \"\u6570\u636e\u663e\u793a\u65b9\u5f0f\u8bbe\u7f6e\"\n-  sv: \"Inst\u00e4llningar som styr hur data visas i detta Tui\"\n-  ru: \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442, \u043a\u0430\u043a \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0442\u0441\u044f \u0432 \u044d\u0442\u043e\u043c Tui\"\n-  es: \"Configuraciones que controlan c\u00f3mo se muestran los datos en este Tui\"\n-  de: \"Einstellungen, die steuern, wie Daten in diesem Tui angezeigt werden\"\n-settings_tab_trace_desc:\n-  en: \"Settings which control the tracing strategy\"\n-  fr: \"Param\u00e8tres qui contr\u00f4lent la strat\u00e9gie de tra\u00e7age\"\n-  tr: \"\u0130zleme stratejisini kontrol eden ayarlar\"\n-  it: \"Impostazioni che controllano la strategia di tracciamento\"\n-  pt: \"Configura\u00e7\u00f5es que controlam a estrat\u00e9gia de rastreamento\"\n-  zh: \"\u8ddf\u8e2a\u7b56\u7565\u8bbe\u7f6e\"\n-  sv: \"Inst\u00e4llningar som styr sp\u00e5rningsstrategin\"\n-  ru: \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044e \u0442\u0440\u0430\u0441\u0441\u0438\u0440\u043e\u0432\u043a\u0438\"\n-  es: \"Configuraciones que controlan la estrategia de rastreo\"\n-  de: \"Einstellungen, die die Tracing-Strategie steuern\"\n-settings_tab_dns_desc:\n-  en: \"Settings which control how DNS lookups are performed\"\n-  fr: \"Param\u00e8tres qui contr\u00f4lent la fa\u00e7on dont les recherches DNS sont effectu\u00e9es\"\n-  tr: \"DNS aramalar\u0131n\u0131n nas\u0131l yap\u0131ld\u0131\u011f\u0131n\u0131 kontrol eden ayarlar\"\n-  it: \"Impostazioni che controllano come vengono eseguite le ricerche DNS\"\n-  pt: \"Configura\u00e7\u00f5es que controlam como as pesquisas DNS s\u00e3o realizadas\"\n-  zh: \"DNS \u67e5\u8be2\u8bbe\u7f6e\"\n-  sv: \"Inst\u00e4llningar som styr hur DNS-uppslag utf\u00f6rs\"\n-  ru: \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442, \u043a\u0430\u043a \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u044e\u0442\u0441\u044f DNS-\u0437\u0430\u043f\u0440\u043e\u0441\u044b\"\n-  es: \"Configuraciones que controlan c\u00f3mo se realizan las b\u00fasquedas de DNS\"\n-  de: \"Einstellungen, die steuern, wie DNS-Lookups durchgef\u00fchrt werden\"\n-settings_tab_geoip_desc:\n-  en: \"Settings relating to GeoIp\"\n-  fr: \"Param\u00e8tres relatifs \u00e0 GeoIp\"\n-  tr: \"GeoIp ile ilgili ayarlar\"\n-  it: \"Impostazioni relative a GeoIp\"\n-  pt: \"Configura\u00e7\u00f5es relacionadas ao GeoIp\"\n-  zh: \"GeoIp \u8bbe\u7f6e\"\n-  sv: \"Inst\u00e4llningar som r\u00f6r GeoIp\"\n-  ru: \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043a\u0430\u0441\u0430\u044e\u0449\u0438\u0435\u0441\u044f GeoIp\"\n-  es: \"Configuraciones relacionadas con GeoIp\"\n-  de: \"Einstellungen im Zusammenhang mit GeoIp\"\n-settings_tab_bindings_desc:\n-  en: \"Tui key bindings\"\n-  fr: \"Raccourcis clavier Tui\"\n-  tr: \"Tui tu\u015f ayarlar\u0131\"\n-  it: \"Collegamenti chiave Tui\"\n-  pt: \"Atalhos de teclado Tui\"\n-  zh: \"\u6309\u952e\u7ed1\u5b9a\u8bbe\u7f6e\"\n-  sv: \"Tui-kortkommando\"\n-  ru: \"\u041f\u0440\u0438\u0432\u044f\u0437\u043a\u0438 \u043a\u043b\u0430\u0432\u0438\u0448 Tui\"\n-  es: \"Atajos de teclado Tui\"\n-  de: \"Tui-Tastenbelegungen\"\n-settings_tab_theme_desc:\n-  en: \"Tui theme colors\"\n-  fr: \"Couleurs du th\u00e8me Tui\"\n-  tr: \"Tui tema renkleri\"\n-  it: \"Colori del tema Tui\"\n-  pt: \"Cores do tema Tui\"\n-  zh: \"\u4e3b\u9898\u989c\u8272\u8bbe\u7f6e\"\n-  sv: \"Tui-temaf\u00e4rger\"\n-  ru: \"\u0426\u0432\u0435\u0442\u0430 \u0442\u0435\u043c\u044b Tui\"\n-  es: \"Colores del tema Tui\"\n-  de: \"Tui-Themefarben\"\n-settings_tab_columns_desc:\n-  en: \"Tui table columns.  Press [%{c}] to toggle a column on or off and use the [%{d}] and [%{u}] keys to change the column order.\"\n-  fr: \"Colonnes de table Tui. Appuyez sur [%{c}] pour activer ou d\u00e9sactiver une colonne et utilisez les touches [%{d}] et [%{u}] pour changer l'ordre des colonnes.\"\n-  tr: \"Tui tablo s\u00fctunlar\u0131. Bir s\u00fctunu a\u00e7mak veya kapatmak i\u00e7in [%{c}] tu\u015funa bas\u0131n ve s\u00fctun s\u0131ras\u0131n\u0131 de\u011fi\u015ftirmek i\u00e7in [%{d}] ve [%{u}] tu\u015flar\u0131n\u0131 kullan\u0131n.\"\n-  it: \"Colonne della tabella Tui. Premi [%{c}] per attivare o disattivare una colonna e usa i tasti [%{d}] e [%{u}] per cambiare l'ordine delle colonne.\"\n-  pt: \"Colunas da tabela Tui. Pressione [%{c}] para ativar ou desativar uma coluna e use as teclas [%{d}] e [%{u}] para alterar a ordem das colunas.\"\n-  zh: \"\u7ec8\u7aef\u7528\u6237\u754c\u9762\u8868\u683c\u5217\u3002\u6309 [%{c}] \u5207\u6362\u5217\u7684\u663e\u793a\u548c\u9690\u85cf\uff0c\u4f7f\u7528 [%{d}] \u548c [%{u}] \u952e\u66f4\u6539\u5217\u7684\u987a\u5e8f\u3002\"\n-  sv: \"Tui-tabellkolumner. Tryck p\u00e5 [%{c}] f\u00f6r att sl\u00e5 p\u00e5 eller av en kolumn och anv\u00e4nd [%{d}] och [%{u}] f\u00f6r att \u00e4ndra kolumnordningen.\"\n-  ru: \"\u0421\u0442\u043e\u043b\u0431\u0446\u044b \u0442\u0430\u0431\u043b\u0438\u0446\u044b Tui. \u041d\u0430\u0436\u043c\u0438\u0442\u0435 [%{c}], \u0447\u0442\u043e\u0431\u044b \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0438\u043b\u0438 \u043e\u0442\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0441\u0442\u043e\u043b\u0431\u0435\u0446, \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043a\u043b\u0430\u0432\u0438\u0448\u0438 [%{d}] \u0438 [%{u}], \u0447\u0442\u043e\u0431\u044b \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432.\"\n-  es: \"Columnas de la tabla Tui. Presiona [%{c}] para activar o desactivar una columna y usa las teclas [%{d}] y [%{u}] para cambiar el orden de las columnas.\"\n-  de: \"Tui-Tabellenspalten. Dr\u00fccken Sie [%{c}], um eine Spalte ein- oder auszuschalten, und verwenden Sie die Tasten [%{d}] und [%{u}], um die Spaltenreihenfolge zu \u00e4ndern.\"\n-settings_table_header_setting:\n-  en: \"Setting\"\n-  fr: \"Param\u00e8tres\"\n-  tr: \"Ayar\"\n-  it: \"Impostazione\"\n-  pt: \"Configura\u00e7\u00e3o\"\n-  zh: \"\u8bbe\u7f6e\"\n-  sv: \"Inst\u00e4llning\"\n-  ru: \"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\"\n-  es: \"Configuraci\u00f3n\"\n-  de: \"Einstellung\"\n-settings_table_header_value:\n-  en: \"Value\"\n-  fr: \"Valeur\"\n-  tr: \"De\u011fer\"\n-  it: \"Valore\"\n-  pt: \"Valor\"\n-  zh: \"\u503c\"\n-  sv: \"V\u00e4rde\"\n-  ru: \"\u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435\"\n-  es: \"Valor\"\n-  de: \"Wert\"\n-column_host:\n-  en: \"Host\"\n-  fr: \"H\u00f4te\"\n-  tr: \"Ana bilgisayar\"\n-  it: \"Host\"\n-  pt: \"Host\"\n-  zh: \"\u4e3b\u673a\"\n-  sv: \"V\u00e4rd\"\n-  ru: \"\u0425\u043e\u0441\u0442\"\n-  es: \"Host\"\n-  de: \"Host\"\n-column_loss_pct:\n-  en: \"Loss%\"\n-  fr: \"% Perdus\"\n-  tr: \"Kay\u0131p%\"\n-  it: \"Persi%\"\n-  pt: \"% Perdidos\"\n-  zh: \"\u4e22\u5305\u7387\"\n-  sv: \"F\u00f6rlust%\"\n-  ru: \"\u041f\u043e\u0442\u0435\u0440\u0438%\"\n-  es: \"% Perdidos\"\n-  de: \"Verlust%\"\n-column_snd:\n-  en: \"Snd\"\n-  fr: \"Envoy\u00e9s\"\n-  tr: \"G\u00f6nderilen\"\n-  it: \"Snd\"\n-  pt: \"Enviados\"\n-  zh: \"\u53d1\u51fa\"\n-  sv: \"Skickat\"\n-  ru: \"\u041e\u0442\u043f\u0440\"\n-  es: \"Enviados\"\n-  de: \"Snd\"\n-column_recv:\n-  en: \"Recv\"\n-  fr: \"Re\u00e7us\"\n-  tr: \"Al\u0131nan\"\n-  it: \"Recv\"\n-  pt: \"Recebidos\"\n-  zh: \"\u63a5\u6536\"\n-  sv: \"Mottagna\"\n-  ru: \"\u041f\u043e\u043b\u0443\u0447\"\n-  es: \"Recibidos\"\n-  de: \"Recv\"\n-column_last:\n-  en: \"Last\"\n-  fr: \"Dernier\"\n-  tr: \"Son\"\n-  it: \"Ultimo\"\n-  pt: \"\u00daltimo\"\n-  zh: \"\u6700\u540e\"\n-  sv: \"Senast\"\n-  ru: \"\u041f\u043e\u0441\u043b\"\n-  es: \"\u00daltimo\"\n-  de: \"Letzte\"\n-column_avg:\n-  en: \"Avg\"\n-  fr: \"Moyenne\"\n-  tr: \"Ort\"\n-  it: \"Media\"\n-  pt: \"M\u00e9dia\"\n-  zh: \"\u5e73\u5747\"\n-  sv: \"Genomsnitt\"\n-  ru: \"\u0421\u0440\u0435\u0434\"\n-  es: \"Prom\"\n-  de: \"Durchschnitt\"\n-column_best:\n-  en: \"Best\"\n-  fr: \"Meilleur\"\n-  tr: \"En iyi\"\n-  it: \"Migliore\"\n-  pt: \"Melhor\"\n-  zh: \"\u6700\u4f73\"\n-  sv: \"B\u00e4st\"\n-  ru: \"\u041b\u0443\u0447\"\n-  es: \"Mejor\"\n-  de: \"Beste\"\n-column_wrst:\n-  en: \"Wrst\"\n-  fr: \"Pire\"\n-  tr: \"En k\u00f6t\u00fc\"\n-  it: \"Peggiore\"\n-  pt: \"Pior\"\n-  zh: \"\u6700\u5dee\"\n-  sv: \"S\u00e4mst\"\n-  ru: \"\u0425\u0443\u0434\"\n-  es: \"Peor\"\n-  de: \"Schlechteste\"\n-column_stdev:\n-  en: \"StDev\"\n-  fr: \"\u00c9cTyp\"\n-  tr: \"StDev\"\n-  it: \"StDev\"\n-  pt: \"DesvPad\"\n-  zh: \"\u6807\u51c6\u5dee\"\n-  sv: \"StDev\"\n-  ru: \"\u0421\u041a\u041e\"\n-  es: \"DesvE\"\n-  de: \"StdAbw\"\n-column_sts:\n-  en: \"Sts\"\n-  fr: \"\u00c9tat\"\n-  tr: \"Sts\"\n-  it: \"Stato\"\n-  pt: \"Est\"\n-  zh: \"\u72b6\u6001\"\n-  sv: \"Sts\"\n-  ru: \"\u0421\u0442\u0430\u0442\u0443\u0441\"\n-  es: \"Est\"\n-  de: \"Sts\"\n-column_jttr:\n-  en: \"Jttr\"\n-  fr: \"Gigue\"\n-  tr: \"Jttr\"\n-  it: \"Jttr\"\n-  pt: \"Jttr\"\n-  zh: \"\u6296\u52a8\"\n-  sv: \"Jttr\"\n-  ru: \"\u0414\u0436\u0438\u0442\u0440\"\n-  es: \"Jttr\"\n-  de: \"Jttr\"\n-column_javg:\n-  en: \"Javg\"\n-  fr: \"GigMoy\"\n-  tr: \"Javg\"\n-  it: \"Javg\"\n-  pt: \"Javg\"\n-  zh: \"\u5747\u6296\"\n-  sv: \"Javg\"\n-  ru: \"\u0421\u0440\u0435\u0434\u043d\u0414\u0436\u0438\u0442\u0440\"\n-  es: \"PromJit\"\n-  de: \"Javg\"\n-column_jmax:\n-  en: \"Jmax\"\n-  fr: \"GigMax\"\n-  tr: \"Jmax\"\n-  it: \"Jmax\"\n-  pt: \"Jmax\"\n-  zh: \"\u6700\u5927\u6296\"\n-  sv: \"Jmax\"\n-  ru: \"\u041c\u0430\u043a\u0441\u0414\u0436\u0438\u0442\u0440\"\n-  es: \"JitMax\"\n-  de: \"Jmax\"\n-column_jint:\n-  en: \"Jint\"\n-  fr: \"GigInt\"\n-  tr: \"Jint\"\n-  it: \"Jint\"\n-  pt: \"Jint\"\n-  zh: \"\u6296\u52a8\u95f4\u9694\"\n-  sv: \"Jint\"\n-  ru: \"\u0418\u043d\u0442\u0414\u0436\u0438\u0442\u0440\"\n-  es: \"JitInt\"\n-  de: \"Jint\"\n-column_sprt:\n-  en: \"Sprt\"\n-  fr: \"Psrc\"\n-  tr: \"Sprt\"\n-  it: \"Sprt\"\n-  zh: \"\u6e90\u7aef\"\n-  sv: \"Sprt\"\n-  ru: \"\u0418\u0441\u0445\"\n-  es: \"Sprt\"\n-  de: \"Sprt\"\n-column_dprt:\n-  en: \"Dprt\"\n-  fr: \"Pdest\"\n-  tr: \"Dprt\"\n-  it: \"Dprt\"\n-  pt: \"Dprt\"\n-  zh: \"\u76ee\u6807\"\n-  sv: \"Dprt\"\n-  ru: \"\u041d\u0430\u0437\u043d\"\n-  es: \"Dprt\"\n-  de: \"Dprt\"\n-column_seq:\n-  en: \"Seq\"\n-  fr: \"Seq\"\n-  tr: \"Seq\"\n-  it: \"Seq\"\n-  pt: \"Seq\"\n-  zh: \"\u5e8f\u5217\"\n-  sv: \"Seq\"\n-  ru: \"\u041f\u043e\u0441\u043b\"\n-  es: \"Seq\"\n-  de: \"Seq\"\n-column_type:\n-  en: \"Type\"\n-  fr: \"Type\"\n-  tr: \"Type\"\n-  it: \"Tipo\"\n-  pt: \"Tipo\"\n-  zh: \"\u7c7b\u578b\"\n-  sv: \"Typ\"\n-  ru: \"\u0422\u0438\u043f\"\n-  es: \"Tipo\"\n-  de: \"Typ\"\n-column_code:\n-  en: \"Code\"\n-  fr: \"Code\"\n-  tr: \"Code\"\n-  it: \"Codice\"\n-  pt: \"C\u00f3digo\"\n-  zh: \"\u4ee3\u7801\"\n-  sv: \"Kod\"\n-  ru: \"\u041a\u043e\u0434\"\n-  es: \"C\u00f3digo\"\n-  de: \"Code\"\n-column_nat:\n-  en: \"Nat\"\n-  fr: \"Nat\"\n-  tr: \"Nat\"\n-  it: \"Nat\"\n-  pt: \"Nat\"\n-  zh: \"\u7f51\u7edc\u5730\u5740\u8f6c\u6362\"\n-  sv: \"Nat\"\n-  ru: \"Nat\"\n-  es: \"Nat\"\n-  de: \"Nat\"\n-column_fail:\n-  en: \"Fail\"\n-  fr: \"\u00c9chec\"\n-  tr: \"Ba\u015far\u0131s\u0131z\"\n-  it: \"Falliti\"\n-  pt: \"Falha\"\n-  zh: \"\u5931\u8d25\"\n-  sv: \"Misslyckades\"\n-  ru: \"\u041d\u0435\u0443\u0434\"\n-  es: \"Fall\u00f3\"\n-  de: \"Fehlgeschlagen\"\n-column_floss:\n-  en: \"Floss\"\n-  fr: \"Floss\"\n-  tr: \"Floss\"\n-  it: \"Floss\"\n-  pt: \"Floss\"\n-  zh: \"Floss\"\n-  sv: \"Floss\"\n-  ru: \"Floss\"\n-  es: \"Floss\"\n-  de: \"Floss\"\n-column_bloss:\n-  en: \"Bloss\"\n-  fr: \"Bloss\"\n-  tr: \"Bloss\"\n-  it: \"Bloss\"\n-  pt: \"Bloss\"\n-  zh: \"Bloss\"\n-  sv: \"Bloss\"\n-  ru: \"Bloss\"\n-  es: \"Bloss\"\n-  de: \"Bloss\"\n-column_floss_pct:\n-  en: \"Floss%\"\n-  fr: \"Floss%\"\n-  tr: \"Floss%\"\n-  it: \"Floss%\"\n-  pt: \"Floss%\"\n-  zh: \"Floss%\"\n-  sv: \"Floss%\"\n-  ru: \"Floss%\"\n-  es: \"Floss%\"\n-  de: \"Floss%\"\n\\ No newline at end of file\n", "instance_id": "fujiapple852__trippy-1424", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to replace the use of `serde_yml` with TOML due to maintainability and dependency concerns. It provides a specific example of how YAML test data can be translated to TOML, which helps in understanding the expected format. Additionally, it mentions the usage of YAML in i18n strings and suggests an alternative library (`i18n-embed`) for handling translations, though it notes this might be too big a change. However, there are minor ambiguities and missing details. For instance, it does not fully specify how to handle YAML-specific features like tags (e.g., `!SingleHost`) beyond a vague suggestion of using optional keys in TOML with additional processing. The scope of the i18n replacement is left unclear, as it is presented as optional and potentially out of scope. Constraints or potential challenges in converting all test data or ensuring compatibility are not explicitly addressed. Overall, while the goal and primary tasks are clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes involves multiple files and modules, including updating dependencies in `Cargo.toml`, modifying test data file formats from YAML to TOML, and adjusting the deserialization logic in the codebase (e.g., changing `serde_yml::from_str` to `toml::from_str` in `state.rs`). While the provided diff shows a moderate amount of code change, the actual implementation may require converting numerous test files, as hinted by the \"almost all usage in test data\" statement. Second, it requires understanding technical concepts such as Rust's serialization/deserialization frameworks (`serde`, `toml`), dependency management with Cargo, and potentially handling data format nuances (e.g., YAML tags to TOML equivalents). Third, there are potential edge cases to consider, such as ensuring all existing YAML structures map correctly to TOML, handling any custom parsing logic, and verifying that test behaviors remain unchanged after the conversion. However, the problem does not appear to impact the core architecture of the system significantly, as it primarily affects test data and possibly i18n configurations rather than runtime logic. The optional i18n replacement with `i18n-embed` adds complexity but is not mandatory per the statement. Overall, this task requires a moderate level of understanding and effort across several files but does not demand deep architectural changes or advanced domain-specific knowledge, placing it at 0.45 on the difficulty scale.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "unbuffered: After `Closed` no `WriteTraffic` state arrives\n**Checklist**\r\n* [x] I've searched the issue tracker for similar bugs.\r\n\r\n**Describe the bug**\r\nI've updated my `pipebuf` crate Rustls wrapper in crate `pipebuf_rustls` to add unbuffered Rustls support. However my tests which succeed in the buffered mode fail for the unbuffered mode. The failing test does the following:\r\n\r\n- Client sends one byte\r\n- Client closes connection\r\n- Server sends one byte\r\n- Server closes connection\r\n\r\nThe byte sent by the server cannot be processed through the unbuffered interface because the `process_tls_records` call does not give a `WriteTraffic` state again after `Closed`. Without access to `WriteTraffic`, I can't send the data. The buffered interface does not have this limitation.\r\n\r\n**To Reproduce**\r\nCheck out `pipebuf_rustls` from github (https://github.com/uazu/pipebuf_rustls).  Run a single test using `cargo test --no-default-features --features unbuffered byte_each_way`. There is an assertion failure due to 1 byte sent but 0 bytes received.\r\n\r\n**Applicable Version(s)**\r\n0.23.4 on Linux\r\n\r\n**Expected behavior**\r\nI'd expect this case to be supported. I don't know how you'd want to support it with your unbuffered API though.\r\n\r\n**Additional context**\r\nThe reason I added unbuffered support to `pipebuf_rustls` was that this is a much better fit to the `pipebuf` model. I added `PBufRd::data_mut` to `pipebuf` 0.3.1 to support Rustls unbuffered mode (which requires `&mut [u8]` for incoming data). So I support the unbuffered effort. Just it doesn't cover all necessary cases yet.\n", "patch": "diff --git a/rustls/src/common_state.rs b/rustls/src/common_state.rs\nindex 3ff4609df8c..12aca203a4b 100644\n--- a/rustls/src/common_state.rs\n+++ b/rustls/src/common_state.rs\n@@ -40,6 +40,8 @@ pub struct CommonState {\n     pub(crate) may_receive_application_data: bool,\n     pub(crate) early_traffic: bool,\n     sent_fatal_alert: bool,\n+    /// If we signaled end of stream.\n+    pub(crate) has_sent_close_notify: bool,\n     /// If the peer has signaled end of stream.\n     pub(crate) has_received_close_notify: bool,\n     #[cfg(feature = \"std\")]\n@@ -74,6 +76,7 @@ impl CommonState {\n             may_receive_application_data: false,\n             early_traffic: false,\n             sent_fatal_alert: false,\n+            has_sent_close_notify: false,\n             has_received_close_notify: false,\n             #[cfg(feature = \"std\")]\n             has_seen_eof: false,\n@@ -573,6 +576,7 @@ impl CommonState {\n         }\n         debug!(\"Sending warning alert {:?}\", AlertDescription::CloseNotify);\n         self.sent_fatal_alert = true;\n+        self.has_sent_close_notify = true;\n         self.send_warning_alert_no_log(AlertDescription::CloseNotify);\n     }\n \ndiff --git a/rustls/src/conn.rs b/rustls/src/conn.rs\nindex 43706241452..426d3770cce 100644\n--- a/rustls/src/conn.rs\n+++ b/rustls/src/conn.rs\n@@ -811,6 +811,7 @@ impl<Data> From<ConnectionCore<Data>> for ConnectionCommon<Data> {\n pub struct UnbufferedConnectionCommon<Data> {\n     pub(crate) core: ConnectionCore<Data>,\n     wants_write: bool,\n+    emitted_peer_closed_state: bool,\n }\n \n impl<Data> From<ConnectionCore<Data>> for UnbufferedConnectionCommon<Data> {\n@@ -818,6 +819,7 @@ impl<Data> From<ConnectionCore<Data>> for UnbufferedConnectionCommon<Data> {\n         Self {\n             core,\n             wants_write: false,\n+            emitted_peer_closed_state: false,\n         }\n     }\n }\ndiff --git a/rustls/src/conn/unbuffered.rs b/rustls/src/conn/unbuffered.rs\nindex ef8146e098e..ffe14c75f7d 100644\n--- a/rustls/src/conn/unbuffered.rs\n+++ b/rustls/src/conn/unbuffered.rs\n@@ -131,6 +131,18 @@ impl<Data> UnbufferedConnectionCommon<Data> {\n                 .core\n                 .common_state\n                 .has_received_close_notify\n+                && !self.emitted_peer_closed_state\n+            {\n+                self.emitted_peer_closed_state = true;\n+                break (buffer.pending_discard(), ConnectionState::PeerClosed);\n+            } else if self\n+                .core\n+                .common_state\n+                .has_received_close_notify\n+                && self\n+                    .core\n+                    .common_state\n+                    .has_sent_close_notify\n             {\n                 break (buffer.pending_discard(), ConnectionState::Closed);\n             } else if self\n@@ -185,7 +197,26 @@ pub enum ConnectionState<'c, 'i, Data> {\n     /// the received data.\n     ReadTraffic(ReadTraffic<'c, 'i, Data>),\n \n-    /// Connection has been cleanly closed by the peer\n+    /// Connection has been cleanly closed by the peer.\n+    ///\n+    /// This state is encountered at most once by each connection -- it is\n+    /// \"edge\" triggered, rather than \"level\" triggered.\n+    ///\n+    /// It delimits the data received from the peer, meaning you can be sure you\n+    /// have received all the data the peer sent.\n+    ///\n+    /// No further application data will be received from the peer, so no further\n+    /// `ReadTraffic` states will be produced.\n+    ///\n+    /// However, it is possible to _send_ further application data via `WriteTraffic`\n+    /// states, or close the connection cleanly by calling\n+    /// [`WriteTraffic::queue_close_notify()`].\n+    PeerClosed,\n+\n+    /// Connection has been cleanly closed by both us and the peer.\n+    ///\n+    /// This is a terminal state.  No other states will be produced for this\n+    /// connection.\n     Closed,\n \n     /// One, or more, early (RTT-0) data records are available\n@@ -262,6 +293,8 @@ impl<Data> fmt::Debug for ConnectionState<'_, '_, Data> {\n         match self {\n             Self::ReadTraffic(..) => f.debug_tuple(\"ReadTraffic\").finish(),\n \n+            Self::PeerClosed => write!(f, \"PeerClosed\"),\n+\n             Self::Closed => write!(f, \"Closed\"),\n \n             Self::ReadEarlyData(..) => f.debug_tuple(\"ReadEarlyData\").finish(),\n", "instance_id": "rustls__rustls-2332", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the unbuffered mode of the Rustls wrapper in the `pipebuf_rustls` crate. It outlines the specific test case that fails (client sends a byte, closes connection, server sends a byte, closes connection) and identifies the core issue: the inability to process a `WriteTraffic` state after a `Closed` state in unbuffered mode. The expected behavior is mentioned (support for this case), and steps to reproduce are provided, including a specific test command. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what the \"unbuffered mode\" entails compared to the buffered mode in terms of API or behavior, which could be critical for someone unfamiliar with the codebase. Additionally, while the issue is described, there are no detailed examples of input/output or constraints that might affect the solution. Edge cases beyond the specific test scenario are not mentioned, leaving some room for interpretation about the broader implications of the fix.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes, while not extensive in terms of lines of code, impacts critical parts of the Rustls library, specifically the state management in `common_state.rs` and `conn/unbuffered.rs`. The modifications involve understanding and altering the connection state logic, which is central to the library's functionality, indicating a moderate impact on the system's architecture. Second, the technical concepts required include a deep understanding of Rustls's unbuffered API, TLS connection states, and state transition logic, which are non-trivial and require familiarity with low-level networking and security protocols. Third, the problem necessitates handling specific edge cases related to connection closure (e.g., ensuring the server can send data after the client closes the connection but before the server closes), which adds complexity to the state machine logic. The code changes introduce new states like `PeerClosed` and track additional flags (`has_sent_close_notify`, `emitted_peer_closed_state`), requiring careful consideration to avoid breaking existing behavior. While the problem does not involve a massive codebase overhaul or advanced domain-specific knowledge beyond TLS, it demands a solid grasp of the Rustls internals and precise modifications to ensure correctness, justifying a score of 0.65. This places it slightly above the medium difficulty range due to the specialized knowledge and potential for subtle bugs in state handling.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Preview text too dark\nThe text is quite a bit too dark to be readable in \"preview\" mode, i.e. when tabbing through keys:\r\n\r\n<img width=\"2519\" alt=\"image\" src=\"https://github.com/ynqa/jnv/assets/25161793/7673d5f9-b6c5-463b-b5d1-aeddd9cbb402\">\r\n\r\nI don't have any visual impairment so if I struggle it's likely impossible to read for people who don't have perfect vision.\r\n\r\nIn particular the dim blue can't be read against the black background.\r\n\r\nIt seems that your terminal shows the blue a bit differently:\r\n![image](https://github.com/ynqa/jnv/assets/25161793/efcf4cd4-5b33-4047-a0d7-8fcbad3742c7)\n", "patch": "diff --git a/src/jnv.rs b/src/jnv.rs\nindex a55956a..645838d 100644\n--- a/src/jnv.rs\n+++ b/src/jnv.rs\n@@ -74,8 +74,8 @@ impl Jnv {\n                 history: Default::default(),\n                 prefix: String::from(\"\u276f\u276f \"),\n                 mask: Default::default(),\n-                prefix_style: StyleBuilder::new().fgc(Color::DarkCyan).build(),\n-                active_char_style: StyleBuilder::new().bgc(Color::DarkMagenta).build(),\n+                prefix_style: StyleBuilder::new().fgc(Color::Blue).build(),\n+                active_char_style: StyleBuilder::new().bgc(Color::Magenta).build(),\n                 inactive_char_style: StyleBuilder::new().build(),\n                 edit_mode,\n                 lines: Default::default(),\n@@ -83,7 +83,7 @@ impl Jnv {\n             hint_message_renderer: text::Renderer {\n                 text: Default::default(),\n                 style: StyleBuilder::new()\n-                    .fgc(Color::DarkGreen)\n+                    .fgc(Color::Green)\n                     .attrs(Attributes::from(Attribute::Bold))\n                     .build(),\n             },\n@@ -92,10 +92,10 @@ impl Jnv {\n                 listbox: listbox::Listbox::from_iter(Vec::<String>::new()),\n                 cursor: String::from(\"\u276f \"),\n                 active_item_style: StyleBuilder::new()\n-                    .fgc(Color::DarkGrey)\n-                    .bgc(Color::DarkYellow)\n+                    .fgc(Color::Grey)\n+                    .bgc(Color::Yellow)\n                     .build(),\n-                inactive_item_style: StyleBuilder::new().fgc(Color::DarkGrey).build(),\n+                inactive_item_style: StyleBuilder::new().fgc(Color::Grey).build(),\n                 lines: Some(suggestion_list_length),\n             },\n             keymap: KeymapManager::new(\"default\", self::keymap::default)\n@@ -121,11 +121,11 @@ impl Jnv {\n                     square_brackets_style: StyleBuilder::new()\n                         .attrs(Attributes::from(Attribute::Bold))\n                         .build(),\n-                    key_style: StyleBuilder::new().fgc(Color::DarkBlue).build(),\n-                    string_value_style: StyleBuilder::new().fgc(Color::DarkGreen).build(),\n+                    key_style: StyleBuilder::new().fgc(Color::Cyan).build(),\n+                    string_value_style: StyleBuilder::new().fgc(Color::Green).build(),\n                     number_value_style: StyleBuilder::new().build(),\n                     boolean_value_style: StyleBuilder::new().build(),\n-                    null_value_style: StyleBuilder::new().fgc(Color::DarkGrey).build(),\n+                    null_value_style: StyleBuilder::new().fgc(Color::Grey).build(),\n                     active_item_attribute: Attribute::Bold,\n                     inactive_item_attribute: Attribute::Dim,\n                     lines: Default::default(),\n@@ -207,7 +207,7 @@ impl Jnv {\n                                     renderer.hint_message_snapshot.after_mut().replace(text::Renderer {\n                                         text: format!(\"JSON query ('{}') was executed, but no results were returned.\", &completed),\n                                         style: StyleBuilder::new()\n-                                            .fgc(Color::DarkRed)\n+                                            .fgc(Color::Red)\n                                             .attrs(Attributes::from(Attribute::Bold))\n                                             .build(),\n                                     });\n@@ -228,7 +228,7 @@ impl Jnv {\n                                                     &completed,\n                                                 ),\n                                                 style: StyleBuilder::new()\n-                                                    .fgc(Color::DarkYellow)\n+                                                    .fgc(Color::Yellow)\n                                                     .attrs(Attributes::from(Attribute::Bold))\n                                                     .build(),\n                                             });\n@@ -250,7 +250,7 @@ impl Jnv {\n                                                 e\n                                             ),\n                                             style: StyleBuilder::new()\n-                                                .fgc(Color::DarkRed)\n+                                                .fgc(Color::Red)\n                                                 .attrs(Attributes::from(Attribute::Bold))\n                                                 .build(),\n                                         })\n@@ -266,7 +266,7 @@ impl Jnv {\n                                 renderer.hint_message_snapshot.after_mut().replace(text::Renderer {\n                                     text: format!(\"Failed to execute jq query '{}'\", &completed),\n                                     style: StyleBuilder::new()\n-                                        .fgc(Color::DarkRed)\n+                                        .fgc(Color::Red)\n                                         .attrs(Attributes::from(Attribute::Bold))\n                                         .build(),\n                                     },\n", "instance_id": "ynqa__jnv-4", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the text color in \"preview\" mode of the application is too dark to be readable, particularly for users with visual impairments. The inclusion of images to illustrate the problem adds to the clarity by visually demonstrating the issue with the color contrast. However, the statement lacks specific details about the expected outcome beyond \"improving readability.\" For instance, it does not specify which colors should be used or provide guidelines for acceptable contrast ratios. Additionally, there are no mentions of edge cases, such as how the color changes should behave across different terminal environments or themes. While the goal is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The code changes provided are straightforward and involve only modifying color values in a single file (`jnv.rs`). The scope of the change is minimal, focusing on updating style attributes (e.g., changing `DarkCyan` to `Blue`, `DarkMagenta` to `Magenta`, etc.) for various UI elements to improve visibility. No deep understanding of the codebase architecture or complex logic is required; the task is essentially a cosmetic adjustment. The technical concepts involved are basic, limited to familiarity with Rust syntax and a specific UI rendering library's style-building API (likely a crate like `tui` or similar). There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as the modifications are purely stylistic and unlikely to introduce functional issues. Overall, this is a very easy task that requires minimal effort and expertise.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "System::refresh_processes() doesn't update CPU\n### My setup\r\n\r\nOS:                  WSL, Ubuntu 24.04 LTS\r\nArchitecture:   x86_64\r\nRust:                1.81.0\r\nSysinfo:            0.31.4\r\n\r\n### Issue \r\n\r\nWhen I call `System::refresh_processes_specifics(ProcessesToUpdate::Some(&[pid], ProcessRefreshKind::new().with_cpu().with_memory())` the cpu usage of provided process is not updated. Calling `Process::cpu_usage()` always returns the same value.\r\n\r\n#### Expected behavious\r\n\r\nCPU usage is updated and can be read from `Process` struct.\r\n\r\n#### Actuall result\r\n\r\nOnly memory gets updated, CPU value always stays the same.\r\n\r\nWhat is interesting, when I replaced `ProcessesToUpdate::Some(...)` with `ProcessesToUpdate::All`, all of a sudden it is updating correctly.\r\n\r\n### Code to reproduce issue\r\n\r\n```rust\r\nuse std::{thread, time::Duration};\r\n\r\nuse sysinfo::{get_current_pid, ProcessRefreshKind, System};\r\n\r\n\r\nfn main() {\r\n    let mut system = System::new_all();\r\n    loop {\r\n        thread::sleep(Duration::from_secs(1));\r\n        let pid = get_current_pid().unwrap();\r\n        system.refresh_processes_specifics(\r\n            sysinfo::ProcessesToUpdate::Some(&[pid]),\r\n            ProcessRefreshKind::everything()\r\n        );\r\n        let process = system.process(pid).unwrap();\r\n\r\n        println!(\"cpu={}, mem={}\", process.cpu_usage(), process.memory());\r\n    }\r\n}\r\n```\r\n![image](https://github.com/user-attachments/assets/adc70ac8-cf57-44f6-8c9e-7401b0238f3f)\r\n\r\n`top -p <pid>` returns CPU usage from 0.0 to 0.3, to the value should fluctuate a bit, but it always remains zero.\n", "patch": "diff --git a/src/unix/linux/cpu.rs b/src/unix/linux/cpu.rs\nindex dfee6b241..5e8d55d30 100644\n--- a/src/unix/linux/cpu.rs\n+++ b/src/unix/linux/cpu.rs\n@@ -19,11 +19,6 @@ macro_rules! to_str {\n pub(crate) struct CpusWrapper {\n     pub(crate) global_cpu: CpuUsage,\n     pub(crate) cpus: Vec<Cpu>,\n-    /// Field set to `false` in `update_cpus` and to `true` in `refresh_processes_specifics`.\n-    ///\n-    /// The reason behind this is to avoid calling the `update_cpus` more than necessary.\n-    /// For example when running `refresh_all` or `refresh_specifics`.\n-    need_cpus_update: bool,\n     got_cpu_frequency: bool,\n     /// This field is needed to prevent updating when not enough time passed since last update.\n     last_update: Option<Instant>,\n@@ -34,7 +29,6 @@ impl CpusWrapper {\n         Self {\n             global_cpu: CpuUsage::default(),\n             cpus: Vec::with_capacity(4),\n-            need_cpus_update: true,\n             got_cpu_frequency: false,\n             last_update: None,\n         }\n@@ -45,9 +39,7 @@ impl CpusWrapper {\n         only_update_global_cpu: bool,\n         refresh_kind: CpuRefreshKind,\n     ) {\n-        if self.need_cpus_update {\n-            self.refresh(only_update_global_cpu, refresh_kind);\n-        }\n+        self.refresh(only_update_global_cpu, refresh_kind);\n     }\n \n     pub(crate) fn refresh(&mut self, only_update_global_cpu: bool, refresh_kind: CpuRefreshKind) {\n@@ -76,7 +68,6 @@ impl CpusWrapper {\n             };\n             let buf = BufReader::new(f);\n \n-            self.need_cpus_update = false;\n             let mut i: usize = 0;\n             let mut it = buf.split(b'\\n');\n \n@@ -193,10 +184,6 @@ impl CpusWrapper {\n     pub(crate) fn is_empty(&self) -> bool {\n         self.cpus.is_empty()\n     }\n-\n-    pub(crate) fn set_need_cpus_update(&mut self) {\n-        self.need_cpus_update = true;\n-    }\n }\n \n /// Struct containing values to compute a CPU usage.\ndiff --git a/src/unix/linux/system.rs b/src/unix/linux/system.rs\nindex 33001e095..fbe89e593 100644\n--- a/src/unix/linux/system.rs\n+++ b/src/unix/linux/system.rs\n@@ -276,10 +276,7 @@ impl SystemInner {\n             processes_to_update,\n             refresh_kind,\n         );\n-        if matches!(processes_to_update, ProcessesToUpdate::All) {\n-            self.update_procs_cpu(refresh_kind);\n-            self.cpus.set_need_cpus_update();\n-        }\n+        self.update_procs_cpu(refresh_kind);\n         nb_updated\n     }\n \n", "instance_id": "GuillaumeGomez__sysinfo-1365", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the CPU usage of a specific process does not update when using `System::refresh_processes_specifics` with `ProcessesToUpdate::Some`, while it works correctly with `ProcessesToUpdate::All`. The goal (updating CPU usage for a specific process) and the actual vs. expected behavior are well-articulated. A reproducible code snippet and environment details (OS, Rust version, library version) are provided, which adds to the clarity. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases (e.g., behavior on different Linux kernels or with multiple processes) or constraints (e.g., performance expectations or system limitations). Additionally, the root cause or expected fix is not hypothesized, leaving some room for interpretation. Overall, it is clear enough to understand the issue but lacks exhaustive detail on edge cases or broader implications.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, affecting only two files (`cpu.rs` and `system.rs`) in the `sysinfo` library, with a small number of lines modified (removing a conditional update mechanism for CPU usage). However, understanding the issue requires a moderate grasp of the library's internal logic, specifically how CPU usage is calculated and updated for processes under different refresh modes (`Some` vs. `All`). The technical concepts involved include Rust's ownership and borrowing model, system-level programming on Linux (reading CPU stats, likely from `/proc`), and the interplay between global CPU stats and per-process stats. The fix itself\u2014removing a flag (`need_cpus_update`) and ensuring CPU updates occur regardless of the refresh mode\u2014suggests a bug in the optimization logic, which requires understanding the original intent behind the flag. Edge cases are not explicitly mentioned in the problem statement, but the fix could potentially introduce performance regressions (e.g., unnecessary CPU updates for unrelated processes) or issues with partial refreshes, which would need to be considered. The changes do not significantly impact the broader architecture but do require careful testing to ensure correctness across different refresh scenarios. Overall, this problem requires a moderate level of expertise in Rust and system programming, along with some debugging and testing effort, placing it in the medium difficulty range (0.4-0.6), slightly above the midpoint due to the need for domain-specific knowledge of Linux system metrics.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "libbpf-cargo: Build fails when a C struct contains an empty union\nI am using `libbpf-cargo` version `0.24.6`. I'm trying to use `struct bio` like this in my `.bpf.c`:\r\n\r\n```c\r\nSEC(\"fentry/submit_bio\")\r\nint BPF_PROG(submit_bio, struct bio *bio)\r\n{\r\n...\r\n\tvcnt = bio->bi_vcnt;\r\n...\r\n}\r\n```\r\n\r\nHowever `cargo build` fails with:\r\n\r\n```\r\n  thread 'main' panicked at /home/jrc/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libbpf-cargo-0.24.6/src/gen/btf.rs:769:49:\r\n  index out of bounds: the len is 0 but the index is 0\r\n```\r\n\r\nThe error refers to [this line](https://github.com/libbpf/libbpf-rs/blob/f73d75c02e4f8fc33dc4809b3b69f4a660c0f90a/libbpf-cargo/src/gen/btf.rs#L769), which is part of the code to \"write a Default implementation for a union\".\r\n\r\nAt [vmlinux_601.h#L7735](https://github.com/libbpf/vmlinux.h/blob/172793d6a409d98d1cfb843c80df73733e9f832f/include/x86/vmlinux_601.h#L7735), we find this:\r\n\r\n```c\r\nstruct bio {\r\n...\r\n\tunion {};\r\n...\r\n};\r\n```\r\n\r\nIt looks like empty unions cause problems for `libbpf-cargo`. Arguably this is an issue of `vmlinux.h`, and I am aware that the `vmlinux.h` repo is advertised as \"Development-only\". This issue could be fixed by removing the empty union in `vmlinux.h`. However, it seems more convenient to me to add support for empty unions to `libbpf-cargo`. Is that reasonable?\n", "patch": "diff --git a/libbpf-cargo/CHANGELOG.md b/libbpf-cargo/CHANGELOG.md\nindex 0c1634a2..acefe2b2 100644\n--- a/libbpf-cargo/CHANGELOG.md\n+++ b/libbpf-cargo/CHANGELOG.md\n@@ -1,3 +1,8 @@\n+Unreleased\n+----------\n+- Fixed handling of empty unions in BPF types\n+\n+\n 0.24.6\n ------\n - Fixed incorrect Cargo environment variable query when used in build\ndiff --git a/libbpf-cargo/src/gen/btf.rs b/libbpf-cargo/src/gen/btf.rs\nindex d3724db4..4acdbf3c 100644\n--- a/libbpf-cargo/src/gen/btf.rs\n+++ b/libbpf-cargo/src/gen/btf.rs\n@@ -578,6 +578,11 @@ impl<'s> GenBtf<'s> {\n         t: types::Composite<'_>,\n         opts: &TypeDeclOpts,\n     ) -> Result<()> {\n+        if t.is_empty_union() {\n+            // Ignore empty union; Rust does not allow unions with no fields.\n+            return Ok(());\n+        }\n+\n         let packed = is_struct_packed(&t, &self.btf)?;\n \n         // fields in the aggregate\n@@ -602,6 +607,14 @@ impl<'s> GenBtf<'s> {\n                 .type_by_id::<BtfType<'_>>(member.ty)\n                 .unwrap()\n                 .skip_mods_and_typedefs();\n+\n+            if let Ok(composite) = TryInto::<types::Composite<'_>>::try_into(field_ty) {\n+                if composite.is_empty_union() {\n+                    // Skip empty union field; we do not generate a type for them.\n+                    continue;\n+                }\n+            }\n+\n             if let Some(next_ty_id) = next_type(field_ty)? {\n                 dependent_types.push(next_ty_id);\n             }\ndiff --git a/libbpf-rs/src/btf/types.rs b/libbpf-rs/src/btf/types.rs\nindex f931c3e3..f784c136 100644\n--- a/libbpf-rs/src/btf/types.rs\n+++ b/libbpf-rs/src/btf/types.rs\n@@ -579,6 +579,13 @@ impl<'btf> TryFrom<Composite<'btf>> for Union<'btf> {\n     }\n }\n \n+impl Composite<'_> {\n+    /// Returns whether this composite type is a `union {}`.\n+    pub fn is_empty_union(&self) -> bool {\n+        !self.is_struct && self.is_empty()\n+    }\n+}\n+\n // Composite\n gen_collection_members_concrete_type! {\n     btf_member as Composite with HasSize;\n", "instance_id": "libbpf__libbpf-rs-975", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a build failure in `libbpf-cargo` when handling a C struct with an empty union. It provides specific context, including the version of the library, the failing code snippet, the error message, and relevant links to the problematic code in both `libbpf-cargo` and `vmlinux.h`. The goal of adding support for empty unions in `libbpf-cargo` is also articulated. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or output for handling empty unions beyond \"adding support.\" Additionally, it lacks discussion of potential edge cases or constraints (e.g., performance implications or compatibility with other parts of the library). While the issue is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, affecting a few specific files (`btf.rs`, `types.rs`, and a changelog update) within the `libbpf-cargo` and `libbpf-rs` codebase. The changes involve modifying logic for type generation to handle empty unions by skipping them, which requires understanding the BTF (BPF Type Format) generation process and the library's internal representation of composite types. This indicates a moderate level of complexity in terms of codebase interaction, as it does not significantly impact the overall architecture but does require precise modifications in a critical area (type handling).\n\nSecond, the technical concepts involved include familiarity with Rust (specifically its type system and limitations around unions), the `libbpf` ecosystem, and BPF-related type definitions. While these are not extremely advanced topics, they do require domain-specific knowledge of BPF and a solid understanding of Rust's safety and type system constraints, which adds to the difficulty for someone not already familiar with these areas.\n\nThird, the problem does not explicitly mention edge cases in the statement, but the code changes suggest a straightforward approach (ignoring empty unions) that may not account for more complex scenarios, such as nested empty unions or interactions with other type definitions. Error handling modifications are minimal, as the solution simply skips empty unions without additional logic for error reporting or fallback behavior.\n\nOverall, this problem requires understanding multiple concepts and making targeted but non-trivial changes across a few files. It does not involve deep architectural refactoring or advanced algorithms, nor does it appear to have significant performance or system-level implications. Therefore, a difficulty score of 0.50 reflects a medium level of challenge, suitable for someone with intermediate experience in Rust and familiarity with BPF tools.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Specify output file in `wezterm record`\n**Is your feature request related to a problem? Please describe.**\nWhen using `wezterm record`, there is no way to specify the output file.\n\n**Describe the solution you'd like**\nAdd a new flag to the cli `-o` that accepts a path and write the asciicast to that file when that flag is present, otherwise, keep writing to a random filename in the temporary directory.\n\n**Describe alternatives you've considered**\nI can always look in the temp directory for the newest file that starts with the `wezterm-recording` prefix, but I think this is a better solution and easy to implement.\n\n**Additional context**\nIf there are no issues with this feature, I'm willing to implement it myself, shouldn't be too hard.\n", "patch": "diff --git a/docs/examples/cmd-synopsis-wezterm-record--help.txt b/docs/examples/cmd-synopsis-wezterm-record--help.txt\nindex 4787a467ebc..0ff345c4d38 100644\n--- a/docs/examples/cmd-synopsis-wezterm-record--help.txt\n+++ b/docs/examples/cmd-synopsis-wezterm-record--help.txt\n@@ -9,4 +9,6 @@ Arguments:\n Options:\n       --cwd <CWD>  Start in the specified directory, instead of the default_cwd\n                    defined by your wezterm configuration\n+  -o <OUTFILE>     Save asciicast to the specified file, instead of using a\n+                   random file name in the temp directory\n   -h, --help       Print help\ndiff --git a/wezterm/src/asciicast.rs b/wezterm/src/asciicast.rs\nindex 7cc05fcd96f..7636da68518 100644\n--- a/wezterm/src/asciicast.rs\n+++ b/wezterm/src/asciicast.rs\n@@ -342,6 +342,11 @@ pub struct RecordCommand {\n     #[arg(long)]\n     cwd: Option<std::path::PathBuf>,\n \n+    /// Save asciicast to the specified file, instead of\n+    /// using a random file name in the temp directory\n+    #[arg(short)]\n+    outfile: Option<std::path::PathBuf>,\n+\n     /// Start prog instead of the default_prog defined by your\n     /// wezterm configuration\n     #[arg(value_parser)]\n@@ -357,12 +362,24 @@ impl RecordCommand {\n \n         let header = Header::new(&config, size, &prog);\n \n-        let (cast_file, cast_file_name) = tempfile::Builder::new()\n-            .prefix(\"wezterm-recording-\")\n-            // We use a .txt suffix for convenice when uploading to GH\n-            .suffix(\".cast.txt\")\n-            .tempfile()?\n-            .keep()?;\n+        let (cast_file, cast_file_name) = match self.outfile.as_ref() {\n+            Some(outfile) => (\n+                std::fs::File::options()\n+                    .write(true)\n+                    .truncate(true)\n+                    .create(true)\n+                    .open(outfile)?,\n+                outfile.clone(),\n+            ),\n+            None => {\n+                tempfile::Builder::new()\n+                    .prefix(\"wezterm-recording-\")\n+                    // We use a .txt suffix for convenice when uploading to GH\n+                    .suffix(\".cast.txt\")\n+                    .tempfile()?\n+                    .keep()?\n+            }\n+        };\n         let mut cast_file = BufWriter::new(cast_file);\n         writeln!(cast_file, \"{}\", serde_json::to_string(&header)?)?;\n \n", "instance_id": "wezterm__wezterm-6626", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the feature request for the `wezterm record` command. The goal is well-defined: add a new `-o` flag to specify an output file for the asciicast instead of using a random temporary file. The solution and its intent are straightforward, and the context provided (e.g., alternatives considered) adds some clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly address potential edge cases, such as what should happen if the specified output file already exists (should it be overwritten or should an error be raised?), or if the path is invalid or inaccessible due to permissions. Additionally, there are no examples of usage or expected behavior in such scenarios. While these omissions are not critical to understanding the core requirement, they leave room for interpretation, which prevents a perfect clarity score.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are localized to a single file (`asciicast.rs`) for the core logic and a documentation file for the help text. The modification involves adding a new command-line argument (`outfile`) using an existing argument parsing framework (likely `clap` or similar, based on the `#[arg]` annotations) and updating the file creation logic to use the provided path if specified. The amount of code change is small (around 20 lines), and it does not impact the broader system architecture or require understanding complex interactions between modules.\n\n2. **Number of Technical Concepts:** The concepts required to solve this problem are basic. The developer needs to understand command-line argument parsing (a standard feature in Rust with libraries like `clap`), file I/O operations (using `std::fs::File` and `tempfile`), and basic error handling with Rust's `Result` type. These are fundamental Rust concepts that do not pose significant complexity for even a junior developer with some experience.\n\n3. **Edge Cases and Error Handling:** The provided code handles the basic case of creating or opening a file at the specified path with options to truncate and write. However, it does not explicitly address edge cases like permission issues, invalid paths, or existing files with specific behaviors (e.g., refusing to overwrite). While the problem statement does not mention these edge cases, the code changes include basic error handling via `?` operator, which propagates errors appropriately. Addressing additional edge cases would be straightforward but is not required based on the current scope.\n\n4. **Overall Complexity:** The task is a simple feature addition that involves minimal risk to the existing codebase. It does not require deep knowledge of the `wezterm` project's internals beyond the `RecordCommand` struct and its associated logic. The implementation is mostly a matter of extending existing patterns (e.g., how `cwd` is handled) to the new `outfile` option.\n\nGiven these factors, a difficulty score of 0.25 reflects an easy task that requires basic modifications and understanding of standard Rust libraries, with minimal complexity in terms of codebase impact or edge case handling.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Len 0 values logged as 0x in responses\nWhen certain TLS extensions of len 0 are used in the Client Hello of a request, rama returns `0x`.\n\nExample for a TLS 1.2 curl request to `echo.ramaproxy.org`:\n```\n\"tls\": {\n             ...\n      {\n        \"data\": \"0x\",\n        \"id\": \"NEXT_PROTOCOL_NEGOTIATION (0x3374)\"\n      },\n      {\n        \"data\": [\n          \"h2\",\n          \"http/1.1\"\n        ],\n        \"id\": \"APPLICATION_LAYER_PROTOCOL_NEGOTIATION (0x0010)\"\n      },\n      {\n        \"data\": \"0x\",\n        \"id\": \"ENCRYPT_THEN_MAC (0x0016)\"\n      },\n      {\n        \"data\": \"0x\",\n        \"id\": \"EXTENDED_MASTER_SECRET (0x0017)\"\n      }\n}\n```\n", "patch": "diff --git a/rama-cli/src/cmd/fp/data.rs b/rama-cli/src/cmd/fp/data.rs\nindex 3ae00dba..424db958 100644\n--- a/rama-cli/src/cmd/fp/data.rs\n+++ b/rama-cli/src/cmd/fp/data.rs\n@@ -258,7 +258,7 @@ pub(super) struct Ja3DisplayInfo {\n #[derive(Debug, Clone, Serialize)]\n pub(super) struct TlsDisplayInfoExtension {\n     pub(super) id: String,\n-    pub(super) data: TlsDisplayInfoExtensionData,\n+    pub(super) data: Option<TlsDisplayInfoExtensionData>,\n }\n \n #[derive(Debug, Clone, Serialize)]\n@@ -306,50 +306,52 @@ pub(super) fn get_tls_display_info(ctx: &Context<Arc<State>>) -> Option<TlsDispl\n             .map(|extension| match extension {\n                 ClientHelloExtension::ServerName(domain) => TlsDisplayInfoExtension {\n                     id: extension.id().to_string(),\n-                    data: TlsDisplayInfoExtensionData::Single(match domain {\n-                        Some(domain) => domain.to_string(),\n-                        None => \"\".to_owned(),\n-                    }),\n+                    data: domain\n+                        .as_ref()\n+                        .map(|d| TlsDisplayInfoExtensionData::Single(d.to_string())),\n                 },\n                 ClientHelloExtension::SignatureAlgorithms(v) => TlsDisplayInfoExtension {\n                     id: extension.id().to_string(),\n-                    data: TlsDisplayInfoExtensionData::Multi(\n+                    data: Some(TlsDisplayInfoExtensionData::Multi(\n                         v.iter().map(|s| s.to_string()).collect(),\n-                    ),\n+                    )),\n                 },\n                 ClientHelloExtension::SupportedVersions(v) => TlsDisplayInfoExtension {\n                     id: extension.id().to_string(),\n-                    data: TlsDisplayInfoExtensionData::Multi(\n+                    data: Some(TlsDisplayInfoExtensionData::Multi(\n                         v.iter().map(|s| s.to_string()).collect(),\n-                    ),\n+                    )),\n                 },\n                 ClientHelloExtension::ApplicationLayerProtocolNegotiation(v) => {\n                     TlsDisplayInfoExtension {\n                         id: extension.id().to_string(),\n-                        data: TlsDisplayInfoExtensionData::Multi(\n+                        data: Some(TlsDisplayInfoExtensionData::Multi(\n                             v.iter().map(|s| s.to_string()).collect(),\n-                        ),\n+                        )),\n                     }\n                 }\n                 ClientHelloExtension::SupportedGroups(v) => TlsDisplayInfoExtension {\n                     id: extension.id().to_string(),\n-                    data: TlsDisplayInfoExtensionData::Multi(\n+                    data: Some(TlsDisplayInfoExtensionData::Multi(\n                         v.iter().map(|s| s.to_string()).collect(),\n-                    ),\n+                    )),\n                 },\n                 ClientHelloExtension::ECPointFormats(v) => TlsDisplayInfoExtension {\n                     id: extension.id().to_string(),\n-                    data: TlsDisplayInfoExtensionData::Multi(\n+                    data: Some(TlsDisplayInfoExtensionData::Multi(\n                         v.iter().map(|s| s.to_string()).collect(),\n-                    ),\n+                    )),\n                 },\n                 ClientHelloExtension::Opaque { id, data } => TlsDisplayInfoExtension {\n                     id: id.to_string(),\n-                    data: TlsDisplayInfoExtensionData::Single(if data.is_empty() {\n-                        \"EMPTY\".to_owned()\n+                    data: if data.is_empty() {\n+                        None\n                     } else {\n-                        format!(\"0x{}\", hex::encode(data))\n-                    }),\n+                        Some(TlsDisplayInfoExtensionData::Single(format!(\n+                            \"0x{}\",\n+                            hex::encode(data)\n+                        )))\n+                    },\n                 },\n             })\n             .collect::<Vec<_>>(),\ndiff --git a/rama-cli/src/cmd/fp/endpoints.rs b/rama-cli/src/cmd/fp/endpoints.rs\nindex 9b086c67..8076bf2c 100644\n--- a/rama-cli/src/cmd/fp/endpoints.rs\n+++ b/rama-cli/src/cmd/fp/endpoints.rs\n@@ -547,18 +547,19 @@ impl From<TlsDisplayInfo> for Vec<Table> {\n             ],\n         });\n         for extension in info.extensions {\n+            let mut rows = vec![(\"ID\".to_owned(), extension.id)];\n+            if let Some(data) = extension.data {\n+                rows.push((\n+                    \"Data\".to_owned(),\n+                    match data {\n+                        TlsDisplayInfoExtensionData::Single(s) => s,\n+                        TlsDisplayInfoExtensionData::Multi(v) => v.join(\", \"),\n+                    },\n+                ));\n+            }\n             vec.push(Table {\n                 title: \"\ud83d\udd12 TLS Client Hello \u2014 Extension\".to_owned(),\n-                rows: vec![\n-                    (\"ID\".to_owned(), extension.id),\n-                    (\n-                        \"Data\".to_owned(),\n-                        match extension.data {\n-                            TlsDisplayInfoExtensionData::Single(s) => s,\n-                            TlsDisplayInfoExtensionData::Multi(v) => v.join(\", \"),\n-                        },\n-                    ),\n-                ],\n+                rows,\n             });\n         }\n         vec\ndiff --git a/src/cli/service/echo.rs b/src/cli/service/echo.rs\nindex d1d12bed..dd3162d4 100644\n--- a/src/cli/service/echo.rs\n+++ b/src/cli/service/echo.rs\n@@ -414,10 +414,16 @@ impl Service<(), Request> for EchoService {\n                             \"id\": extension.id().to_string(),\n                             \"data\": v.iter().map(|s| s.to_string()).collect::<Vec<_>>(),\n                         }),\n-                        ClientHelloExtension::Opaque { id, data } => json!({\n-                            \"id\": id.to_string(),\n-                            \"data\": format!(\"0x{}\", hex::encode(data)),\n-                        }),\n+                        ClientHelloExtension::Opaque { id, data } => if data.is_empty() {\n+                            json!({\n+                                \"id\": id.to_string()\n+                            })\n+                        } else {\n+                            json!({\n+                                \"id\": id.to_string(),\n+                                \"data\": format!(\"0x{}\", hex::encode(data))\n+                            })\n+                        },\n                     }).collect::<Vec<_>>(),\n                     \"ja3\": ja3,\n                     \"ja4\": ja4,\n", "instance_id": "plabayo__rama-433", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when certain TLS extensions with a length of 0 are used in a Client Hello request, the response incorrectly shows \"0x\" as the data value. The example provided (a JSON snippet from a response) helps illustrate the issue. However, the statement lacks explicit details about the expected behavior or output for such cases (e.g., should empty data be omitted, represented as an empty string, or handled differently?). Additionally, there are no mentions of specific constraints or edge cases beyond the len 0 scenario. While the goal is understandable, these missing minor details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluated factors:\n\n1. **Scope and Depth of Code Changes:** The changes are localized to three files (`rama-cli/src/cmd/fp/data.rs`, `rama-cli/src/cmd/fp/endpoints.rs`, and `src/cli/service/echo.rs`) and primarily involve modifying how empty data in TLS extensions is handled and displayed. The modifications are relatively small, focusing on changing a field from a mandatory type to an optional type (`Option<TlsDisplayInfoExtensionData>`) and adjusting the serialization and display logic accordingly. There is no significant impact on the system's architecture, as the changes are confined to data representation and output formatting.\n\n2. **Number of Technical Concepts:** Solving this requires a basic understanding of Rust's type system (specifically using `Option` to handle absence of data), serialization with `serde` (for JSON output), and string formatting (e.g., hex encoding). These are fundamental concepts in Rust and do not involve advanced language features, complex algorithms, or domain-specific knowledge beyond basic TLS extension handling, which appears to be already abstracted in the codebase.\n\n3. **Potential Edge Cases and Error Handling:** The problem focuses on a specific edge case (TLS extensions with length 0), and the code changes address it by conditionally omitting or altering the output for empty data. No additional error handling or complex edge case logic is introduced beyond this straightforward check. The problem statement does not hint at other edge cases to consider, so the scope remains narrow.\n\n4. **Overall Complexity:** The logic required to fix the issue is simple\u2014checking if data is empty and adjusting the output (e.g., omitting the \"data\" field in JSON or not displaying it). The changes involve understanding a small part of the codebase related to TLS extension processing and do not require deep architectural knowledge or extensive refactoring.\n\nGiven these factors, a score of 0.35 reflects an \"Easy\" problem that requires understanding some code logic and making straightforward modifications to handle a specific case. It is slightly above the lower end of the easy range due to the need to coordinate changes across three files, but it remains a relatively simple task for a developer familiar with Rust and the codebase.", "clarity_label": 1, "difficulty_label": 1, "human_clarity": -1, "human_difficulty": -1}
