{"problem_statement": "Support `textDocument/documentLink` requests to jump to container registries in Compose files\nWe should support opening the websites for the following images.\n```YAML\nservices:\n  test:\n    image: alpine\n  test2:\n    image: mcr.microsoft.com/powershell\n  test3:\n    image: ghcr.io/super-linter/super-linter\n  test4:\n    image: quay.io/prometheus/node-exporter\n```\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 0701ce4..e82ca80 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -13,6 +13,8 @@ All notable changes to the Docker Language Server will be documented in this fil\n     - suggests completion items for the attributes of an object inside an array ([#95](https://github.com/docker/docker-language-server/issues/95))\n   - textDocument/definition\n     - support lookup of `configs`, `networks`, and `secrets` referenced inside `services` object ([#91](https://github.com/docker/docker-language-server/issues/91))\n+  - textDocument/documentLink\n+    - support opening a referenced image's page as a link ([#91](https://github.com/docker/docker-language-server/issues/91))\n \n ## [0.3.8] - 2025-04-24\n \ndiff --git a/internal/compose/documentLink.go b/internal/compose/documentLink.go\nindex e053fb9..cc5178e 100644\n--- a/internal/compose/documentLink.go\n+++ b/internal/compose/documentLink.go\n@@ -12,14 +12,14 @@ import (\n \t\"github.com/docker/docker-language-server/internal/types\"\n )\n \n-func DocumentLink(ctx context.Context, documentURI protocol.URI, document document.ComposeDocument) ([]protocol.DocumentLink, error) {\n+func DocumentLink(ctx context.Context, documentURI protocol.URI, doc document.ComposeDocument) ([]protocol.DocumentLink, error) {\n \turl, err := url.Parse(string(documentURI))\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"LSP client sent invalid URI: %v\", string(documentURI))\n \t}\n \n \tlinks := []protocol.DocumentLink{}\n-\tresults, _ := DocumentSymbol(ctx, document)\n+\tresults, _ := DocumentSymbol(ctx, doc)\n \tfor _, result := range results {\n \t\tif symbol, ok := result.(*protocol.DocumentSymbol); ok && symbol.Kind == protocol.SymbolKindModule {\n \t\t\tabs, err := types.AbsolutePath(url, symbol.Name)\n@@ -32,5 +32,78 @@ func DocumentLink(ctx context.Context, documentURI protocol.URI, document docume\n \t\t\t}\n \t\t}\n \t}\n+\n+\troot := doc.RootNode()\n+\tif len(root.Content) > 0 {\n+\t\tfor i := range root.Content[0].Content {\n+\t\t\tswitch root.Content[0].Content[i].Value {\n+\t\t\tcase \"services\":\n+\t\t\t\tfor j := 0; j < len(root.Content[0].Content[i+1].Content); j += 2 {\n+\t\t\t\t\tserviceProperties := root.Content[0].Content[i+1].Content[j+1].Content\n+\t\t\t\t\tfor k := 0; k < len(serviceProperties); k += 2 {\n+\t\t\t\t\t\tif serviceProperties[k].Value == \"image\" {\n+\t\t\t\t\t\t\timageNode := serviceProperties[k+1]\n+\t\t\t\t\t\t\tlinkedText, link := extractImageLink(imageNode.Value)\n+\t\t\t\t\t\t\tlinks = append(links, protocol.DocumentLink{\n+\t\t\t\t\t\t\t\tRange: protocol.Range{\n+\t\t\t\t\t\t\t\t\tStart: protocol.Position{\n+\t\t\t\t\t\t\t\t\t\tLine:      protocol.UInteger(imageNode.Line) - 1,\n+\t\t\t\t\t\t\t\t\t\tCharacter: protocol.UInteger(imageNode.Column) - 1,\n+\t\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\t\tEnd: protocol.Position{\n+\t\t\t\t\t\t\t\t\t\tLine:      protocol.UInteger(imageNode.Line) - 1,\n+\t\t\t\t\t\t\t\t\t\tCharacter: protocol.UInteger(imageNode.Column - 1 + len(linkedText)),\n+\t\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\tTarget:  types.CreateStringPointer(link),\n+\t\t\t\t\t\t\t\tTooltip: types.CreateStringPointer(link),\n+\t\t\t\t\t\t\t})\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n \treturn links, nil\n }\n+\n+func extractImageLink(nodeValue string) (string, string) {\n+\tif strings.HasPrefix(nodeValue, \"ghcr.io/\") {\n+\t\tidx := strings.LastIndex(nodeValue, \":\")\n+\t\tif idx == -1 {\n+\t\t\treturn nodeValue, fmt.Sprintf(\"https://%v\", nodeValue)\n+\t\t}\n+\t\treturn nodeValue[0:idx], fmt.Sprintf(\"https://%v\", nodeValue[0:idx])\n+\t}\n+\n+\tif strings.HasPrefix(nodeValue, \"mcr.microsoft.com\") {\n+\t\tidx := strings.LastIndex(nodeValue, \":\")\n+\t\tif idx == -1 {\n+\t\t\treturn nodeValue, fmt.Sprintf(\"https://mcr.microsoft.com/artifact/mar/%v\", nodeValue[18:])\n+\t\t}\n+\t\treturn nodeValue[0:idx], fmt.Sprintf(\"https://mcr.microsoft.com/artifact/mar/%v\", nodeValue[18:idx])\n+\t}\n+\n+\tif strings.HasPrefix(nodeValue, \"quay.io/\") {\n+\t\tidx := strings.LastIndex(nodeValue, \":\")\n+\t\tif idx == -1 {\n+\t\t\treturn nodeValue, fmt.Sprintf(\"https://quay.io/repository/%v\", nodeValue[8:])\n+\t\t}\n+\t\treturn nodeValue[0:idx], fmt.Sprintf(\"https://quay.io/repository/%v\", nodeValue[8:idx])\n+\t}\n+\n+\tidx := strings.LastIndex(nodeValue, \":\")\n+\tif idx == -1 {\n+\t\tidx := strings.Index(nodeValue, \"/\")\n+\t\tif idx == -1 {\n+\t\t\treturn nodeValue, fmt.Sprintf(\"https://hub.docker.com/_/%v\", nodeValue)\n+\t\t}\n+\t\treturn nodeValue, fmt.Sprintf(\"https://hub.docker.com/r/%v\", nodeValue)\n+\t}\n+\n+\tslashIndex := strings.Index(nodeValue, \"/\")\n+\tif slashIndex == -1 {\n+\t\treturn nodeValue[0:idx], fmt.Sprintf(\"https://hub.docker.com/_/%v\", nodeValue[0:idx])\n+\t}\n+\treturn nodeValue[0:idx], fmt.Sprintf(\"https://hub.docker.com/r/%v\", nodeValue[0:idx])\n+}\n", "instance_id": "docker__docker-language-server-100", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in its intent to support `textDocument/documentLink` requests for jumping to container registry websites from Compose files. It provides a clear example of the YAML structure with different image sources (e.g., Docker Hub, GitHub Container Registry, Microsoft Container Registry, and Quay.io), which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the links should be formatted or whether specific registries require unique handling (though the code changes imply this). Additionally, there is no mention of edge cases, such as invalid image names, missing tags, or unsupported registries, which could impact the implementation. Overall, while the goal is clear, the lack of detailed requirements around edge cases and link formatting prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`documentLink.go`) with the addition of logic to parse image names and generate appropriate URLs for different container registries. The changeset is moderate in size, with around 70-80 lines of new code, and does not impact the broader system architecture. However, it requires understanding multiple technical concepts, including parsing YAML structures (via the `ComposeDocument` type), working with the Language Server Protocol (LSP) for document links, and handling string manipulation to extract and format URLs. The logic for determining the correct registry URL based on prefixes (e.g., `ghcr.io`, `mcr.microsoft.com`) adds some complexity, as it involves conditional checks and string slicing. Edge cases, such as handling image names with or without tags and supporting default cases for Docker Hub, are addressed in the code but not explicitly mentioned in the problem statement, indicating a moderate level of foresight required by the developer. Overall, this task requires a solid understanding of the codebase's document parsing logic and LSP integration, but it does not involve deep architectural changes or highly complex algorithms, placing it in the 0.4-0.6 range with a score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[FR] Provide an API to know if the third-party services integration settings are configured\n- [x] I have read this document: https://miniflux.app/opinionated.html#feature-request\r\n\r\nI have reviewed the documentation multiple times but couldn't find an API that allows checking whether the third-party services integration settings are configured.\r\nIf such an API already exists, could you please point me to the relevant documentation?\r\nIf not, I would like to request the implementation of an API endpoint that provides the ability to determine if third-party services integration settings are configured or not.\r\n\r\nThis feature would be particularly helpful in scenarios where we need to decide whether or not to display `Save` option in the UI, based on the configuration status of third-party services.\n", "patch": "diff --git a/.github/workflows/linters.yml b/.github/workflows/linters.yml\nindex d5ecac7fa2d..695dea78b15 100644\n--- a/.github/workflows/linters.yml\n+++ b/.github/workflows/linters.yml\n@@ -32,7 +32,11 @@ jobs:\n       - run: \"go vet ./...\"\n       - uses: golangci/golangci-lint-action@v6\n         with:\n-          args: --timeout 10m --skip-dirs tests --disable errcheck --enable sqlclosecheck --enable misspell --enable gofmt --enable goimports --enable whitespace --enable gocritic\n+          args: >\n+            --timeout 10m\n+            --exclude-dirs=tests\n+            --disable errcheck\n+            --enable sqlclosecheck,misspell,gofmt,goimports,whitespace,gocritic\n       - uses: dominikh/staticcheck-action@v1.3.1\n         with:\n           version: \"2024.1.1\"\ndiff --git a/client/client.go b/client/client.go\nindex 11659164fba..a463b2d725a 100644\n--- a/client/client.go\n+++ b/client/client.go\n@@ -185,6 +185,25 @@ func (c *Client) MarkAllAsRead(userID int64) error {\n \treturn err\n }\n \n+// FetchIntegrationsStatus fetches the integrations status for a user.\n+func (c *Client) FetchIntegrationsStatus() (bool, error) {\n+\tbody, err := c.request.Get(\"/v1/users/integrations/status\")\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\tdefer body.Close()\n+\n+\tvar response struct {\n+\t\tHasIntegrations bool `json:\"has_integrations\"`\n+\t}\n+\n+\tif err := json.NewDecoder(body).Decode(&response); err != nil {\n+\t\treturn false, fmt.Errorf(\"miniflux: response error (%v)\", err)\n+\t}\n+\n+\treturn response.HasIntegrations, nil\n+}\n+\n // Discover try to find subscriptions from a website.\n func (c *Client) Discover(url string) (Subscriptions, error) {\n \tbody, err := c.request.Post(\"/v1/discover\", map[string]string{\"url\": url})\ndiff --git a/internal/api/api.go b/internal/api/api.go\nindex 48d13039cf6..19d5ba62d0c 100644\n--- a/internal/api/api.go\n+++ b/internal/api/api.go\n@@ -37,6 +37,7 @@ func Serve(router *mux.Router, store *storage.Storage, pool *worker.Pool) {\n \tsr.HandleFunc(\"/users/{userID:[0-9]+}\", handler.updateUser).Methods(http.MethodPut)\n \tsr.HandleFunc(\"/users/{userID:[0-9]+}\", handler.removeUser).Methods(http.MethodDelete)\n \tsr.HandleFunc(\"/users/{userID:[0-9]+}/mark-all-as-read\", handler.markUserAsRead).Methods(http.MethodPut)\n+\tsr.HandleFunc(\"/users/integrations/status\", handler.getIntegrationsStatus).Methods(http.MethodGet)\n \tsr.HandleFunc(\"/users/{username}\", handler.userByUsername).Methods(http.MethodGet)\n \tsr.HandleFunc(\"/me\", handler.currentUser).Methods(http.MethodGet)\n \tsr.HandleFunc(\"/categories\", handler.createCategory).Methods(http.MethodPost)\ndiff --git a/internal/api/user.go b/internal/api/user.go\nindex 71d70e9c5b5..112c19a25d4 100644\n--- a/internal/api/user.go\n+++ b/internal/api/user.go\n@@ -130,6 +130,25 @@ func (h *handler) markUserAsRead(w http.ResponseWriter, r *http.Request) {\n \tjson.NoContent(w, r)\n }\n \n+func (h *handler) getIntegrationsStatus(w http.ResponseWriter, r *http.Request) {\n+\tuserID := request.UserID(r)\n+\n+\tif _, err := h.store.UserByID(userID); err != nil {\n+\t\tjson.NotFound(w, r)\n+\t\treturn\n+\t}\n+\n+\thasIntegrations := h.store.HasSaveEntry(userID)\n+\n+\tresponse := struct {\n+\t\tHasIntegrations bool `json:\"has_integrations\"`\n+\t}{\n+\t\tHasIntegrations: hasIntegrations,\n+\t}\n+\n+\tjson.OK(w, r, response)\n+}\n+\n func (h *handler) users(w http.ResponseWriter, r *http.Request) {\n \tif !request.IsAdminUser(r) {\n \t\tjson.Forbidden(w, r)\n", "instance_id": "miniflux__v2-2880", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent: it requests an API endpoint to check if third-party services integration settings are configured, which is useful for UI decisions like displaying a \"Save\" option. The goal is well-defined, and the context of the request is provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify what constitutes \"configured\" (e.g., whether it means any integration is set up or if specific services must be enabled). Additionally, there are no explicit mentions of input/output formats, constraints, or edge cases in the statement itself, though the code changes provide some clarity on the expected output (a boolean indicating configuration status). Due to these minor gaps, the clarity score is rated as 2 (Mostly Clear) rather than 3.", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, affecting a few files (`client.go`, `api.go`, `user.go`) in a straightforward manner. The modifications involve adding a new API endpoint, a client method to fetch the status, and the logic to check if integrations are configured. There is no significant impact on the system's architecture, and the changes do not require deep interaction with other modules beyond the existing storage layer. The amount of code added is minimal, focusing on a single feature.\n\n2. **Number of Technical Concepts:** The solution requires understanding of basic Go concepts such as HTTP routing (using `mux.Router`), JSON encoding/decoding, and interacting with a storage layer. These are standard and not particularly complex for a developer familiar with Go. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond typical web API development.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes include basic error handling (e.g., checking for user existence, handling JSON decoding errors). There are no complex edge cases apparent from the problem or code, such as handling concurrent access or invalid configurations. The error handling added is straightforward and aligns with existing patterns in the codebase.\n\n4. **Overall Complexity:** The task involves adding a simple feature to an existing API framework, requiring only a moderate understanding of the codebase's structure (e.g., how user data and integrations are stored). It does not demand deep architectural changes or performance optimizations.\n\nGiven these considerations, I assign a difficulty score of 0.35, placing it on the higher end of the Easy range due to the need to understand and integrate with an existing API and storage system, but still well below Medium difficulty as the changes are straightforward and self-contained.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Pi-Hole v6 AAAA dual record causes crashback loop\n<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\n-->\n\n**What happened**:\n\nWhen using AAAA dual records (e.g. `::ffff:192.168.20.3`), the Pi-Hole v6 client goes into a crash loop.\n\nhttps://github.com/kubernetes-sigs/external-dns/pull/5226#issuecomment-2781088132\n\nThis may also effect the v5 client? I never tried to use these dual records with it, but it works fine with other external dns providers (e.g. UniFi webhook)\n\nIf I use `--exclude-record-types=AAAA` and manually create the records in Pi-Hole everything works fine as well, so I believe Pi-Hole is fine with these types of records.\n\n**What you expected to happen**:\n\nAAAA dual records should work. For instance, it works fine with the UniFi webhook.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n\nUsing the staging image `v20250403-v0.16.1-70-gc5af75e3`\n\nI get the following error when I have the sources set to `service` (I get the same fatal error for both both `upsert-only` and `sync`, I'm using `sync` here when writing it up):\n\n(No records at all in the `Pi-Hole` UI when I startup external-dns)\n\n```sh\ntime=\"2025-04-03T21:41:04-05:00\" level=debug msg=\"Endpoints generated from service: kube-system/cilium-gateway-internal: [internal.domain.name 0 IN A  192.168.20.3 [] internal.domain.name 0 IN AAAA  ::ffff:192.168.20.3 []]\"\n...\ntime=\"2025-04-03T21:41:04-05:00\" level=info msg=\"PUT internal.domain.name IN AAAA -> ::ffff:192.168.20.3\"\ntime=\"2025-04-03T21:41:04-05:00\" level=info msg=\"PUT internal.domain.name IN A -> 192.168.20.3\"\n...\ntime=\"2025-04-03T21:41:09-05:00\" level=debug msg=\"Endpoints generated from service: kube-system/cilium-gateway-internal: [internal.domain.name 0 IN A  192.168.20.3 [] internal.domain.name 0 IN AAAA  ::ffff:192.168.20.3 []]\"\n...\ntime=\"2025-04-03T21:41:09-05:00\" level=info msg=\"PUT internal.domain.name IN AAAA -> ::ffff:192.168.20.3\"\ntime=\"2025-04-03T21:41:09-05:00\" level=debug msg=\"Error on request %!s(<nil>)\"\ntime=\"2025-04-03T21:41:09-05:00\" level=fatal msg=\"Failed to do run once: received 400 status code from request: [bad_request] Item already present (Uniqueness of items is enforced) - 0.000316s\"\n```\n\n(These are the only logs with debug enabled that include `192.168.20.3`)\n\nThis causes the pod to enter `CrashLoopBackOff`.\n\nWhen the pod retries, it shows instead:\n\n```sh\ntime=\"2025-04-03T21:48:40-05:00\" level=debug msg=\"Endpoints generated from service: kube-system/cilium-gateway-external: [internal.domain.name 0 IN A  192.168.20.3 [] internal.domain.name 0 IN AAAA  ::ffff:192.1\n68.20.3 []]\"\n...\ntime=\"2025-04-03T21:48:40-05:00\" level=info msg=\"PUT internal.domain.name IN AAAA -> ::ffff:192.168.20.3\"\ntime=\"2025-04-03T21:48:40-05:00\" level=debug msg=\"Error on request %!s(<nil>)\"\ntime=\"2025-04-03T21:48:40-05:00\" level=fatal msg=\"Failed to do run once: received 400 status code from request: [bad_request] Item already present (Uniqueness of items is enforced) - 0.000280s\"\n```\n\n(So only the one, instead of showing it twice)\n\nIf I switch from `service` as the source to `gateway-httproute`, the `A` record get deleted and my `CNAME` records get created as expected, but the `AAAA` records do not get deleted, I still see them in the Pi-Hole UI. (And I see the `All records are already up to date`, the pod does not crash)\n\nIf I then switch back again from `gateway-httproute` to `service`, all the `CNAME` records get deleted, but I get the fatal error before the `A` record even get created, since it fails to re-create the `AAAA` record first.\n\nAdding `--exclude-record-types=AAAA` fixes the crash, but obviously the `AAAA` records do not get created.\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- External-DNS version (use `external-dns --version`): v20250403-v0.16.1-70-gc5af75e3\n- DNS provider: Pi-Hole v6\n- Others:\n `pihole-FTL --version`: v6.1\n", "patch": "diff --git a/provider/pihole/clientV6.go b/provider/pihole/clientV6.go\nindex ae6ae399b3..ebaaabd6a2 100644\n--- a/provider/pihole/clientV6.go\n+++ b/provider/pihole/clientV6.go\n@@ -25,6 +25,7 @@ import (\n \t\"fmt\"\n \t\"io\"\n \t\"net/http\"\n+\t\"net/netip\"\n \t\"net/url\"\n \t\"strconv\"\n \t\"strings\"\n@@ -63,6 +64,7 @@ func newPiholeClientV6(cfg PiholeConfig) (piholeAPI, error) {\n \t\t\t},\n \t\t},\n \t}\n+\n \tcl := instrumented_http.NewClient(httpClient, &instrumented_http.Callbacks{})\n \n \tp := &piholeClientV6{\n@@ -114,6 +116,32 @@ func (p *piholeClientV6) getConfigValue(ctx context.Context, rtype string) ([]st\n \treturn results, nil\n }\n \n+/**\n+ * isValidIPv4 checks if the given IP address is a valid IPv4 address.\n+ * It returns true if the IP address is valid, false otherwise.\n+ * If the IP address is in IPv6 format, it will return false.\n+ */\n+func isValidIPv4(ip string) bool {\n+\taddr, err := netip.ParseAddr(ip)\n+\tif err != nil {\n+\t\treturn false\n+\t}\n+\treturn addr.Is4()\n+}\n+\n+/**\n+ * isValidIPv6 checks if the given IP address is a valid IPv6 address.\n+ * It returns true if the IP address is valid, false otherwise.\n+ * If the IP address is in IPv6 with dual format y:y:y:y:y:y:x.x.x.x. , it will return true.\n+ */\n+func isValidIPv6(ip string) bool {\n+\taddr, err := netip.ParseAddr(ip)\n+\tif err != nil {\n+\t\treturn false\n+\t}\n+\treturn addr.Is6()\n+}\n+\n func (p *piholeClientV6) listRecords(ctx context.Context, rtype string) ([]*endpoint.Endpoint, error) {\n \tout := make([]*endpoint.Endpoint, 0)\n \tresults, err := p.getConfigValue(ctx, rtype)\n@@ -126,42 +154,39 @@ func (p *piholeClientV6) listRecords(ctx context.Context, rtype string) ([]*endp\n \t\t\treturn r == ' ' || r == ','\n \t\t})\n \t\tif len(recs) < 2 {\n-\t\t\tlog.Warnf(\"skipping record %s: invalid format\", rec)\n+\t\t\tlog.Warnf(\"skipping record %s: invalid format received from PiHole\", rec)\n \t\t\tcontinue\n \t\t}\n \t\tvar DNSName, Target string\n-\t\tvar Ttl endpoint.TTL = 0\n+\t\tvar Ttl = endpoint.TTL(0)\n \t\t// A/AAAA record format is target(IP) DNSName\n \t\tDNSName, Target = recs[1], recs[0]\n-\n \t\tswitch rtype {\n \t\tcase endpoint.RecordTypeA:\n-\t\t\tif strings.Contains(Target, \":\") {\n+\t\t\t//PiHole return A and AAAA records. Filter to only keep the A records\n+\t\t\tif !isValidIPv4(Target) {\n \t\t\t\tcontinue\n \t\t\t}\n \t\tcase endpoint.RecordTypeAAAA:\n-\t\t\tif strings.Contains(Target, \".\") {\n+\t\t\t//PiHole return A and AAAA records. Filter to only keep the AAAA records\n+\t\t\tif !isValidIPv6(Target) {\n \t\t\t\tcontinue\n \t\t\t}\n \t\tcase endpoint.RecordTypeCNAME:\n-\t\t\t// CNAME format is DNSName,target\n+\t\t\t//PiHole return only CNAME records.\n+\t\t\t// CNAME format is DNSName,target, ttl?\n \t\t\tDNSName, Target = recs[0], recs[1]\n \t\t\tif len(recs) == 3 { // TTL is present\n \t\t\t\t// Parse string to int64 first\n \t\t\t\tif ttlInt, err := strconv.ParseInt(recs[2], 10, 64); err == nil {\n \t\t\t\t\tTtl = endpoint.TTL(ttlInt)\n \t\t\t\t} else {\n-\t\t\t\t\tlog.Warnf(\"failed to parse TTL value '%s': %v; using a TTL of %d\", recs[2], err, Ttl)\n+\t\t\t\t\tlog.Warnf(\"failed to parse TTL value received from PiHole '%s': %v; using a TTL of %d\", recs[2], err, Ttl)\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\tout = append(out, &endpoint.Endpoint{\n-\t\t\tDNSName:    DNSName,\n-\t\t\tTargets:    []string{Target},\n-\t\t\tRecordTTL:  Ttl,\n-\t\t\tRecordType: rtype,\n-\t\t})\n+\t\tout = append(out, endpoint.NewEndpointWithTTL(DNSName, rtype, Ttl, Target))\n \t}\n \treturn out, nil\n }\n@@ -375,7 +400,13 @@ func (p *piholeClientV6) do(req *http.Request) ([]byte, error) {\n \t\tif err := json.Unmarshal(jRes, &apiError); err != nil {\n \t\t\treturn nil, fmt.Errorf(\"failed to unmarshal error response: %w\", err)\n \t\t}\n-\t\tlog.Debugf(\"Error on request %s\", req.Body)\n+\t\tif log.IsLevelEnabled(log.DebugLevel) {\n+\t\t\tlog.Debugf(\"Error on request %s\", req.URL)\n+\t\t\tif req.Body != nil {\n+\t\t\t\tlog.Debugf(\"Body of the request %s\", req.Body)\n+\t\t\t}\n+\t\t}\n+\n \t\tif res.StatusCode == http.StatusUnauthorized && p.token != \"\" {\n \t\t\ttryCount := 1\n \t\t\tmaxRetries := 3\n", "instance_id": "kubernetes-sigs__external-dns-5253", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Pi-Hole v6 crashes into a loop when handling AAAA dual records (e.g., `::ffff:192.168.20.3`) with the External-DNS provider. It provides detailed logs, reproduction steps, and environmental context (e.g., version numbers, specific configurations like `sync` or `upsert-only`). The expected behavior (AAAA dual records should work as they do with other providers like UniFi webhook) and the workaround (`--exclude-record-types=AAAA`) are also mentioned. However, there are minor ambiguities: the problem statement does not explicitly define what constitutes a \"dual record\" beyond the example, nor does it clarify whether the issue lies solely in External-DNS or in its interaction with Pi-Hole's API. Additionally, edge cases or specific constraints around record formats or API behavior are not fully detailed, which could impact the solution's completeness. Overall, it is clear enough to understand the issue and start working on a fix, but some minor details are missing.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`clientV6.go`) in the Pi-Hole provider module of the External-DNS codebase. The changes involve adding validation logic for IPv4 and IPv6 addresses using the `net/netip` package and refining how A and AAAA records are filtered and processed, which requires understanding the existing logic for record parsing and API interaction. Second, the technical concepts involved include familiarity with Go's networking libraries (`net/netip`), logging practices, HTTP client interactions, and DNS record handling, which are moderately complex but not overly advanced. Third, the problem requires addressing potential edge cases, such as malformed IP addresses or unexpected API responses, though these are not extensively detailed in the statement. The code changes also improve error logging for debugging, indicating some attention to robustness. However, the modifications do not appear to impact the broader system architecture or require deep refactoring, and the crash loop issue seems to stem from a specific API error (`400 Bad Request: Item already present`), which narrows the problem space. Overall, solving this requires a solid understanding of the provider's logic and moderate debugging skills, but it does not demand advanced domain-specific knowledge or extensive cross-module changes, placing it at a difficulty of 0.50.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Malformed output when log contains an equals sign\n**Describe the bug**\r\nWhenever a container's console output contains `a = b`, Dozzle seems to misinterpret the line as a series of variables and ends up malforming the entire line.\r\n\r\n**To Reproduce**\r\n\r\n1. Run some variation of the following code, in any container. This example is in Python but the language doesn't matter. The only important part is the second print statement.\r\n```py\r\nimport time\r\n\r\nprint(\"hello world\")\r\nprint(\"Failed to run somecommand with somevariable = 1\")\r\n\r\nwhile True:\r\n    time.sleep(1)\r\n```\r\n\r\n2. Open the Dozzle logs for the container\r\n3. Observe that the output looks like the screenshot below\r\n\r\n**Expected behavior**\r\nI expected the message to be printed as-is: `Failed to run somecommand with somevariable = 1`\r\n\r\n**Screenshots**\r\n\r\n![image](https://github.com/amir20/dozzle/assets/12799103/51c326de-b575-4bd6-9bd5-ffd17a7eade5)\r\n\r\n**Desktop (please complete the following information):**\r\n\r\n- OS: Windows 10\r\n- Docker version: 25.0.3\r\n- Browser & version: Chrome 122.0.6261.129\r\n- Version: v6.3.1\r\n\n", "patch": "diff --git a/internal/docker/event_generator.go b/internal/docker/event_generator.go\nindex c555e42bed86..aeb870086cc3 100644\n--- a/internal/docker/event_generator.go\n+++ b/internal/docker/event_generator.go\n@@ -156,7 +156,8 @@ func readEvent(reader *bufio.Reader, tty bool) (string, StdType, error) {\n \t}\n }\n \n-var validLogFmtKey = regexp.MustCompile(`^[a-zA-Z0-9_]+$`)\n+var validLogFmtMessage = regexp.MustCompile(`([a-zA-Z0-9_.-]+)=(?:(?:\"(.*)\")|(?:(?:([^\\s]+)[\\s])))`)\n+var validLogFmtKey = regexp.MustCompile(`^[a-zA-Z0-9_.-]+$`)\n \n func createEvent(message string, streamType StdType) *LogEvent {\n \th := fnv.New32a()\n@@ -180,7 +181,7 @@ func createEvent(message string, streamType StdType) *LogEvent {\n \t\t\t\t} else {\n \t\t\t\t\tlogEvent.Message = data\n \t\t\t\t}\n-\t\t\t} else if strings.Contains(message, \"=\") {\n+\t\t\t} else if validLogFmtMessage.MatchString(message) {\n \t\t\t\tbuffer := bufPool.Get().(*bytes.Buffer)\n \t\t\t\tbuffer.Reset()\n \t\t\t\tdefer bufPool.Put(buffer)\n", "instance_id": "amir20__dozzle-2835", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the bug, including steps to reproduce, expected behavior, and a screenshot of the issue. The goal is well-defined: to fix the malformed output in Dozzle logs when a line contains an equals sign. However, there are minor ambiguities, such as the lack of explicit mention of specific edge cases beyond the provided example (e.g., different formats of key-value pairs or special characters). Additionally, the problem statement does not specify constraints or requirements for the fix, such as performance considerations or compatibility with existing log formats. Despite these minor gaps, the issue is reproducible and the intent is clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows that the fix involves a small, localized change in a single file (`event_generator.go`). The modification updates a regular expression (`validLogFmtMessage`) to better handle log messages with equals signs and adjusts the condition for parsing such messages. The change is minimal, affecting only a few lines of code, and does not impact the broader system architecture or require modifications across multiple modules.\n\n2. **Technical Concepts Involved:** Solving this issue requires a basic understanding of regular expressions in Go, as the fix hinges on crafting a more precise regex pattern to match key-value pairs in log messages. Familiarity with string manipulation and conditional logic in Go is also necessary, but these are fundamental concepts. No advanced algorithms, design patterns, or domain-specific knowledge (beyond basic log parsing) are required.\n\n3. **Edge Cases and Error Handling:** The problem statement provides a specific example of malformed output (`somevariable = 1`), but does not explicitly mention other potential edge cases, such as nested equals signs, quoted values, or special characters. The code change introduces a regex that attempts to handle quoted and unquoted values, suggesting some consideration of varied input formats. However, the complexity of handling additional edge cases appears limited, and no significant error-handling logic is added or modified in the diff.\n\n4. **Overall Complexity:** The task requires understanding a small part of the codebase related to log parsing and making a targeted fix. While it involves some logic to correctly interpret log formats, the problem does not demand deep knowledge of the entire system or complex refactoring. It is a straightforward bug fix that a developer with intermediate Go skills could address with minimal effort.\n\nGiven these points, a difficulty score of 0.35 reflects an Easy problem that requires understanding specific code logic (regex and log parsing) and making a simple, localized modification. It is slightly above the lower end of the Easy range due to the need for regex expertise, but it does not approach Medium difficulty as it lacks broader impact or complex edge case handling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove tech-detect info from default output to stdout\n### Please describe your feature request:\r\nStarting in either 1.6.1 or 1.6.2 the default output to stdout includes the info from --tech-detect. While this info is useful in certain scenarios, it goes against the streamlined integration with other tools/utilities that httpx and other projectdiscovery tools are known for. Now it's required to add the -tech-detect=false flag every time you use httpx and want to pipe the output to another tool or just copy the URL list without extra data. \r\n\r\nThis feature request is requesting that the default output be changed back to how it was in 1.6.0 and prior, removing the tech-detect info from stdout by default and only including it when the --tech-detect flag is used. \r\n\r\nHere's a screenshot of an example of the behaviour pre and post upgrade\r\n![httpx](https://github.com/projectdiscovery/httpx/assets/22598243/f6ec8758-d5f0-4532-928e-47ce86072fd2)\r\n\r\n\r\n### Describe the use case of this feature:\r\nI know it's a minor annoyance, but when trying to integrate httpx with other tools and workflows it makes much more sense to include the additional information only when the user requests it rather than by default. \r\n\n", "patch": "diff --git a/runner/options.go b/runner/options.go\nindex 3a7243ec7..8a94fceb9 100644\n--- a/runner/options.go\n+++ b/runner/options.go\n@@ -330,7 +330,7 @@ func ParseOptions() *Options {\n \t\tflagSet.BoolVar(&options.ExtractTitle, \"title\", false, \"display page title\"),\n \t\tflagSet.DynamicVarP(&options.ResponseBodyPreviewSize, \"body-preview\", \"bp\", 100, \"display first N characters of response body\"),\n \t\tflagSet.BoolVarP(&options.OutputServerHeader, \"web-server\", \"server\", false, \"display server name\"),\n-\t\tflagSet.BoolVarP(&options.TechDetect, \"tech-detect\", \"td\", true, \"display technology in use based on wappalyzer dataset\"),\n+\t\tflagSet.BoolVarP(&options.TechDetect, \"tech-detect\", \"td\", false, \"display technology in use based on wappalyzer dataset\"),\n \t\tflagSet.BoolVar(&options.OutputMethod, \"method\", false, \"display http request method\"),\n \t\tflagSet.BoolVar(&options.OutputWebSocket, \"websocket\", false, \"display server using websocket\"),\n \t\tflagSet.BoolVar(&options.OutputIP, \"ip\", false, \"display host ip\"),\ndiff --git a/runner/runner.go b/runner/runner.go\nindex b07bb76d7..9423652b8 100644\n--- a/runner/runner.go\n+++ b/runner/runner.go\n@@ -106,7 +106,7 @@ func New(options *Options) (*Runner, error) {\n \t\toptions: options,\n \t}\n \tvar err error\n-\tif options.TechDetect {\n+\tif options.TechDetect || options.JSONOutput || options.CSVOutput {\n \t\trunner.wappalyzer, err = wappalyzer.New()\n \t}\n \tif err != nil {\n@@ -276,7 +276,7 @@ func New(options *Options) (*Runner, error) {\n \tscanopts.OutputResponseTime = options.OutputResponseTime\n \tscanopts.NoFallback = options.NoFallback\n \tscanopts.NoFallbackScheme = options.NoFallbackScheme\n-\tscanopts.TechDetect = options.TechDetect\n+\tscanopts.TechDetect = options.TechDetect || options.JSONOutput || options.CSVOutput\n \tscanopts.StoreChain = options.StoreChain\n \tscanopts.StoreVisionReconClusters = options.StoreVisionReconClusters\n \tscanopts.MaxResponseBodySizeToSave = options.MaxResponseBodySizeToSave\n@@ -2004,7 +2004,7 @@ retry:\n \t\t\t// As we now have headless body, we can also use it for detecting\n \t\t\t// more technologies in the response. This is a quick trick to get\n \t\t\t// more detected technologies.\n-\t\t\tif r.options.TechDetect {\n+\t\t\tif r.options.TechDetect || r.options.JSONOutput || r.options.CSVOutput {\n \t\t\t\tmoreMatches := r.wappalyzer.FingerprintWithInfo(resp.Headers, []byte(headlessBody))\n \t\t\t\tfor match, data := range moreMatches {\n \t\t\t\t\ttechnologies = append(technologies, match)\n", "instance_id": "projectdiscovery__httpx-1732", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the feature request to revert the default behavior of including tech-detect information in the stdout output of the httpx tool. It provides context about the change in behavior from version 1.6.0 to later versions and includes a screenshot to illustrate the issue. The use case is also described, emphasizing the need for streamlined integration with other tools by excluding additional data unless explicitly requested. However, there are minor ambiguities: the problem statement does not explicitly define the expected output format after the change (though it can be inferred), and it lacks mention of potential edge cases or specific constraints related to different output formats (e.g., JSON or CSV). These missing details prevent it from being fully comprehensive, but the overall intent and goal are clear enough to proceed with implementation.", "difficulty_explanation": "The difficulty of this task falls in the easy range (0.2-0.4) due to the straightforward nature of the required changes. The code modifications are limited to a single file (`runner/options.go` for changing the default flag value and `runner/runner.go` for conditional logic adjustments) and involve a small number of lines. The changes primarily focus on toggling the default value of the `TechDetect` flag from `true` to `false` and ensuring that tech detection logic is also triggered for JSON and CSV outputs. This requires basic understanding of Go flag handling and conditional logic, with no complex algorithms, design patterns, or deep architectural changes involved. The scope of impact is minimal, as it does not affect the broader system architecture or require extensive refactoring. Additionally, there are no explicit edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, further reducing complexity. The primary technical concept to understand is the behavior of command-line flags and how they interact with output formatting logic, which is relatively simple for a developer familiar with Go. Overall, this is a low-effort task that can be completed with basic knowledge of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "SIGSEGV when using include and find_in_parent_folders\n## Describe the bug\n\nI've created a directory tree to manage Grafana IaC that is setup like this:\n\n```\nsystems\n\u251c\u2500\u2500 account-card\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 eng_teams.hcl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prod\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 <app>\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 qa\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 <app>\n```\n\nEach <app> sub directory has it's own `terragrunt.hcl` file, which will do the actual work.\n\nThis structure started with plain Terraform, so there are still tfvars files that I'm trying to replace with the combination of a HCL file, `include` and the `find_in_parent_folders` function.\n\nI can reference `eng_teams.hcl` like this:\n\n```hcl\ninclude {\n  path = find_in_parent_folders(\"eng_teams.hcl\")\n}\n```\n\nAnd it works as expect. But if I want to use another HCL right under `systems` directory, I get the following error:\n\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x1aa296d]\n\ngoroutine 9 [running]:\ngithub.com/gruntwork-io/terragrunt/config.PartialParseConfig(0xc000b1dc80, 0xc001333300, 0x0)\n\t/home/circleci/project/config/config_partial.go:389 +0x3cd\ngithub.com/gruntwork-io/terragrunt/config.TerragruntConfigFromPartialConfig(0xc000b1dc80, 0xc001333300, 0x0)\n\t/home/circleci/project/config/config_partial.go:326 +0x3c5\ngithub.com/gruntwork-io/terragrunt/config.PartialParseConfigFile(0xc000b1dc80, {0xc000b163f0, 0x69}, 0x0)\n\t/home/circleci/project/config/config_partial.go:304 +0x1e6\ngithub.com/gruntwork-io/terragrunt/cli/commands/run.getTerragruntConfig({0x2f44ce8, 0xc0012ddd40}, 0xc000bde008)\n\t/home/circleci/project/cli/commands/run/run.go:916 +0x105\ngithub.com/gruntwork-io/terragrunt/cli/commands/run.CheckVersionConstraints({0x2f44ce8, 0xc0012ddd40}, 0xc000bde008)\n\t/home/circleci/project/cli/commands/run/version_check.go:37 +0x2f\ngithub.com/gruntwork-io/terragrunt/cli/commands/run.run({0x2f44ce8, 0xc0012ddd40}, 0xc000bde008, 0xc00137bde8)\n\t/home/circleci/project/cli/commands/run/run.go:106 +0x186\ngithub.com/gruntwork-io/terragrunt/cli/commands/run.Run({0x2f44ce8?, 0xc0012ddd40?}, 0xc0012eb6d0?)\n\t/home/circleci/project/cli/commands/run/run.go:88 +0x56\ngithub.com/gruntwork-io/terragrunt/cli/commands/run.NewSubcommands.func1.Action.1(0xc0012eb6d0)\n\t/home/circleci/project/cli/commands/run/cli.go:82 +0x9c\ngithub.com/gruntwork-io/terragrunt/cli/commands/run.NewSubcommands.func1(0x0?)\n\t/home/circleci/project/cli/commands/run/cli.go:63 +0x2f\ngithub.com/gruntwork-io/terragrunt/cli/commands/common/runall.WrapCommand.func1(0x0?, 0x0?)\n\t/home/circleci/project/cli/commands/common/runall/cli.go:45 +0xcd\ngithub.com/gruntwork-io/terragrunt/internal/cli.(*Command).WrapAction.func1(0x0?)\n\t/home/circleci/project/internal/cli/command.go:292 +0x1b\ngithub.com/gruntwork-io/terragrunt/cli/commands/common/graph.WrapCommand.func1(0x0?, 0x0?)\n\t/home/circleci/project/cli/commands/common/graph/cli.go:40 +0xcd\ngithub.com/gruntwork-io/terragrunt/internal/cli.(*Command).WrapAction.func1(0x0?)\n\t/home/circleci/project/internal/cli/command.go:292 +0x1b\ngithub.com/gruntwork-io/terragrunt/cli/commands.NewShortcutsCommands.func1(0x0?)\n\t/home/circleci/project/cli/commands/shortcuts.go:64 +0x82\ngithub.com/gruntwork-io/terragrunt/cli.runAction.func2()\n\t/home/circleci/project/cli/app.go:224 +0x47\ngolang.org/x/sync/errgroup.(*Group).Go.func1()\n\t/home/circleci/go/pkg/mod/golang.org/x/sync@v0.12.0/errgroup/errgroup.go:78 +0x50\ncreated by golang.org/x/sync/errgroup.(*Group).Go in goroutine 1\n\t/home/circleci/go/pkg/mod/golang.org/x/sync@v0.12.0/errgroup/errgroup.go:75 +0x96\n```\n\nIt doesn't matter the filename or the content (even if it's empty) I'm using, the result is the same.\n\nThis is the line that I'm using to reference this \"global\" configuration:\n\n```hcl\ninclude {\n  path = find_in_parent_folders(\"_envcommon.hcl\")\n}\n```\n\nOnce commented out or removed, the unit works as expected.\n\nInteresting fact is that I have a tfvars (which I'm trying to replace) at the same level of `systems` and it works as expected too:\n\n```hcl\nlocals {\n  environment = jsondecode(read_tfvars_file(\"../../../../environments.tfvars.json\"))\n}\n\ninputs = merge(\n  { environment = basename(dirname(get_terragrunt_dir())),\n    app = {\n      \"kind\"      = \"deployment\",\n      \"namespace\" = \"foobar\",\n      \"slug\"      = \"barfoo\"\n    }\n  },\n  local.environment\n)\n```\n\n## Steps To Reproduce\n\n1. Create the directory tree\n2. Create a unit configuration at the application level\n3. Create the `eng_teams.hcl` at the system level. The file only defines an `input` block.\n4. Create the `_envcommon.hcl` file right under `systems`\n5. Move to the application sub directory (where the unit is) and run `terragrunt plan` or `terragrunt apply`.\n\n```hcl\nterraform {\n  source = \"../../../../base\"\n}\n\nlocals {\n  teams       = jsondecode(read_tfvars_file(\"../../../../teams.tfvars\"))\n  // the line below should replaced by the include of _envcommon.hcl\n  environment = jsondecode(read_tfvars_file(\"../../../../environments.tfvars.json\"))\n}\n\ninclude {\n  path = find_in_parent_folders(\"eng_teams.hcl\")\n}\n\n// if the line below is removed, everything works fine\ninclude {\n  path = find_in_parent_folders(\"_envcommon.hcl\")\n}\n\ninputs = merge(\n  { environment = basename(dirname(get_terragrunt_dir())),\n    app = {\n      \"kind\"      = \"deployment\",\n      \"namespace\" = \"foobar\",\n      \"slug\"      = \"barfoo\"\n    }\n  },\n  local.teams,\n  local.environment\n)\n```\n\n## Expected behavior\n\nThe selected operation from terragrunt should be executed without errors, and the shared configuration can be used all other application unit's.\n\n## Nice to haves\n\n- [X] Terminal output\n- [ ] Screenshots\n\n## Versions\n\n- Terragrunt version: v0.77.22\n- Terraform version: v1.11.3\n- Environment details: Ubuntu 22.04.5 LTS\n\n## Additional context\n\n`base` modules providers:\n\n```hcl\nterraform {\n  required_providers {\n    grafana = {\n      source  = \"grafana/grafana\"\n      version = \"3.22.2\"\n    }\n    http = {\n      source  = \"hashicorp/http\"\n      version = \"3.4.5\"\n    }\n  }\n}\n\nprovider \"grafana\" {\n  url  = \"https://${var.environments.servers[var.environment]}\"\n  auth = var.grafana_token\n}\n```\n\nInside this module, several of modules are included with `module` blocks, all of them using relative paths in the file system.\n\n", "patch": "diff --git a/config/config.go b/config/config.go\nindex 61206fcc80..2c63db0efe 100644\n--- a/config/config.go\n+++ b/config/config.go\n@@ -1288,9 +1288,11 @@ func ParseConfig(ctx *ParsingContext, file *hclparse.File, includeFromChild *Inc\n \t\terrs = errs.Append(err)\n \t}\n \n-\tctx = ctx.WithTrackInclude(baseBlocks.TrackInclude)\n-\tctx = ctx.WithFeatures(baseBlocks.FeatureFlags)\n-\tctx = ctx.WithLocals(baseBlocks.Locals)\n+\tif baseBlocks != nil {\n+\t\tctx = ctx.WithTrackInclude(baseBlocks.TrackInclude)\n+\t\tctx = ctx.WithFeatures(baseBlocks.FeatureFlags)\n+\t\tctx = ctx.WithLocals(baseBlocks.Locals)\n+\t}\n \n \tif ctx.DecodedDependencies == nil {\n \t\t// Decode just the `dependency` blocks, retrieving the outputs from the target terragrunt config in the\ndiff --git a/config/config_partial.go b/config/config_partial.go\nindex 48cd12f5d4..b0c0180972 100644\n--- a/config/config_partial.go\n+++ b/config/config_partial.go\n@@ -383,9 +383,11 @@ func PartialParseConfig(ctx *ParsingContext, file *hclparse.File, includeFromChi\n \t\terrs = errs.Append(err)\n \t}\n \n-\tctx = ctx.WithTrackInclude(baseBlocks.TrackInclude)\n-\tctx = ctx.WithFeatures(baseBlocks.FeatureFlags)\n-\tctx = ctx.WithLocals(baseBlocks.Locals)\n+\tif baseBlocks != nil {\n+\t\tctx = ctx.WithTrackInclude(baseBlocks.TrackInclude)\n+\t\tctx = ctx.WithFeatures(baseBlocks.FeatureFlags)\n+\t\tctx = ctx.WithLocals(baseBlocks.Locals)\n+\t}\n \n \t// Set parsed Locals on the parsed config\n \toutput, err := convertToTerragruntConfig(ctx, file.ConfigPath, &terragruntConfigFile{})\n", "instance_id": "gruntwork-io__terragrunt-4226", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue, including the directory structure, the specific error (SIGSEGV), and the steps to reproduce the bug. It also includes relevant code snippets and expected behavior, which help in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem statement does not fully clarify the content or purpose of the `_envcommon.hcl` file beyond mentioning it causes a crash. Additionally, while the error stack trace is provided, there is no explicit hypothesis or analysis of the root cause from the user's perspective, which could aid in understanding the issue's scope. Edge cases or specific constraints related to the `find_in_parent_folders` function are also not explicitly discussed. Overall, the statement is valid and clear but lacks some minor details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the clarity of the problem statement, while mostly good, requires the developer to infer some details about the root cause of the SIGSEGV error, which points to a nil pointer dereference in the Terragrunt codebase. The code changes provided are minimal (adding null checks for `baseBlocks` in two files), but they address a critical issue in the parsing logic of configuration files, indicating a need for deep understanding of the Terragrunt internals, specifically how `include` and `find_in_parent_folders` interact with the parsing context. The scope of the change is narrow (two files), but the impact is significant as it prevents a runtime crash, likely affecting many users with similar directory structures. The technical concepts involved include Go's error handling, pointer management, and familiarity with HCL (HashiCorp Configuration Language) parsing libraries, which are moderately complex. Additionally, understanding the Terragrunt configuration parsing flow and the interaction between parent and child configurations adds to the difficulty. While edge cases are not explicitly mentioned in the problem statement, the nature of the bug (nil pointer dereference) suggests that edge case handling (e.g., missing or invalid configuration files) is implicitly critical to the solution. Overall, solving this requires a solid grasp of the codebase architecture and careful consideration of configuration parsing logic, justifying a difficulty score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Feature Request] Customizable shell command flag for command execution\n### Checklist\n\n- [X] I have read through the manual page (`man fzf`)\n- [X] I have searched through the existing issues\n- [X] For bug reports, I have checked if the bug is reproducible in the latest version of fzf\n\n### Output of `fzf --version`\n\n0.50.0 (brew)\n\n### OS\n\n- [ ] Linux\n- [X] macOS\n- [ ] Windows\n- [ ] Etc.\n\n### Shell\n\n- [ ] bash\n- [X] zsh\n- [ ] fish\n\n### Problem / Steps to reproduce\n\n\r\nThis report was created from a discussion on a discarded PR: #3726\r\n\r\n---\r\n\r\n### Issue\r\n\r\n`fzf` currently executes commands with `$SHELL` and a hardcoded `-c` flag.\r\n\r\nhttps://github.com/junegunn/fzf/blob/f97d2754134607b24849fc4a2062dbfcaafddd6a/src/util/util_unix.go#L22-L29\r\n\r\nIf a user wants to use a different tool with a different flag to execute commands, they must prepend the desired command to each string, making proper quoting more cumbersome and leading to repetitive addition of the command.\r\n\r\n```bash\r\n# [bun] https://github.com/oven-sh/bun\r\nfzf --bind \"start:reload:bun --print \\\"Object.keys(Bun.env).sort().join('\\n')\\\"\"\r\n```\r\n\r\n---\r\n\r\n### Feature Proposal\r\n\r\nAbility to change the flag that is passed to the `$SHELL` command.\r\n\r\nIf the flag would be subject to change, then one would only need to write the string.\r\n\r\n```bash\r\n# Syntax example, not actual usage\r\nSHELL=$(which bun) FZF_SHELL_FLAG=\"--print\" \\\r\n  fzf --bind \"start:reload:Object.keys(Bun.env).sort().join('\\n')\"\r\n\r\n# Different syntax example proposed by the maintainer\r\nfzf --with-shell \"bun --print\" \\\r\n  --bind \"start:reload:Object.keys(Bun.env).sort().join('\\n')\"\r\n```\r\n\r\nHere are a few examples of tools with their corresponding flags to illustrate the variety of flags that might be used:\r\n- `jsc -e`: a short, single-letter option [^1]\r\n- `bun --print`: a long, word-based option [^2]\r\n- `osascript -l JavaScript -e`: multiple short, single-letter options and an argument [^3]\r\n\r\n---\r\n\r\n### Workaround\r\nThe maintainer provided a workaround [^4].\r\n\r\n```bash\r\nfzf2() (\r\n  export SHELL=/tmp/fzfsh\r\n  if ! [[ -x $SHELL ]]; then\r\n    echo '#!/bin/sh' > \"$SHELL\"\r\n    echo 'shift; echo \"$@\"' >> \"$SHELL\"\r\n    chmod +x \"$SHELL\"\r\n  fi\r\n  command fzf \"$@\"\r\n)\r\n\r\nfzf2 --preview 'run this command: {}'\r\n```\r\n\r\n[^1]: [JSC \u2013 WebKit](https://trac.webkit.org/wiki/JSC)\r\n[^2]: [Bun \u2014 A fast all-in-one JavaScript runtime](https://bun.sh/)\r\n[^3]: [JXA - JavaScript for Automation](https://developer.apple.com/library/archive/releasenotes/InterapplicationCommunication/RN-JavaScriptForAutomation/Articles/Introduction.html)\r\n[^4]: [PR #3726 (comment)](https://github.com/junegunn/fzf/pull/3726#issuecomment-2054007955)\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 51e9a603d24..920bceff2a3 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -3,10 +3,23 @@ CHANGELOG\n \n 0.51.0\n ------\n+- Added `--with-shell` option to start child processes with a custom shell command and flags\n+  ```sh\n+  gem list | fzf --with-shell 'ruby -e' \\\n+    --preview 'pp Gem::Specification.find_by_name({1})' \\\n+    --bind 'ctrl-o:execute-silent:\n+        spec = Gem::Specification.find_by_name({1})\n+        [spec.homepage, *spec.metadata.filter { _1.end_with?(\"uri\") }.values].uniq.each do\n+          system \"open\", _1\n+        end\n+    '\n+  ```\n - Added `change-multi` action for dynamically changing `--multi` option\n     - `change-multi` - enable multi-select mode with no limit\n     - `change-multi(NUM)` - enable multi-select mode with a limit\n     - `change-multi(0)` - disable multi-select mode\n+- `become` action is now supported on Windows\n+    - Unlike in *nix, this does not use `execve(2)`. Instead it spawns a new process and waits for it to finish, so the exact behavior may differ.\n - Bug fixes and improvements\n \n 0.50.0\ndiff --git a/man/man1/fzf.1 b/man/man1/fzf.1\nindex 458c6a5fc7f..742fba5aa43 100644\n--- a/man/man1/fzf.1\n+++ b/man/man1/fzf.1\n@@ -818,6 +818,16 @@ the finder only after the input stream is complete.\n e.g. \\fBfzf --multi | fzf --sync\\fR\n .RE\n .TP\n+.B \"--with-shell=STR\"\n+Shell command and flags to start child processes with. On *nix Systems, the\n+default value is \\fB$SHELL -c\\fR if \\fB$SHELL\\fR is set, otherwise \\fBsh -c\\fR.\n+On Windows, the default value is \\fBcmd /v:on/s/c\\fR when \\fB$SHELL\\fR is not\n+set.\n+\n+.RS\n+e.g. \\fBgem list | fzf --with-shell 'ruby -e' --preview 'pp Gem::Specification.find_by_name({1})'\\fR\n+.RE\n+.TP\n .B \"--listen[=[ADDR:]PORT]\" \"--listen-unsafe[=[ADDR:]PORT]\"\n Start HTTP server and listen on the given address. It allows external processes\n to send actions to perform via POST method.\n@@ -932,6 +942,8 @@ you need to protect against DNS rebinding and privilege escalation attacks.\n .br\n .BR 2 \"      Error\"\n .br\n+.BR 127 \"    Invalid shell command for \\fBbecome\\fR action\"\n+.br\n .BR 130 \"    Interrupted with \\fBCTRL-C\\fR or \\fBESC\\fR\"\n \n .SH FIELD INDEX EXPRESSION\n@@ -1441,8 +1453,6 @@ call.\n \n     \\fBfzf --bind \"enter:become(vim {})\"\\fR\n \n-\\fBbecome(...)\\fR is not supported on Windows.\n-\n .SS RELOAD INPUT\n \n \\fBreload(...)\\fR action is used to dynamically update the input list\ndiff --git a/src/core.go b/src/core.go\nindex ec137698bdf..14aa781f07a 100644\n--- a/src/core.go\n+++ b/src/core.go\n@@ -121,13 +121,16 @@ func Run(opts *Options, version string, revision string) {\n \t\t})\n \t}\n \n+\t// Process executor\n+\texecutor := util.NewExecutor(opts.WithShell)\n+\n \t// Reader\n \tstreamingFilter := opts.Filter != nil && !sort && !opts.Tac && !opts.Sync\n \tvar reader *Reader\n \tif !streamingFilter {\n \t\treader = NewReader(func(data []byte) bool {\n \t\t\treturn chunkList.Push(data)\n-\t\t}, eventBox, opts.ReadZero, opts.Filter == nil)\n+\t\t}, eventBox, executor, opts.ReadZero, opts.Filter == nil)\n \t\tgo reader.ReadSource(opts.WalkerRoot, opts.WalkerOpts, opts.WalkerSkip)\n \t}\n \n@@ -178,7 +181,7 @@ func Run(opts *Options, version string, revision string) {\n \t\t\t\t\t\tmutex.Unlock()\n \t\t\t\t\t}\n \t\t\t\t\treturn false\n-\t\t\t\t}, eventBox, opts.ReadZero, false)\n+\t\t\t\t}, eventBox, executor, opts.ReadZero, false)\n \t\t\treader.ReadSource(opts.WalkerRoot, opts.WalkerOpts, opts.WalkerSkip)\n \t\t} else {\n \t\t\teventBox.Unwatch(EvtReadNew)\n@@ -209,7 +212,7 @@ func Run(opts *Options, version string, revision string) {\n \tgo matcher.Loop()\n \n \t// Terminal I/O\n-\tterminal := NewTerminal(opts, eventBox)\n+\tterminal := NewTerminal(opts, eventBox, executor)\n \tmaxFit := 0 // Maximum number of items that can fit on screen\n \tpadHeight := 0\n \theightUnknown := opts.Height.auto\ndiff --git a/src/options.go b/src/options.go\nindex c8a3fa15c3a..66e0554e66b 100644\n--- a/src/options.go\n+++ b/src/options.go\n@@ -120,6 +120,7 @@ const usage = `usage: fzf [options]\n     --read0                Read input delimited by ASCII NUL characters\n     --print0               Print output delimited by ASCII NUL characters\n     --sync                 Synchronous search for multi-staged filtering\n+    --with-shell=STR       Shell command and flags to start child processes with\n     --listen[=[ADDR:]PORT] Start HTTP server to receive actions (POST /)\n                            (To allow remote process execution, use --listen-unsafe)\n     --version              Display version information and exit\n@@ -356,6 +357,7 @@ type Options struct {\n \tUnicode      bool\n \tAmbidouble   bool\n \tTabstop      int\n+\tWithShell    string\n \tListenAddr   *listenAddress\n \tUnsafe       bool\n \tClearOnExit  bool\n@@ -1327,10 +1329,6 @@ func parseActionList(masked string, original string, prevActions []*action, putA\n \t\t\t\t\tactions = append(actions, &action{t: t, a: actionArg})\n \t\t\t\t}\n \t\t\t\tswitch t {\n-\t\t\t\tcase actBecome:\n-\t\t\t\t\tif util.IsWindows() {\n-\t\t\t\t\t\texit(\"become action is not supported on Windows\")\n-\t\t\t\t\t}\n \t\t\t\tcase actUnbind, actRebind:\n \t\t\t\t\tparseKeyChordsImpl(actionArg, spec[0:offset]+\" target required\", exit)\n \t\t\t\tcase actChangePreviewWindow:\n@@ -1957,6 +1955,8 @@ func parseOptions(opts *Options, allArgs []string) {\n \t\t\t\tnextString(allArgs, &i, \"padding required (TRBL / TB,RL / T,RL,B / T,R,B,L)\"))\n \t\tcase \"--tabstop\":\n \t\t\topts.Tabstop = nextInt(allArgs, &i, \"tab stop required\")\n+\t\tcase \"--with-shell\":\n+\t\t\topts.WithShell = nextString(allArgs, &i, \"shell command and flags required\")\n \t\tcase \"--listen\", \"--listen-unsafe\":\n \t\t\tgiven, str := optionalNextString(allArgs, &i)\n \t\t\taddr := defaultListenAddr\n@@ -2073,6 +2073,8 @@ func parseOptions(opts *Options, allArgs []string) {\n \t\t\t\topts.Padding = parseMargin(\"padding\", value)\n \t\t\t} else if match, value := optString(arg, \"--tabstop=\"); match {\n \t\t\t\topts.Tabstop = atoi(value)\n+\t\t\t} else if match, value := optString(arg, \"--with-shell=\"); match {\n+\t\t\t\topts.WithShell = value\n \t\t\t} else if match, value := optString(arg, \"--listen=\"); match {\n \t\t\t\taddr, err := parseListenAddress(value)\n \t\t\t\tif err != nil {\ndiff --git a/src/reader.go b/src/reader.go\nindex 82648a6800e..8fa864e749c 100644\n--- a/src/reader.go\n+++ b/src/reader.go\n@@ -18,6 +18,7 @@ import (\n // Reader reads from command or standard input\n type Reader struct {\n \tpusher   func([]byte) bool\n+\texecutor *util.Executor\n \teventBox *util.EventBox\n \tdelimNil bool\n \tevent    int32\n@@ -30,8 +31,8 @@ type Reader struct {\n }\n \n // NewReader returns new Reader object\n-func NewReader(pusher func([]byte) bool, eventBox *util.EventBox, delimNil bool, wait bool) *Reader {\n-\treturn &Reader{pusher, eventBox, delimNil, int32(EvtReady), make(chan bool, 1), sync.Mutex{}, nil, nil, false, wait}\n+func NewReader(pusher func([]byte) bool, eventBox *util.EventBox, executor *util.Executor, delimNil bool, wait bool) *Reader {\n+\treturn &Reader{pusher, executor, eventBox, delimNil, int32(EvtReady), make(chan bool, 1), sync.Mutex{}, nil, nil, false, wait}\n }\n \n func (r *Reader) startEventPoller() {\n@@ -242,7 +243,7 @@ func (r *Reader) readFromCommand(command string, environ []string) bool {\n \tr.mutex.Lock()\n \tr.killed = false\n \tr.command = &command\n-\tr.exec = util.ExecCommand(command, true)\n+\tr.exec = r.executor.ExecCommand(command, true)\n \tif environ != nil {\n \t\tr.exec.Env = environ\n \t}\ndiff --git a/src/terminal.go b/src/terminal.go\nindex 25f301504fa..8d114e1b3ea 100644\n--- a/src/terminal.go\n+++ b/src/terminal.go\n@@ -7,7 +7,6 @@ import (\n \t\"io\"\n \t\"math\"\n \t\"os\"\n-\t\"os/exec\"\n \t\"os/signal\"\n \t\"regexp\"\n \t\"sort\"\n@@ -245,6 +244,7 @@ type Terminal struct {\n \tlistenUnsafe       bool\n \tborderShape        tui.BorderShape\n \tcleanExit          bool\n+\texecutor           *util.Executor\n \tpaused             bool\n \tborder             tui.Window\n \twindow             tui.Window\n@@ -640,7 +640,7 @@ func evaluateHeight(opts *Options, termHeight int) int {\n }\n \n // NewTerminal returns new Terminal object\n-func NewTerminal(opts *Options, eventBox *util.EventBox) *Terminal {\n+func NewTerminal(opts *Options, eventBox *util.EventBox, executor *util.Executor) *Terminal {\n \tinput := trimQuery(opts.Query)\n \tvar delay time.Duration\n \tif opts.Tac {\n@@ -736,6 +736,7 @@ func NewTerminal(opts *Options, eventBox *util.EventBox) *Terminal {\n \t\tpreviewLabel:       nil,\n \t\tpreviewLabelOpts:   opts.PreviewLabel,\n \t\tcleanExit:          opts.ClearOnExit,\n+\t\texecutor:           executor,\n \t\tpaused:             opts.Phony,\n \t\tcycle:              opts.Cycle,\n \t\theaderVisible:      true,\n@@ -2522,6 +2523,7 @@ type replacePlaceholderParams struct {\n \tallItems   []*Item\n \tlastAction actionType\n \tprompt     string\n+\texecutor   *util.Executor\n }\n \n func (t *Terminal) replacePlaceholder(template string, forcePlus bool, input string, list []*Item) string {\n@@ -2535,6 +2537,7 @@ func (t *Terminal) replacePlaceholder(template string, forcePlus bool, input str\n \t\tallItems:   list,\n \t\tlastAction: t.lastAction,\n \t\tprompt:     t.promptString,\n+\t\texecutor:   t.executor,\n \t})\n }\n \n@@ -2595,7 +2598,7 @@ func replacePlaceholder(params replacePlaceholderParams) string {\n \t\tcase escaped:\n \t\t\treturn match\n \t\tcase match == \"{q}\" || match == \"{fzf:query}\":\n-\t\t\treturn quoteEntry(params.query)\n+\t\t\treturn params.executor.QuoteEntry(params.query)\n \t\tcase match == \"{}\":\n \t\t\treplace = func(item *Item) string {\n \t\t\t\tswitch {\n@@ -2608,13 +2611,13 @@ func replacePlaceholder(params replacePlaceholderParams) string {\n \t\t\t\tcase flags.file:\n \t\t\t\t\treturn item.AsString(params.stripAnsi)\n \t\t\t\tdefault:\n-\t\t\t\t\treturn quoteEntry(item.AsString(params.stripAnsi))\n+\t\t\t\t\treturn params.executor.QuoteEntry(item.AsString(params.stripAnsi))\n \t\t\t\t}\n \t\t\t}\n \t\tcase match == \"{fzf:action}\":\n \t\t\treturn params.lastAction.Name()\n \t\tcase match == \"{fzf:prompt}\":\n-\t\t\treturn quoteEntry(params.prompt)\n+\t\t\treturn params.executor.QuoteEntry(params.prompt)\n \t\tdefault:\n \t\t\t// token type and also failover (below)\n \t\t\trangeExpressions := strings.Split(match[1:len(match)-1], \",\")\n@@ -2648,7 +2651,7 @@ func replacePlaceholder(params replacePlaceholderParams) string {\n \t\t\t\t\tstr = strings.TrimSpace(str)\n \t\t\t\t}\n \t\t\t\tif !flags.file {\n-\t\t\t\t\tstr = quoteEntry(str)\n+\t\t\t\t\tstr = params.executor.QuoteEntry(str)\n \t\t\t\t}\n \t\t\t\treturn str\n \t\t\t}\n@@ -2688,7 +2691,7 @@ func (t *Terminal) executeCommand(template string, forcePlus bool, background bo\n \t\treturn line\n \t}\n \tcommand := t.replacePlaceholder(template, forcePlus, string(t.input), list)\n-\tcmd := util.ExecCommand(command, false)\n+\tcmd := t.executor.ExecCommand(command, false)\n \tcmd.Env = t.environ()\n \tt.executing.Set(true)\n \tif !background {\n@@ -2965,7 +2968,7 @@ func (t *Terminal) Loop() {\n \t\t\t\tif items[0] != nil {\n \t\t\t\t\t_, query := t.Input()\n \t\t\t\t\tcommand := t.replacePlaceholder(commandTemplate, false, string(query), items)\n-\t\t\t\t\tcmd := util.ExecCommand(command, true)\n+\t\t\t\t\tcmd := t.executor.ExecCommand(command, true)\n \t\t\t\t\tenv := t.environ()\n \t\t\t\t\tif pwindowSize.Lines > 0 {\n \t\t\t\t\t\tlines := fmt.Sprintf(\"LINES=%d\", pwindowSize.Lines)\n@@ -3372,27 +3375,21 @@ func (t *Terminal) Loop() {\n \t\t\t\tvalid, list := t.buildPlusList(a.a, false)\n \t\t\t\tif valid {\n \t\t\t\t\tcommand := t.replacePlaceholder(a.a, false, string(t.input), list)\n-\t\t\t\t\tshell := os.Getenv(\"SHELL\")\n-\t\t\t\t\tif len(shell) == 0 {\n-\t\t\t\t\t\tshell = \"sh\"\n-\t\t\t\t\t}\n-\t\t\t\t\tshellPath, err := exec.LookPath(shell)\n-\t\t\t\t\tif err == nil {\n-\t\t\t\t\t\tt.tui.Close()\n-\t\t\t\t\t\tif t.history != nil {\n-\t\t\t\t\t\t\tt.history.append(string(t.input))\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\t/*\n-\t\t\t\t\t\t\tFIXME: It is not at all clear why this is required.\n-\t\t\t\t\t\t\tThe following command will report 'not a tty', unless we open\n-\t\t\t\t\t\t\t/dev/tty *twice* after closing the standard input for 'reload'\n-\t\t\t\t\t\t\tin Reader.terminate().\n-\t\t\t\t\t\t\t\t: | fzf --bind 'start:reload:ls' --bind 'enter:become:tty'\n-\t\t\t\t\t\t*/\n-\t\t\t\t\t\ttui.TtyIn()\n-\t\t\t\t\t\tutil.SetStdin(tui.TtyIn())\n-\t\t\t\t\t\tsyscall.Exec(shellPath, []string{shell, \"-c\", command}, os.Environ())\n+\t\t\t\t\tt.tui.Close()\n+\t\t\t\t\tif t.history != nil {\n+\t\t\t\t\t\tt.history.append(string(t.input))\n \t\t\t\t\t}\n+\n+\t\t\t\t\t/*\n+\t\t\t\t\t FIXME: It is not at all clear why this is required.\n+\t\t\t\t\t The following command will report 'not a tty', unless we open\n+\t\t\t\t\t /dev/tty *twice* after closing the standard input for 'reload'\n+\t\t\t\t\t in Reader.terminate().\n+\n+\t\t\t\t\t   while : | fzf --bind 'start:reload:ls' --bind 'load:become:tty'; do echo; done\n+\t\t\t\t\t*/\n+\t\t\t\t\ttui.TtyIn()\n+\t\t\t\t\tt.executor.Become(tui.TtyIn(), t.environ(), command)\n \t\t\t\t}\n \t\t\tcase actExecute, actExecuteSilent:\n \t\t\t\tt.executeCommand(a.a, false, a.t == actExecuteSilent, false, false)\ndiff --git a/src/terminal_unix.go b/src/terminal_unix.go\nindex c7fa7f12b48..d0b00f2f4e4 100644\n--- a/src/terminal_unix.go\n+++ b/src/terminal_unix.go\n@@ -5,26 +5,11 @@ package fzf\n import (\n \t\"os\"\n \t\"os/signal\"\n-\t\"strings\"\n \t\"syscall\"\n \n \t\"golang.org/x/sys/unix\"\n )\n \n-var escaper *strings.Replacer\n-\n-func init() {\n-\ttokens := strings.Split(os.Getenv(\"SHELL\"), \"/\")\n-\tif tokens[len(tokens)-1] == \"fish\" {\n-\t\t// https://fishshell.com/docs/current/language.html#quotes\n-\t\t// > The only meaningful escape sequences in single quotes are \\', which\n-\t\t// > escapes a single quote and \\\\, which escapes the backslash symbol.\n-\t\tescaper = strings.NewReplacer(\"\\\\\", \"\\\\\\\\\", \"'\", \"\\\\'\")\n-\t} else {\n-\t\tescaper = strings.NewReplacer(\"'\", \"'\\\\''\")\n-\t}\n-}\n-\n func notifyOnResize(resizeChan chan<- os.Signal) {\n \tsignal.Notify(resizeChan, syscall.SIGWINCH)\n }\n@@ -41,7 +26,3 @@ func notifyStop(p *os.Process) {\n func notifyOnCont(resizeChan chan<- os.Signal) {\n \tsignal.Notify(resizeChan, syscall.SIGCONT)\n }\n-\n-func quoteEntry(entry string) string {\n-\treturn \"'\" + escaper.Replace(entry) + \"'\"\n-}\ndiff --git a/src/terminal_windows.go b/src/terminal_windows.go\nindex a1ea7a22d23..112cd68d7e3 100644\n--- a/src/terminal_windows.go\n+++ b/src/terminal_windows.go\n@@ -4,8 +4,6 @@ package fzf\n \n import (\n \t\"os\"\n-\t\"regexp\"\n-\t\"strings\"\n )\n \n func notifyOnResize(resizeChan chan<- os.Signal) {\n@@ -19,27 +17,3 @@ func notifyStop(p *os.Process) {\n func notifyOnCont(resizeChan chan<- os.Signal) {\n \t// NOOP\n }\n-\n-func quoteEntry(entry string) string {\n-\tshell := os.Getenv(\"SHELL\")\n-\tif len(shell) == 0 {\n-\t\tshell = \"cmd\"\n-\t}\n-\n-\tif strings.Contains(shell, \"cmd\") {\n-\t\t// backslash escaping is done here for applications\n-\t\t// (see ripgrep test case in terminal_test.go#TestWindowsCommands)\n-\t\tescaped := strings.Replace(entry, `\\`, `\\\\`, -1)\n-\t\tescaped = `\"` + strings.Replace(escaped, `\"`, `\\\"`, -1) + `\"`\n-\t\t// caret is the escape character for cmd shell\n-\t\tr, _ := regexp.Compile(`[&|<>()@^%!\"]`)\n-\t\treturn r.ReplaceAllStringFunc(escaped, func(match string) string {\n-\t\t\treturn \"^\" + match\n-\t\t})\n-\t} else if strings.Contains(shell, \"pwsh\") || strings.Contains(shell, \"powershell\") {\n-\t\tescaped := strings.Replace(entry, `\"`, `\\\"`, -1)\n-\t\treturn \"'\" + strings.Replace(escaped, \"'\", \"''\", -1) + \"'\"\n-\t} else {\n-\t\treturn \"'\" + strings.Replace(entry, \"'\", \"'\\\\''\", -1) + \"'\"\n-\t}\n-}\ndiff --git a/src/util/util_unix.go b/src/util/util_unix.go\nindex 2991fd2cf36..4410a9bf12d 100644\n--- a/src/util/util_unix.go\n+++ b/src/util/util_unix.go\n@@ -3,31 +3,71 @@\n package util\n \n import (\n+\t\"fmt\"\n \t\"os\"\n \t\"os/exec\"\n+\t\"strings\"\n \t\"syscall\"\n \n \t\"golang.org/x/sys/unix\"\n )\n \n-// ExecCommand executes the given command with $SHELL\n-func ExecCommand(command string, setpgid bool) *exec.Cmd {\n+type Executor struct {\n+\tshell   string\n+\targs    []string\n+\tescaper *strings.Replacer\n+}\n+\n+func NewExecutor(withShell string) *Executor {\n \tshell := os.Getenv(\"SHELL\")\n-\tif len(shell) == 0 {\n-\t\tshell = \"sh\"\n+\targs := strings.Fields(withShell)\n+\tif len(args) > 0 {\n+\t\tshell = args[0]\n+\t\targs = args[1:]\n+\t} else {\n+\t\tif len(shell) == 0 {\n+\t\t\tshell = \"sh\"\n+\t\t}\n+\t\targs = []string{\"-c\"}\n \t}\n-\treturn ExecCommandWith(shell, command, setpgid)\n+\n+\tvar escaper *strings.Replacer\n+\ttokens := strings.Split(shell, \"/\")\n+\tif tokens[len(tokens)-1] == \"fish\" {\n+\t\t// https://fishshell.com/docs/current/language.html#quotes\n+\t\t// > The only meaningful escape sequences in single quotes are \\', which\n+\t\t// > escapes a single quote and \\\\, which escapes the backslash symbol.\n+\t\tescaper = strings.NewReplacer(\"\\\\\", \"\\\\\\\\\", \"'\", \"\\\\'\")\n+\t} else {\n+\t\tescaper = strings.NewReplacer(\"'\", \"'\\\\''\")\n+\t}\n+\treturn &Executor{shell, args, escaper}\n }\n \n-// ExecCommandWith executes the given command with the specified shell\n-func ExecCommandWith(shell string, command string, setpgid bool) *exec.Cmd {\n-\tcmd := exec.Command(shell, \"-c\", command)\n+// ExecCommand executes the given command with $SHELL\n+func (x *Executor) ExecCommand(command string, setpgid bool) *exec.Cmd {\n+\tcmd := exec.Command(x.shell, append(x.args, command)...)\n \tif setpgid {\n \t\tcmd.SysProcAttr = &syscall.SysProcAttr{Setpgid: true}\n \t}\n \treturn cmd\n }\n \n+func (x *Executor) QuoteEntry(entry string) string {\n+\treturn \"'\" + x.escaper.Replace(entry) + \"'\"\n+}\n+\n+func (x *Executor) Become(stdin *os.File, environ []string, command string) {\n+\tshellPath, err := exec.LookPath(x.shell)\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"fzf (become): %s\\n\", err.Error())\n+\t\tExit(127)\n+\t}\n+\targs := append([]string{shellPath}, append(x.args, command)...)\n+\tSetStdin(stdin)\n+\tsyscall.Exec(shellPath, args, environ)\n+}\n+\n // KillCommand kills the process for the given command\n func KillCommand(cmd *exec.Cmd) error {\n \treturn syscall.Kill(-cmd.Process.Pid, syscall.SIGKILL)\ndiff --git a/src/util/util_windows.go b/src/util/util_windows.go\nindex aa69b99dbfe..cbaa8ce0f56 100644\n--- a/src/util/util_windows.go\n+++ b/src/util/util_windows.go\n@@ -6,60 +6,102 @@ import (\n \t\"fmt\"\n \t\"os\"\n \t\"os/exec\"\n+\t\"regexp\"\n \t\"strings\"\n \t\"sync/atomic\"\n \t\"syscall\"\n )\n \n-var shellPath atomic.Value\n+type Executor struct {\n+\tshell     string\n+\targs      []string\n+\tshellPath atomic.Value\n+}\n+\n+func NewExecutor(withShell string) *Executor {\n+\tshell := os.Getenv(\"SHELL\")\n+\targs := strings.Fields(withShell)\n+\tif len(args) > 0 {\n+\t\tshell = args[0]\n+\t} else if len(shell) == 0 {\n+\t\tshell = \"cmd\"\n+\t}\n+\n+\tif len(args) > 0 {\n+\t\targs = args[1:]\n+\t} else if strings.Contains(shell, \"cmd\") {\n+\t\targs = []string{\"/v:on/s/c\"}\n+\t} else if strings.Contains(shell, \"pwsh\") || strings.Contains(shell, \"powershell\") {\n+\t\targs = []string{\"-NoProfile\", \"-Command\"}\n+\t} else {\n+\t\targs = []string{\"-c\"}\n+\t}\n+\treturn &Executor{shell: shell, args: args}\n+}\n \n // ExecCommand executes the given command with $SHELL\n-func ExecCommand(command string, setpgid bool) *exec.Cmd {\n-\tvar shell string\n-\tif cached := shellPath.Load(); cached != nil {\n+// FIXME: setpgid is unused. We set it in the Unix implementation so that we\n+// can kill preview process with its child processes at once.\n+// NOTE: For \"powershell\", we should ideally set output encoding to UTF8,\n+// but it is left as is now because no adverse effect has been observed.\n+func (x *Executor) ExecCommand(command string, setpgid bool) *exec.Cmd {\n+\tshell := x.shell\n+\tif cached := x.shellPath.Load(); cached != nil {\n \t\tshell = cached.(string)\n \t} else {\n-\t\tshell = os.Getenv(\"SHELL\")\n-\t\tif len(shell) == 0 {\n-\t\t\tshell = \"cmd\"\n-\t\t} else if strings.Contains(shell, \"/\") {\n+\t\tif strings.Contains(shell, \"/\") {\n \t\t\tout, err := exec.Command(\"cygpath\", \"-w\", shell).Output()\n \t\t\tif err == nil {\n \t\t\t\tshell = strings.Trim(string(out), \"\\n\")\n \t\t\t}\n \t\t}\n-\t\tshellPath.Store(shell)\n+\t\tx.shellPath.Store(shell)\n \t}\n-\treturn ExecCommandWith(shell, command, setpgid)\n+\tcmd := exec.Command(shell, append(x.args, command)...)\n+\tcmd.SysProcAttr = &syscall.SysProcAttr{\n+\t\tHideWindow:    false,\n+\t\tCreationFlags: 0,\n+\t}\n+\treturn cmd\n }\n \n-// ExecCommandWith executes the given command with the specified shell\n-// FIXME: setpgid is unused. We set it in the Unix implementation so that we\n-// can kill preview process with its child processes at once.\n-// NOTE: For \"powershell\", we should ideally set output encoding to UTF8,\n-// but it is left as is now because no adverse effect has been observed.\n-func ExecCommandWith(shell string, command string, setpgid bool) *exec.Cmd {\n-\tvar cmd *exec.Cmd\n-\tif strings.Contains(shell, \"cmd\") {\n-\t\tcmd = exec.Command(shell)\n-\t\tcmd.SysProcAttr = &syscall.SysProcAttr{\n-\t\t\tHideWindow:    false,\n-\t\t\tCmdLine:       fmt.Sprintf(` /v:on/s/c \"%s\"`, command),\n-\t\t\tCreationFlags: 0,\n+func (x *Executor) Become(stdin *os.File, environ []string, command string) {\n+\tcmd := x.ExecCommand(command, false)\n+\tcmd.Stdin = stdin\n+\tcmd.Stdout = os.Stdout\n+\tcmd.Stderr = os.Stderr\n+\tcmd.Env = environ\n+\terr := cmd.Start()\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"fzf (become): %s\\n\", err.Error())\n+\t\tExit(127)\n+\t}\n+\terr = cmd.Wait()\n+\tif err != nil {\n+\t\tif exitError, ok := err.(*exec.ExitError); ok {\n+\t\t\tExit(exitError.ExitCode())\n \t\t}\n-\t\treturn cmd\n \t}\n+\tExit(0)\n+}\n \n-\tif strings.Contains(shell, \"pwsh\") || strings.Contains(shell, \"powershell\") {\n-\t\tcmd = exec.Command(shell, \"-NoProfile\", \"-Command\", command)\n+func (x *Executor) QuoteEntry(entry string) string {\n+\tif strings.Contains(x.shell, \"cmd\") {\n+\t\t// backslash escaping is done here for applications\n+\t\t// (see ripgrep test case in terminal_test.go#TestWindowsCommands)\n+\t\tescaped := strings.Replace(entry, `\\`, `\\\\`, -1)\n+\t\tescaped = `\"` + strings.Replace(escaped, `\"`, `\\\"`, -1) + `\"`\n+\t\t// caret is the escape character for cmd shell\n+\t\tr, _ := regexp.Compile(`[&|<>()@^%!\"]`)\n+\t\treturn r.ReplaceAllStringFunc(escaped, func(match string) string {\n+\t\t\treturn \"^\" + match\n+\t\t})\n+\t} else if strings.Contains(x.shell, \"pwsh\") || strings.Contains(x.shell, \"powershell\") {\n+\t\tescaped := strings.Replace(entry, `\"`, `\\\"`, -1)\n+\t\treturn \"'\" + strings.Replace(escaped, \"'\", \"''\", -1) + \"'\"\n \t} else {\n-\t\tcmd = exec.Command(shell, \"-c\", command)\n-\t}\n-\tcmd.SysProcAttr = &syscall.SysProcAttr{\n-\t\tHideWindow:    false,\n-\t\tCreationFlags: 0,\n+\t\treturn \"'\" + strings.Replace(entry, \"'\", \"'\\\\''\", -1) + \"'\"\n \t}\n-\treturn cmd\n }\n \n // KillCommand kills the process for the given command\n", "instance_id": "junegunn__fzf-3746", "clarity": 3, "difficulty": 0.65, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly defines the goal of adding a customizable shell command flag for command execution in the `fzf` tool. The input and output expectations are implicitly clear through the provided examples and syntax proposals. Constraints and context are well-articulated, with references to specific tools and their flags (e.g., `bun --print`, `jsc -e`), illustrating the variety of use cases. The statement includes a detailed description of the current issue (hardcoded `-c` flag), the proposed feature, and even a workaround provided by the maintainer. Additionally, it links to a related PR discussion for further context. There are no significant ambiguities, and the examples provided make the requirements explicit. The only minor omission is the lack of explicit mention of edge cases or error handling in the problem statement itself, though these are partially addressed in the code changes (e.g., error handling for shell path lookup). Overall, the clarity is excellent, warranting a score of 3.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files (`core.go`, `options.go`, `reader.go`, `terminal.go`, `util_unix.go`, `util_windows.go`, etc.) and affecting core components of the `fzf` tool, such as command execution and shell interaction. This requires a deep understanding of the codebase architecture, particularly how shell commands are executed and how environment variables are handled across Unix and Windows platforms. Second, the technical concepts involved are moderately complex, including Go's `exec` package, system calls (`syscall.Exec` on Unix, process spawning on Windows), string escaping for different shells (`fish`, `cmd`, `powershell`), and environment variable handling. Third, the changes impact critical functionality (command execution), necessitating careful handling of edge cases, such as invalid shell paths, shell-specific quoting rules, and platform-specific behaviors (e.g., Windows not supporting `execve` for the `become` action). The code changes also introduce a new `Executor` struct to abstract shell interactions, which adds to the architectural complexity. While the problem does not require advanced domain-specific knowledge beyond shell and process management, the cross-platform considerations and the need to maintain backward compatibility (e.g., defaulting to `$SHELL -c`) increase the challenge. A score of 0.65 reflects the need for a solid grasp of Go, system programming, and the `fzf` codebase, along with careful testing to ensure robustness across different environments.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "gui: Address override not respected in fresh default config\n### What happened?\n\nWhen generating a new `config.xml` file with default options, the GUI address is populated with a hard-coded default value of `127.0.0.1:8384`, except for a random free port if that default one is occupied.  This ignores any `STGUIADDRESS` override given via environment variable or command-line option, thus probing for the default port instead of the one specified via override.\r\n\r\nWhen not probing for free ports (`--skip-port-probing`), the override should still be persisted rather than the default address.  Only when generating a fresh default `config.xml`, never on an existing one.\n\n### Syncthing version\n\n1.28.0\n\n### Platform & operating system\n\nLinux amd64\n\n### Browser version\n\n_No response_\n\n### Relevant log output\n\n_No response_\n", "patch": "diff --git a/lib/config/config.go b/lib/config/config.go\nindex 550a485f3c6..44f8a3f95f4 100644\n--- a/lib/config/config.go\n+++ b/lib/config/config.go\n@@ -49,7 +49,6 @@ var (\n \t\t\"dynamic+https://relays.syncthing.net/endpoint\",\n \t\tnetutil.AddressURL(\"quic\", net.JoinHostPort(\"0.0.0.0\", strconv.Itoa(DefaultQUICPort))),\n \t}\n-\tDefaultGUIPort = 8384\n \t// DefaultDiscoveryServersV4 should be substituted when the configuration\n \t// contains <globalAnnounceServer>default-v4</globalAnnounceServer>.\n \tDefaultDiscoveryServersV4 = []string{\n@@ -116,11 +115,19 @@ func New(myID protocol.DeviceID) Configuration {\n }\n \n func (cfg *Configuration) ProbeFreePorts() error {\n-\tport, err := getFreePort(\"127.0.0.1\", DefaultGUIPort)\n+\tguiHost, guiPort, err := net.SplitHostPort(cfg.GUI.Address())\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"get default port (GUI): %w\", err)\n+\t}\n+\tport, err := strconv.Atoi(guiPort)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"convert default port (GUI): %w\", err)\n+\t}\n+\tport, err = getFreePort(guiHost, port)\n \tif err != nil {\n \t\treturn fmt.Errorf(\"get free port (GUI): %w\", err)\n \t}\n-\tcfg.GUI.RawAddress = fmt.Sprintf(\"127.0.0.1:%d\", port)\n+\tcfg.GUI.RawAddress = net.JoinHostPort(guiHost, strconv.Itoa(port))\n \n \tport, err = getFreePort(\"0.0.0.0\", DefaultTCPPort)\n \tif err != nil {\ndiff --git a/lib/syncthing/utils.go b/lib/syncthing/utils.go\nindex 4971da05072..15195c33444 100644\n--- a/lib/syncthing/utils.go\n+++ b/lib/syncthing/utils.go\n@@ -63,6 +63,8 @@ func DefaultConfig(path string, myID protocol.DeviceID, evLogger events.Logger,\n \n \tif skipPortProbing {\n \t\tl.Infoln(\"Using default network port numbers instead of probing for free ports\")\n+\t\t// Record address override initially\n+\t\tnewCfg.GUI.RawAddress = newCfg.GUI.Address()\n \t} else if err := newCfg.ProbeFreePorts(); err != nil {\n \t\treturn nil, err\n \t}\n", "instance_id": "syncthing__syncthing-9675", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the GUI address in a newly generated `config.xml` file does not respect overrides provided via environment variables or command-line options, instead defaulting to a hard-coded value or probing for a free port. The goal is evident\u2014ensure that the override is respected when generating a fresh default configuration. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define the expected behavior when port probing is disabled (`--skip-port-probing`) beyond \"persisting the override,\" nor does it clarify how the override should interact with existing configurations (though it mentions it should not affect them). Additionally, edge cases such as invalid override values or conflicts with other configuration settings are not addressed. Despite these minor gaps, the problem's intent and context are sufficiently clear for a developer familiar with the codebase to proceed with a solution.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, affecting two files (`config.go` and `utils.go`) within the `lib` directory. The modifications involve updating how the GUI address is handled during configuration initialization and port probing. The changes are not extensive\u2014primarily adjusting logic to parse and respect the provided GUI address rather than hard-coding a default. There is no significant impact on the broader system architecture, as the fix is confined to configuration handling.\n\n2. **Technical Concepts Involved:** Solving this requires a basic understanding of Go's standard library (e.g., `net` for host-port splitting, `strconv` for string-to-integer conversion) and familiarity with the project's configuration structure. The concepts are straightforward\u2014parsing addresses, handling port probing, and updating struct fields. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic networking are needed.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes introduce basic error handling for parsing the GUI address (e.g., `net.SplitHostPort` and `strconv.Atoi`). These are simple and standard checks. Potential edge cases like invalid override formats or port conflicts are not addressed in the provided diff, but they do not appear to significantly increase the complexity of the solution.\n\n4. **Overall Complexity:** The task requires understanding the existing logic for port probing and configuration initialization, but the modifications are relatively simple\u2014replacing hard-coded defaults with user-provided values and ensuring they are persisted. The problem does not demand deep knowledge of the codebase beyond the affected modules or complex refactoring.\n\nA score of 0.35 reflects that while the problem is slightly more involved than a trivial fix (e.g., changing a constant), it remains within the realm of an easy bug fix that a developer with moderate Go experience can handle without significant challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Compression breaks transmission of files with incorrect Content-Type\n### Welcome!\n\n- [x] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [x] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nI have a backend service that returns incorrect `Content-Type` headers with double charset:\n\n```\nHTTP/1.1 200 OK\nContent-Disposition: inline; filename=main.js\nContent-Type: text/javascript; charset=UTF-8; charset=utf-8\nContent-Length: 31340\nLast-Modified: Sat, 16 Nov 2024 06:21:28 GMT\nCache-Control: no-cache\nETag: \"1731738088.0-31340-228396532\"\nDate: Sat, 08 Feb 2025 12:21:10 GMT\nContent-Security-Policy: default-src 'self' 'unsafe-inline' 'unsafe-eval'; font-src 'self' data:; img-src 'self' data:; object-src 'none';\nX-Content-Type-Options: nosniff\nX-Frame-Options: SAMEORIGIN\nX-XSS-Protection: 1; mode=block\nStrict-Transport-Security: max-age=31536000\n```\n\nWhen accessing the service through Traefik v3.3.3 with the compression middleware, I fail to download any javascript file when using compression.\n\nDisabling the compression middleware fixes the problem.\n\nI understand that the backend service is using a malformed `Content-Type`, but the compression middleware should be more robust when handling these cases for example by skipping compression.\n\nSee the related bug report of the backend service: [https://github.com/crocodilestick/Calibre-Web-Automated/issues/217](https://github.com/crocodilestick/Calibre-Web-Automated/issues/217)\n\n### What did you see instead?\n\nWhen using compression, download of javascript files fail:\n\n```\n$ curl -kv --compressed   https://cwa.example.com/static/js/main.js\n\n> GET /static/js/main.js HTTP/2\n> Host: cwa.example.com\n> User-Agent: curl/8.9.1\n> Accept: */*\n> Accept-Encoding: deflate, gzip, br\n> \n* Request completely sent off\n* HTTP/2 stream 1 was not closed cleanly: INTERNAL_ERROR (err 2)\n* Connection #0 to host cwa.example.com left intact\ncurl: (92) HTTP/2 stream 1 was not closed cleanly: INTERNAL_ERROR (err 2)\n```\n\n\n### What version of Traefik are you using?\n\n```\nVersion:      3.3.3\nCodename:     saintnectaire\nGo version:   go1.23.5\nBuilt:        2025-01-31T14:55:01Z\nOS/Arch:      linux/amd64\n```\n\n### What is your environment & configuration?\n\nstatic conf:\n\n```yaml\nglobal:\n  checkNewVersion: true\n  sendAnonymousUsage: false\n\nentryPoints:\n  web:\n    address: :80\n  websecure:\n    address: :443\n    http3: {}\n    transport:\n      respondingTimeouts:\n        readTimeout: 300\n  traefik:\n    address: :8080\n\nlog:\n  level: INFO\n\naccessLog:\n  filePath: /var/log/traefik/access.log\n\napi: {}\n\nping: {}\n\nmetrics:\n  prometheus: {}\n\nproviders:\n  file:\n    directory: /etc/traefik/conf.d\n    watch: true\n  docker:\n    exposedByDefault: false\n    endpoint: 'tcp://podman-socket-proxy:2375'\n    network: traefik\n\ncertificatesResolvers:\n  le-prod:\n    acme:\n      email: XXXXX\n      storage: /certs/le-prod.json\n      dnsChallenge:\n        provider: cloudflare\n  le-staging:\n    acme:\n      email: XXXXX\n      storage: /certs/le-staging.json\n      caServer: \"https://acme-staging-v02.api.letsencrypt.org/directory\"\n      dnsChallenge:\n        provider: cloudflare\n  step-ca:\n    acme:\n      email: XXXX\n      storage: /certs/step-ca.json\n      caServer: \"https://XXXXX\"\n      httpChallenge:\n        entryPoint: web\n```\n\nDynamic config:\n\n```\nhttp:\n  routers:\n    api:\n      entryPoints:\n        - traefik\n      rule: Host(`traefik.example.com`)\n      service: api@internal\n      tls:\n        certResolver: step-ca\n      middlewares:\n        - restrict-to-internal\n        - forward-auth\n\n    public-http:\n      entryPoints:\n        - web\n      rule: PathPrefix(`/`)\n      priority: 1\n      service: noop@internal\n      middlewares:\n        - external-middlewares\n\n  middlewares:\n    forward-auth:\n      forwardAuth:\n        address: http://authelia:9091/api/authz/forward-auth\n        trustForwardHeader: true\n        preserveLocationHeader: true\n        authResponseHeaders:\n          - Remote-User\n          - Remote-Groups\n          - Remote-Email\n          - Remote-Name\n\n    internal-middlewares:\n      chain:\n        middlewares:\n          - restrict-to-internal\n          - redirect-to-https\n          - internal-headers\n          - compress\n\n    external-middlewares:\n      chain:\n        middlewares:\n          - redirect-to-https\n          - external-headers\n          - custom-error-pages\n          - compress\n\n    external-headers:\n      headers:\n        contentTypeNosniff: true                          # X-Content-Type-Options=nosniff\n        referrerPolicy: \"strict-origin-when-cross-origin\"\n        stsIncludeSubdomains: true                        # Add includeSubdomains to the Strict-Transport-Security header\n        stsPreload: true                                  # Add preload flag appended to the Strict-Transport-Security header\n        stsSeconds: 63072000                              # Set the max-age of the Strict-Transport-Security header (63072000 = 2 years)\n        frameDeny: true\n        browserXssFilter: true\n        customResponseHeaders:\n          server: \"\" # Remove header\n          X-Powered-By: \"\" # Remove header\n          X-Robots-Tag: \"noindex, nofollow\"\n\n    internal-headers:\n      headers:\n        contentTypeNosniff: true\n        referrerPolicy: \"strict-origin-when-cross-origin\"\n        customFrameOptionsValue: \"SAMEORIGIN\"\n        browserXssFilter: true\n\n    compress:\n      compress: {}\n\n    redirect-to-https:\n      redirectScheme:\n        scheme: https\n        permanent: true\n\n    custom-error-pages:\n      errors:\n        status:\n          - \"404\"\n          - \"500\"\n          - \"502\"\n          - \"503\"\n          - \"504\"\n        service: nginx@docker\n        query: \"/errors/{status}.html\"\n\ntls:\n  options:\n    default:\n      cipherSuites:\n        - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n        - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n        - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\n      minVersion: VersionTLS12\n      sniStrict: true\n\n```\n\nTest container uses these labels:\n\n```\n          traefik.enable: \"true\"\n          traefik.docker.network: cwa-test\n          traefik.http.routers.cwa-test.rule: Host(`cwa.example.com`)\n          traefik.http.routers.cwa-test.entrypoints: websecure\n          traefik.http.routers.cwa-test.middlewares: internal-middlewares@file\n          traefik.http.routers.cwa-test.tls: \"true\"\n          traefik.http.routers.cwa-test.tls.certresolver: step-ca\n```\n\nAdd more configuration information here.\n\n\n### If applicable, please paste the log output in DEBUG level\n\nThis is logged in Traefik when requesting the javascript file using compression:\n\n```\n2025-02-08T11:57:30Z DBG github.com/traefik/traefik/v3/pkg/middlewares/compress/compress.go:127 > Unable to parse MIME type error=\"mime: no media type\" middlewareName=compress@file middlewareType=Compress\n```\n", "patch": "diff --git a/pkg/middlewares/compress/compression_handler.go b/pkg/middlewares/compress/compression_handler.go\nindex 215607ccde..07583028b9 100644\n--- a/pkg/middlewares/compress/compression_handler.go\n+++ b/pkg/middlewares/compress/compression_handler.go\n@@ -233,8 +233,12 @@ func (r *responseWriter) Write(p []byte) (int, error) {\n \t// Disable compression according to user wishes in excludedContentTypes or includedContentTypes.\n \tif ct := r.rw.Header().Get(contentType); ct != \"\" {\n \t\tmediaType, params, err := mime.ParseMediaType(ct)\n+\t\t// To align the behavior with the klauspost handler for Gzip,\n+\t\t// if the MIME type is not parsable the compression is disabled.\n \t\tif err != nil {\n-\t\t\treturn 0, fmt.Errorf(\"parsing content-type media type: %w\", err)\n+\t\t\tr.compressionDisabled = true\n+\t\t\tr.rw.WriteHeader(r.statusCode)\n+\t\t\treturn r.rw.Write(p)\n \t\t}\n \n \t\tif len(r.includedContentTypes) > 0 {\n", "instance_id": "traefik__traefik-11628", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a backend service returns a malformed `Content-Type` header with double charset values, which causes Traefik's compression middleware to fail when downloading JavaScript files. The goal is implied\u2014to make the compression middleware more robust by skipping compression for malformed headers. The input (HTTP response headers), output (successful file download or skipping compression), and context (Traefik v3.3.3 with compression middleware) are provided, along with detailed logs, configuration files, and a related bug report. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior for all malformed headers (e.g., should compression always be disabled, or are there specific cases to handle?), and edge cases beyond the double charset are not mentioned. Additionally, constraints or performance expectations for the fix are absent. Despite these minor gaps, the issue is well-documented with reproducible steps and relevant data, making it mostly clear.", "difficulty_explanation": "The difficulty of this problem falls in the easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`compression_handler.go`) and modifies a small section of logic within the `responseWriter.Write` method. It does not impact the broader system architecture or require changes across multiple modules. The change involves altering the error handling for MIME type parsing to disable compression instead of returning an error, which is a straightforward modification. The diff shows minimal lines of code changed (about 5 lines), indicating a low amount of effort in terms of raw code volume.\n\n2. **Number of Technical Concepts**: Solving this requires understanding basic Go programming concepts, specifically error handling and HTTP middleware behavior. Familiarity with the `mime` package for parsing media types is needed, but this is a standard library feature and not particularly complex. Knowledge of Traefik's middleware architecture is helpful but not critical, as the change is isolated to a specific handler. No advanced algorithms, design patterns, or domain-specific knowledge (beyond HTTP headers) are required.\n\n3. **Edge Cases and Error Handling**: The problem statement highlights a specific edge case (malformed `Content-Type` with double charset), and the code change addresses it by disabling compression when MIME parsing fails. However, the solution does not explore other potential malformed header formats or their implications, and no additional error handling beyond this case is introduced. The complexity of edge cases is low, as the fix is a simple fallback to disable compression.\n\n4. **Overall Complexity**: The task requires understanding a small part of the codebase and making a simple logical change to handle an error condition differently. It does not involve deep architectural knowledge of Traefik, performance optimization, or complex debugging. The fix aligns with the behavior of another handler (klauspost Gzip handler), as noted in the code comment, suggesting a clear precedent for the solution.\n\nGiven these points, a difficulty score of 0.30 reflects an easy problem that requires minimal code changes and a basic understanding of Go and HTTP middleware logic. It is slightly above the very easy range (0.0-0.2) due to the need to understand the specific error condition and its impact on compression behavior, but it does not approach medium difficulty as it lacks broader scope or complex interactions.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Allow to add path prefix for both api and dashboard endpoints\n### Do you want to request a *feature* or report a *bug*?\r\n\r\nFeature\r\n\r\n### What did you expect to see?\r\n\r\nAs #5374 has been fixed, now dashboard assets can be served under any path prefix, but it continues to fetch api data from `/api`, which seems to be determined at build time or hard coded (?)\r\n\r\nI would suggest to add something like `--api.pathprefix=/path/prefix` then these endpoints will be served under `/path/prefix/api` and `/path/prefix/dashboard`. Which will make them easier to integrate into existing system, for example:\r\n\r\n-  session cookie under some path prefix.\r\n-  avoid conflict with existent endpoints.\r\n\n", "patch": "diff --git a/docs/content/operations/dashboard.md b/docs/content/operations/dashboard.md\nindex c2b3c21e91..b7df89025c 100644\n--- a/docs/content/operations/dashboard.md\n+++ b/docs/content/operations/dashboard.md\n@@ -87,8 +87,44 @@ rule = \"Host(`traefik.example.com`) && (PathPrefix(`/api`) || PathPrefix(`/dashb\n ??? example \"Dashboard Dynamic Configuration Examples\"\n     --8<-- \"content/operations/include-dashboard-examples.md\"\n \n+### Custom API Base Path\n+\n+As shown above, by default Traefik exposes its API and Dashboard under the `/` base path,\n+which means that respectively the API is served under the `/api` path,\n+and the dashboard under the `/dashboard` path.\n+\n+However, it is possible to configure this base path:\n+\n+```yaml tab=\"File (YAML)\"\n+api:\n+  # Customizes the base path:\n+  # - Serving API under `/traefik/api`\n+  # - Serving Dashboard under `/traefik/dashboard`\n+  basePath: /traefik\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[api]\n+  # Customizes the base path:\n+  # - Serving API under `/traefik/api`\n+  # - Serving Dashboard under `/traefik/dashboard`\n+  basePath = \"/traefik\"\n+```\n+\n+```bash tab=\"CLI\"\n+# Customizes the base path:\n+# - Serving API under `/traefik/api`\n+# - Serving Dashboard under `/traefik/dashboard`\n+--api.basePath=/traefik\n+```\n+\n+??? example \"Dashboard Under Custom Path Dynamic Configuration Examples\"\n+    --8<-- \"content/operations/include-dashboard-custom-path-examples.md\"\n+\n ## Insecure Mode\n \n+!!! warning \"Please note that this mode is incompatible with the [custom API base path option](#custom-api-base-path).\"\n+\n When _insecure_ mode is enabled, one can access the dashboard on the `traefik` port (default: `8080`) of the Traefik instance,\n at the following URL: `http://<Traefik IP>:8080/dashboard/` (trailing slash is mandatory).\n \ndiff --git a/docs/content/operations/include-dashboard-custom-path-examples.md b/docs/content/operations/include-dashboard-custom-path-examples.md\nnew file mode 100644\nindex 0000000000..6767afc45b\n--- /dev/null\n+++ b/docs/content/operations/include-dashboard-custom-path-examples.md\n@@ -0,0 +1,83 @@\n+```yaml tab=\"Docker & Swarm\"\n+# Dynamic Configuration\n+labels:\n+  - \"traefik.http.routers.dashboard.rule=Host(`traefik.example.com`) && PathPrefix(`/traefik`)\"\n+  - \"traefik.http.routers.dashboard.service=api@internal\"\n+  - \"traefik.http.routers.dashboard.middlewares=auth\"\n+  - \"traefik.http.middlewares.auth.basicauth.users=test:$$apr1$$H6uskkkW$$IgXLP6ewTrSuBkTrqE8wj/,test2:$$apr1$$d9hr9HBB$$4HxwgUir3HP4EsggP/QNo0\"\n+```\n+\n+```yaml tab=\"Docker (Swarm)\"\n+# Dynamic Configuration\n+deploy:\n+  labels:\n+    - \"traefik.http.routers.dashboard.rule=Host(`traefik.example.com`) && PathPrefix(`/traefik`)\"\n+    - \"traefik.http.routers.dashboard.service=api@internal\"\n+    - \"traefik.http.routers.dashboard.middlewares=auth\"\n+    - \"traefik.http.middlewares.auth.basicauth.users=test:$$apr1$$H6uskkkW$$IgXLP6ewTrSuBkTrqE8wj/,test2:$$apr1$$d9hr9HBB$$4HxwgUir3HP4EsggP/QNo0\"\n+    # Dummy service for Swarm port detection. The port can be any valid integer value.\n+    - \"traefik.http.services.dummy-svc.loadbalancer.server.port=9999\"\n+```\n+\n+```yaml tab=\"Kubernetes CRD\"\n+apiVersion: traefik.io/v1alpha1\n+kind: IngressRoute\n+metadata:\n+  name: traefik-dashboard\n+spec:\n+  routes:\n+  - match: Host(`traefik.example.com`) && PathPrefix(`/traefik`)\n+    kind: Rule\n+    services:\n+    - name: api@internal\n+      kind: TraefikService\n+    middlewares:\n+      - name: auth\n+---\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: auth\n+spec:\n+  basicAuth:\n+    secret: secretName # Kubernetes secret named \"secretName\"\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+# Dynamic Configuration\n+- \"traefik.http.routers.dashboard.rule=Host(`traefik.example.com`) && PathPrefix(`/traefik`)\"\n+- \"traefik.http.routers.dashboard.service=api@internal\"\n+- \"traefik.http.routers.dashboard.middlewares=auth\"\n+- \"traefik.http.middlewares.auth.basicauth.users=test:$$apr1$$H6uskkkW$$IgXLP6ewTrSuBkTrqE8wj/,test2:$$apr1$$d9hr9HBB$$4HxwgUir3HP4EsggP/QNo0\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+# Dynamic Configuration\n+http:\n+  routers:\n+    dashboard:\n+      rule: Host(`traefik.example.com`) && PathPrefix(`/traefik`)\n+      service: api@internal\n+      middlewares:\n+        - auth\n+  middlewares:\n+    auth:\n+      basicAuth:\n+        users:\n+          - \"test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/\"\n+          - \"test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0\"\n+```\n+\n+```toml tab=\"File (TOML)\"\n+# Dynamic Configuration\n+[http.routers.my-api]\n+  rule = \"Host(`traefik.example.com`) && PathPrefix(`/traefik`)\"\n+  service = \"api@internal\"\n+  middlewares = [\"auth\"]\n+\n+[http.middlewares.auth.basicAuth]\n+  users = [\n+    \"test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/\",\n+    \"test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0\",\n+  ]\n+```\ndiff --git a/docs/content/reference/static-configuration/cli-ref.md b/docs/content/reference/static-configuration/cli-ref.md\nindex 7e18a9b8bd..25c5a57f1f 100644\n--- a/docs/content/reference/static-configuration/cli-ref.md\n+++ b/docs/content/reference/static-configuration/cli-ref.md\n@@ -42,6 +42,9 @@ Access log format: json | common (Default: ```common```)\n `--api`:  \n Enable api/dashboard. (Default: ```false```)\n \n+`--api.basepath`:  \n+Defines the base path where the API and Dashboard will be exposed. (Default: ```/```)\n+\n `--api.dashboard`:  \n Activate dashboard. (Default: ```true```)\n \ndiff --git a/docs/content/reference/static-configuration/env-ref.md b/docs/content/reference/static-configuration/env-ref.md\nindex 61f0a28b07..362c34591c 100644\n--- a/docs/content/reference/static-configuration/env-ref.md\n+++ b/docs/content/reference/static-configuration/env-ref.md\n@@ -42,6 +42,9 @@ Access log format: json | common (Default: ```common```)\n `TRAEFIK_API`:  \n Enable api/dashboard. (Default: ```false```)\n \n+`TRAEFIK_API_BASEPATH`:  \n+Defines the base path where the API and Dashboard will be exposed. (Default: ```/```)\n+\n `TRAEFIK_API_DASHBOARD`:  \n Activate dashboard. (Default: ```true```)\n \ndiff --git a/docs/content/reference/static-configuration/file.toml b/docs/content/reference/static-configuration/file.toml\nindex 9bfa02e1ed..d0338aba10 100644\n--- a/docs/content/reference/static-configuration/file.toml\n+++ b/docs/content/reference/static-configuration/file.toml\n@@ -294,6 +294,7 @@\n       name1 = \"foobar\"\n \n [api]\n+  basePath = \"foobar\"\n   insecure = true\n   dashboard = true\n   debug = true\ndiff --git a/docs/content/reference/static-configuration/file.yaml b/docs/content/reference/static-configuration/file.yaml\nindex 1fb8a7f303..b74bd56454 100644\n--- a/docs/content/reference/static-configuration/file.yaml\n+++ b/docs/content/reference/static-configuration/file.yaml\n@@ -330,6 +330,7 @@ providers:\n       name0: foobar\n       name1: foobar\n api:\n+  basePath: foobar\n   insecure: true\n   dashboard: true\n   debug: true\ndiff --git a/pkg/api/dashboard/dashboard.go b/pkg/api/dashboard/dashboard.go\nindex a0ea9eb965..21eeee8fb0 100644\n--- a/pkg/api/dashboard/dashboard.go\n+++ b/pkg/api/dashboard/dashboard.go\n@@ -1,36 +1,88 @@\n package dashboard\n \n import (\n+\t\"fmt\"\n \t\"io/fs\"\n \t\"net/http\"\n \t\"strings\"\n+\t\"text/template\"\n \n \t\"github.com/gorilla/mux\"\n+\t\"github.com/rs/zerolog/log\"\n \t\"github.com/traefik/traefik/v3/webui\"\n )\n \n+type indexTemplateData struct {\n+\tAPIUrl string\n+}\n+\n // Handler expose dashboard routes.\n type Handler struct {\n+\tBasePath string\n+\n \tassets fs.FS // optional assets, to override the webui.FS default\n }\n \n+func (h Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n+\tassets := h.assets\n+\tif assets == nil {\n+\t\tassets = webui.FS\n+\t}\n+\n+\t// allow iframes from traefik domains only\n+\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/frame-src\n+\tw.Header().Set(\"Content-Security-Policy\", \"frame-src 'self' https://traefik.io https://*.traefik.io;\")\n+\n+\t// The content type must be guessed by the file server.\n+\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Content-Type-Options\n+\tw.Header().Del(\"Content-Type\")\n+\n+\tif r.RequestURI == \"/\" {\n+\t\tindexTemplate, err := template.ParseFS(assets, \"index.html\")\n+\t\tif err != nil {\n+\t\t\tlog.Error().Err(err).Msg(\"Unable to parse index template\")\n+\t\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n+\t\t\treturn\n+\t\t}\n+\n+\t\tapiPath := strings.TrimSuffix(h.BasePath, \"/\") + \"/api/\"\n+\t\tif err = indexTemplate.Execute(w, indexTemplateData{APIUrl: apiPath}); err != nil {\n+\t\t\tlog.Error().Err(err).Msg(\"Unable to render index template\")\n+\t\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n+\t\t\treturn\n+\t\t}\n+\n+\t\treturn\n+\t}\n+\n+\thttp.FileServerFS(assets).ServeHTTP(w, r)\n+}\n+\n // Append adds dashboard routes on the given router, optionally using the given\n // assets (or webui.FS otherwise).\n-func Append(router *mux.Router, customAssets fs.FS) {\n+func Append(router *mux.Router, basePath string, customAssets fs.FS) error {\n \tassets := customAssets\n \tif assets == nil {\n \t\tassets = webui.FS\n \t}\n+\n+\tindexTemplate, err := template.ParseFS(assets, \"index.html\")\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"parsing index template: %w\", err)\n+\t}\n+\n+\tdashboardPath := strings.TrimSuffix(basePath, \"/\") + \"/dashboard/\"\n+\n \t// Expose dashboard\n \trouter.Methods(http.MethodGet).\n-\t\tPath(\"/\").\n+\t\tPath(basePath).\n \t\tHandlerFunc(func(resp http.ResponseWriter, req *http.Request) {\n \t\t\tprefix := strings.TrimSuffix(req.Header.Get(\"X-Forwarded-Prefix\"), \"/\")\n-\t\t\thttp.Redirect(resp, req, prefix+\"/dashboard/\", http.StatusFound)\n+\t\t\thttp.Redirect(resp, req, prefix+dashboardPath, http.StatusFound)\n \t\t})\n \n \trouter.Methods(http.MethodGet).\n-\t\tPathPrefix(\"/dashboard/\").\n+\t\tPath(dashboardPath).\n \t\tHandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n \t\t\t// allow iframes from our domains only\n \t\t\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/frame-src\n@@ -40,22 +92,26 @@ func Append(router *mux.Router, customAssets fs.FS) {\n \t\t\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Content-Type-Options\n \t\t\tw.Header().Del(\"Content-Type\")\n \n-\t\t\thttp.StripPrefix(\"/dashboard/\", http.FileServerFS(assets)).ServeHTTP(w, r)\n+\t\t\tapiPath := strings.TrimSuffix(basePath, \"/\") + \"/api/\"\n+\t\t\tif err = indexTemplate.Execute(w, indexTemplateData{APIUrl: apiPath}); err != nil {\n+\t\t\t\tlog.Error().Err(err).Msg(\"Unable to render index template\")\n+\t\t\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n+\t\t\t\treturn\n+\t\t\t}\n \t\t})\n-}\n \n-func (g Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tassets := g.assets\n-\tif assets == nil {\n-\t\tassets = webui.FS\n-\t}\n-\t// allow iframes from our domains only\n-\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/frame-src\n-\tw.Header().Set(\"Content-Security-Policy\", \"frame-src 'self' https://traefik.io https://*.traefik.io;\")\n+\trouter.Methods(http.MethodGet).\n+\t\tPathPrefix(dashboardPath).\n+\t\tHandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+\t\t\t// allow iframes from traefik domains only\n+\t\t\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/frame-src\n+\t\t\tw.Header().Set(\"Content-Security-Policy\", \"frame-src 'self' https://traefik.io https://*.traefik.io;\")\n \n-\t// The content type must be guessed by the file server.\n-\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Content-Type-Options\n-\tw.Header().Del(\"Content-Type\")\n+\t\t\t// The content type must be guessed by the file server.\n+\t\t\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Content-Type-Options\n+\t\t\tw.Header().Del(\"Content-Type\")\n \n-\thttp.FileServerFS(assets).ServeHTTP(w, r)\n+\t\t\thttp.StripPrefix(dashboardPath, http.FileServerFS(assets)).ServeHTTP(w, r)\n+\t\t})\n+\treturn nil\n }\ndiff --git a/pkg/api/handler.go b/pkg/api/handler.go\nindex b137e5401b..d0f90150e8 100644\n--- a/pkg/api/handler.go\n+++ b/pkg/api/handler.go\n@@ -78,38 +78,40 @@ func New(staticConfig static.Configuration, runtimeConfig *runtime.Configuration\n func (h Handler) createRouter() *mux.Router {\n \trouter := mux.NewRouter().UseEncodedPath()\n \n+\tapiRouter := router.PathPrefix(h.staticConfig.API.BasePath).Subrouter().UseEncodedPath()\n+\n \tif h.staticConfig.API.Debug {\n-\t\tDebugHandler{}.Append(router)\n+\t\tDebugHandler{}.Append(apiRouter)\n \t}\n \n-\trouter.Methods(http.MethodGet).Path(\"/api/rawdata\").HandlerFunc(h.getRuntimeConfiguration)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/rawdata\").HandlerFunc(h.getRuntimeConfiguration)\n \n \t// Experimental endpoint\n-\trouter.Methods(http.MethodGet).Path(\"/api/overview\").HandlerFunc(h.getOverview)\n-\n-\trouter.Methods(http.MethodGet).Path(\"/api/entrypoints\").HandlerFunc(h.getEntryPoints)\n-\trouter.Methods(http.MethodGet).Path(\"/api/entrypoints/{entryPointID}\").HandlerFunc(h.getEntryPoint)\n-\n-\trouter.Methods(http.MethodGet).Path(\"/api/http/routers\").HandlerFunc(h.getRouters)\n-\trouter.Methods(http.MethodGet).Path(\"/api/http/routers/{routerID}\").HandlerFunc(h.getRouter)\n-\trouter.Methods(http.MethodGet).Path(\"/api/http/services\").HandlerFunc(h.getServices)\n-\trouter.Methods(http.MethodGet).Path(\"/api/http/services/{serviceID}\").HandlerFunc(h.getService)\n-\trouter.Methods(http.MethodGet).Path(\"/api/http/middlewares\").HandlerFunc(h.getMiddlewares)\n-\trouter.Methods(http.MethodGet).Path(\"/api/http/middlewares/{middlewareID}\").HandlerFunc(h.getMiddleware)\n-\n-\trouter.Methods(http.MethodGet).Path(\"/api/tcp/routers\").HandlerFunc(h.getTCPRouters)\n-\trouter.Methods(http.MethodGet).Path(\"/api/tcp/routers/{routerID}\").HandlerFunc(h.getTCPRouter)\n-\trouter.Methods(http.MethodGet).Path(\"/api/tcp/services\").HandlerFunc(h.getTCPServices)\n-\trouter.Methods(http.MethodGet).Path(\"/api/tcp/services/{serviceID}\").HandlerFunc(h.getTCPService)\n-\trouter.Methods(http.MethodGet).Path(\"/api/tcp/middlewares\").HandlerFunc(h.getTCPMiddlewares)\n-\trouter.Methods(http.MethodGet).Path(\"/api/tcp/middlewares/{middlewareID}\").HandlerFunc(h.getTCPMiddleware)\n-\n-\trouter.Methods(http.MethodGet).Path(\"/api/udp/routers\").HandlerFunc(h.getUDPRouters)\n-\trouter.Methods(http.MethodGet).Path(\"/api/udp/routers/{routerID}\").HandlerFunc(h.getUDPRouter)\n-\trouter.Methods(http.MethodGet).Path(\"/api/udp/services\").HandlerFunc(h.getUDPServices)\n-\trouter.Methods(http.MethodGet).Path(\"/api/udp/services/{serviceID}\").HandlerFunc(h.getUDPService)\n-\n-\tversion.Handler{}.Append(router)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/overview\").HandlerFunc(h.getOverview)\n+\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/entrypoints\").HandlerFunc(h.getEntryPoints)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/entrypoints/{entryPointID}\").HandlerFunc(h.getEntryPoint)\n+\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/http/routers\").HandlerFunc(h.getRouters)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/http/routers/{routerID}\").HandlerFunc(h.getRouter)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/http/services\").HandlerFunc(h.getServices)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/http/services/{serviceID}\").HandlerFunc(h.getService)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/http/middlewares\").HandlerFunc(h.getMiddlewares)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/http/middlewares/{middlewareID}\").HandlerFunc(h.getMiddleware)\n+\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/tcp/routers\").HandlerFunc(h.getTCPRouters)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/tcp/routers/{routerID}\").HandlerFunc(h.getTCPRouter)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/tcp/services\").HandlerFunc(h.getTCPServices)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/tcp/services/{serviceID}\").HandlerFunc(h.getTCPService)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/tcp/middlewares\").HandlerFunc(h.getTCPMiddlewares)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/tcp/middlewares/{middlewareID}\").HandlerFunc(h.getTCPMiddleware)\n+\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/udp/routers\").HandlerFunc(h.getUDPRouters)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/udp/routers/{routerID}\").HandlerFunc(h.getUDPRouter)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/udp/services\").HandlerFunc(h.getUDPServices)\n+\tapiRouter.Methods(http.MethodGet).Path(\"/api/udp/services/{serviceID}\").HandlerFunc(h.getUDPService)\n+\n+\tversion.Handler{}.Append(apiRouter)\n \n \treturn router\n }\ndiff --git a/pkg/config/static/static_config.go b/pkg/config/static/static_config.go\nindex 7f26e053a4..6451bc77fc 100644\n--- a/pkg/config/static/static_config.go\n+++ b/pkg/config/static/static_config.go\n@@ -3,6 +3,7 @@ package static\n import (\n \t\"errors\"\n \t\"fmt\"\n+\t\"path\"\n \t\"strings\"\n \t\"time\"\n \n@@ -145,16 +146,18 @@ type TLSClientConfig struct {\n \n // API holds the API configuration.\n type API struct {\n-\tInsecure           bool `description:\"Activate API directly on the entryPoint named traefik.\" json:\"insecure,omitempty\" toml:\"insecure,omitempty\" yaml:\"insecure,omitempty\" export:\"true\"`\n-\tDashboard          bool `description:\"Activate dashboard.\" json:\"dashboard,omitempty\" toml:\"dashboard,omitempty\" yaml:\"dashboard,omitempty\" export:\"true\"`\n-\tDebug              bool `description:\"Enable additional endpoints for debugging and profiling.\" json:\"debug,omitempty\" toml:\"debug,omitempty\" yaml:\"debug,omitempty\" export:\"true\"`\n-\tDisableDashboardAd bool `description:\"Disable ad in the dashboard.\" json:\"disableDashboardAd,omitempty\" toml:\"disableDashboardAd,omitempty\" yaml:\"disableDashboardAd,omitempty\" export:\"true\"`\n+\tBasePath           string `description:\"Defines the base path where the API and Dashboard will be exposed.\" json:\"basePath,omitempty\" toml:\"basePath,omitempty\" yaml:\"basePath,omitempty\" export:\"true\"`\n+\tInsecure           bool   `description:\"Activate API directly on the entryPoint named traefik.\" json:\"insecure,omitempty\" toml:\"insecure,omitempty\" yaml:\"insecure,omitempty\" export:\"true\"`\n+\tDashboard          bool   `description:\"Activate dashboard.\" json:\"dashboard,omitempty\" toml:\"dashboard,omitempty\" yaml:\"dashboard,omitempty\" export:\"true\"`\n+\tDebug              bool   `description:\"Enable additional endpoints for debugging and profiling.\" json:\"debug,omitempty\" toml:\"debug,omitempty\" yaml:\"debug,omitempty\" export:\"true\"`\n+\tDisableDashboardAd bool   `description:\"Disable ad in the dashboard.\" json:\"disableDashboardAd,omitempty\" toml:\"disableDashboardAd,omitempty\" yaml:\"disableDashboardAd,omitempty\" export:\"true\"`\n \t// TODO: Re-enable statistics\n \t// Statistics      *types.Statistics `description:\"Enable more detailed statistics.\" json:\"statistics,omitempty\" toml:\"statistics,omitempty\" yaml:\"statistics,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n func (a *API) SetDefaults() {\n+\ta.BasePath = \"/\"\n \ta.Dashboard = true\n }\n \n@@ -360,6 +363,10 @@ func (c *Configuration) ValidateConfiguration() error {\n \t\t}\n \t}\n \n+\tif c.API != nil && !path.IsAbs(c.API.BasePath) {\n+\t\treturn errors.New(\"API basePath must be a valid absolute path\")\n+\t}\n+\n \treturn nil\n }\n \ndiff --git a/pkg/server/service/managerfactory.go b/pkg/server/service/managerfactory.go\nindex ce1fa8a271..19da05d7b2 100644\n--- a/pkg/server/service/managerfactory.go\n+++ b/pkg/server/service/managerfactory.go\n@@ -4,6 +4,7 @@ import (\n \t\"net/http\"\n \n \t\"github.com/gorilla/mux\"\n+\t\"github.com/rs/zerolog/log\"\n \t\"github.com/traefik/traefik/v3/pkg/api\"\n \t\"github.com/traefik/traefik/v3/pkg/api/dashboard\"\n \t\"github.com/traefik/traefik/v3/pkg/config/runtime\"\n@@ -44,10 +45,13 @@ func NewManagerFactory(staticConfiguration static.Configuration, routinesPool *s\n \t\tapiRouterBuilder := api.NewBuilder(staticConfiguration)\n \n \t\tif staticConfiguration.API.Dashboard {\n-\t\t\tfactory.dashboardHandler = dashboard.Handler{}\n+\t\t\tfactory.dashboardHandler = dashboard.Handler{BasePath: staticConfiguration.API.BasePath}\n \t\t\tfactory.api = func(configuration *runtime.Configuration) http.Handler {\n \t\t\t\trouter := apiRouterBuilder(configuration).(*mux.Router)\n-\t\t\t\tdashboard.Append(router, nil)\n+\t\t\t\tif err := dashboard.Append(router, staticConfiguration.API.BasePath, nil); err != nil {\n+\t\t\t\t\tlog.Error().Err(err).Msg(\"Error appending dashboard to API router\")\n+\t\t\t\t}\n+\n \t\t\t\treturn router\n \t\t\t}\n \t\t} else {\ndiff --git a/webui/index.html b/webui/index.html\nindex 6eb6d6091a..654437a273 100644\n--- a/webui/index.html\n+++ b/webui/index.html\n@@ -1,6 +1,13 @@\n <!DOCTYPE html>\n <html>\n   <head>\n+\n+    {{if .APIUrl}}\n+    <script>\n+      window.APIURL = \"{{.APIUrl}}\"\n+    </script>\n+    {{end}}\n+\n     <title><%= productName %></title>\n \n     <meta charset=\"utf-8\">\ndiff --git a/webui/src/boot/api.js b/webui/src/boot/api.js\nindex 8d4f5b3149..1d8f6deaf1 100644\n--- a/webui/src/boot/api.js\n+++ b/webui/src/boot/api.js\n@@ -4,7 +4,7 @@ import { APP } from '../_helpers/APP'\n \n // Set config defaults when creating the instance\n const api = axios.create({\n-  baseURL: APP.config.apiUrl\n+  baseURL: window.APIURL || APP.config.apiUrl\n })\n \n export default boot(({ app }) => {\n", "instance_id": "traefik__traefik-11250", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "\nThe problem statement is mostly clear in describing the desired feature: adding a configurable path prefix for both API and dashboard endpoints in Traefik to facilitate integration into existing systems (e.g., avoiding endpoint conflicts or aligning with session cookies under specific paths). The goal is evident, and the expected behavior is outlined with examples like serving endpoints under `/path/prefix/api` and `/path/prefix/dashboard`. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly address how the prefix should interact with existing configurations or whether there are specific constraints on the prefix format (e.g., must it start with a slash, or are there reserved paths?). Additionally, edge cases such as invalid prefixes, conflicts with other Traefik configurations, or backward compatibility concerns are not mentioned. While the intent is clear, these missing details prevent it from being comprehensive.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.55, placing this problem in the medium range, as it involves moderate complexity across multiple dimensions. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes span multiple files and modules, including configuration handling (`static_config.go`), API and dashboard routing logic (`handler.go`, `dashboard.go`), web UI adjustments (`index.html`, `api.js`), and documentation updates. The modifications are not confined to a single function or file but require coordinated changes across the codebase. While the changes do not fundamentally alter the system's architecture, they impact how paths are managed and exposed, which is a core aspect of Traefik's routing functionality. The amount of code change is moderate, with significant updates to routing logic and configuration validation.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding several concepts, including HTTP routing with `gorilla/mux`, template rendering for the dashboard UI, configuration management in Traefik (YAML, TOML, CLI, and environment variables), and path manipulation in Go. Additionally, familiarity with Traefik's internal architecture (e.g., how API and dashboard handlers are wired) is necessary. These concepts are not overly advanced but require a solid grasp of web server middleware and routing mechanisms, making the problem moderately complex.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes introduce some considerations, such as validating the `basePath` to ensure it is an absolute path (added in `static_config.go`). Additional edge cases that might need handling include trailing slashes in prefixes, conflicts with existing routes, and ensuring backward compatibility with configurations that do not specify a prefix. The code also adds error handling for template parsing and rendering, which increases the complexity slightly. However, these edge cases are not extraordinarily intricate, aligning with a medium difficulty level.\n\n4. **Overall Impact and Understanding Required**: Implementing this feature requires a moderate understanding of Traefik's codebase, particularly its routing and configuration subsystems. The changes are not trivial (e.g., not just changing a constant or fixing a typo) but also not at the level of refactoring core components or implementing complex algorithms. The task involves integrating a new configuration option (`basePath`) and ensuring it propagates correctly through the API and dashboard handlers, which demands careful attention to detail across multiple files.\n\nIn summary, this problem sits in the medium difficulty range (0.4-0.6) due to the need for coordinated changes across several files, a moderate number of technical concepts to understand, and some edge case handling. It does not reach the hard or very hard range as it does not require deep architectural changes, advanced algorithms, or highly specialized domain knowledge beyond typical web server development.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Traefik v3 breaks Kubernetes Ingress Prefix Path Regex\n### Welcome!\r\n\r\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\r\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\r\n\r\n### What did you do?\r\n\r\nI upgraded from Traefik v2 to v3. Then I realized that all Kubernetes Ingress configurations which use a regex within as part of the prefix path (`pathType=Prefix`) break.\r\n\r\n\r\n# Workaround\r\n\r\nAdd the following lines to the static config:\r\n\r\n```yaml\r\ncore:\r\n  defaultRuleSyntax: v2\r\n```\r\n\r\n# Suggestions\r\n\r\n1. Revert to previous behavior for `pathType=Prefix`\r\n2. Have an option for the Kubernetes Ingress provider to default to v2 behavior, by default\r\n3. Have the Kubernetes Ingress provider to use `PathRegexp('^...')` insternally, instead\r\n4. Add new `PathRegexp` value to `traefik.ingress.kubernetes.io/router.pathmatcher` annotation\r\n\r\n\r\n### What did you see instead?\r\n\r\nKubernetes Ingress configurations which use a regex within as part of the Prefix path no longer work.\r\n\r\n### What version of Traefik are you using?\r\n\r\nv3.0.0\r\n\r\n### What is your environment & configuration?\r\n\r\n```yaml\r\nproviders:\r\n  kubernetesIngress:\r\n    ingressClass: traefik\r\n    allowEmptyServices: true\r\n    allowExternalNameServices: true\r\n    throttleDuration: \"3s\"\r\n```\r\n\r\nKubernetes Ingress Example:\r\n\r\n```yaml\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: catalogservice-ingress\r\n  namespace: uplift\r\n  annotations:\r\n    traefik.ingress.kubernetes.io/router.entrypoints: https\r\n    traefik.ingress.kubernetes.io/router.tls: \"true\"\r\nspec:\r\n  ingressClassName: traefik\r\n  rules:\r\n    - host: 'api.xxx'\r\n      http:\r\n        paths:\r\n          - path: /xxx/v1/yyyy/{customer:[a-z0-9]+}/push/\r\n            pathType: Prefix\r\n            backend:\r\n              service:\r\n                name: catalogservice-service\r\n                port:\r\n                  name: http\r\n```\r\n\r\n\r\n### If applicable, please paste the log output in DEBUG level\r\n\r\n_No response_\n", "patch": "diff --git a/docs/content/migration/v2-to-v3-details.md b/docs/content/migration/v2-to-v3-details.md\nindex e40ab39f32..abd8ae587e 100644\n--- a/docs/content/migration/v2-to-v3-details.md\n+++ b/docs/content/migration/v2-to-v3-details.md\n@@ -541,6 +541,19 @@ it is now unsupported and would prevent Traefik to start.\n \n All Pilot related configuration should be removed from the static configuration.\n \n+### Kubernetes Ingress Path Matching\n+\n+In v3, the Kubernetes Ingress default path matching does not support regexes anymore.\n+\n+#### Remediation\n+\n+Two levels of remediation are possible:\n+\n+- Interpret the default path matcher `PathPrefix` with v2 syntax.\n+This can done globally for all routers with the [static configuration](#configure-the-default-syntax-in-static-configuration) or on a per-router basis by using the [traefik.ingress.kubernetes.io/router.rulesyntax](../routing/providers/kubernetes-ingress.md#annotations) annotation.\n+\n+- Adapt the path regex to be compatible with the Go regex syntax and change the default path matcher to use the `PathRegexp` matcher with the [`traefik.ingress.kubernetes.io/router.pathmatcher`](../routing/providers/kubernetes-ingress.md#annotations) annotation.\n+\n ## Operations Changes\n \n ### Traefik RBAC Update\ndiff --git a/docs/content/routing/providers/kubernetes-ingress.md b/docs/content/routing/providers/kubernetes-ingress.md\nindex 80bd7becbb..158543b464 100644\n--- a/docs/content/routing/providers/kubernetes-ingress.md\n+++ b/docs/content/routing/providers/kubernetes-ingress.md\n@@ -229,10 +229,18 @@ which in turn will create the resulting routers, services, handlers, etc.\n     traefik.ingress.kubernetes.io/router.priority: \"42\"\n     ```\n \n+??? info \"`traefik.ingress.kubernetes.io/router.rulesyntax`\"\n+\n+    See [rule syntax](../routers/index.md#rulesyntax) for more information.\n+\n+    ```yaml\n+    traefik.ingress.kubernetes.io/router.rulesyntax: \"v2\"\n+    ```\n+\n ??? info \"`traefik.ingress.kubernetes.io/router.pathmatcher`\"\n \n     Overrides the default router rule type used for a path.\n-    Only path-related matcher name can be specified: `Path`, `PathPrefix`.\n+    Only path-related matcher name should be specified: `Path`, `PathPrefix` or `PathRegexp`.\n \n     Default `PathPrefix`\n \ndiff --git a/pkg/provider/kubernetes/ingress/annotations.go b/pkg/provider/kubernetes/ingress/annotations.go\nindex 00d5d6649f..144dd2a460 100644\n--- a/pkg/provider/kubernetes/ingress/annotations.go\n+++ b/pkg/provider/kubernetes/ingress/annotations.go\n@@ -26,6 +26,7 @@ type RouterIng struct {\n \tEntryPoints []string                 `json:\"entryPoints,omitempty\"`\n \tMiddlewares []string                 `json:\"middlewares,omitempty\"`\n \tPriority    int                      `json:\"priority,omitempty\"`\n+\tRuleSyntax  string                   `json:\"ruleSyntax,omitempty\"`\n \tTLS         *dynamic.RouterTLSConfig `json:\"tls,omitempty\" label:\"allowEmpty\"`\n }\n \ndiff --git a/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-annotations.yml b/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-annotations.yml\nindex bf4679f305..3a57a63458 100644\n--- a/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-annotations.yml\n+++ b/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-annotations.yml\n@@ -10,6 +10,7 @@ metadata:\n     traefik.ingress.kubernetes.io/router.entrypoints: ep1,ep2\n     traefik.ingress.kubernetes.io/router.middlewares: md1,md2\n     traefik.ingress.kubernetes.io/router.priority: \"42\"\n+    traefik.ingress.kubernetes.io/router.rulesyntax: \"v2\"\n     traefik.ingress.kubernetes.io/router.tls: \"true\"\n     traefik.ingress.kubernetes.io/router.tls.certresolver:   foobar\n     traefik.ingress.kubernetes.io/router.tls.domains.0.main: domain.com\ndiff --git a/pkg/provider/kubernetes/ingress/kubernetes.go b/pkg/provider/kubernetes/ingress/kubernetes.go\nindex 490271b8c9..982cc09e87 100644\n--- a/pkg/provider/kubernetes/ingress/kubernetes.go\n+++ b/pkg/provider/kubernetes/ingress/kubernetes.go\n@@ -760,6 +760,7 @@ func loadRouter(rule netv1.IngressRule, pa netv1.HTTPIngressPath, rtConfig *Rout\n \t}\n \n \tif rtConfig != nil && rtConfig.Router != nil {\n+\t\trt.RuleSyntax = rtConfig.Router.RuleSyntax\n \t\trt.Priority = rtConfig.Router.Priority\n \t\trt.EntryPoints = rtConfig.Router.EntryPoints\n \t\trt.Middlewares = rtConfig.Router.Middlewares\n", "instance_id": "traefik__traefik-10985", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Traefik v3 breaks Kubernetes Ingress configurations that use regex within prefix paths (`pathType=Prefix`). It provides context about the upgrade from v2 to v3, includes a workaround, and offers suggestions for resolution. Additionally, it provides configuration examples and specifies the version of Traefik in use. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the expected behavior of regex in prefix paths in v2 versus v3, nor does it provide detailed examples of what \"breaking\" means (e.g., specific error messages or logs beyond \"no longer works\"). Edge cases, such as specific regex patterns or complex Ingress configurations, are not mentioned, which could be critical for fully understanding the scope of the issue. Overall, while the problem is valid and mostly clear, these minor gaps in detail result in a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files, including documentation updates (`v2-to-v3-details.md`, `kubernetes-ingress.md`) and code modifications in the Kubernetes Ingress provider (`annotations.go`, `kubernetes.go`). This indicates a need to understand interactions between configuration parsing, routing logic, and provider behavior in Traefik, which adds moderate complexity. Second, the number of technical concepts required includes familiarity with Traefik's routing mechanisms, Kubernetes Ingress specifications, regex handling in Go, and annotations for customizing router behavior. While these concepts are not overly advanced, they require a solid understanding of Traefik's internals and Kubernetes integration. Third, the changes impact how path matching is interpreted (e.g., supporting `PathRegexp` and rule syntax versions), which could affect the system's behavior for many users, though it does not appear to alter the core architecture significantly. Finally, potential edge cases, such as handling invalid regex patterns or conflicts between v2 and v3 syntax, are not explicitly addressed in the problem statement but are implied by the nature of regex and path matching; implementing robust error handling for these cases adds some complexity. Overall, this problem requires moderate expertise and effort to implement and test across multiple components, justifying a difficulty score of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "cipher list is different when DefaultTLSOptions is used and when an unmarshaled TLSOption with nil CipherSuites\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\ntraefik 2.11.2\r\nConfigured using kubernetes.\r\n\r\nReference TLSOption:\r\n```yaml\r\napiVersion: traefik.io/v1alpha1\r\nkind: cipher\r\nmetadata:\r\n  name: default\r\n  namespace: default\r\nspec:\r\n  sniStrict: true\r\n```\r\n\r\nThe cipher list presented to `nmap -p 443 --script ssl-enum-ciphers <domain>` is different when TLSOption is absent  vs when present, even though the above TLSOption configuration has no ciphers present. Specifically, the cipher \"TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA\" is present when TLSOption has been applied, even if no ciphers are listed.\r\n\r\nCiphers without TLSOption applied:\r\n```\r\nPORT    STATE SERVICE\r\n443/tcp open  https\r\n| ssl-enum-ciphers: \r\n|   TLSv1.2: \r\n|     ciphers: \r\n|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (secp256r1) - A\r\n|     compressors: \r\n|       NULL\r\n|     cipher preference: client\r\n|   TLSv1.3: \r\n|     ciphers: \r\n|       TLS_AKE_WITH_AES_128_GCM_SHA256 (ecdh_x25519) - A\r\n|       TLS_AKE_WITH_AES_256_GCM_SHA384 (ecdh_x25519) - A\r\n|       TLS_AKE_WITH_CHACHA20_POLY1305_SHA256 (ecdh_x25519) - A\r\n|     cipher preference: server\r\n|_  least strength: A\r\n```\r\n\r\nCiphers with TLSOption applied:\r\n```\r\nPORT    STATE SERVICE\r\n443/tcp open  https\r\n| ssl-enum-ciphers: \r\n|   TLSv1.2: \r\n|     ciphers: \r\n|       TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA (secp256r1) - C\r\n|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (secp256r1) - A\r\n|       TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (secp256r1) - A\r\n|     compressors: \r\n|       NULL\r\n|     cipher preference: client\r\n|     warnings: \r\n|       64-bit block cipher 3DES vulnerable to SWEET32 attack\r\n|   TLSv1.3: \r\n|     ciphers: \r\n|       TLS_AKE_WITH_AES_128_GCM_SHA256 (ecdh_x25519) - A\r\n|       TLS_AKE_WITH_AES_256_GCM_SHA384 (ecdh_x25519) - A\r\n|       TLS_AKE_WITH_CHACHA20_POLY1305_SHA256 (ecdh_x25519) - A\r\n|     cipher preference: server\r\n|_  least strength: C\r\n```\r\n\r\nThe difference is the addition of `TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA` when TLSOption is applied.\r\n\r\nCode Analysis\r\n---\r\nAs of 2024-04-12 commit 1ffbffb26a3676b368c4fc52c4fb737de56cd65d on master.\r\n\r\n`pkg/tls/tlsmanager.go:37` function `getCipherSuites()` builds the list from `crypto/tls.CipherSuites()`.  This results in the 3DES not being applied because the CipherSuites() call excludes that cipher.\r\n\r\n`pkg/tls/tlsmanager.go:384` in function `buildTLSConfig`, checks if the `TLSOptions.CipherSuites` is not nil. Because the default TLSOption builds the cipher suite (above) is called, when no user defined TLSOption is present, this will be called an a list of ciphers will be constructed here. However, If a TLSOption is provided, but leaves the `CipherSuites` off, then the `*tls.Config.CipherSuites` will be unset, which will result in a different list of ciphers getting applied.\r\n\r\nThis results in a different list of applied ciphers if no user supplied TLSOption is supplied vs a TLSOption is supplied but no ciphers are listed.\r\n\r\nOne solution would be to make the default ciphers explicit in all situations by adjusting `pkg/tls/tlsmanager.go:384`\r\n```diff\r\ndiff --git a/pkg/tls/tlsmanager.go b/pkg/tls/tlsmanager.go\r\nindex ecf9692cf..776fc5854 100644\r\n--- a/pkg/tls/tlsmanager.go\r\n+++ b/pkg/tls/tlsmanager.go\r\n@@ -381,16 +381,19 @@ func buildTLSConfig(tlsOption Options) (*tls.Config, error) {\r\n \t}\r\n \r\n \t// Set the list of CipherSuites if set in the config\r\n-\tif tlsOption.CipherSuites != nil {\r\n-\t\t// if our list of CipherSuites is defined in the entryPoint config, we can re-initialize the suites list as empty\r\n-\t\tconf.CipherSuites = make([]uint16, 0)\r\n-\t\tfor _, cipher := range tlsOption.CipherSuites {\r\n-\t\t\tif cipherConst, exists := CipherSuites[cipher]; exists {\r\n-\t\t\t\tconf.CipherSuites = append(conf.CipherSuites, cipherConst)\r\n-\t\t\t} else {\r\n-\t\t\t\t// CipherSuite listed in the toml does not exist in our listed\r\n-\t\t\t\treturn nil, fmt.Errorf(\"invalid CipherSuite: %s\", cipher)\r\n-\t\t\t}\r\n+\t// Set default TLS Ciphers consistently.\r\n+\tciphers := tlsOption.CipherSuites\r\n+\tif len(ciphers) == 0 {\r\n+\t\tciphers = getCipherSuites()\r\n+\t}\r\n+\t// if our list of CipherSuites is defined in the entryPoint config, we can re-initialize the suites list as empty\r\n+\tconf.CipherSuites = make([]uint16, 0)\r\n+\tfor _, cipher := range ciphers {\r\n+\t\tif cipherConst, exists := CipherSuites[cipher]; exists {\r\n+\t\t\tconf.CipherSuites = append(conf.CipherSuites, cipherConst)\r\n+\t\t} else {\r\n+\t\t\t// CipherSuite listed in the toml does not exist in our listed\r\n+\t\t\treturn nil, fmt.Errorf(\"invalid CipherSuite: %s\", cipher)\r\n \t\t}\r\n \t}\r\n ```\r\n\r\n\n\n### What did you see instead?\n\nInconsistent list of default ciphers, depending on if I built the TLSOption or if the default TLSOption was provided by traefik.\n\n### What version of Traefik are you using?\n\nv2.11.2\n\n### What is your environment & configuration?\n\nLinux\r\nKubernetes\n\n### If applicable, please paste the log output in DEBUG level\n\n_No response_\n", "patch": "diff --git a/pkg/provider/kubernetes/crd/kubernetes.go b/pkg/provider/kubernetes/crd/kubernetes.go\nindex 0ef4d78ad2..83f0b48460 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes.go\n@@ -876,64 +876,69 @@ func createChainMiddleware(ctx context.Context, namespace string, chain *traefik\n }\n \n func buildTLSOptions(ctx context.Context, client Client) map[string]tls.Options {\n-\ttlsOptionsCRD := client.GetTLSOptions()\n+\ttlsOptionsCRDs := client.GetTLSOptions()\n \tvar tlsOptions map[string]tls.Options\n \n-\tif len(tlsOptionsCRD) == 0 {\n+\tif len(tlsOptionsCRDs) == 0 {\n \t\treturn tlsOptions\n \t}\n \ttlsOptions = make(map[string]tls.Options)\n \tvar nsDefault []string\n \n-\tfor _, tlsOption := range tlsOptionsCRD {\n-\t\tlogger := log.FromContext(log.With(ctx, log.Str(\"tlsOption\", tlsOption.Name), log.Str(\"namespace\", tlsOption.Namespace)))\n+\tfor _, tlsOptionsCRD := range tlsOptionsCRDs {\n+\t\tlogger := log.FromContext(log.With(ctx, log.Str(\"tlsOption\", tlsOptionsCRD.Name), log.Str(\"namespace\", tlsOptionsCRD.Namespace)))\n \t\tvar clientCAs []tls.FileOrContent\n \n-\t\tfor _, secretName := range tlsOption.Spec.ClientAuth.SecretNames {\n-\t\t\tsecret, exists, err := client.GetSecret(tlsOption.Namespace, secretName)\n+\t\tfor _, secretName := range tlsOptionsCRD.Spec.ClientAuth.SecretNames {\n+\t\t\tsecret, exists, err := client.GetSecret(tlsOptionsCRD.Namespace, secretName)\n \t\t\tif err != nil {\n-\t\t\t\tlogger.Errorf(\"Failed to fetch secret %s/%s: %v\", tlsOption.Namespace, secretName, err)\n+\t\t\t\tlogger.Errorf(\"Failed to fetch secret %s/%s: %v\", tlsOptionsCRD.Namespace, secretName, err)\n \t\t\t\tcontinue\n \t\t\t}\n \n \t\t\tif !exists {\n-\t\t\t\tlogger.Warnf(\"Secret %s/%s does not exist\", tlsOption.Namespace, secretName)\n+\t\t\t\tlogger.Warnf(\"Secret %s/%s does not exist\", tlsOptionsCRD.Namespace, secretName)\n \t\t\t\tcontinue\n \t\t\t}\n \n-\t\t\tcert, err := getCABlocks(secret, tlsOption.Namespace, secretName)\n+\t\t\tcert, err := getCABlocks(secret, tlsOptionsCRD.Namespace, secretName)\n \t\t\tif err != nil {\n-\t\t\t\tlogger.Errorf(\"Failed to extract CA from secret %s/%s: %v\", tlsOption.Namespace, secretName, err)\n+\t\t\t\tlogger.Errorf(\"Failed to extract CA from secret %s/%s: %v\", tlsOptionsCRD.Namespace, secretName, err)\n \t\t\t\tcontinue\n \t\t\t}\n \n \t\t\tclientCAs = append(clientCAs, tls.FileOrContent(cert))\n \t\t}\n \n-\t\tid := makeID(tlsOption.Namespace, tlsOption.Name)\n+\t\tid := makeID(tlsOptionsCRD.Namespace, tlsOptionsCRD.Name)\n \t\t// If the name is default, we override the default config.\n-\t\tif tlsOption.Name == tls.DefaultTLSConfigName {\n-\t\t\tid = tlsOption.Name\n-\t\t\tnsDefault = append(nsDefault, tlsOption.Namespace)\n+\t\tif tlsOptionsCRD.Name == tls.DefaultTLSConfigName {\n+\t\t\tid = tlsOptionsCRD.Name\n+\t\t\tnsDefault = append(nsDefault, tlsOptionsCRD.Namespace)\n \t\t}\n \n-\t\talpnProtocols := tls.DefaultTLSOptions.ALPNProtocols\n-\t\tif len(tlsOption.Spec.ALPNProtocols) > 0 {\n-\t\t\talpnProtocols = tlsOption.Spec.ALPNProtocols\n+\t\ttlsOption := tls.Options{}\n+\t\ttlsOption.SetDefaults()\n+\n+\t\ttlsOption.MinVersion = tlsOptionsCRD.Spec.MinVersion\n+\t\ttlsOption.MaxVersion = tlsOptionsCRD.Spec.MaxVersion\n+\n+\t\tif tlsOptionsCRD.Spec.CipherSuites != nil {\n+\t\t\ttlsOption.CipherSuites = tlsOptionsCRD.Spec.CipherSuites\n \t\t}\n \n-\t\ttlsOptions[id] = tls.Options{\n-\t\t\tMinVersion:       tlsOption.Spec.MinVersion,\n-\t\t\tMaxVersion:       tlsOption.Spec.MaxVersion,\n-\t\t\tCipherSuites:     tlsOption.Spec.CipherSuites,\n-\t\t\tCurvePreferences: tlsOption.Spec.CurvePreferences,\n-\t\t\tClientAuth: tls.ClientAuth{\n-\t\t\t\tCAFiles:        clientCAs,\n-\t\t\t\tClientAuthType: tlsOption.Spec.ClientAuth.ClientAuthType,\n-\t\t\t},\n-\t\t\tSniStrict:     tlsOption.Spec.SniStrict,\n-\t\t\tALPNProtocols: alpnProtocols,\n+\t\ttlsOption.CurvePreferences = tlsOptionsCRD.Spec.CurvePreferences\n+\t\ttlsOption.ClientAuth = tls.ClientAuth{\n+\t\t\tCAFiles:        clientCAs,\n+\t\t\tClientAuthType: tlsOptionsCRD.Spec.ClientAuth.ClientAuthType,\n \t\t}\n+\t\ttlsOption.SniStrict = tlsOptionsCRD.Spec.SniStrict\n+\n+\t\tif tlsOptionsCRD.Spec.ALPNProtocols != nil {\n+\t\t\ttlsOption.ALPNProtocols = tlsOptionsCRD.Spec.ALPNProtocols\n+\t\t}\n+\n+\t\ttlsOptions[id] = tlsOption\n \t}\n \n \tif len(nsDefault) > 1 {\ndiff --git a/pkg/tls/tls.go b/pkg/tls/tls.go\nindex bacc3e3376..3bf3668051 100644\n--- a/pkg/tls/tls.go\n+++ b/pkg/tls/tls.go\n@@ -32,6 +32,7 @@ type Options struct {\n func (o *Options) SetDefaults() {\n \t// ensure http2 enabled\n \to.ALPNProtocols = DefaultTLSOptions.ALPNProtocols\n+\to.CipherSuites = DefaultTLSOptions.CipherSuites\n }\n \n // +k8s:deepcopy-gen=true\n", "instance_id": "traefik__traefik-10907", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: the cipher list in Traefik differs when a `TLSOption` is applied (even with no ciphers specified) compared to when the default `TLSOption` is used. The goal is to ensure consistent cipher behavior, and the issue is demonstrated with detailed output from an `nmap` scan showing the discrepancy (e.g., the addition of `TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA` when a `TLSOption` is applied). The problem statement also includes a code analysis section that pinpoints the root cause in the codebase (`tlsmanager.go`) and proposes a solution via a diff. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior (e.g., should the default cipher list always exclude 3DES, or should it match the user-provided `TLSOption` behavior?). Additionally, edge cases or potential side effects of applying the proposed change are not discussed, such as whether this could break existing configurations or impact security policies. Overall, the statement is valid and mostly clear but lacks some minor details.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as medium (0.50) based on the following analysis of the factors:\n\n1. **Clarity and Complexity of the Problem Description**: While the problem is mostly clear, the minor ambiguities around expected behavior and lack of discussion on edge cases add a slight layer of complexity to fully understanding the desired outcome. However, the provided code analysis and diff reduce the effort needed to grasp the issue.\n\n2. **Scope and Depth of Code Changes**: The proposed changes impact two files (`tlsmanager.go` and `kubernetes.go`) and involve modifying logic related to TLS configuration and cipher suite handling. The changes are not extensive in terms of lines of code, but they affect a critical part of the system (TLS security configuration). The modifications require understanding how TLS options are initialized and applied across different contexts (default vs. user-defined). While the changes do not significantly alter the system's architecture, they do impact a core security feature, which adds some weight to the scope.\n\n3. **Number of Technical Concepts**: Solving this requires familiarity with several concepts: Go programming (specifically struct initialization and pointer handling), TLS configuration in Go's `crypto/tls` package, Traefik's internal architecture for handling TLS options, and Kubernetes CRD (Custom Resource Definition) integration for configuration. Additionally, understanding cipher suites and their security implications (e.g., why 3DES is undesirable due to SWEET32 vulnerability) is necessary. These concepts are moderately complex, especially for someone not already familiar with TLS internals or Traefik's codebase.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the nature of TLS configuration implies several potential issues: backward compatibility with existing user configurations, ensuring that default cipher suites are secure, handling invalid or unsupported cipher suites, and potential performance impacts of changing cipher negotiation. The proposed code changes do not introduce new error handling but modify existing logic to ensure consistency, which could indirectly affect error paths if not thoroughly tested. These considerations add moderate complexity to the solution.\n\nOverall, this problem falls into the medium difficulty range because it requires understanding multiple technical concepts and making targeted changes to a critical system component (TLS configuration). It is not overly complex or architecture-altering (which would push it into the hard range), but it is beyond a simple bug fix due to the need to understand TLS internals and ensure no unintended side effects. A developer with moderate experience in Go and some familiarity with TLS could tackle this with reasonable effort, provided they carefully test the changes.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "ECS provider misdetecs tasks with attached volumes as missing network interfaces when not in `awsvpc` network mode\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nThe ECS provider should register tasks that have no network interfaces when the network type is not `awsvpc`. The [current](https://github.com/traefik/traefik/blob/c1ef7429771104e79f2e87b236b21495cb5765f0/pkg/provider/ecs/ecs.go#L322) logic rejects tasks that have attachments with no network interfaces, but this is for example a valid configuration when using `bridge` networking and having EBS volumes attached.\r\n\r\nHere is an excerpt from what the API returns when describing the task in question - this task has functional networking and doesn't have any interfaces attached as it uses bridge networking.\r\n```\r\n{\r\n    \"tasks\": [\r\n        {\r\n            \"attachments\": [\r\n                {\r\n                    \"id\": \"xxx\",\r\n                    \"type\": \"AmazonElasticBlockStorage\",\r\n                    \"status\": \"ATTACHED\",\r\n                    \"details\": [\r\n                        {\r\n                            \"name\": \"deleteOnTermination\",\r\n                            \"value\": \"true\"\r\n                        },\r\n                        {\r\n                            \"name\": \"roleArn\",\r\n                            \"value\": \"arn:aws:iam::123456789012:role/task-role\"\r\n                        },\r\n                        {\r\n                            \"name\": \"volumeName\",\r\n                            \"value\": \"ebs-volume\"\r\n                        },\r\n                        {\r\n                            \"name\": \"volumeId\",\r\n                            \"value\": \"vol-abc123xyz890\"\r\n                        }\r\n                    ]\r\n                }\r\n            ],\r\n...\r\n            \"containers\": [\r\n                {\r\n                    \"networkInterfaces\": [],\r\n...\r\n```\r\n\r\nExcerpt from task definition:\r\n```\r\n{\r\n...\r\n        \"networkMode\": \"bridge\",\r\n...\r\n```\n\n### What did you see instead?\n\nTraefik's ECS provider misdetects ECS tasks with an attached volume as missing a network interface and refuses to register them:\r\n\r\n```\r\n{\"level\":\"error\",\"msg\":\"Skip container FOO: no network interfaces\",\"providerName\":\"ecs\",\"time\":\"2024-03-27T09:32:44Z\"}\r\n```\n\n### What version of Traefik are you using?\n\n[2.11.0](https://hub.docker.com/layers/library/traefik/v2.11.0/images/sha256-903f89ddacc3059657565d3e5b3b8e8dcba4cc3b8af08183dad6fdc415459111)\r\n\r\n```\r\nVersion:      2.11.0\r\nCodename:     cheddar\r\nGo version:   go1.22.0\r\nBuilt:        2024-02-12T15:26:45Z\r\nOS/Arch:      linux/amd64\r\n```\n\n### What is your environment & configuration?\n\nRelevant portion:\r\n\r\n```\r\n            --providers.ecs.autodiscoverclusters=true \\\r\n            --providers.ecs.exposedbydefault=false\r\n```\n\n### If applicable, please paste the log output in DEBUG level\n\n_No response_\n", "patch": "diff --git a/pkg/provider/ecs/ecs.go b/pkg/provider/ecs/ecs.go\nindex e2edfbbc7f..2a970f81cb 100644\n--- a/pkg/provider/ecs/ecs.go\n+++ b/pkg/provider/ecs/ecs.go\n@@ -314,7 +314,7 @@ func (p *Provider) listInstances(ctx context.Context, client *awsClient) ([]ecsI\n \t\t\t\t}\n \n \t\t\t\tvar mach *machine\n-\t\t\t\tif len(task.Attachments) != 0 {\n+\t\t\t\tif aws.StringValue(taskDef.NetworkMode) == \"awsvpc\" && len(task.Attachments) != 0 {\n \t\t\t\t\tif len(container.NetworkInterfaces) == 0 {\n \t\t\t\t\t\tlogger.Errorf(\"Skip container %s: no network interfaces\", aws.StringValue(container.Name))\n \t\t\t\t\t\tcontinue\n", "instance_id": "traefik__traefik-10550", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue with the ECS provider in Traefik. It explains the problem context (misdetection of tasks with attached volumes as missing network interfaces when not in `awsvpc` mode), includes relevant excerpts from the API response and task definition, and specifies the observed behavior with error logs. However, there are minor ambiguities: the statement does not explicitly discuss potential edge cases (e.g., other network modes or attachment types beyond EBS volumes) or constraints that might affect the solution. Additionally, while examples are provided, they are not exhaustive, and the expected behavior for all scenarios isn't fully detailed. Overall, the problem is valid and clear, but minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is minimal, involving a single-line modification in one file (`ecs.go`). It adds a condition to check if the network mode is `awsvpc` before enforcing the network interface requirement. This change is localized and does not impact the broader system architecture or require understanding complex interactions across multiple modules. The amount of code change is trivial.\n\n2. **Number of Technical Concepts:** Solving this issue requires basic familiarity with Go (specifically, conditional logic and string comparison), as well as a rudimentary understanding of AWS ECS networking modes (`awsvpc` vs. `bridge`). No advanced language features, libraries, algorithms, or design patterns are involved. The domain-specific knowledge (ECS networking) is straightforward for anyone with minimal exposure to AWS.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond the `bridge` networking mode with EBS volumes. The code change does not introduce new error handling logic or address complex edge cases. While there might be implicit edge cases (e.g., other network modes or attachment types), the provided solution does not account for them, and the fix appears to be a simple conditional check without deeper complexity.\n\n4. **Overall Complexity:** The problem requires understanding a specific bug in the ECS provider logic and applying a targeted fix. It does not demand deep knowledge of the Traefik codebase or intricate system interactions. The task is akin to fixing a simple bug by adjusting a condition, which aligns with an \"Easy\" difficulty level.\n\nGiven these factors, a difficulty score of 0.25 is appropriate, reflecting a straightforward bug fix that requires minimal effort and technical depth.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "List-part API call returns incorrect Size field for multi-part upload when compression is enabled\nHi team,\r\n\r\nwe're using Minio to store images pushed by Harbor (tried on MinIO 2024-03-30 and 2024-12-13)\r\nWhen enabling compression on Minio for all new files (`mc admin config set s3 compression enable=\"on\" extensions=\"\" mime_types=\"\"`), uploads by Harbor start failing.\r\n\r\n## Expected Behavior\r\nUploads by Harbor should succeed whether compression is enabled or not on MinIO as this is a transparent feature\r\n\r\n## Current Behavior\r\nUploads by Harbor fail when compression is enabled due to size mismatch between uploaded chunks size reported by Minio vs actual file size uploaded\r\n\r\nInternally, Harbor seems to be using \"distribution\", which, when pushing multi-parts objects, compute the total size by listing uploaded parts :\r\n\r\nhttps://github.com/distribution/distribution/blob/main/registry/storage/driver/s3-aws/s3.go#L1331\r\nhttps://github.com/distribution/distribution/blob/main/registry/storage/driver/s3-aws/s3.go#L713\r\n\r\nIf the sum of all uploaded parts is different from the size of the uploaded file, Harbor fails.\r\n\r\nWe think that, in the list part call, MinIO reports the size of the actual part file on disk (`Size`, compressed) and not the size of the uncompressed part (`ActualSize`)\r\n\r\nWhen listing uploaded parts, care is taken to fix the size reported for encrypted objects, but maybe not for the compressed ones ( we think it comes from here https://github.com/minio/minio/blob/16f8cf1c52f0a77eeb8f7565aaf7f7df12454583/cmd/object-multipart-handlers.go#L1188 )\r\n\r\nAccording to AWS doc, the `Size` field contains the size of the uploaded data (https://docs.aws.amazon.com/AmazonS3/latest/API/API_Part.html)\r\n\r\n\r\n\r\n## Steps to Reproduce (for bugs)\r\n\r\nReproducer (with aws-cli to permit using the list-parts call):\r\n\r\n```\r\n# Create a file with 2001 bytes\r\n- python3 -c 'print(\"A\" * 2000)' > part\r\n# Enable compression on minio\r\n- mc admin config set s3 compression enable=\"on\" extensions=\".testcompress\" mime_types=\"application/testcompress\"\r\n# Create a multipart upload \r\n- UPLOAD_ID=$(aws --endpoint-url https://s3 s3api create-multipart-upload --bucket debug --key test.testcompress --content-type application/testcompress | jq -r .UploadId)\r\n# Upload the 2001 bytes part\r\n- aws --endpoint-url https://s3 s3api upload-part --bucket debug --key test.testcompress --part-number 1 --body part --upload-id \"$UPLOAD_ID\"\r\n# List parts\r\n- aws --endpoint-url https://s3 s3api list-parts --bucket debug --key test.testcompress --upload-id \"$UPLOAD_ID\"\r\n\r\n```\r\n\r\nWhen compression is enabled, reported size is 30:\r\n\r\n```\r\n{\r\n    \"Parts\": [\r\n        {\r\n            \"PartNumber\": 1,\r\n            \"LastModified\": \"2024-12-19T10:50:46.776000+00:00\",\r\n            \"ETag\": \"\\\"ba413841fb575b59013d38230c171473\\\"\",\r\n            \"Size\": 30\r\n        }\r\n    ],\r\n    \"ChecksumAlgorithm\": \"\",\r\n    \"Initiator\": {\r\n        \"ID\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\",\r\n        \"DisplayName\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\r\n    },\r\n    \"Owner\": {\r\n        \"DisplayName\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\",\r\n        \"ID\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\r\n    },\r\n    \"StorageClass\": \"STANDARD\"\r\n}\r\n```\r\n\r\nWhen compression is disabled (disable compression, delete the \"part\" file and upload it again), reported size is the correct one (2001)\r\n\r\n```\r\n{\r\n    \"Parts\": [\r\n        {\r\n            \"PartNumber\": 1,\r\n            \"LastModified\": \"2024-12-19T10:54:10.801000+00:00\",\r\n            \"ETag\": \"\\\"ba413841fb575b59013d38230c171473\\\"\",\r\n            \"Size\": 2001\r\n        }\r\n    ],\r\n    \"ChecksumAlgorithm\": \"\",\r\n    \"Initiator\": {\r\n        \"ID\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\",\r\n        \"DisplayName\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\r\n    },\r\n    \"Owner\": {\r\n        \"DisplayName\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\",\r\n        \"ID\": \"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\r\n    },\r\n    \"StorageClass\": \"STANDARD\"\r\n}\r\n```\r\n\r\n## Your Environment\r\nTested on MinIO 2024-03-30 and 2024-12-13\r\n\n", "patch": "diff --git a/cmd/object-multipart-handlers.go b/cmd/object-multipart-handlers.go\nindex d32ad30d42f5f..bd0c01684c279 100644\n--- a/cmd/object-multipart-handlers.go\n+++ b/cmd/object-multipart-handlers.go\n@@ -1204,6 +1204,10 @@ func (api objectAPIHandlers) ListObjectPartsHandler(w http.ResponseWriter, r *ht\n \t\t\tlistPartsInfo.Parts[i].ETag = tryDecryptETag(objectEncryptionKey, p.ETag, kind == crypto.S3)\n \t\t\tlistPartsInfo.Parts[i].Size = p.ActualSize\n \t\t}\n+\t} else if _, ok := listPartsInfo.UserDefined[ReservedMetadataPrefix+\"compression\"]; ok {\n+\t\tfor i, p := range listPartsInfo.Parts {\n+\t\t\tlistPartsInfo.Parts[i].Size = p.ActualSize\n+\t\t}\n \t}\n \n \tresponse := generateListPartsResponse(listPartsInfo, encodingType)\n", "instance_id": "minio__minio-20782", "clarity": 3, "difficulty": 0.35, "clarity_explanation": "The problem statement is comprehensive and well-detailed. It clearly describes the issue: MinIO's ListObjectParts API returns the compressed size instead of the actual uploaded size for multi-part uploads when compression is enabled, causing failures in Harbor uploads due to size mismatches. The expected behavior, current behavior, and steps to reproduce are explicitly provided, including specific commands and output examples. Additionally, references to relevant code in both MinIO and the Harbor/distribution repository are included, along with links to AWS documentation for expected behavior. The problem statement also identifies the likely root cause in the MinIO codebase. There are no significant ambiguities, and the inclusion of detailed reproduction steps and environment information further enhances clarity. Therefore, this problem statement merits a score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`object-multipart-handlers.go`) and involves a small modification (adding a conditional block of 4 lines). It does not impact the broader system architecture or require changes across multiple modules. The change simply adjusts the reported size for compressed objects to use `ActualSize` instead of the compressed size, mirroring the logic already in place for encrypted objects.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of MinIO's internal handling of multi-part uploads and metadata, specifically how compression metadata is stored and accessed (`ReservedMetadataPrefix+\"compression\"`). Familiarity with Go and HTTP handlers is necessary, but these are relatively straightforward concepts for a senior engineer. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic S3 API behavior are required.\n\n3. **Edge Cases and Error Handling**: The problem statement and code change do not explicitly address complex edge cases or require additional error handling beyond the existing logic for encrypted objects. The focus is on correcting the size field for compressed parts, which appears to be a straightforward fix. However, there is a minor risk of unforeseen interactions with other metadata or compression-related features, though this is not highlighted as a concern in the problem statement.\n\n4. **Overall Complexity**: The fix is relatively simple and does not require deep architectural changes or extensive debugging. The root cause is already identified, and the solution aligns with existing patterns in the codebase (e.g., handling for encrypted objects). However, it does require some understanding of the MinIO codebase and S3 API semantics, which slightly elevates the difficulty beyond \"Very Easy.\"\n\nGiven these considerations, a difficulty score of 0.35 is appropriate, reflecting an \"Easy\" problem that requires understanding specific code logic and making a targeted modification, but does not involve significant complexity or broad impact.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Error in parsing response date\nWhen creating a service account, if the expiration time is, for example, 2024-05-04T09:35:47.1<ins>**00**</ins>Z (or 2024-05-04T09:35:47.12<ins>**0**</ins>Z). Then minio truncates 0 in milliseconds in response and the converter cannot parse the date, since the pattern requires 3 fraction-of-second ([\"yyyy-MM-dd'T'HH':'mm':'ss'.'<ins>**SSS**</ins>'Z'\"](https://github.com/minio/minio-java/blob/8.5.10/api/src/main/java/io/minio/Time.java#L32))\r\n\r\nStacktrace: \r\n```\r\nCaused by: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.ValueInstantiationException: Cannot construct instance of `io.minio.messages.ResponseDate`, problem: Text '2024-05-04T09:35:47.1Z' could not be parsed at index 19\r\n at [Source: REDACTED (`StreamReadFeature.INCLUDE_SOURCE_IN_LOCATION` disabled); line: 1, column: 120] (through reference chain: io.minio.admin.AddServiceAccountResp[\"credentials\"]->io.minio.credentials.Credentials[\"expiration\"])\r\n\t... 15 more\r\nCaused by: com.fasterxml.jackson.databind.exc.ValueInstantiationException: Cannot construct instance of `io.minio.messages.ResponseDate`, problem: Text '2024-05-04T09:35:47.1Z' could not be parsed at index 19\r\n at [Source: REDACTED (`StreamReadFeature.INCLUDE_SOURCE_IN_LOCATION` disabled); line: 1, column: 120] (through reference chain: io.minio.admin.AddServiceAccountResp[\"credentials\"]->io.minio.credentials.Credentials[\"expiration\"])\r\n\tat com.fasterxml.jackson.databind.exc.ValueInstantiationException.from(ValueInstantiationException.java:47)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.instantiationException(DeserializationContext.java:2014)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.wrapAsJsonMappingException(StdValueInstantiator.java:598)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.rewrapCtorProblem(StdValueInstantiator.java:621)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:346)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:265)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromString(BeanDeserializerBase.java:1588)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:197)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:440)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1493)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:348)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:185)\r\n\tat com.fasterxml.jackson.databind.deser.impl.FieldProperty.deserializeAndSet(FieldProperty.java:138)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:310)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:177)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:342)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4899)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3907)\r\n\tat io.minio.admin.MinioAdminClient.addServiceAccount(MinioAdminClient.java:693)\r\n\t... 90 more\r\nCaused by: java.time.format.DateTimeParseException: Text '2024-05-04T09:35:47.1Z' could not be parsed at index 19\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1954)\r\n\tat java.base/java.time.ZonedDateTime.parse(ZonedDateTime.java:600)\r\n\tat io.minio.messages.ResponseDate.fromString(ResponseDate.java:59)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat com.fasterxml.jackson.databind.introspect.AnnotatedMethod.call1(AnnotatedMethod.java:110)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromString(StdValueInstantiator.java:343)\r\n\t... 108 more\r\n```\n", "patch": "diff --git a/cmd/admin-handlers-users.go b/cmd/admin-handlers-users.go\nindex b7b76e0c293f2..b37931400c199 100644\n--- a/cmd/admin-handlers-users.go\n+++ b/cmd/admin-handlers-users.go\n@@ -2476,6 +2476,12 @@ func commonAddServiceAccount(r *http.Request) (context.Context, auth.Credentials\n \t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n \t}\n \n+\tif createReq.Expiration != nil && !createReq.Expiration.IsZero() {\n+\t\t// truncate expiration at the second.\n+\t\ttruncateTime := createReq.Expiration.Truncate(time.Second)\n+\t\tcreateReq.Expiration = &truncateTime\n+\t}\n+\n \t// service account access key cannot have space characters beginning and end of the string.\n \tif hasSpaceBE(createReq.AccessKey) {\n \t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n", "instance_id": "minio__minio-19674", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a date parsing error occurs when the expiration time in a service account response from MinIO has truncated milliseconds (e.g., '2024-05-04T09:35:47.1Z' instead of the expected '2024-05-04T09:35:47.100Z'), which does not match the expected format requiring three digits for milliseconds. The stack trace provided further clarifies the root cause as a `DateTimeParseException` in the Java client library. However, the problem statement lacks explicit mention of the desired solution or expected behavior (e.g., whether the server should pad the milliseconds or if the client should handle variable millisecond precision). Additionally, edge cases or constraints around expiration time formats are not specified. Despite these minor ambiguities, the issue is understandable with the provided context and stack trace, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The issue involves a straightforward bug fix in the server-side code to truncate the expiration time to seconds before sending it in the response, as shown in the code changes. The modification is localized to a single file (`cmd/admin-handlers-users.go`) and involves adding a simple conditional block of a few lines to truncate the time using `time.Truncate(time.Second)`. This requires basic understanding of Go's `time` package and minimal interaction with the rest of the codebase. No complex algorithms, design patterns, or architectural changes are needed. While there is a minor consideration of ensuring the truncation does not affect other parts of the system, the problem does not explicitly mention complex edge cases or performance implications beyond the date format mismatch. Error handling is not significantly impacted as the change is preventative rather than reactive. Overall, this is a simple fix that a developer with basic Go knowledge can implement, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "feat: Completing Multipart Uploads with Checksum That Contains Number of Parts Fails\n## Expected Behavior\r\nWhen completing multipart upload checksum in format `{checksum}-{numberOfParts}` should be supported.\r\n\r\n## Current Behavior\r\nWhen completing multipart upload checksum in format `{checksum}-{numberOfParts}`, `InvalidArgument: Invalid arguments provided for checksum-bug/minio-checksum-bug/2024-05-04T17:52:34.793Z.mp4: (invalid/unknown checksum sent: invalid checksum)` error is returned.\r\n\r\nVarious scenarios:\r\n1. both s3 and minio allow completing without checksum \r\n2. both s3 and minio fail on invalid checkum\r\n3. both s3 and minio allow completing with checksum that does not contain number of parts, eg. `ejt070PMd2pF/mFNTgP6LDtAo2px3L+i/l91VTgvRSc=`.\r\n4. **only s3 allows completing when checkusm with number of parts as a suffix is provided, eg. `ejt070PMd2pF/mFNTgP6LDtAo2px3L+i/l91VTgvRSc=-2`.**\r\n\r\n## Possible Solution\r\nFix somewhere in [erasure-multipart.go](https://github.com/minio/minio/blob/8ff70ea5a9210b545e3c95acb23b5a983b64da9c/cmd/erasure-multipart.go).\r\n\r\n## Steps to Reproduce (for bugs)\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n<!--- and make sure you have followed https://github.com/minio/minio/tree/release/docs/debugging to capture relevant logs -->\r\n\r\n1. Create multipart upload with `ChecksumAlgorithm: \"SHA256\"` (likely all other algorithms are also impacted)\r\n5. Upload parts with checksum\r\n6. Complete multipart upload with checksum combined with number of parts\r\n\r\n<details>\r\n<summary>Sample code (JS)</summary>\r\n\r\n```javascript\r\n\r\n// tested with node@20.12.2, minio version RELEASE.2024-05-01T01-11-10Z\r\n// save as .mjs file\r\n// set flowing env variables before starting or here:\r\n// process.env.S3_BUCKET_NAME = \"\";\r\n// process.env.AWS_REGION = \"\"; // only for s3\r\n// process.env.AWS_ACCESS_KEY_ID = \"\";\r\n// process.env.AWS_SECRET_ACCESS_KEY = \"\";\r\n// process.env.AWS_ENDPOINT_URL = \"\"; // only for minio\r\n\r\nimport {\r\n  CompleteMultipartUploadCommand,\r\n  CreateMultipartUploadCommand,\r\n  ListPartsCommand,\r\n  S3Client,\r\n  UploadPartCommand,\r\n} from \"@aws-sdk/client-s3\"; // tested with version 3.569.0\r\nimport crypto from \"node:crypto\";\r\n\r\nfunction getChecksum(buffer) {\r\n  const hash = crypto.createHash(\"sha256\");\r\n  hash.update(new Uint8Array(buffer));\r\n  return hash.digest();\r\n}\r\n\r\nconst video = await fetch(\r\n  \"https://github.com/minio/minio/assets/610941/2653d680-2c87-42ea-98d0-8feab199d3ef\"\r\n).then((response) => response.arrayBuffer());\r\n\r\nconst s3 = new S3Client({\r\n  forcePathStyle: !!process.env.AWS_ENDPOINT_URL,\r\n});\r\n\r\nconst MB_5 = 5 * 1024 * 1024;\r\nconst KEY = `minio-checksum-bug/${new Date().toISOString()}.mp4`;\r\nconst PARTS = Math.ceil(video.byteLength / MB_5);\r\n\r\nconsole.log({ KEY, PARTS });\r\n\r\nconst multipartUpload = await s3.send(\r\n  new CreateMultipartUploadCommand({\r\n    Bucket: process.env.S3_BUCKET_NAME,\r\n    Key: KEY,\r\n    ChecksumAlgorithm: \"SHA256\",\r\n  })\r\n);\r\n\r\nconst uploadId = multipartUpload.UploadId;\r\n\r\nconst checksums = [];\r\n\r\nfor (let i = 0; i < PARTS; i++) {\r\n  const start = i * MB_5;\r\n  const end = Math.min((i + 1) * MB_5, video.byteLength);\r\n  const body = video.slice(start, end);\r\n  const checksum = getChecksum(body);\r\n  const checksumBase64 = checksum.toString(\"base64\");\r\n  checksums.push(checksum);\r\n  console.log({ i, checksumBase64 });\r\n\r\n  await s3.send(\r\n    new UploadPartCommand({\r\n      Bucket: process.env.S3_BUCKET_NAME,\r\n      Key: KEY,\r\n      UploadId: uploadId,\r\n      PartNumber: i + 1,\r\n      Body: body,\r\n      ChecksumSHA256: checksum.toString(\"base64\"),\r\n    })\r\n  );\r\n}\r\n\r\nconst partsList = await s3.send(\r\n  new ListPartsCommand({\r\n    Bucket: process.env.S3_BUCKET_NAME,\r\n    Key: KEY,\r\n    UploadId: uploadId,\r\n  })\r\n);\r\n\r\nconst checksumTotal = getChecksum(Buffer.concat(checksums));\r\nconst checksumTotalBase64 = checksumTotal.toString(\"base64\");\r\nconsole.log({ checksumTotalBase64 });\r\n\r\nawait s3.send(\r\n  new CompleteMultipartUploadCommand({\r\n    Bucket: process.env.S3_BUCKET_NAME,\r\n    Key: KEY,\r\n    UploadId: uploadId,\r\n    MultipartUpload: {\r\n      Parts: partsList.Parts.map((part) => ({\r\n        PartNumber: part.PartNumber,\r\n        ETag: part.ETag,\r\n        ChecksumSHA256: part.ChecksumSHA256,\r\n      })),\r\n    },\r\n    ChecksumSHA256: \"a\" + checksumTotalBase64, // GOOD does not work in both s3 and minio\r\n    ChecksumSHA256: checksumTotalBase64, // GOOD works in both s3 and minio\r\n    ChecksumSHA256: `${checksumTotalBase64}-${PARTS}`, // BAD works in s3, does not work in minio\r\n  })\r\n);\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>Test file storage</summary>\r\n\r\nhttps://github.com/minio/minio/assets/610941/2653d680-2c87-42ea-98d0-8feab199d3ef\r\n\r\n</details>\r\n\r\n## Context\r\nNot a big deal for us at this point, we can remove part numbers from the checksum, but reporting this as a it is compatibility issue.\r\n\r\n## Regression\r\nProbably not.\r\n\r\n## Your Environment\r\nTesting locally on Windows WSL, not really relevant for this issue.\r\n```\r\nminio version RELEASE.2024-05-01T01-11-10Z (commit-id=7926401cbd5cceaacd9509f2e50e1f7d636c2eb8)\r\nRuntime: go1.21.9 linux/amd64\r\nLicense: GNU AGPLv3 <https://www.gnu.org/licenses/agpl-3.0.html>\r\nCopyright: 2015-2024 MinIO, Inc.\r\n```\r\n```\r\nLinux KUBA-PC 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n\n", "patch": "diff --git a/cmd/erasure-multipart.go b/cmd/erasure-multipart.go\nindex 536b0f3db4bab..16183609eaf81 100644\n--- a/cmd/erasure-multipart.go\n+++ b/cmd/erasure-multipart.go\n@@ -1231,7 +1231,7 @@ func (er erasureObjects) CompleteMultipartUpload(ctx context.Context, bucket str\n \t}\n \n \tif opts.WantChecksum != nil {\n-\t\terr := opts.WantChecksum.Matches(checksumCombined)\n+\t\terr := opts.WantChecksum.Matches(checksumCombined, len(parts))\n \t\tif err != nil {\n \t\t\treturn oi, err\n \t\t}\ndiff --git a/internal/hash/checksum.go b/internal/hash/checksum.go\nindex eaf81de6d7bf0..1466cec137551 100644\n--- a/internal/hash/checksum.go\n+++ b/internal/hash/checksum.go\n@@ -27,6 +27,7 @@ import (\n \t\"hash\"\n \t\"hash/crc32\"\n \t\"net/http\"\n+\t\"strconv\"\n \t\"strings\"\n \n \t\"github.com/minio/minio/internal/hash/sha256\"\n@@ -71,9 +72,10 @@ const (\n \n // Checksum is a type and base 64 encoded value.\n type Checksum struct {\n-\tType    ChecksumType\n-\tEncoded string\n-\tRaw     []byte\n+\tType      ChecksumType\n+\tEncoded   string\n+\tRaw       []byte\n+\tWantParts int\n }\n \n // Is returns if c is all of t.\n@@ -260,13 +262,14 @@ func ReadPartCheckSums(b []byte) (res []map[string]string) {\n \t\t}\n \t\t// Skip main checksum\n \t\tb = b[length:]\n-\t\tif !typ.Is(ChecksumIncludesMultipart) {\n-\t\t\tcontinue\n-\t\t}\n \t\tparts, n := binary.Uvarint(b)\n \t\tif n <= 0 {\n \t\t\tbreak\n \t\t}\n+\t\tif !typ.Is(ChecksumIncludesMultipart) {\n+\t\t\tcontinue\n+\t\t}\n+\n \t\tif len(res) == 0 {\n \t\t\tres = make([]map[string]string, parts)\n \t\t}\n@@ -292,11 +295,25 @@ func NewChecksumWithType(alg ChecksumType, value string) *Checksum {\n \tif !alg.IsSet() {\n \t\treturn nil\n \t}\n+\twantParts := 0\n+\tif strings.ContainsRune(value, '-') {\n+\t\tvalSplit := strings.Split(value, \"-\")\n+\t\tif len(valSplit) != 2 {\n+\t\t\treturn nil\n+\t\t}\n+\t\tvalue = valSplit[0]\n+\t\tnParts, err := strconv.Atoi(valSplit[1])\n+\t\tif err != nil {\n+\t\t\treturn nil\n+\t\t}\n+\t\talg |= ChecksumMultipart\n+\t\twantParts = nParts\n+\t}\n \tbvalue, err := base64.StdEncoding.DecodeString(value)\n \tif err != nil {\n \t\treturn nil\n \t}\n-\tc := Checksum{Type: alg, Encoded: value, Raw: bvalue}\n+\tc := Checksum{Type: alg, Encoded: value, Raw: bvalue, WantParts: wantParts}\n \tif !c.Valid() {\n \t\treturn nil\n \t}\n@@ -325,12 +342,15 @@ func (c *Checksum) AppendTo(b []byte, parts []byte) []byte {\n \tb = append(b, crc...)\n \tif c.Type.Is(ChecksumMultipart) {\n \t\tvar checksums int\n+\t\tif c.WantParts > 0 && !c.Type.Is(ChecksumIncludesMultipart) {\n+\t\t\tchecksums = c.WantParts\n+\t\t}\n \t\t// Ensure we don't divide by 0:\n \t\tif c.Type.RawByteLen() == 0 || len(parts)%c.Type.RawByteLen() != 0 {\n \t\t\thashLogIf(context.Background(), fmt.Errorf(\"internal error: Unexpected checksum length: %d, each checksum %d\", len(parts), c.Type.RawByteLen()))\n \t\t\tchecksums = 0\n \t\t\tparts = nil\n-\t\t} else {\n+\t\t} else if len(parts) > 0 {\n \t\t\tchecksums = len(parts) / c.Type.RawByteLen()\n \t\t}\n \t\tif !c.Type.Is(ChecksumIncludesMultipart) {\n@@ -358,7 +378,7 @@ func (c Checksum) Valid() bool {\n }\n \n // Matches returns whether given content matches c.\n-func (c Checksum) Matches(content []byte) error {\n+func (c Checksum) Matches(content []byte, parts int) error {\n \tif len(c.Encoded) == 0 {\n \t\treturn nil\n \t}\n@@ -368,6 +388,13 @@ func (c Checksum) Matches(content []byte) error {\n \t\treturn err\n \t}\n \tsum := hasher.Sum(nil)\n+\tif c.WantParts > 0 && c.WantParts != parts {\n+\t\treturn ChecksumMismatch{\n+\t\t\tWant: fmt.Sprintf(\"%s-%d\", c.Encoded, c.WantParts),\n+\t\t\tGot:  fmt.Sprintf(\"%s-%d\", base64.StdEncoding.EncodeToString(sum), parts),\n+\t\t}\n+\t}\n+\n \tif !bytes.Equal(sum, c.Raw) {\n \t\treturn ChecksumMismatch{\n \t\t\tWant: c.Encoded,\n", "instance_id": "minio__minio-19680", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "\nThe problem statement is mostly clear and provides a detailed description of the issue, including the expected behavior, current behavior, and specific scenarios where the issue occurs. It also includes a reproducible example with JavaScript code and steps to reproduce the bug, which is very helpful. The context of the issue as a compatibility problem with S3 is also mentioned, adding to the clarity of the goal. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the exact format or constraints of the checksum string beyond the `{checksum}-{numberOfParts}` pattern, nor does it discuss potential edge cases (e.g., invalid or malformed checksum strings, or behavior when the number of parts is zero or extremely large). Additionally, while a possible solution is hinted at (modifying `erasure-multipart.go`), there is no detailed discussion of the expected fix or its implications. These minor gaps result in a score of 2 (Mostly Clear) rather than 3.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.55, placing this problem in the medium range, as it requires understanding multiple technical concepts and making targeted modifications across a couple of files, with some attention to edge cases. Here's the breakdown based on the evaluation factors:\n\n1. **Clarity and Complexity of Problem Description**: While the problem is mostly clear, the logic behind checksum validation and multipart upload completion in the context of S3 compatibility introduces moderate complexity. Understanding the specific behavior difference between S3 and MinIO requires familiarity with their respective APIs and checksum handling.\n\n2. **Scope and Depth of Code Changes**: The provided code changes are relatively focused, affecting two files (`erasure-multipart.go` and `checksum.go`) in the MinIO codebase. The modifications involve altering the checksum validation logic to handle the `{checksum}-{numberOfParts}` format, which requires changes to how checksums are parsed and validated. The changes do not appear to impact the broader system architecture significantly, but they do require understanding the interaction between multipart upload completion and checksum validation logic. The amount of code change is moderate, with additions for parsing the number of parts suffix and adjusting validation logic.\n\n3. **Number of Technical Concepts**: Solving this problem requires knowledge of several concepts, including:\n   - Go programming language features (e.g., string parsing, base64 encoding/decoding, and error handling).\n   - Understanding of checksum algorithms (e.g., SHA256) and their representation in S3/MinIO.\n   - Familiarity with S3 API compatibility, specifically multipart upload and checksum handling.\n   - Domain-specific knowledge of distributed storage systems and how MinIO implements erasure coding and multipart uploads.\n   These concepts are moderately complex, especially for someone unfamiliar with S3 compatibility or MinIO's internals, but they are not overly advanced for an experienced engineer.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes introduce logic to handle the checksum suffix format, including parsing the number of parts and validating it against the expected value. Potential edge cases include malformed checksum strings (e.g., multiple hyphens, non-numeric suffixes), zero or negative parts, or mismatches between the provided and actual number of parts. The code changes address some of these by adding validation, but the complexity of handling all possible edge cases is moderate. Additional error handling logic is introduced (e.g., `ChecksumMismatch` with part count), which adds to the task's difficulty.\n\nOverall, this problem falls into the medium difficulty range (0.4-0.6) because it requires a solid understanding of specific parts of the MinIO codebase, moderate code changes across two files, and attention to compatibility and error handling. It does not reach the hard range (0.6-0.8) as it does not involve deep architectural changes, complex algorithms, or extensive performance optimizations. A score of 0.55 reflects the balance between the need for targeted expertise and the relatively contained scope of the changes.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Driver \"zfs\" failed to remove root filesystem: \"cannot open '<dataset-path>': dataset does not exist\"\n<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/moby/moby/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support for **docker** can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Slack - community.docker.com #general channel\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\nGeneral support for **moby** can be found at the following locations:\r\n\r\n- Moby Project Forums - https://forums.mobyproject.org\r\n- Slack - community.docker.com #moby-project channel\r\n- Post a question on StackOverflow, using the Moby tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\nwe are using [openebs/zfs ](https://github.com/openebs/zfs-localpv/blob/develop/deploy/zfs-operator.yaml) and we have multiple reports where users are having issues that boils down to the docker with zfs storage driver.\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\nPods are stuck in terminating state:\r\n```\r\ncontainer <ID> driver \"zfs\" failed to remove root filesystem: exit status 1: \"zfs fs destroy -r <dataset-path>\" => cannot open '<dataset-path>': dataset does not exist\"\r\n```\r\n\r\nReports:\r\n- https://jira.ixsystems.com/browse/NAS-112418\r\n- https://www.truenas.com/community/threads/openebs-zfs-driver-removal-in-progress-for-4-weeks.96192/\r\n- https://help.nextcloud.com/t/solved-cannot-start-service-db-error-creating-zfs-mount-after-zfs-snapshot-rollback/61695\r\n\r\nThis seems to have a workaround where you manually create missing datasets to make docker happy and things come back to normal.. which is tedious.\r\n\r\n**Steps to reproduce the issue:**\r\nUnable to find concrete steps to reproduce, it seems to be a raise condition or non-atomic operation where driver cloudn't finish the removal of the layers datasets but gets into crashloop and most of times its on system restart / power cut when pods are stuck in terminating state giving the following logs:\r\n\r\n**Describe the results you received:**\r\n\r\n```bash\r\ntruenas# docker ps -a\r\nCONTAINER ID   IMAGE                         COMMAND                  CREATED       STATUS                     PORTS     NAMES\r\nd39f9d831680   openebs/zfs-driver            \"/usr/local/bin/zfs-\u2026\"   4 weeks ago   Removal In Progress                  k8s_openebs-zfs-plugin_openebs-zfs-node-ds5sd_kube-system_d692ac39-ac25-4da6-adbb-93615d9cef3b_203\r\n4f7053116229   rancher/pause:3.1             \"/pause\"                 4 weeks ago   Exited (255) 4 weeks ago             k8s_POD_openebs-zfs-node-ds5sd_kube-system_d692ac39-ac25-4da6-adbb-93615d9cef3b_27\r\n\r\n\r\ntruenas# k get pods -A\r\nNAMESPACE          NAME                                    READY   STATUS        RESTARTS   AGE\r\nkube-system        openebs-zfs-node-ds5sd                  0/2     Terminating   315        106d\r\nkube-system        coredns-7448499f4d-hmsv9                1/1     Running       0          8h\r\nkube-system        openebs-zfs-controller-0                5/5     Running       0          8h\r\n\r\n#######\r\n## LOGS:\r\n#######\r\nOct 27 01:36:10 truenas k3s[8629]: I1027 01:36:10.703743    8629 scope.go:111] \"RemoveContainer\" containerID=\"d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b\"\r\nOct 27 01:36:10 truenas dockerd[8071]: time=\"2021-10-27T01:36:10.739172443+02:00\" level=error msg=\"Error removing mounted layer d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b: exit status 1: \\\"/usr/sbin/zfs fs destroy -r storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24\\\" => cannot open 'storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24': dataset does not exist\\n\"\r\nOct 27 01:36:10 truenas dockerd[8071]: time=\"2021-10-27T01:36:10.739870694+02:00\" level=error msg=\"Handler for DELETE /v1.41/containers/d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b returned error: container d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b: driver \\\"zfs\\\" failed to remove root filesystem: exit status 1: \\\"/usr/sbin/zfs fs destroy -r storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24\\\" => cannot open 'storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24': dataset does not exist\\n\"\r\nOct 27 01:36:10 truenas k3s[8629]: E1027 01:36:10.740889    8629 remote_runtime.go:296] \"RemoveContainer from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to remove container \\\"d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b\\\": Error response from daemon: container d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b: driver \\\"zfs\\\" failed to remove root filesystem: exit status 1: \\\"/usr/sbin/zfs fs destroy -r storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24\\\" => cannot open 'storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24': dataset does not exist\" containerID=\"d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b\"\r\nOct 27 01:36:10 truenas k3s[8629]: E1027 01:36:10.741271    8629 kuberuntime_gc.go:146] \"Failed to remove container\" err=\"rpc error: code = Unknown desc = failed to remove container \\\"d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b\\\": Error response from daemon: container d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b: driver \\\"zfs\\\" failed to remove root filesystem: exit status 1: \\\"/usr/sbin/zfs fs destroy -r storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24\\\" => cannot open 'storage_404/ix-applications/docker/3114cdddca0eb01f1de848925eba4d027796b424b91f12d843673e174667bd24': dataset does not exist\" containerID=\"d39f9d83168090d640c8473d2311ec52be722e58951549aef32f187e59f90f8b\"\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n```\r\nClient: Docker Engine - Community\r\n Version:           20.10.11\r\n API version:       1.41\r\n Go version:        go1.16.9\r\n Git commit:        dea9396\r\n Built:             Thu Nov 18 00:37:22 2021\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.11\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.16.9\r\n  Git commit:       847da18\r\n  Built:            Thu Nov 18 00:35:31 2021\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.12\r\n  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d\r\n runc:\r\n  Version:          1.0.2\r\n  GitCommit:        v1.0.2-0-g52b36a2\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.6.3-docker)\r\n  scan: Docker Scan (Docker Inc., v0.9.0)\r\n\r\nServer:\r\n Containers: 36\r\n  Running: 33\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 23\r\n Server Version: 20.10.11\r\n Storage Driver: zfs\r\n  Zpool: evo\r\n  Zpool Health: ONLINE\r\n  Parent Dataset: evo/ix-applications/docker\r\n  Space Used By Parent: 2543288320\r\n  Space Available: 16198967296\r\n  Parent Quota: no\r\n  Compression: lz4\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7b11cfaabd73bb80907dd23182b9347b4245eb5d\r\n runc version: v1.0.2-0-g52b36a2\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.10.70+truenas\r\n Operating System: Debian GNU/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 13.53GiB\r\n Name: truenas.local\r\n ID: NDIM:5V6V:MQ5E:QNFP:VPAB:4EYO:YKNL:EF55:WW5R:2IQM:ARAF:5NIG\r\n Docker Root Dir: /mnt/evo/ix-applications/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\n", "patch": "diff --git a/daemon/graphdriver/zfs/zfs.go b/daemon/graphdriver/zfs/zfs.go\nindex cca34d6284739..625daf43786b3 100644\n--- a/daemon/graphdriver/zfs/zfs.go\n+++ b/daemon/graphdriver/zfs/zfs.go\n@@ -361,6 +361,17 @@ func (d *Driver) Remove(id string) error {\n \tname := d.zfsPath(id)\n \tdataset := zfs.Dataset{Name: name}\n \terr := dataset.Destroy(zfs.DestroyRecursive)\n+\tif err != nil {\n+\t\tvar errZfs *zfs.Error\n+\t\tisZfsError := errors.As(err, &errZfs)\n+\t\tif isZfsError && strings.HasSuffix(strings.TrimSpace(errZfs.Stderr), \"dataset does not exist\") {\n+\t\t\tlog.G(context.TODO()).WithFields(log.Fields{\n+\t\t\t\t\"error\":          err,\n+\t\t\t\t\"storage-driver\": \"zfs\",\n+\t\t\t}).Warnf(\"Tried to destroy inexistent dataset %q\", name)\n+\t\t\terr = nil\n+\t\t}\n+\t}\n \tif err == nil {\n \t\td.Lock()\n \t\tdelete(d.filesystemsCache, name)\n", "instance_id": "moby__moby-48520", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: Docker with the ZFS storage driver fails to remove a root filesystem due to a missing dataset, leading to pods being stuck in a terminating state. The description includes relevant logs, environment details, and links to related reports, which provide context. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement lacks concrete steps to reproduce the issue, mentioning only that it seems to be a race condition or non-atomic operation during system restarts or power cuts. Additionally, there are no explicit mentions of expected behavior beyond a general desire for the issue to be resolved without manual intervention. Edge cases or specific conditions under which the problem occurs are not detailed, which could complicate debugging or solution design. Overall, while the goal and impact are clear, the lack of reproducibility steps and incomplete specification of edge cases or constraints slightly lowers the clarity score.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.35, placing this problem in the \"Easy\" range (0.2-0.4), as it requires understanding some code logic and making a relatively simple modification, though with a slight increase in complexity due to the domain context.\n\n1. **Clarity and Complexity of Problem Description**: The problem, while not fully detailed, points to a specific error in the ZFS driver when attempting to destroy a dataset that does not exist. The logic to solve this\u2014ignoring the error if the dataset is already gone\u2014is straightforward, as reflected in the code change.\n\n2. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`zfs.go`) and a specific function (`Remove` in the `Driver` struct). It involves a small modification (adding a conditional check for a specific error message) with minimal impact on the broader codebase or system architecture. The change is essentially a patch to handle a specific error condition, requiring only about 10 lines of code. There is no need to interact with multiple modules or refactor significant portions of the system.\n\n3. **Number of Technical Concepts**: Solving this requires a basic understanding of Go (error handling with `errors.As`, string manipulation), familiarity with the ZFS storage driver in Docker, and logging practices (using `log.G`). These are relatively simple concepts for a senior engineer. No advanced algorithms, design patterns, or deep domain-specific knowledge beyond basic ZFS operations are needed. The primary challenge is understanding the context of the error message and ensuring the fix does not introduce unintended side effects.\n\n4. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond the dataset not existing, and the code change directly addresses this by suppressing the error in that specific scenario. However, there is a minor risk of masking other errors if the error message check is too broad or if future changes to ZFS error outputs alter the expected string. The error handling logic added is simple but requires careful consideration to avoid overgeneralization. No complex edge cases (e.g., partial dataset deletion, permissions issues) are mentioned or handled in the change, which keeps the difficulty low.\n\nOverall, this problem is on the higher end of \"Easy\" due to the need to understand the ZFS driver context and ensure the error suppression logic is precise. However, it does not reach \"Medium\" difficulty (0.4-0.6) because the change is isolated, the concepts are basic, and the impact on the codebase is minimal. A senior engineer with experience in Go and storage drivers should be able to implement and verify this fix quickly, likely within a few hours, including testing.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Act fails when tag doesn't start with \"v\"\n### Bug report info\n\n```plain text\nact version:            0.2.68\r\nGOOS:                   linux\r\nGOARCH:                 amd64\r\nNumCPU:                 8\r\nDocker host:            DOCKER_HOST environment variable is not set\r\nSockets found:\r\n        /var/run/docker.sock\r\nBuild info:\r\n        Go version:            go1.21.13\r\n        Module path:           github.com/nektos/act\r\n        Main version:          (devel)\r\n        Main path:             github.com/nektos/act\r\n        Main checksum:         \r\n        Build settings:\r\n                -buildmode:           exe\r\n                -compiler:            gc\r\n                -ldflags:             -s -w -X main.version=0.2.68 -X main.commit=03a4480cd024910c88c8946da8d589b9ef0c6660 -X main.date=2024-10-01T02:31:47Z -X main.builtBy=goreleaser\r\n                CGO_ENABLED:          0\r\n                GOARCH:               amd64\r\n                GOOS:                 linux\r\n                GOAMD64:              v1\r\n                vcs:                  git\r\n                vcs.revision:         03a4480cd024910c88c8946da8d589b9ef0c6660\r\n                vcs.time:             2024-10-01T02:31:30Z\r\n                vcs.modified:         false\r\nDocker Engine:\r\n        Engine version:        27.3.1\r\n        Engine runtime:        runc\r\n        Cgroup version:        2\r\n        Cgroup driver:         systemd\r\n        Storage driver:        overlay2\r\n        Registry URI:          https://index.docker.io/v1/\r\n        OS:                    elementary OS 7.1 Horus\r\n        OS type:               linux\r\n        OS version:            7.1\r\n        OS arch:               x86_64\r\n        OS kernel:             6.8.0-48-generic\r\n        OS CPU:                8\r\n        OS memory:             15705 MB\r\n        Security options:\r\n                name=apparmor\r\n                name=seccomp,profile=builtin\r\n                name=cgroupns\n```\n\n\n### Command used with act\n\n```sh\nact\n```\n\n\n### Describe issue\n\nAct incorrectly reports:\r\n\r\n> Unable to resolve action `http://github.com/mad9000/actions-find-and-replace-string@5`, the provided ref `5` is the shortened version of a commit SHA, which is not supported. Please use the full commit SHA `51ad904dfdc3aea1a4f06e925a18a4cfddae7041` instead\r\n\r\nThe workflow is valid: https://github.com/mad9000/actions-find-and-replace-string/blob/main/.github/workflows/main.yml\n\n### Link to GitHub repository\n\nhttps://github.com/mad9000/actions-find-and-replace-string\n\n### Workflow content\n\n```yml\non: [push]\r\n\r\njobs:\r\n  find-and-replace-job:\r\n    runs-on: ubuntu-latest\r\n    name: 'Find and replace on a string'\r\n    steps:\r\n      - name: Checkout\r\n        uses: actions/checkout@v4\r\n      - name: Run find-and-replace to remove refs/heads/\r\n        uses: mad9000/actions-find-and-replace-string@5\r\n        id: findandreplace\r\n        with:\r\n          source: ${{ github.ref }}\r\n          find: 'refs/heads/'\r\n          replace: ''\r\n      - name: Get the above output\r\n        run: echo \"The replaced value is ${{ steps.findandreplace.outputs.value }}\"\r\n      - name: Run find-and-replace to remove slashes\r\n        uses: mad9000/actions-find-and-replace-string@5\r\n        id: findandreplace2\r\n        with:\r\n          source: ${{ steps.findandreplace.outputs.value }}\r\n          find: '/'\r\n          replace: '-'\r\n      - name: Get the final output\r\n        run: echo \"The replaced value is ${{ steps.findandreplace2.outputs.value }}\"\n```\n\n\n### Relevant log output\n\n```sh\n\u2601  git clone 'http://github.com/mad9000/actions-find-and-replace-string' # ref=5\r\n  cloning http://github.com/mad9000/actions-find-and-replace-string to /root/.cache/act/bb58a4f8ee533c336ce085aec3674f7930cd02c237df00206beaa4e78ed0a721\r\nUnable to resolve action `http://github.com/mad9000/actions-find-and-replace-string@5`, the provided ref `5` is the shortened version of a commit SHA, which is not supported. Please use the full commit SHA `51ad904dfdc3aea1a4f06e925a18a4cfddae7041` instead\n```\n\n\n### Additional information\n\n_No response_\n", "patch": "diff --git a/pkg/common/git/git.go b/pkg/common/git/git.go\nindex 706f9ca5802..3c5af7b3f5c 100644\n--- a/pkg/common/git/git.go\n+++ b/pkg/common/git/git.go\n@@ -330,7 +330,7 @@ func NewGitCloneExecutor(input NewGitCloneExecutorInput) common.Executor {\n \t\t\tlogger.Errorf(\"Unable to resolve %s: %v\", input.Ref, err)\n \t\t}\n \n-\t\tif hash.String() != input.Ref && strings.HasPrefix(hash.String(), input.Ref) {\n+\t\tif hash.String() != input.Ref && len(input.Ref) >= 4 && strings.HasPrefix(hash.String(), input.Ref) {\n \t\t\treturn &Error{\n \t\t\t\terr:    ErrShortRef,\n \t\t\t\tcommit: hash.String(),\n", "instance_id": "nektos__act-2540", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the tool 'act' fails to resolve a GitHub action when the provided reference (e.g., '5') is a shortened version of a commit SHA. The goal is to address this failure, and the issue is supported by relevant logs, workflow content, and a link to the repository. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior (e.g., should 'act' automatically resolve short refs to full SHAs, or reject them with a better error message?). Additionally, edge cases such as invalid or ambiguous short refs are not mentioned. While the provided log output and workflow content help contextualize the issue, the lack of explicit requirements or constraints slightly reduces the clarity. Hence, a score of 2 (Mostly Clear) is appropriate.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the 'Easy' category (0.2-0.4). The code change provided is minimal, involving a single line modification in a specific function within the 'git.go' file. The change adds a condition to check if the input reference length is at least 4 characters before determining if it is a shortened SHA, which suggests a simple bug fix to prevent overly short refs from triggering the error. The scope of the change is limited to a single file and does not impact the broader architecture or require understanding complex interactions across the codebase. The technical concepts involved are basic: string manipulation and conditional logic in Go, with no advanced algorithms, libraries, or domain-specific knowledge required. Edge cases are not explicitly handled in the provided change, and the problem statement does not highlight complex error conditions beyond the primary issue of short refs. Overall, this is a straightforward fix that requires minimal effort and understanding of the surrounding code, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Postgres\u6570\u636e\u5e93\u8fde\u63a5\u65f6\u4e0d\u652f\u6301\u7a7a\u5bc6\u7801\n### Please make sure of the following things\n\n- [x] I have read the [documentation](https://alist.nn.ci).\n\u6211\u5df2\u7ecf\u9605\u8bfb\u4e86[\u6587\u6863](https://alist.nn.ci)\u3002\n\n- [x] I'm sure there are no duplicate issues or discussions.\n\u6211\u786e\u5b9a\u6ca1\u6709\u91cd\u590d\u7684issue\u6216\u8ba8\u8bba\u3002\n\n- [x] I'm sure it's due to `AList` and not something else(such as [Network](https://alist.nn.ci/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host) ,`Dependencies` or `Operational`).\n\u6211\u786e\u5b9a\u662f`AList`\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5176\u4ed6\u539f\u56e0\uff08\u4f8b\u5982[\u7f51\u7edc](https://alist.nn.ci/zh/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host)\uff0c`\u4f9d\u8d56`\u6216`\u64cd\u4f5c`\uff09\u3002\n\n- [x] I'm sure this issue is not fixed in the latest version.\n\u6211\u786e\u5b9a\u8fd9\u4e2a\u95ee\u9898\u5728\u6700\u65b0\u7248\u672c\u4e2d\u6ca1\u6709\u88ab\u4fee\u590d\u3002\n\n\n### AList Version / AList \u7248\u672c\n\nv3.42.0/docker:main-aio\n\n### Driver used / \u4f7f\u7528\u7684\u5b58\u50a8\u9a71\u52a8\n\n*\n\n### Describe the bug / \u95ee\u9898\u63cf\u8ff0\n\n\u5173\u8054issue: #5168,#4133\n\n\u5728\u4e2a\u4eba\u914d\u7f6e\u6ca1\u6709\u8bbe\u7f6e\u5bc6\u7801\u7684\u60c5\u51b5\u4e0b\uff0calist\u4f1a\u5ffd\u89c6`database.name`\u9879\u914d\u7f6e\uff0c\u6570\u636e\u5e93\u8868\u5747\u751f\u6210\u5728`postgres/public`\u4e0b, \u4e5f\u5c31\u662f\u5f53\u524d\u7528\u6237\u4e0b\n\u5728\u5185\u7f51\u4f7f\u7528\u65f6, postgres\u53ef\u4ee5\u901a\u8fc7hba.conf\u8bbe\u7f6e\u5185\u7f51ip\u6bb5\u4e3atrust, \u4ece\u800c\u65e0\u9700\u5bc6\u7801\u8fde\u63a5\u3002\n\u8fd9\u662f\u4e00\u4e2a\u5f88\u6b63\u5e38\u7684\u4f7f\u7528\u573a\u666f\uff0c\u5e0c\u671b\u4fee\u590d\u3002\n\n\u9644\u4e0a\u4e34\u65f6\u7684\u89e3\u51b3\u65b9\u6848: password\u5904\u4e0d\u4e3a\u7a7a\u5373\u53ef, \u4e2a\u4eba\u6d4b\u8bd5`@`\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\n\n### Reproduction / \u590d\u73b0\u94fe\u63a5\n\n\u6682\u65e0\n\n### Config / \u914d\u7f6e\n\n  ```\n\"database\": {\n    \"type\": \"postgres\",\n    \"host\": \"postgres\",\n    \"port\": 5432,\n    \"user\": \"postgres\",\n    \"password\": \"\",\n    \"name\": \"alist\",\n    \"table_prefix\": \"x_\",\n    \"ssl_mode\": \"disable\"\n  }\n```\n\n### Logs / \u65e5\u5fd7\n\n_No response_\n", "patch": "diff --git a/internal/bootstrap/db.go b/internal/bootstrap/db.go\nindex 39b659b78f1..5f5f6fcef3e 100644\n--- a/internal/bootstrap/db.go\n+++ b/internal/bootstrap/db.go\n@@ -68,8 +68,13 @@ func InitDB() {\n \t\t\t{\n \t\t\t\tdsn := database.DSN\n \t\t\t\tif dsn == \"\" {\n-\t\t\t\t\tdsn = fmt.Sprintf(\"host=%s user=%s password=%s dbname=%s port=%d sslmode=%s TimeZone=Asia/Shanghai\",\n-\t\t\t\t\t\tdatabase.Host, database.User, database.Password, database.Name, database.Port, database.SSLMode)\n+\t\t\t\t\tif database.Password != \"\" {\n+\t\t\t\t\t\tdsn = fmt.Sprintf(\"host=%s user=%s password=%s dbname=%s port=%d sslmode=%s TimeZone=Asia/Shanghai\",\n+\t\t\t\t\t\t\tdatabase.Host, database.User, database.Password, database.Name, database.Port, database.SSLMode)\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tdsn = fmt.Sprintf(\"host=%s user=%s dbname=%s port=%d sslmode=%s TimeZone=Asia/Shanghai\",\n+\t\t\t\t\t\t\tdatabase.Host, database.User, database.Name, database.Port, database.SSLMode)\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\tdB, err = gorm.Open(postgres.Open(dsn), gormConfig)\n \t\t\t}\n", "instance_id": "AlistGo__alist-8198", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: AList does not support connecting to a Postgres database with an empty password, which leads to the database tables being created under the default 'postgres/public' schema instead of the specified database name. The user also mentions a common use case (intranet setups with trust-based authentication via hba.conf) and provides a temporary workaround. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention expected behavior beyond \"fixing\" the issue (e.g., should the connection string format change, or should there be additional validation?). Additionally, there are no specific examples of reproduction steps or detailed logs to confirm the issue's scope. While the intent is understandable, these missing details prevent it from being fully comprehensive.\n", "difficulty_explanation": "\nThe difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal and confined to a single file (`db.go`). It involves a simple conditional check to handle the case of an empty password by modifying the DSN (Data Source Name) string format used for the Postgres connection. The change does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is small, adding just a few lines.\n\n2. **Number of Technical Concepts**: The solution requires basic knowledge of string formatting in Go (`fmt.Sprintf`) and an understanding of how database connection strings are constructed for Postgres. It also involves familiarity with the `gorm` library for database operations in Go, but the usage here is straightforward. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases beyond the empty password scenario, and the code change does not introduce new error handling logic. The modification is a direct fix for the reported issue without apparent additional complexity in handling edge cases. However, a developer might need to consider if other connection parameters could be empty or invalid, though this is not indicated as a concern in the problem or code change.\n\n4. **Overall Complexity**: The task is a simple bug fix that requires understanding a small part of the codebase related to database initialization. It does not involve deep architectural changes, performance considerations, or complex logic. The fix is essentially a conditional tweak to the connection string format, which is a routine task for someone familiar with Go and database connections.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic code modification and minimal understanding of the surrounding logic.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use \"git cherry-pick\" instead of interactive rebase to implement cherry-picking\nWe are currently using an interactive rebase to implement cherry-picking. The main reason for doing this is that when you cherry-pick multiple commits, and one of them conflicts, then you get lazygit's nice visualization of the rebase todo list to show you where in the sequence you are, what the conflicting commit is, how many are left etc. Another advantage is that this makes it easy to support cherry-picking in the middle of an interactive rebase, by just inserting the pasted commits as \"pick\" entries into `git-rebase-todo` at that point.\r\n\r\nThe downside is that cherry-picking merge commits is not supported this way, since the \"pick\" command doesn't work with merges (see [this mailing list thread](https://public-inbox.org/git/424f2e08-a2ad-4bb2-8a6b-136c426dc127@haller-berlin.de/)). Cherry-picking merge commits is a useful and requested feature though, see #1374.\r\n\r\nIf we add a way to visualize the `.git/sequencer/todo` file (which is used by git during cherry-picking) in a similar way to how we do for `.git/rebase-merge/git-rebase-todo` (just read-only), including the logic to show the fake \"conflict\" entry, then we could use `git cherry-pick -m1` for implementing copy/paste, which would then work for merge commits.\r\n\r\nThe downside is that it's much more complex to do; especially for the case that the cherry-pick happened in the middle of an interactive rebase, and conflicted. Our todo section would then have to show a merged view of both `.git/sequencer/todo` and `git-rebase-todo`.\r\n\r\nAll of this is very similar to what we would have to do to support #2442.\n", "patch": "diff --git a/pkg/app/daemon/daemon.go b/pkg/app/daemon/daemon.go\nindex 782df5f1fed..16b9bf5e5ee 100644\n--- a/pkg/app/daemon/daemon.go\n+++ b/pkg/app/daemon/daemon.go\n@@ -8,7 +8,6 @@ import (\n \t\"os/exec\"\n \t\"strconv\"\n \n-\t\"github.com/jesseduffield/lazygit/pkg/commands/models\"\n \t\"github.com/jesseduffield/lazygit/pkg/common\"\n \t\"github.com/jesseduffield/lazygit/pkg/utils\"\n \t\"github.com/samber/lo\"\n@@ -33,7 +32,6 @@ const (\n \n \tDaemonKindExitImmediately\n \tDaemonKindRemoveUpdateRefsForCopiedBranch\n-\tDaemonKindCherryPick\n \tDaemonKindMoveTodosUp\n \tDaemonKindMoveTodosDown\n \tDaemonKindInsertBreak\n@@ -56,7 +54,6 @@ func getInstruction() Instruction {\n \tmapping := map[DaemonKind]func(string) Instruction{\n \t\tDaemonKindExitImmediately:                 deserializeInstruction[*ExitImmediatelyInstruction],\n \t\tDaemonKindRemoveUpdateRefsForCopiedBranch: deserializeInstruction[*RemoveUpdateRefsForCopiedBranchInstruction],\n-\t\tDaemonKindCherryPick:                      deserializeInstruction[*CherryPickCommitsInstruction],\n \t\tDaemonKindChangeTodoActions:               deserializeInstruction[*ChangeTodoActionsInstruction],\n \t\tDaemonKindDropMergeCommit:                 deserializeInstruction[*DropMergeCommitInstruction],\n \t\tDaemonKindMoveFixupCommitDown:             deserializeInstruction[*MoveFixupCommitDownInstruction],\n@@ -180,39 +177,6 @@ func NewRemoveUpdateRefsForCopiedBranchInstruction() Instruction {\n \treturn &RemoveUpdateRefsForCopiedBranchInstruction{}\n }\n \n-type CherryPickCommitsInstruction struct {\n-\tTodo string\n-}\n-\n-func NewCherryPickCommitsInstruction(commits []*models.Commit) Instruction {\n-\ttodoLines := lo.Map(commits, func(commit *models.Commit, _ int) TodoLine {\n-\t\treturn TodoLine{\n-\t\t\tAction: \"pick\",\n-\t\t\tCommit: commit,\n-\t\t}\n-\t})\n-\n-\ttodo := TodoLinesToString(todoLines)\n-\n-\treturn &CherryPickCommitsInstruction{\n-\t\tTodo: todo,\n-\t}\n-}\n-\n-func (self *CherryPickCommitsInstruction) Kind() DaemonKind {\n-\treturn DaemonKindCherryPick\n-}\n-\n-func (self *CherryPickCommitsInstruction) SerializedInstructions() string {\n-\treturn serializeInstruction(self)\n-}\n-\n-func (self *CherryPickCommitsInstruction) run(common *common.Common) error {\n-\treturn handleInteractiveRebase(common, func(path string) error {\n-\t\treturn utils.PrependStrToTodoFile(path, []byte(self.Todo))\n-\t})\n-}\n-\n type ChangeTodoActionsInstruction struct {\n \tChanges []ChangeTodoAction\n }\ndiff --git a/pkg/commands/git_commands/rebase.go b/pkg/commands/git_commands/rebase.go\nindex 1c0497da8a6..188fb72cd56 100644\n--- a/pkg/commands/git_commands/rebase.go\n+++ b/pkg/commands/git_commands/rebase.go\n@@ -533,35 +533,15 @@ func (self *RebaseCommands) DiscardOldFileChanges(commits []*models.Commit, comm\n \n // CherryPickCommits begins an interactive rebase with the given hashes being cherry picked onto HEAD\n func (self *RebaseCommands) CherryPickCommits(commits []*models.Commit) error {\n-\tcommitLines := lo.Map(commits, func(commit *models.Commit, _ int) string {\n-\t\treturn fmt.Sprintf(\"%s %s\", utils.ShortHash(commit.Hash), commit.Name)\n-\t})\n-\tmsg := utils.ResolvePlaceholderString(\n-\t\tself.Tr.Log.CherryPickCommits,\n-\t\tmap[string]string{\n-\t\t\t\"commitLines\": strings.Join(commitLines, \"\\n\"),\n-\t\t},\n-\t)\n-\tself.os.LogCommand(msg, false)\n-\n-\treturn self.PrepareInteractiveRebaseCommand(PrepareInteractiveRebaseCommandOpts{\n-\t\tbaseHashOrRoot: \"HEAD\",\n-\t\tinstruction:    daemon.NewCherryPickCommitsInstruction(commits),\n-\t}).Run()\n-}\n-\n-// CherryPickCommitsDuringRebase simply prepends the given commits to the existing git-rebase-todo file\n-func (self *RebaseCommands) CherryPickCommitsDuringRebase(commits []*models.Commit) error {\n-\ttodoLines := lo.Map(commits, func(commit *models.Commit, _ int) daemon.TodoLine {\n-\t\treturn daemon.TodoLine{\n-\t\t\tAction: \"pick\",\n-\t\t\tCommit: commit,\n-\t\t}\n-\t})\n+\thasMergeCommit := lo.SomeBy(commits, func(c *models.Commit) bool { return c.IsMerge() })\n+\tcmdArgs := NewGitCmd(\"cherry-pick\").\n+\t\tArg(\"--allow-empty\").\n+\t\tArgIf(self.version.IsAtLeast(2, 45, 0), \"--empty=keep\", \"--keep-redundant-commits\").\n+\t\tArgIf(hasMergeCommit, \"-m1\").\n+\t\tArg(lo.Reverse(lo.Map(commits, func(c *models.Commit, _ int) string { return c.Hash }))...).\n+\t\tToArgv()\n \n-\ttodo := daemon.TodoLinesToString(todoLines)\n-\tfilePath := filepath.Join(self.repoPaths.worktreeGitDirPath, \"rebase-merge/git-rebase-todo\")\n-\treturn utils.PrependStrToTodoFile(filePath, []byte(todo))\n+\treturn self.cmd.New(cmdArgs).Run()\n }\n \n func (self *RebaseCommands) DropMergeCommit(commits []*models.Commit, commitIndex int) error {\ndiff --git a/pkg/gui/controllers/basic_commits_controller.go b/pkg/gui/controllers/basic_commits_controller.go\nindex 925f9e9d585..76dd5500632 100644\n--- a/pkg/gui/controllers/basic_commits_controller.go\n+++ b/pkg/gui/controllers/basic_commits_controller.go\n@@ -366,10 +366,6 @@ func (self *BasicCommitsController) canCopyCommits(selectedCommits []*models.Com\n \t\tif commit.Hash == \"\" {\n \t\t\treturn &types.DisabledReason{Text: self.c.Tr.CannotCherryPickNonCommit, ShowErrorInPanel: true}\n \t\t}\n-\n-\t\tif commit.IsMerge() {\n-\t\t\treturn &types.DisabledReason{Text: self.c.Tr.CannotCherryPickMergeCommit, ShowErrorInPanel: true}\n-\t\t}\n \t}\n \n \treturn nil\ndiff --git a/pkg/gui/controllers/helpers/cherry_pick_helper.go b/pkg/gui/controllers/helpers/cherry_pick_helper.go\nindex 1cb2285d002..6b13b0fda74 100644\n--- a/pkg/gui/controllers/helpers/cherry_pick_helper.go\n+++ b/pkg/gui/controllers/helpers/cherry_pick_helper.go\n@@ -77,41 +77,23 @@ func (self *CherryPickHelper) Paste() error {\n \t\t\t\t\"numCommits\": strconv.Itoa(len(self.getData().CherryPickedCommits)),\n \t\t\t}),\n \t\tHandleConfirm: func() error {\n-\t\t\tisInRebase, err := self.c.Git().Status.IsInRebase()\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tif isInRebase {\n-\t\t\t\tif err := self.c.Git().Rebase.CherryPickCommitsDuringRebase(self.getData().CherryPickedCommits); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\terr = self.c.Refresh(types.RefreshOptions{\n-\t\t\t\t\tMode: types.SYNC, Scope: []types.RefreshableView{types.REBASE_COMMITS},\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\n-\t\t\t\treturn self.Reset()\n-\t\t\t}\n-\n \t\t\treturn self.c.WithWaitingStatus(self.c.Tr.CherryPickingStatus, func(gocui.Task) error {\n \t\t\t\tself.c.LogAction(self.c.Tr.Actions.CherryPick)\n-\t\t\t\terr := self.c.Git().Rebase.CherryPickCommits(self.getData().CherryPickedCommits)\n-\t\t\t\terr = self.rebaseHelper.CheckMergeOrRebase(err)\n+\t\t\t\tresult := self.c.Git().Rebase.CherryPickCommits(self.getData().CherryPickedCommits)\n+\t\t\t\terr := self.rebaseHelper.CheckMergeOrRebase(result)\n \t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n+\t\t\t\t\treturn result\n \t\t\t\t}\n \n-\t\t\t\t// If we're in an interactive rebase at this point, it must\n+\t\t\t\t// If we're in the cherry-picking state at this point, it must\n \t\t\t\t// be because there were conflicts. Don't clear the copied\n-\t\t\t\t// commits in this case, since we might want to abort and\n-\t\t\t\t// try pasting them again.\n-\t\t\t\tisInRebase, err = self.c.Git().Status.IsInRebase()\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n+\t\t\t\t// commits in this case, since we might want to abort and try\n+\t\t\t\t// pasting them again.\n+\t\t\t\tisInCherryPick, result := self.c.Git().Status.IsInCherryPick()\n+\t\t\t\tif result != nil {\n+\t\t\t\t\treturn result\n \t\t\t\t}\n-\t\t\t\tif !isInRebase {\n+\t\t\t\tif !isInCherryPick {\n \t\t\t\t\tself.getData().DidPaste = true\n \t\t\t\t\tself.rerender()\n \t\t\t\t}\ndiff --git a/pkg/gui/modes/cherrypicking/cherry_picking.go b/pkg/gui/modes/cherrypicking/cherry_picking.go\nindex d84cec39ace..82dae33230b 100644\n--- a/pkg/gui/modes/cherrypicking/cherry_picking.go\n+++ b/pkg/gui/modes/cherrypicking/cherry_picking.go\n@@ -59,11 +59,7 @@ func (self *CherryPicking) Remove(selectedCommit *models.Commit, commitsList []*\n }\n \n func (self *CherryPicking) update(selectedHashSet *set.Set[string], commitsList []*models.Commit) {\n-\tcherryPickedCommits := lo.Filter(commitsList, func(commit *models.Commit, _ int) bool {\n+\tself.CherryPickedCommits = lo.Filter(commitsList, func(commit *models.Commit, _ int) bool {\n \t\treturn selectedHashSet.Includes(commit.Hash)\n \t})\n-\n-\tself.CherryPickedCommits = lo.Map(cherryPickedCommits, func(commit *models.Commit, _ int) *models.Commit {\n-\t\treturn &models.Commit{Name: commit.Name, Hash: commit.Hash}\n-\t})\n }\n", "instance_id": "jesseduffield__lazygit-4443", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the goal of switching from interactive rebase to `git cherry-pick` for implementing cherry-picking functionality in a Git tool (lazygit). It outlines the motivation (support for cherry-picking merge commits), the current approach's limitations, and the proposed solution of using `git cherry-pick -m1` along with visualizing the `.git/sequencer/todo` file. It also mentions challenges, such as handling conflicts during an interactive rebase. However, there are minor ambiguities and missing details. For instance, the statement does not fully specify how the visualization of `.git/sequencer/todo` should work or how to handle the merged view of both `.git/sequencer/todo` and `.git/rebase-merge/git-rebase-todo` in conflict scenarios. Additionally, edge cases (beyond conflicts and merge commits) and specific error handling requirements are not explicitly detailed. While references to related issues (#1374, #2442) provide context, they require external lookup, which slightly reduces clarity. Overall, the problem is valid and mostly clear but lacks some specifics on implementation details and edge case handling.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes, as seen in the provided diff, spans multiple files (`daemon.go`, `rebase.go`, `basic_commits_controller.go`, `cherry_pick_helper.go`, `cherry_picking.go`) and involves replacing a core mechanism (interactive rebase) with `git cherry-pick`. While the provided changes focus on removing rebase-based cherry-picking logic and implementing direct `git cherry-pick` commands, the problem statement hints at additional unshown complexity, such as visualizing the `.git/sequencer/todo` file and handling conflicts during an interactive rebase. This suggests deeper architectural impact and interaction with Git's internal state files, which requires a strong understanding of Git's internals.\n\nSecond, the number of technical concepts involved is significant. Solving this requires knowledge of Git commands (`cherry-pick`, `rebase`), Git's internal file structures (`.git/sequencer/todo`, `.git/rebase-merge/git-rebase-todo`), and Go programming (including libraries like `samber/lo` for functional programming). Additionally, it involves understanding lazygit's codebase architecture, particularly how rebase and cherry-pick states are managed and rendered in the UI.\n\nThird, potential edge cases and error handling add to the difficulty. The problem statement mentions conflicts during cherry-picking (especially in the context of an ongoing rebase) and the need to support merge commits, which introduces complexity in command arguments (e.g., `-m1` for merge commits) and state management. The code changes show modifications to handle cherry-pick status and preserve copied commits during conflicts, indicating non-trivial error handling logic. However, the full visualization and conflict resolution UI logic are not fully implemented in the provided diff, suggesting further challenges.\n\nOverall, this problem requires a deep understanding of both Git and the lazygit codebase, complex modifications across multiple modules, and careful handling of edge cases like conflicts and merge commits. I rate it at 0.75, as it approaches \"Very Hard\" due to the potential for additional unshown complexity (e.g., UI visualization), but the provided code changes are somewhat contained and focused on command substitution rather than a complete system overhaul.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Process suspended after shell command execution\n**Describe the bug**\r\n\r\nWhen shell command executed with `:` hotkey lazygit process become suspended.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Open lazygit\r\n2. Type `:` to enter shell command menu\r\n3. Type any command, ex.: `date`\r\n4. Command output displayed and lazygit process suspended, your shell is open.\r\n\r\n**Expected behavior**\r\n\r\nLazygit ask to press any key and after return to lazygit.\r\n\r\n**Screenshots**\r\n\r\n[demo.webm](https://github.com/user-attachments/assets/42fc45ca-b509-4dac-814e-48449a49bf00)\r\n\r\n**Version info:**\r\n_Run `lazygit --version` and paste the result here_\r\n\r\ncommit=2d0c7cb0fc85e3c262837eb9691813fa97e00e80, build date=2024-09-07T02:26:26Z, build source=binaryRelease, version=0.44.0, os=linux, arch=amd64, git version=2.39.2\r\n\r\n_Run `git --version` and paste the result here_\r\n\r\ngit version 2.39.2\r\n\r\n**Additional context**\r\n\r\nProblem appears with version 0.44.0 and master.\r\n\n", "patch": "diff --git a/pkg/commands/oscommands/cmd_obj_builder.go b/pkg/commands/oscommands/cmd_obj_builder.go\nindex 3e89ce10293..c96f2c5bf7d 100644\n--- a/pkg/commands/oscommands/cmd_obj_builder.go\n+++ b/pkg/commands/oscommands/cmd_obj_builder.go\n@@ -52,7 +52,7 @@ func (self *CmdObjBuilder) NewShell(commandStr string) ICmdObj {\n }\n \n func (self *CmdObjBuilder) NewInteractiveShell(commandStr string) ICmdObj {\n-\tquotedCommand := self.quotedCommandString(commandStr)\n+\tquotedCommand := self.quotedCommandString(commandStr + self.platform.InteractiveShellExit)\n \tcmdArgs := str.ToArgv(fmt.Sprintf(\"%s %s %s %s\", self.platform.InteractiveShell, self.platform.InteractiveShellArg, self.platform.ShellArg, quotedCommand))\n \n \treturn self.New(cmdArgs)\ndiff --git a/pkg/commands/oscommands/dummies.go b/pkg/commands/oscommands/dummies.go\nindex ab528782fa6..1e015023842 100644\n--- a/pkg/commands/oscommands/dummies.go\n+++ b/pkg/commands/oscommands/dummies.go\n@@ -51,13 +51,14 @@ func NewDummyCmdObjBuilder(runner ICmdObjRunner) *CmdObjBuilder {\n }\n \n var dummyPlatform = &Platform{\n-\tOS:                  \"darwin\",\n-\tShell:               \"bash\",\n-\tInteractiveShell:    \"bash\",\n-\tShellArg:            \"-c\",\n-\tInteractiveShellArg: \"-i\",\n-\tOpenCommand:         \"open {{filename}}\",\n-\tOpenLinkCommand:     \"open {{link}}\",\n+\tOS:                   \"darwin\",\n+\tShell:                \"bash\",\n+\tInteractiveShell:     \"bash\",\n+\tShellArg:             \"-c\",\n+\tInteractiveShellArg:  \"-i\",\n+\tInteractiveShellExit: \"; exit $?\",\n+\tOpenCommand:          \"open {{filename}}\",\n+\tOpenLinkCommand:      \"open {{link}}\",\n }\n \n func NewDummyOSCommandWithRunner(runner *FakeCmdObjRunner) *OSCommand {\ndiff --git a/pkg/commands/oscommands/os.go b/pkg/commands/oscommands/os.go\nindex cbeb99d4364..8da5572dcb2 100644\n--- a/pkg/commands/oscommands/os.go\n+++ b/pkg/commands/oscommands/os.go\n@@ -35,13 +35,14 @@ type OSCommand struct {\n \n // Platform stores the os state\n type Platform struct {\n-\tOS                  string\n-\tShell               string\n-\tInteractiveShell    string\n-\tShellArg            string\n-\tInteractiveShellArg string\n-\tOpenCommand         string\n-\tOpenLinkCommand     string\n+\tOS                   string\n+\tShell                string\n+\tInteractiveShell     string\n+\tShellArg             string\n+\tInteractiveShellArg  string\n+\tInteractiveShellExit string\n+\tOpenCommand          string\n+\tOpenLinkCommand      string\n }\n \n // NewOSCommand os command runner\ndiff --git a/pkg/commands/oscommands/os_default_platform.go b/pkg/commands/oscommands/os_default_platform.go\nindex 196e4d9f634..f5ea96900f1 100644\n--- a/pkg/commands/oscommands/os_default_platform.go\n+++ b/pkg/commands/oscommands/os_default_platform.go\n@@ -10,13 +10,14 @@ import (\n \n func GetPlatform() *Platform {\n \treturn &Platform{\n-\t\tOS:                  runtime.GOOS,\n-\t\tShell:               \"bash\",\n-\t\tInteractiveShell:    getUserShell(),\n-\t\tShellArg:            \"-c\",\n-\t\tInteractiveShellArg: \"-i\",\n-\t\tOpenCommand:         \"open {{filename}}\",\n-\t\tOpenLinkCommand:     \"open {{link}}\",\n+\t\tOS:                   runtime.GOOS,\n+\t\tShell:                \"bash\",\n+\t\tInteractiveShell:     getUserShell(),\n+\t\tShellArg:             \"-c\",\n+\t\tInteractiveShellArg:  \"-i\",\n+\t\tInteractiveShellExit: \"; exit $?\",\n+\t\tOpenCommand:          \"open {{filename}}\",\n+\t\tOpenLinkCommand:      \"open {{link}}\",\n \t}\n }\n \ndiff --git a/pkg/commands/oscommands/os_windows.go b/pkg/commands/oscommands/os_windows.go\nindex 32cd59edb0a..a2088a40737 100644\n--- a/pkg/commands/oscommands/os_windows.go\n+++ b/pkg/commands/oscommands/os_windows.go\n@@ -2,10 +2,11 @@ package oscommands\n \n func GetPlatform() *Platform {\n \treturn &Platform{\n-\t\tOS:                  \"windows\",\n-\t\tShell:               \"cmd\",\n-\t\tInteractiveShell:    \"cmd\",\n-\t\tShellArg:            \"/c\",\n-\t\tInteractiveShellArg: \"\",\n+\t\tOS:                   \"windows\",\n+\t\tShell:                \"cmd\",\n+\t\tInteractiveShell:     \"cmd\",\n+\t\tShellArg:             \"/c\",\n+\t\tInteractiveShellArg:  \"\",\n+\t\tInteractiveShellExit: \"\",\n \t}\n }\n", "instance_id": "jesseduffield__lazygit-4126", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the bug: when a shell command is executed using the ':' hotkey in lazygit, the process becomes suspended, and the user is left in the shell instead of returning to lazygit. The steps to reproduce are provided, along with the expected behavior (prompting the user to press a key to return to lazygit). A demo video is included, which adds to the clarity. However, there are minor ambiguities: the problem statement does not explicitly mention whether this behavior occurs on all platforms or only specific ones (though version info suggests Linux), and it lacks details on potential edge cases or constraints (e.g., specific shell types or command types that cause the issue). Additionally, there is no mention of whether this impacts other functionalities or if there are security considerations. Overall, the statement is valid and clear but misses some minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, affecting a few files in the `oscommands` package. The primary modification involves adding an `InteractiveShellExit` field to the `Platform` struct and appending an exit command to the shell command string in `NewInteractiveShell`. The changes span multiple files (5 files), but they are consistent and repetitive (adding the same field and updating its value), indicating a low-to-moderate scope. There is no significant impact on the system's architecture, as this is a behavioral fix rather than a structural change.\n\n2. **Number of Technical Concepts:** Solving this requires a basic understanding of shell command execution in Go, specifically how interactive shells are invoked and managed. Familiarity with platform-specific shell behaviors (e.g., `bash` on Linux/Unix, `cmd` on Windows) is necessary, but the concepts are not overly complex. No advanced algorithms, design patterns, or domain-specific knowledge are required beyond standard OS command handling in Go.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes suggest a straightforward fix (appending an exit command to ensure the shell exits after execution). Potential edge cases might include commands that do not terminate properly or platform-specific shell behaviors, but the provided solution does not address these, and the complexity of handling such cases appears minimal at this stage. Error handling modifications are not evident in the diff.\n\n4. **Overall Complexity:** The bug fix involves understanding the interaction between lazygit and the shell, but the solution is a simple modification to ensure the shell exits after command execution. The changes are not deeply invasive, and the problem does not require extensive debugging or performance optimization.\n\nGiven these considerations, a difficulty score of 0.35 is appropriate, as the task requires understanding some code logic and making simple modifications across a few files, fitting within the Easy category. It is slightly above the lower end of the range due to the multi-file changes and the need to consider platform-specific shell behavior.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Opening outside of the git repository results in error and exits\n**Describe the bug**\r\nOpening outside of the git repository results in error and exits with the following message:\r\n`2024/07/12 14:07:03 Error: must be run inside a git repository`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Try to launch `lazygit` outside of a git repository\r\n2. `lazygit` emits the error message and exits\r\n\r\n**Expected behavior**\r\nIt shoud behave in a way specified by the `notARepository` setting.\r\n\r\n**Version info:**\r\n_Run `lazygit --version` and paste the result here_\r\n`commit=e0377f2bc, build date=20240712.112152, build source=unknown, version=v0.42.0.e0377f2bc, os=linux, arch=amd64, git version=2.45.2`\r\n\r\n**Additional context**\r\nIt worked normally in\r\n`commit=6fcb7eb8b, build date=20240519.113145, build source=unknown, version=v0.42.0.6fcb7eb8b, os=linux, arch=amd64, git version=2.45.2`\r\n\n", "patch": "diff --git a/pkg/app/app.go b/pkg/app/app.go\nindex e12461e2803..755f707c493 100644\n--- a/pkg/app/app.go\n+++ b/pkg/app/app.go\n@@ -118,11 +118,11 @@ func NewApp(config config.AppConfigurer, test integrationTypes.IntegrationTest,\n \t\treturn app, err\n \t}\n \n-\t// If we're not in a repo, repoPaths will be nil. The error is moot for us\n+\t// If we're not in a repo, GetRepoPaths will return an error. The error is moot for us\n \t// at this stage, since we'll try to init a new repo in setupRepo(), below\n \trepoPaths, err := git_commands.GetRepoPaths(app.OSCommand.Cmd, gitVersion)\n \tif err != nil {\n-\t\treturn app, err\n+\t\tcommon.Log.Infof(\"Error getting repo paths: %v\", err)\n \t}\n \n \tshowRecentRepos, err := app.setupRepo(repoPaths)\n", "instance_id": "jesseduffield__lazygit-3779", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the bug: launching `lazygit` outside of a git repository results in an error and exits, which is not the expected behavior as defined by the `notARepository` setting. The steps to reproduce are provided, along with version information and additional context about when it previously worked. However, there are minor ambiguities. The expected behavior tied to the `notARepository` setting is mentioned but not detailed (e.g., what specific behavior should occur based on this setting). Additionally, there are no explicit mentions of edge cases or constraints that might affect the solution. Despite these minor gaps, the problem is valid and understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The code change is minimal and localized to a single file (`app.go`), involving a simple modification to handle an error condition differently by logging it instead of returning it. This requires basic understanding of error handling in Go and familiarity with logging mechanisms (likely using a custom `common.Log` utility). The scope of the change is small, with no impact on the broader architecture or interactions between multiple modules. There are no complex technical concepts, algorithms, or design patterns involved, and the change does not introduce significant edge cases or require intricate error handling beyond logging the error. The primary challenge is understanding the intent behind ignoring the error at this stage (to allow repo initialization later), which is straightforward for someone with basic Go experience. Thus, a difficulty score of 0.25 is appropriate, reflecting a simple bug fix with minimal complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Only remote branches should be suggested when creating a PR against selected branch\n**Is your feature request related to a problem? Please describe.**\r\n\r\nWhen creating a PR against a selected branch (via ` O` = \"create pull request options\"), the suggestion area is populated with all local branches. I'd say that only remote branches should be listed there - it doesn't really make sense to create a PR against a branch that doesn't exist on the remote (at least for Github).\r\n\r\n**To Reproduce**\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Checkout local branch\r\n2. Press 'O'\r\n3. Select '... -> select branch' option\r\n\r\n**Describe the solution you'd like**\r\n\r\nOnly remote branches are listed as suggestions.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nn/a\r\n\r\n**Additional context**\r\n\r\nThe suggestion function used in [createPullRequestMenu()](https://github.com/jesseduffield/lazygit/blob/a34bdf1a046c90c22a1c0b653241b8107e89c7f9/pkg/gui/controllers/branches_controller.go#L378) returns all local branches. Simply replacing it with [GetRemoteBranchesSuggestionsFunc()](https://github.com/jesseduffield/lazygit/blob/a34bdf1a046c90c22a1c0b653241b8107e89c7f9/pkg/gui/controllers/helpers/suggestions_helper.go#L173) doesn't work since   the corresponding remote is then included in the branch name (i.e.  `<remote>/<branch>`) - breaking the generated link.\r\n\r\nI'd be happy to have a go at fixing this, but I'd need some pointers: I haven't fully grasped yet how the whole \"create PR\" feature deals with multiple remotes, for example - and whether that must be taken into account. Would a new suggestion function that lists remote branches without the `<remote>/` prefix (and possibly for `origin` only) do the trick?\r\n\r\n\r\n\n", "patch": "diff --git a/pkg/gui/controllers/branches_controller.go b/pkg/gui/controllers/branches_controller.go\nindex a97168fc15f..4bed5c6e39c 100644\n--- a/pkg/gui/controllers/branches_controller.go\n+++ b/pkg/gui/controllers/branches_controller.go\n@@ -730,11 +730,23 @@ func (self *BranchesController) createPullRequestMenu(selectedBranch *models.Bra\n \t\t\t{\n \t\t\t\tLabelColumns: fromToLabelColumns(branch.Name, self.c.Tr.SelectBranch),\n \t\t\t\tOnPress: func() error {\n+\t\t\t\t\tif !branch.IsTrackingRemote() {\n+\t\t\t\t\t\treturn errors.New(self.c.Tr.PullRequestNoUpstream)\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tif len(self.c.Model().Remotes) == 1 {\n+\t\t\t\t\t\ttoRemote := self.c.Model().Remotes[0].Name\n+\t\t\t\t\t\tself.c.Log.Debugf(\"PR will target the only existing remote '%s'\", toRemote)\n+\t\t\t\t\t\treturn self.promptForTargetBranchNameAndCreatePullRequest(branch, toRemote)\n+\t\t\t\t\t}\n+\n \t\t\t\t\tself.c.Prompt(types.PromptOpts{\n-\t\t\t\t\t\tTitle:               branch.Name + \" \u2192\",\n-\t\t\t\t\t\tFindSuggestionsFunc: self.c.Helpers().Suggestions.GetRemoteBranchesSuggestionsFunc(\"/\"),\n-\t\t\t\t\t\tHandleConfirm: func(targetBranchName string) error {\n-\t\t\t\t\t\t\treturn self.createPullRequest(branch.Name, targetBranchName)\n+\t\t\t\t\t\tTitle:               self.c.Tr.SelectTargetRemote,\n+\t\t\t\t\t\tFindSuggestionsFunc: self.c.Helpers().Suggestions.GetRemoteSuggestionsFunc(),\n+\t\t\t\t\t\tHandleConfirm: func(toRemote string) error {\n+\t\t\t\t\t\t\tself.c.Log.Debugf(\"PR will target remote '%s'\", toRemote)\n+\n+\t\t\t\t\t\t\treturn self.promptForTargetBranchNameAndCreatePullRequest(branch, toRemote)\n \t\t\t\t\t\t},\n \t\t\t\t\t})\n \n@@ -764,6 +776,26 @@ func (self *BranchesController) createPullRequestMenu(selectedBranch *models.Bra\n \treturn self.c.Menu(types.CreateMenuOptions{Title: fmt.Sprint(self.c.Tr.CreatePullRequestOptions), Items: menuItems})\n }\n \n+func (self *BranchesController) promptForTargetBranchNameAndCreatePullRequest(fromBranch *models.Branch, toRemote string) error {\n+\tremoteDoesNotExist := lo.NoneBy(self.c.Model().Remotes, func(remote *models.Remote) bool {\n+\t\treturn remote.Name == toRemote\n+\t})\n+\tif remoteDoesNotExist {\n+\t\treturn fmt.Errorf(self.c.Tr.NoValidRemoteName, toRemote)\n+\t}\n+\n+\tself.c.Prompt(types.PromptOpts{\n+\t\tTitle:               fmt.Sprintf(\"%s \u2192 %s/\", fromBranch.UpstreamBranch, toRemote),\n+\t\tFindSuggestionsFunc: self.c.Helpers().Suggestions.GetRemoteBranchesForRemoteSuggestionsFunc(toRemote),\n+\t\tHandleConfirm: func(toBranch string) error {\n+\t\t\tself.c.Log.Debugf(\"PR will target branch '%s' on remote '%s'\", toBranch, toRemote)\n+\t\t\treturn self.createPullRequest(fromBranch.UpstreamBranch, toBranch)\n+\t\t},\n+\t})\n+\n+\treturn nil\n+}\n+\n func (self *BranchesController) createPullRequest(from string, to string) error {\n \turl, err := self.c.Helpers().Host.GetPullRequestURL(from, to)\n \tif err != nil {\ndiff --git a/pkg/gui/controllers/helpers/suggestions_helper.go b/pkg/gui/controllers/helpers/suggestions_helper.go\nindex 441a488b52e..e5e933a8c7c 100644\n--- a/pkg/gui/controllers/helpers/suggestions_helper.go\n+++ b/pkg/gui/controllers/helpers/suggestions_helper.go\n@@ -162,10 +162,26 @@ func (self *SuggestionsHelper) getRemoteBranchNames(separator string) []string {\n \t})\n }\n \n+func (self *SuggestionsHelper) getRemoteBranchNamesForRemote(remoteName string) []string {\n+\tremote, ok := lo.Find(self.c.Model().Remotes, func(remote *models.Remote) bool {\n+\t\treturn remote.Name == remoteName\n+\t})\n+\tif ok {\n+\t\treturn lo.Map(remote.Branches, func(branch *models.RemoteBranch, _ int) string {\n+\t\t\treturn branch.Name\n+\t\t})\n+\t}\n+\treturn nil\n+}\n+\n func (self *SuggestionsHelper) GetRemoteBranchesSuggestionsFunc(separator string) func(string) []*types.Suggestion {\n \treturn FilterFunc(self.getRemoteBranchNames(separator), self.c.UserConfig().Gui.UseFuzzySearch())\n }\n \n+func (self *SuggestionsHelper) GetRemoteBranchesForRemoteSuggestionsFunc(remoteName string) func(string) []*types.Suggestion {\n+\treturn FilterFunc(self.getRemoteBranchNamesForRemote(remoteName), self.c.UserConfig().Gui.UseFuzzySearch())\n+}\n+\n func (self *SuggestionsHelper) getTagNames() []string {\n \treturn lo.Map(self.c.Model().Tags, func(tag *models.Tag, _ int) string {\n \t\treturn tag.Name\ndiff --git a/pkg/i18n/english.go b/pkg/i18n/english.go\nindex fa28130a94a..aa942093b8b 100644\n--- a/pkg/i18n/english.go\n+++ b/pkg/i18n/english.go\n@@ -686,6 +686,8 @@ type TranslationSet struct {\n \tCreatePullRequestOptions                 string\n \tDefaultBranch                            string\n \tSelectBranch                             string\n+\tSelectTargetRemote                       string\n+\tNoValidRemoteName                        string\n \tCreatePullRequest                        string\n \tSelectConfigFile                         string\n \tNoConfigFileFoundErr                     string\n@@ -1676,6 +1678,8 @@ func EnglishTranslationSet() *TranslationSet {\n \t\tCreatePullRequestOptions:                 \"View create pull request options\",\n \t\tDefaultBranch:                            \"Default branch\",\n \t\tSelectBranch:                             \"Select branch\",\n+\t\tSelectTargetRemote:                       \"Select target remote\",\n+\t\tNoValidRemoteName:                        \"A remote named '%s' does not exist\",\n \t\tSelectConfigFile:                         \"Select config file\",\n \t\tNoConfigFileFoundErr:                     \"No config file found\",\n \t\tLoadingFileSuggestions:                   \"Loading file suggestions\",\n", "instance_id": "jesseduffield__lazygit-1889", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the desired feature: when creating a pull request (PR) in the `lazygit` tool, only remote branches should be suggested as target branches instead of local branches. The issue is well-contextualized with steps to reproduce and a reference to the specific function (`createPullRequestMenu()`) that needs modification. However, there are minor ambiguities and missing details. For instance, the problem statement raises a question about handling multiple remotes but does not specify whether the solution should prioritize a specific remote (e.g., `origin`) or provide a selection mechanism. Additionally, edge cases such as what happens when no remote branches exist or when a branch does not track a remote are not explicitly addressed in the problem description, though the code changes attempt to handle some of these. Overall, the statement provides a clear goal but lacks comprehensive details on edge cases and specific requirements for multi-remote scenarios.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, involving modifications across multiple files (`branches_controller.go`, `suggestions_helper.go`, and `english.go`) and the addition of new logic to handle remote branch suggestions and remote selection. The changes require understanding the existing codebase structure, particularly how suggestions are generated and how pull request creation interacts with remote and branch data models. Second, the technical concepts involved include familiarity with Go programming, handling of data structures (e.g., filtering remote branches), and user interface logic for prompts and suggestions in the `lazygit` tool. Third, the code changes address some edge cases, such as validating the existence of a remote and checking if a branch tracks a remote, which adds a layer of complexity to error handling. However, the problem does not require deep architectural changes or advanced algorithms, nor does it impact core system performance or design. It is a feature enhancement with localized impact, making it moderately challenging for someone with intermediate experience in Go and familiarity with the `lazygit` codebase. A score of 0.45 reflects this balance of moderate complexity in logic, multi-file changes, and edge case handling without requiring advanced expertise or extensive system-wide refactoring.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add tables for CloudWatch Logs Vended Log Delivery (CloudFront V2 Logging Support)\n **Is your feature request related to a problem? Please describe.**\n\nCurrently, the `aws_cloudfront_distribution` table includes a `logging` column, but it only reflects basic logging configuration and does **not support CloudFront Standard V2 or Vended Logs**.\n\nWith the rollout of CloudWatch Logs **vended log delivery**, CloudFront now supports **advanced logging options** (near real-time delivery, structured JSON format, flexible destinations like CloudWatch Logs, S3, or Firehose). These configurations are **not surfaced in the current plugin**, making it difficult to audit or manage CloudFront logging comprehensively via Steampipe.\n\n\n**Describe the solution you'd like**\n\nWe should support **CloudFront V2 logging configuration** by introducing **dedicated tables** based on the AWS CloudWatch Logs delivery APIs. These new tables could include:\n\n- `aws_cloudwatch_log_delivery_source`  \n- `aws_cloudwatch_log_delivery`  \n- `aws_cloudwatch_log_destination`\n\nThese tables would allow querying logging configurations across CloudFront and other AWS services that use CloudWatch's vended log delivery.\n\nEach API supports pagination and represents distinct resource types, so they should be modeled as separate tables.\n\n---\n\n **Describe alternatives you've considered**\n\n- Extending the existing `aws_cloudfront_distribution` table with a `v2_logging` column, though this wouldn't scale well since CloudWatch Vended Logs are shared across services (e.g., Lambda, RDS).\n- Manually querying AWS using the SDK or CLI (`logs:GetDelivery`, etc.), but this is inconsistent and not queryable in a unified way via SQL.\n\n---\n\n **Additional context**\n\n\ud83d\udcd8 **Relevant APIs and Docs:**\n\n- [logs:GetDelivery](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_GetDelivery.html)\n- [logs:GetDeliverySource](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_GetDeliverySource.html)  \n- [logs:GetDeliveryDestination](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_GetDeliveryDestination.html)  \n- [logs:GetDeliveryDestinationPolicy](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_GetDeliveryDestinationPolicy.html)\n- [logs:ListDeliveries](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_ListDeliveries.html)  \n- [logs:ListDeliverySources](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_ListDeliverySources.html)- [logs:ListDeliveryDestinations](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_ListDeliveryDestinations.html)  \n\n\ud83d\udcc4 CloudWatch Logs Vended Logs Overview:  \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AWS-logs-and-resource-policy.html\n\n\ud83d\udcc4 CloudFront V2 Logging Docs:  \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/standard-logging.html\n\n", "patch": "diff --git a/aws/plugin.go b/aws/plugin.go\nindex c8d556d45..53540d31e 100644\n--- a/aws/plugin.go\n+++ b/aws/plugin.go\n@@ -138,6 +138,10 @@ func Plugin(ctx context.Context) *plugin.Plugin {\n \t\t\t\"aws_cloudtrail_trail\":                                         tableAwsCloudtrailTrail(ctx),\n \t\t\t\"aws_cloudtrail_trail_event\":                                   tableAwsCloudtrailTrailEvent(ctx),\n \t\t\t\"aws_cloudwatch_alarm\":                                         tableAwsCloudWatchAlarm(ctx),\n+\t\t\t\"aws_cloudwatch_log_delivery_destination\":                      tableAwsCloudWatchLogDeliveryDestination(ctx),\n+\t\t\t\"aws_cloudwatch_log_delivery_source\":                           tableAwsCloudWatchLogDeliverySource(ctx),\n+\t\t\t\"aws_cloudwatch_log_delivery\":                                  tableAwsCloudWatchLogDelivery(ctx),\n+\t\t\t\"aws_cloudwatch_log_destination\":                               tableAwsCloudWatchLogDestination(ctx),\n \t\t\t\"aws_cloudwatch_log_event\":                                     tableAwsCloudwatchLogEvent(ctx),\n \t\t\t\"aws_cloudwatch_log_group\":                                     tableAwsCloudwatchLogGroup(ctx),\n \t\t\t\"aws_cloudwatch_log_metric_filter\":                             tableAwsCloudwatchLogMetricFilter(ctx),\ndiff --git a/aws/table_aws_cloudwatch_log_delivery.go b/aws/table_aws_cloudwatch_log_delivery.go\nnew file mode 100644\nindex 000000000..cfb00501d\n--- /dev/null\n+++ b/aws/table_aws_cloudwatch_log_delivery.go\n@@ -0,0 +1,153 @@\n+package aws\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin/transform\"\n+)\n+\n+func tableAwsCloudWatchLogDelivery(_ context.Context) *plugin.Table {\n+\treturn &plugin.Table{\n+\t\tName:        \"aws_cloudwatch_log_delivery\",\n+\t\tDescription: \"AWS CloudWatch Log Delivery represents a delivery configuration for vended logs.\",\n+\t\tGet: &plugin.GetConfig{\n+\t\t\tKeyColumns: plugin.SingleColumn(\"id\"),\n+\t\t\tHydrate:    getCloudWatchLogDelivery,\n+\t\t\tTags:       map[string]string{\"service\": \"logs\", \"action\": \"GetDelivery\"},\n+\t\t},\n+\t\tList: &plugin.ListConfig{\n+\t\t\tHydrate: listCloudWatchLogDeliveries,\n+\t\t\tTags:    map[string]string{\"service\": \"logs\", \"action\": \"ListDeliveries\"},\n+\t\t},\n+\t\tGetMatrixItemFunc: CloudWatchRegionsMatrix,\n+\t\tColumns: []*plugin.Column{\n+\t\t\t{\n+\t\t\t\tName:        \"id\",\n+\t\t\t\tDescription: \"The unique ID that identifies this delivery in your account.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"arn\",\n+\t\t\t\tDescription: \"The Amazon Resource Name (ARN) that uniquely identifies this delivery.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"delivery_source_name\",\n+\t\t\t\tDescription: \"The name of the delivery source that is associated with this delivery.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"delivery_destination_arn\",\n+\t\t\t\tDescription: \"The ARN of the delivery destination that is associated with this delivery.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"delivery_destination_type\",\n+\t\t\t\tDescription: \"Displays whether the delivery destination associated with this delivery is CloudWatch Logs, Amazon S3, or Firehose.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\n+\t\t\t// Standard columns\n+\t\t\t{\n+\t\t\t\tName:        \"title\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"title\"),\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\tTransform:   transform.FromField(\"Id\"),\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"tags\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"tags\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"akas\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"akas\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\tTransform:   transform.FromField(\"Arn\").Transform(transform.EnsureStringArray),\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+//// LIST FUNCTION\n+\n+func listCloudWatchLogDeliveries(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery.listCloudWatchLogDeliveries\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\t// Limiting the results\n+\tmaxLimit := int32(50)\n+\tif d.QueryContext.Limit != nil {\n+\t\tlimit := int32(*d.QueryContext.Limit)\n+\t\tif limit < maxLimit {\n+\t\t\tmaxLimit = limit\n+\t\t}\n+\t}\n+\n+\tinput := &cloudwatchlogs.DescribeDeliveriesInput{\n+\t\tLimit: &maxLimit,\n+\t}\n+\n+\tpaginator := cloudwatchlogs.NewDescribeDeliveriesPaginator(svc, input, func(o *cloudwatchlogs.DescribeDeliveriesPaginatorOptions) {\n+\t\to.Limit = maxLimit\n+\t\to.StopOnDuplicateToken = true\n+\t})\n+\n+\tfor paginator.HasMorePages() {\n+\t\t// apply rate limiting\n+\t\td.WaitForListRateLimit(ctx)\n+\n+\t\toutput, err := paginator.NextPage(ctx)\n+\t\tif err != nil {\n+\t\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery.listCloudWatchLogDeliveries\", \"api_error\", err)\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tfor _, delivery := range output.Deliveries {\n+\t\t\td.StreamListItem(ctx, delivery)\n+\n+\t\t\t// Context may get cancelled due to manual cancellation or if the limit has been reached\n+\t\t\tif d.RowsRemaining(ctx) == 0 {\n+\t\t\t\treturn nil, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil, nil\n+}\n+\n+//// HYDRATE FUNCTIONS\n+\n+func getCloudWatchLogDelivery(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\tdeliveryId := d.EqualsQuals[\"id\"].GetStringValue()\n+\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery.getCloudWatchLogDelivery\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tparams := &cloudwatchlogs.GetDeliveryInput{\n+\t\tId: &deliveryId,\n+\t}\n+\n+\top, err := svc.GetDelivery(ctx, params)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery.getCloudWatchLogDelivery\", \"api_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tif op.Delivery != nil {\n+\t\treturn *op.Delivery, nil\n+\t}\n+\n+\treturn nil, nil\n+}\ndiff --git a/aws/table_aws_cloudwatch_log_delivery_destination.go b/aws/table_aws_cloudwatch_log_delivery_destination.go\nnew file mode 100644\nindex 000000000..e7c909a74\n--- /dev/null\n+++ b/aws/table_aws_cloudwatch_log_delivery_destination.go\n@@ -0,0 +1,189 @@\n+package aws\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\n+\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin/transform\"\n+)\n+\n+func tableAwsCloudWatchLogDeliveryDestination(_ context.Context) *plugin.Table {\n+\treturn &plugin.Table{\n+\t\tName:        \"aws_cloudwatch_log_delivery_destination\",\n+\t\tDescription: \"AWS CloudWatch Log Delivery Destination represents a destination configuration for vended log delivery.\",\n+\t\tGet: &plugin.GetConfig{\n+\t\t\tKeyColumns: plugin.SingleColumn(\"name\"),\n+\t\t\tHydrate:    getCloudWatchLogDeliveryDestination,\n+\t\t\tTags:       map[string]string{\"service\": \"logs\", \"action\": \"GetDeliveryDestination\"},\n+\t\t},\n+\t\tList: &plugin.ListConfig{\n+\t\t\tHydrate: listCloudWatchLogDeliveryDestinations,\n+\t\t\tTags:    map[string]string{\"service\": \"logs\", \"action\": \"ListDeliveryDestinations\"},\n+\t\t},\n+\t\tGetMatrixItemFunc: CloudWatchRegionsMatrix,\n+\t\tColumns: []*plugin.Column{\n+\t\t\t{\n+\t\t\t\tName:        \"name\",\n+\t\t\t\tDescription: \"The name of the delivery destination.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"arn\",\n+\t\t\t\tDescription: \"The Amazon Resource Name (ARN) that uniquely identifies this delivery destination.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"destination_resource_arn\",\n+\t\t\t\tDescription: \"The ARN of the Amazon Web Services destination that this delivery destination represents.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\tTransform:   transform.FromField(\"DeliveryDestinationConfiguration.DestinationResourceArn\"),\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"delivery_destination_type\",\n+\t\t\t\tDescription: \"Displays whether this delivery destination is CloudWatch Logs, Amazon S3, or Firehose.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"output_format\",\n+\t\t\t\tDescription: \"The format of the logs that are sent to this delivery destination.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"policy\",\n+\t\t\t\tDescription: \"The policy of the delivery destination.\",\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\tHydrate:     getCloudWatchLogDeliveryDestinationPolicy,\n+\t\t\t\tTransform:   transform.FromValue(),\n+\t\t\t},\n+\n+\t\t\t// Standard columns\n+\t\t\t{\n+\t\t\t\tName:        \"title\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"title\"),\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\tTransform:   transform.FromField(\"Name\"),\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"tags\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"tags\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"akas\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"akas\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\tTransform:   transform.FromField(\"Arn\").Transform(transform.EnsureStringArray),\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+//// LIST FUNCTION\n+\n+func listCloudWatchLogDeliveryDestinations(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_destination.listCloudWatchLogDeliveryDestinations\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\t// Limiting the results\n+\tmaxLimit := int32(50)\n+\tif d.QueryContext.Limit != nil {\n+\t\tlimit := int32(*d.QueryContext.Limit)\n+\t\tif limit < maxLimit {\n+\t\t\tmaxLimit = limit\n+\t\t}\n+\t}\n+\n+\tinput := &cloudwatchlogs.DescribeDeliveryDestinationsInput{\n+\t\tLimit: &maxLimit,\n+\t}\n+\n+\tpaginator := cloudwatchlogs.NewDescribeDeliveryDestinationsPaginator(svc, input, func(o *cloudwatchlogs.DescribeDeliveryDestinationsPaginatorOptions) {\n+\t\to.Limit = maxLimit\n+\t\to.StopOnDuplicateToken = true\n+\t})\n+\n+\tfor paginator.HasMorePages() {\n+\t\t// apply rate limiting\n+\t\td.WaitForListRateLimit(ctx)\n+\n+\t\toutput, err := paginator.NextPage(ctx)\n+\t\tif err != nil {\n+\t\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_destination.listCloudWatchLogDeliveryDestinations\", \"api_error\", err)\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tfor _, destination := range output.DeliveryDestinations {\n+\t\t\td.StreamListItem(ctx, destination)\n+\n+\t\t\t// Context may get cancelled due to manual cancellation or if the limit has been reached\n+\t\t\tif d.RowsRemaining(ctx) == 0 {\n+\t\t\t\treturn nil, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil, nil\n+}\n+\n+//// HYDRATE FUNCTIONS\n+\n+func getCloudWatchLogDeliveryDestination(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\tdestinationName := d.EqualsQuals[\"name\"].GetStringValue()\n+\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_destination.getCloudWatchLogDeliveryDestination\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tparams := &cloudwatchlogs.GetDeliveryDestinationInput{\n+\t\tName: &destinationName,\n+\t}\n+\n+\top, err := svc.GetDeliveryDestination(ctx, params)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_destination.getCloudWatchLogDeliveryDestination\", \"api_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tif op.DeliveryDestination != nil {\n+\t\treturn *op.DeliveryDestination, nil\n+\t}\n+\n+\treturn nil, nil\n+}\n+\n+func getCloudWatchLogDeliveryDestinationPolicy(ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {\n+\titem := h.Item.(types.DeliveryDestination)\n+\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_destination.getCloudWatchLogDeliveryDestination\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tparams := &cloudwatchlogs.GetDeliveryDestinationPolicyInput{\n+\t\tDeliveryDestinationName: item.Name,\n+\t}\n+\n+\top, err := svc.GetDeliveryDestinationPolicy(ctx, params)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_destination.getCloudWatchLogDeliveryDestination\", \"api_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tif op.Policy != nil {\n+\t\treturn op.Policy, nil\n+\t}\n+\n+\treturn nil, nil\n+}\ndiff --git a/aws/table_aws_cloudwatch_log_delivery_source.go b/aws/table_aws_cloudwatch_log_delivery_source.go\nnew file mode 100644\nindex 000000000..51a82c219\n--- /dev/null\n+++ b/aws/table_aws_cloudwatch_log_delivery_source.go\n@@ -0,0 +1,153 @@\n+package aws\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin/transform\"\n+)\n+\n+func tableAwsCloudWatchLogDeliverySource(_ context.Context) *plugin.Table {\n+\treturn &plugin.Table{\n+\t\tName:        \"aws_cloudwatch_log_delivery_source\",\n+\t\tDescription: \"AWS CloudWatch Log Delivery Source represents a source configuration for vended log delivery.\",\n+\t\tGet: &plugin.GetConfig{\n+\t\t\tKeyColumns: plugin.SingleColumn(\"name\"),\n+\t\t\tHydrate:    getCloudWatchLogDeliverySource,\n+\t\t\tTags:       map[string]string{\"service\": \"logs\", \"action\": \"GetDeliverySource\"},\n+\t\t},\n+\t\tList: &plugin.ListConfig{\n+\t\t\tHydrate: listCloudWatchLogDeliverySources,\n+\t\t\tTags:    map[string]string{\"service\": \"logs\", \"action\": \"ListDeliverySources\"},\n+\t\t},\n+\t\tGetMatrixItemFunc: CloudWatchRegionsMatrix,\n+\t\tColumns: []*plugin.Column{\n+\t\t\t{\n+\t\t\t\tName:        \"name\",\n+\t\t\t\tDescription: \"The name of the delivery source.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"arn\",\n+\t\t\t\tDescription: \"The Amazon Resource Name (ARN) that uniquely identifies this delivery source.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"service\",\n+\t\t\t\tDescription: \"The Amazon Web Services service that is sending logs.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"log_type\",\n+\t\t\t\tDescription: \"The type of log that the source is sending.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"resource_arns\",\n+\t\t\t\tDescription: \"This array contains the ARN of the Amazon Web Services resource that sends logs and is represented by this delivery source\",\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t},\n+\n+\t\t\t// Standard columns\n+\t\t\t{\n+\t\t\t\tName:        \"title\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"title\"),\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\tTransform:   transform.FromField(\"Name\"),\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"tags\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"tags\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"akas\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"akas\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\tTransform:   transform.FromField(\"Arn\").Transform(transform.EnsureStringArray),\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+//// LIST FUNCTION\n+\n+func listCloudWatchLogDeliverySources(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_source.listCloudWatchLogDeliverySources\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\t// Limiting the results\n+\tmaxLimit := int32(50)\n+\tif d.QueryContext.Limit != nil {\n+\t\tlimit := int32(*d.QueryContext.Limit)\n+\t\tif limit < maxLimit {\n+\t\t\tmaxLimit = limit\n+\t\t}\n+\t}\n+\n+\tinput := &cloudwatchlogs.DescribeDeliverySourcesInput{\n+\t\tLimit: &maxLimit,\n+\t}\n+\n+\tpaginator := cloudwatchlogs.NewDescribeDeliverySourcesPaginator(svc, input, func(o *cloudwatchlogs.DescribeDeliverySourcesPaginatorOptions) {\n+\t\to.Limit = maxLimit\n+\t\to.StopOnDuplicateToken = true\n+\t})\n+\n+\tfor paginator.HasMorePages() {\n+\t\t// apply rate limiting\n+\t\td.WaitForListRateLimit(ctx)\n+\n+\t\toutput, err := paginator.NextPage(ctx)\n+\t\tif err != nil {\n+\t\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_source.listCloudWatchLogDeliverySources\", \"api_error\", err)\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tfor _, source := range output.DeliverySources {\n+\t\t\td.StreamListItem(ctx, source)\n+\n+\t\t\t// Context may get cancelled due to manual cancellation or if the limit has been reached\n+\t\t\tif d.RowsRemaining(ctx) == 0 {\n+\t\t\t\treturn nil, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil, nil\n+}\n+\n+//// HYDRATE FUNCTIONS\n+\n+func getCloudWatchLogDeliverySource(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\tsourceName := d.EqualsQuals[\"name\"].GetStringValue()\n+\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_source.getCloudWatchLogDeliverySource\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tparams := &cloudwatchlogs.GetDeliverySourceInput{\n+\t\tName: &sourceName,\n+\t}\n+\n+\top, err := svc.GetDeliverySource(ctx, params)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_delivery_source.getCloudWatchLogDeliverySource\", \"api_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tif op.DeliverySource != nil {\n+\t\treturn *op.DeliverySource, nil\n+\t}\n+\n+\treturn nil, nil\n+}\ndiff --git a/aws/table_aws_cloudwatch_log_destination.go b/aws/table_aws_cloudwatch_log_destination.go\nnew file mode 100644\nindex 000000000..230f281b0\n--- /dev/null\n+++ b/aws/table_aws_cloudwatch_log_destination.go\n@@ -0,0 +1,115 @@\n+package aws\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin/transform\"\n+)\n+\n+func tableAwsCloudWatchLogDestination(_ context.Context) *plugin.Table {\n+\treturn &plugin.Table{\n+\t\tName:        \"aws_cloudwatch_log_destination\",\n+\t\tDescription: \"AWS CloudWatch Log Destination represents a destination configuration for vended log delivery.\",\n+\t\tList: &plugin.ListConfig{\n+\t\t\tHydrate: listCloudWatchLogDestinations,\n+\t\t\tTags:    map[string]string{\"service\": \"logs\", \"action\": \"ListDestinations\"},\n+\t\t},\n+\t\tGetMatrixItemFunc: CloudWatchRegionsMatrix,\n+\t\tColumns: []*plugin.Column{\n+\t\t\t{\n+\t\t\t\tName:        \"destination_name\",\n+\t\t\t\tDescription: \"The name of the destination.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"arn\",\n+\t\t\t\tDescription: \"The Amazon Resource Name (ARN) of this destination.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"creation_time\",\n+\t\t\t\tDescription: \"The creation time of the destination.\",\n+\t\t\t\tType:        proto.ColumnType_TIMESTAMP,\n+\t\t\t\tTransform:   transform.FromField(\"CreationTime\").Transform(transform.UnixMsToTimestamp),\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"role_arn\",\n+\t\t\t\tDescription: \"A role for impersonation, used when delivering log events to the target.\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"target_arn\",\n+\t\t\t\tDescription: \"The Amazon Resource Name (ARN) of the physical target where the log events are delivered (for example, a Kinesis stream).\",\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t},\n+\n+\t\t\t// Standard columns\n+\t\t\t{\n+\t\t\t\tName:        \"title\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"title\"),\n+\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\tTransform:   transform.FromField(\"DestinationName\"),\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tName:        \"akas\",\n+\t\t\t\tDescription: resourceInterfaceDescription(\"akas\"),\n+\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\tTransform:   transform.FromField(\"Arn\").Transform(transform.EnsureStringArray),\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+//// LIST FUNCTION\n+\n+func listCloudWatchLogDestinations(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\t// Create session\n+\tsvc, err := CloudWatchLogsClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_destination.listCloudWatchLogDestinations\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\t// Limiting the results\n+\tmaxLimit := int32(50)\n+\tif d.QueryContext.Limit != nil {\n+\t\tlimit := int32(*d.QueryContext.Limit)\n+\t\tif limit < maxLimit {\n+\t\t\tmaxLimit = limit\n+\t\t}\n+\t}\n+\n+\tinput := &cloudwatchlogs.DescribeDestinationsInput{\n+\t\tLimit: &maxLimit,\n+\t}\n+\n+\tpaginator := cloudwatchlogs.NewDescribeDestinationsPaginator(svc, input, func(o *cloudwatchlogs.DescribeDestinationsPaginatorOptions) {\n+\t\to.Limit = maxLimit\n+\t\to.StopOnDuplicateToken = true\n+\t})\n+\n+\tfor paginator.HasMorePages() {\n+\t\t// apply rate limiting\n+\t\td.WaitForListRateLimit(ctx)\n+\n+\t\toutput, err := paginator.NextPage(ctx)\n+\t\tif err != nil {\n+\t\t\tplugin.Logger(ctx).Error(\"aws_cloudwatch_log_destination.listCloudWatchLogDestinations\", \"api_error\", err)\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tfor _, destination := range output.Destinations {\n+\t\t\td.StreamListItem(ctx, destination)\n+\n+\t\t\t// Context may get cancelled due to manual cancellation or if the limit has been reached\n+\t\t\tif d.RowsRemaining(ctx) == 0 {\n+\t\t\t\treturn nil, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil, nil\n+}\ndiff --git a/docs/tables/aws_cloudwatch_log_delivery.md b/docs/tables/aws_cloudwatch_log_delivery.md\nnew file mode 100644\nindex 000000000..480e31e5d\n--- /dev/null\n+++ b/docs/tables/aws_cloudwatch_log_delivery.md\n@@ -0,0 +1,202 @@\n+---\n+title: \"Steampipe Table: aws_cloudwatch_log_delivery - Query AWS CloudWatch Log Deliveries using SQL\"\n+description: \"Allows users to query AWS CloudWatch Log Deliveries, providing information about delivery configurations for vended log delivery.\"\n+---\n+\n+# Table: aws_cloudwatch_log_delivery - Query AWS CloudWatch Log Deliveries using SQL\n+\n+AWS CloudWatch Log Delivery represents a delivery configuration for vended log delivery, connecting a delivery source to a destination. These deliveries define how logs from AWS services like CloudFront are delivered to destinations such as S3 buckets, CloudWatch Logs, or Firehose delivery streams, enabling near real-time log analysis and storage.\n+\n+## Table Usage Guide\n+\n+The `aws_cloudwatch_log_delivery` table in Steampipe provides you with information about delivery configurations for vended log delivery in AWS CloudWatch Logs. This table allows you, as a DevOps engineer, security analyst, or cloud administrator, to query delivery-specific details, including the associated source, destination, and status. You can utilize this table to gather insights on log delivery configurations, such as identifying where specific services are sending their logs, determining which destinations are receiving logs, and monitoring the status of your log delivery pipelines. The schema outlines the various attributes of the delivery configuration for you, including the delivery ID, source name, destination ARN, creation time, and associated tags.\n+\n+## Examples\n+\n+### Basic info\n+Gain insights into your log delivery configurations to understand how logs are routed within your AWS environment.\n+\n+```sql+postgres\n+select\n+  id,\n+  arn,\n+  delivery_source_name,\n+  delivery_destination_arn,\n+  delivery_destination_type\n+from\n+  aws_cloudwatch_log_delivery;\n+```\n+\n+```sql+sqlite\n+select\n+  id,\n+  arn,\n+  delivery_source_name,\n+  delivery_destination_arn,\n+  delivery_destination_type\n+from\n+  aws_cloudwatch_log_delivery;\n+```\n+\n+### Count deliveries by destination type\n+Analyze the distribution of deliveries across different destination types to understand your log delivery targets.\n+\n+```sql+postgres\n+select\n+  delivery_destination_type,\n+  count(*) as delivery_count\n+from\n+  aws_cloudwatch_log_delivery\n+group by\n+  delivery_destination_type;\n+```\n+\n+```sql+sqlite\n+select\n+  delivery_destination_type,\n+  count(*) as delivery_count\n+from\n+  aws_cloudwatch_log_delivery\n+group by\n+  delivery_destination_type;\n+```\n+\n+### Identify CloudFront deliveries to CloudWatch Logs\n+Find all CloudFront sources configured to deliver logs to CloudWatch Log groups.\n+\n+```sql+postgres\n+select\n+  d.id as delivery_id,\n+  s.name as source_name,\n+  s.service,\n+  d.delivery_destination_type,\n+  split_part(d.delivery_destination_arn, ':', 7) as log_group_name\n+from\n+  aws_cloudwatch_log_delivery d\n+join\n+  aws_cloudwatch_log_delivery_source s on d.delivery_source_name = s.name\n+where\n+  s.service = 'cloudfront'\n+  and d.delivery_destination_type = 'CloudWatchLogs';\n+```\n+\n+```sql+sqlite\n+select\n+  d.id as delivery_id,\n+  s.name as source_name,\n+  s.service,\n+  d.delivery_destination_type,\n+  substr(\n+    d.delivery_destination_arn,\n+    instr(d.delivery_destination_arn, 'log-group:') + 10\n+  ) as log_group_name\n+from\n+  aws_cloudwatch_log_delivery d\n+join\n+  aws_cloudwatch_log_delivery_source s on d.delivery_source_name = s.name\n+where\n+  s.service = 'cloudfront'\n+  and d.delivery_destination_type = 'CloudWatchLogs';\n+```\n+\n+### Get S3 bucket details for vended log deliveries\n+List the S3 buckets used as delivery destinations for CloudWatch logs.\n+\n+```sql+postgres\n+select\n+  d.id as delivery_id,\n+  s.name as source_name,\n+  s.service,\n+  b.name as bucket_name,\n+  b.region,\n+  b.creation_date\n+from\n+  aws_cloudwatch_log_delivery d\n+join\n+  aws_cloudwatch_log_delivery_source s on d.delivery_source_name = s.name\n+join\n+  aws_s3_bucket b on b.arn = d.delivery_destination_arn\n+where\n+  d.delivery_destination_type = 'S3';\n+```\n+\n+```sql+sqlite\n+select\n+  d.id as delivery_id,\n+  s.name as source_name,\n+  s.service,\n+  b.name as bucket_name,\n+  b.region,\n+  b.creation_date\n+from\n+  aws_cloudwatch_log_delivery d\n+join\n+  aws_cloudwatch_log_delivery_source s on d.delivery_source_name = s.name\n+join\n+  aws_s3_bucket b on b.arn = d.delivery_destination_arn\n+where\n+  d.delivery_destination_type = 'S3';\n+```\n+\n+### Review full CloudFront logging configuration\n+Compare logging targets for CloudFront distributions, including S3, CloudWatch Logs, and Firehose.\n+\n+```sql+postgres\n+select\n+  cf.id as distribution_id,\n+  cf.domain_name,\n+  s.name as source_name,\n+  d.id as delivery_id,\n+  d.delivery_destination_type,\n+  case\n+    when d.delivery_destination_type = 'S3' then b.name\n+    when d.delivery_destination_type = 'CloudWatchLogs' then split_part(d.delivery_destination_arn, ':', 7)\n+    when d.delivery_destination_type = 'Firehose' then split_part(d.delivery_destination_arn, '/', 2)\n+    else 'Unknown'\n+  end as destination_name\n+from\n+  aws_cloudfront_distribution cf\n+join\n+  aws_cloudwatch_log_delivery_source s\n+    on s.resource_arns @> to_jsonb(array[cf.arn])\n+    and s.service = 'cloudfront'\n+join\n+  aws_cloudwatch_log_delivery d\n+    on s.name = d.delivery_source_name\n+left join\n+  aws_s3_bucket b\n+    on b.arn = d.delivery_destination_arn\n+    and d.delivery_destination_type = 'S3';\n+```\n+\n+```sql+sqlite\n+select\n+  cf.id as distribution_id,\n+  cf.domain_name,\n+  s.name as source_name,\n+  d.id as delivery_id,\n+  d.delivery_destination_type,\n+  case\n+    when d.delivery_destination_type = 'S3' then b.name\n+    when d.delivery_destination_type = 'CloudWatchLogs' then split(d.delivery_destination_arn, ':')[7]\n+    when d.delivery_destination_type = 'Firehose' then split(d.delivery_destination_arn, '/')[2]\n+    else 'Unknown'\n+  end as destination_name\n+from\n+  aws_cloudwatch_log_delivery_source s\n+join\n+  json_each(s.resource_arns) as ra\n+    on ra.value = cf.arn\n+join\n+  aws_cloudfront_distribution cf\n+    on true\n+join\n+  aws_cloudwatch_log_delivery d\n+    on s.name = d.delivery_source_name\n+left join\n+  aws_s3_bucket b\n+    on b.arn = d.delivery_destination_arn\n+    and d.delivery_destination_type = 'S3'\n+where\n+  s.service = 'cloudfront';\n+```\n\\ No newline at end of file\ndiff --git a/docs/tables/aws_cloudwatch_log_delivery_destination.md b/docs/tables/aws_cloudwatch_log_delivery_destination.md\nnew file mode 100644\nindex 000000000..0ddffb11c\n--- /dev/null\n+++ b/docs/tables/aws_cloudwatch_log_delivery_destination.md\n@@ -0,0 +1,200 @@\n+---\n+title: \"Steampipe Table: aws_cloudwatch_log_delivery_destination - Query AWS CloudWatch Log Delivery Destinations using SQL\"\n+description: \"Allows users to query AWS CloudWatch Log Delivery Destinations, providing information about destination configurations for vended log delivery.\"\n+---\n+\n+# Table: aws_cloudwatch_log_delivery_destination - Query AWS CloudWatch Log Delivery Destinations using SQL\n+\n+AWS CloudWatch Log Delivery Destination represents a destination configuration for vended log delivery. These destinations define where logs can be sent, such as S3 buckets, CloudWatch Logs, or Firehose delivery streams. Delivery destinations are part of CloudWatch Logs Vended Log Delivery, which enables near real-time log delivery from AWS services like CloudFront.\n+\n+## Table Usage Guide\n+\n+The `aws_cloudwatch_log_delivery_destination` table in Steampipe provides you with information about destination configurations for vended log delivery in AWS CloudWatch Logs. This table allows you, as a DevOps engineer, security analyst, or cloud administrator, to query destination-specific details, including the destination type, ARN, output format, and associated policy. You can utilize this table to gather insights on log delivery destinations, such as identifying all S3 buckets configured as log destinations, understanding output formats in use, and managing delivery policies. The schema outlines the various attributes of the delivery destination for you, including the name, ARN, destination resource ARN, delivery destination type, output format, and associated tags.\n+\n+## Examples\n+\n+### Basic info\n+Explore the configuration details of CloudWatch Log delivery destinations in your AWS account to understand where logs can be sent.\n+\n+```sql+postgres\n+select\n+  name,\n+  arn,\n+  destination_resource_arn,\n+  delivery_destination_type,\n+  output_format\n+from\n+  aws_cloudwatch_log_delivery_destination;\n+```\n+\n+```sql+sqlite\n+select\n+  name,\n+  arn,\n+  destination_resource_arn,\n+  delivery_destination_type,\n+  output_format\n+from\n+  aws_cloudwatch_log_delivery_destination;\n+```\n+\n+### Count destinations by type\n+Count the number of log delivery destinations grouped by their destination type to get a quick summary of log delivery architecture.\n+\n+```sql+postgres\n+select\n+  delivery_destination_type,\n+  count(*) as destination_count\n+from\n+  aws_cloudwatch_log_delivery_destination\n+group by\n+  delivery_destination_type\n+order by\n+  destination_count desc;\n+```\n+\n+```sql+sqlite\n+select\n+  delivery_destination_type,\n+  count(*) as destination_count\n+from\n+  aws_cloudwatch_log_delivery_destination\n+group by\n+  delivery_destination_type\n+order by\n+  destination_count desc;\n+```\n+\n+### Get S3 buckets used as log destinations\n+Retrieve a list of S3 buckets used as destinations for vended log delivery to validate where your logs are being stored.\n+\n+```sql+postgres\n+select\n+  d.name as destination_name,\n+  d.arn as destination_arn,\n+  d.destination_resource_arn,\n+  b.name as bucket_name,\n+  b.region,\n+  d.output_format\n+from\n+  aws_cloudwatch_log_delivery_destination d\n+join\n+  aws_s3_bucket b on d.destination_resource_arn like concat('arn:aws:s3:::', b.name, '%')\n+where\n+  d.delivery_destination_type = 'S3';\n+```\n+\n+```sql+sqlite\n+select\n+  d.name as destination_name,\n+  d.arn as destination_arn,\n+  d.destination_resource_arn,\n+  b.name as bucket_name,\n+  b.region,\n+  d.output_format\n+from\n+  aws_cloudwatch_log_delivery_destination d\n+join\n+  aws_s3_bucket b on d.destination_resource_arn like 'arn:aws:s3:::' || b.name || '%'\n+where\n+  d.delivery_destination_type = 'S3';\n+```\n+\n+### Count destinations by output format\n+Analyze how many destinations use each output format to understand log formatting standards across destinations.\n+\n+```sql+postgres\n+select\n+  output_format,\n+  delivery_destination_type,\n+  count(*) as destination_count\n+from\n+  aws_cloudwatch_log_delivery_destination\n+group by\n+  output_format,\n+  delivery_destination_type\n+order by\n+  output_format,\n+  destination_count desc;\n+```\n+\n+```sql+sqlite\n+select\n+  output_format,\n+  delivery_destination_type,\n+  count(*) as destination_count\n+from\n+  aws_cloudwatch_log_delivery_destination\n+group by\n+  output_format,\n+  delivery_destination_type\n+order by\n+  output_format,\n+  destination_count desc;\n+```\n+\n+### View destinations with defined policies\n+Identify destinations that have explicit policies defined to assess access control configurations.\n+\n+```sql+postgres\n+select\n+  name,\n+  delivery_destination_type,\n+  policy\n+from\n+  aws_cloudwatch_log_delivery_destination\n+where\n+  policy is not null;\n+```\n+\n+```sql+sqlite\n+select\n+  name,\n+  delivery_destination_type,\n+  policy\n+from\n+  aws_cloudwatch_log_delivery_destination\n+where\n+  policy is not null;\n+```\n+\n+### Identify which sources are using each destination\n+Identify the source destination mappings.\n+\n+```sql+postgres\n+select\n+  dest.name as destination_name,\n+  dest.delivery_destination_type,\n+  dest.output_format,\n+  s.name as source_name,\n+  s.service,\n+  s.log_type,\n+  d.id as delivery_id\n+from\n+  aws_cloudwatch_log_delivery_destination dest\n+join\n+  aws_cloudwatch_log_delivery d on dest.arn = d.delivery_destination_arn\n+join\n+  aws_cloudwatch_log_delivery_source s on d.delivery_source_name = s.name\n+order by\n+  dest.name, s.service;\n+```\n+\n+```sql+sqlite\n+select\n+  dest.name as destination_name,\n+  dest.delivery_destination_type,\n+  dest.output_format,\n+  s.name as source_name,\n+  s.service,\n+  s.log_type,\n+  d.id as delivery_id\n+from\n+  aws_cloudwatch_log_delivery_destination dest\n+join\n+  aws_cloudwatch_log_delivery d on dest.arn = d.delivery_destination_arn\n+join\n+  aws_cloudwatch_log_delivery_source s on d.delivery_source_name = s.name\n+order by\n+  dest.name, s.service;\n+```\n\\ No newline at end of file\ndiff --git a/docs/tables/aws_cloudwatch_log_delivery_source.md b/docs/tables/aws_cloudwatch_log_delivery_source.md\nnew file mode 100644\nindex 000000000..a9f5fe163\n--- /dev/null\n+++ b/docs/tables/aws_cloudwatch_log_delivery_source.md\n@@ -0,0 +1,233 @@\n+---\n+title: \"Steampipe Table: aws_cloudwatch_log_delivery_source - Query AWS CloudWatch Log Delivery Sources using SQL\"\n+description: \"Allows users to query AWS CloudWatch Log Delivery Sources, providing information about source configurations for vended log delivery.\"\n+---\n+\n+# Table: aws_cloudwatch_log_delivery_source - Query AWS CloudWatch Log Delivery Sources using SQL\n+\n+AWS CloudWatch Log Delivery Source represents a source configuration for vended log delivery. These sources define AWS services that can send logs to destinations like S3 buckets, CloudWatch Logs, or Firehose delivery streams. Delivery sources are part of CloudWatch Logs Vended Log Delivery, which enables services like CloudFront to deliver logs in near real-time.\n+\n+## Table Usage Guide\n+\n+The `aws_cloudwatch_log_delivery_source` table in Steampipe provides you with information about source configurations for vended log delivery in AWS CloudWatch Logs. This table allows you, as a DevOps engineer, security analyst, or cloud administrator, to query source-specific details, including the associated service, log type, and resource ARNs. You can utilize this table to gather insights on log delivery sources, such as identifying all CloudFront distributions configured for vended logs, understanding which services are configured to send logs, and monitoring your overall logging architecture. The schema outlines the various attributes of the delivery source for you, including the source name, ARN, service, creation time, and associated tags.\n+\n+## Examples\n+\n+### Basic info\n+Explore the configuration details of CloudWatch Log delivery sources in your AWS account to understand what services are sending logs to which destinations.\n+\n+```sql+postgres\n+select\n+  name,\n+  arn,\n+  service,\n+  log_type\n+from\n+  aws_cloudwatch_log_delivery_source;\n+```\n+\n+```sql+sqlite\n+select\n+  name,\n+  arn,\n+  service,\n+  log_type\n+from\n+  aws_cloudwatch_log_delivery_source;\n+```\n+\n+### List CloudFront delivery sources\n+Identify CloudFront distributions that are configured as log delivery sources.\n+\n+```sql+postgres\n+select\n+  name,\n+  service,\n+  log_type,\n+  resource_arns\n+from\n+  aws_cloudwatch_log_delivery_source\n+where\n+  service = 'cloudfront';\n+```\n+\n+```sql+sqlite\n+select\n+  name,\n+  service,\n+  log_type,\n+  resource_arns\n+from\n+  aws_cloudwatch_log_delivery_source\n+where\n+  service = 'cloudfront';\n+```\n+\n+### Count sources by service\n+Count the number of delivery sources grouped by their originating AWS service.\n+\n+```sql+postgres\n+select\n+  service,\n+  count(*) as source_count\n+from\n+  aws_cloudwatch_log_delivery_source\n+group by\n+  service\n+order by\n+  source_count desc;\n+```\n+\n+```sql+sqlite\n+select\n+  service,\n+  count(*) as source_count\n+from\n+  aws_cloudwatch_log_delivery_source\n+group by\n+  service\n+order by\n+  source_count desc;\n+```\n+\n+### Get destinations for each delivery source\n+Find out where each delivery source is sending its logs by joining with the delivery configuration.\n+\n+```sql+postgres\n+select\n+  s.name as source_name,\n+  s.service,\n+  s.log_type,\n+  d.id as delivery_id,\n+  d.delivery_destination_type\n+from\n+  aws_cloudwatch_log_delivery_source s\n+left join\n+  aws_cloudwatch_log_delivery d\n+on\n+  s.name = d.delivery_source_name;\n+```\n+\n+```sql+sqlite\n+select\n+  s.name as source_name,\n+  s.service,\n+  s.log_type,\n+  d.id as delivery_id,\n+  d.delivery_destination_type\n+from\n+  aws_cloudwatch_log_delivery_source s\n+left join\n+  aws_cloudwatch_log_delivery d\n+on\n+  s.name = d.delivery_source_name;\n+```\n+\n+### Find CloudFront log sources delivering to S3\n+Get CloudFront sources that are sending vended logs to S3 destinations.\n+\n+```sql+postgres\n+select\n+  s.name as source_name,\n+  cf.id as distribution_id,\n+  cf.domain_name,\n+  d.id as delivery_id,\n+  d.delivery_destination_type,\n+  s3.name as s3_bucket_name\n+from\n+  aws_cloudwatch_log_delivery_source s\n+join\n+  aws_cloudfront_distribution cf\n+    on s.resource_arns @> to_jsonb(array[cf.arn])\n+left join\n+  aws_cloudwatch_log_delivery d\n+    on s.name = d.delivery_source_name\n+left join\n+  aws_s3_bucket s3\n+    on s3.arn = d.delivery_destination_arn\n+where\n+  s.service = 'cloudfront'\n+  and d.delivery_destination_type = 'S3';\n+```\n+\n+```sql+sqlite\n+select\n+  s.name as source_name,\n+  cf.id as distribution_id,\n+  cf.domain_name,\n+  d.id as delivery_id,\n+  d.delivery_destination_type,\n+  s3.name as s3_bucket_name\n+from\n+  aws_cloudwatch_log_delivery_source s\n+join\n+  aws_cloudfront_distribution cf\n+    on cf.arn in (\n+      select value\n+      from json_each(s.resource_arns)\n+    )\n+left join\n+  aws_cloudwatch_log_delivery d\n+    on s.name = d.delivery_source_name\n+left join\n+  aws_s3_bucket s3\n+    on s3.arn = d.delivery_destination_arn\n+where\n+  s.service = 'cloudfront'\n+  and d.delivery_destination_type = 'S3';\n+```\n+\n+### Compare CloudFront standard vs vended logging\n+Compare CloudFront distributions with standard logging and those with vended log delivery to evaluate logging configurations.\n+\n+```sql+postgres\n+select\n+  cf.id as distribution_id,\n+  cf.domain_name,\n+  case\n+    when cf.logging->>'Enabled' = 'true' then 'Standard'\n+    else 'Not configured'\n+  end as standard_logging_status,\n+  case\n+    when s.name is not null then 'Configured'\n+    else 'Not configured'\n+  end as vended_logging_status,\n+  s.name as source_name,\n+  d.delivery_destination_type\n+from\n+  aws_cloudfront_distribution cf\n+left join\n+  aws_cloudwatch_log_delivery_source s\n+    on s.resource_arns @> to_jsonb(array[cf.arn])\n+    and s.service = 'cloudfront'\n+left join\n+  aws_cloudwatch_log_delivery d\n+    on s.name = d.delivery_source_name;\n+```\n+\n+```sql+sqlite\n+select\n+  cf.id as distribution_id,\n+  cf.domain_name,\n+  case\n+    when cf.logging ->> 'Enabled' = 'true' then 'Standard'\n+    else 'Not configured'\n+  end as standard_logging_status,\n+  case\n+    when s.name is not null then 'Configured'\n+    else 'Not configured'\n+  end as vended_logging_status,\n+  s.name as source_name,\n+  d.delivery_destination_type\n+from\n+  aws_cloudfront_distribution cf\n+left join\n+  aws_cloudwatch_log_delivery_source s\n+    on s.service = 'cloudfront'\n+left join\n+  json_each(s.resource_arns) as ra\n+    on ra.value = cf.arn\n+left join\n+  aws_cloudwatch_log_delivery d\n+    on s.name = d.delivery_source_name;\n+```\n\\ No newline at end of file\ndiff --git a/docs/tables/aws_cloudwatch_log_destination.md b/docs/tables/aws_cloudwatch_log_destination.md\nnew file mode 100644\nindex 000000000..0905de607\n--- /dev/null\n+++ b/docs/tables/aws_cloudwatch_log_destination.md\n@@ -0,0 +1,179 @@\n+---\n+title: \"Steampipe Table: aws_cloudwatch_log_destination - Query AWS CloudWatch Log Destinations using SQL\"\n+description: \"Allows users to query AWS CloudWatch Log Destinations, providing information about destination configurations for vended log delivery.\"\n+---\n+\n+# Table: aws_cloudwatch_log_destination - Query AWS CloudWatch Log Destinations using SQL\n+\n+AWS CloudWatch Log Destination represents a destination configuration for vended log delivery. These destinations define where logs can be sent, such as S3 buckets, CloudWatch Logs, or Firehose delivery streams. Destinations are part of CloudWatch Logs Vended Log Delivery, which enables near real-time log delivery from AWS services like CloudFront.\n+\n+## Table Usage Guide\n+\n+The `aws_cloudwatch_log_destination` table in Steampipe provides you with information about destination configurations for vended log delivery in AWS CloudWatch Logs. This table allows you, as a DevOps engineer, security analyst, or cloud administrator, to query destination-specific details, including the destination type, ARN, and configuration. You can utilize this table to gather insights on log delivery destinations, such as identifying all S3 buckets configured as log destinations, understanding which CloudWatch Log groups are receiving logs, and monitoring your overall logging architecture. The schema outlines the various attributes of the delivery destination for you, including the destination name, ARN, type, creation time, and associated tags.\n+\n+## Examples\n+\n+### Basic info\n+Explore the configuration details of CloudWatch Log delivery destinations in your AWS account to understand where logs can be sent.\n+\n+```sql+postgres\n+select\n+  destination_name,\n+  arn,\n+  role_arn,\n+  target_arn,\n+  creation_time\n+from\n+  aws_cloudwatch_log_destination;\n+```\n+\n+```sql+sqlite\n+select\n+  destination_name,\n+  arn,\n+  role_arn,\n+  target_arn,\n+  creation_time\n+from\n+  aws_cloudwatch_log_destination;\n+```\n+\n+### Classify destinations by target type\n+Determine the types of destinations used for log delivery by classifying based on the target ARN.\n+\n+```sql+postgres\n+select\n+  destination_name,\n+  case\n+    when target_arn like 'arn:aws:s3:%' then 'S3'\n+    when target_arn like 'arn:aws:firehose:%' then 'Firehose'\n+    when target_arn like 'arn:aws:logs:%' then 'CloudWatch Logs'\n+    else 'Other'\n+  end as target_type,\n+  target_arn\n+from\n+  aws_cloudwatch_log_destination\n+order by\n+  target_type;\n+```\n+\n+```sql+sqlite\n+select\n+  destination_name,\n+  case\n+    when target_arn like 'arn:aws:s3:%' then 'S3'\n+    when target_arn like 'arn:aws:firehose:%' then 'Firehose'\n+    when target_arn like 'arn:aws:logs:%' then 'CloudWatch Logs'\n+    else 'Other'\n+  end as target_type,\n+  target_arn\n+from\n+  aws_cloudwatch_log_destination\n+order by\n+  target_type;\n+```\n+\n+### List S3 buckets configured as log destinations\n+Identify S3 buckets that are used as targets for log delivery in your AWS environment.\n+\n+```sql+postgres\n+select\n+  d.destination_name,\n+  d.arn as destination_arn,\n+  d.target_arn,\n+  b.name as bucket_name,\n+  b.region,\n+  b.creation_date\n+from\n+  aws_cloudwatch_log_destination d\n+join\n+  aws_s3_bucket b on d.target_arn like concat('arn:aws:s3:::', b.name, '%')\n+where\n+  d.target_arn like 'arn:aws:s3:%';\n+```\n+\n+```sql+sqlite\n+select\n+  d.destination_name,\n+  d.arn as destination_arn,\n+  d.target_arn,\n+  b.name as bucket_name,\n+  b.region,\n+  b.creation_date\n+from\n+  aws_cloudwatch_log_destination d\n+join\n+  aws_s3_bucket b on d.target_arn like 'arn:aws:s3:::' || b.name || '%'\n+where\n+  d.target_arn like 'arn:aws:s3:%';\n+```\n+\n+### List recently created destinations\n+Track recent changes in your logging setup by viewing recently created log destinations.\n+\n+```sql+postgres\n+select\n+  destination_name,\n+  target_arn,\n+  role_arn,\n+  creation_time\n+from\n+  aws_cloudwatch_log_destination\n+where\n+  creation_time > now() - interval '7 days'\n+order by\n+  creation_time desc;\n+```\n+\n+```sql+sqlite\n+select\n+  destination_name,\n+  target_arn,\n+  role_arn,\n+  creation_time\n+from\n+  aws_cloudwatch_log_destination\n+where\n+  creation_time > datetime('now', '-7 days')\n+order by\n+  creation_time desc;\n+```\n+\n+### Trace full delivery pipeline from source to destination\n+Get end-to-end visibility of the log delivery pipeline across source, delivery, and destination.\n+\n+```sql+postgres\n+select\n+  s.name as source_name,\n+  s.service,\n+  s.log_type,\n+  d.id as delivery_id,\n+  dst.destination_name,\n+  dst.target_arn\n+from\n+  aws_cloudwatch_log_delivery_source s\n+join\n+  aws_cloudwatch_log_delivery d on s.name = d.delivery_source_name\n+join\n+  aws_cloudwatch_log_destination dst on d.delivery_destination_arn = dst.arn\n+order by\n+  s.service, s.name;\n+```\n+\n+```sql+sqlite\n+select\n+  s.name as source_name,\n+  s.service,\n+  s.log_type,\n+  d.id as delivery_id,\n+  dst.destination_name,\n+  dst.target_arn\n+from\n+  aws_cloudwatch_log_delivery_source s\n+join\n+  aws_cloudwatch_log_delivery d on s.name = d.delivery_source_name\n+join\n+  aws_cloudwatch_log_destination dst on d.delivery_destination_arn = dst.arn\n+order by\n+  s.service, s.name;\n+```\n\\ No newline at end of file\n", "instance_id": "turbot__steampipe-plugin-aws-2469", "clarity": 3, "difficulty": 0.55, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly defines the goal of adding support for CloudWatch Logs Vended Log Delivery for CloudFront V2 Logging by introducing new dedicated tables in a Steampipe plugin. The input and output requirements are implicitly clear through the context of AWS APIs and the proposed table structures. Constraints and scope are well-articulated with references to specific AWS APIs (e.g., `GetDelivery`, `ListDeliveries`) and documentation links. The statement also includes alternatives considered, which adds depth to the rationale behind the chosen solution. Additionally, the problem description highlights the need for scalability across AWS services, avoiding ambiguity about the broader applicability of the feature. The provided examples in the documentation further enhance clarity by demonstrating practical use cases. There are no significant ambiguities or missing critical details, making this a very clear and detailed request.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is significant, involving the addition of four new tables (`aws_cloudwatch_log_delivery`, `aws_cloudwatch_log_delivery_source`, `aws_cloudwatch_log_delivery_destination`, and `aws_cloudwatch_log_destination`) to the Steampipe plugin, each with its own list and get functions, pagination handling, and integration with AWS SDK for CloudWatch Logs. This requires modifying multiple files and adding a substantial amount of new code (over 600 lines across several files), though the changes are largely isolated to new table definitions and do not impact the broader system architecture significantly. Second, the technical concepts involved include familiarity with AWS SDK for Go (specifically CloudWatch Logs APIs), pagination handling, Steampipe plugin development patterns, and basic error handling, which are moderately complex but follow established patterns in the codebase. Third, while the problem statement does not explicitly mention edge cases, the code changes include rate limiting and pagination, indicating some consideration for API constraints, though deeper error handling (e.g., specific AWS error codes or retry logic) might be needed but is not explicitly required. Overall, this task requires understanding multiple concepts and making complex modifications across several files, but it does not demand deep architectural changes or advanced domain-specific knowledge beyond typical AWS and plugin development expertise, placing it at a medium difficulty level of 0.55.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[bug]: 0.19.0rc1 on testnet4:  `lncli` reports all errors as \"wallet is encrypted\"\n### Background\n\nAny lnd error returns the same message to lncli\n\n### Your environment\n\n* version of `lnd` 0.19.0-beta.rc1 commit=v0.19.0-beta.rc1\n* which operating system (`uname -a` on *Nix) macos/docker\n* version of `btcd`, `bitcoind`, or other backend bitcoind 28.1\n* any other relevant environment details fresh install of testnet4\n\n### Steps to reproduce\n ```\n% lncli openchannel --node_key 030f289f0f931cd33cc3435dc5f5fc2a6a65bb0297327d43e82562aded10df74a7 --fee_rate_ppm 1 --local_amt 15000000\n[lncli] wallet is encrypted - please unlock using 'lncli unlock', or set password using 'lncli create' if this is the first time starting lnd\n```\n\nwhile in the log:\n```\n2025-03-26 13:24:03.150 [INF] CHFD: Performing funding tx coin selection using 8626 sat/kw as fee rate\n2025-03-26 13:24:03.150 [ERR] RPCS: unable to open channel to NodeKey(030f289f0f931cd33cc3435dc5f5fc2a6a65bb0297327d43e82562aded10df74a7): not enough witness outputs to create funding transaction, need 0.15000000 BTC only have 0 BTC available\n2025-03-26 13:24:03.151 [ERR] RPCS: [/lnrpc.Lightning/OpenChannel]: not enough witness outputs to create funding transaction, need 0.15000000 BTC only have 0 BTC available\n```\n\n\n", "patch": "diff --git a/cmd/commands/commands.go b/cmd/commands/commands.go\nindex 9cbd0bf7a7..29060ae9a6 100644\n--- a/cmd/commands/commands.go\n+++ b/cmd/commands/commands.go\n@@ -24,6 +24,7 @@ import (\n \t\"github.com/lightningnetwork/lnd/lnwire\"\n \t\"github.com/lightningnetwork/lnd/routing\"\n \t\"github.com/lightningnetwork/lnd/routing/route\"\n+\t\"github.com/lightningnetwork/lnd/rpcperms\"\n \t\"github.com/lightningnetwork/lnd/signal\"\n \t\"github.com/urfave/cli\"\n \t\"golang.org/x/term\"\n@@ -258,7 +259,7 @@ func actionDecorator(f func(*cli.Context) error) func(*cli.Context) error {\n \t\t\treturn nil\n \t\t}\n \n-\t\t// Try to parse the Status representatio from this error.\n+\t\t// Try to parse the Status representation from this error.\n \t\ts, ok := status.FromError(err)\n \n \t\t// If this cannot be represented by a Status, exit early.\n@@ -270,21 +271,22 @@ func actionDecorator(f func(*cli.Context) error) func(*cli.Context) error {\n \t\t// 'unlock') but the wallet is already unlocked, then these\n \t\t// methods aren't recognized any more because this service is\n \t\t// shut down after successful unlock.\n-\t\tif s.Code() == codes.Unknown &&\n-\t\t\t(c.Command.Name == \"create\" ||\n-\t\t\t\tc.Command.Name == \"unlock\" ||\n-\t\t\t\tc.Command.Name == \"changepassword\" ||\n-\t\t\t\tc.Command.Name == \"createwatchonly\") {\n+\t\tif s.Code() == codes.Unknown && strings.Contains(\n+\t\t\ts.Message(), rpcperms.ErrWalletUnlocked.Error(),\n+\t\t) && (c.Command.Name == \"create\" ||\n+\t\t\tc.Command.Name == \"unlock\" ||\n+\t\t\tc.Command.Name == \"changepassword\" ||\n+\t\t\tc.Command.Name == \"createwatchonly\") {\n \n \t\t\treturn errors.New(\"wallet is already unlocked\")\n \t\t}\n \n \t\t// lnd might be active, but not possible to contact using RPC if\n-\t\t// the wallet is encrypted. If we get error code Unknown, it\n-\t\t// means that lnd is running, but the RPC server is not active\n-\t\t// yet (only WalletUnlocker server active) and most likely this\n-\t\t// is because of an encrypted wallet.\n-\t\tif s.Code() == codes.Unknown {\n+\t\t// the wallet is encrypted.\n+\t\tif s.Code() == codes.Unknown && strings.Contains(\n+\t\t\ts.Message(), rpcperms.ErrWalletLocked.Error(),\n+\t\t) {\n+\n \t\t\treturn errors.New(\"wallet is encrypted - please \" +\n \t\t\t\t\"unlock using 'lncli unlock', or set \" +\n \t\t\t\t\"password using 'lncli create' if this is \" +\n", "instance_id": "lightningnetwork__lnd-9649", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: `lncli` reports all errors as \"wallet is encrypted\" even when the actual error (e.g., insufficient funds for a channel opening) is unrelated to wallet encryption. The background, environment details, steps to reproduce, and logs provide sufficient context to understand the bug. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior or desired output for error reporting. Additionally, edge cases or other scenarios where this misreporting might occur are not specified. Overall, the problem is valid and mostly clear, but it misses some finer details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows that the change is localized to a single file (`commands.go`) and involves modifying a specific error-handling logic block within the `actionDecorator` function. The change does not impact the broader system architecture or require modifications across multiple modules. The amount of code change is minimal, focusing on refining how error messages are interpreted and displayed.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Go, error handling using the `status` package (likely from gRPC), and string matching with `strings.Contains`. Additionally, familiarity with the specific error messages (`ErrWalletUnlocked` and `ErrWalletLocked`) from the `rpcperms` package is needed. These concepts are relatively straightforward for someone with moderate experience in Go and gRPC-based systems.\n\n3. **Edge Cases and Error Handling**: The problem focuses on improving error message accuracy by distinguishing between specific error conditions (wallet locked vs. other errors). The code change addresses this by checking for specific error strings in the message, which is a simple but effective approach. However, there is no mention of additional edge cases (e.g., other unrelated errors being misreported) in the problem statement, and the code change does not introduce complex error-handling logic beyond the current scope.\n\n4. **Overall Complexity**: The task involves understanding the existing error-handling flow and making a targeted modification to improve user feedback. It does not require deep knowledge of the `lnd` codebase's architecture or complex interactions between components. The fix is more about refining an existing mechanism than introducing new functionality or solving a deeply intricate problem.\n\nGiven these points, a difficulty score of 0.35 reflects an \"Easy\" problem that requires some understanding of the code logic and a simple, localized modification. It is slightly above the lower end of the easy range due to the need to understand gRPC error handling and the specific context of the `lnd` CLI tool, but it remains a relatively straightforward bug fix for a developer with basic to intermediate skills in Go.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Panic crash on unexpected content in TXT record on google provider\n<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\n-->\n\n**What happened**:\nExternalDNS crashed on unexpected TXT record returned by google provider in zone it manages\n\nPanic log:\n```\ntime=\"2025-02-26T16:55:30Z\" level=info msg=\"config: {APIServerURL: KubeConfig: RequestTimeout:30s DefaultTargets:[] GlooNamespaces:[gloo-system] SkipperRouteGroupVersion:zalando.org/v1 Sources:[service ingress] Namespace: AnnotationFilter: LabelFilter: IngressClassNames:[] FQDNTemplate: CombineFQDNAndAnnotation:false IgnoreHostnameAnnotation:false IgnoreIngressTLSSpec:false IgnoreIngressRulesSpec:false GatewayNamespace: GatewayLabelFilter: Compatibility: PublishInternal:false PublishHostIP:false AlwaysPublishNotReadyAddresses:false ConnectorSourceServer:localhost:8080 Provider:google ProviderCacheTime:0s GoogleProject:d360-devops-tests GoogleBatchChangeSize:1000 GoogleBatchChangeInterval:1s GoogleZoneVisibility: DomainFilter:[dev-apps-gcloud-666.europe-central2-gcloud.example.com dev-apps-gcloud-666.example.com dev-apps-gcloud-666.europe-central2-gcloud.internal.example.com dev-apps-gcloud-666.mtls.internal.example.com] ExcludeDomains:[] RegexDomainFilter: RegexDomainExclusion: ZoneNameFilter:[] ZoneIDFilter:[] TargetNetFilter:[] ExcludeTargetNets:[] AlibabaCloudConfigFile:/etc/kubernetes/alibaba-cloud.json AlibabaCloudZoneType: AWSZoneType: AWSZoneTagFilter:[] AWSAssumeRole: AWSProfiles:[] AWSAssumeRoleExternalID: AWSBatchChangeSize:1000 AWSBatchChangeSizeBytes:32000 AWSBatchChangeSizeValues:1000 AWSBatchChangeInterval:1s AWSEvaluateTargetHealth:true AWSAPIRetries:3 AWSPreferCNAME:false AWSZoneCacheDuration:0s AWSSDServiceCleanup:false AWSSDCreateTag:map[] AWSZoneMatchParent:false AWSDynamoDBRegion: AWSDynamoDBTable:external-dns AzureConfigFile:/etc/kubernetes/azure.json AzureResourceGroup: AzureSubscriptionID: AzureUserAssignedIdentityClientID: AzureActiveDirectoryAuthorityHost: AzureZonesCacheDuration:0s CloudflareProxied:false CloudflareDNSRecordsPerPage:100 CloudflareRegionKey: CoreDNSPrefix:/skydns/ AkamaiServiceConsumerDomain: AkamaiClientToken: AkamaiClientSecret: AkamaiAccessToken: AkamaiEdgercPath: AkamaiEdgercSection: OCIConfigFile:/etc/kubernetes/oci.yaml OCICompartmentOCID: OCIAuthInstancePrincipal:false OCIZoneScope:GLOBAL OCIZoneCacheDuration:0s InMemoryZones:[] OVHEndpoint:ovh-eu OVHApiRateLimit:20 PDNSServer:http://localhost:8081 PDNSServerID:localhost PDNSAPIKey: PDNSSkipTLSVerify:false TLSCA: TLSClientCert: TLSClientCertKey: Policy:sync Registry:txt TXTOwnerID:default TXTPrefix: TXTSuffix: TXTEncryptEnabled:false TXTEncryptAESKey: Interval:1m0s MinEventSyncInterval:5s Once:false DryRun:false UpdateEvents:false LogFormat:text MetricsAddress::7979 LogLevel:debug TXTCacheInterval:0s TXTWildcardReplacement: ExoscaleEndpoint: ExoscaleAPIKey: ExoscaleAPISecret: ExoscaleAPIEnvironment:api ExoscaleAPIZone:ch-gva-2 CRDSourceAPIVersion:externaldns.k8s.io/v1alpha1 CRDSourceKind:DNSEndpoint ServiceTypeFilter:[] CFAPIEndpoint: CFUsername: CFPassword: ResolveServiceLoadBalancerHostname:false RFC2136Host: RFC2136Port:0 RFC2136Zone:[] RFC2136Insecure:false RFC2136GSSTSIG:false RFC2136CreatePTR:false RFC2136KerberosRealm: RFC2136KerberosUsername: RFC2136KerberosPassword: RFC2136TSIGKeyName: RFC2136TSIGSecret: RFC2136TSIGSecretAlg: RFC2136TAXFR:false RFC2136MinTTL:0s RFC2136BatchChangeSize:50 RFC2136UseTLS:false RFC2136SkipTLSVerify:false NS1Endpoint: NS1IgnoreSSL:false NS1MinTTLSeconds:0 TransIPAccountName: TransIPPrivateKeyFile: DigitalOceanAPIPageSize:50 ManagedDNSRecordTypes:[A AAAA CNAME] ExcludeDNSRecordTypes:[] GoDaddyAPIKey: GoDaddySecretKey: GoDaddyTTL:0 GoDaddyOTE:false OCPRouterName: IBMCloudProxied:false IBMCloudConfigFile:/etc/kubernetes/ibmcloud.json TencentCloudConfigFile:/etc/kubernetes/tencent-cloud.json TencentCloudZoneType: PiholeServer: PiholePassword: PiholeTLSInsecureSkipVerify:false PluralCluster: PluralProvider: WebhookProviderURL:http://localhost:8888 WebhookProviderReadTimeout:5s WebhookProviderWriteTimeout:10s WebhookServer:false TraefikDisableLegacy:false TraefikDisableNew:false NAT64Networks:[]}\"\ntime=\"2025-02-26T16:55:30Z\" level=info msg=\"Instantiating new Kubernetes client\"\ntime=\"2025-02-26T16:55:30Z\" level=debug msg=\"apiServerURL: \"\ntime=\"2025-02-26T16:55:30Z\" level=debug msg=\"kubeConfig: \"\ntime=\"2025-02-26T16:55:30Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\"\ntime=\"2025-02-26T16:55:30Z\" level=info msg=\"Created Kubernetes client https://10.14.192.1:443\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Matching zones against domain filters: {[dev-apps-gcloud-666.europe-central2-gcloud.example.com dev-apps-gcloud-666.example.com dev-apps-gcloud-666.europe-central2-gcloud.internal.example.com dev-apps-gcloud-666.mtls.internal.example.com] [] <nil> <nil>}\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered jx.cicd.internal.example.com. (zone: cicd-internal-example-com) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-upgrade.europe-west3-gcloud.internal.example.com. (zone: picked-tick-internal) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-upgrade.europe-west3-gcloud.example.com. (zone: picked-tick-public) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-upgrade.example.com. (zone: picked-tick-public-short) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Matched dev-apps-gcloud-666.europe-central2-gcloud.internal.example.com. (zone: ruling-kite-internal) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Matched dev-apps-gcloud-666.mtls.internal.example.com. (zone: ruling-kite-internal-mtls) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Matched dev-apps-gcloud-666.europe-central2-gcloud.example.com. (zone: ruling-kite-public) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Matched dev-apps-gcloud-666.example.com. (zone: ruling-kite-public-short) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-777.europe-central2-gcloud.internal.example.com. (zone: skilled-bird-internal) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-777.mtls.internal.example.com. (zone: skilled-bird-internal-mtls) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-777.europe-central2-gcloud.example.com. (zone: skilled-bird-public) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-777.example.com. (zone: skilled-bird-public-short) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-888.europe-west3-gcloud.internal.example.com. (zone: social-kingfish-internal) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-888.mtls.internal.example.com. (zone: social-kingfish-internal-mtls) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-888.europe-west3-gcloud.example.com. (zone: social-kingfish-public) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Filtered dev-apps-gcloud-888.example.com. (zone: social-kingfish-public-short) (visibility: public)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Considering zone: ruling-kite-internal (domain: dev-apps-gcloud-666.europe-central2-gcloud.internal.example.com.)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Considering zone: ruling-kite-internal-mtls (domain: dev-apps-gcloud-666.mtls.internal.example.com.)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Considering zone: ruling-kite-public (domain: dev-apps-gcloud-666.europe-central2-gcloud.example.com.)\"\ntime=\"2025-02-26T16:55:31Z\" level=debug msg=\"Considering zone: ruling-kite-public-short (domain: dev-apps-gcloud-666.example.com.)\"\npanic: runtime error: index out of range [0] with length 0\n\ngoroutine 1 [running]:\nsigs.k8s.io/external-dns/registry.(*TXTRegistry).Records(0xc00098f6c0, {0x4599c58?, 0xc00090a370?})\n        sigs.k8s.io/external-dns/registry/txt.go:133 +0xd51\nsigs.k8s.io/external-dns/controller.(*Controller).RunOnce(0xc000c4ca80, {0x4599c58, 0xc00090a370})\n        sigs.k8s.io/external-dns/controller/controller.go:212 +0xf8\nsigs.k8s.io/external-dns/controller.(*Controller).Run(0xc000c4ca80, {0x4599c58, 0xc00090a370})\n        sigs.k8s.io/external-dns/controller/controller.go:360 +0xc5\nmain.main()\n        sigs.k8s.io/external-dns/main.go:434 +0x396f\n```\n\nDNS record on which externalDNS crashes:\n```\n\u276f gcloud dns record-sets list --zone ruling-kite-public-short --name 'test.dev-apps-gcloud-666.example.com.'\nNAME                                    TYPE  TTL  DATA\ntest.dev-apps-gcloud-666.example.com.  TXT   300  20.0: \"test-dst\"\n```\n\n```\n\u276f dig @ns-cloud-a1.googledomains.com. test.dev-apps-gcloud-666.example.com. -t txt\n\n; <<>> DiG 9.18.33 <<>> @ns-cloud-a1.googledomains.com. test.dev-apps-gcloud-666.example.com. -t txt\n; (2 servers found)\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 40507\n;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n;; WARNING: recursion requested but not available\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n; COOKIE: bfba31b944a3d4bf01b4cd56d22f9098fc460ddd94c30939a608faf7d8 (good)\n;; QUESTION SECTION:\n;test.dev-apps-gcloud-666.example.com. IN TXT\n\n;; ANSWER SECTION:\ntest.dev-apps-gcloud-666.example.com. 300 IN TXT \"test-dst\"\n\n;; Query time: 107 msec\n;; SERVER: 216.239.32.106#53(ns-cloud-a1.googledomains.com.) (UDP)\n;; MSG SIZE  rcvd: 120\n```\n\n\n**What you expected to happen**:\nExternalDNS ignores unexpected TXT records and continues to work (and probably logs ignored records)  \n\n**How to reproduce it (as minimally and precisely as possible)**:\n\nSetup externalDNS to use google provider and create \"weighted round robin\" record like this\n```\ngcloud dns record-sets create 'test.dev-apps-gcloud-666.example.com.' --zone=ruling-kite-public-short --type=TXT --routing-policy-type=WRR --routing-policy-data=\"20=test-dst\" --ttl=300\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- External-DNS version (use `external-dns --version`): `registry.k8s.io/external-dns/external-dns:v0.15.1`\n- DNS provider: `google`\n- Others: CLI parameters\n```\n        - --metrics-address=:7979\n        - --log-level=debug\n        - --log-format=text\n        - --domain-filter=dev-apps-gcloud-666.europe-central2-gcloud.example.com\n        - --domain-filter=dev-apps-gcloud-666.example.com\n        - --domain-filter=dev-apps-gcloud-666.europe-central2-gcloud.internal.example.com\n        - --domain-filter=dev-apps-gcloud-666.mtls.internal.example.com\n        - --policy=sync\n        - --provider=google\n        - --registry=txt\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --google-project=d360-devops-tests\n        - --google-batch-change-size=1000\n```\n\n", "patch": "diff --git a/registry/txt.go b/registry/txt.go\nindex 1ed7cb0b92..06a8314a78 100644\n--- a/registry/txt.go\n+++ b/registry/txt.go\n@@ -146,6 +146,11 @@ func (im *TXTRegistry) Records(ctx context.Context) ([]*endpoint.Endpoint, error\n \t\t\tcontinue\n \t\t}\n \t\t// We simply assume that TXT records for the registry will always have only one target.\n+\t\t// If there are no targets (e.g for routing policy based records in google), direct targets will be empty\n+\t\tif len(record.Targets) == 0 {\n+\t\t\tlog.Errorf(\"TXT record has no targets %s\", record.DNSName)\n+\t\t\tcontinue\n+\t\t}\n \t\tlabels, err := endpoint.NewLabelsFromString(record.Targets[0], im.txtEncryptAESKey)\n \t\tif errors.Is(err, endpoint.ErrInvalidHeritage) {\n \t\t\t// if no heritage is found or it is invalid\n", "instance_id": "kubernetes-sigs__external-dns-5149", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue, including the context of the crash (unexpected TXT record content in the Google DNS provider), the expected behavior (ignoring unexpected records), and steps to reproduce the issue. It also includes relevant logs, environment details, and specific CLI parameters used. However, there are minor ambiguities: the problem statement does not explicitly define what constitutes an \"unexpected\" TXT record beyond the example provided (e.g., routing policy-based records with no direct targets). Additionally, edge cases beyond the provided example are not discussed, which could leave room for interpretation when generalizing the solution. Overall, the statement is valid and clear but lacks exhaustive detail on potential variations of the issue.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`registry/txt.go`) and involves a small modification (adding a check for empty targets in TXT records). It does not impact the broader architecture of the system or require changes across multiple modules. The change is straightforward, adding a conditional check and a log statement before proceeding with processing.\n\n2. **Clarity and Complexity of Problem Description**: While the problem statement is mostly clear, the solution aligns directly with the issue (handling empty targets in TXT records). The logic to solve the problem is not inherently complex, as it involves basic error checking and logging.\n\n3. **Number of Technical Concepts**: The solution requires understanding of basic Go programming constructs (e.g., conditionals, logging), as well as familiarity with the specific behavior of TXT records in the context of ExternalDNS and the Google DNS provider. However, no advanced language features, complex algorithms, or design patterns are needed. The domain-specific knowledge (DNS records and routing policies) is moderately specialized but not overly challenging for someone familiar with DNS management tools.\n\n4. **Edge Cases and Error Handling**: The problem statement highlights a specific edge case (TXT records with no targets due to routing policies), and the code change addresses it directly by skipping such records and logging an error. While there might be other edge cases related to TXT record formats or provider-specific behaviors, the provided solution does not require extensive additional error handling beyond the current fix. The complexity of edge cases appears manageable within the scope of this change.\n\nOverall, this problem requires understanding some code logic and making a simple modification to handle a specific edge case. It does not demand deep architectural changes or advanced technical expertise, justifying a difficulty score of 0.30. The primary challenge lies in verifying that the fix generalizes to other potential edge cases, but this is not indicated as a significant concern in the current context.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Dozzle not self reporting version correctly\nDescribe the bug:\r\nThe self reported version since v8.0.2 just says 'head'. This doesn't seem to have any adverse effects other than whilst being on the latest version the update announcement button says I've got a lot of catching up to do :-).\r\n\r\nSteps to reproduce the behavior:\r\nRun any version since v8.0.2 and check Dozzle's logs at just the info level, the message for the self reported version is just:\r\n`msg=Dozzle version head`\r\n\r\nExpected behavior:\r\nThe version is reported correctly, like v8.0.2.\r\n`msg=Dozzle version v8.0.2`\r\n\r\nScreenshots:\r\n![image](https://github.com/amir20/dozzle/assets/65852870/c380e59b-f07a-4106-adfc-72da7a04e274)\r\n\r\n\r\n- OS: Debian 12\r\n- Docker version: 27.0.3\r\n- Browser & version: Chrome 126.0.6478.127\r\n- Version: v8.0.3 (latest)\n", "patch": "diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml\nindex b69407cebd56..d25502759166 100644\n--- a/.github/workflows/deploy.yml\n+++ b/.github/workflows/deploy.yml\n@@ -88,11 +88,6 @@ jobs:\n     name: Release\n     runs-on: ubuntu-latest\n     steps:\n-      - name: Docker meta\n-        id: meta\n-        uses: docker/metadata-action@v5\n-        with:\n-          images: amir20/dozzle\n       - name: Set up Docker Buildx\n         uses: docker/setup-buildx-action@4fd812986e6c8c2a69e18311145f9371337f27d4 # v3.4.0\n       - name: Login to DockerHub\n@@ -102,6 +97,11 @@ jobs:\n           password: ${{ secrets.DOCKER_PASSWORD }}\n       - name: Checkout\n         uses: actions/checkout@v4\n+      - name: Docker meta\n+        id: meta\n+        uses: docker/metadata-action@v5\n+        with:\n+          images: amir20/dozzle\n       - name: Writing certs to file\n         run: |\n           echo \"${{ secrets.TTL_KEY }}\" > shared_key.pem\ndiff --git a/.github/workflows/dev.yml b/.github/workflows/dev.yml\nindex b1ee0980d1eb..aaf2ab8c7084 100644\n--- a/.github/workflows/dev.yml\n+++ b/.github/workflows/dev.yml\n@@ -12,11 +12,6 @@ jobs:\n     runs-on: ubuntu-latest\n     if: ${{ !github.event.repository.fork && !github.event.pull_request.head.repo.fork && (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == 'amir20/dozzle') }}\n     steps:\n-      - name: Docker meta\n-        id: meta\n-        uses: docker/metadata-action@v5\n-        with:\n-          images: amir20/dozzle\n       - name: Set up Docker Buildx\n         uses: docker/setup-buildx-action@4fd812986e6c8c2a69e18311145f9371337f27d4 # v3.4.0\n       - name: Login to DockerHub\n@@ -26,6 +21,11 @@ jobs:\n           password: ${{ secrets.DOCKER_PASSWORD }}\n       - name: Checkout\n         uses: actions/checkout@v4\n+      - name: Docker meta\n+        id: meta\n+        uses: docker/metadata-action@v5\n+        with:\n+          images: amir20/dozzle\n       - name: Writing certs to file\n         run: |\n           echo \"${{ secrets.TTL_KEY }}\" > shared_key.pem\ndiff --git a/Dockerfile b/Dockerfile\nindex 58608877ebe7..4ddabee6a4aa 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -53,7 +53,7 @@ ARG TARGETOS TARGETARCH\n RUN go generate\n \n # Build binary\n-RUN GOOS=$TARGETOS GOARCH=$TARGETARCH CGO_ENABLED=0 go build -ldflags \"-s -w -X cli.version=$TAG\"  -o dozzle\n+RUN GOOS=$TARGETOS GOARCH=$TARGETARCH CGO_ENABLED=0 go build -ldflags \"-s -w -X github.com/amir20/dozzle/internal/support/cli.Version=$TAG\" -o dozzle\n \n RUN mkdir /data\n \ndiff --git a/Makefile b/Makefile\nindex fe38512d0fc7..d463522f659b 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -26,11 +26,11 @@ test: fake_assets generate\n \n .PHONY: build\n build: dist generate\n-\tCGO_ENABLED=0 go build -ldflags \"-s -w\"\n+\tCGO_ENABLED=0 go build -ldflags \"-s -w -X github.com/amir20/dozzle/internal/support/cli.Version=local\"\n \n .PHONY: docker\n docker: shared_key.pem shared_cert.pem\n-\t@docker build -t amir20/dozzle .\n+\t@docker build  --build-arg TAG=local -t amir20/dozzle .\n \n generate: shared_key.pem shared_cert.pem $(GEN_FILES)\n \ndiff --git a/internal/support/cli/args.go b/internal/support/cli/args.go\nindex d34150eacf3d..9b1c4cf1f137 100644\n--- a/internal/support/cli/args.go\n+++ b/internal/support/cli/args.go\n@@ -6,9 +6,7 @@ import (\n \t\"github.com/alexflint/go-arg\"\n )\n \n-var (\n-\tversion = \"head\"\n-)\n+var Version = \"head\"\n \n type Args struct {\n \tAddr            string              `arg:\"env:DOZZLE_ADDR\" default:\":8080\" help:\"sets host:port to bind for server. This is rarely needed inside a docker container.\"`\n@@ -46,7 +44,7 @@ type GenerateCmd struct {\n }\n \n func (Args) Version() string {\n-\treturn version\n+\treturn Version\n }\n \n func ParseArgs() (Args, interface{}) {\ndiff --git a/internal/support/cli/clients.go b/internal/support/cli/clients.go\nindex effdc54e953b..32e9e6832e56 100644\n--- a/internal/support/cli/clients.go\n+++ b/internal/support/cli/clients.go\n@@ -52,12 +52,12 @@ func CreateMultiHostService(embededCerts embed.FS, args Args) *docker_support.Mu\n \t\tif err != nil {\n \t\t\tlog.Debugf(\"could not connect to local Docker Engine: %s\", err)\n \t\t\tif !args.NoAnalytics {\n-\t\t\t\tgo StartEvent(version, args.Mode, args.RemoteAgent, args.RemoteHost, nil, \"\")\n+\t\t\t\tgo StartEvent(args.Version(), args.Mode, args.RemoteAgent, args.RemoteHost, nil, \"\")\n \t\t\t}\n \t\t} else {\n \t\t\tlog.Debugf(\"connected to local Docker Engine\")\n \t\t\tif !args.NoAnalytics {\n-\t\t\t\tgo StartEvent(version, args.Mode, args.RemoteAgent, args.RemoteHost, localClient, \"\")\n+\t\t\t\tgo StartEvent(args.Version(), args.Mode, args.RemoteAgent, args.RemoteHost, localClient, \"\")\n \t\t\t}\n \t\t\tclients = append(clients, docker_support.NewDockerClientService(localClient))\n \t\t}\n", "instance_id": "amir20__dozzle-3084", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Dozzle is not reporting its version correctly since v8.0.2, instead showing 'head' in the logs. The expected behavior (reporting the correct version like 'v8.0.2') and steps to reproduce the issue are provided, along with a screenshot and environment details. However, there are minor ambiguities, such as a lack of explicit mention of how the version information is used within the application beyond logging and update notifications, and whether there are specific constraints or requirements for how the version should be set or retrieved. Additionally, edge cases or potential side effects of changing the version reporting mechanism are not discussed. Overall, the statement is valid and clear but misses some minor details that could provide a more comprehensive understanding of the problem context.", "difficulty_explanation": "The difficulty of this problem falls in the 'Easy' range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes involve multiple files, including CI/CD workflows (deploy.yml, dev.yml), the Dockerfile, Makefile, and application code (args.go, clients.go). However, the changes are relatively straightforward, primarily focusing on how the version string is set during the build process and accessed in the code. The modifications do not impact the system's core architecture and are limited to build configurations and a few variable references. The overall amount of code change is small and localized.\n\n2. **Number of Technical Concepts:** Solving this requires a basic understanding of Go build flags (ldflags for setting variables at compile time), Docker build arguments, and GitHub Actions workflows for CI/CD. These concepts are not particularly complex for someone with moderate experience in Go and containerized environments. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond standard build and deployment practices.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not introduce significant error handling requirements. The primary focus is on correctly passing and displaying the version string, which is unlikely to involve complex edge cases beyond ensuring the version is set correctly in different build environments (e.g., local vs. CI/CD). There are no apparent performance or compatibility concerns introduced by these changes.\n\n4. **Overall Complexity:** The task requires understanding how build-time variables are set in Go and ensuring consistency across local and CI/CD builds. While it spans multiple files, the logic is simple\u2014update the way the version is injected during the build and referenced in the code. This is a typical bug fix for build configuration rather than a deep architectural change or complex feature implementation.\n\nA score of 0.30 reflects the need for some code logic understanding and modifications across a few files, but the problem remains straightforward with minimal risk of unintended consequences or need for advanced expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "acme-dns: add CNAME provisioned flag for HTTP storage\nWith the new acme-dns HTTP storage feature, it is possible to complete the challenge if the HTTP storage server will also create the CNAME records.  Currently, the acme-dns provider will always return an error indicating that the CNAME must be created.\r\n\r\nThis adds an environment configuration to tell the acme-dns provider to presume the CNAME record has been added to the DNS zone.  This allows the challenge to complete successfully when the CNAME record is created when a new DNS challenge is registered with acme-dns.  This is mainly useful when using the HTTP storage backend if the server will also create the CNAME record.\r\n\r\nRelated to (#2393)\n", "patch": "diff --git a/providers/dns/acmedns/acmedns.go b/providers/dns/acmedns/acmedns.go\nindex 8aabaa14c9..9dd63d0a2d 100644\n--- a/providers/dns/acmedns/acmedns.go\n+++ b/providers/dns/acmedns/acmedns.go\n@@ -178,16 +178,26 @@ func (d *DNSProvider) register(ctx context.Context, domain, fqdn string) error {\n \t\treturn err\n \t}\n \n+\tvar cnameCreated bool\n+\n \t// Store the new account in the storage and call save to persist the data.\n \terr = d.storage.Put(ctx, domain, newAcct)\n \tif err != nil {\n-\t\treturn err\n+\t\tcnameCreated = errors.Is(err, internal.ErrCNAMEAlreadyCreated)\n+\t\tif !cnameCreated {\n+\t\t\treturn err\n+\t\t}\n \t}\n+\n \terr = d.storage.Save(ctx)\n \tif err != nil {\n \t\treturn err\n \t}\n \n+\tif cnameCreated {\n+\t\treturn nil\n+\t}\n+\n \t// Stop issuance by returning an error.\n \t// The user needs to perform a manual one-time CNAME setup in their DNS zone\n \t// to complete the setup of the new account we created.\ndiff --git a/providers/dns/acmedns/internal/http_storage.go b/providers/dns/acmedns/internal/http_storage.go\nindex 1a1e8e2eee..7a535eb20b 100644\n--- a/providers/dns/acmedns/internal/http_storage.go\n+++ b/providers/dns/acmedns/internal/http_storage.go\n@@ -4,6 +4,7 @@ import (\n \t\"bytes\"\n \t\"context\"\n \t\"encoding/json\"\n+\t\"errors\"\n \t\"fmt\"\n \t\"io\"\n \t\"net/http\"\n@@ -17,6 +18,8 @@ import (\n \n var _ goacmedns.Storage = (*HTTPStorage)(nil)\n \n+var ErrCNAMEAlreadyCreated = errors.New(\"the CNAME has already been created\")\n+\n // HTTPStorage is an implementation of [acmedns.Storage] over HTTP.\n type HTTPStorage struct {\n \tclient  *http.Client\n@@ -98,6 +101,11 @@ func (s *HTTPStorage) do(req *http.Request, result any) error {\n \t}\n \n \tif result == nil {\n+\t\t// Hack related to `Put`.\n+\t\tif resp.StatusCode == http.StatusCreated {\n+\t\t\treturn ErrCNAMEAlreadyCreated\n+\t\t}\n+\n \t\treturn nil\n \t}\n \ndiff --git a/providers/dns/acmedns/internal/readme.md b/providers/dns/acmedns/internal/readme.md\nindex bccdc5388b..b667d3d23c 100644\n--- a/providers/dns/acmedns/internal/readme.md\n+++ b/providers/dns/acmedns/internal/readme.md\n@@ -61,7 +61,9 @@ Endpoint: `POST <BaseURL>/<domain>`\n \n ### Response\n \n-Response status code 200.\n+Response status code:\n+- 200: the process will be stopped to allow the user to create the CNAME.\n+- 201: the process will continue without error (the CNAME should be created by the server)\n \n No expected body.\n \n", "instance_id": "go-acme__lego-2437", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a feature to the acme-dns provider to handle CNAME record creation via an HTTP storage backend. It specifies the intent to allow the challenge to complete successfully by presuming the CNAME record has been added through an environment configuration. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the environment configuration is set or accessed in the code, nor does it mention specific edge cases or error conditions that might arise from this change (e.g., what happens if the server fails to create the CNAME despite returning a success status). Additionally, the statement lacks examples or detailed input/output expectations for the HTTP storage interaction. Despite these minor gaps, the overall intent and context are understandable, especially when paired with the code changes, which provide further clarity on the implementation approach.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, involving modifications across three files: the main provider logic (`acmedns.go`), the HTTP storage implementation (`http_storage.go`), and documentation (`readme.md`). The changes are not extensive in terms of lines of code but require understanding the interaction between the storage backend and the DNS provider logic. Second, the technical concepts involved include error handling in Go (using custom errors like `ErrCNAMEAlreadyCreated`), HTTP client interactions, and conditional logic based on response status codes (e.g., distinguishing between HTTP 200 and 201). These concepts are not overly complex for an experienced developer but do require familiarity with Go's error handling patterns and HTTP semantics. Third, the problem introduces a specific edge case\u2014handling the scenario where the CNAME is presumed to be created by the server\u2014but the code changes address this directly with minimal additional complexity. There are no significant architectural impacts or performance considerations evident from the changes. Overall, this task requires a moderate level of understanding of the codebase and careful implementation of conditional logic and error handling, justifying a score of 0.45, which sits in the medium difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Atlantis plans less projects than anticipated by modified files \n<!--- Please keep this note for the community --->\r\n\r\n### Community Note\r\n\r\n* Please vote on this issue by adding a \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request. Searching for pre-existing feature requests helps us consolidate datapoints for identical requirements into a single place, thank you!\r\n* Please do not leave \"+1\" or other comments that do not add relevant new information or questions, they generate extra noise for issue followers and do not help prioritize the request.\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment.\r\n\r\n<!--- Thank you for keeping this note for the community --->\r\n\r\n---\r\n\r\n<!---\r\nWhen filing a bug, please include the following headings if possible.\r\nAny example text in this template can be deleted.\r\n--->\r\n\r\n### Overview of the Issue\r\n\r\n<!---\r\nPlease describe the issue you are having and how you encountered the problem.\r\n--->\r\nI believe that the GetModifiedFiles function in [atlantis](https://github.com/runatlantis/atlantis/tree/main)/[server](https://github.com/runatlantis/atlantis/tree/main/server)/[events](https://github.com/runatlantis/atlantis/tree/main/server/events)/[vcs](https://github.com/runatlantis/atlantis/tree/main/server/events/vcs)/[azuredevops_client.go](https://github.com/runatlantis/atlantis/blob/main/server/events/vcs/azuredevops_client.go), is not returning all changes to the files. This is due to the pagination of the API being called from [go-azuredevops](https://github.com/mcdafydd/go-azuredevops/tree/master)/[azuredevops](https://github.com/mcdafydd/go-azuredevops/tree/master/azuredevops)/[git.go](https://github.com/mcdafydd/go-azuredevops/blob/master/azuredevops/git.go). \r\n\r\nSee the reference to the API : https://learn.microsoft.com/en-us/rest/api/azure/devops/git/diffs/get?view=azure-devops-rest-5.1&tabs=HTTP\r\n\r\nThere is 2 parameters that should be used for pagination the $top and $skip. The $top default value is to return 100 elements. This results in getting less plans for projects than anticipated based on changes to files. \r\n\r\n### Reproduction Steps\r\n\r\n<!--- \r\nIn order to effectively and quickly resolve the issue, please provide exact steps that allow us the reproduce the problem. If no steps are provided, then it will likely take longer to get the issue resolved.\r\n--->\r\nTo reproduce the issue, in an Azure DevOps git repository you need to create a large pull request involving touching more than 100 directories and files combined. Then get atlantis to plan, you should see less plans than what you expect based on the number of files modified. \r\n\r\n### Logs\r\n\r\n<!---\r\nProvide log files from Atlantis server\r\n\r\nlogs can be retrieved from the deployment or from atlantis comments by adding `--debug` such as `atlantis plan --debug`\r\n\r\n<details>\r\n  <summary>Logs</summary>\r\n\r\n```\r\nlog output\r\n```\r\n\r\n</details>\r\n--->\r\n\"caller\":\"events/project_command_builder.go:338\",\"msg\":\"100 files were modified in this pull request. Modified files:\r\n\r\n\r\n### Environment details\r\n\r\n- Atlantis version: 0.28.5\r\n<!---\r\nIf not already included, please provide the following:\r\n\r\n- Atlantis version:\r\n- Deployment method: ecs/eks/helm/tf module\r\n- If not running the latest Atlantis version have you tried to reproduce this issue on the latest version: \r\n- Atlantis flags:\r\n\r\nAtlantis server-side config file:\r\n\r\n```yaml\r\n# config file\r\n```\r\n\r\nRepo `atlantis.yaml` file:\r\n\r\n```yaml\r\n# config file\r\n```\r\n\r\nAny other information you can provide about the environment/deployment (efs/nfs, aws/gcp, k8s/fargate, etc)\r\n--->\r\n\r\n### Additional Context\r\n\r\n<!---\r\nAdditional context on the problem. Docs, links to blogs, or other material that lead you to discover this issue or were helpful in troubleshooting the issue. \r\n\r\nUse a bulleted list to link to tickets\r\n--->\r\n\r\n\r\n\r\n\n", "patch": "diff --git a/go.mod b/go.mod\nindex 4337986c73..119a958c19 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -27,7 +27,7 @@ require (\n \tgithub.com/hashicorp/terraform-config-inspect v0.0.0-20241129133400-c404f8227ea6\n \tgithub.com/jpillora/backoff v1.0.0\n \tgithub.com/kr/pretty v0.3.1\n-\tgithub.com/mcdafydd/go-azuredevops v0.12.1\n+\tgithub.com/drmaxgit/go-azuredevops v0.13.1\n \tgithub.com/microcosm-cc/bluemonday v1.0.27\n \tgithub.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db\n \tgithub.com/mitchellh/go-homedir v1.1.0\ndiff --git a/go.sum b/go.sum\nindex 577d442608..8394032195 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -114,6 +114,8 @@ github.com/davidmz/go-pageant v1.0.2 h1:bPblRCh5jGU+Uptpz6LgMZGD5hJoOt7otgT454Wv\n github.com/davidmz/go-pageant v1.0.2/go.mod h1:P2EDDnMqIwG5Rrp05dTRITj9z2zpGcD9efWSkTNKLIE=\n github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=\n github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=\n+github.com/drmaxgit/go-azuredevops v0.13.1 h1:mqRxiwnbKraHAtqfTFa+xD75Atf84MgpjKJdA8hJeS4=\n+github.com/drmaxgit/go-azuredevops v0.13.1/go.mod h1:m1pO2fW60I9FahzLHMmHYq3bM446ZMZKDpd8+AEKzxc=\n github.com/emirpasic/gods v1.18.1 h1:FXtiHYKDGKCW2KzwZKx0iC0PQmdlorYgdFG9jPXJ1Bc=\n github.com/emirpasic/gods v1.18.1/go.mod h1:8tpGGwCnJ5H4r6BWwaV6OrWmMoPhUl5jm/FMNAnJvWQ=\n github.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ndiff --git a/server/controllers/events/azuredevops_request_validator.go b/server/controllers/events/azuredevops_request_validator.go\nindex 17e48fe7cb..4fa489c51f 100644\n--- a/server/controllers/events/azuredevops_request_validator.go\n+++ b/server/controllers/events/azuredevops_request_validator.go\n@@ -5,7 +5,7 @@ import (\n \t\"io\"\n \t\"net/http\"\n \n-\t\"github.com/mcdafydd/go-azuredevops/azuredevops\"\n+\t\"github.com/drmaxgit/go-azuredevops/azuredevops\"\n )\n \n //go:generate pegomock generate --package mocks -o mocks/mock_azuredevops_request_validator.go AzureDevopsRequestValidator\ndiff --git a/server/controllers/events/events_controller.go b/server/controllers/events/events_controller.go\nindex e3b2606332..1836795dd3 100644\n--- a/server/controllers/events/events_controller.go\n+++ b/server/controllers/events/events_controller.go\n@@ -22,8 +22,8 @@ import (\n \t\"strconv\"\n \t\"strings\"\n \n+\t\"github.com/drmaxgit/go-azuredevops/azuredevops\"\n \t\"github.com/google/go-github/v68/github\"\n-\t\"github.com/mcdafydd/go-azuredevops/azuredevops\"\n \t\"github.com/microcosm-cc/bluemonday\"\n \t\"github.com/pkg/errors\"\n \t\"github.com/runatlantis/atlantis/server/events\"\ndiff --git a/server/events/command_runner.go b/server/events/command_runner.go\nindex 30a82105a9..90870a467a 100644\n--- a/server/events/command_runner.go\n+++ b/server/events/command_runner.go\n@@ -17,8 +17,8 @@ import (\n \t\"fmt\"\n \t\"strconv\"\n \n+\t\"github.com/drmaxgit/go-azuredevops/azuredevops\"\n \t\"github.com/google/go-github/v68/github\"\n-\t\"github.com/mcdafydd/go-azuredevops/azuredevops\"\n \t\"github.com/pkg/errors\"\n \t\"github.com/runatlantis/atlantis/server/core/config/valid\"\n \t\"github.com/runatlantis/atlantis/server/events/command\"\ndiff --git a/server/events/event_parser.go b/server/events/event_parser.go\nindex c9cae1c828..c7c7117588 100644\n--- a/server/events/event_parser.go\n+++ b/server/events/event_parser.go\n@@ -23,10 +23,10 @@ import (\n \n \tgiteasdk \"code.gitea.io/sdk/gitea\"\n \n+\t\"github.com/drmaxgit/go-azuredevops/azuredevops\"\n \t\"github.com/go-playground/validator/v10\"\n \t\"github.com/google/go-github/v68/github\"\n \tlru \"github.com/hashicorp/golang-lru/v2\"\n-\t\"github.com/mcdafydd/go-azuredevops/azuredevops\"\n \t\"github.com/pkg/errors\"\n \t\"github.com/runatlantis/atlantis/server/events/command\"\n \t\"github.com/runatlantis/atlantis/server/events/models\"\ndiff --git a/server/events/mocks/mock_azuredevops_pull_getter.go b/server/events/mocks/mock_azuredevops_pull_getter.go\nindex 1ed64c3a10..2c298efa17 100644\n--- a/server/events/mocks/mock_azuredevops_pull_getter.go\n+++ b/server/events/mocks/mock_azuredevops_pull_getter.go\n@@ -4,7 +4,7 @@\n package mocks\n \n import (\n-\tazuredevops \"github.com/mcdafydd/go-azuredevops/azuredevops\"\n+\tazuredevops \"github.com/drmaxgit/go-azuredevops/azuredevops\"\n \tpegomock \"github.com/petergtz/pegomock/v4\"\n \tmodels \"github.com/runatlantis/atlantis/server/events/models\"\n \tlogging \"github.com/runatlantis/atlantis/server/logging\"\ndiff --git a/server/events/mocks/mock_event_parsing.go b/server/events/mocks/mock_event_parsing.go\nindex e6b72acacd..eb22979a56 100644\n--- a/server/events/mocks/mock_event_parsing.go\n+++ b/server/events/mocks/mock_event_parsing.go\n@@ -5,8 +5,8 @@ package mocks\n \n import (\n \tgitea \"code.gitea.io/sdk/gitea\"\n+\tazuredevops \"github.com/drmaxgit/go-azuredevops/azuredevops\"\n \tgithub \"github.com/google/go-github/v68/github\"\n-\tazuredevops \"github.com/mcdafydd/go-azuredevops/azuredevops\"\n \tpegomock \"github.com/petergtz/pegomock/v4\"\n \tmodels \"github.com/runatlantis/atlantis/server/events/models\"\n \tgitea0 \"github.com/runatlantis/atlantis/server/events/vcs/gitea\"\ndiff --git a/server/events/vcs/azuredevops_client.go b/server/events/vcs/azuredevops_client.go\nindex fee01ff552..01cf1f669b 100644\n--- a/server/events/vcs/azuredevops_client.go\n+++ b/server/events/vcs/azuredevops_client.go\n@@ -9,7 +9,7 @@ import (\n \t\"strings\"\n \t\"time\"\n \n-\t\"github.com/mcdafydd/go-azuredevops/azuredevops\"\n+\t\"github.com/drmaxgit/go-azuredevops/azuredevops\"\n \t\"github.com/pkg/errors\"\n \t\"github.com/runatlantis/atlantis/server/events/models\"\n \t\"github.com/runatlantis/atlantis/server/events/vcs/common\"\n@@ -68,28 +68,40 @@ func (g *AzureDevopsClient) GetModifiedFiles(logger logging.SimpleLogging, repo\n \ttargetRefName := strings.Replace(pullRequest.GetTargetRefName(), \"refs/heads/\", \"\", 1)\n \tsourceRefName := strings.Replace(pullRequest.GetSourceRefName(), \"refs/heads/\", \"\", 1)\n \n-\tr, resp, err := g.Client.Git.GetDiffs(g.ctx, owner, project, repoName, targetRefName, sourceRefName)\n-\tif err != nil {\n-\t\treturn nil, errors.Wrap(err, \"getting pull request\")\n-\t}\n-\tif resp.StatusCode != http.StatusOK {\n-\t\treturn nil, errors.Wrapf(err, \"http response code %d getting diff %s to %s\", resp.StatusCode, sourceRefName, targetRefName)\n-\t}\n+\tconst pageSize = 100 // Number of files from diff call\n+\tvar skip int\n \n-\tfor _, change := range r.Changes {\n-\t\titem := change.GetItem()\n-\t\t// Convert the path to a relative path from the repo's root.\n-\t\trelativePath := filepath.Clean(\"./\" + item.GetPath())\n-\t\tfiles = append(files, relativePath)\n+\tfor {\n+\t\tr, resp, err := g.Client.Git.GetDiffs(g.ctx, owner, project, repoName, targetRefName, sourceRefName, &azuredevops.GitDiffListOptions{\n+\t\t\tTop:  pageSize,\n+\t\t\tSkip: skip,\n+\t\t})\n+\t\tif err != nil {\n+\t\t\treturn nil, errors.Wrap(err, \"getting pull request\")\n+\t\t}\n+\t\tif resp.StatusCode != http.StatusOK {\n+\t\t\treturn nil, errors.Wrapf(err, \"http response code %d getting diff %s to %s\", resp.StatusCode, sourceRefName, targetRefName)\n+\t\t}\n \n-\t\t// If the file was renamed, we'll want to run plan in the directory\n-\t\t// it was moved from as well.\n-\t\tchangeType := azuredevops.Rename.String()\n-\t\tif change.ChangeType == &changeType {\n+\t\tfor _, change := range r.Changes {\n+\t\t\titem := change.GetItem()\n \t\t\t// Convert the path to a relative path from the repo's root.\n-\t\t\trelativePath = filepath.Clean(\"./\" + change.GetSourceServerItem())\n+\t\t\trelativePath := filepath.Clean(\"./\" + item.GetPath())\n \t\t\tfiles = append(files, relativePath)\n+\n+\t\t\t// If the file was renamed, we'll want to run plan in the directory\n+\t\t\t// it was moved from as well.\n+\t\t\tchangeType := azuredevops.Rename.String()\n+\t\t\tif change.ChangeType == &changeType {\n+\t\t\t\trelativePath = filepath.Clean(\"./\" + change.GetSourceServerItem())\n+\t\t\t\tfiles = append(files, relativePath)\n+\t\t\t}\n+\t\t}\n+\n+\t\tif len(r.Changes) < pageSize {\n+\t\t\tbreak // Break if we have reached the end\n \t\t}\n+\t\tskip += pageSize // Move to next page\n \t}\n \n \treturn files, nil\n", "instance_id": "runatlantis__atlantis-5298", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue with the `GetModifiedFiles` function in the Atlantis project, specifically pointing out the lack of pagination handling in the Azure DevOps API call which results in incomplete file change detection. The issue is well-contextualized with references to the specific API documentation and the parameters (`$top` and `$skip`) that need to be addressed. Reproduction steps are provided, which is helpful for validation, and the environment details add further context. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior for edge cases (e.g., API rate limits, partial failures during pagination) or provide detailed examples of the discrepancy between expected and actual plans. Additionally, the impact on the broader system (e.g., performance implications of fetching large datasets) is not discussed. Overall, while the core issue is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is relatively focused, primarily involving the `GetModifiedFiles` function in `azuredevops_client.go` to implement pagination using the `$top` and `$skip` parameters of the Azure DevOps API. The provided diff shows a straightforward loop to handle pagination, which is a moderate complexity task. However, it requires understanding the Azure DevOps API client library (`go-azuredevops`), specifically how to pass pagination options, and ensuring that the loop correctly handles all pages of data. The changes also involve updating the dependency to a forked or updated version (`drmaxgit/go-azuredevops`), which adds a small layer of complexity in verifying compatibility. \n\nThe number of technical concepts to understand is moderate: it includes API pagination, handling HTTP responses, and basic file path manipulation in Go. No advanced algorithms or design patterns are required. The changes impact multiple files due to the dependency update and import path changes across the codebase, but these are mechanical and do not require deep architectural understanding. \n\nEdge cases and error handling add some complexity. The problem statement does not explicitly mention edge cases like API rate limiting, network failures during pagination, or very large pull requests that might cause performance issues, but the code changes must account for these implicitly (e.g., ensuring the loop terminates correctly and handles API errors). The current diff does not add explicit error handling beyond the existing checks, which might be insufficient for robustness.\n\nOverall, this task requires a moderate level of understanding of the codebase and API interactions, with manageable code changes across a few files. It does not significantly impact the system's architecture or require advanced technical knowledge, placing it in the 0.4-0.6 range. I assign a score of 0.45 as it leans towards the lower end of medium difficulty due to the focused nature of the fix, despite the need for careful pagination handling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "A panic occurs when validating optional workspace during the execution of pipelinerun with CustomRun\n# Expected Behavior\n\nPipelineRun can execute normally, and the controller does not panic.\n\n# Actual Behavior\n\nThe controller has panicked.\n\n# Steps to Reproduce the Problem\n\n1. Install Tekton Pipelines\n```shell\nkubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.68.0/release.yaml\n```\n2. (Optional) Install Pipelines-in-Pipelines component\n```shell\n# Ref: https://github.com/tektoncd/experimental/tree/main/pipelines-in-pipelines\nkubectl apply --filename https://storage.googleapis.com/tekton-releases-nightly/pipelines-in-pipelines/latest/release.yaml\n```\n3. run the pipelinerun\n```yaml\ncat <<'EOF' | kubectl apply -f -\napiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: build-app\n  namespace: default\nspec:\n  workspaces:\n    - name: source\n      optional: true\n  params:\n    - name: step\n      type: string\n  tasks:\n    - name: step-1\n      workspaces:\n        - name: source\n          workspace: source\n      params:\n        - name: step\n          value: $(params.step)\n      taskSpec:\n        results:\n          - name: outputValue\n            description: the sum of the first and second operand\n        workspaces:\n          - name: source\n            optional: true\n        params:\n          - name: step\n            type: string\n        steps:\n          - name: echo\n            image: ubuntu\n            imagePullPolicy: IfNotPresent\n            workingDir: $(workspaces.source.path)\n            script: |\n              $(params.step)\n\n              echo -n \"`date`: $(context.taskRun.name)\" | tee $(results.outputValue.path)\n---\napiVersion: tekton.dev/v1\nkind: PipelineRun\nmetadata:\n  name: build-apps\nspec:\n  workspaces:\n    - name: source\n      volumeClaimTemplate:\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 1Gi\n  pipelineSpec:\n    workspaces:\n      - name: source\n        description: Workspace for shared code source\n        optional: true      # <- This configuration will trigger a panic.\n    tasks:\n      - name: build-app-1\n        taskRef:\n          apiVersion: tekton.dev/v1beta1\n          kind: Pipeline\n          name: build-app\n        params:\n          - name: step\n            value: |\n              #!/usr/bin/env bash\n              ls -artl\n        workspaces:\n          - name: source\n            workspace: source\nEOF\n```\n\n# Additional Info\n\n- Kubernetes version:\n\n  **Output of `kubectl version`:**\n\n```\nClient Version: v1.32.1\nKustomize Version: v5.5.0\nServer Version: v1.28.8\nWARNING: version difference between client (1.32) and server (1.28) exceeds the supported minor version skew of +/-1\n```\n\n- Tekton Pipeline version:\n\n  **Output of `tkn version` or `kubectl get pods -n tekton-pipelines -l app=tekton-pipelines-controller -o=jsonpath='{.items[0].metadata.labels.version}'`**\n\n```\nClient version: 0.39.1\nPipeline version: v0.68.0\n```\n\n\n<!-- Any other additional information -->\n\n", "patch": "diff --git a/pkg/reconciler/pipelinerun/resources/validate_dependencies.go b/pkg/reconciler/pipelinerun/resources/validate_dependencies.go\nindex 81261da82bd..a5d65dc351e 100644\n--- a/pkg/reconciler/pipelinerun/resources/validate_dependencies.go\n+++ b/pkg/reconciler/pipelinerun/resources/validate_dependencies.go\n@@ -101,12 +101,10 @@ func ValidateOptionalWorkspaces(pipelineWorkspaces []v1.PipelineWorkspaceDeclara\n \n \tfor _, rpt := range state {\n \t\tfor _, pws := range rpt.PipelineTask.Workspaces {\n-\t\t\tif optionalWorkspaces.Has(pws.Workspace) {\n+\t\t\tif rpt.ResolvedTask != nil && rpt.ResolvedTask.TaskSpec != nil && optionalWorkspaces.Has(pws.Workspace) {\n \t\t\t\tfor _, tws := range rpt.ResolvedTask.TaskSpec.Workspaces {\n-\t\t\t\t\tif tws.Name == pws.Name {\n-\t\t\t\t\t\tif !tws.Optional {\n-\t\t\t\t\t\t\treturn fmt.Errorf(\"pipeline workspace %q is marked optional but pipeline task %q requires it be provided\", pws.Workspace, rpt.PipelineTask.Name)\n-\t\t\t\t\t\t}\n+\t\t\t\t\tif tws.Name == pws.Name && !tws.Optional {\n+\t\t\t\t\t\treturn fmt.Errorf(\"pipeline workspace %q is marked optional but pipeline task %q requires it be provided\", pws.Workspace, rpt.PipelineTask.Name)\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n", "instance_id": "tektoncd__pipeline-8562", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a panic occurs in the Tekton Pipelines controller when validating optional workspaces during a PipelineRun with a CustomRun. The expected and actual behaviors are explicitly stated, and detailed steps to reproduce the issue are provided, including specific YAML configurations and installation instructions. Additionally, relevant version information for Kubernetes and Tekton Pipelines is included, which aids in contextualizing the problem. However, there are minor ambiguities that prevent a perfect score. The problem statement does not explicitly describe the root cause of the panic (e.g., a specific error message or stack trace), nor does it mention specific edge cases or constraints beyond the optional workspace configuration. While the reproduction steps are detailed, the lack of explicit discussion on the expected fix or the nature of the panic (e.g., whether it\u2019s a nil pointer dereference or another issue) leaves some room for interpretation. Overall, the statement is valid and clear but misses minor details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is minimal and localized to a single file (`validate_dependencies.go`) and a specific function, with only a small diff (a few lines modified). The change addresses a specific bug by adding a nil check for `rpt.ResolvedTask` and `rpt.ResolvedTask.TaskSpec` before accessing related fields, preventing a potential panic. This indicates a straightforward fix that does not impact the broader system architecture or require extensive refactoring. Second, the technical concepts involved are relatively basic: understanding Go's pointer semantics, conditional checks, and error handling. No advanced algorithms, design patterns, or domain-specific knowledge beyond familiarity with Tekton Pipelines' workspace validation logic are required. Third, while the problem involves a specific edge case (optional workspaces in a PipelineRun), the fix does not introduce complex error handling or require deep analysis of multiple edge cases. The primary challenge lies in identifying the need for the nil check, which requires a moderate understanding of the codebase's logic around task resolution. However, this is not overly complex for someone with experience in Go and debugging similar issues. Overall, the problem requires understanding some code logic and making a simple modification, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Introduce freelist interface\nbbolt features two types of freelist: \"array\" and \"hashmap\". They are currently implemented in one monolithic struct, with their differences in functions separated into two go files. \r\n\r\nTesting has been a pain so far, with many of the freelist details (i.e.releasing pages and serde to disk) leaking outside of the freelist struct. This makes reasoning about the implementations very difficult, too. Adding a new type of list is becoming increasingly cumbersome without a proper interface. \r\n\r\nQuickly pulling out the existing functions into an interface could look like this:\r\n\r\n```\r\ntype Freelist interface {\r\n\r\n\t// Init initializes this freelist with the given list of pages. \r\n        // TODO currently is \"readIDs\", maybe should be called Read(page *common.Page)\r\n\tInit(ids common.Pgids) \r\n\r\n\t// Allocate returns the starting page id of a contiguous block of a given size in number of pages.\r\n\t// If a contiguous block cannot be found then 0 is returned.\r\n\tAllocate(txid common.Txid, numPages int) common.Pgid\r\n\r\n\t// Count returns the number of free pages.\r\n\tCount() int\r\n\r\n\t// FreePageIds returns all free pages.\r\n\tFreePageIds() common.Pgids\r\n\r\n\t// MergeSpans merges the given pages into the existing freelist.\r\n\tMergeSpans(ids common.Pgids) // TODO should probably named \"Release\" or \"Free\"\r\n\r\n\t// Write writes the freelist into the given page. The freelist MUST fit into the given page.\r\n\tWrite(page *common.Page) error\r\n}\r\n```\r\n\r\nIdeally we would move this into its own internal package in order to avoid leaking any more internals in the future. HashMap and Array should be two implementation as its own struct. Some shared functions  (ie read/write) could be implemented in a common struct for now. Even though I expect those to become implementation dependent at some point, it seems quite inefficient to write low-digit uint64 integers without vint compression (or as a bitset), for example.\r\n\r\n\n", "patch": "diff --git a/bucket.go b/bucket.go\nindex 785ad9bd5..6371ace97 100644\n--- a/bucket.go\n+++ b/bucket.go\n@@ -903,7 +903,7 @@ func (b *Bucket) free() {\n \tvar tx = b.tx\n \tb.forEachPageNode(func(p *common.Page, n *node, _ int) {\n \t\tif p != nil {\n-\t\t\ttx.db.freelist.free(tx.meta.Txid(), p)\n+\t\t\ttx.db.freelist.Free(tx.meta.Txid(), p)\n \t\t} else {\n \t\t\tn.free()\n \t\t}\ndiff --git a/cmd/bbolt/command_version.go b/cmd/bbolt/command_version.go\nindex 73019c798..39d756bd9 100644\n--- a/cmd/bbolt/command_version.go\n+++ b/cmd/bbolt/command_version.go\n@@ -5,6 +5,7 @@ import (\n \t\"runtime\"\n \n \t\"github.com/spf13/cobra\"\n+\n \t\"go.etcd.io/bbolt/version\"\n )\n \ndiff --git a/db.go b/db.go\nindex 236698212..349f187ae 100644\n--- a/db.go\n+++ b/db.go\n@@ -12,6 +12,7 @@ import (\n \n \tberrors \"go.etcd.io/bbolt/errors\"\n \t\"go.etcd.io/bbolt/internal/common\"\n+\tfl \"go.etcd.io/bbolt/internal/freelist\"\n )\n \n // The time elapsed between consecutive file locking attempts.\n@@ -133,7 +134,7 @@ type DB struct {\n \trwtx     *Tx\n \ttxs      []*Tx\n \n-\tfreelist     *freelist\n+\tfreelist     fl.Interface\n \tfreelistLoad sync.Once\n \n \tpagePool sync.Pool\n@@ -418,12 +419,12 @@ func (db *DB) loadFreelist() {\n \t\tdb.freelist = newFreelist(db.FreelistType)\n \t\tif !db.hasSyncedFreelist() {\n \t\t\t// Reconstruct free list by scanning the DB.\n-\t\t\tdb.freelist.readIDs(db.freepages())\n+\t\t\tdb.freelist.Init(db.freepages())\n \t\t} else {\n \t\t\t// Read free list from freelist page.\n-\t\t\tdb.freelist.read(db.page(db.meta().Freelist()))\n+\t\t\tdb.freelist.Read(db.page(db.meta().Freelist()))\n \t\t}\n-\t\tdb.stats.FreePageN = db.freelist.free_count()\n+\t\tdb.stats.FreePageN = db.freelist.FreeCount()\n \t})\n }\n \n@@ -797,7 +798,7 @@ func (db *DB) beginTx() (*Tx, error) {\n \tdb.txs = append(db.txs, t)\n \tn := len(db.txs)\n \tif db.freelist != nil {\n-\t\tdb.freelist.addReadonlyTXID(t.meta.Txid())\n+\t\tdb.freelist.AddReadonlyTXID(t.meta.Txid())\n \t}\n \n \t// Unlock the meta pages.\n@@ -843,7 +844,7 @@ func (db *DB) beginRWTx() (*Tx, error) {\n \tt := &Tx{writable: true}\n \tt.init(db)\n \tdb.rwtx = t\n-\tdb.freelist.freePages()\n+\tdb.freelist.ReleasePendingPages()\n \treturn t, nil\n }\n \n@@ -867,7 +868,7 @@ func (db *DB) removeTx(tx *Tx) {\n \t}\n \tn := len(db.txs)\n \tif db.freelist != nil {\n-\t\tdb.freelist.removeReadonlyTXID(tx.meta.Txid())\n+\t\tdb.freelist.RemoveReadonlyTXID(tx.meta.Txid())\n \t}\n \n \t// Unlock the meta pages.\n@@ -1155,7 +1156,7 @@ func (db *DB) allocate(txid common.Txid, count int) (*common.Page, error) {\n \tp.SetOverflow(uint32(count - 1))\n \n \t// Use pages from the freelist if they are available.\n-\tp.SetId(db.freelist.allocate(txid, count))\n+\tp.SetId(db.freelist.Allocate(txid, count))\n \tif p.Id() != 0 {\n \t\treturn p, nil\n \t}\n@@ -1261,6 +1262,13 @@ func (db *DB) freepages() []common.Pgid {\n \treturn fids\n }\n \n+func newFreelist(freelistType FreelistType) fl.Interface {\n+\tif freelistType == FreelistMapType {\n+\t\treturn fl.NewHashMapFreelist()\n+\t}\n+\treturn fl.NewArrayFreelist()\n+}\n+\n // Options represents the options that can be set when opening a database.\n type Options struct {\n \t// Timeout is the amount of time to wait to obtain a file lock.\ndiff --git a/freelist.go b/freelist.go\ndeleted file mode 100644\nindex 49b6c200b..000000000\n--- a/freelist.go\n+++ /dev/null\n@@ -1,371 +0,0 @@\n-package bbolt\n-\n-import (\n-\t\"fmt\"\n-\t\"math\"\n-\t\"sort\"\n-\t\"unsafe\"\n-\n-\t\"go.etcd.io/bbolt/internal/common\"\n-)\n-\n-// txPending holds a list of pgids and corresponding allocation txns\n-// that are pending to be freed.\n-type txPending struct {\n-\tids              []common.Pgid\n-\talloctx          []common.Txid // txids allocating the ids\n-\tlastReleaseBegin common.Txid   // beginning txid of last matching releaseRange\n-}\n-\n-// pidSet holds the set of starting pgids which have the same span size\n-type pidSet map[common.Pgid]struct{}\n-\n-// freelist represents a list of all pages that are available for allocation.\n-// It also tracks pages that have been freed but are still in use by open transactions.\n-type freelist struct {\n-\tfreelistType   FreelistType                              // freelist type\n-\tids            []common.Pgid                             // all free and available free page ids.\n-\treadonlyTXIDs  []common.Txid                             // all readonly transaction IDs.\n-\tallocs         map[common.Pgid]common.Txid               // mapping of Txid that allocated a pgid.\n-\tpending        map[common.Txid]*txPending                // mapping of soon-to-be free page ids by tx.\n-\tcache          map[common.Pgid]struct{}                  // fast lookup of all free and pending page ids.\n-\tfreemaps       map[uint64]pidSet                         // key is the size of continuous pages(span), value is a set which contains the starting pgids of same size\n-\tforwardMap     map[common.Pgid]uint64                    // key is start pgid, value is its span size\n-\tbackwardMap    map[common.Pgid]uint64                    // key is end pgid, value is its span size\n-\tfreePagesCount uint64                                    // count of free pages(hashmap version)\n-\tallocate       func(txid common.Txid, n int) common.Pgid // the freelist allocate func\n-\tfree_count     func() int                                // the function which gives you free page number\n-\tmergeSpans     func(ids common.Pgids)                    // the mergeSpan func\n-\tgetFreePageIDs func() []common.Pgid                      // get free pgids func\n-\treadIDs        func(pgids []common.Pgid)                 // readIDs func reads list of pages and init the freelist\n-}\n-\n-// newFreelist returns an empty, initialized freelist.\n-func newFreelist(freelistType FreelistType) *freelist {\n-\tf := &freelist{\n-\t\tfreelistType: freelistType,\n-\t\tallocs:       make(map[common.Pgid]common.Txid),\n-\t\tpending:      make(map[common.Txid]*txPending),\n-\t\tcache:        make(map[common.Pgid]struct{}),\n-\t\tfreemaps:     make(map[uint64]pidSet),\n-\t\tforwardMap:   make(map[common.Pgid]uint64),\n-\t\tbackwardMap:  make(map[common.Pgid]uint64),\n-\t}\n-\n-\tif freelistType == FreelistMapType {\n-\t\tf.allocate = f.hashmapAllocate\n-\t\tf.free_count = f.hashmapFreeCount\n-\t\tf.mergeSpans = f.hashmapMergeSpans\n-\t\tf.getFreePageIDs = f.hashmapGetFreePageIDs\n-\t\tf.readIDs = f.hashmapReadIDs\n-\t} else {\n-\t\tf.allocate = f.arrayAllocate\n-\t\tf.free_count = f.arrayFreeCount\n-\t\tf.mergeSpans = f.arrayMergeSpans\n-\t\tf.getFreePageIDs = f.arrayGetFreePageIDs\n-\t\tf.readIDs = f.arrayReadIDs\n-\t}\n-\n-\treturn f\n-}\n-\n-// size returns the size of the page after serialization.\n-func (f *freelist) size() int {\n-\tn := f.count()\n-\tif n >= 0xFFFF {\n-\t\t// The first element will be used to store the count. See freelist.write.\n-\t\tn++\n-\t}\n-\treturn int(common.PageHeaderSize) + (int(unsafe.Sizeof(common.Pgid(0))) * n)\n-}\n-\n-// count returns count of pages on the freelist\n-func (f *freelist) count() int {\n-\treturn f.free_count() + f.pending_count()\n-}\n-\n-// pending_count returns count of pending pages\n-func (f *freelist) pending_count() int {\n-\tvar count int\n-\tfor _, txp := range f.pending {\n-\t\tcount += len(txp.ids)\n-\t}\n-\treturn count\n-}\n-\n-// copyall copies a list of all free ids and all pending ids in one sorted list.\n-// f.count returns the minimum length required for dst.\n-func (f *freelist) copyall(dst []common.Pgid) {\n-\tm := make(common.Pgids, 0, f.pending_count())\n-\tfor _, txp := range f.pending {\n-\t\tm = append(m, txp.ids...)\n-\t}\n-\tsort.Sort(m)\n-\tcommon.Mergepgids(dst, f.getFreePageIDs(), m)\n-}\n-\n-// free releases a page and its overflow for a given transaction id.\n-// If the page is already free then a panic will occur.\n-func (f *freelist) free(txid common.Txid, p *common.Page) {\n-\tif p.Id() <= 1 {\n-\t\tpanic(fmt.Sprintf(\"cannot free page 0 or 1: %d\", p.Id()))\n-\t}\n-\n-\t// Free page and all its overflow pages.\n-\ttxp := f.pending[txid]\n-\tif txp == nil {\n-\t\ttxp = &txPending{}\n-\t\tf.pending[txid] = txp\n-\t}\n-\tallocTxid, ok := f.allocs[p.Id()]\n-\tif ok {\n-\t\tdelete(f.allocs, p.Id())\n-\t} else if p.IsFreelistPage() {\n-\t\t// Freelist is always allocated by prior tx.\n-\t\tallocTxid = txid - 1\n-\t}\n-\n-\tfor id := p.Id(); id <= p.Id()+common.Pgid(p.Overflow()); id++ {\n-\t\t// Verify that page is not already free.\n-\t\tif _, ok := f.cache[id]; ok {\n-\t\t\tpanic(fmt.Sprintf(\"page %d already freed\", id))\n-\t\t}\n-\t\t// Add to the freelist and cache.\n-\t\ttxp.ids = append(txp.ids, id)\n-\t\ttxp.alloctx = append(txp.alloctx, allocTxid)\n-\t\tf.cache[id] = struct{}{}\n-\t}\n-}\n-\n-// release moves all page ids for a transaction id (or older) to the freelist.\n-func (f *freelist) release(txid common.Txid) {\n-\tm := make(common.Pgids, 0)\n-\tfor tid, txp := range f.pending {\n-\t\tif tid <= txid {\n-\t\t\t// Move transaction's pending pages to the available freelist.\n-\t\t\t// Don't remove from the cache since the page is still free.\n-\t\t\tm = append(m, txp.ids...)\n-\t\t\tdelete(f.pending, tid)\n-\t\t}\n-\t}\n-\tf.mergeSpans(m)\n-}\n-\n-// releaseRange moves pending pages allocated within an extent [begin,end] to the free list.\n-func (f *freelist) releaseRange(begin, end common.Txid) {\n-\tif begin > end {\n-\t\treturn\n-\t}\n-\tvar m common.Pgids\n-\tfor tid, txp := range f.pending {\n-\t\tif tid < begin || tid > end {\n-\t\t\tcontinue\n-\t\t}\n-\t\t// Don't recompute freed pages if ranges haven't updated.\n-\t\tif txp.lastReleaseBegin == begin {\n-\t\t\tcontinue\n-\t\t}\n-\t\tfor i := 0; i < len(txp.ids); i++ {\n-\t\t\tif atx := txp.alloctx[i]; atx < begin || atx > end {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tm = append(m, txp.ids[i])\n-\t\t\ttxp.ids[i] = txp.ids[len(txp.ids)-1]\n-\t\t\ttxp.ids = txp.ids[:len(txp.ids)-1]\n-\t\t\ttxp.alloctx[i] = txp.alloctx[len(txp.alloctx)-1]\n-\t\t\ttxp.alloctx = txp.alloctx[:len(txp.alloctx)-1]\n-\t\t\ti--\n-\t\t}\n-\t\ttxp.lastReleaseBegin = begin\n-\t\tif len(txp.ids) == 0 {\n-\t\t\tdelete(f.pending, tid)\n-\t\t}\n-\t}\n-\tf.mergeSpans(m)\n-}\n-\n-// rollback removes the pages from a given pending tx.\n-func (f *freelist) rollback(txid common.Txid) {\n-\t// Remove page ids from cache.\n-\ttxp := f.pending[txid]\n-\tif txp == nil {\n-\t\treturn\n-\t}\n-\tvar m common.Pgids\n-\tfor i, pgid := range txp.ids {\n-\t\tdelete(f.cache, pgid)\n-\t\ttx := txp.alloctx[i]\n-\t\tif tx == 0 {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif tx != txid {\n-\t\t\t// Pending free aborted; restore page back to alloc list.\n-\t\t\tf.allocs[pgid] = tx\n-\t\t} else {\n-\t\t\t// Freed page was allocated by this txn; OK to throw away.\n-\t\t\tm = append(m, pgid)\n-\t\t}\n-\t}\n-\t// Remove pages from pending list and mark as free if allocated by txid.\n-\tdelete(f.pending, txid)\n-\tf.mergeSpans(m)\n-}\n-\n-// freed returns whether a given page is in the free list.\n-func (f *freelist) freed(pgId common.Pgid) bool {\n-\t_, ok := f.cache[pgId]\n-\treturn ok\n-}\n-\n-// read initializes the freelist from a freelist page.\n-func (f *freelist) read(p *common.Page) {\n-\tif !p.IsFreelistPage() {\n-\t\tpanic(fmt.Sprintf(\"invalid freelist page: %d, page type is %s\", p.Id(), p.Typ()))\n-\t}\n-\n-\tids := p.FreelistPageIds()\n-\n-\t// Copy the list of page ids from the freelist.\n-\tif len(ids) == 0 {\n-\t\tf.ids = nil\n-\t} else {\n-\t\t// copy the ids, so we don't modify on the freelist page directly\n-\t\tidsCopy := make([]common.Pgid, len(ids))\n-\t\tcopy(idsCopy, ids)\n-\t\t// Make sure they're sorted.\n-\t\tsort.Sort(common.Pgids(idsCopy))\n-\n-\t\tf.readIDs(idsCopy)\n-\t}\n-}\n-\n-// write writes the page ids onto a freelist page. All free and pending ids are\n-// saved to disk since in the event of a program crash, all pending ids will\n-// become free.\n-func (f *freelist) write(p *common.Page) error {\n-\t// Combine the old free pgids and pgids waiting on an open transaction.\n-\n-\t// Update the header flag.\n-\tp.SetFlags(common.FreelistPageFlag)\n-\n-\t// The page.count can only hold up to 64k elements so if we overflow that\n-\t// number then we handle it by putting the size in the first element.\n-\tl := f.count()\n-\tif l == 0 {\n-\t\tp.SetCount(uint16(l))\n-\t} else if l < 0xFFFF {\n-\t\tp.SetCount(uint16(l))\n-\t\tdata := common.UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p))\n-\t\tids := unsafe.Slice((*common.Pgid)(data), l)\n-\t\tf.copyall(ids)\n-\t} else {\n-\t\tp.SetCount(0xFFFF)\n-\t\tdata := common.UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p))\n-\t\tids := unsafe.Slice((*common.Pgid)(data), l+1)\n-\t\tids[0] = common.Pgid(l)\n-\t\tf.copyall(ids[1:])\n-\t}\n-\n-\treturn nil\n-}\n-\n-// reload reads the freelist from a page and filters out pending items.\n-func (f *freelist) reload(p *common.Page) {\n-\tf.read(p)\n-\n-\t// Build a cache of only pending pages.\n-\tpcache := make(map[common.Pgid]bool)\n-\tfor _, txp := range f.pending {\n-\t\tfor _, pendingID := range txp.ids {\n-\t\t\tpcache[pendingID] = true\n-\t\t}\n-\t}\n-\n-\t// Check each page in the freelist and build a new available freelist\n-\t// with any pages not in the pending lists.\n-\tvar a []common.Pgid\n-\tfor _, id := range f.getFreePageIDs() {\n-\t\tif !pcache[id] {\n-\t\t\ta = append(a, id)\n-\t\t}\n-\t}\n-\n-\tf.readIDs(a)\n-}\n-\n-// noSyncReload reads the freelist from Pgids and filters out pending items.\n-func (f *freelist) noSyncReload(Pgids []common.Pgid) {\n-\t// Build a cache of only pending pages.\n-\tpcache := make(map[common.Pgid]bool)\n-\tfor _, txp := range f.pending {\n-\t\tfor _, pendingID := range txp.ids {\n-\t\t\tpcache[pendingID] = true\n-\t\t}\n-\t}\n-\n-\t// Check each page in the freelist and build a new available freelist\n-\t// with any pages not in the pending lists.\n-\tvar a []common.Pgid\n-\tfor _, id := range Pgids {\n-\t\tif !pcache[id] {\n-\t\t\ta = append(a, id)\n-\t\t}\n-\t}\n-\n-\tf.readIDs(a)\n-}\n-\n-// reindex rebuilds the free cache based on available and pending free lists.\n-func (f *freelist) reindex() {\n-\tids := f.getFreePageIDs()\n-\tf.cache = make(map[common.Pgid]struct{}, len(ids))\n-\tfor _, id := range ids {\n-\t\tf.cache[id] = struct{}{}\n-\t}\n-\tfor _, txp := range f.pending {\n-\t\tfor _, pendingID := range txp.ids {\n-\t\t\tf.cache[pendingID] = struct{}{}\n-\t\t}\n-\t}\n-}\n-\n-func (f *freelist) addReadonlyTXID(tid common.Txid) {\n-\tf.readonlyTXIDs = append(f.readonlyTXIDs, tid)\n-}\n-\n-func (f *freelist) removeReadonlyTXID(tid common.Txid) {\n-\tfor i := range f.readonlyTXIDs {\n-\t\tif f.readonlyTXIDs[i] == tid {\n-\t\t\tlast := len(f.readonlyTXIDs) - 1\n-\t\t\tf.readonlyTXIDs[i] = f.readonlyTXIDs[last]\n-\t\t\tf.readonlyTXIDs = f.readonlyTXIDs[:last]\n-\t\t\tbreak\n-\t\t}\n-\t}\n-}\n-\n-type txIDx []common.Txid\n-\n-func (t txIDx) Len() int           { return len(t) }\n-func (t txIDx) Swap(i, j int)      { t[i], t[j] = t[j], t[i] }\n-func (t txIDx) Less(i, j int) bool { return t[i] < t[j] }\n-\n-// freePages releases any pages associated with closed read-only transactions.\n-func (f *freelist) freePages() {\n-\t// Free all pending pages prior to the earliest open transaction.\n-\tsort.Sort(txIDx(f.readonlyTXIDs))\n-\tminid := common.Txid(math.MaxUint64)\n-\tif len(f.readonlyTXIDs) > 0 {\n-\t\tminid = f.readonlyTXIDs[0]\n-\t}\n-\tif minid > 0 {\n-\t\tf.release(minid - 1)\n-\t}\n-\t// Release unused txid extents.\n-\tfor _, tid := range f.readonlyTXIDs {\n-\t\tf.releaseRange(minid, tid-1)\n-\t\tminid = tid + 1\n-\t}\n-\tf.releaseRange(minid, common.Txid(math.MaxUint64))\n-\t// Any page both allocated and freed in an extent is safe to release.\n-}\ndiff --git a/freelist_array.go b/internal/freelist/array.go\nsimilarity index 74%\nrename from freelist_array.go\nrename to internal/freelist/array.go\nindex 2f0a7e4aa..93ccc5edc 100644\n--- a/freelist_array.go\n+++ b/internal/freelist/array.go\n@@ -1,4 +1,4 @@\n-package bbolt\n+package freelist\n \n import (\n \t\"fmt\"\n@@ -7,14 +7,18 @@ import (\n \t\"go.etcd.io/bbolt/internal/common\"\n )\n \n-// arrayFreeCount returns count of free pages(array version)\n-func (f *freelist) arrayFreeCount() int {\n-\treturn len(f.ids)\n+type array struct {\n+\t*shared\n+\n+\tids []common.Pgid // all free and available free page ids.\n+}\n+\n+func (f *array) Init(ids common.Pgids) {\n+\tf.ids = ids\n+\tf.reindex()\n }\n \n-// arrayAllocate returns the starting page id of a contiguous list of pages of a given size.\n-// If a contiguous block cannot be found then 0 is returned.\n-func (f *freelist) arrayAllocate(txid common.Txid, n int) common.Pgid {\n+func (f *array) Allocate(txid common.Txid, n int) common.Pgid {\n \tif len(f.ids) == 0 {\n \t\treturn 0\n \t}\n@@ -56,18 +60,15 @@ func (f *freelist) arrayAllocate(txid common.Txid, n int) common.Pgid {\n \treturn 0\n }\n \n-// arrayReadIDs initializes the freelist from a given list of ids.\n-func (f *freelist) arrayReadIDs(ids []common.Pgid) {\n-\tf.ids = ids\n-\tf.reindex()\n+func (f *array) FreeCount() int {\n+\treturn len(f.ids)\n }\n \n-func (f *freelist) arrayGetFreePageIDs() []common.Pgid {\n+func (f *array) freePageIds() common.Pgids {\n \treturn f.ids\n }\n \n-// arrayMergeSpans try to merge list of pages(represented by pgids) with existing spans but using array\n-func (f *freelist) arrayMergeSpans(ids common.Pgids) {\n+func (f *array) mergeSpans(ids common.Pgids) {\n \tsort.Sort(ids)\n \tcommon.Verify(func() {\n \t\tidsIdx := make(map[common.Pgid]struct{})\n@@ -96,3 +97,11 @@ func (f *freelist) arrayMergeSpans(ids common.Pgids) {\n \t})\n \tf.ids = common.Pgids(f.ids).Merge(ids)\n }\n+\n+func NewArrayFreelist() Interface {\n+\ta := &array{\n+\t\tshared: newShared(),\n+\t}\n+\ta.Interface = a\n+\treturn a\n+}\ndiff --git a/internal/freelist/freelist.go b/internal/freelist/freelist.go\nnew file mode 100644\nindex 000000000..3d77d8f94\n--- /dev/null\n+++ b/internal/freelist/freelist.go\n@@ -0,0 +1,82 @@\n+package freelist\n+\n+import (\n+\t\"go.etcd.io/bbolt/internal/common\"\n+)\n+\n+type ReadWriter interface {\n+\t// Read calls Init with the page ids stored in the given page.\n+\tRead(page *common.Page)\n+\n+\t// Write writes the freelist into the given page.\n+\tWrite(page *common.Page)\n+\n+\t// EstimatedWritePageSize returns the size of the freelist after serialization in Write.\n+\t// This should never underestimate the size.\n+\tEstimatedWritePageSize() int\n+}\n+\n+type Interface interface {\n+\tReadWriter\n+\n+\t// Init initializes this freelist with the given list of pages.\n+\tInit(ids common.Pgids)\n+\n+\t// Allocate tries to allocate the given number of contiguous pages\n+\t// from the free list pages. It returns the starting page ID if\n+\t// available; otherwise, it returns 0.\n+\tAllocate(txid common.Txid, numPages int) common.Pgid\n+\n+\t// Count returns the number of free and pending pages.\n+\tCount() int\n+\n+\t// FreeCount returns the number of free pages.\n+\tFreeCount() int\n+\n+\t// PendingCount returns the number of pending pages.\n+\tPendingCount() int\n+\n+\t// AddReadonlyTXID adds a given read-only transaction id for pending page tracking.\n+\tAddReadonlyTXID(txid common.Txid)\n+\n+\t// RemoveReadonlyTXID removes a given read-only transaction id for pending page tracking.\n+\tRemoveReadonlyTXID(txid common.Txid)\n+\n+\t// ReleasePendingPages releases any pages associated with closed read-only transactions.\n+\tReleasePendingPages()\n+\n+\t// Free releases a page and its overflow for a given transaction id.\n+\t// If the page is already free then a panic will occur.\n+\tFree(txId common.Txid, p *common.Page)\n+\n+\t// Freed returns whether a given page is in the free list.\n+\tFreed(pgId common.Pgid) bool\n+\n+\t// Rollback removes the pages from a given pending tx.\n+\tRollback(txId common.Txid)\n+\n+\t// Copyall copies a list of all free ids and all pending ids in one sorted list.\n+\t// f.count returns the minimum length required for dst.\n+\tCopyall(dst []common.Pgid)\n+\n+\t// Reload reads the freelist from a page and filters out pending items.\n+\tReload(p *common.Page)\n+\n+\t// NoSyncReload reads the freelist from Pgids and filters out pending items.\n+\tNoSyncReload(pgIds common.Pgids)\n+\n+\t// freePageIds returns the IDs of all free pages.\n+\tfreePageIds() common.Pgids\n+\n+\t// pendingPageIds returns all pending pages by transaction id.\n+\tpendingPageIds() map[common.Txid]*txPending\n+\n+\t// release moves all page ids for a transaction id (or older) to the freelist.\n+\trelease(txId common.Txid)\n+\n+\t// releaseRange moves pending pages allocated within an extent [begin,end] to the free list.\n+\treleaseRange(begin, end common.Txid)\n+\n+\t// mergeSpans is merging the given pages into the freelist\n+\tmergeSpans(ids common.Pgids)\n+}\ndiff --git a/freelist_hmap.go b/internal/freelist/hashmap.go\nsimilarity index 79%\nrename from freelist_hmap.go\nrename to internal/freelist/hashmap.go\nindex c5c09f55e..a6bad8976 100644\n--- a/freelist_hmap.go\n+++ b/internal/freelist/hashmap.go\n@@ -1,4 +1,4 @@\n-package bbolt\n+package freelist\n \n import (\n \t\"fmt\"\n@@ -8,26 +8,57 @@ import (\n \t\"go.etcd.io/bbolt/internal/common\"\n )\n \n-// hashmapFreeCount returns count of free pages(hashmap version)\n-func (f *freelist) hashmapFreeCount() int {\n-\tcommon.Verify(func() {\n-\t\texpectedFreePageCount := f.hashmapFreeCountSlow()\n-\t\tcommon.Assert(int(f.freePagesCount) == expectedFreePageCount,\n-\t\t\t\"freePagesCount (%d) is out of sync with free pages map (%d)\", f.freePagesCount, expectedFreePageCount)\n-\t})\n-\treturn int(f.freePagesCount)\n+// pidSet holds the set of starting pgids which have the same span size\n+type pidSet map[common.Pgid]struct{}\n+\n+type hashMap struct {\n+\t*shared\n+\n+\tfreePagesCount uint64                 // count of free pages(hashmap version)\n+\tfreemaps       map[uint64]pidSet      // key is the size of continuous pages(span), value is a set which contains the starting pgids of same size\n+\tforwardMap     map[common.Pgid]uint64 // key is start pgid, value is its span size\n+\tbackwardMap    map[common.Pgid]uint64 // key is end pgid, value is its span size\n }\n \n-func (f *freelist) hashmapFreeCountSlow() int {\n-\tcount := 0\n-\tfor _, size := range f.forwardMap {\n-\t\tcount += int(size)\n+func (f *hashMap) Init(pgids common.Pgids) {\n+\tif len(pgids) == 0 {\n+\t\treturn\n \t}\n-\treturn count\n+\n+\tsize := uint64(1)\n+\tstart := pgids[0]\n+\t// reset the counter when freelist init\n+\tf.freePagesCount = 0\n+\n+\tif !sort.SliceIsSorted([]common.Pgid(pgids), func(i, j int) bool { return pgids[i] < pgids[j] }) {\n+\t\tpanic(\"pgids not sorted\")\n+\t}\n+\n+\tf.freemaps = make(map[uint64]pidSet)\n+\tf.forwardMap = make(map[common.Pgid]uint64)\n+\tf.backwardMap = make(map[common.Pgid]uint64)\n+\n+\tfor i := 1; i < len(pgids); i++ {\n+\t\t// continuous page\n+\t\tif pgids[i] == pgids[i-1]+1 {\n+\t\t\tsize++\n+\t\t} else {\n+\t\t\tf.addSpan(start, size)\n+\n+\t\t\tsize = 1\n+\t\t\tstart = pgids[i]\n+\t\t}\n+\t}\n+\n+\t// init the tail\n+\tif size != 0 && start != 0 {\n+\t\tf.addSpan(start, size)\n+\t}\n+\n+\tf.reindex()\n }\n \n-// hashmapAllocate serves the same purpose as arrayAllocate, but use hashmap as backend\n-func (f *freelist) hashmapAllocate(txid common.Txid, n int) common.Pgid {\n+func (f *hashMap) Allocate(txid common.Txid, n int) common.Pgid {\n \tif n == 0 {\n \t\treturn 0\n \t}\n@@ -74,17 +105,17 @@ func (f *freelist) hashmapAllocate(txid common.Txid, n int) common.Pgid {\n \treturn 0\n }\n \n-// hashmapReadIDs reads pgids as input an initial the freelist(hashmap version)\n-func (f *freelist) hashmapReadIDs(pgids []common.Pgid) {\n-\tf.init(pgids)\n-\n-\t// Rebuild the page cache.\n-\tf.reindex()\n+func (f *hashMap) FreeCount() int {\n+\tcommon.Verify(func() {\n+\t\texpectedFreePageCount := f.hashmapFreeCountSlow()\n+\t\tcommon.Assert(int(f.freePagesCount) == expectedFreePageCount,\n+\t\t\t\"freePagesCount (%d) is out of sync with free pages map (%d)\", f.freePagesCount, expectedFreePageCount)\n+\t})\n+\treturn int(f.freePagesCount)\n }\n \n-// hashmapGetFreePageIDs returns the sorted free page ids\n-func (f *freelist) hashmapGetFreePageIDs() []common.Pgid {\n-\tcount := f.free_count()\n+func (f *hashMap) freePageIds() common.Pgids {\n+\tcount := f.FreeCount()\n \tif count == 0 {\n \t\treturn nil\n \t}\n@@ -108,8 +139,36 @@ func (f *freelist) hashmapGetFreePageIDs() []common.Pgid {\n \treturn m\n }\n \n-// hashmapMergeSpans try to merge list of pages(represented by pgids) with existing spans\n-func (f *freelist) hashmapMergeSpans(ids common.Pgids) {\n+func (f *hashMap) hashmapFreeCountSlow() int {\n+\tcount := 0\n+\tfor _, size := range f.forwardMap {\n+\t\tcount += int(size)\n+\t}\n+\treturn count\n+}\n+\n+func (f *hashMap) addSpan(start common.Pgid, size uint64) {\n+\tf.backwardMap[start-1+common.Pgid(size)] = size\n+\tf.forwardMap[start] = size\n+\tif _, ok := f.freemaps[size]; !ok {\n+\t\tf.freemaps[size] = make(map[common.Pgid]struct{})\n+\t}\n+\n+\tf.freemaps[size][start] = struct{}{}\n+\tf.freePagesCount += size\n+}\n+\n+func (f *hashMap) delSpan(start common.Pgid, size uint64) {\n+\tdelete(f.forwardMap, start)\n+\tdelete(f.backwardMap, start+common.Pgid(size-1))\n+\tdelete(f.freemaps[size], start)\n+\tif len(f.freemaps[size]) == 0 {\n+\t\tdelete(f.freemaps, size)\n+\t}\n+\tf.freePagesCount -= size\n+}\n+\n+func (f *hashMap) mergeSpans(ids common.Pgids) {\n \tcommon.Verify(func() {\n \t\tids1Freemap := f.idsFromFreemaps()\n \t\tids2Forward := f.idsFromForwardMap()\n@@ -144,7 +203,7 @@ func (f *freelist) hashmapMergeSpans(ids common.Pgids) {\n }\n \n // mergeWithExistingSpan merges pid to the existing free spans, try to merge it backward and forward\n-func (f *freelist) mergeWithExistingSpan(pid common.Pgid) {\n+func (f *hashMap) mergeWithExistingSpan(pid common.Pgid) {\n \tprev := pid - 1\n \tnext := pid + 1\n \n@@ -171,68 +230,9 @@ func (f *freelist) mergeWithExistingSpan(pid common.Pgid) {\n \tf.addSpan(newStart, newSize)\n }\n \n-func (f *freelist) addSpan(start common.Pgid, size uint64) {\n-\tf.backwardMap[start-1+common.Pgid(size)] = size\n-\tf.forwardMap[start] = size\n-\tif _, ok := f.freemaps[size]; !ok {\n-\t\tf.freemaps[size] = make(map[common.Pgid]struct{})\n-\t}\n-\n-\tf.freemaps[size][start] = struct{}{}\n-\tf.freePagesCount += size\n-}\n-\n-func (f *freelist) delSpan(start common.Pgid, size uint64) {\n-\tdelete(f.forwardMap, start)\n-\tdelete(f.backwardMap, start+common.Pgid(size-1))\n-\tdelete(f.freemaps[size], start)\n-\tif len(f.freemaps[size]) == 0 {\n-\t\tdelete(f.freemaps, size)\n-\t}\n-\tf.freePagesCount -= size\n-}\n-\n-// initial from pgids using when use hashmap version\n-// pgids must be sorted\n-func (f *freelist) init(pgids []common.Pgid) {\n-\tif len(pgids) == 0 {\n-\t\treturn\n-\t}\n-\n-\tsize := uint64(1)\n-\tstart := pgids[0]\n-\t// reset the counter when freelist init\n-\tf.freePagesCount = 0\n-\n-\tif !sort.SliceIsSorted([]common.Pgid(pgids), func(i, j int) bool { return pgids[i] < pgids[j] }) {\n-\t\tpanic(\"pgids not sorted\")\n-\t}\n-\n-\tf.freemaps = make(map[uint64]pidSet)\n-\tf.forwardMap = make(map[common.Pgid]uint64)\n-\tf.backwardMap = make(map[common.Pgid]uint64)\n-\n-\tfor i := 1; i < len(pgids); i++ {\n-\t\t// continuous page\n-\t\tif pgids[i] == pgids[i-1]+1 {\n-\t\t\tsize++\n-\t\t} else {\n-\t\t\tf.addSpan(start, size)\n-\n-\t\t\tsize = 1\n-\t\t\tstart = pgids[i]\n-\t\t}\n-\t}\n-\n-\t// init the tail\n-\tif size != 0 && start != 0 {\n-\t\tf.addSpan(start, size)\n-\t}\n-}\n-\n // idsFromFreemaps get all free page IDs from f.freemaps.\n // used by test only.\n-func (f *freelist) idsFromFreemaps() map[common.Pgid]struct{} {\n+func (f *hashMap) idsFromFreemaps() map[common.Pgid]struct{} {\n \tids := make(map[common.Pgid]struct{})\n \tfor size, idSet := range f.freemaps {\n \t\tfor start := range idSet {\n@@ -250,7 +250,7 @@ func (f *freelist) idsFromFreemaps() map[common.Pgid]struct{} {\n \n // idsFromForwardMap get all free page IDs from f.forwardMap.\n // used by test only.\n-func (f *freelist) idsFromForwardMap() map[common.Pgid]struct{} {\n+func (f *hashMap) idsFromForwardMap() map[common.Pgid]struct{} {\n \tids := make(map[common.Pgid]struct{})\n \tfor start, size := range f.forwardMap {\n \t\tfor i := 0; i < int(size); i++ {\n@@ -266,7 +266,7 @@ func (f *freelist) idsFromForwardMap() map[common.Pgid]struct{} {\n \n // idsFromBackwardMap get all free page IDs from f.backwardMap.\n // used by test only.\n-func (f *freelist) idsFromBackwardMap() map[common.Pgid]struct{} {\n+func (f *hashMap) idsFromBackwardMap() map[common.Pgid]struct{} {\n \tids := make(map[common.Pgid]struct{})\n \tfor end, size := range f.backwardMap {\n \t\tfor i := 0; i < int(size); i++ {\n@@ -279,3 +279,14 @@ func (f *freelist) idsFromBackwardMap() map[common.Pgid]struct{} {\n \t}\n \treturn ids\n }\n+\n+func NewHashMapFreelist() Interface {\n+\thm := &hashMap{\n+\t\tshared:      newShared(),\n+\t\tfreemaps:    make(map[uint64]pidSet),\n+\t\tforwardMap:  make(map[common.Pgid]uint64),\n+\t\tbackwardMap: make(map[common.Pgid]uint64),\n+\t}\n+\thm.Interface = hm\n+\treturn hm\n+}\ndiff --git a/internal/freelist/shared.go b/internal/freelist/shared.go\nnew file mode 100644\nindex 000000000..ac06309df\n--- /dev/null\n+++ b/internal/freelist/shared.go\n@@ -0,0 +1,321 @@\n+package freelist\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\t\"sort\"\n+\t\"unsafe\"\n+\n+\t\"go.etcd.io/bbolt/internal/common\"\n+)\n+\n+type txPending struct {\n+\tids              []common.Pgid\n+\talloctx          []common.Txid // txids allocating the ids\n+\tlastReleaseBegin common.Txid   // beginning txid of last matching releaseRange\n+}\n+\n+type shared struct {\n+\tInterface\n+\n+\treadonlyTXIDs []common.Txid               // all readonly transaction IDs.\n+\tallocs        map[common.Pgid]common.Txid // mapping of Txid that allocated a pgid.\n+\tcache         map[common.Pgid]struct{}    // fast lookup of all free and pending page ids.\n+\tpending       map[common.Txid]*txPending  // mapping of soon-to-be free page ids by tx.\n+}\n+\n+func newShared() *shared {\n+\treturn &shared{\n+\t\tpending: make(map[common.Txid]*txPending),\n+\t\tallocs:  make(map[common.Pgid]common.Txid),\n+\t\tcache:   make(map[common.Pgid]struct{}),\n+\t}\n+}\n+\n+func (t *shared) pendingPageIds() map[common.Txid]*txPending {\n+\treturn t.pending\n+}\n+\n+func (t *shared) PendingCount() int {\n+\tvar count int\n+\tfor _, txp := range t.pending {\n+\t\tcount += len(txp.ids)\n+\t}\n+\treturn count\n+}\n+\n+func (t *shared) Count() int {\n+\treturn t.FreeCount() + t.PendingCount()\n+}\n+\n+func (t *shared) Freed(pgId common.Pgid) bool {\n+\t_, ok := t.cache[pgId]\n+\treturn ok\n+}\n+\n+func (t *shared) Free(txid common.Txid, p *common.Page) {\n+\tif p.Id() <= 1 {\n+\t\tpanic(fmt.Sprintf(\"cannot free page 0 or 1: %d\", p.Id()))\n+\t}\n+\n+\t// Free page and all its overflow pages.\n+\ttxp := t.pending[txid]\n+\tif txp == nil {\n+\t\ttxp = &txPending{}\n+\t\tt.pending[txid] = txp\n+\t}\n+\tallocTxid, ok := t.allocs[p.Id()]\n+\tif ok {\n+\t\tdelete(t.allocs, p.Id())\n+\t} else if p.IsFreelistPage() {\n+\t\t// Freelist is always allocated by prior tx.\n+\t\tallocTxid = txid - 1\n+\t}\n+\n+\tfor id := p.Id(); id <= p.Id()+common.Pgid(p.Overflow()); id++ {\n+\t\t// Verify that page is not already free.\n+\t\tif _, ok := t.cache[id]; ok {\n+\t\t\tpanic(fmt.Sprintf(\"page %d already freed\", id))\n+\t\t}\n+\t\t// Add to the freelist and cache.\n+\t\ttxp.ids = append(txp.ids, id)\n+\t\ttxp.alloctx = append(txp.alloctx, allocTxid)\n+\t\tt.cache[id] = struct{}{}\n+\t}\n+}\n+\n+func (t *shared) Rollback(txid common.Txid) {\n+\t// Remove page ids from cache.\n+\ttxp := t.pending[txid]\n+\tif txp == nil {\n+\t\treturn\n+\t}\n+\tvar m common.Pgids\n+\tfor i, pgid := range txp.ids {\n+\t\tdelete(t.cache, pgid)\n+\t\ttx := txp.alloctx[i]\n+\t\tif tx == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif tx != txid {\n+\t\t\t// Pending free aborted; restore page back to alloc list.\n+\t\t\tt.allocs[pgid] = tx\n+\t\t} else {\n+\t\t\t// Freed page was allocated by this txn; OK to throw away.\n+\t\t\tm = append(m, pgid)\n+\t\t}\n+\t}\n+\t// Remove pages from pending list and mark as free if allocated by txid.\n+\tdelete(t.pending, txid)\n+\tt.mergeSpans(m)\n+}\n+\n+func (t *shared) AddReadonlyTXID(tid common.Txid) {\n+\tt.readonlyTXIDs = append(t.readonlyTXIDs, tid)\n+}\n+\n+func (t *shared) RemoveReadonlyTXID(tid common.Txid) {\n+\tfor i := range t.readonlyTXIDs {\n+\t\tif t.readonlyTXIDs[i] == tid {\n+\t\t\tlast := len(t.readonlyTXIDs) - 1\n+\t\t\tt.readonlyTXIDs[i] = t.readonlyTXIDs[last]\n+\t\t\tt.readonlyTXIDs = t.readonlyTXIDs[:last]\n+\t\t\tbreak\n+\t\t}\n+\t}\n+}\n+\n+type txIDx []common.Txid\n+\n+func (t txIDx) Len() int           { return len(t) }\n+func (t txIDx) Swap(i, j int)      { t[i], t[j] = t[j], t[i] }\n+func (t txIDx) Less(i, j int) bool { return t[i] < t[j] }\n+\n+func (t *shared) ReleasePendingPages() {\n+\t// Free all pending pages prior to the earliest open transaction.\n+\tsort.Sort(txIDx(t.readonlyTXIDs))\n+\tminid := common.Txid(math.MaxUint64)\n+\tif len(t.readonlyTXIDs) > 0 {\n+\t\tminid = t.readonlyTXIDs[0]\n+\t}\n+\tif minid > 0 {\n+\t\tt.release(minid - 1)\n+\t}\n+\t// Release unused txid extents.\n+\tfor _, tid := range t.readonlyTXIDs {\n+\t\tt.releaseRange(minid, tid-1)\n+\t\tminid = tid + 1\n+\t}\n+\tt.releaseRange(minid, common.Txid(math.MaxUint64))\n+\t// Any page both allocated and freed in an extent is safe to release.\n+}\n+\n+func (t *shared) release(txid common.Txid) {\n+\tm := make(common.Pgids, 0)\n+\tfor tid, txp := range t.pending {\n+\t\tif tid <= txid {\n+\t\t\t// Move transaction's pending pages to the available freelist.\n+\t\t\t// Don't remove from the cache since the page is still free.\n+\t\t\tm = append(m, txp.ids...)\n+\t\t\tdelete(t.pending, tid)\n+\t\t}\n+\t}\n+\tt.mergeSpans(m)\n+}\n+\n+func (t *shared) releaseRange(begin, end common.Txid) {\n+\tif begin > end {\n+\t\treturn\n+\t}\n+\tvar m common.Pgids\n+\tfor tid, txp := range t.pending {\n+\t\tif tid < begin || tid > end {\n+\t\t\tcontinue\n+\t\t}\n+\t\t// Don't recompute freed pages if ranges haven't updated.\n+\t\tif txp.lastReleaseBegin == begin {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor i := 0; i < len(txp.ids); i++ {\n+\t\t\tif atx := txp.alloctx[i]; atx < begin || atx > end {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tm = append(m, txp.ids[i])\n+\t\t\ttxp.ids[i] = txp.ids[len(txp.ids)-1]\n+\t\t\ttxp.ids = txp.ids[:len(txp.ids)-1]\n+\t\t\ttxp.alloctx[i] = txp.alloctx[len(txp.alloctx)-1]\n+\t\t\ttxp.alloctx = txp.alloctx[:len(txp.alloctx)-1]\n+\t\t\ti--\n+\t\t}\n+\t\ttxp.lastReleaseBegin = begin\n+\t\tif len(txp.ids) == 0 {\n+\t\t\tdelete(t.pending, tid)\n+\t\t}\n+\t}\n+\tt.mergeSpans(m)\n+}\n+\n+// Copyall copies a list of all free ids and all pending ids in one sorted list.\n+// f.count returns the minimum length required for dst.\n+func (t *shared) Copyall(dst []common.Pgid) {\n+\tm := make(common.Pgids, 0, t.PendingCount())\n+\tfor _, txp := range t.pendingPageIds() {\n+\t\tm = append(m, txp.ids...)\n+\t}\n+\tsort.Sort(m)\n+\tcommon.Mergepgids(dst, t.freePageIds(), m)\n+}\n+\n+func (t *shared) Reload(p *common.Page) {\n+\tt.Read(p)\n+\n+\t// Build a cache of only pending pages.\n+\tpcache := make(map[common.Pgid]bool)\n+\tfor _, txp := range t.pending {\n+\t\tfor _, pendingID := range txp.ids {\n+\t\t\tpcache[pendingID] = true\n+\t\t}\n+\t}\n+\n+\t// Check each page in the freelist and build a new available freelist\n+\t// with any pages not in the pending lists.\n+\tvar a []common.Pgid\n+\tfor _, id := range t.freePageIds() {\n+\t\tif !pcache[id] {\n+\t\t\ta = append(a, id)\n+\t\t}\n+\t}\n+\n+\tt.Init(a)\n+}\n+\n+func (t *shared) NoSyncReload(pgIds common.Pgids) {\n+\t// Build a cache of only pending pages.\n+\tpcache := make(map[common.Pgid]bool)\n+\tfor _, txp := range t.pending {\n+\t\tfor _, pendingID := range txp.ids {\n+\t\t\tpcache[pendingID] = true\n+\t\t}\n+\t}\n+\n+\t// Check each page in the freelist and build a new available freelist\n+\t// with any pages not in the pending lists.\n+\tvar a []common.Pgid\n+\tfor _, id := range pgIds {\n+\t\tif !pcache[id] {\n+\t\t\ta = append(a, id)\n+\t\t}\n+\t}\n+\n+\tt.Init(a)\n+}\n+\n+// reindex rebuilds the free cache based on available and pending free lists.\n+func (t *shared) reindex() {\n+\tfree := t.freePageIds()\n+\tpending := t.pendingPageIds()\n+\tt.cache = make(map[common.Pgid]struct{}, len(free))\n+\tfor _, id := range free {\n+\t\tt.cache[id] = struct{}{}\n+\t}\n+\tfor _, txp := range pending {\n+\t\tfor _, pendingID := range txp.ids {\n+\t\t\tt.cache[pendingID] = struct{}{}\n+\t\t}\n+\t}\n+}\n+\n+func (t *shared) Read(p *common.Page) {\n+\tif !p.IsFreelistPage() {\n+\t\tpanic(fmt.Sprintf(\"invalid freelist page: %d, page type is %s\", p.Id(), p.Typ()))\n+\t}\n+\n+\tids := p.FreelistPageIds()\n+\n+\t// Copy the list of page ids from the freelist.\n+\tif len(ids) == 0 {\n+\t\tt.Init(nil)\n+\t} else {\n+\t\t// copy the ids, so we don't modify on the freelist page directly\n+\t\tidsCopy := make([]common.Pgid, len(ids))\n+\t\tcopy(idsCopy, ids)\n+\t\t// Make sure they're sorted.\n+\t\tsort.Sort(common.Pgids(idsCopy))\n+\n+\t\tt.Init(idsCopy)\n+\t}\n+}\n+\n+func (t *shared) EstimatedWritePageSize() int {\n+\tn := t.Count()\n+\tif n >= 0xFFFF {\n+\t\t// The first element will be used to store the count. See freelist.write.\n+\t\tn++\n+\t}\n+\treturn int(common.PageHeaderSize) + (int(unsafe.Sizeof(common.Pgid(0))) * n)\n+}\n+\n+func (t *shared) Write(p *common.Page) {\n+\t// Combine the old free pgids and pgids waiting on an open transaction.\n+\n+\t// Update the header flag.\n+\tp.SetFlags(common.FreelistPageFlag)\n+\n+\t// The page.count can only hold up to 64k elements so if we overflow that\n+\t// number then we handle it by putting the size in the first element.\n+\tl := t.Count()\n+\tif l == 0 {\n+\t\tp.SetCount(uint16(l))\n+\t} else if l < 0xFFFF {\n+\t\tp.SetCount(uint16(l))\n+\t\tdata := common.UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p))\n+\t\tids := unsafe.Slice((*common.Pgid)(data), l)\n+\t\tt.Copyall(ids)\n+\t} else {\n+\t\tp.SetCount(0xFFFF)\n+\t\tdata := common.UnsafeAdd(unsafe.Pointer(p), unsafe.Sizeof(*p))\n+\t\tids := unsafe.Slice((*common.Pgid)(data), l+1)\n+\t\tids[0] = common.Pgid(l)\n+\t\tt.Copyall(ids[1:])\n+\t}\n+}\ndiff --git a/node.go b/node.go\nindex fe67c3c89..022b1001e 100644\n--- a/node.go\n+++ b/node.go\n@@ -316,7 +316,7 @@ func (n *node) spill() error {\n \tfor _, node := range nodes {\n \t\t// Add node's page to the freelist if it's not new.\n \t\tif node.pgid > 0 {\n-\t\t\ttx.db.freelist.free(tx.meta.Txid(), tx.page(node.pgid))\n+\t\t\ttx.db.freelist.Free(tx.meta.Txid(), tx.page(node.pgid))\n \t\t\tnode.pgid = 0\n \t\t}\n \n@@ -493,7 +493,7 @@ func (n *node) dereference() {\n // free adds the node's underlying page to the freelist.\n func (n *node) free() {\n \tif n.pgid != 0 {\n-\t\tn.bucket.tx.db.freelist.free(n.bucket.tx.meta.Txid(), n.bucket.tx.page(n.pgid))\n+\t\tn.bucket.tx.db.freelist.Free(n.bucket.tx.meta.Txid(), n.bucket.tx.page(n.pgid))\n \t\tn.pgid = 0\n \t}\n }\ndiff --git a/tx.go b/tx.go\nindex 011e2c382..e03db9154 100644\n--- a/tx.go\n+++ b/tx.go\n@@ -213,7 +213,7 @@ func (tx *Tx) Commit() (err error) {\n \n \t// Free the old freelist because commit writes out a fresh freelist.\n \tif tx.meta.Freelist() != common.PgidNoFreelist {\n-\t\ttx.db.freelist.free(tx.meta.Txid(), tx.db.page(tx.meta.Freelist()))\n+\t\ttx.db.freelist.Free(tx.meta.Txid(), tx.db.page(tx.meta.Freelist()))\n \t}\n \n \tif !tx.db.NoFreelistSync {\n@@ -285,15 +285,13 @@ func (tx *Tx) Commit() (err error) {\n func (tx *Tx) commitFreelist() error {\n \t// Allocate new pages for the new free list. This will overestimate\n \t// the size of the freelist but not underestimate the size (which would be bad).\n-\tp, err := tx.allocate((tx.db.freelist.size() / tx.db.pageSize) + 1)\n+\tp, err := tx.allocate((tx.db.freelist.EstimatedWritePageSize() / tx.db.pageSize) + 1)\n \tif err != nil {\n \t\ttx.rollback()\n \t\treturn err\n \t}\n-\tif err := tx.db.freelist.write(p); err != nil {\n-\t\ttx.rollback()\n-\t\treturn err\n-\t}\n+\n+\ttx.db.freelist.Write(p)\n \ttx.meta.SetFreelist(p.Id())\n \n \treturn nil\n@@ -316,7 +314,7 @@ func (tx *Tx) nonPhysicalRollback() {\n \t\treturn\n \t}\n \tif tx.writable {\n-\t\ttx.db.freelist.rollback(tx.meta.Txid())\n+\t\ttx.db.freelist.Rollback(tx.meta.Txid())\n \t}\n \ttx.close()\n }\n@@ -327,17 +325,17 @@ func (tx *Tx) rollback() {\n \t\treturn\n \t}\n \tif tx.writable {\n-\t\ttx.db.freelist.rollback(tx.meta.Txid())\n+\t\ttx.db.freelist.Rollback(tx.meta.Txid())\n \t\t// When mmap fails, the `data`, `dataref` and `datasz` may be reset to\n \t\t// zero values, and there is no way to reload free page IDs in this case.\n \t\tif tx.db.data != nil {\n \t\t\tif !tx.db.hasSyncedFreelist() {\n \t\t\t\t// Reconstruct free page list by scanning the DB to get the whole free page list.\n-\t\t\t\t// Note: scaning the whole db is heavy if your db size is large in NoSyncFreeList mode.\n-\t\t\t\ttx.db.freelist.noSyncReload(tx.db.freepages())\n+\t\t\t\t// Note: scanning the whole db is heavy if your db size is large in NoSyncFreeList mode.\n+\t\t\t\ttx.db.freelist.NoSyncReload(tx.db.freepages())\n \t\t\t} else {\n \t\t\t\t// Read free page list from freelist page.\n-\t\t\t\ttx.db.freelist.reload(tx.db.page(tx.db.meta().Freelist()))\n+\t\t\t\ttx.db.freelist.Reload(tx.db.page(tx.db.meta().Freelist()))\n \t\t\t}\n \t\t}\n \t}\n@@ -350,9 +348,9 @@ func (tx *Tx) close() {\n \t}\n \tif tx.writable {\n \t\t// Grab freelist stats.\n-\t\tvar freelistFreeN = tx.db.freelist.free_count()\n-\t\tvar freelistPendingN = tx.db.freelist.pending_count()\n-\t\tvar freelistAlloc = tx.db.freelist.size()\n+\t\tvar freelistFreeN = tx.db.freelist.FreeCount()\n+\t\tvar freelistPendingN = tx.db.freelist.PendingCount()\n+\t\tvar freelistAlloc = tx.db.freelist.EstimatedWritePageSize()\n \n \t\t// Remove transaction ref & writer lock.\n \t\ttx.db.rwtx = nil\n@@ -639,7 +637,7 @@ func (tx *Tx) Page(id int) (*common.PageInfo, error) {\n \t}\n \n \t// Determine the type (or if it's free).\n-\tif tx.db.freelist.freed(common.Pgid(id)) {\n+\tif tx.db.freelist.Freed(common.Pgid(id)) {\n \t\tinfo.Type = \"free\"\n \t} else {\n \t\tinfo.Type = p.Typ()\ndiff --git a/tx_check.go b/tx_check.go\nindex 4e3c41ae4..c3ecbb975 100644\n--- a/tx_check.go\n+++ b/tx_check.go\n@@ -41,8 +41,8 @@ func (tx *Tx) check(cfg checkConfig, ch chan error) {\n \n \t// Check if any pages are double freed.\n \tfreed := make(map[common.Pgid]bool)\n-\tall := make([]common.Pgid, tx.db.freelist.count())\n-\ttx.db.freelist.copyall(all)\n+\tall := make([]common.Pgid, tx.db.freelist.Count())\n+\ttx.db.freelist.Copyall(all)\n \tfor _, id := range all {\n \t\tif freed[id] {\n \t\t\tch <- fmt.Errorf(\"page %d: already freed\", id)\n", "instance_id": "etcd-io__bbolt-775", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent to introduce a freelist interface to improve modularity and testing in the bbolt database library. It outlines the goal of separating the existing monolithic freelist implementation into an interface with two concrete implementations (array and hashmap) and moving it into an internal package to prevent leaking internals. The proposed interface is provided, which helps in understanding the expected functionality. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define how the interface should interact with the rest of the codebase beyond general mentions of initialization and page allocation. Additionally, edge cases or specific constraints (e.g., performance requirements, backward compatibility) are not mentioned. The TODO comments in the interface definition (e.g., renaming methods) indicate some uncertainty in the final design, which could lead to confusion during implementation. Overall, while the goal and high-level approach are clear, some finer details and potential challenges are left unspecified.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, involving refactoring a core component (freelist) of the bbolt database library across multiple files. The changes include creating a new internal package, defining an interface, splitting the existing implementation into two distinct structs (array and hashmap), and updating all references to the freelist throughout the codebase. This requires a deep understanding of the interactions between the freelist and other components like transactions and page management, as well as ensuring that the refactoring does not break existing functionality. Second, the number of technical concepts involved is substantial, including Go interfaces, struct composition, package organization, and domain-specific knowledge of database page allocation and freelist management. The developer must also handle shared functionality (e.g., read/write operations) in a way that avoids duplication while allowing for future implementation-specific optimizations (e.g., vint compression). Third, while the problem statement does not explicitly mention edge cases, the nature of the freelist (managing page allocation and deallocation) inherently involves complex scenarios such as handling concurrent transactions, ensuring no double-freeing of pages, and managing overflow pages, all of which are evident in the code changes. The refactoring must preserve or enhance error handling and correctness in these scenarios. Finally, the impact on the system's architecture is moderate, as it introduces better modularity but requires careful integration to avoid introducing bugs or performance regressions in a critical component of a database system. Given these factors, a difficulty score of 0.65 reflects the need for a deep understanding of the codebase, careful refactoring across multiple modules, and attention to potential edge cases, placing it in the lower end of the \"Hard\" range but not at the extreme end due to the absence of system-level or highly domain-specific challenges beyond database internals.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Tildes break directory autocomplete\n### What happened?\n\nWhen you go to create a new folder and use `~` as a shortcut for your home directory, autocomplete stops working.\n\nHere's what it looks like with `~` (there is no autocomplete):\n![Image](https://github.com/user-attachments/assets/557ab5bf-7948-44c0-a991-d047826b593d)\n\nAnd with `/home/marcus` (real path of my home directory):\n\n![Image](https://github.com/user-attachments/assets/da06aed5-b76d-49f4-a686-b2b7422cdb98)\n\nThis behavior (paired with the log output) is strange, as calling the REST API directly through curl with tildes behaves correctly and lists everything in my home directory.\n\n### Syncthing version\n\nv1.29.3\n\n### Platform & operating system\n\n(Arch, with all optional repositories enabled) Linux (64-bit AMD)\n\n### Browser version\n\nLibreWolf 136.0.1\n\n### Relevant log output\n\n```shell\n2025-03-14 10:24:45 http: GET \"/rest/system/browse?current=%2Fhome%2Fmarcus%2F\": status 200, 1473 bytes in 0.94 ms\n2025-03-14 10:26:21 http: GET \"/rest/svc/random/string?length=10\": status 200, 29 bytes in 0.13 ms\n2025-03-14 10:26:21 http: GET \"/rest/config/defaults/folder\": status 200, 1361 bytes in 0.16 ms\n2025-03-14 10:26:23 http: GET \"/rest/system/browse?current=~\": status 200, 21 bytes in 0.21 ms\n2025-03-14 10:26:24 http: GET \"/rest/system/browse?current=~%2F\": status 200, 1473 bytes in 1.10 ms\n```\n", "patch": "diff --git a/lib/api/api.go b/lib/api/api.go\nindex b5be612e1fb..8d07546b9f1 100644\n--- a/lib/api/api.go\n+++ b/lib/api/api.go\n@@ -1764,7 +1764,7 @@ func browse(fsType fs.FilesystemType, current string) []string {\n \t\treturn browseRoots(fsType)\n \t}\n \n-\tparent, base := parentAndBase(current)\n+\tparent, base := filepath.Split(current)\n \tffs := fs.NewFilesystem(fsType, parent)\n \tfiles := browseFiles(ffs, base)\n \tfor i := range files {\n@@ -1800,27 +1800,6 @@ func browseRoots(fsType fs.FilesystemType) []string {\n \treturn nil\n }\n \n-// parentAndBase returns the parent directory and the remaining base of the\n-// path. The base may be empty if the path ends with a path separator.\n-func parentAndBase(current string) (string, string) {\n-\tsearch, _ := fs.ExpandTilde(current)\n-\tpathSeparator := string(fs.PathSeparator)\n-\n-\tif strings.HasSuffix(current, pathSeparator) && !strings.HasSuffix(search, pathSeparator) {\n-\t\tsearch = search + pathSeparator\n-\t}\n-\tsearchDir := filepath.Dir(search)\n-\n-\t// The searchFile should be the last component of search, or empty if it\n-\t// ends with a path separator\n-\tvar searchFile string\n-\tif !strings.HasSuffix(search, pathSeparator) {\n-\t\tsearchFile = filepath.Base(search)\n-\t}\n-\n-\treturn searchDir, searchFile\n-}\n-\n func browseFiles(ffs fs.Filesystem, search string) []string {\n \tsubdirectories, _ := ffs.DirNames(\".\")\n \tpathSeparator := string(fs.PathSeparator)\n", "instance_id": "syncthing__syncthing-9992", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the use of the tilde (~) as a shortcut for the home directory breaks directory autocomplete in Syncthing. It provides visual evidence through images and relevant log output, which helps in understanding the behavior. The goal is implicitly clear\u2014fix the autocomplete functionality when using the tilde. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly specify the expected behavior (e.g., should ~ always expand to the home directory before processing?) or mention specific edge cases (e.g., what if ~ is used in a non-standard context or with invalid paths?). Additionally, constraints or requirements for the fix (e.g., performance considerations or compatibility with different filesystems) are not provided. Despite these minor gaps, the issue is well-documented with logs and visuals, making it mostly clear.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows that the fix involves a relatively small and localized change in a single file (`api.go`). The modification replaces a custom `parentAndBase` function with the standard `filepath.Split` function and removes the custom logic for tilde expansion and path handling. This indicates a straightforward refactoring of path processing logic without impacting multiple modules or the broader system architecture. The amount of code change is minimal, primarily deleting custom logic and leveraging a built-in function.\n\n2. **Number of Technical Concepts**: Solving this requires understanding basic file path handling in Go, specifically the `filepath` package and its behavior with path separators. It also involves familiarity with the Syncthing codebase's filesystem abstraction (`fs.FilesystemType`) and how paths are processed for autocomplete. However, these concepts are not particularly complex for a developer with moderate experience in Go. No advanced algorithms, design patterns, or domain-specific knowledge (beyond basic filesystem operations) are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change suggests that the original custom logic for handling tildes and path separators might have been error-prone. The fix simplifies this by relying on a standard library function, which likely handles edge cases better. However, the developer must still consider potential issues like invalid paths or platform-specific path separator differences, though these are not overly complex to address. No significant new error handling logic appears to be introduced in the diff.\n\n4. **Overall Complexity**: The issue is a bug fix rather than a feature addition or major refactoring. The solution involves simplifying existing logic rather than introducing new complexity. While it requires some understanding of the codebase (e.g., how the `browse` function interacts with the filesystem), it does not demand deep architectural knowledge or extensive modifications.\n\nGiven these points, I assign a difficulty score of 0.35, as the problem is slightly more involved than a trivial fix (e.g., changing a constant) due to the need to understand path handling and ensure the fix does not introduce regressions, but it remains within the realm of an easy task for a developer familiar with Go and basic filesystem concepts.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Adding HTTP server load balancing algorithms\n<!-- PLEASE FOLLOW THE ISSUE TEMPLATE TO HELP TRIAGE AND SUPPORT! -->\r\n\r\n### Do you want to request a *feature* or report a *bug*?\r\n\r\n<!--\r\nDO NOT FILE ISSUES FOR GENERAL SUPPORT QUESTIONS.\r\n\r\nThe issue tracker is for reporting bugs and feature requests only.\r\nFor end-user related support questions, please refer to one of the following:\r\n\r\n- the Traefik community forum: https://community.containo.us/\r\n\r\n-->\r\n\r\nFeature\r\n\r\n### What did you expect to see?\r\n\r\nAn easy way to add server-level HTTP load balancing algorithms.\r\n\r\nI'm looking into adding HTTP load balancing algorithms as part of a research project, with the prospect of integrating what works into the main tree.\r\nAfter looking into it for a few days, I found that the following two things are a hindrance:\r\n\r\n1. The configuration schema does not allow for concrete load-balancing algorithms and their properties\r\n2. The `healthcheck.Balancer` interface is not general enough\r\n\r\n#### Load-balancer configuration\r\nPoint 1. seems easy enough, it's just a question of agreeing on a format. I have a suggestion, but don't know whether this conflicts with any existing proposals. My suggestion is documented here: [1], and I have a sandbox here [2]\r\n\r\n  [1]: https://community.containo.us/t/implementing-server-load-balancing-strategies/6705\r\n  [2]: https://github.com/containous/traefik/compare/master...thrau:load-balancers\r\n\r\n#### Interface & abstraction\r\n\r\nPoint 2. looks more difficult.\r\n\r\nFirst, it seems to me that coupling the `Balancer` interface to oxy's `roundrobin.ServerOption` severely limits the extensibility of the interface and the utility of the abstraction. There's no way of reusing `roundrobin.ServerOptions` due to the visibility of oxy's `server` struct it takes as input.\r\n\r\nA question that I've asked in the community forum, which is probably more suitable here: Is there a particular reason oxy was used for load balancing? In terms of code re-use, it seems that most of the code is already there anyway (the service level load balancing is very similar).\r\n\r\nMaybe adding traefik's own http `Server` and `ServerOption` abstraction could help. A concrete binding to oxy can then facilitate the use of oxy components, and also allow easier implementation of new load-balancing algorithms. However, that would likely create significant amount of boilerplate code that may require reflection and also needs maintaining when oxy changes.\r\n\r\nI was hoping to get some input from folks working on the HTTP load balancer implementation before I start writing code. In particular, I wanted to first find out whether there are any proposal or plans for changing/extending the load balancer implementation, and what I should look out for if i start digging into it.\r\n\r\n  Thomas\r\n\n", "patch": "diff --git a/docs/content/migration/v3.md b/docs/content/migration/v3.md\nindex 4f2f1d6e6a..f38081fa73 100644\n--- a/docs/content/migration/v3.md\n+++ b/docs/content/migration/v3.md\n@@ -187,3 +187,18 @@ and will be removed in the next major version.\n \n In `v3.3.4`, the OpenTelemetry Request Duration metric (named `traefik_(entrypoint|router|service)_request_duration_seconds`) unit has been changed from milliseconds to seconds.\n To be consistent with the naming and other metrics providers, the metric now reports the duration in seconds.\n+\n+## v3.3 to v3.4\n+\n+### Kubernetes CRD Provider\n+\n+In `v3.4`, the HTTP service definition has been updated.\n+The strategy field now supports two new values: `wrr` and `p2c` (please refer to the [HTTP Services Load Balancing documentation](../../routing/services/#load-balancing-strategy) for more details).\n+\n+CRDs can be updated with this command:\n+\n+```shell\n+kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v3.4/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n+```\n+\n+Please note that the `RoundRobin` strategy value is now deprecated, but still supported and equivalent to `wrr`, and will be removed in the next major release.\ndiff --git a/docs/content/reference/dynamic-configuration/docker-labels.yml b/docs/content/reference/dynamic-configuration/docker-labels.yml\nindex bc8b268f91..d6534790b5 100644\n--- a/docs/content/reference/dynamic-configuration/docker-labels.yml\n+++ b/docs/content/reference/dynamic-configuration/docker-labels.yml\n@@ -221,6 +221,7 @@\n - \"traefik.http.services.service02.loadbalancer.sticky.cookie.path=foobar\"\n - \"traefik.http.services.service02.loadbalancer.sticky.cookie.samesite=foobar\"\n - \"traefik.http.services.service02.loadbalancer.sticky.cookie.secure=true\"\n+- \"traefik.http.services.service02.loadbalancer.strategy=foobar\"\n - \"traefik.http.services.service02.loadbalancer.server.port=foobar\"\n - \"traefik.http.services.service02.loadbalancer.server.preservepath=true\"\n - \"traefik.http.services.service02.loadbalancer.server.scheme=foobar\"\ndiff --git a/docs/content/reference/dynamic-configuration/file.toml b/docs/content/reference/dynamic-configuration/file.toml\nindex 0c5ae3bbdf..1c54defe80 100644\n--- a/docs/content/reference/dynamic-configuration/file.toml\n+++ b/docs/content/reference/dynamic-configuration/file.toml\n@@ -54,6 +54,7 @@\n         [http.services.Service01.failover.healthCheck]\n     [http.services.Service02]\n       [http.services.Service02.loadBalancer]\n+        strategy = \"foobar\"\n         passHostHeader = true\n         serversTransport = \"foobar\"\n         [http.services.Service02.loadBalancer.sticky]\ndiff --git a/docs/content/reference/dynamic-configuration/file.yaml b/docs/content/reference/dynamic-configuration/file.yaml\nindex 7c0766810b..2a8956bbcb 100644\n--- a/docs/content/reference/dynamic-configuration/file.yaml\n+++ b/docs/content/reference/dynamic-configuration/file.yaml\n@@ -80,6 +80,7 @@ http:\n           - url: foobar\n             weight: 42\n             preservePath: true\n+        strategy: foobar\n         healthCheck:\n           scheme: foobar\n           mode: foobar\ndiff --git a/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml b/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\nindex 0b5982e204..76f596493a 100644\n--- a/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n+++ b/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n@@ -290,10 +290,14 @@ spec:\n                                 type: object\n                             type: object\n                           strategy:\n+                            default: wrr\n                             description: |-\n                               Strategy defines the load balancing strategy between the servers.\n-                              RoundRobin is the only supported value at the moment.\n+                              Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                              RoundRobin value is deprecated and supported for backward compatibility.\n                             enum:\n+                            - wrr\n+                            - p2c\n                             - RoundRobin\n                             type: string\n                           weight:\n@@ -1217,10 +1221,14 @@ spec:\n                             type: object\n                         type: object\n                       strategy:\n+                        default: wrr\n                         description: |-\n                           Strategy defines the load balancing strategy between the servers.\n-                          RoundRobin is the only supported value at the moment.\n+                          Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                          RoundRobin value is deprecated and supported for backward compatibility.\n                         enum:\n+                        - wrr\n+                        - p2c\n                         - RoundRobin\n                         type: string\n                       weight:\n@@ -2924,10 +2932,14 @@ spec:\n                               type: object\n                           type: object\n                         strategy:\n+                          default: wrr\n                           description: |-\n                             Strategy defines the load balancing strategy between the servers.\n-                            RoundRobin is the only supported value at the moment.\n+                            Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                            RoundRobin value is deprecated and supported for backward compatibility.\n                           enum:\n+                          - wrr\n+                          - p2c\n                           - RoundRobin\n                           type: string\n                         weight:\n@@ -3048,10 +3060,14 @@ spec:\n                         type: object\n                     type: object\n                   strategy:\n+                    default: wrr\n                     description: |-\n                       Strategy defines the load balancing strategy between the servers.\n-                      RoundRobin is the only supported value at the moment.\n+                      Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                      RoundRobin value is deprecated and supported for backward compatibility.\n                     enum:\n+                    - wrr\n+                    - p2c\n                     - RoundRobin\n                     type: string\n                   weight:\n@@ -3250,10 +3266,14 @@ spec:\n                               type: object\n                           type: object\n                         strategy:\n+                          default: wrr\n                           description: |-\n                             Strategy defines the load balancing strategy between the servers.\n-                            RoundRobin is the only supported value at the moment.\n+                            Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                            RoundRobin value is deprecated and supported for backward compatibility.\n                           enum:\n+                          - wrr\n+                          - p2c\n                           - RoundRobin\n                           type: string\n                         weight:\ndiff --git a/docs/content/reference/dynamic-configuration/kv-ref.md b/docs/content/reference/dynamic-configuration/kv-ref.md\nindex c1a0809255..4d218e5992 100644\n--- a/docs/content/reference/dynamic-configuration/kv-ref.md\n+++ b/docs/content/reference/dynamic-configuration/kv-ref.md\n@@ -297,6 +297,7 @@ THIS FILE MUST NOT BE EDITED BY HAND\n | `traefik/http/services/Service02/loadBalancer/sticky/cookie/path` | `foobar` |\n | `traefik/http/services/Service02/loadBalancer/sticky/cookie/sameSite` | `foobar` |\n | `traefik/http/services/Service02/loadBalancer/sticky/cookie/secure` | `true` |\n+| `traefik/http/services/Service02/loadBalancer/strategy` | `foobar` |\n | `traefik/http/services/Service03/mirroring/healthCheck` | `` |\n | `traefik/http/services/Service03/mirroring/maxBodySize` | `42` |\n | `traefik/http/services/Service03/mirroring/mirrorBody` | `true` |\ndiff --git a/docs/content/reference/dynamic-configuration/traefik.io_ingressroutes.yaml b/docs/content/reference/dynamic-configuration/traefik.io_ingressroutes.yaml\nindex c1cb4024de..8fde2b818a 100644\n--- a/docs/content/reference/dynamic-configuration/traefik.io_ingressroutes.yaml\n+++ b/docs/content/reference/dynamic-configuration/traefik.io_ingressroutes.yaml\n@@ -290,10 +290,14 @@ spec:\n                                 type: object\n                             type: object\n                           strategy:\n+                            default: wrr\n                             description: |-\n                               Strategy defines the load balancing strategy between the servers.\n-                              RoundRobin is the only supported value at the moment.\n+                              Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                              RoundRobin value is deprecated and supported for backward compatibility.\n                             enum:\n+                            - wrr\n+                            - p2c\n                             - RoundRobin\n                             type: string\n                           weight:\ndiff --git a/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml b/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\nindex cf0da07197..5ae821eb7f 100644\n--- a/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\n+++ b/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\n@@ -454,10 +454,14 @@ spec:\n                             type: object\n                         type: object\n                       strategy:\n+                        default: wrr\n                         description: |-\n                           Strategy defines the load balancing strategy between the servers.\n-                          RoundRobin is the only supported value at the moment.\n+                          Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                          RoundRobin value is deprecated and supported for backward compatibility.\n                         enum:\n+                        - wrr\n+                        - p2c\n                         - RoundRobin\n                         type: string\n                       weight:\ndiff --git a/docs/content/reference/dynamic-configuration/traefik.io_traefikservices.yaml b/docs/content/reference/dynamic-configuration/traefik.io_traefikservices.yaml\nindex 1e87f2f256..de76611314 100644\n--- a/docs/content/reference/dynamic-configuration/traefik.io_traefikservices.yaml\n+++ b/docs/content/reference/dynamic-configuration/traefik.io_traefikservices.yaml\n@@ -314,10 +314,14 @@ spec:\n                               type: object\n                           type: object\n                         strategy:\n+                          default: wrr\n                           description: |-\n                             Strategy defines the load balancing strategy between the servers.\n-                            RoundRobin is the only supported value at the moment.\n+                            Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                            RoundRobin value is deprecated and supported for backward compatibility.\n                           enum:\n+                          - wrr\n+                          - p2c\n                           - RoundRobin\n                           type: string\n                         weight:\n@@ -438,10 +442,14 @@ spec:\n                         type: object\n                     type: object\n                   strategy:\n+                    default: wrr\n                     description: |-\n                       Strategy defines the load balancing strategy between the servers.\n-                      RoundRobin is the only supported value at the moment.\n+                      Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                      RoundRobin value is deprecated and supported for backward compatibility.\n                     enum:\n+                    - wrr\n+                    - p2c\n                     - RoundRobin\n                     type: string\n                   weight:\n@@ -640,10 +648,14 @@ spec:\n                               type: object\n                           type: object\n                         strategy:\n+                          default: wrr\n                           description: |-\n                             Strategy defines the load balancing strategy between the servers.\n-                            RoundRobin is the only supported value at the moment.\n+                            Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                            RoundRobin value is deprecated and supported for backward compatibility.\n                           enum:\n+                          - wrr\n+                          - p2c\n                           - RoundRobin\n                           type: string\n                         weight:\ndiff --git a/docs/content/routing/providers/consul-catalog.md b/docs/content/routing/providers/consul-catalog.md\nindex f539a68688..76b326b550 100644\n--- a/docs/content/routing/providers/consul-catalog.md\n+++ b/docs/content/routing/providers/consul-catalog.md\n@@ -346,6 +346,14 @@ you'd add the tag `traefik.http.services.{name-of-your-choice}.loadbalancer.pass\n     traefik.http.services.myservice.loadbalancer.responseforwarding.flushinterval=10\n     ```\n \n+??? info \"`traefik.http.services.<service_name>.loadbalancer.strategy`\"\n+\n+    See [load balancing strategy](../services/index.md#load-balancing-strategy) for more information.\n+\n+    ```yaml\n+    traefik.http.services.myservice.loadbalancer.strategy=p2c\n+    ```\n+\n ### Middleware\n \n You can declare pieces of middleware using tags starting with `traefik.http.middlewares.{name-of-your-choice}.`, followed by the middleware type/options.\ndiff --git a/docs/content/routing/providers/docker.md b/docs/content/routing/providers/docker.md\nindex 928fafef1c..aedbfe09f4 100644\n--- a/docs/content/routing/providers/docker.md\n+++ b/docs/content/routing/providers/docker.md\n@@ -461,6 +461,14 @@ you'd add the label `traefik.http.services.<name-of-your-choice>.loadbalancer.pa\n     - \"traefik.http.services.myservice.loadbalancer.responseforwarding.flushinterval=10\"\n     ```\n \n+??? info \"`traefik.http.services.<service_name>.loadbalancer.strategy`\"\n+\n+    See [load balancing strategy](../services/index.md#load-balancing-strategy) for more information.\n+\n+    ```yaml\n+    - \"traefik.http.services.myservice.loadbalancer.strategy=p2c\"\n+    ```\n+\n ### Middleware\n \n You can declare pieces of middleware using labels starting with `traefik.http.middlewares.<name-of-your-choice>.`,\ndiff --git a/docs/content/routing/providers/ecs.md b/docs/content/routing/providers/ecs.md\nindex 6fed9e9ae4..e859754096 100644\n--- a/docs/content/routing/providers/ecs.md\n+++ b/docs/content/routing/providers/ecs.md\n@@ -350,6 +350,14 @@ you'd add the label `traefik.http.services.{name-of-your-choice}.loadbalancer.pa\n     traefik.http.services.myservice.loadbalancer.responseforwarding.flushinterval=10\n     ```\n \n+??? info \"`traefik.http.services.<service_name>.loadbalancer.strategy`\"\n+\n+    See [load balancing strategy](../services/index.md#load-balancing-strategy) for more information.\n+\n+    ```yaml\n+    traefik.http.services.myservice.loadbalancer.strategy=p2c\n+    ```\n+\n ### Middleware\n \n You can declare pieces of middleware using labels starting with `traefik.http.middlewares.{name-of-your-choice}.`, followed by the middleware type/options.\ndiff --git a/docs/content/routing/providers/kubernetes-crd.md b/docs/content/routing/providers/kubernetes-crd.md\nindex 9188837dfd..be1e038d21 100644\n--- a/docs/content/routing/providers/kubernetes-crd.md\n+++ b/docs/content/routing/providers/kubernetes-crd.md\n@@ -358,19 +358,19 @@ Register the `IngressRoute` [kind](../../reference/dynamic-configuration/kuberne\n               maxAge: 42  \n               path: /foo\n               domain: foo.com\n-          strategy: RoundRobin\n+          strategy: wrr                 # [16]\n           weight: 10\n-          nativeLB: true                # [16]\n-          nodePortLB: true              # [17]\n-      tls:                              # [18]\n-        secretName: supersecret         # [19]\n-        options:                        # [20]\n-          name: opt                     # [21]\n-          namespace: default            # [22]\n-        certResolver: foo               # [23]\n-        domains:                        # [24]\n-        - main: example.net             # [25]\n-          sans:                         # [26]\n+          nativeLB: true                # [17]\n+          nodePortLB: true              # [18]\n+      tls:                              # [19]\n+        secretName: supersecret         # [20]\n+        options:                        # [21]\n+          name: opt                     # [22]\n+          namespace: default            # [23]\n+        certResolver: foo               # [24]\n+        domains:                        # [25]\n+        - main: example.net             # [26]\n+          sans:                         # [27]\n           - a.example.net\n           - b.example.net\n     ```\n@@ -392,17 +392,18 @@ Register the `IngressRoute` [kind](../../reference/dynamic-configuration/kuberne\n | [13] | `services[n].port`             | Defines the port of a [Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/). This can be a reference to a named port.                                                                                                                                       |\n | [14] | `services[n].serversTransport` | Defines the reference to a [ServersTransport](#kind-serverstransport). The ServersTransport namespace is assumed to be the [Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/) namespace (see [ServersTransport reference](#serverstransport-reference)). |\n | [15] | `services[n].healthCheck`      | Defines the HealthCheck when service references a [Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/) of type ExternalName.                                                                                                                               |\n-| [16] | `services[n].nativeLB`         | Controls, when creating the load-balancer, whether the LB's children are directly the pods IPs or if the only child is the Kubernetes Service clusterIP.                                                                                                                                     |\n-| [17] | `services[n].nodePortLB`       | Controls, when creating the load-balancer, whether the LB's children are directly the nodes internal IPs using the nodePort when the service type is NodePort.                                                                                                                               |\n-| [18] | `tls`                          | Defines [TLS](../routers/index.md#tls) certificate configuration                                                                                                                                                                                                                             |\n-| [19] | `tls.secretName`               | Defines the [secret](https://kubernetes.io/docs/concepts/configuration/secret/) name used to store the certificate (in the `IngressRoute` namespace)                                                                                                                                         |\n-| [20] | `tls.options`                  | Defines the reference to a [TLSOption](#kind-tlsoption)                                                                                                                                                                                                                                      |\n-| [21] | `options.name`                 | Defines the [TLSOption](#kind-tlsoption) name                                                                                                                                                                                                                                                |\n-| [22] | `options.namespace`            | Defines the [TLSOption](#kind-tlsoption) namespace                                                                                                                                                                                                                                           |\n-| [23] | `tls.certResolver`             | Defines the reference to a [CertResolver](../routers/index.md#certresolver)                                                                                                                                                                                                                  |\n-| [24] | `tls.domains`                  | List of [domains](../routers/index.md#domains)                                                                                                                                                                                                                                               |\n-| [25] | `domains[n].main`              | Defines the main domain name                                                                                                                                                                                                                                                                 |\n-| [26] | `domains[n].sans`              | List of SANs (alternative domains)                                                                                                                                                                                                                                                           |\n+| [16] | `services[n].strategy`         | Defines the load-balancing strategy for the load-balancer. Supported values are `wrr` and `p2c`, please refer to the [Load Balancing documentation](../routing/services/#load-balancing-strategy) for more information.                                                                      |\n+| [17] | `services[n].nativeLB`         | Controls, when creating the load-balancer, whether the LB's children are directly the pods IPs or if the only child is the Kubernetes Service clusterIP.                                                                                                                                     |\n+| [18] | `services[n].nodePortLB`       | Controls, when creating the load-balancer, whether the LB's children are directly the nodes internal IPs using the nodePort when the service type is NodePort.                                                                                                                               |\n+| [19] | `tls`                          | Defines [TLS](../routers/index.md#tls) certificate configuration                                                                                                                                                                                                                             |\n+| [20] | `tls.secretName`               | Defines the [secret](https://kubernetes.io/docs/concepts/configuration/secret/) name used to store the certificate (in the `IngressRoute` namespace)                                                                                                                                         |\n+| [21] | `tls.options`                  | Defines the reference to a [TLSOption](#kind-tlsoption)                                                                                                                                                                                                                                      |\n+| [22] | `options.name`                 | Defines the [TLSOption](#kind-tlsoption) name                                                                                                                                                                                                                                                |\n+| [23] | `options.namespace`            | Defines the [TLSOption](#kind-tlsoption) namespace                                                                                                                                                                                                                                           |\n+| [24] | `tls.certResolver`             | Defines the reference to a [CertResolver](../routers/index.md#certresolver)                                                                                                                                                                                                                  |\n+| [25] | `tls.domains`                  | List of [domains](../routers/index.md#domains)                                                                                                                                                                                                                                               |\n+| [26] | `domains[n].main`              | Defines the main domain name                                                                                                                                                                                                                                                                 |\n+| [27] | `domains[n].sans`              | List of SANs (alternative domains)                                                                                                                                                                                                                                                           |\n \n ??? example \"Declaring an IngressRoute\"\n \n@@ -605,7 +606,7 @@ Register the `IngressRoute` [kind](../../reference/dynamic-configuration/kuberne\n \n #### Load Balancing\n \n-More information in the dedicated server [load balancing](../services/index.md#load-balancing) section.\n+More information in the dedicated server [load balancing](../services/index.md#load-balancing-strategy) section.\n \n !!! info \"Declaring and using Kubernetes Service Load Balancing\"\n \ndiff --git a/docs/content/routing/providers/kv.md b/docs/content/routing/providers/kv.md\nindex 1234d1357f..37e6148f56 100644\n--- a/docs/content/routing/providers/kv.md\n+++ b/docs/content/routing/providers/kv.md\n@@ -300,6 +300,14 @@ A Story of key & values\n     |---------------------------------------------------------------------------------|-------|\n     | `traefik/http/services/myservice/loadbalancer/responseforwarding/flushinterval` | `10`  |\n \n+??? info \"`traefik/http/services/<service_name>/loadbalancer/strategy`\"\n+\n+    See [load balancing strategy](../services/index.md#load-balancing-strategy) for more information.\n+\n+    | Key (Path)                                              | Value |\n+    |---------------------------------------------------------|-------|\n+    | `traefik/http/services/myservice/loadbalancer/strategy` | `p2c` |\n+\n ??? info \"`traefik/http/services/<service_name>/mirroring/service`\"\n \n     | Key (Path)                                               | Value    |\ndiff --git a/docs/content/routing/providers/nomad.md b/docs/content/routing/providers/nomad.md\nindex 175f1d61c1..613f35a7bf 100644\n--- a/docs/content/routing/providers/nomad.md\n+++ b/docs/content/routing/providers/nomad.md\n@@ -338,6 +338,14 @@ you'd add the tag `traefik.http.services.{name-of-your-choice}.loadbalancer.pass\n     traefik.http.services.myservice.loadbalancer.responseforwarding.flushinterval=10\n     ```\n \n+??? info \"`traefik.http.services.<service_name>.loadbalancer.strategy`\"\n+\n+    See [load balancing strategy](../services/index.md#load-balancing-strategy) for more information.\n+\n+    ```yaml\n+    traefik.http.services.myservice.loadbalancer.strategy=p2c\n+    ```\n+\n ### Middleware\n \n You can declare pieces of middleware using tags starting with `traefik.http.middlewares.{name-of-your-choice}.`, followed by the middleware type/options.\ndiff --git a/docs/content/routing/providers/swarm.md b/docs/content/routing/providers/swarm.md\nindex 754931a71b..bd9685ef63 100644\n--- a/docs/content/routing/providers/swarm.md\n+++ b/docs/content/routing/providers/swarm.md\n@@ -467,6 +467,14 @@ you'd add the label `traefik.http.services.<name-of-your-choice>.loadbalancer.pa\n     - \"traefik.http.services.myservice.loadbalancer.responseforwarding.flushinterval=10\"\n     ```\n \n+??? info \"`traefik.http.services.<service_name>.loadbalancer.strategy`\"\n+\n+    See [load balancing strategy](../services/index.md#load-balancing-strategy) for more information.\n+\n+    ```yaml\n+    - \"traefik.http.services.myservice.loadbalancer.strategy=p2c\"\n+    ```\n+\n ### Middleware\n \n You can declare pieces of middleware using labels starting with `traefik.http.middlewares.<name-of-your-choice>.`,\ndiff --git a/docs/content/routing/services/index.md b/docs/content/routing/services/index.md\nindex 9cc266acc5..199fb81a36 100644\n--- a/docs/content/routing/services/index.md\n+++ b/docs/content/routing/services/index.md\n@@ -139,9 +139,13 @@ The `url` option point to a specific instance.\n           url = \"http://private-ip-server-1/\"\n     ```\n \n-The `weight` option allows for weighted load balancing on the servers.\n+The `preservePath` option allows to preserve the URL path.\n \n-??? example \"A Service with Two Servers with Weight -- Using the [File Provider](../../providers/file.md)\"\n+!!! info \"Health Check\"\n+\n+    When a [health check](#health-check) is configured for the server, the path is not preserved.\n+\n+??? example \"A Service with One Server and PreservePath -- Using the [File Provider](../../providers/file.md)\"\n \n     ```yaml tab=\"YAML\"\n     ## Dynamic configuration\n@@ -150,11 +154,8 @@ The `weight` option allows for weighted load balancing on the servers.\n         my-service:\n           loadBalancer:\n             servers:\n-              - url: \"http://private-ip-server-1/\"\n-                weight: 2\n-              - url: \"http://private-ip-server-2/\"\n-                weight: 1\n-\n+              - url: \"http://private-ip-server-1/base\"\n+                preservePath: true\n     ```\n \n     ```toml tab=\"TOML\"\n@@ -162,20 +163,26 @@ The `weight` option allows for weighted load balancing on the servers.\n     [http.services]\n       [http.services.my-service.loadBalancer]\n         [[http.services.my-service.loadBalancer.servers]]\n-          url = \"http://private-ip-server-1/\"\n-          weight = 2\n-        [[http.services.my-service.loadBalancer.servers]]\n-          url = \"http://private-ip-server-2/\"\n-          weight = 1\n+          url = \"http://private-ip-server-1/base\"\n+          preservePath = true\n     ```\n \n-The `preservePath` option allows to preserve the URL path.\n+#### Load Balancing Strategy\n \n-!!! info \"Health Check\"\n+The `strategy` option allows to choose the load balancing algorithm.\n \n-    When a [health check](#health-check) is configured for the server, the path is not preserved.\n+Two load balancing algorithms are supported:\n \n-??? example \"A Service with One Server and PreservePath -- Using the [File Provider](../../providers/file.md)\"\n+- Weighed round-robin (wrr)\n+- Power of two choices (p2c)\n+\n+##### WRR\n+\n+Weighed round-robin is the default strategy (and does not need to be specified).\n+\n+The `weight` option allows for weighted load balancing on the servers.\n+\n+??? example \"A Service with Two Servers with Weight -- Using the [File Provider](../../providers/file.md)\"\n \n     ```yaml tab=\"YAML\"\n     ## Dynamic configuration\n@@ -184,8 +191,11 @@ The `preservePath` option allows to preserve the URL path.\n         my-service:\n           loadBalancer:\n             servers:\n-              - url: \"http://private-ip-server-1/base\"\n-                preservePath: true\n+              - url: \"http://private-ip-server-1/\"\n+                weight: 2\n+              - url: \"http://private-ip-server-2/\"\n+                weight: 1\n+\n     ```\n \n     ```toml tab=\"TOML\"\n@@ -193,15 +203,18 @@ The `preservePath` option allows to preserve the URL path.\n     [http.services]\n       [http.services.my-service.loadBalancer]\n         [[http.services.my-service.loadBalancer.servers]]\n-          url = \"http://private-ip-server-1/base\"\n-          preservePath = true\n+          url = \"http://private-ip-server-1/\"\n+          weight = 2\n+        [[http.services.my-service.loadBalancer.servers]]\n+          url = \"http://private-ip-server-2/\"\n+          weight = 1\n     ```\n \n-#### Load-balancing\n+##### P2C\n \n-For now, only round robin load balancing is supported:\n+Power of two choices algorithm is a load balancing strategy that selects two servers at random and chooses the one with the least number of active requests.\n \n-??? example \"Load Balancing -- Using the [File Provider](../../providers/file.md)\"\n+??? example \"P2C Load Balancing -- Using the [File Provider](../../providers/file.md)\"\n \n     ```yaml tab=\"YAML\"\n     ## Dynamic configuration\n@@ -209,19 +222,24 @@ For now, only round robin load balancing is supported:\n       services:\n         my-service:\n           loadBalancer:\n+            strategy: \"p2c\"\n             servers:\n             - url: \"http://private-ip-server-1/\"\n             - url: \"http://private-ip-server-2/\"\n+            - url: \"http://private-ip-server-3/\"\n     ```\n \n     ```toml tab=\"TOML\"\n     ## Dynamic configuration\n     [http.services]\n       [http.services.my-service.loadBalancer]\n+        strategy = \"p2c\"\n         [[http.services.my-service.loadBalancer.servers]]\n           url = \"http://private-ip-server-1/\"\n         [[http.services.my-service.loadBalancer.servers]]\n-          url = \"http://private-ip-server-2/\"\n+          url = \"http://private-ip-server-2/\"       \n+        [[http.services.my-service.loadBalancer.servers]]\n+          url = \"http://private-ip-server-3/\"\n     ```\n \n #### Sticky sessions\ndiff --git a/integration/fixtures/k8s/01-traefik-crd.yml b/integration/fixtures/k8s/01-traefik-crd.yml\nindex 0b5982e204..76f596493a 100644\n--- a/integration/fixtures/k8s/01-traefik-crd.yml\n+++ b/integration/fixtures/k8s/01-traefik-crd.yml\n@@ -290,10 +290,14 @@ spec:\n                                 type: object\n                             type: object\n                           strategy:\n+                            default: wrr\n                             description: |-\n                               Strategy defines the load balancing strategy between the servers.\n-                              RoundRobin is the only supported value at the moment.\n+                              Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                              RoundRobin value is deprecated and supported for backward compatibility.\n                             enum:\n+                            - wrr\n+                            - p2c\n                             - RoundRobin\n                             type: string\n                           weight:\n@@ -1217,10 +1221,14 @@ spec:\n                             type: object\n                         type: object\n                       strategy:\n+                        default: wrr\n                         description: |-\n                           Strategy defines the load balancing strategy between the servers.\n-                          RoundRobin is the only supported value at the moment.\n+                          Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                          RoundRobin value is deprecated and supported for backward compatibility.\n                         enum:\n+                        - wrr\n+                        - p2c\n                         - RoundRobin\n                         type: string\n                       weight:\n@@ -2924,10 +2932,14 @@ spec:\n                               type: object\n                           type: object\n                         strategy:\n+                          default: wrr\n                           description: |-\n                             Strategy defines the load balancing strategy between the servers.\n-                            RoundRobin is the only supported value at the moment.\n+                            Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                            RoundRobin value is deprecated and supported for backward compatibility.\n                           enum:\n+                          - wrr\n+                          - p2c\n                           - RoundRobin\n                           type: string\n                         weight:\n@@ -3048,10 +3060,14 @@ spec:\n                         type: object\n                     type: object\n                   strategy:\n+                    default: wrr\n                     description: |-\n                       Strategy defines the load balancing strategy between the servers.\n-                      RoundRobin is the only supported value at the moment.\n+                      Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                      RoundRobin value is deprecated and supported for backward compatibility.\n                     enum:\n+                    - wrr\n+                    - p2c\n                     - RoundRobin\n                     type: string\n                   weight:\n@@ -3250,10 +3266,14 @@ spec:\n                               type: object\n                           type: object\n                         strategy:\n+                          default: wrr\n                           description: |-\n                             Strategy defines the load balancing strategy between the servers.\n-                            RoundRobin is the only supported value at the moment.\n+                            Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+                            RoundRobin value is deprecated and supported for backward compatibility.\n                           enum:\n+                          - wrr\n+                          - p2c\n                           - RoundRobin\n                           type: string\n                         weight:\ndiff --git a/pkg/config/dynamic/http_config.go b/pkg/config/dynamic/http_config.go\nindex 571fe501f6..deb2abeda4 100644\n--- a/pkg/config/dynamic/http_config.go\n+++ b/pkg/config/dynamic/http_config.go\n@@ -211,12 +211,22 @@ func (c *Cookie) SetDefaults() {\n \tc.Path = &defaultPath\n }\n \n+type BalancerStrategy string\n+\n+const (\n+\t// BalancerStrategyWRR is the weighted round-robin strategy.\n+\tBalancerStrategyWRR BalancerStrategy = \"wrr\"\n+\t// BalancerStrategyP2C is the power of two choices strategy.\n+\tBalancerStrategyP2C BalancerStrategy = \"p2c\"\n+)\n+\n // +k8s:deepcopy-gen=true\n \n // ServersLoadBalancer holds the ServersLoadBalancer configuration.\n type ServersLoadBalancer struct {\n-\tSticky  *Sticky  `json:\"sticky,omitempty\" toml:\"sticky,omitempty\" yaml:\"sticky,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" kv:\"allowEmpty\" export:\"true\"`\n-\tServers []Server `json:\"servers,omitempty\" toml:\"servers,omitempty\" yaml:\"servers,omitempty\" label-slice-as-struct:\"server\" export:\"true\"`\n+\tSticky   *Sticky          `json:\"sticky,omitempty\" toml:\"sticky,omitempty\" yaml:\"sticky,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" kv:\"allowEmpty\" export:\"true\"`\n+\tServers  []Server         `json:\"servers,omitempty\" toml:\"servers,omitempty\" yaml:\"servers,omitempty\" label-slice-as-struct:\"server\" export:\"true\"`\n+\tStrategy BalancerStrategy `json:\"strategy,omitempty\" toml:\"strategy,omitempty\" yaml:\"strategy,omitempty\" export:\"true\"`\n \t// HealthCheck enables regular active checks of the responsiveness of the\n \t// children servers of this load-balancer. To propagate status changes (e.g. all\n \t// servers of this service are down) upwards, HealthCheck must also be enabled on\n@@ -249,6 +259,7 @@ func (l *ServersLoadBalancer) SetDefaults() {\n \tdefaultPassHostHeader := DefaultPassHostHeader\n \tl.PassHostHeader = &defaultPassHostHeader\n \n+\tl.Strategy = BalancerStrategyWRR\n \tl.ResponseForwarding = &ResponseForwarding{}\n \tl.ResponseForwarding.SetDefaults()\n }\ndiff --git a/pkg/provider/kubernetes/crd/kubernetes_http.go b/pkg/provider/kubernetes/crd/kubernetes_http.go\nindex 2f7afe328b..a2d89dd425 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes_http.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes_http.go\n@@ -21,9 +21,8 @@ import (\n )\n \n const (\n-\troundRobinStrategy = \"RoundRobin\"\n-\thttpsProtocol      = \"https\"\n-\thttpProtocol       = \"http\"\n+\thttpsProtocol = \"https\"\n+\thttpProtocol  = \"http\"\n )\n \n func (p *Provider) loadIngressRouteConfiguration(ctx context.Context, client Client, tlsConfigs map[string]*tls.CertAndStores) *dynamic.HTTPConfiguration {\n@@ -322,13 +321,33 @@ func (c configBuilder) buildMirroring(ctx context.Context, tService *traefikv1al\n \n // buildServersLB creates the configuration for the load-balancer of servers defined by svc.\n func (c configBuilder) buildServersLB(namespace string, svc traefikv1alpha1.LoadBalancerSpec) (*dynamic.Service, error) {\n+\tlb := &dynamic.ServersLoadBalancer{}\n+\tlb.SetDefaults()\n+\n+\t// This is required by the tests as the fake client does not apply default values.\n+\t// TODO: remove this when the fake client apply default values.\n+\tif svc.Strategy != \"\" {\n+\t\tswitch svc.Strategy {\n+\t\tcase dynamic.BalancerStrategyWRR, dynamic.BalancerStrategyP2C:\n+\t\t\tlb.Strategy = svc.Strategy\n+\n+\t\t// Here we are just logging a warning as the default value is already applied.\n+\t\tcase \"RoundRobin\":\n+\t\t\tlog.Warn().\n+\t\t\t\tStr(\"namespace\", namespace).\n+\t\t\t\tStr(\"service\", svc.Name).\n+\t\t\t\tMsgf(\"RoundRobin strategy value is deprecated, please use %s value instead\", dynamic.BalancerStrategyWRR)\n+\n+\t\tdefault:\n+\t\t\treturn nil, fmt.Errorf(\"load-balancer strategy %s is not supported\", svc.Strategy)\n+\t\t}\n+\t}\n+\n \tservers, err := c.loadServers(namespace, svc)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n-\tlb := &dynamic.ServersLoadBalancer{}\n-\tlb.SetDefaults()\n \tlb.Servers = servers\n \n \tif svc.HealthCheck != nil {\n@@ -421,14 +440,6 @@ func (c configBuilder) makeServersTransportKey(parentNamespace string, serversTr\n }\n \n func (c configBuilder) loadServers(parentNamespace string, svc traefikv1alpha1.LoadBalancerSpec) ([]dynamic.Server, error) {\n-\tstrategy := svc.Strategy\n-\tif strategy == \"\" {\n-\t\tstrategy = roundRobinStrategy\n-\t}\n-\tif strategy != roundRobinStrategy {\n-\t\treturn nil, fmt.Errorf(\"load balancing strategy %s is not supported\", strategy)\n-\t}\n-\n \tnamespace := namespaceOrFallback(svc, parentNamespace)\n \n \tif !isNamespaceAllowed(c.allowCrossNamespace, parentNamespace, namespace) {\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go\nindex 164d1d39d1..3fa3e5ef80 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go\n@@ -113,9 +113,11 @@ type LoadBalancerSpec struct {\n \t// It defaults to https when Kubernetes Service port is 443, http otherwise.\n \tScheme string `json:\"scheme,omitempty\"`\n \t// Strategy defines the load balancing strategy between the servers.\n-\t// RoundRobin is the only supported value at the moment.\n-\t// +kubebuilder:validation:Enum=RoundRobin\n-\tStrategy string `json:\"strategy,omitempty\"`\n+\t// Supported values are: wrr (Weighed round-robin) and p2c (Power of two choices).\n+\t// RoundRobin value is deprecated and supported for backward compatibility.\n+\t// +kubebuilder:validation:Enum=wrr;p2c;RoundRobin\n+\t// +kubebuilder:default:=wrr\n+\tStrategy dynamic.BalancerStrategy `json:\"strategy,omitempty\"`\n \t// PassHostHeader defines whether the client Host header is forwarded to the upstream Kubernetes Service.\n \t// By default, passHostHeader is true.\n \tPassHostHeader *bool `json:\"passHostHeader,omitempty\"`\ndiff --git a/pkg/server/service/loadbalancer/p2c/p2c.go b/pkg/server/service/loadbalancer/p2c/p2c.go\nnew file mode 100644\nindex 0000000000..994df35bcb\n--- /dev/null\n+++ b/pkg/server/service/loadbalancer/p2c/p2c.go\n@@ -0,0 +1,227 @@\n+package p2c\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"math/rand\"\n+\t\"net/http\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"github.com/rs/zerolog/log\"\n+\t\"github.com/traefik/traefik/v3/pkg/config/dynamic\"\n+\t\"github.com/traefik/traefik/v3/pkg/server/service/loadbalancer\"\n+)\n+\n+type namedHandler struct {\n+\thttp.Handler\n+\n+\t// name is the handler name.\n+\tname string\n+\t// inflight is the number of inflight requests.\n+\t// It is used to implement the \"power-of-two-random-choices\" algorithm.\n+\tinflight atomic.Int64\n+}\n+\n+func (h *namedHandler) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n+\th.inflight.Add(1)\n+\tdefer h.inflight.Add(-1)\n+\n+\th.Handler.ServeHTTP(rw, req)\n+}\n+\n+type rnd interface {\n+\tIntn(n int) int\n+}\n+\n+// Balancer implements the power-of-two-random-choices algorithm for load balancing.\n+// The idea is to randomly select two of the available backends and choose the one with the fewest in-flight requests.\n+// This algorithm balances the load more effectively than a round-robin approach, while maintaining a constant time for the selection:\n+// The strategy also has more advantageous \"herd\" behavior than the \"fewest connections\" algorithm, especially when the load balancer\n+// doesn't have perfect knowledge of the global number of connections to the backend, for example, when running in a distributed fashion.\n+type Balancer struct {\n+\twantsHealthCheck bool\n+\n+\thandlersMu sync.RWMutex\n+\thandlers   []*namedHandler\n+\t// status is a record of which child services of the Balancer are healthy, keyed\n+\t// by name of child service. A service is initially added to the map when it is\n+\t// created via Add, and it is later removed or added to the map as needed,\n+\t// through the SetStatus method.\n+\tstatus map[string]struct{}\n+\t// updaters is the list of hooks that are run (to update the Balancer\n+\t// parent(s)), whenever the Balancer status changes.\n+\tupdaters []func(bool)\n+\t// fenced is the list of terminating yet still serving child services.\n+\tfenced map[string]struct{}\n+\n+\tsticky *loadbalancer.Sticky\n+\n+\trand rnd\n+}\n+\n+// New creates a new power-of-two-random-choices load balancer.\n+func New(stickyConfig *dynamic.Sticky, wantsHealthCheck bool) *Balancer {\n+\tbalancer := &Balancer{\n+\t\tstatus:           make(map[string]struct{}),\n+\t\tfenced:           make(map[string]struct{}),\n+\t\twantsHealthCheck: wantsHealthCheck,\n+\t\trand:             rand.New(rand.NewSource(time.Now().UnixNano())),\n+\t}\n+\tif stickyConfig != nil && stickyConfig.Cookie != nil {\n+\t\tbalancer.sticky = loadbalancer.NewSticky(*stickyConfig.Cookie)\n+\t}\n+\n+\treturn balancer\n+}\n+\n+// SetStatus sets on the balancer that its given child is now of the given\n+// status. balancerName is only needed for logging purposes.\n+func (b *Balancer) SetStatus(ctx context.Context, childName string, up bool) {\n+\tb.handlersMu.Lock()\n+\tdefer b.handlersMu.Unlock()\n+\n+\tupBefore := len(b.status) > 0\n+\n+\tstatus := \"DOWN\"\n+\tif up {\n+\t\tstatus = \"UP\"\n+\t}\n+\n+\tlog.Ctx(ctx).Debug().Msgf(\"Setting status of %s to %v\", childName, status)\n+\n+\tif up {\n+\t\tb.status[childName] = struct{}{}\n+\t} else {\n+\t\tdelete(b.status, childName)\n+\t}\n+\n+\tupAfter := len(b.status) > 0\n+\tstatus = \"DOWN\"\n+\tif upAfter {\n+\t\tstatus = \"UP\"\n+\t}\n+\n+\t// No Status Change\n+\tif upBefore == upAfter {\n+\t\t// We're still with the same status, no need to propagate\n+\t\tlog.Ctx(ctx).Debug().Msgf(\"Still %s, no need to propagate\", status)\n+\t\treturn\n+\t}\n+\n+\t// Status Change\n+\tlog.Ctx(ctx).Debug().Msgf(\"Propagating new %s status\", status)\n+\tfor _, fn := range b.updaters {\n+\t\tfn(upAfter)\n+\t}\n+}\n+\n+// RegisterStatusUpdater adds fn to the list of hooks that are run when the\n+// status of the Balancer changes.\n+// Not thread safe.\n+func (b *Balancer) RegisterStatusUpdater(fn func(up bool)) error {\n+\tif !b.wantsHealthCheck {\n+\t\treturn errors.New(\"healthCheck not enabled in config for this weighted service\")\n+\t}\n+\tb.updaters = append(b.updaters, fn)\n+\treturn nil\n+}\n+\n+var errNoAvailableServer = errors.New(\"no available server\")\n+\n+func (b *Balancer) nextServer() (*namedHandler, error) {\n+\t// We kept the same representation (map) as in the WRR strategy to improve maintainability.\n+\t// However, with the P2C strategy, we only need a slice of healthy servers.\n+\tb.handlersMu.RLock()\n+\tvar healthy []*namedHandler\n+\tfor _, h := range b.handlers {\n+\t\tif _, ok := b.status[h.name]; ok {\n+\t\t\tif _, fenced := b.fenced[h.name]; !fenced {\n+\t\t\t\thealthy = append(healthy, h)\n+\t\t\t}\n+\t\t}\n+\t}\n+\tb.handlersMu.RUnlock()\n+\n+\tif len(healthy) == 0 {\n+\t\treturn nil, errNoAvailableServer\n+\t}\n+\n+\t// If there is only one healthy server, return it.\n+\tif len(healthy) == 1 {\n+\t\treturn healthy[0], nil\n+\t}\n+\t// In order to not get the same backend twice, we make the second call to s.rand.IntN one fewer\n+\t// than the length of the slice. We then have to shift over the second index if it is equal or\n+\t// greater than the first index, wrapping round if needed.\n+\tn1, n2 := b.rand.Intn(len(healthy)), b.rand.Intn(len(healthy))\n+\tif n2 == n1 {\n+\t\tn2 = (n2 + 1) % len(healthy)\n+\t}\n+\n+\th1, h2 := healthy[n1], healthy[n2]\n+\t// Ensure h1 has fewer inflight requests than h2.\n+\tif h2.inflight.Load() < h1.inflight.Load() {\n+\t\tlog.Debug().Msgf(\"Service selected by P2C: %s\", h2.name)\n+\t\treturn h2, nil\n+\t}\n+\n+\tlog.Debug().Msgf(\"Service selected by P2C: %s\", h1.name)\n+\treturn h1, nil\n+}\n+\n+func (b *Balancer) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n+\tif b.sticky != nil {\n+\t\th, rewrite, err := b.sticky.StickyHandler(req)\n+\t\tif err != nil {\n+\t\t\tlog.Error().Err(err).Msg(\"Error while getting sticky handler\")\n+\t\t} else if h != nil {\n+\t\t\tif _, ok := b.status[h.Name]; ok {\n+\t\t\t\tif rewrite {\n+\t\t\t\t\tif err := b.sticky.WriteStickyCookie(rw, h.Name); err != nil {\n+\t\t\t\t\t\tlog.Error().Err(err).Msg(\"Writing sticky cookie\")\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\th.ServeHTTP(rw, req)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tserver, err := b.nextServer()\n+\tif err != nil {\n+\t\tif errors.Is(err, errNoAvailableServer) {\n+\t\t\thttp.Error(rw, errNoAvailableServer.Error(), http.StatusServiceUnavailable)\n+\t\t} else {\n+\t\t\thttp.Error(rw, err.Error(), http.StatusInternalServerError)\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\tif b.sticky != nil {\n+\t\tif err := b.sticky.WriteStickyCookie(rw, server.name); err != nil {\n+\t\t\tlog.Error().Err(err).Msg(\"Error while writing sticky cookie\")\n+\t\t}\n+\t}\n+\n+\tserver.ServeHTTP(rw, req)\n+}\n+\n+// AddServer adds a handler with a server.\n+func (b *Balancer) AddServer(name string, handler http.Handler, server dynamic.Server) {\n+\th := &namedHandler{Handler: handler, name: name}\n+\n+\tb.handlersMu.Lock()\n+\tb.handlers = append(b.handlers, h)\n+\tb.status[name] = struct{}{}\n+\tif server.Fenced {\n+\t\tb.fenced[name] = struct{}{}\n+\t}\n+\tb.handlersMu.Unlock()\n+\n+\tif b.sticky != nil {\n+\t\tb.sticky.AddHandler(name, h)\n+\t}\n+}\ndiff --git a/pkg/server/service/loadbalancer/sticky.go b/pkg/server/service/loadbalancer/sticky.go\nnew file mode 100644\nindex 0000000000..380234e184\n--- /dev/null\n+++ b/pkg/server/service/loadbalancer/sticky.go\n@@ -0,0 +1,179 @@\n+package loadbalancer\n+\n+import (\n+\t\"crypto/sha256\"\n+\t\"encoding/hex\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"hash/fnv\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\n+\t\"github.com/traefik/traefik/v3/pkg/config/dynamic\"\n+)\n+\n+// NamedHandler is a http.Handler with a name.\n+type NamedHandler struct {\n+\thttp.Handler\n+\n+\tName string\n+}\n+\n+// stickyCookie represents a sticky cookie.\n+type stickyCookie struct {\n+\tname     string\n+\tsecure   bool\n+\thttpOnly bool\n+\tsameSite http.SameSite\n+\tmaxAge   int\n+\tpath     string\n+\tdomain   string\n+}\n+\n+// Sticky ensures that client consistently interacts with the same HTTP handler by adding a sticky cookie to the response.\n+// This cookie allows subsequent requests from the same client to be routed to the same handler,\n+// enabling session persistence across multiple requests.\n+type Sticky struct {\n+\t// cookie is the sticky cookie configuration.\n+\tcookie *stickyCookie\n+\n+\t// References all the handlers by name and also by the hashed value of the name.\n+\thandlersMu             sync.RWMutex\n+\thashMap                map[string]string\n+\tstickyMap              map[string]*NamedHandler\n+\tcompatibilityStickyMap map[string]*NamedHandler\n+}\n+\n+// NewSticky creates a new Sticky instance.\n+func NewSticky(cookieConfig dynamic.Cookie) *Sticky {\n+\tcookie := &stickyCookie{\n+\t\tname:     cookieConfig.Name,\n+\t\tsecure:   cookieConfig.Secure,\n+\t\thttpOnly: cookieConfig.HTTPOnly,\n+\t\tsameSite: convertSameSite(cookieConfig.SameSite),\n+\t\tmaxAge:   cookieConfig.MaxAge,\n+\t\tpath:     \"/\",\n+\t\tdomain:   cookieConfig.Domain,\n+\t}\n+\tif cookieConfig.Path != nil {\n+\t\tcookie.path = *cookieConfig.Path\n+\t}\n+\n+\treturn &Sticky{\n+\t\tcookie:                 cookie,\n+\t\thashMap:                make(map[string]string),\n+\t\tstickyMap:              make(map[string]*NamedHandler),\n+\t\tcompatibilityStickyMap: make(map[string]*NamedHandler),\n+\t}\n+}\n+\n+// AddHandler adds a http.Handler to the sticky pool.\n+func (s *Sticky) AddHandler(name string, h http.Handler) {\n+\ts.handlersMu.Lock()\n+\tdefer s.handlersMu.Unlock()\n+\n+\tsha256HashedName := sha256Hash(name)\n+\ts.hashMap[name] = sha256HashedName\n+\n+\thandler := &NamedHandler{\n+\t\tHandler: h,\n+\t\tName:    name,\n+\t}\n+\n+\ts.stickyMap[sha256HashedName] = handler\n+\ts.compatibilityStickyMap[name] = handler\n+\n+\thashedName := fnvHash(name)\n+\ts.compatibilityStickyMap[hashedName] = handler\n+\n+\t// server.URL was fnv hashed in service.Manager\n+\t// so we can have \"double\" fnv hash in already existing cookies\n+\thashedName = fnvHash(hashedName)\n+\ts.compatibilityStickyMap[hashedName] = handler\n+}\n+\n+// StickyHandler returns the NamedHandler corresponding to the sticky cookie if one.\n+// It also returns a boolean which indicates if the sticky cookie has to be overwritten because it uses a deprecated hash algorithm.\n+func (s *Sticky) StickyHandler(req *http.Request) (*NamedHandler, bool, error) {\n+\tcookie, err := req.Cookie(s.cookie.name)\n+\tif err != nil && errors.Is(err, http.ErrNoCookie) {\n+\t\treturn nil, false, nil\n+\t}\n+\tif err != nil {\n+\t\treturn nil, false, fmt.Errorf(\"reading cookie: %w\", err)\n+\t}\n+\n+\ts.handlersMu.RLock()\n+\thandler, ok := s.stickyMap[cookie.Value]\n+\ts.handlersMu.RUnlock()\n+\n+\tif ok && handler != nil {\n+\t\treturn handler, false, nil\n+\t}\n+\n+\ts.handlersMu.RLock()\n+\thandler, ok = s.compatibilityStickyMap[cookie.Value]\n+\ts.handlersMu.RUnlock()\n+\n+\treturn handler, ok, nil\n+}\n+\n+// WriteStickyCookie writes a sticky cookie to the response to stick the client to the given handler name.\n+func (s *Sticky) WriteStickyCookie(rw http.ResponseWriter, name string) error {\n+\ts.handlersMu.RLock()\n+\thash, ok := s.hashMap[name]\n+\ts.handlersMu.RUnlock()\n+\tif !ok {\n+\t\treturn fmt.Errorf(\"no hash found for handler named %s\", name)\n+\t}\n+\n+\tcookie := &http.Cookie{\n+\t\tName:     s.cookie.name,\n+\t\tValue:    hash,\n+\t\tPath:     s.cookie.path,\n+\t\tDomain:   s.cookie.domain,\n+\t\tHttpOnly: s.cookie.httpOnly,\n+\t\tSecure:   s.cookie.secure,\n+\t\tSameSite: s.cookie.sameSite,\n+\t\tMaxAge:   s.cookie.maxAge,\n+\t}\n+\thttp.SetCookie(rw, cookie)\n+\n+\treturn nil\n+}\n+\n+func convertSameSite(sameSite string) http.SameSite {\n+\tswitch sameSite {\n+\tcase \"none\":\n+\t\treturn http.SameSiteNoneMode\n+\tcase \"lax\":\n+\t\treturn http.SameSiteLaxMode\n+\tcase \"strict\":\n+\t\treturn http.SameSiteStrictMode\n+\tdefault:\n+\t\treturn http.SameSiteDefaultMode\n+\t}\n+}\n+\n+// fnvHash returns the FNV-64 hash of the input string.\n+func fnvHash(input string) string {\n+\thasher := fnv.New64()\n+\t// We purposely ignore the error because the implementation always returns nil.\n+\t_, _ = hasher.Write([]byte(input))\n+\n+\treturn strconv.FormatUint(hasher.Sum64(), 16)\n+}\n+\n+// sha256 returns the SHA-256 hash, truncated to 16 characters, of the input string.\n+func sha256Hash(input string) string {\n+\thash := sha256.New()\n+\t// We purposely ignore the error because the implementation always returns nil.\n+\t_, _ = hash.Write([]byte(input))\n+\n+\thashedInput := hex.EncodeToString(hash.Sum(nil))\n+\tif len(hashedInput) < 16 {\n+\t\treturn hashedInput\n+\t}\n+\treturn hashedInput[:16]\n+}\ndiff --git a/pkg/server/service/loadbalancer/wrr/wrr.go b/pkg/server/service/loadbalancer/wrr/wrr.go\nindex b26b06320a..8206b97aae 100644\n--- a/pkg/server/service/loadbalancer/wrr/wrr.go\n+++ b/pkg/server/service/loadbalancer/wrr/wrr.go\n@@ -3,47 +3,20 @@ package wrr\n import (\n \t\"container/heap\"\n \t\"context\"\n-\t\"crypto/sha256\"\n-\t\"encoding/hex\"\n \t\"errors\"\n-\t\"hash/fnv\"\n \t\"net/http\"\n-\t\"strconv\"\n \t\"sync\"\n \n \t\"github.com/rs/zerolog/log\"\n \t\"github.com/traefik/traefik/v3/pkg/config/dynamic\"\n+\t\"github.com/traefik/traefik/v3/pkg/server/service/loadbalancer\"\n )\n \n type namedHandler struct {\n \thttp.Handler\n-\tname       string\n-\thashedName string\n-\tweight     float64\n-\tdeadline   float64\n-}\n-\n-type stickyCookie struct {\n \tname     string\n-\tsecure   bool\n-\thttpOnly bool\n-\tsameSite string\n-\tmaxAge   int\n-\tpath     string\n-\tdomain   string\n-}\n-\n-func convertSameSite(sameSite string) http.SameSite {\n-\tswitch sameSite {\n-\tcase \"none\":\n-\t\treturn http.SameSiteNoneMode\n-\tcase \"lax\":\n-\t\treturn http.SameSiteLaxMode\n-\tcase \"strict\":\n-\t\treturn http.SameSiteStrictMode\n-\tdefault:\n-\t\treturn http.SameSiteDefaultMode\n-\t}\n+\tweight   float64\n+\tdeadline float64\n }\n \n // Balancer is a WeightedRoundRobin load balancer based on Earliest Deadline First (EDF).\n@@ -52,15 +25,10 @@ func convertSameSite(sameSite string) http.SameSite {\n // Entries have deadlines set at currentDeadline + 1 / weight,\n // providing weighted round-robin behavior with floating point weights and an O(log n) pick time.\n type Balancer struct {\n-\tstickyCookie     *stickyCookie\n \twantsHealthCheck bool\n \n \thandlersMu sync.RWMutex\n-\t// References all the handlers by name and also by the hashed value of the name.\n-\tstickyMap              map[string]*namedHandler\n-\tcompatibilityStickyMap map[string]*namedHandler\n-\thandlers               []*namedHandler\n-\tcurDeadline            float64\n+\thandlers   []*namedHandler\n \t// status is a record of which child services of the Balancer are healthy, keyed\n \t// by name of child service. A service is initially added to the map when it is\n \t// created via Add, and it is later removed or added to the map as needed,\n@@ -71,31 +39,21 @@ type Balancer struct {\n \tupdaters []func(bool)\n \t// fenced is the list of terminating yet still serving child services.\n \tfenced map[string]struct{}\n+\n+\tsticky *loadbalancer.Sticky\n+\n+\tcurDeadline float64\n }\n \n // New creates a new load balancer.\n-func New(sticky *dynamic.Sticky, wantHealthCheck bool) *Balancer {\n+func New(sticky *dynamic.Sticky, wantsHealthCheck bool) *Balancer {\n \tbalancer := &Balancer{\n \t\tstatus:           make(map[string]struct{}),\n \t\tfenced:           make(map[string]struct{}),\n-\t\twantsHealthCheck: wantHealthCheck,\n+\t\twantsHealthCheck: wantsHealthCheck,\n \t}\n \tif sticky != nil && sticky.Cookie != nil {\n-\t\tbalancer.stickyCookie = &stickyCookie{\n-\t\t\tname:     sticky.Cookie.Name,\n-\t\t\tsecure:   sticky.Cookie.Secure,\n-\t\t\thttpOnly: sticky.Cookie.HTTPOnly,\n-\t\t\tsameSite: sticky.Cookie.SameSite,\n-\t\t\tmaxAge:   sticky.Cookie.MaxAge,\n-\t\t\tpath:     \"/\",\n-\t\t\tdomain:   sticky.Cookie.Domain,\n-\t\t}\n-\t\tif sticky.Cookie.Path != nil {\n-\t\t\tbalancer.stickyCookie.path = *sticky.Cookie.Path\n-\t\t}\n-\n-\t\tbalancer.stickyMap = make(map[string]*namedHandler)\n-\t\tbalancer.compatibilityStickyMap = make(map[string]*namedHandler)\n+\t\tbalancer.sticky = loadbalancer.NewSticky(*sticky.Cookie)\n \t}\n \n \treturn balancer\n@@ -216,43 +174,21 @@ func (b *Balancer) nextServer() (*namedHandler, error) {\n \treturn handler, nil\n }\n \n-func (b *Balancer) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n-\tif b.stickyCookie != nil {\n-\t\tcookie, err := req.Cookie(b.stickyCookie.name)\n-\n-\t\tif err != nil && !errors.Is(err, http.ErrNoCookie) {\n-\t\t\tlog.Warn().Err(err).Msg(\"Error while reading cookie\")\n-\t\t}\n-\n-\t\tif err == nil && cookie != nil {\n-\t\t\tb.handlersMu.RLock()\n-\t\t\thandler, ok := b.stickyMap[cookie.Value]\n-\t\t\tb.handlersMu.RUnlock()\n-\n-\t\t\tif ok && handler != nil {\n-\t\t\t\tb.handlersMu.RLock()\n-\t\t\t\t_, isHealthy := b.status[handler.name]\n-\t\t\t\tb.handlersMu.RUnlock()\n-\t\t\t\tif isHealthy {\n-\t\t\t\t\thandler.ServeHTTP(w, req)\n-\t\t\t\t\treturn\n+func (b *Balancer) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n+\tif b.sticky != nil {\n+\t\th, rewrite, err := b.sticky.StickyHandler(req)\n+\t\tif err != nil {\n+\t\t\tlog.Error().Err(err).Msg(\"Error while getting sticky handler\")\n+\t\t} else if h != nil {\n+\t\t\tif _, ok := b.status[h.Name]; ok {\n+\t\t\t\tif rewrite {\n+\t\t\t\t\tif err := b.sticky.WriteStickyCookie(rw, h.Name); err != nil {\n+\t\t\t\t\t\tlog.Error().Err(err).Msg(\"Writing sticky cookie\")\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tb.handlersMu.RLock()\n-\t\t\thandler, ok = b.compatibilityStickyMap[cookie.Value]\n-\t\t\tb.handlersMu.RUnlock()\n \n-\t\t\tif ok && handler != nil {\n-\t\t\t\tb.handlersMu.RLock()\n-\t\t\t\t_, isHealthy := b.status[handler.name]\n-\t\t\t\tb.handlersMu.RUnlock()\n-\t\t\t\tif isHealthy {\n-\t\t\t\t\tb.writeStickyCookie(w, handler)\n-\n-\t\t\t\t\thandler.ServeHTTP(w, req)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n+\t\t\t\th.ServeHTTP(rw, req)\n+\t\t\t\treturn\n \t\t\t}\n \t\t}\n \t}\n@@ -260,32 +196,25 @@ func (b *Balancer) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n \tserver, err := b.nextServer()\n \tif err != nil {\n \t\tif errors.Is(err, errNoAvailableServer) {\n-\t\t\thttp.Error(w, errNoAvailableServer.Error(), http.StatusServiceUnavailable)\n+\t\t\thttp.Error(rw, errNoAvailableServer.Error(), http.StatusServiceUnavailable)\n \t\t} else {\n-\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n+\t\t\thttp.Error(rw, err.Error(), http.StatusInternalServerError)\n \t\t}\n \t\treturn\n \t}\n \n-\tif b.stickyCookie != nil {\n-\t\tb.writeStickyCookie(w, server)\n+\tif b.sticky != nil {\n+\t\tif err := b.sticky.WriteStickyCookie(rw, server.name); err != nil {\n+\t\t\tlog.Error().Err(err).Msg(\"Error while writing sticky cookie\")\n+\t\t}\n \t}\n \n-\tserver.ServeHTTP(w, req)\n+\tserver.ServeHTTP(rw, req)\n }\n \n-func (b *Balancer) writeStickyCookie(w http.ResponseWriter, handler *namedHandler) {\n-\tcookie := &http.Cookie{\n-\t\tName:     b.stickyCookie.name,\n-\t\tValue:    handler.hashedName,\n-\t\tPath:     b.stickyCookie.path,\n-\t\tHttpOnly: b.stickyCookie.httpOnly,\n-\t\tSecure:   b.stickyCookie.secure,\n-\t\tSameSite: convertSameSite(b.stickyCookie.sameSite),\n-\t\tMaxAge:   b.stickyCookie.maxAge,\n-\t\tDomain:   b.stickyCookie.domain,\n-\t}\n-\thttp.SetCookie(w, cookie)\n+// AddServer adds a handler with a server.\n+func (b *Balancer) AddServer(name string, handler http.Handler, server dynamic.Server) {\n+\tb.Add(name, handler, server.Weight, server.Fenced)\n }\n \n // Add adds a handler.\n@@ -309,41 +238,9 @@ func (b *Balancer) Add(name string, handler http.Handler, weight *int, fenced bo\n \tif fenced {\n \t\tb.fenced[name] = struct{}{}\n \t}\n-\n-\tif b.stickyCookie != nil {\n-\t\tsha256HashedName := sha256Hash(name)\n-\t\th.hashedName = sha256HashedName\n-\n-\t\tb.stickyMap[sha256HashedName] = h\n-\t\tb.compatibilityStickyMap[name] = h\n-\n-\t\thashedName := fnvHash(name)\n-\t\tb.compatibilityStickyMap[hashedName] = h\n-\n-\t\t// server.URL was fnv hashed in service.Manager\n-\t\t// so we can have \"double\" fnv hash in already existing cookies\n-\t\thashedName = fnvHash(hashedName)\n-\t\tb.compatibilityStickyMap[hashedName] = h\n-\t}\n \tb.handlersMu.Unlock()\n-}\n-\n-func fnvHash(input string) string {\n-\thasher := fnv.New64()\n-\t// We purposely ignore the error because the implementation always returns nil.\n-\t_, _ = hasher.Write([]byte(input))\n-\n-\treturn strconv.FormatUint(hasher.Sum64(), 16)\n-}\n-\n-func sha256Hash(input string) string {\n-\thash := sha256.New()\n-\t// We purposely ignore the error because the implementation always returns nil.\n-\t_, _ = hash.Write([]byte(input))\n \n-\thashedInput := hex.EncodeToString(hash.Sum(nil))\n-\tif len(hashedInput) < 16 {\n-\t\treturn hashedInput\n+\tif b.sticky != nil {\n+\t\tb.sticky.AddHandler(name, handler)\n \t}\n-\treturn hashedInput[:16]\n }\ndiff --git a/pkg/server/service/service.go b/pkg/server/service/service.go\nindex 245cb08f6b..c44a0a7e59 100644\n--- a/pkg/server/service/service.go\n+++ b/pkg/server/service/service.go\n@@ -29,6 +29,7 @@ import (\n \t\"github.com/traefik/traefik/v3/pkg/server/provider\"\n \t\"github.com/traefik/traefik/v3/pkg/server/service/loadbalancer/failover\"\n \t\"github.com/traefik/traefik/v3/pkg/server/service/loadbalancer/mirror\"\n+\t\"github.com/traefik/traefik/v3/pkg/server/service/loadbalancer/p2c\"\n \t\"github.com/traefik/traefik/v3/pkg/server/service/loadbalancer/wrr\"\n \t\"google.golang.org/grpc/status\"\n )\n@@ -304,6 +305,13 @@ func (m *Manager) getServiceHandler(ctx context.Context, service dynamic.WRRServ\n \t}\n }\n \n+type serverBalancer interface {\n+\thttp.Handler\n+\thealthcheck.StatusSetter\n+\n+\tAddServer(name string, handler http.Handler, server dynamic.Server)\n+}\n+\n func (m *Manager) getLoadBalancerServiceHandler(ctx context.Context, serviceName string, info *runtime.ServiceInfo) (http.Handler, error) {\n \tservice := info.LoadBalancer\n \n@@ -330,7 +338,18 @@ func (m *Manager) getLoadBalancerServiceHandler(ctx context.Context, serviceName\n \t\tpassHostHeader = *service.PassHostHeader\n \t}\n \n-\tlb := wrr.New(service.Sticky, service.HealthCheck != nil)\n+\tvar lb serverBalancer\n+\tswitch service.Strategy {\n+\t// Here we are handling the empty value to comply with providers that are not applying defaults (e.g. REST provider)\n+\t// TODO: remove this when all providers apply default values.\n+\tcase dynamic.BalancerStrategyWRR, \"\":\n+\t\tlb = wrr.New(service.Sticky, service.HealthCheck != nil)\n+\tcase dynamic.BalancerStrategyP2C:\n+\t\tlb = p2c.New(service.Sticky, service.HealthCheck != nil)\n+\tdefault:\n+\t\treturn nil, fmt.Errorf(\"unsupported load-balancer strategy %q\", service.Strategy)\n+\t}\n+\n \thealthCheckTargets := make(map[string]*url.URL)\n \n \tfor i, server := range shuffle(service.Servers, m.rand) {\n@@ -385,7 +404,7 @@ func (m *Manager) getLoadBalancerServiceHandler(ctx context.Context, serviceName\n \t\t\tproxy, _ = capture.Wrap(proxy)\n \t\t}\n \n-\t\tlb.Add(server.URL, proxy, server.Weight, server.Fenced)\n+\t\tlb.AddServer(server.URL, proxy, server)\n \n \t\t// servers are considered UP by default.\n \t\tinfo.UpdateServerStatus(target.String(), runtime.StatusUp)\n", "instance_id": "traefik__traefik-11547", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in its intent to add HTTP server load balancing algorithms to the Traefik project. It identifies two main issues: the configuration schema lacking support for specific load balancing algorithms and the limitations of the `healthcheck.Balancer` interface. The goal of enhancing load balancing capabilities is evident, and the proposer provides references to community discussions and a sandbox for context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input/output formats for the new load balancing configurations or provide concrete examples of how the algorithms (e.g., weighted round-robin or power of two choices) should behave under specific conditions. Additionally, while the code changes show the implementation direction (e.g., adding `wrr` and `p2c` strategies), the problem statement lacks detailed requirements or constraints for these algorithms, such as performance expectations or specific edge cases to handle. Overall, it is clear enough to understand the intent and scope but falls short of being comprehensive due to these minor gaps.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category, due to several factors:\n\n1. **Scope and Depth of Code Changes**: The code changes span multiple files and modules within the Traefik codebase, including configuration definitions, documentation, and core load balancing logic. The modifications are not limited to a single function or file but involve updates to dynamic configuration schemas (e.g., Kubernetes CRDs, YAML/TOML configs), provider implementations, and the introduction of new load balancing algorithms (`p2c` and refactoring of `wrr`). Additionally, there is a significant refactoring of sticky session handling into a reusable component. While the changes do not appear to fundamentally alter the system's architecture, they impact critical components like load balancing, which requires a deep understanding of how different parts of the codebase interact (e.g., configuration parsing, service management, and HTTP handling).\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several advanced concepts, such as HTTP load balancing algorithms (weighted round-robin and power of two choices), sticky session management, and health checking mechanisms. It also involves understanding Traefik's dynamic configuration system, Kubernetes CRD definitions, and provider-specific logic. The implementation of the `p2c` algorithm introduces a non-trivial selection strategy based on in-flight requests, which requires careful handling of concurrency (e.g., using `atomic.Int64` for request counting) and random selection logic. Additionally, the refactoring of sticky session logic into a separate module demonstrates a need for design patterns like separation of concerns, and the code must handle backward compatibility with deprecated strategies (e.g., `RoundRobin`).\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes reveal several that need to be addressed, such as handling scenarios with no available servers (resulting in HTTP 503 errors), managing sticky sessions when servers become unhealthy, and ensuring backward compatibility with existing configurations. The `p2c` implementation must handle edge cases like having only one or two servers available, and the sticky session logic must manage cookie mismatches or invalid cookies gracefully. These aspects add complexity to the implementation, as they require robust error handling and status propagation logic (e.g., updating parent components on status changes).\n\n4. **Overall Complexity**: The task requires a deep understanding of Traefik's internals, particularly its service and load balancing architecture. Implementing a new load balancing strategy like `p2c` is non-trivial, as it involves balancing performance (constant-time selection) with correctness (choosing the least loaded server). The changes also necessitate extensive updates to documentation and configuration schemas to ensure users can adopt the new features, which adds to the workload. While not at the extreme end of difficulty (e.g., redesigning a distributed system), this problem demands significant expertise in Go, HTTP handling, and load balancing concepts, along with careful consideration of concurrency and state management.\n\nIn summary, the combination of widespread code changes, advanced technical concepts, and implicit edge case handling makes this a hard problem, warranting a score of 0.75. It is challenging but feasible for a senior engineer with relevant experience in load balancing and proxy systems.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support logging via OpenTelemetry\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you expect to see?\n\nI was very happy when I recently discovered that Traefik Proxy v3 brings support for exporting traces and metrics to the OpenTelemetry collector. Given that one of the other big ticket items for v3 is improved logging, why not also support exporting logs via OTLP to the collector?\r\n\r\nIn our environments we route all telemetry via collectors where possible. This strategy allows the collector to act as an abstraction over whatever services we then forward the logs, metrics and traces out to. It also gives us one place to manage adding/removing labels, and filtering out noise. Overall, supporting applications and the platform is made simpler.\r\n\r\nAn example of a Traefik problem I would like to solve using logging to OTLP is how the Traefik and access logs are handled today. I'm in a kubernetes environment, and can only scrape logs output from Traefik currently. Traefik doesn't apply any metadata to the logs it outputs so it isn't easy to separate the two log types during collection or querying. If logs were instead pushed via OTLP with attributes marking the log type, this would be solved.\r\n\n", "patch": "diff --git a/cmd/traefik/logger.go b/cmd/traefik/logger.go\nindex 8ba6ac6826..58e664d795 100644\n--- a/cmd/traefik/logger.go\n+++ b/cmd/traefik/logger.go\n@@ -1,6 +1,7 @@\n package main\n \n import (\n+\t\"fmt\"\n \t\"io\"\n \tstdlog \"log\"\n \t\"os\"\n@@ -20,12 +21,13 @@ func init() {\n \tzerolog.SetGlobalLevel(zerolog.ErrorLevel)\n }\n \n-func setupLogger(staticConfiguration *static.Configuration) {\n+func setupLogger(staticConfiguration *static.Configuration) error {\n \t// configure log format\n \tw := getLogWriter(staticConfiguration)\n \n \t// configure log level\n \tlogLevel := getLogLevel(staticConfiguration)\n+\tzerolog.SetGlobalLevel(logLevel)\n \n \t// create logger\n \tlogCtx := zerolog.New(w).With().Timestamp()\n@@ -34,8 +36,16 @@ func setupLogger(staticConfiguration *static.Configuration) {\n \t}\n \n \tlog.Logger = logCtx.Logger().Level(logLevel)\n+\n+\tif staticConfiguration.Log != nil && staticConfiguration.Log.OTLP != nil {\n+\t\tvar err error\n+\t\tlog.Logger, err = logs.SetupOTelLogger(log.Logger, staticConfiguration.Log.OTLP)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"setting up OpenTelemetry logger: %w\", err)\n+\t\t}\n+\t}\n+\n \tzerolog.DefaultContextLogger = &log.Logger\n-\tzerolog.SetGlobalLevel(logLevel)\n \n \t// Global logrus replacement (related to lib like go-rancher-metadata, docker, etc.)\n \tlogrus.StandardLogger().Out = logs.NoLevel(log.Logger, zerolog.DebugLevel)\n@@ -43,11 +53,16 @@ func setupLogger(staticConfiguration *static.Configuration) {\n \t// configure default standard log.\n \tstdlog.SetFlags(stdlog.Lshortfile | stdlog.LstdFlags)\n \tstdlog.SetOutput(logs.NoLevel(log.Logger, zerolog.DebugLevel))\n+\n+\treturn nil\n }\n \n func getLogWriter(staticConfiguration *static.Configuration) io.Writer {\n-\tvar w io.Writer = os.Stdout\n+\tif staticConfiguration.Log != nil && staticConfiguration.Log.OTLP != nil {\n+\t\treturn io.Discard\n+\t}\n \n+\tvar w io.Writer = os.Stdout\n \tif staticConfiguration.Log != nil && len(staticConfiguration.Log.FilePath) > 0 {\n \t\t_, _ = os.OpenFile(staticConfiguration.Log.FilePath, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0o666)\n \t\tw = &lumberjack.Logger{\ndiff --git a/cmd/traefik/traefik.go b/cmd/traefik/traefik.go\nindex 96953fb0a4..45df4fbf2c 100644\n--- a/cmd/traefik/traefik.go\n+++ b/cmd/traefik/traefik.go\n@@ -90,7 +90,9 @@ Complete documentation is available at https://traefik.io`,\n }\n \n func runCmd(staticConfiguration *static.Configuration) error {\n-\tsetupLogger(staticConfiguration)\n+\tif err := setupLogger(staticConfiguration); err != nil {\n+\t\treturn fmt.Errorf(\"setting up logger: %w\", err)\n+\t}\n \n \thttp.DefaultTransport.(*http.Transport).Proxy = http.ProxyFromEnvironment\n \ndiff --git a/docs/content/migration/v3.md b/docs/content/migration/v3.md\nindex c72296e7bd..939cb5f73c 100644\n--- a/docs/content/migration/v3.md\n+++ b/docs/content/migration/v3.md\n@@ -167,3 +167,9 @@ Please refer to the Forwarded headers [documentation](../routing/entrypoints.md#\n \n In `v3.3`, the `acme.dnsChallenge.delaybeforecheck` and `acme.dnsChallenge.disablepropagationcheck` options of the ACME certificate resolver are deprecated, \n please use respectively `acme.dnsChallenge.propagation.delayBeforeCheck` and `acme.dnsChallenge.propagation.disableAllChecks` options instead.\n+\n+### Tracing Global Attributes\n+\n+In `v3.3`, the `tracing.globalAttributes` option has been deprecated, please use the `tracing.resourceAttributes` option instead.\n+The `tracing.globalAttributes` option is misleading as its name does not reflect the operation of adding resource attributes to be sent to the collector,\n+and will be removed in the next major version.\ndiff --git a/docs/content/observability/access-logs.md b/docs/content/observability/access-logs.md\nindex 8a24603fa2..6f659a9c16 100644\n--- a/docs/content/observability/access-logs.md\n+++ b/docs/content/observability/access-logs.md\n@@ -30,7 +30,7 @@ accessLog: {}\n \n _Optional, Default=\"false\"_\n \n-Enables accessLogs for internal resources (e.g.: `ping@internal`).\n+Enables access logs for internal resources (e.g.: `ping@internal`).\n \n ```yaml tab=\"File (YAML)\"\n accesslog:\n@@ -306,4 +306,401 @@ services:\n       - /var/run/docker.sock:/var/run/docker.sock\n ```\n \n+## OpenTelemetry\n+\n+To enable the OpenTelemetry Logger for access logs:\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp: {}\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp]\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp=true\n+```\n+\n+!!! info \"Default protocol\"\n+\n+    The OpenTelemetry Logger exporter will export access logs to the collector using HTTPS by default to https://localhost:4318/v1/logs, see the [gRPC Section](#grpc-configuration) to use gRPC.\n+\n+### HTTP configuration\n+\n+_Optional_\n+\n+This instructs the exporter to send access logs to the OpenTelemetry Collector using HTTP.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http: {}\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http]\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http=true\n+```\n+\n+#### `endpoint`\n+\n+_Optional, Default=\"`https://localhost:4318/v1/logs`\", Format=\"`<scheme>://<host>:<port><path>`\"_\n+\n+URL of the OpenTelemetry Collector to send access logs to.\n+\n+!!! info \"Insecure mode\"\n+\n+    To disable TLS, use `http://` instead of `https://` in the `endpoint` configuration.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http:\n+      endpoint: https://collector:4318/v1/logs\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http]\n+  endpoint = \"https://collector:4318/v1/logs\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http.endpoint=https://collector:4318/v1/logs\n+```\n+\n+#### `headers`\n+\n+_Optional, Default={}_\n+\n+Additional headers sent with access logs by the exporter to the OpenTelemetry Collector.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http:\n+      headers:\n+        foo: bar\n+        baz: buz\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http.headers]\n+  foo = \"bar\"\n+  baz = \"buz\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http.headers.foo=bar --accesslog.otlp.http.headers.baz=buz\n+```\n+\n+#### `tls`\n+\n+_Optional_\n+\n+Defines the Client TLS configuration used by the exporter to send access logs to the OpenTelemetry Collector.\n+\n+##### `ca`\n+\n+_Optional_\n+\n+`ca` is the path to the certificate authority used for the secure connection to the OpenTelemetry Collector,\n+it defaults to the system bundle.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http:\n+      tls:\n+        ca: path/to/ca.crt\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http.tls]\n+  ca = \"path/to/ca.crt\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http.tls.ca=path/to/ca.crt\n+```\n+\n+##### `cert`\n+\n+_Optional_\n+\n+`cert` is the path to the public certificate used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `key` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http.tls.cert=path/to/foo.cert\n+--accesslog.otlp.http.tls.key=path/to/foo.key\n+```\n+\n+##### `key`\n+\n+_Optional_\n+\n+`key` is the path to the private key used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `cert` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http.tls.cert=path/to/foo.cert\n+--accesslog.otlp.http.tls.key=path/to/foo.key\n+```\n+\n+##### `insecureSkipVerify`\n+\n+_Optional, Default=false_\n+\n+If `insecureSkipVerify` is `true`,\n+the TLS connection to the OpenTelemetry Collector accepts any certificate presented by the server regardless of the hostnames it covers.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    http:\n+      tls:\n+        insecureSkipVerify: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.http.tls]\n+  insecureSkipVerify = true\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.http.tls.insecureSkipVerify=true\n+```\n+\n+### gRPC configuration\n+\n+_Optional_\n+\n+This instructs the exporter to send access logs to the OpenTelemetry Collector using gRPC.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc: {}\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc]\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc=true\n+```\n+\n+#### `endpoint`\n+\n+_Required, Default=\"localhost:4317\", Format=\"`<host>:<port>`\"_\n+\n+Address of the OpenTelemetry Collector to send access logs to.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      endpoint: localhost:4317\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc]\n+  endpoint = \"localhost:4317\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.endpoint=localhost:4317\n+```\n+\n+#### `insecure`\n+\n+_Optional, Default=false_\n+\n+Allows exporter to send access logs to the OpenTelemetry Collector without using a secured protocol.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      insecure: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc]\n+  insecure = true\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.insecure=true\n+```\n+\n+#### `headers`\n+\n+_Optional, Default={}_\n+\n+Additional headers sent with access logs by the exporter to the OpenTelemetry Collector.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      headers:\n+        foo: bar\n+        baz: buz\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc.headers]\n+  foo = \"bar\"\n+  baz = \"buz\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.headers.foo=bar --accesslog.otlp.grpc.headers.baz=buz\n+```\n+\n+#### `tls`\n+\n+_Optional_\n+\n+Defines the Client TLS configuration used by the exporter to send access logs to the OpenTelemetry Collector.\n+\n+##### `ca`\n+\n+_Optional_\n+\n+`ca` is the path to the certificate authority used for the secure connection to the OpenTelemetry Collector,\n+it defaults to the system bundle.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      tls:\n+        ca: path/to/ca.crt\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc.tls]\n+  ca = \"path/to/ca.crt\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.tls.ca=path/to/ca.crt\n+```\n+\n+##### `cert`\n+\n+_Optional_\n+\n+`cert` is the path to the public certificate used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `key` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.tls.cert=path/to/foo.cert\n+--accesslog.otlp.grpc.tls.key=path/to/foo.key\n+```\n+\n+##### `key`\n+\n+_Optional_\n+\n+`key` is the path to the private key used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `cert` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.tls.cert=path/to/foo.cert\n+--accesslog.otlp.grpc.tls.key=path/to/foo.key\n+```\n+\n+##### `insecureSkipVerify`\n+\n+_Optional, Default=false_\n+\n+If `insecureSkipVerify` is `true`,\n+the TLS connection to the OpenTelemetry Collector accepts any certificate presented by the server regardless of the hostnames it covers.\n+\n+```yaml tab=\"File (YAML)\"\n+accesslog:\n+  otlp:\n+    grpc:\n+      tls:\n+        insecureSkipVerify: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[accesslog.otlp.grpc.tls]\n+  insecureSkipVerify = true\n+```\n+\n+```bash tab=\"CLI\"\n+--accesslog.otlp.grpc.tls.insecureSkipVerify=true\n+```\n+\n {!traefik-for-business-applications.md!}\ndiff --git a/docs/content/observability/logs.md b/docs/content/observability/logs.md\nindex 4ee168e66e..8b774c406f 100644\n--- a/docs/content/observability/logs.md\n+++ b/docs/content/observability/logs.md\n@@ -181,4 +181,401 @@ log:\n --log.compress=true\n ```\n \n+## OpenTelemetry\n+\n+To enable the OpenTelemetry Logger for logs:\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp: {}\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp]\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp=true\n+```\n+\n+!!! info \"Default protocol\"\n+\n+    The OpenTelemetry Logger exporter will export logs to the collector using HTTPS by default to https://localhost:4318/v1/logs, see the [gRPC Section](#grpc-configuration) to use gRPC.\n+\n+### HTTP configuration\n+\n+_Optional_\n+\n+This instructs the exporter to send logs to the OpenTelemetry Collector using HTTP.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http: {}\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http]\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http=true\n+```\n+\n+#### `endpoint`\n+\n+_Optional, Default=\"`https://localhost:4318/v1/logs`\", Format=\"`<scheme>://<host>:<port><path>`\"_\n+\n+URL of the OpenTelemetry Collector to send logs to.\n+\n+!!! info \"Insecure mode\"\n+\n+    To disable TLS, use `http://` instead of `https://` in the `endpoint` configuration.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http:\n+      endpoint: https://collector:4318/v1/logs\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http]\n+  endpoint = \"https://collector:4318/v1/logs\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http.endpoint=https://collector:4318/v1/logs\n+```\n+\n+#### `headers`\n+\n+_Optional, Default={}_\n+\n+Additional headers sent with logs by the exporter to the OpenTelemetry Collector.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http:\n+      headers:\n+        foo: bar\n+        baz: buz\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http.headers]\n+  foo = \"bar\"\n+  baz = \"buz\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http.headers.foo=bar --log.otlp.http.headers.baz=buz\n+```\n+\n+#### `tls`\n+\n+_Optional_\n+\n+Defines the Client TLS configuration used by the exporter to send logs to the OpenTelemetry Collector.\n+\n+##### `ca`\n+\n+_Optional_\n+\n+`ca` is the path to the certificate authority used for the secure connection to the OpenTelemetry Collector,\n+it defaults to the system bundle.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http:\n+      tls:\n+        ca: path/to/ca.crt\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http.tls]\n+  ca = \"path/to/ca.crt\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http.tls.ca=path/to/ca.crt\n+```\n+\n+##### `cert`\n+\n+_Optional_\n+\n+`cert` is the path to the public certificate used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `key` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http.tls.cert=path/to/foo.cert\n+--log.otlp.http.tls.key=path/to/foo.key\n+```\n+\n+##### `key`\n+\n+_Optional_\n+\n+`key` is the path to the private key used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `cert` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http.tls.cert=path/to/foo.cert\n+--log.otlp.http.tls.key=path/to/foo.key\n+```\n+\n+##### `insecureSkipVerify`\n+\n+_Optional, Default=false_\n+\n+If `insecureSkipVerify` is `true`,\n+the TLS connection to the OpenTelemetry Collector accepts any certificate presented by the server regardless of the hostnames it covers.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    http:\n+      tls:\n+        insecureSkipVerify: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.http.tls]\n+  insecureSkipVerify = true\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.http.tls.insecureSkipVerify=true\n+```\n+\n+### gRPC configuration\n+\n+_Optional_\n+\n+This instructs the exporter to send logs to the OpenTelemetry Collector using gRPC.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc: {}\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc]\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc=true\n+```\n+\n+#### `endpoint`\n+\n+_Required, Default=\"localhost:4317\", Format=\"`<host>:<port>`\"_\n+\n+Address of the OpenTelemetry Collector to send logs to.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      endpoint: localhost:4317\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc]\n+  endpoint = \"localhost:4317\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.endpoint=localhost:4317\n+```\n+\n+#### `insecure`\n+\n+_Optional, Default=false_\n+\n+Allows exporter to send logs to the OpenTelemetry Collector without using a secured protocol.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      insecure: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc]\n+  insecure = true\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.insecure=true\n+```\n+\n+#### `headers`\n+\n+_Optional, Default={}_\n+\n+Additional headers sent with logs by the exporter to the OpenTelemetry Collector.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      headers:\n+        foo: bar\n+        baz: buz\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc.headers]\n+  foo = \"bar\"\n+  baz = \"buz\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.headers.foo=bar --log.otlp.grpc.headers.baz=buz\n+```\n+\n+#### `tls`\n+\n+_Optional_\n+\n+Defines the Client TLS configuration used by the exporter to send logs to the OpenTelemetry Collector.\n+\n+##### `ca`\n+\n+_Optional_\n+\n+`ca` is the path to the certificate authority used for the secure connection to the OpenTelemetry Collector,\n+it defaults to the system bundle.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      tls:\n+        ca: path/to/ca.crt\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc.tls]\n+  ca = \"path/to/ca.crt\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.tls.ca=path/to/ca.crt\n+```\n+\n+##### `cert`\n+\n+_Optional_\n+\n+`cert` is the path to the public certificate used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `key` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.tls.cert=path/to/foo.cert\n+--log.otlp.grpc.tls.key=path/to/foo.key\n+```\n+\n+##### `key`\n+\n+_Optional_\n+\n+`key` is the path to the private key used for the secure connection to the OpenTelemetry Collector.\n+When using this option, setting the `cert` option is required.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      tls:\n+        cert: path/to/foo.cert\n+        key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc.tls]\n+  cert = \"path/to/foo.cert\"\n+  key = \"path/to/foo.key\"\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.tls.cert=path/to/foo.cert\n+--log.otlp.grpc.tls.key=path/to/foo.key\n+```\n+\n+##### `insecureSkipVerify`\n+\n+_Optional, Default=false_\n+\n+If `insecureSkipVerify` is `true`,\n+the TLS connection to the OpenTelemetry Collector accepts any certificate presented by the server regardless of the hostnames it covers.\n+\n+```yaml tab=\"File (YAML)\"\n+log:\n+  otlp:\n+    grpc:\n+      tls:\n+        insecureSkipVerify: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[log.otlp.grpc.tls]\n+  insecureSkipVerify = true\n+```\n+\n+```bash tab=\"CLI\"\n+--log.otlp.grpc.tls.insecureSkipVerify=true\n+```\n+\n {!traefik-for-business-applications.md!}\ndiff --git a/docs/content/observability/metrics/opentelemetry.md b/docs/content/observability/metrics/opentelemetry.md\nindex e71e5ec63b..0bcb965878 100644\n--- a/docs/content/observability/metrics/opentelemetry.md\n+++ b/docs/content/observability/metrics/opentelemetry.md\n@@ -23,7 +23,7 @@ metrics:\n \n !!! info \"Default protocol\"\n \n-    The OpenTelemetry exporter will export metrics to the collector using HTTP by default to https://localhost:4318/v1/metrics, see the [gRPC Section](#grpc-configuration) to use gRPC.\n+    The OpenTelemetry exporter will export metrics to the collector using HTTPS by default to https://localhost:4318/v1/metrics, see the [gRPC Section](#grpc-configuration) to use gRPC.\n \n #### `addEntryPointsLabels`\n \n@@ -184,25 +184,29 @@ metrics:\n \n #### `endpoint`\n \n-_Required, Default=\"http://localhost:4318/v1/metrics\", Format=\"`<scheme>://<host>:<port><path>`\"_\n+_Optional, Default=\"https://localhost:4318/v1/metrics\", Format=\"`<scheme>://<host>:<port><path>`\"_\n \n URL of the OpenTelemetry Collector to send metrics to.\n \n+!!! info \"Insecure mode\"\n+\n+    To disable TLS, use `http://` instead of `https://` in the `endpoint` configuration.\n+\n ```yaml tab=\"File (YAML)\"\n metrics:\n   otlp:\n     http:\n-      endpoint: http://localhost:4318/v1/metrics\n+      endpoint: https://collector:4318/v1/metrics\n ```\n \n ```toml tab=\"File (TOML)\"\n [metrics]\n   [metrics.otlp.http]\n-    endpoint = \"http://localhost:4318/v1/metrics\"\n+    endpoint = \"https://collector:4318/v1/metrics\"\n ```\n \n ```bash tab=\"CLI\"\n---metrics.otlp.http.endpoint=http://localhost:4318/v1/metrics\n+--metrics.otlp.http.endpoint=https://collector:4318/v1/metrics\n ```\n \n #### `headers`\ndiff --git a/docs/content/observability/tracing/opentelemetry.md b/docs/content/observability/tracing/opentelemetry.md\nindex 3844882c3b..921ce8399e 100644\n--- a/docs/content/observability/tracing/opentelemetry.md\n+++ b/docs/content/observability/tracing/opentelemetry.md\n@@ -25,7 +25,7 @@ tracing:\n \n !!! info \"Default protocol\"\n \n-    The OpenTelemetry trace exporter will export traces to the collector using HTTP by default to https://localhost:4318/v1/traces, see the [gRPC Section](#grpc-configuration) to use gRPC.\n+    The OpenTelemetry trace exporter will export traces to the collector using HTTPS by default to https://localhost:4318/v1/traces, see the [gRPC Section](#grpc-configuration) to use gRPC.\n \n !!! info \"Trace sampling\"\n \n@@ -72,25 +72,29 @@ tracing:\n \n #### `endpoint`\n \n-_Required, Default=\"http://localhost:4318/v1/traces\", Format=\"`<scheme>://<host>:<port><path>`\"_\n+_Optional, Default=\"https://localhost:4318/v1/traces\", Format=\"`<scheme>://<host>:<port><path>`\"_\n \n URL of the OpenTelemetry Collector to send spans to.\n \n+!!! info \"Insecure mode\"\n+\n+    To disable TLS, use `http://` instead of `https://` in the `endpoint` configuration.\n+\n ```yaml tab=\"File (YAML)\"\n tracing:\n   otlp:\n     http:\n-      endpoint: http://localhost:4318/v1/traces\n+      endpoint: https://collector:4318/v1/traces\n ```\n \n ```toml tab=\"File (TOML)\"\n [tracing]\n   [tracing.otlp.http]\n-    endpoint = \"http://localhost:4318/v1/traces\"\n+    endpoint = \"https://collector:4318/v1/traces\"\n ```\n \n ```bash tab=\"CLI\"\n---tracing.otlp.http.endpoint=http://localhost:4318/v1/traces\n+--tracing.otlp.http.endpoint=https://collector:4318/v1/traces\n ```\n \n #### `headers`\ndiff --git a/docs/content/observability/tracing/overview.md b/docs/content/observability/tracing/overview.md\nindex 73840f89d0..891ff5c0bb 100644\n--- a/docs/content/observability/tracing/overview.md\n+++ b/docs/content/observability/tracing/overview.md\n@@ -92,29 +92,29 @@ tracing:\n --tracing.sampleRate=0.2\n ```\n \n-#### `globalAttributes`\n+#### `resourceAttributes`\n \n _Optional, Default=empty_\n \n-Applies a list of shared key:value attributes on all spans.\n+Defines additional resource attributes to be sent to the collector.\n \n ```yaml tab=\"File (YAML)\"\n tracing:\n-  globalAttributes:\n+  resourceAttributes:\n     attr1: foo\n     attr2: bar\n ```\n \n ```toml tab=\"File (TOML)\"\n [tracing]\n-  [tracing.globalAttributes]\n+  [tracing.resourceAttributes]\n     attr1 = \"foo\"\n     attr2 = \"bar\"\n ```\n \n ```bash tab=\"CLI\"\n---tracing.globalAttributes.attr1=foo\n---tracing.globalAttributes.attr2=bar\n+--tracing.resourceAttributes.attr1=foo\n+--tracing.resourceAttributes.attr2=bar\n ```\n \n #### `capturedRequestHeaders`\ndiff --git a/docs/content/reference/static-configuration/cli-ref.md b/docs/content/reference/static-configuration/cli-ref.md\nindex 6d72066a84..0c65188fe8 100644\n--- a/docs/content/reference/static-configuration/cli-ref.md\n+++ b/docs/content/reference/static-configuration/cli-ref.md\n@@ -39,6 +39,60 @@ Keep access logs with status codes in the specified range.\n `--accesslog.format`:  \n Access log format: json | common (Default: ```common```)\n \n+`--accesslog.otlp`:  \n+Settings for OpenTelemetry. (Default: ```false```)\n+\n+`--accesslog.otlp.grpc`:  \n+gRPC configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`--accesslog.otlp.grpc.endpoint`:  \n+Sets the gRPC endpoint (host:port) of the collector. (Default: ```localhost:4317```)\n+\n+`--accesslog.otlp.grpc.headers.<name>`:  \n+Headers sent with payload.\n+\n+`--accesslog.otlp.grpc.insecure`:  \n+Disables client transport security for the exporter. (Default: ```false```)\n+\n+`--accesslog.otlp.grpc.tls.ca`:  \n+TLS CA\n+\n+`--accesslog.otlp.grpc.tls.cert`:  \n+TLS cert\n+\n+`--accesslog.otlp.grpc.tls.insecureskipverify`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`--accesslog.otlp.grpc.tls.key`:  \n+TLS key\n+\n+`--accesslog.otlp.http`:  \n+HTTP configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`--accesslog.otlp.http.endpoint`:  \n+Sets the HTTP endpoint (scheme://host:port/path) of the collector. (Default: ```https://localhost:4318```)\n+\n+`--accesslog.otlp.http.headers.<name>`:  \n+Headers sent with payload.\n+\n+`--accesslog.otlp.http.tls.ca`:  \n+TLS CA\n+\n+`--accesslog.otlp.http.tls.cert`:  \n+TLS cert\n+\n+`--accesslog.otlp.http.tls.insecureskipverify`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`--accesslog.otlp.http.tls.key`:  \n+TLS key\n+\n+`--accesslog.otlp.resourceattributes.<name>`:  \n+Defines additional resource attributes (key:value).\n+\n+`--accesslog.otlp.servicename`:  \n+Set the name for this service. (Default: ```traefik```)\n+\n `--api`:  \n Enable api/dashboard. (Default: ```false```)\n \n@@ -333,6 +387,60 @@ Maximum size in megabytes of the log file before it gets rotated. (Default: ```0\n `--log.nocolor`:  \n When using the 'common' format, disables the colorized output. (Default: ```false```)\n \n+`--log.otlp`:  \n+Settings for OpenTelemetry. (Default: ```false```)\n+\n+`--log.otlp.grpc`:  \n+gRPC configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`--log.otlp.grpc.endpoint`:  \n+Sets the gRPC endpoint (host:port) of the collector. (Default: ```localhost:4317```)\n+\n+`--log.otlp.grpc.headers.<name>`:  \n+Headers sent with payload.\n+\n+`--log.otlp.grpc.insecure`:  \n+Disables client transport security for the exporter. (Default: ```false```)\n+\n+`--log.otlp.grpc.tls.ca`:  \n+TLS CA\n+\n+`--log.otlp.grpc.tls.cert`:  \n+TLS cert\n+\n+`--log.otlp.grpc.tls.insecureskipverify`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`--log.otlp.grpc.tls.key`:  \n+TLS key\n+\n+`--log.otlp.http`:  \n+HTTP configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`--log.otlp.http.endpoint`:  \n+Sets the HTTP endpoint (scheme://host:port/path) of the collector. (Default: ```https://localhost:4318```)\n+\n+`--log.otlp.http.headers.<name>`:  \n+Headers sent with payload.\n+\n+`--log.otlp.http.tls.ca`:  \n+TLS CA\n+\n+`--log.otlp.http.tls.cert`:  \n+TLS cert\n+\n+`--log.otlp.http.tls.insecureskipverify`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`--log.otlp.http.tls.key`:  \n+TLS key\n+\n+`--log.otlp.resourceattributes.<name>`:  \n+Defines additional resource attributes (key:value).\n+\n+`--log.otlp.servicename`:  \n+Set the name for this service. (Default: ```traefik```)\n+\n `--metrics.addinternals`:  \n Enables metrics for internal services (ping, dashboard, etc...). (Default: ```false```)\n \n@@ -1138,7 +1246,7 @@ Defines the allowed SPIFFE IDs (takes precedence over the SPIFFE TrustDomain).\n Defines the allowed SPIFFE trust domain.\n \n `--tracing`:  \n-OpenTracing configuration. (Default: ```false```)\n+Tracing configuration. (Default: ```false```)\n \n `--tracing.addinternals`:  \n Enables tracing for internal services (ping, dashboard, etc...). (Default: ```false```)\n@@ -1150,7 +1258,7 @@ Request headers to add as attributes for server and client spans.\n Response headers to add as attributes for server and client spans.\n \n `--tracing.globalattributes.<name>`:  \n-Defines additional attributes (key:value) on all spans.\n+(Deprecated) Defines additional resource attributes (key:value).\n \n `--tracing.otlp`:  \n Settings for OpenTelemetry. (Default: ```false```)\n@@ -1200,6 +1308,9 @@ TLS insecure skip verify (Default: ```false```)\n `--tracing.otlp.http.tls.key`:  \n TLS key\n \n+`--tracing.resourceattributes.<name>`:  \n+Defines additional resource attributes (key:value).\n+\n `--tracing.safequeryparams`:  \n Query params to not redact.\n \n@@ -1207,4 +1318,4 @@ Query params to not redact.\n Sets the rate between 0.0 and 1.0 of requests to trace. (Default: ```1.000000```)\n \n `--tracing.servicename`:  \n-Set the name for this service. (Default: ```traefik```)\n+Sets the name for this service. (Default: ```traefik```)\ndiff --git a/docs/content/reference/static-configuration/env-ref.md b/docs/content/reference/static-configuration/env-ref.md\nindex 296ac60e21..056f9b29f7 100644\n--- a/docs/content/reference/static-configuration/env-ref.md\n+++ b/docs/content/reference/static-configuration/env-ref.md\n@@ -39,6 +39,60 @@ Keep access logs with status codes in the specified range.\n `TRAEFIK_ACCESSLOG_FORMAT`:  \n Access log format: json | common (Default: ```common```)\n \n+`TRAEFIK_ACCESSLOG_OTLP`:  \n+Settings for OpenTelemetry. (Default: ```false```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC`:  \n+gRPC configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_ENDPOINT`:  \n+Sets the gRPC endpoint (host:port) of the collector. (Default: ```localhost:4317```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_HEADERS_<NAME>`:  \n+Headers sent with payload.\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_INSECURE`:  \n+Disables client transport security for the exporter. (Default: ```false```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_TLS_CA`:  \n+TLS CA\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_TLS_CERT`:  \n+TLS cert\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_TLS_INSECURESKIPVERIFY`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_GRPC_TLS_KEY`:  \n+TLS key\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP`:  \n+HTTP configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP_ENDPOINT`:  \n+Sets the HTTP endpoint (scheme://host:port/path) of the collector. (Default: ```https://localhost:4318```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP_HEADERS_<NAME>`:  \n+Headers sent with payload.\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP_TLS_CA`:  \n+TLS CA\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP_TLS_CERT`:  \n+TLS cert\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP_TLS_INSECURESKIPVERIFY`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`TRAEFIK_ACCESSLOG_OTLP_HTTP_TLS_KEY`:  \n+TLS key\n+\n+`TRAEFIK_ACCESSLOG_OTLP_RESOURCEATTRIBUTES_<NAME>`:  \n+Defines additional resource attributes (key:value).\n+\n+`TRAEFIK_ACCESSLOG_OTLP_SERVICENAME`:  \n+Set the name for this service. (Default: ```traefik```)\n+\n `TRAEFIK_API`:  \n Enable api/dashboard. (Default: ```false```)\n \n@@ -333,6 +387,60 @@ Maximum size in megabytes of the log file before it gets rotated. (Default: ```0\n `TRAEFIK_LOG_NOCOLOR`:  \n When using the 'common' format, disables the colorized output. (Default: ```false```)\n \n+`TRAEFIK_LOG_OTLP`:  \n+Settings for OpenTelemetry. (Default: ```false```)\n+\n+`TRAEFIK_LOG_OTLP_GRPC`:  \n+gRPC configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`TRAEFIK_LOG_OTLP_GRPC_ENDPOINT`:  \n+Sets the gRPC endpoint (host:port) of the collector. (Default: ```localhost:4317```)\n+\n+`TRAEFIK_LOG_OTLP_GRPC_HEADERS_<NAME>`:  \n+Headers sent with payload.\n+\n+`TRAEFIK_LOG_OTLP_GRPC_INSECURE`:  \n+Disables client transport security for the exporter. (Default: ```false```)\n+\n+`TRAEFIK_LOG_OTLP_GRPC_TLS_CA`:  \n+TLS CA\n+\n+`TRAEFIK_LOG_OTLP_GRPC_TLS_CERT`:  \n+TLS cert\n+\n+`TRAEFIK_LOG_OTLP_GRPC_TLS_INSECURESKIPVERIFY`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`TRAEFIK_LOG_OTLP_GRPC_TLS_KEY`:  \n+TLS key\n+\n+`TRAEFIK_LOG_OTLP_HTTP`:  \n+HTTP configuration for the OpenTelemetry collector. (Default: ```false```)\n+\n+`TRAEFIK_LOG_OTLP_HTTP_ENDPOINT`:  \n+Sets the HTTP endpoint (scheme://host:port/path) of the collector. (Default: ```https://localhost:4318```)\n+\n+`TRAEFIK_LOG_OTLP_HTTP_HEADERS_<NAME>`:  \n+Headers sent with payload.\n+\n+`TRAEFIK_LOG_OTLP_HTTP_TLS_CA`:  \n+TLS CA\n+\n+`TRAEFIK_LOG_OTLP_HTTP_TLS_CERT`:  \n+TLS cert\n+\n+`TRAEFIK_LOG_OTLP_HTTP_TLS_INSECURESKIPVERIFY`:  \n+TLS insecure skip verify (Default: ```false```)\n+\n+`TRAEFIK_LOG_OTLP_HTTP_TLS_KEY`:  \n+TLS key\n+\n+`TRAEFIK_LOG_OTLP_RESOURCEATTRIBUTES_<NAME>`:  \n+Defines additional resource attributes (key:value).\n+\n+`TRAEFIK_LOG_OTLP_SERVICENAME`:  \n+Set the name for this service. (Default: ```traefik```)\n+\n `TRAEFIK_METRICS_ADDINTERNALS`:  \n Enables metrics for internal services (ping, dashboard, etc...). (Default: ```false```)\n \n@@ -1138,7 +1246,7 @@ Defines the allowed SPIFFE IDs (takes precedence over the SPIFFE TrustDomain).\n Defines the allowed SPIFFE trust domain.\n \n `TRAEFIK_TRACING`:  \n-OpenTracing configuration. (Default: ```false```)\n+Tracing configuration. (Default: ```false```)\n \n `TRAEFIK_TRACING_ADDINTERNALS`:  \n Enables tracing for internal services (ping, dashboard, etc...). (Default: ```false```)\n@@ -1150,7 +1258,7 @@ Request headers to add as attributes for server and client spans.\n Response headers to add as attributes for server and client spans.\n \n `TRAEFIK_TRACING_GLOBALATTRIBUTES_<NAME>`:  \n-Defines additional attributes (key:value) on all spans.\n+(Deprecated) Defines additional resource attributes (key:value).\n \n `TRAEFIK_TRACING_OTLP`:  \n Settings for OpenTelemetry. (Default: ```false```)\n@@ -1200,6 +1308,9 @@ TLS insecure skip verify (Default: ```false```)\n `TRAEFIK_TRACING_OTLP_HTTP_TLS_KEY`:  \n TLS key\n \n+`TRAEFIK_TRACING_RESOURCEATTRIBUTES_<NAME>`:  \n+Defines additional resource attributes (key:value).\n+\n `TRAEFIK_TRACING_SAFEQUERYPARAMS`:  \n Query params to not redact.\n \n@@ -1207,4 +1318,4 @@ Query params to not redact.\n Sets the rate between 0.0 and 1.0 of requests to trace. (Default: ```1.000000```)\n \n `TRAEFIK_TRACING_SERVICENAME`:  \n-Set the name for this service. (Default: ```traefik```)\n+Sets the name for this service. (Default: ```traefik```)\ndiff --git a/docs/content/reference/static-configuration/file.toml b/docs/content/reference/static-configuration/file.toml\nindex 08b030a6c4..1987627ecb 100644\n--- a/docs/content/reference/static-configuration/file.toml\n+++ b/docs/content/reference/static-configuration/file.toml\n@@ -381,6 +381,32 @@\n   maxAge = 42\n   maxBackups = 42\n   compress = true\n+  [log.otlp]\n+    serviceName = \"foobar\"\n+    [log.otlp.resourceAttributes]\n+      name0 = \"foobar\"\n+      name1 = \"foobar\"\n+    [log.otlp.grpc]\n+      endpoint = \"foobar\"\n+      insecure = true\n+      [log.otlp.grpc.tls]\n+        ca = \"foobar\"\n+        cert = \"foobar\"\n+        key = \"foobar\"\n+        insecureSkipVerify = true\n+      [log.otlp.grpc.headers]\n+        name0 = \"foobar\"\n+        name1 = \"foobar\"\n+    [log.otlp.http]\n+      endpoint = \"foobar\"\n+      [log.otlp.http.tls]\n+        ca = \"foobar\"\n+        cert = \"foobar\"\n+        key = \"foobar\"\n+        insecureSkipVerify = true\n+      [log.otlp.http.headers]\n+        name0 = \"foobar\"\n+        name1 = \"foobar\"\n \n [accessLog]\n   filePath = \"foobar\"\n@@ -401,6 +427,32 @@\n       [accessLog.fields.headers.names]\n         name0 = \"foobar\"\n         name1 = \"foobar\"\n+  [accessLog.otlp]\n+    serviceName = \"foobar\"\n+    [accessLog.otlp.resourceAttributes]\n+      name0 = \"foobar\"\n+      name1 = \"foobar\"\n+    [accessLog.otlp.grpc]\n+      endpoint = \"foobar\"\n+      insecure = true\n+      [accessLog.otlp.grpc.tls]\n+        ca = \"foobar\"\n+        cert = \"foobar\"\n+        key = \"foobar\"\n+        insecureSkipVerify = true\n+      [accessLog.otlp.grpc.headers]\n+        name0 = \"foobar\"\n+        name1 = \"foobar\"\n+    [accessLog.otlp.http]\n+      endpoint = \"foobar\"\n+      [accessLog.otlp.http.tls]\n+        ca = \"foobar\"\n+        cert = \"foobar\"\n+        key = \"foobar\"\n+        insecureSkipVerify = true\n+      [accessLog.otlp.http.headers]\n+        name0 = \"foobar\"\n+        name1 = \"foobar\"\n \n [tracing]\n   serviceName = \"foobar\"\n@@ -409,7 +461,7 @@\n   safeQueryParams = [\"foobar\", \"foobar\"]\n   sampleRate = 42.0\n   addInternals = true\n-  [tracing.globalAttributes]\n+  [tracing.resourceAttributes]\n     name0 = \"foobar\"\n     name1 = \"foobar\"\n   [tracing.otlp]\n@@ -434,6 +486,9 @@\n       [tracing.otlp.http.headers]\n         name0 = \"foobar\"\n         name1 = \"foobar\"\n+  [tracing.globalAttributes]\n+    name0 = \"foobar\"\n+    name1 = \"foobar\"\n \n [hostResolver]\n   cnameFlattening = true\ndiff --git a/docs/content/reference/static-configuration/file.yaml b/docs/content/reference/static-configuration/file.yaml\nindex cbc8dc0201..2bebf70177 100644\n--- a/docs/content/reference/static-configuration/file.yaml\n+++ b/docs/content/reference/static-configuration/file.yaml\n@@ -418,6 +418,32 @@ log:\n   maxAge: 42\n   maxBackups: 42\n   compress: true\n+  otlp:\n+    serviceName: foobar\n+    resourceAttributes:\n+      name0: foobar\n+      name1: foobar\n+    grpc:\n+      endpoint: foobar\n+      insecure: true\n+      tls:\n+        ca: foobar\n+        cert: foobar\n+        key: foobar\n+        insecureSkipVerify: true\n+      headers:\n+        name0: foobar\n+        name1: foobar\n+    http:\n+      endpoint: foobar\n+      tls:\n+        ca: foobar\n+        cert: foobar\n+        key: foobar\n+        insecureSkipVerify: true\n+      headers:\n+        name0: foobar\n+        name1: foobar\n accessLog:\n   filePath: foobar\n   format: foobar\n@@ -439,9 +465,35 @@ accessLog:\n         name1: foobar\n   bufferingSize: 42\n   addInternals: true\n+  otlp:\n+    serviceName: foobar\n+    resourceAttributes:\n+      name0: foobar\n+      name1: foobar\n+    grpc:\n+      endpoint: foobar\n+      insecure: true\n+      tls:\n+        ca: foobar\n+        cert: foobar\n+        key: foobar\n+        insecureSkipVerify: true\n+      headers:\n+        name0: foobar\n+        name1: foobar\n+    http:\n+      endpoint: foobar\n+      tls:\n+        ca: foobar\n+        cert: foobar\n+        key: foobar\n+        insecureSkipVerify: true\n+      headers:\n+        name0: foobar\n+        name1: foobar\n tracing:\n   serviceName: foobar\n-  globalAttributes:\n+  resourceAttributes:\n     name0: foobar\n     name1: foobar\n   capturedRequestHeaders:\n@@ -477,6 +529,9 @@ tracing:\n       headers:\n         name0: foobar\n         name1: foobar\n+  globalAttributes:\n+    name0: foobar\n+    name1: foobar\n hostResolver:\n   cnameFlattening: true\n   resolvConfig: foobar\ndiff --git a/go.mod b/go.mod\nindex 40b544195e..aef9401b13 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -51,7 +51,7 @@ require (\n \tgithub.com/prometheus/client_golang v1.19.1\n \tgithub.com/prometheus/client_model v0.6.1\n \tgithub.com/quic-go/quic-go v0.47.0\n-\tgithub.com/rs/zerolog v1.29.0\n+\tgithub.com/rs/zerolog v1.33.0\n \tgithub.com/sirupsen/logrus v1.9.3\n \tgithub.com/spiffe/go-spiffe/v2 v2.1.1\n \tgithub.com/stealthrocket/wasi-go v0.8.0\n@@ -72,23 +72,28 @@ require (\n \tgithub.com/vulcand/oxy/v2 v2.0.0\n \tgithub.com/vulcand/predicate v1.2.0\n \tgo.opentelemetry.io/collector/pdata v1.10.0\n+\tgo.opentelemetry.io/contrib/bridges/otellogrus v0.7.0\n \tgo.opentelemetry.io/contrib/propagators/autoprop v0.53.0\n-\tgo.opentelemetry.io/otel v1.29.0\n+\tgo.opentelemetry.io/otel v1.32.0\n+\tgo.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc v0.8.0\n+\tgo.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp v0.8.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc v1.28.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetrichttp v1.28.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.28.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.28.0\n \tgo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp v1.28.0\n-\tgo.opentelemetry.io/otel/metric v1.29.0\n-\tgo.opentelemetry.io/otel/sdk v1.28.0\n+\tgo.opentelemetry.io/otel/log v0.8.0\n+\tgo.opentelemetry.io/otel/metric v1.32.0\n+\tgo.opentelemetry.io/otel/sdk v1.32.0\n+\tgo.opentelemetry.io/otel/sdk/log v0.8.0\n \tgo.opentelemetry.io/otel/sdk/metric v1.28.0\n-\tgo.opentelemetry.io/otel/trace v1.29.0\n+\tgo.opentelemetry.io/otel/trace v1.32.0\n \tgolang.org/x/exp v0.0.0-20240909161429-701f63a606c0 // No tag on the repo.\n \tgolang.org/x/mod v0.21.0\n \tgolang.org/x/net v0.30.0\n-\tgolang.org/x/sync v0.8.0\n-\tgolang.org/x/sys v0.26.0\n-\tgolang.org/x/text v0.19.0\n+\tgolang.org/x/sync v0.9.0\n+\tgolang.org/x/sys v0.27.0\n+\tgolang.org/x/text v0.20.0\n \tgolang.org/x/time v0.7.0\n \tgolang.org/x/tools v0.25.0\n \tgoogle.golang.org/grpc v1.67.1\n@@ -219,7 +224,7 @@ require (\n \tgithub.com/gophercloud/gophercloud v1.14.1 // indirect\n \tgithub.com/gophercloud/utils v0.0.0-20231010081019-80377eca5d56 // indirect\n \tgithub.com/gravitational/trace v1.1.16-0.20220114165159-14a9a7dd6aaf // indirect\n-\tgithub.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0 // indirect\n+\tgithub.com/grpc-ecosystem/grpc-gateway/v2 v2.23.0 // indirect\n \tgithub.com/hashicorp/cronexpr v1.1.2 // indirect\n \tgithub.com/hashicorp/errwrap v1.1.0 // indirect\n \tgithub.com/hashicorp/go-cleanhttp v0.5.2 // indirect\n@@ -364,8 +369,8 @@ require (\n \tgolang.org/x/term v0.25.0 // indirect\n \tgoogle.golang.org/api v0.204.0 // indirect\n \tgoogle.golang.org/genproto v0.0.0-20241021214115-324edc3d5d38 // indirect\n-\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20241015192408-796eee8c2d53 // indirect\n-\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241021214115-324edc3d5d38 // indirect\n+\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28 // indirect\n+\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28 // indirect\n \tgoogle.golang.org/protobuf v1.35.1 // indirect\n \tgopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect\n \tgopkg.in/h2non/gock.v1 v1.0.16 // indirect\ndiff --git a/go.sum b/go.sum\nindex 6c8112747c..b9b9878035 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -262,7 +262,6 @@ github.com/coreos/go-semver v0.3.1/go.mod h1:irMmmIw/7yzSRPWryHsK7EYSg09caPQL03V\n github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e h1:Wf6HqHfScWJN9/ZjdUKyjop4mf3Qdd+1TvvltAvM3m8=\n github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\n github.com/coreos/go-systemd/v22 v22.3.2/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\n-github.com/coreos/go-systemd/v22 v22.3.3-0.20220203105225-a9a7ef127534/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\n github.com/coreos/go-systemd/v22 v22.5.0 h1:RrqgGjYQKalulkV8NGVIfkXQf6YYmOyiJKk8iXXhfZs=\n github.com/coreos/go-systemd/v22 v22.5.0/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\n github.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f/go.mod h1:E3G3o1h8I7cfcXa63jLwjI0eiQQMgzzUDFVpN/nH/eA=\n@@ -576,8 +575,8 @@ github.com/grpc-ecosystem/go-grpc-middleware v1.0.0/go.mod h1:FiyG127CGDf3tlThmg\n github.com/grpc-ecosystem/go-grpc-prometheus v1.2.0/go.mod h1:8NvIoxWQoOIhqOTXgfV/d3M/q6VIi02HzZEHgUlZvzk=\n github.com/grpc-ecosystem/grpc-gateway v1.9.0/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=\n github.com/grpc-ecosystem/grpc-gateway v1.16.0/go.mod h1:BDjrQk3hbvj6Nolgz8mAMFbcEtjT1g+wF4CSlocrBnw=\n-github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0 h1:bkypFPDjIYGfCYD5mRBvpqxfYX1YCS1PXdKYWi8FsN0=\n-github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0/go.mod h1:P+Lt/0by1T8bfcF3z737NnSbmxQAppXMRziHUxPOC8k=\n+github.com/grpc-ecosystem/grpc-gateway/v2 v2.23.0 h1:ad0vkEBuk23VJzZR9nkLVG0YAoN9coASF1GusYX6AlU=\n+github.com/grpc-ecosystem/grpc-gateway/v2 v2.23.0/go.mod h1:igFoXX2ELCW06bol23DWPB5BEWfZISOzSP5K2sbLea0=\n github.com/h2non/parth v0.0.0-20190131123155-b4df798d6542 h1:2VTzZjLZBgl62/EtslCrtky5vbi9dd7HrQPQIx6wqiw=\n github.com/h2non/parth v0.0.0-20190131123155-b4df798d6542/go.mod h1:Ow0tF8D4Kplbc8s8sSb3V2oUCygFHVp8gC3Dn6U4MNI=\n github.com/hashicorp/consul/api v1.1.0/go.mod h1:VmuI/Lkw1nC05EYQWNKwWGbkg+FbDBtguAZLlVdkD9Q=\n@@ -818,6 +817,7 @@ github.com/mattn/go-isatty v0.0.11/go.mod h1:PhnuNfih5lzO57/f3n+odYbM4JtupLOxQOA\n github.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\n github.com/mattn/go-isatty v0.0.14/go.mod h1:7GGIvUiUoEMVVmxf/4nioHXj79iQHKdU27kJ6hsGG94=\n github.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=\n+github.com/mattn/go-isatty v0.0.19/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=\n github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=\n github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=\n github.com/mattn/go-runewidth v0.0.6/go.mod h1:H031xJmbD/WCDINGzjvQ9THkh0rPKHF+m2gUSrubnMI=\n@@ -1046,13 +1046,13 @@ github.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6L\n github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\n github.com/rogpeppe/go-internal v1.6.1/go.mod h1:xXDCJY+GAPziupqXw64V24skbSoqbTEfhy4qGm1nDQc=\n github.com/rogpeppe/go-internal v1.8.0/go.mod h1:WmiCO8CzOY8rg0OYDC4/i/2WRWAB6poM+XZ2dLUbcbE=\n-github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=\n-github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=\n+github.com/rogpeppe/go-internal v1.13.1 h1:KvO1DLK/DRN07sQ1LQKScxyZJuNnedQ5/wKSR38lUII=\n+github.com/rogpeppe/go-internal v1.13.1/go.mod h1:uMEvuHeurkdAXX61udpOXGD/AzZDWNMNyH2VO9fmH0o=\n github.com/rs/cors v1.7.0 h1:+88SsELBHx5r+hZ8TCkggzSstaWNbDvThkVK8H6f9ik=\n github.com/rs/cors v1.7.0/go.mod h1:gFx+x8UowdsKA9AchylcLynDq+nNFfI8FkUZdN/jGCU=\n-github.com/rs/xid v1.4.0/go.mod h1:trrq9SKmegXys3aeAKXMUTdJsYXVwGY3RLcfgqegfbg=\n-github.com/rs/zerolog v1.29.0 h1:Zes4hju04hjbvkVkOhdl2HpZa+0PmVwigmo8XoORE5w=\n-github.com/rs/zerolog v1.29.0/go.mod h1:NILgTygv/Uej1ra5XxGf82ZFSLk58MFGAUS2o6usyD0=\n+github.com/rs/xid v1.5.0/go.mod h1:trrq9SKmegXys3aeAKXMUTdJsYXVwGY3RLcfgqegfbg=\n+github.com/rs/zerolog v1.33.0 h1:1cU2KZkvPxNyfgEmhHAz/1A9Bz+llsdYzklWFzgp0r8=\n+github.com/rs/zerolog v1.33.0/go.mod h1:/7mN4D5sKwJLZQ2b/znpjC3/GQWY/xaDXUM0kKWRHss=\n github.com/russross/blackfriday/v2 v2.0.1/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\n github.com/ryanuber/columnize v0.0.0-20160712163229-9b3edd62028f/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\n github.com/sacloud/api-client-go v0.2.10 h1:+rv3jDohD+pkdYwOTBiB+jZsM0xK3AxadXRzhp3q66c=\n@@ -1288,6 +1288,8 @@ go.opencensus.io v0.24.0 h1:y73uSU6J157QMP2kn2r30vwW1A2W2WFwSCGnAVxeaD0=\n go.opencensus.io v0.24.0/go.mod h1:vNK8G9p7aAivkbmorf4v+7Hgx+Zs0yY+0fOtgBfjQKo=\n go.opentelemetry.io/collector/pdata v1.10.0 h1:oLyPLGvPTQrcRT64ZVruwvmH/u3SHTfNo01pteS4WOE=\n go.opentelemetry.io/collector/pdata v1.10.0/go.mod h1:IHxHsp+Jq/xfjORQMDJjSH6jvedOSTOyu3nbxqhWSYE=\n+go.opentelemetry.io/contrib/bridges/otellogrus v0.7.0 h1:vPSzn6dQvdPq9ZiXFs+jUSJnzoKJkADD9yBdx/a1WgI=\n+go.opentelemetry.io/contrib/bridges/otellogrus v0.7.0/go.mod h1:yZFNJIjn97IBhuMB3tTGPti9xasYLIdh3ChZIzyhz8A=\n go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0 h1:TT4fX+nBOA/+LUkobKGW1ydGcn+G3vRw9+g5HwCphpk=\n go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.54.0/go.mod h1:L7UH0GbB0p47T4Rri3uHjbpCFYrVrwc1I25QhNPiGK8=\n go.opentelemetry.io/contrib/propagators/autoprop v0.53.0 h1:4zaVLcJ5mvYw0vlk63TX62qS4qty/4jAY1BKZ1usu18=\n@@ -1300,8 +1302,12 @@ go.opentelemetry.io/contrib/propagators/jaeger v1.28.0 h1:xQ3ktSVS128JWIaN1DiPGI\n go.opentelemetry.io/contrib/propagators/jaeger v1.28.0/go.mod h1:O9HIyI2kVBrFoEwQZ0IN6PHXykGoit4mZV2aEjkTRH4=\n go.opentelemetry.io/contrib/propagators/ot v1.28.0 h1:rmlG+2pc5k5M7Y7izDrxAHZUIwDERdGMTD9oMV7llMk=\n go.opentelemetry.io/contrib/propagators/ot v1.28.0/go.mod h1:MNgXIn+UrMbNGpd7xyckyo2LCHIgCdmdjEE7YNZGG+w=\n-go.opentelemetry.io/otel v1.29.0 h1:PdomN/Al4q/lN6iBJEN3AwPvUiHPMlt93c8bqTG5Llw=\n-go.opentelemetry.io/otel v1.29.0/go.mod h1:N/WtXPs1CNCUEx+Agz5uouwCba+i+bJGFicT8SR4NP8=\n+go.opentelemetry.io/otel v1.32.0 h1:WnBN+Xjcteh0zdk01SVqV55d/m62NJLJdIyb4y/WO5U=\n+go.opentelemetry.io/otel v1.32.0/go.mod h1:00DCVSB0RQcnzlwyTfqtxSm+DRr9hpYrHjNGiBHVQIg=\n+go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc v0.8.0 h1:WzNab7hOOLzdDF/EoWCt4glhrbMPVMOO5JYTmpz36Ls=\n+go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc v0.8.0/go.mod h1:hKvJwTzJdp90Vh7p6q/9PAOd55dI6WA6sWj62a/JvSs=\n+go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp v0.8.0 h1:S+LdBGiQXtJdowoJoQPEtI52syEP/JYBUpjO49EQhV8=\n+go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp v0.8.0/go.mod h1:5KXybFvPGds3QinJWQT7pmXf+TN5YIa7CNYObWRkj50=\n go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc v1.28.0 h1:U2guen0GhqH8o/G2un8f/aG/y++OuW6MyCo6hT9prXk=\n go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc v1.28.0/go.mod h1:yeGZANgEcpdx/WK0IvvRFC+2oLiMS2u4L/0Rj2M2Qr0=\n go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetrichttp v1.28.0 h1:aLmmtjRke7LPDQ3lvpFz+kNEH43faFhzW7v8BFIEydg=\n@@ -1312,14 +1318,18 @@ go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.28.0 h1:R3X6Z\n go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.28.0/go.mod h1:QWFXnDavXWwMx2EEcZsf3yxgEKAqsxQ+Syjp+seyInw=\n go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp v1.28.0 h1:j9+03ymgYhPKmeXGk5Zu+cIZOlVzd9Zv7QIiyItjFBU=\n go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp v1.28.0/go.mod h1:Y5+XiUG4Emn1hTfciPzGPJaSI+RpDts6BnCIir0SLqk=\n-go.opentelemetry.io/otel/metric v1.29.0 h1:vPf/HFWTNkPu1aYeIsc98l4ktOQaL6LeSoeV2g+8YLc=\n-go.opentelemetry.io/otel/metric v1.29.0/go.mod h1:auu/QWieFVWx+DmQOUMgj0F8LHWdgalxXqvp7BII/W8=\n-go.opentelemetry.io/otel/sdk v1.28.0 h1:b9d7hIry8yZsgtbmM0DKyPWMMUMlK9NEKuIG4aBqWyE=\n-go.opentelemetry.io/otel/sdk v1.28.0/go.mod h1:oYj7ClPUA7Iw3m+r7GeEjz0qckQRJK2B8zjcZEfu7Pg=\n+go.opentelemetry.io/otel/log v0.8.0 h1:egZ8vV5atrUWUbnSsHn6vB8R21G2wrKqNiDt3iWertk=\n+go.opentelemetry.io/otel/log v0.8.0/go.mod h1:M9qvDdUTRCopJcGRKg57+JSQ9LgLBrwwfC32epk5NX8=\n+go.opentelemetry.io/otel/metric v1.32.0 h1:xV2umtmNcThh2/a/aCP+h64Xx5wsj8qqnkYZktzNa0M=\n+go.opentelemetry.io/otel/metric v1.32.0/go.mod h1:jH7CIbbK6SH2V2wE16W05BHCtIDzauciCRLoc/SyMv8=\n+go.opentelemetry.io/otel/sdk v1.32.0 h1:RNxepc9vK59A8XsgZQouW8ue8Gkb4jpWtJm9ge5lEG4=\n+go.opentelemetry.io/otel/sdk v1.32.0/go.mod h1:LqgegDBjKMmb2GC6/PrTnteJG39I8/vJCAP9LlJXEjU=\n+go.opentelemetry.io/otel/sdk/log v0.8.0 h1:zg7GUYXqxk1jnGF/dTdLPrK06xJdrXgqgFLnI4Crxvs=\n+go.opentelemetry.io/otel/sdk/log v0.8.0/go.mod h1:50iXr0UVwQrYS45KbruFrEt4LvAdCaWWgIrsN3ZQggo=\n go.opentelemetry.io/otel/sdk/metric v1.28.0 h1:OkuaKgKrgAbYrrY0t92c+cC+2F6hsFNnCQArXCKlg08=\n go.opentelemetry.io/otel/sdk/metric v1.28.0/go.mod h1:cWPjykihLAPvXKi4iZc1dpER3Jdq2Z0YLse3moQUCpg=\n-go.opentelemetry.io/otel/trace v1.29.0 h1:J/8ZNK4XgR7a21DZUAsbF8pZ5Jcw1VhACmnYt39JTi4=\n-go.opentelemetry.io/otel/trace v1.29.0/go.mod h1:eHl3w0sp3paPkYstJOmAimxhiFXPg+MMTlEh3nsQgWQ=\n+go.opentelemetry.io/otel/trace v1.32.0 h1:WIC9mYrXf8TmY/EXuULKc8hR17vE+Hjv2cssQDe03fM=\n+go.opentelemetry.io/otel/trace v1.32.0/go.mod h1:+i4rkvCraA+tG6AzwloGaCtkx53Fa+L+V8e9a7YvhT8=\n go.opentelemetry.io/proto/otlp v0.7.0/go.mod h1:PqfVotwruBrMGOCsRd/89rSnXhoiJIqeYNgFYFoEGnI=\n go.opentelemetry.io/proto/otlp v1.3.1 h1:TrMUixzpM0yuc/znrFTP9MMRh8trP93mkCiDVeXrui0=\n go.opentelemetry.io/proto/otlp v1.3.1/go.mod h1:0X1WI4de4ZsLrrJNLAQbFeLCm3T7yBkR0XqQ7niQU+8=\n@@ -1508,8 +1518,8 @@ golang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJ\n golang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.8.0 h1:3NFvSEYkUoMifnESzZl15y791HH1qU2xm6eCJU5ZPXQ=\n-golang.org/x/sync v0.8.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n+golang.org/x/sync v0.9.0 h1:fEo0HyrW1GIgZdpbhCRO0PkJajUS5H9IFUztCgEo2jQ=\n+golang.org/x/sync v0.9.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n golang.org/x/sys v0.0.0-20180823144017-11551d06cbcc/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n@@ -1597,11 +1607,12 @@ golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.10.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.11.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n+golang.org/x/sys v0.12.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.15.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n golang.org/x/sys v0.17.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n golang.org/x/sys v0.20.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n-golang.org/x/sys v0.26.0 h1:KHjCJyddX0LoSTb3J+vWpupP9p0oznkqVk/IfjymZbo=\n-golang.org/x/sys v0.26.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n+golang.org/x/sys v0.27.0 h1:wBqf8DvsY9Y/2P8gAfPDEYNuS30J4lPHJxXSb/nJZ+s=\n+golang.org/x/sys v0.27.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n golang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=\n golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\n golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\n@@ -1631,8 +1642,8 @@ golang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=\n golang.org/x/text v0.11.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=\n golang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\n golang.org/x/text v0.15.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\n-golang.org/x/text v0.19.0 h1:kTxAhCbGbxhK0IwgSKiMO5awPoDQ0RpfiVYBfK860YM=\n-golang.org/x/text v0.19.0/go.mod h1:BuEKDfySbSR4drPmRPG/7iBdf8hvFMuRexcpahXilzY=\n+golang.org/x/text v0.20.0 h1:gK/Kv2otX8gz+wn7Rmb3vT96ZwuoxnQlY+HlJVj7Qug=\n+golang.org/x/text v0.20.0/go.mod h1:D4IsuqiFMhST5bX19pQ9ikHC2GsaKyk/oF+pn3ducp4=\n golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n@@ -1770,10 +1781,10 @@ google.golang.org/genproto v0.0.0-20210602131652-f16073e35f0c/go.mod h1:UODoCrxH\n google.golang.org/genproto v0.0.0-20210917145530-b395a37504d4/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n google.golang.org/genproto v0.0.0-20241021214115-324edc3d5d38 h1:Q3nlH8iSQSRUwOskjbcSMcF2jiYMNiQYZ0c2KEJLKKU=\n google.golang.org/genproto v0.0.0-20241021214115-324edc3d5d38/go.mod h1:xBI+tzfqGGN2JBeSebfKXFSdBpWVQ7sLW40PTupVRm4=\n-google.golang.org/genproto/googleapis/api v0.0.0-20241015192408-796eee8c2d53 h1:fVoAXEKA4+yufmbdVYv+SE73+cPZbbbe8paLsHfkK+U=\n-google.golang.org/genproto/googleapis/api v0.0.0-20241015192408-796eee8c2d53/go.mod h1:riSXTwQ4+nqmPGtobMFyW5FqVAmIs0St6VPp4Ug7CE4=\n-google.golang.org/genproto/googleapis/rpc v0.0.0-20241021214115-324edc3d5d38 h1:zciRKQ4kBpFgpfC5QQCVtnnNAcLIqweL7plyZRQHVpI=\n-google.golang.org/genproto/googleapis/rpc v0.0.0-20241021214115-324edc3d5d38/go.mod h1:GX3210XPVPUjJbTUbvwI8f2IpZDMZuPJWDzDuebbviI=\n+google.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28 h1:M0KvPgPmDZHPlbRbaNU1APr28TvwvvdUPlSv7PUvy8g=\n+google.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28/go.mod h1:dguCy7UOdZhTvLzDyt15+rOrawrpM4q7DD9dQ1P11P4=\n+google.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28 h1:XVhgTWWV3kGQlwJHR3upFWZeTsei6Oks1apkZSeonIE=\n+google.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28/go.mod h1:GX3210XPVPUjJbTUbvwI8f2IpZDMZuPJWDzDuebbviI=\n google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\n google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\n google.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ndiff --git a/pkg/cli/deprecation.go b/pkg/cli/deprecation.go\nindex 7466b2758e..822c2c030e 100644\n--- a/pkg/cli/deprecation.go\n+++ b/pkg/cli/deprecation.go\n@@ -502,14 +502,17 @@ func (e *experimental) deprecationNotice(logger zerolog.Logger) bool {\n \treturn false\n }\n \n+//\n+\n type tracing struct {\n-\tSpanNameLimit *int           `json:\"spanNameLimit,omitempty\" toml:\"spanNameLimit,omitempty\" yaml:\"spanNameLimit,omitempty\"`\n-\tJaeger        map[string]any `json:\"jaeger,omitempty\" toml:\"jaeger,omitempty\" yaml:\"jaeger,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n-\tZipkin        map[string]any `json:\"zipkin,omitempty\" toml:\"zipkin,omitempty\" yaml:\"zipkin,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n-\tDatadog       map[string]any `json:\"datadog,omitempty\" toml:\"datadog,omitempty\" yaml:\"datadog,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n-\tInstana       map[string]any `json:\"instana,omitempty\" toml:\"instana,omitempty\" yaml:\"instana,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n-\tHaystack      map[string]any `json:\"haystack,omitempty\" toml:\"haystack,omitempty\" yaml:\"haystack,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n-\tElastic       map[string]any `json:\"elastic,omitempty\" toml:\"elastic,omitempty\" yaml:\"elastic,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n+\tSpanNameLimit    *int              `json:\"spanNameLimit,omitempty\" toml:\"spanNameLimit,omitempty\" yaml:\"spanNameLimit,omitempty\"`\n+\tGlobalAttributes map[string]string `json:\"globalAttributes,omitempty\" toml:\"globalAttributes,omitempty\" yaml:\"globalAttributes,omitempty\" export:\"true\"`\n+\tJaeger           map[string]any    `json:\"jaeger,omitempty\" toml:\"jaeger,omitempty\" yaml:\"jaeger,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n+\tZipkin           map[string]any    `json:\"zipkin,omitempty\" toml:\"zipkin,omitempty\" yaml:\"zipkin,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n+\tDatadog          map[string]any    `json:\"datadog,omitempty\" toml:\"datadog,omitempty\" yaml:\"datadog,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n+\tInstana          map[string]any    `json:\"instana,omitempty\" toml:\"instana,omitempty\" yaml:\"instana,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n+\tHaystack         map[string]any    `json:\"haystack,omitempty\" toml:\"haystack,omitempty\" yaml:\"haystack,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n+\tElastic          map[string]any    `json:\"elastic,omitempty\" toml:\"elastic,omitempty\" yaml:\"elastic,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\"`\n }\n \n func (t *tracing) deprecationNotice(logger zerolog.Logger) bool {\n@@ -523,6 +526,14 @@ func (t *tracing) deprecationNotice(logger zerolog.Logger) bool {\n \t\t\t\"For more information please read the migration guide: https://doc.traefik.io/traefik/v3.2/migration/v2-to-v3/#tracing\")\n \t}\n \n+\tif t.GlobalAttributes != nil {\n+\t\tlog.Warn().Msgf(\"tracing.globalAttributes option is now deprecated, please use tracing.resourceAttributes instead.\")\n+\n+\t\tlogger.Error().Msg(\"`tracing.globalAttributes` option has been deprecated in v3.3, and will be removed in the next major version.\" +\n+\t\t\t\"Please use the `tracing.resourceAttributes` option instead.\" +\n+\t\t\t\"For more information please read the migration guide: https://doc.traefik.io/traefik/v3.3/migration/v3/#tracing-global-attributes\")\n+\t}\n+\n \tif t.Jaeger != nil {\n \t\tincompatible = true\n \t\tlogger.Error().Msg(\"Jaeger Tracing backend has been removed in v3, please remove all Jaeger-related Tracing static configuration for Traefik to start.\" +\ndiff --git a/pkg/config/static/static_config.go b/pkg/config/static/static_config.go\nindex 979baa3fd9..4f6dad22aa 100644\n--- a/pkg/config/static/static_config.go\n+++ b/pkg/config/static/static_config.go\n@@ -28,7 +28,6 @@ import (\n \t\"github.com/traefik/traefik/v3/pkg/provider/kv/zk\"\n \t\"github.com/traefik/traefik/v3/pkg/provider/nomad\"\n \t\"github.com/traefik/traefik/v3/pkg/provider/rest\"\n-\t\"github.com/traefik/traefik/v3/pkg/tracing/opentelemetry\"\n \t\"github.com/traefik/traefik/v3/pkg/types\"\n )\n \n@@ -69,7 +68,7 @@ type Configuration struct {\n \n \tLog       *types.TraefikLog `description:\"Traefik log settings.\" json:\"log,omitempty\" toml:\"log,omitempty\" yaml:\"log,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n \tAccessLog *types.AccessLog  `description:\"Access log settings.\" json:\"accessLog,omitempty\" toml:\"accessLog,omitempty\" yaml:\"accessLog,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n-\tTracing   *Tracing          `description:\"OpenTracing configuration.\" json:\"tracing,omitempty\" toml:\"tracing,omitempty\" yaml:\"tracing,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\tTracing   *Tracing          `description:\"Tracing configuration.\" json:\"tracing,omitempty\" toml:\"tracing,omitempty\" yaml:\"tracing,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n \n \tHostResolver *types.HostResolverConfig `description:\"Enable CNAME Flattening.\" json:\"hostResolver,omitempty\" toml:\"hostResolver,omitempty\" yaml:\"hostResolver,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n \n@@ -200,15 +199,17 @@ func (a *LifeCycle) SetDefaults() {\n \n // Tracing holds the tracing configuration.\n type Tracing struct {\n-\tServiceName             string            `description:\"Set the name for this service.\" json:\"serviceName,omitempty\" toml:\"serviceName,omitempty\" yaml:\"serviceName,omitempty\" export:\"true\"`\n-\tGlobalAttributes        map[string]string `description:\"Defines additional attributes (key:value) on all spans.\" json:\"globalAttributes,omitempty\" toml:\"globalAttributes,omitempty\" yaml:\"globalAttributes,omitempty\" export:\"true\"`\n-\tCapturedRequestHeaders  []string          `description:\"Request headers to add as attributes for server and client spans.\" json:\"capturedRequestHeaders,omitempty\" toml:\"capturedRequestHeaders,omitempty\" yaml:\"capturedRequestHeaders,omitempty\" export:\"true\"`\n-\tCapturedResponseHeaders []string          `description:\"Response headers to add as attributes for server and client spans.\" json:\"capturedResponseHeaders,omitempty\" toml:\"capturedResponseHeaders,omitempty\" yaml:\"capturedResponseHeaders,omitempty\" export:\"true\"`\n-\tSafeQueryParams         []string          `description:\"Query params to not redact.\" json:\"safeQueryParams,omitempty\" toml:\"safeQueryParams,omitempty\" yaml:\"safeQueryParams,omitempty\" export:\"true\"`\n-\tSampleRate              float64           `description:\"Sets the rate between 0.0 and 1.0 of requests to trace.\" json:\"sampleRate,omitempty\" toml:\"sampleRate,omitempty\" yaml:\"sampleRate,omitempty\" export:\"true\"`\n-\tAddInternals            bool              `description:\"Enables tracing for internal services (ping, dashboard, etc...).\" json:\"addInternals,omitempty\" toml:\"addInternals,omitempty\" yaml:\"addInternals,omitempty\" export:\"true\"`\n-\n-\tOTLP *opentelemetry.Config `description:\"Settings for OpenTelemetry.\" json:\"otlp,omitempty\" toml:\"otlp,omitempty\" yaml:\"otlp,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\tServiceName             string             `description:\"Sets the name for this service.\" json:\"serviceName,omitempty\" toml:\"serviceName,omitempty\" yaml:\"serviceName,omitempty\" export:\"true\"`\n+\tResourceAttributes      map[string]string  `description:\"Defines additional resource attributes (key:value).\" json:\"resourceAttributes,omitempty\" toml:\"resourceAttributes,omitempty\" yaml:\"resourceAttributes,omitempty\" export:\"true\"`\n+\tCapturedRequestHeaders  []string           `description:\"Request headers to add as attributes for server and client spans.\" json:\"capturedRequestHeaders,omitempty\" toml:\"capturedRequestHeaders,omitempty\" yaml:\"capturedRequestHeaders,omitempty\" export:\"true\"`\n+\tCapturedResponseHeaders []string           `description:\"Response headers to add as attributes for server and client spans.\" json:\"capturedResponseHeaders,omitempty\" toml:\"capturedResponseHeaders,omitempty\" yaml:\"capturedResponseHeaders,omitempty\" export:\"true\"`\n+\tSafeQueryParams         []string           `description:\"Query params to not redact.\" json:\"safeQueryParams,omitempty\" toml:\"safeQueryParams,omitempty\" yaml:\"safeQueryParams,omitempty\" export:\"true\"`\n+\tSampleRate              float64            `description:\"Sets the rate between 0.0 and 1.0 of requests to trace.\" json:\"sampleRate,omitempty\" toml:\"sampleRate,omitempty\" yaml:\"sampleRate,omitempty\" export:\"true\"`\n+\tAddInternals            bool               `description:\"Enables tracing for internal services (ping, dashboard, etc...).\" json:\"addInternals,omitempty\" toml:\"addInternals,omitempty\" yaml:\"addInternals,omitempty\" export:\"true\"`\n+\tOTLP                    *types.OTelTracing `description:\"Settings for OpenTelemetry.\" json:\"otlp,omitempty\" toml:\"otlp,omitempty\" yaml:\"otlp,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\n+\t// Deprecated: please use ResourceAttributes instead.\n+\tGlobalAttributes map[string]string `description:\"(Deprecated) Defines additional resource attributes (key:value).\" json:\"globalAttributes,omitempty\" toml:\"globalAttributes,omitempty\" yaml:\"globalAttributes,omitempty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n@@ -216,7 +217,7 @@ func (t *Tracing) SetDefaults() {\n \tt.ServiceName = \"traefik\"\n \tt.SampleRate = 1.0\n \n-\tt.OTLP = &opentelemetry.Config{}\n+\tt.OTLP = &types.OTelTracing{}\n \tt.OTLP.SetDefaults()\n }\n \n@@ -270,6 +271,10 @@ func (c *Configuration) SetEffectiveConfiguration() {\n \t\t}\n \t}\n \n+\tif c.Tracing != nil && c.Tracing.GlobalAttributes != nil && c.Tracing.ResourceAttributes == nil {\n+\t\tc.Tracing.ResourceAttributes = c.Tracing.GlobalAttributes\n+\t}\n+\n \tif c.Providers.Docker != nil {\n \t\tif c.Providers.Docker.HTTPClientTimeout < 0 {\n \t\t\tc.Providers.Docker.HTTPClientTimeout = 0\n@@ -381,6 +386,18 @@ func (c *Configuration) ValidateConfiguration() error {\n \t\t}\n \t}\n \n+\tif c.AccessLog != nil && c.AccessLog.OTLP != nil {\n+\t\tif c.AccessLog.OTLP.GRPC != nil && c.AccessLog.OTLP.GRPC.TLS != nil && c.AccessLog.OTLP.GRPC.Insecure {\n+\t\t\treturn errors.New(\"access logs OTLP GRPC: TLS and Insecure options are mutually exclusive\")\n+\t\t}\n+\t}\n+\n+\tif c.Log != nil && c.Log.OTLP != nil {\n+\t\tif c.Log.OTLP.GRPC != nil && c.Log.OTLP.GRPC.TLS != nil && c.Log.OTLP.GRPC.Insecure {\n+\t\t\treturn errors.New(\"logs OTLP GRPC: TLS and Insecure options are mutually exclusive\")\n+\t\t}\n+\t}\n+\n \tif c.Tracing != nil && c.Tracing.OTLP != nil {\n \t\tif c.Tracing.OTLP.GRPC != nil && c.Tracing.OTLP.GRPC.TLS != nil && c.Tracing.OTLP.GRPC.Insecure {\n \t\t\treturn errors.New(\"tracing OTLP GRPC: TLS and Insecure options are mutually exclusive\")\ndiff --git a/pkg/logs/otel.go b/pkg/logs/otel.go\nnew file mode 100644\nindex 0000000000..bc8f95443b\n--- /dev/null\n+++ b/pkg/logs/otel.go\n@@ -0,0 +1,120 @@\n+package logs\n+\n+import (\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"reflect\"\n+\t\"time\"\n+\n+\t\"github.com/rs/zerolog\"\n+\t\"github.com/traefik/traefik/v3/pkg/types\"\n+\totellog \"go.opentelemetry.io/otel/log\"\n+)\n+\n+// SetupOTelLogger sets up the OpenTelemetry logger.\n+func SetupOTelLogger(logger zerolog.Logger, config *types.OTelLog) (zerolog.Logger, error) {\n+\tif config == nil {\n+\t\treturn logger, nil\n+\t}\n+\n+\tprovider, err := config.NewLoggerProvider()\n+\tif err != nil {\n+\t\treturn zerolog.Logger{}, fmt.Errorf(\"setting up OpenTelemetry logger provider: %w\", err)\n+\t}\n+\n+\treturn logger.Hook(&otelLoggerHook{logger: provider.Logger(\"traefik\")}), nil\n+}\n+\n+// otelLoggerHook is a zerolog hook that forwards logs to OpenTelemetry.\n+type otelLoggerHook struct {\n+\tlogger otellog.Logger\n+}\n+\n+// Run forwards the log message to OpenTelemetry.\n+func (h *otelLoggerHook) Run(e *zerolog.Event, level zerolog.Level, message string) {\n+\tif level == zerolog.Disabled {\n+\t\treturn\n+\t}\n+\n+\t// Discard the event to avoid double logging.\n+\te.Discard()\n+\n+\tvar record otellog.Record\n+\trecord.SetTimestamp(time.Now().UTC())\n+\trecord.SetSeverity(otelLogSeverity(level))\n+\trecord.SetBody(otellog.StringValue(message))\n+\n+\t// See https://github.com/rs/zerolog/issues/493.\n+\t// This is a workaround to get the log fields from the event.\n+\t// At the moment there's no way to get the log fields from the event, so we use reflection to get the buffer and parse it.\n+\tlogData := make(map[string]any)\n+\teventBuffer := fmt.Sprintf(\"%s}\", reflect.ValueOf(e).Elem().FieldByName(\"buf\"))\n+\tif err := json.Unmarshal([]byte(eventBuffer), &logData); err != nil {\n+\t\trecord.AddAttributes(otellog.String(\"parsing_error\", fmt.Sprintf(\"parsing log fields: %s\", err)))\n+\t\th.logger.Emit(e.GetCtx(), record)\n+\t\treturn\n+\t}\n+\n+\trecordAttributes := make([]otellog.KeyValue, 0, len(logData))\n+\tfor k, v := range logData {\n+\t\tif k == \"level\" {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif k == \"time\" {\n+\t\t\teventTimestamp, ok := v.(string)\n+\t\t\tif !ok {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tt, err := time.Parse(time.RFC3339, eventTimestamp)\n+\t\t\tif err == nil {\n+\t\t\t\trecord.SetTimestamp(t)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t}\n+\t\tvar attributeValue otellog.Value\n+\t\tswitch v := v.(type) {\n+\t\tcase string:\n+\t\t\tattributeValue = otellog.StringValue(v)\n+\t\tcase int:\n+\t\t\tattributeValue = otellog.IntValue(v)\n+\t\tcase int64:\n+\t\t\tattributeValue = otellog.Int64Value(v)\n+\t\tcase float64:\n+\t\t\tattributeValue = otellog.Float64Value(v)\n+\t\tcase bool:\n+\t\t\tattributeValue = otellog.BoolValue(v)\n+\t\tcase []byte:\n+\t\t\tattributeValue = otellog.BytesValue(v)\n+\t\tdefault:\n+\t\t\tattributeValue = otellog.StringValue(fmt.Sprintf(\"%v\", v))\n+\t\t}\n+\t\trecordAttributes = append(recordAttributes, otellog.KeyValue{\n+\t\t\tKey:   k,\n+\t\t\tValue: attributeValue,\n+\t\t})\n+\t}\n+\trecord.AddAttributes(recordAttributes...)\n+\n+\th.logger.Emit(e.GetCtx(), record)\n+}\n+\n+func otelLogSeverity(level zerolog.Level) otellog.Severity {\n+\tswitch level {\n+\tcase zerolog.TraceLevel:\n+\t\treturn otellog.SeverityTrace\n+\tcase zerolog.DebugLevel:\n+\t\treturn otellog.SeverityDebug\n+\tcase zerolog.InfoLevel:\n+\t\treturn otellog.SeverityInfo\n+\tcase zerolog.WarnLevel:\n+\t\treturn otellog.SeverityWarn\n+\tcase zerolog.ErrorLevel:\n+\t\treturn otellog.SeverityError\n+\tcase zerolog.FatalLevel:\n+\t\treturn otellog.SeverityFatal\n+\tcase zerolog.PanicLevel:\n+\t\treturn otellog.SeverityFatal4\n+\tdefault:\n+\t\treturn otellog.SeverityUndefined\n+\t}\n+}\ndiff --git a/pkg/metrics/opentelemetry.go b/pkg/metrics/otel.go\nsimilarity index 99%\nrename from pkg/metrics/opentelemetry.go\nrename to pkg/metrics/otel.go\nindex 21f91a89ab..2b1b1b5b74 100644\n--- a/pkg/metrics/opentelemetry.go\n+++ b/pkg/metrics/otel.go\n@@ -237,7 +237,7 @@ func newOpenTelemetryMeterProvider(ctx context.Context, config *types.OTLP) (*sd\n \treturn meterProvider, nil\n }\n \n-func newHTTPExporter(ctx context.Context, config *types.OtelHTTP) (sdkmetric.Exporter, error) {\n+func newHTTPExporter(ctx context.Context, config *types.OTelHTTP) (sdkmetric.Exporter, error) {\n \tendpoint, err := url.Parse(config.Endpoint)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"invalid collector endpoint %q: %w\", config.Endpoint, err)\n@@ -269,7 +269,7 @@ func newHTTPExporter(ctx context.Context, config *types.OtelHTTP) (sdkmetric.Exp\n \treturn otlpmetrichttp.New(ctx, opts...)\n }\n \n-func newGRPCExporter(ctx context.Context, config *types.OtelGRPC) (sdkmetric.Exporter, error) {\n+func newGRPCExporter(ctx context.Context, config *types.OTelGRPC) (sdkmetric.Exporter, error) {\n \thost, port, err := net.SplitHostPort(config.Endpoint)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"invalid collector endpoint %q: %w\", config.Endpoint, err)\ndiff --git a/pkg/middlewares/accesslog/logger.go b/pkg/middlewares/accesslog/logger.go\nindex d6055795e0..dec5b29009 100644\n--- a/pkg/middlewares/accesslog/logger.go\n+++ b/pkg/middlewares/accesslog/logger.go\n@@ -23,6 +23,7 @@ import (\n \t\"github.com/traefik/traefik/v3/pkg/middlewares/capture\"\n \ttraefiktls \"github.com/traefik/traefik/v3/pkg/tls\"\n \t\"github.com/traefik/traefik/v3/pkg/types\"\n+\t\"go.opentelemetry.io/contrib/bridges/otellogrus\"\n )\n \n type key string\n@@ -52,6 +53,7 @@ func (n noopCloser) Close() error {\n }\n \n type handlerParams struct {\n+\tctx          context.Context\n \tlogDataTable *LogData\n }\n \n@@ -106,6 +108,16 @@ func NewHandler(config *types.AccessLog) (*Handler, error) {\n \t\tLevel:     logrus.InfoLevel,\n \t}\n \n+\tif config.OTLP != nil {\n+\t\totelLoggerProvider, err := config.OTLP.NewLoggerProvider()\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"setting up OpenTelemetry logger provider: %w\", err)\n+\t\t}\n+\n+\t\tlogger.Hooks.Add(otellogrus.NewHook(\"traefik\", otellogrus.WithLoggerProvider(otelLoggerProvider)))\n+\t\tlogger.Out = io.Discard\n+\t}\n+\n \t// Transform header names to a canonical form, to be used as is without further transformations,\n \t// and transform field names to lower case, to enable case-insensitive lookup.\n \tif config.Fields != nil {\n@@ -150,7 +162,7 @@ func NewHandler(config *types.AccessLog) (*Handler, error) {\n \t\tgo func() {\n \t\t\tdefer logHandler.wg.Done()\n \t\t\tfor handlerParams := range logHandler.logHandlerChan {\n-\t\t\t\tlogHandler.logTheRoundTrip(handlerParams.logDataTable)\n+\t\t\t\tlogHandler.logTheRoundTrip(handlerParams.ctx, handlerParams.logDataTable)\n \t\t\t}\n \t\t}()\n \t}\n@@ -256,12 +268,13 @@ func (h *Handler) ServeHTTP(rw http.ResponseWriter, req *http.Request, next http\n \n \t\tif h.config.BufferingSize > 0 {\n \t\t\th.logHandlerChan <- handlerParams{\n+\t\t\t\tctx:          req.Context(),\n \t\t\t\tlogDataTable: logDataTable,\n \t\t\t}\n \t\t\treturn\n \t\t}\n \n-\t\th.logTheRoundTrip(logDataTable)\n+\t\th.logTheRoundTrip(req.Context(), logDataTable)\n \t}()\n \n \tnext.ServeHTTP(rw, reqWithDataTable)\n@@ -313,7 +326,7 @@ func usernameIfPresent(theURL *url.URL) string {\n }\n \n // Logging handler to log frontend name, backend name, and elapsed time.\n-func (h *Handler) logTheRoundTrip(logDataTable *LogData) {\n+func (h *Handler) logTheRoundTrip(ctx context.Context, logDataTable *LogData) {\n \tcore := logDataTable.Core\n \n \tretryAttempts, ok := core[RetryAttempts].(int)\n@@ -359,7 +372,7 @@ func (h *Handler) logTheRoundTrip(logDataTable *LogData) {\n \n \t\th.mu.Lock()\n \t\tdefer h.mu.Unlock()\n-\t\th.logger.WithFields(fields).Println()\n+\t\th.logger.WithContext(ctx).WithFields(fields).Println()\n \t}\n }\n \ndiff --git a/pkg/middlewares/observability/entrypoint.go b/pkg/middlewares/observability/entrypoint.go\nindex 4849f25b2d..5d1d1b8776 100644\n--- a/pkg/middlewares/observability/entrypoint.go\n+++ b/pkg/middlewares/observability/entrypoint.go\n@@ -9,6 +9,7 @@ import (\n \t\"time\"\n \n \t\"github.com/containous/alice\"\n+\t\"github.com/rs/zerolog/log\"\n \t\"github.com/traefik/traefik/v3/pkg/metrics\"\n \t\"github.com/traefik/traefik/v3/pkg/middlewares\"\n \t\"github.com/traefik/traefik/v3/pkg/middlewares/accesslog\"\n@@ -64,7 +65,11 @@ func (e *entryPointTracing) ServeHTTP(rw http.ResponseWriter, req *http.Request)\n \tstart := time.Now()\n \ttracingCtx, span := e.tracer.Start(tracingCtx, \"EntryPoint\", trace.WithSpanKind(trace.SpanKindServer), trace.WithTimestamp(start))\n \n-\treq = req.WithContext(tracingCtx)\n+\t// Associate the request context with the logger.\n+\tlogger := log.Ctx(tracingCtx).With().Ctx(tracingCtx).Logger()\n+\tloggerCtx := logger.WithContext(tracingCtx)\n+\n+\treq = req.WithContext(loggerCtx)\n \n \tspan.SetAttributes(attribute.String(\"entry_point\", e.entryPoint))\n \ndiff --git a/pkg/server/middleware/observability.go b/pkg/server/middleware/observability.go\nindex cf8386999a..902938acb4 100644\n--- a/pkg/server/middleware/observability.go\n+++ b/pkg/server/middleware/observability.go\n@@ -54,6 +54,12 @@ func (o *ObservabilityMgr) BuildEPChain(ctx context.Context, entryPointName stri\n \t\t}\n \t}\n \n+\t// As the Entry point observability middleware ensures that the tracing is added to the request and logger context,\n+\t// it needs to be added before the access log middleware to ensure that the trace ID is logged.\n+\tif (o.tracer != nil && o.ShouldAddTracing(resourceName)) || (o.metricsRegistry != nil && o.metricsRegistry.IsEpEnabled() && o.ShouldAddMetrics(resourceName)) {\n+\t\tchain = chain.Append(observability.WrapEntryPointHandler(ctx, o.tracer, o.semConvMetricRegistry, entryPointName))\n+\t}\n+\n \tif o.accessLoggerMiddleware != nil && o.ShouldAddAccessLogs(resourceName) {\n \t\tchain = chain.Append(accesslog.WrapHandler(o.accessLoggerMiddleware))\n \t\tchain = chain.Append(func(next http.Handler) (http.Handler, error) {\n@@ -61,10 +67,6 @@ func (o *ObservabilityMgr) BuildEPChain(ctx context.Context, entryPointName stri\n \t\t})\n \t}\n \n-\tif (o.tracer != nil && o.ShouldAddTracing(resourceName)) || (o.metricsRegistry != nil && o.metricsRegistry.IsEpEnabled() && o.ShouldAddMetrics(resourceName)) {\n-\t\tchain = chain.Append(observability.WrapEntryPointHandler(ctx, o.tracer, o.semConvMetricRegistry, entryPointName))\n-\t}\n-\n \tif o.metricsRegistry != nil && o.metricsRegistry.IsEpEnabled() && o.ShouldAddMetrics(resourceName) {\n \t\tmetricsHandler := metricsMiddle.WrapEntryPointHandler(ctx, o.metricsRegistry, entryPointName)\n \ndiff --git a/pkg/tracing/tracing.go b/pkg/tracing/tracing.go\nindex 1dc2f898d1..884c96b7ca 100644\n--- a/pkg/tracing/tracing.go\n+++ b/pkg/tracing/tracing.go\n@@ -13,7 +13,7 @@ import (\n \n \t\"github.com/rs/zerolog/log\"\n \t\"github.com/traefik/traefik/v3/pkg/config/static\"\n-\t\"github.com/traefik/traefik/v3/pkg/tracing/opentelemetry\"\n+\t\"github.com/traefik/traefik/v3/pkg/types\"\n \t\"go.opentelemetry.io/contrib/propagators/autoprop\"\n \t\"go.opentelemetry.io/otel\"\n \t\"go.opentelemetry.io/otel/attribute\"\n@@ -38,7 +38,7 @@ func NewTracing(conf *static.Tracing) (*Tracer, io.Closer, error) {\n \n \tif backend == nil {\n \t\tlog.Debug().Msg(\"Could not initialize tracing, using OpenTelemetry by default\")\n-\t\tdefaultBackend := &opentelemetry.Config{}\n+\t\tdefaultBackend := &types.OTelTracing{}\n \t\tbackend = defaultBackend\n \t}\n \ndiff --git a/pkg/types/logs.go b/pkg/types/logs.go\nindex e0a50ec712..87aef4e404 100644\n--- a/pkg/types/logs.go\n+++ b/pkg/types/logs.go\n@@ -1,6 +1,22 @@\n package types\n \n-import \"github.com/traefik/paerser/types\"\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"net\"\n+\t\"net/url\"\n+\n+\t\"github.com/traefik/paerser/types\"\n+\t\"github.com/traefik/traefik/v3/pkg/version\"\n+\t\"go.opentelemetry.io/otel/attribute\"\n+\t\"go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc\"\n+\t\"go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp\"\n+\totelsdk \"go.opentelemetry.io/otel/sdk/log\"\n+\t\"go.opentelemetry.io/otel/sdk/resource\"\n+\tsemconv \"go.opentelemetry.io/otel/semconv/v1.27.0\"\n+\t\"google.golang.org/grpc/credentials\"\n+\t\"google.golang.org/grpc/encoding/gzip\"\n+)\n \n const (\n \t// AccessLogKeep is the keep string value.\n@@ -14,11 +30,10 @@ const (\n const (\n \t// CommonFormat is the common logging format (CLF).\n \tCommonFormat string = \"common\"\n-\n-\t// JSONFormat is the JSON logging format.\n-\tJSONFormat string = \"json\"\n )\n \n+const OTelTraefikServiceName = \"traefik\"\n+\n // TraefikLog holds the configuration settings for the traefik logger.\n type TraefikLog struct {\n \tLevel   string `description:\"Log level set to traefik logs.\" json:\"level,omitempty\" toml:\"level,omitempty\" yaml:\"level,omitempty\" export:\"true\"`\n@@ -30,6 +45,8 @@ type TraefikLog struct {\n \tMaxAge     int    `description:\"Maximum number of days to retain old log files based on the timestamp encoded in their filename.\" json:\"maxAge,omitempty\" toml:\"maxAge,omitempty\" yaml:\"maxAge,omitempty\" export:\"true\"`\n \tMaxBackups int    `description:\"Maximum number of old log files to retain.\" json:\"maxBackups,omitempty\" toml:\"maxBackups,omitempty\" yaml:\"maxBackups,omitempty\" export:\"true\"`\n \tCompress   bool   `description:\"Determines if the rotated log files should be compressed using gzip.\" json:\"compress,omitempty\" toml:\"compress,omitempty\" yaml:\"compress,omitempty\" export:\"true\"`\n+\n+\tOTLP *OTelLog `description:\"Settings for OpenTelemetry.\" json:\"otlp,omitempty\" toml:\"otlp,omitempty\" yaml:\"otlp,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n@@ -46,6 +63,8 @@ type AccessLog struct {\n \tFields        *AccessLogFields  `description:\"AccessLogFields.\" json:\"fields,omitempty\" toml:\"fields,omitempty\" yaml:\"fields,omitempty\" export:\"true\"`\n \tBufferingSize int64             `description:\"Number of access log lines to process in a buffered way.\" json:\"bufferingSize,omitempty\" toml:\"bufferingSize,omitempty\" yaml:\"bufferingSize,omitempty\" export:\"true\"`\n \tAddInternals  bool              `description:\"Enables access log for internal services (ping, dashboard, etc...).\" json:\"addInternals,omitempty\" toml:\"addInternals,omitempty\" yaml:\"addInternals,omitempty\" export:\"true\"`\n+\n+\tOTLP *OTelLog `description:\"Settings for OpenTelemetry.\" json:\"otlp,omitempty\" toml:\"otlp,omitempty\" yaml:\"otlp,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n@@ -128,3 +147,123 @@ func checkFieldHeaderValue(value, defaultValue string) string {\n \t}\n \treturn defaultValue\n }\n+\n+// OTelLog provides configuration settings for the open-telemetry logger.\n+type OTelLog struct {\n+\tServiceName        string            `description:\"Set the name for this service.\" json:\"serviceName,omitempty\" toml:\"serviceName,omitempty\" yaml:\"serviceName,omitempty\" export:\"true\"`\n+\tResourceAttributes map[string]string `description:\"Defines additional resource attributes (key:value).\" json:\"resourceAttributes,omitempty\" toml:\"resourceAttributes,omitempty\" yaml:\"resourceAttributes,omitempty\"`\n+\tGRPC               *OTelGRPC         `description:\"gRPC configuration for the OpenTelemetry collector.\" json:\"grpc,omitempty\" toml:\"grpc,omitempty\" yaml:\"grpc,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\tHTTP               *OTelHTTP         `description:\"HTTP configuration for the OpenTelemetry collector.\" json:\"http,omitempty\" toml:\"http,omitempty\" yaml:\"http,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+}\n+\n+// SetDefaults sets the default values.\n+func (o *OTelLog) SetDefaults() {\n+\to.ServiceName = OTelTraefikServiceName\n+\to.HTTP = &OTelHTTP{}\n+\to.HTTP.SetDefaults()\n+}\n+\n+// NewLoggerProvider creates a new OpenTelemetry logger provider.\n+func (o *OTelLog) NewLoggerProvider() (*otelsdk.LoggerProvider, error) {\n+\tvar (\n+\t\terr      error\n+\t\texporter otelsdk.Exporter\n+\t)\n+\tif o.GRPC != nil {\n+\t\texporter, err = o.buildGRPCExporter()\n+\t} else {\n+\t\texporter, err = o.buildHTTPExporter()\n+\t}\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"setting up exporter: %w\", err)\n+\t}\n+\n+\tattr := []attribute.KeyValue{\n+\t\tsemconv.ServiceNameKey.String(o.ServiceName),\n+\t\tsemconv.ServiceVersionKey.String(version.Version),\n+\t}\n+\n+\tfor k, v := range o.ResourceAttributes {\n+\t\tattr = append(attr, attribute.String(k, v))\n+\t}\n+\n+\tres, err := resource.New(context.Background(),\n+\t\tresource.WithAttributes(attr...),\n+\t\tresource.WithFromEnv(),\n+\t\tresource.WithTelemetrySDK(),\n+\t\tresource.WithOSType(),\n+\t\tresource.WithProcessCommandArgs(),\n+\t)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"building resource: %w\", err)\n+\t}\n+\n+\t// Register the trace provider to allow the global logger to access it.\n+\tbp := otelsdk.NewBatchProcessor(exporter)\n+\tloggerProvider := otelsdk.NewLoggerProvider(\n+\t\totelsdk.WithResource(res),\n+\t\totelsdk.WithProcessor(bp),\n+\t)\n+\n+\treturn loggerProvider, nil\n+}\n+\n+func (o *OTelLog) buildHTTPExporter() (*otlploghttp.Exporter, error) {\n+\tendpoint, err := url.Parse(o.HTTP.Endpoint)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"invalid collector endpoint %q: %w\", o.HTTP.Endpoint, err)\n+\t}\n+\n+\topts := []otlploghttp.Option{\n+\t\totlploghttp.WithEndpoint(endpoint.Host),\n+\t\totlploghttp.WithHeaders(o.HTTP.Headers),\n+\t\totlploghttp.WithCompression(otlploghttp.GzipCompression),\n+\t}\n+\n+\tif endpoint.Scheme == \"http\" {\n+\t\topts = append(opts, otlploghttp.WithInsecure())\n+\t}\n+\n+\tif endpoint.Path != \"\" {\n+\t\topts = append(opts, otlploghttp.WithURLPath(endpoint.Path))\n+\t}\n+\n+\tif o.HTTP.TLS != nil {\n+\t\ttlsConfig, err := o.HTTP.TLS.CreateTLSConfig(context.Background())\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"creating TLS client config: %w\", err)\n+\t\t}\n+\n+\t\topts = append(opts, otlploghttp.WithTLSClientConfig(tlsConfig))\n+\t}\n+\n+\treturn otlploghttp.New(context.Background(), opts...)\n+}\n+\n+func (o *OTelLog) buildGRPCExporter() (*otlploggrpc.Exporter, error) {\n+\thost, port, err := net.SplitHostPort(o.GRPC.Endpoint)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"invalid collector endpoint %q: %w\", o.GRPC.Endpoint, err)\n+\t}\n+\n+\topts := []otlploggrpc.Option{\n+\t\totlploggrpc.WithEndpoint(fmt.Sprintf(\"%s:%s\", host, port)),\n+\t\totlploggrpc.WithHeaders(o.GRPC.Headers),\n+\t\totlploggrpc.WithCompressor(gzip.Name),\n+\t}\n+\n+\tif o.GRPC.Insecure {\n+\t\topts = append(opts, otlploggrpc.WithInsecure())\n+\t}\n+\n+\tif o.GRPC.TLS != nil {\n+\t\ttlsConfig, err := o.GRPC.TLS.CreateTLSConfig(context.Background())\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"creating TLS client config: %w\", err)\n+\t\t}\n+\n+\t\topts = append(opts, otlploggrpc.WithTLSCredentials(credentials.NewTLS(tlsConfig)))\n+\t}\n+\n+\treturn otlploggrpc.New(context.Background(), opts...)\n+}\ndiff --git a/pkg/types/metrics.go b/pkg/types/metrics.go\nindex 88b62c0031..8ca8dec68d 100644\n--- a/pkg/types/metrics.go\n+++ b/pkg/types/metrics.go\n@@ -108,8 +108,8 @@ func (i *InfluxDB2) SetDefaults() {\n \n // OTLP contains specific configuration used by the OpenTelemetry Metrics exporter.\n type OTLP struct {\n-\tGRPC *OtelGRPC `description:\"gRPC configuration for the OpenTelemetry collector.\" json:\"grpc,omitempty\" toml:\"grpc,omitempty\" yaml:\"grpc,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n-\tHTTP *OtelHTTP `description:\"HTTP configuration for the OpenTelemetry collector.\" json:\"http,omitempty\" toml:\"http,omitempty\" yaml:\"http,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\tGRPC *OTelGRPC `description:\"gRPC configuration for the OpenTelemetry collector.\" json:\"grpc,omitempty\" toml:\"grpc,omitempty\" yaml:\"grpc,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\tHTTP *OTelHTTP `description:\"HTTP configuration for the OpenTelemetry collector.\" json:\"http,omitempty\" toml:\"http,omitempty\" yaml:\"http,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n \n \tAddEntryPointsLabels bool           `description:\"Enable metrics on entry points.\" json:\"addEntryPointsLabels,omitempty\" toml:\"addEntryPointsLabels,omitempty\" yaml:\"addEntryPointsLabels,omitempty\" export:\"true\"`\n \tAddRoutersLabels     bool           `description:\"Enable metrics on routers.\" json:\"addRoutersLabels,omitempty\" toml:\"addRoutersLabels,omitempty\" yaml:\"addRoutersLabels,omitempty\" export:\"true\"`\n@@ -121,14 +121,14 @@ type OTLP struct {\n \n // SetDefaults sets the default values.\n func (o *OTLP) SetDefaults() {\n-\to.HTTP = &OtelHTTP{}\n+\to.HTTP = &OTelHTTP{}\n \to.HTTP.SetDefaults()\n \n \to.AddEntryPointsLabels = true\n \to.AddServicesLabels = true\n \to.ExplicitBoundaries = []float64{.005, .01, .025, .05, .075, .1, .25, .5, .75, 1, 2.5, 5, 7.5, 10}\n \to.PushInterval = types.Duration(10 * time.Second)\n-\to.ServiceName = \"traefik\"\n+\to.ServiceName = OTelTraefikServiceName\n }\n \n // Statistics provides options for monitoring request and response stats.\n@@ -140,28 +140,3 @@ type Statistics struct {\n func (s *Statistics) SetDefaults() {\n \ts.RecentErrors = 10\n }\n-\n-// OtelGRPC provides configuration settings for the gRPC open-telemetry.\n-type OtelGRPC struct {\n-\tEndpoint string            `description:\"Sets the gRPC endpoint (host:port) of the collector.\" json:\"endpoint,omitempty\" toml:\"endpoint,omitempty\" yaml:\"endpoint,omitempty\"`\n-\tInsecure bool              `description:\"Disables client transport security for the exporter.\" json:\"insecure,omitempty\" toml:\"insecure,omitempty\" yaml:\"insecure,omitempty\" export:\"true\"`\n-\tTLS      *ClientTLS        `description:\"Defines client transport security parameters.\" json:\"tls,omitempty\" toml:\"tls,omitempty\" yaml:\"tls,omitempty\" export:\"true\"`\n-\tHeaders  map[string]string `description:\"Headers sent with payload.\" json:\"headers,omitempty\" toml:\"headers,omitempty\" yaml:\"headers,omitempty\"`\n-}\n-\n-// SetDefaults sets the default values.\n-func (c *OtelGRPC) SetDefaults() {\n-\tc.Endpoint = \"localhost:4317\"\n-}\n-\n-// OtelHTTP provides configuration settings for the HTTP open-telemetry.\n-type OtelHTTP struct {\n-\tEndpoint string            `description:\"Sets the HTTP endpoint (scheme://host:port/path) of the collector.\" json:\"endpoint,omitempty\" toml:\"endpoint,omitempty\" yaml:\"endpoint,omitempty\"`\n-\tTLS      *ClientTLS        `description:\"Defines client transport security parameters.\" json:\"tls,omitempty\" toml:\"tls,omitempty\" yaml:\"tls,omitempty\" export:\"true\"`\n-\tHeaders  map[string]string `description:\"Headers sent with payload.\" json:\"headers,omitempty\" toml:\"headers,omitempty\" yaml:\"headers,omitempty\"`\n-}\n-\n-// SetDefaults sets the default values.\n-func (c *OtelHTTP) SetDefaults() {\n-\tc.Endpoint = \"https://localhost:4318\"\n-}\ndiff --git a/pkg/types/otel.go b/pkg/types/otel.go\nnew file mode 100644\nindex 0000000000..281d83e1d4\n--- /dev/null\n+++ b/pkg/types/otel.go\n@@ -0,0 +1,26 @@\n+package types\n+\n+// OTelGRPC provides configuration settings for the gRPC open-telemetry.\n+type OTelGRPC struct {\n+\tEndpoint string            `description:\"Sets the gRPC endpoint (host:port) of the collector.\" json:\"endpoint,omitempty\" toml:\"endpoint,omitempty\" yaml:\"endpoint,omitempty\"`\n+\tInsecure bool              `description:\"Disables client transport security for the exporter.\" json:\"insecure,omitempty\" toml:\"insecure,omitempty\" yaml:\"insecure,omitempty\" export:\"true\"`\n+\tTLS      *ClientTLS        `description:\"Defines client transport security parameters.\" json:\"tls,omitempty\" toml:\"tls,omitempty\" yaml:\"tls,omitempty\" export:\"true\"`\n+\tHeaders  map[string]string `description:\"Headers sent with payload.\" json:\"headers,omitempty\" toml:\"headers,omitempty\" yaml:\"headers,omitempty\"`\n+}\n+\n+// SetDefaults sets the default values.\n+func (o *OTelGRPC) SetDefaults() {\n+\to.Endpoint = \"localhost:4317\"\n+}\n+\n+// OTelHTTP provides configuration settings for the HTTP open-telemetry.\n+type OTelHTTP struct {\n+\tEndpoint string            `description:\"Sets the HTTP endpoint (scheme://host:port/path) of the collector.\" json:\"endpoint,omitempty\" toml:\"endpoint,omitempty\" yaml:\"endpoint,omitempty\"`\n+\tTLS      *ClientTLS        `description:\"Defines client transport security parameters.\" json:\"tls,omitempty\" toml:\"tls,omitempty\" yaml:\"tls,omitempty\" export:\"true\"`\n+\tHeaders  map[string]string `description:\"Headers sent with payload.\" json:\"headers,omitempty\" toml:\"headers,omitempty\" yaml:\"headers,omitempty\"`\n+}\n+\n+// SetDefaults sets the default values.\n+func (o *OTelHTTP) SetDefaults() {\n+\to.Endpoint = \"https://localhost:4318\"\n+}\ndiff --git a/pkg/tracing/opentelemetry/opentelemetry.go b/pkg/types/tracing.go\nsimilarity index 80%\nrename from pkg/tracing/opentelemetry/opentelemetry.go\nrename to pkg/types/tracing.go\nindex cc15335240..2346e3b69c 100644\n--- a/pkg/tracing/opentelemetry/opentelemetry.go\n+++ b/pkg/types/tracing.go\n@@ -1,4 +1,4 @@\n-package opentelemetry\n+package types\n \n import (\n \t\"context\"\n@@ -9,7 +9,6 @@ import (\n \t\"time\"\n \n \t\"github.com/rs/zerolog/log\"\n-\t\"github.com/traefik/traefik/v3/pkg/types\"\n \t\"github.com/traefik/traefik/v3/pkg/version\"\n \t\"go.opentelemetry.io/otel\"\n \t\"go.opentelemetry.io/otel/attribute\"\n@@ -18,26 +17,26 @@ import (\n \t\"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp\"\n \t\"go.opentelemetry.io/otel/sdk/resource\"\n \tsdktrace \"go.opentelemetry.io/otel/sdk/trace\"\n-\tsemconv \"go.opentelemetry.io/otel/semconv/v1.26.0\"\n+\tsemconv \"go.opentelemetry.io/otel/semconv/v1.27.0\"\n \t\"go.opentelemetry.io/otel/trace\"\n \t\"google.golang.org/grpc/credentials\"\n \t\"google.golang.org/grpc/encoding/gzip\"\n )\n \n-// Config provides configuration settings for the open-telemetry tracer.\n-type Config struct {\n-\tGRPC *types.OtelGRPC `description:\"gRPC configuration for the OpenTelemetry collector.\" json:\"grpc,omitempty\" toml:\"grpc,omitempty\" yaml:\"grpc,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n-\tHTTP *types.OtelHTTP `description:\"HTTP configuration for the OpenTelemetry collector.\" json:\"http,omitempty\" toml:\"http,omitempty\" yaml:\"http,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+// OTelTracing provides configuration settings for the open-telemetry tracer.\n+type OTelTracing struct {\n+\tGRPC *OTelGRPC `description:\"gRPC configuration for the OpenTelemetry collector.\" json:\"grpc,omitempty\" toml:\"grpc,omitempty\" yaml:\"grpc,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n+\tHTTP *OTelHTTP `description:\"HTTP configuration for the OpenTelemetry collector.\" json:\"http,omitempty\" toml:\"http,omitempty\" yaml:\"http,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n-func (c *Config) SetDefaults() {\n-\tc.HTTP = &types.OtelHTTP{}\n+func (c *OTelTracing) SetDefaults() {\n+\tc.HTTP = &OTelHTTP{}\n \tc.HTTP.SetDefaults()\n }\n \n // Setup sets up the tracer.\n-func (c *Config) Setup(serviceName string, sampleRate float64, globalAttributes map[string]string) (trace.Tracer, io.Closer, error) {\n+func (c *OTelTracing) Setup(serviceName string, sampleRate float64, globalAttributes map[string]string) (trace.Tracer, io.Closer, error) {\n \tvar (\n \t\terr      error\n \t\texporter *otlptrace.Exporter\n@@ -87,7 +86,7 @@ func (c *Config) Setup(serviceName string, sampleRate float64, globalAttributes\n \treturn tracerProvider.Tracer(\"github.com/traefik/traefik\"), &tpCloser{provider: tracerProvider}, err\n }\n \n-func (c *Config) setupHTTPExporter() (*otlptrace.Exporter, error) {\n+func (c *OTelTracing) setupHTTPExporter() (*otlptrace.Exporter, error) {\n \tendpoint, err := url.Parse(c.HTTP.Endpoint)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"invalid collector endpoint %q: %w\", c.HTTP.Endpoint, err)\n@@ -119,7 +118,7 @@ func (c *Config) setupHTTPExporter() (*otlptrace.Exporter, error) {\n \treturn otlptrace.New(context.Background(), otlptracehttp.NewClient(opts...))\n }\n \n-func (c *Config) setupGRPCExporter() (*otlptrace.Exporter, error) {\n+func (c *OTelTracing) setupGRPCExporter() (*otlptrace.Exporter, error) {\n \thost, port, err := net.SplitHostPort(c.GRPC.Endpoint)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"invalid collector endpoint %q: %w\", c.GRPC.Endpoint, err)\n", "instance_id": "traefik__traefik-11319", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the goal of adding support for logging via OpenTelemetry (OTLP) in Traefik Proxy v3, with a specific use case provided (separating log types in a Kubernetes environment using metadata). It outlines the motivation and expected benefits, such as routing telemetry through collectors for better management. However, it lacks detailed technical specifications, such as explicit input/output formats, specific constraints, or comprehensive examples of log structures with OTLP. Additionally, edge cases or potential challenges (e.g., performance impact, compatibility issues) are not mentioned. While the intent and context are clear, these missing details prevent it from being fully comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files and modules in the Traefik codebase, including logging setup, configuration handling, and integration with OpenTelemetry libraries for both HTTP and gRPC protocols. This requires understanding interactions between different components (e.g., logging, middleware, and observability systems) and impacts core functionality like log output behavior. Second, the number of technical concepts involved is substantial, including familiarity with OpenTelemetry's logging APIs (e.g., otel/sdk/log, otlp exporters), zerolog integration, gRPC/HTTP client configurations, TLS setup, and Traefik's internal architecture for logging and middleware. Implementing this also involves updating documentation extensively, which adds to the workload. Third, while edge cases are not explicitly mentioned in the problem statement, the code changes reveal the need to handle configurations like TLS vs. insecure modes, protocol-specific settings, and log field parsing (with a workaround using reflection due to zerolog limitations), which introduces complexity in error handling and compatibility. Although it does not reach the \"Very Hard\" range (0.8-1.0) as it leverages existing OpenTelemetry libraries rather than requiring novel algorithm design or system-level innovations, the depth of integration and cross-module impact justify a score of 0.65, reflecting a challenging task that demands a deep understanding of the codebase and observability standards.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Log files not persisted on container restart\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nThe defined log files (traefik.log and access.log) on the host are wiped and start afresh when the container is restarted. This behaviour seems to be specific to v3 as it does not seem to be present on v2 with the same configuration.\n\n### What did you see instead?\n\nLogs should remain if the container restarts - file should be appended.\n\n### What version of Traefik are you using?\n\nVersion:      3.0.0\r\nCodename:     beaufort\r\nGo version:   go1.22.2\r\nBuilt:        2024-04-29T14:25:59Z\r\nOS/Arch:      linux/amd64\n\n### What is your environment & configuration?\n\n```yaml\r\n  traefik:\r\n    container_name: \"traefik\"\r\n    image: \"traefik:latest\"\r\n    restart: \"unless-stopped\"\r\n    depends_on:\r\n      - cetusguard\r\n    networks:\r\n      - \"cetusguard\"\r\n      - \"traefik\"\r\n    healthcheck:\r\n    # Run traefik healthcheck command\r\n    # https://doc.traefik.io/traefik/operations/cli/#healthcheck\r\n      test: [\"CMD\", \"traefik\", \"healthcheck\", \"--ping\"]\r\n      interval: 10s\r\n      timeout: 5s\r\n      retries: 3\r\n      start_period: 5s\r\n    ports:\r\n      # To be able to listen on port 80 (http)\r\n      - mode: host\r\n        published: 80\r\n        target: 80\r\n        protocol: tcp\r\n      # To be able to listen on port 443 (https)\r\n      - mode: host\r\n        published: 443\r\n        target: 443\r\n        protocol: tcp\r\n      # To enable HTTP/3 (QUIC)\r\n      - mode: host\r\n        published: 443\r\n        target: 443\r\n        protocol: udp\r\n    security_opt:\r\n      - no-new-privileges:true\r\n    cap_drop:\r\n      - ALL\r\n    secrets:\r\n      - do_auth_token\r\n    environment:\r\n      - DO_AUTH_TOKEN_FILE=/run/secrets/do_auth_token\r\n    volumes:\r\n      - type: bind\r\n        source: /etc/localtime\r\n        target: /etc/localtime\r\n        read_only: true\r\n      - type: bind                                                    # Location of data folder\r\n        source: /etc/traefik/data\r\n        target: /data\r\n      - type: bind\r\n        source: /var/log/traefik\r\n        target: /var/log/traefik\r\n      - type: bind                                                    # Location to write certificates to\r\n        source: /etc/traefik/acme/acme.json\r\n        target: /acme.json\r\n      - type: bind\r\n        source: /etc/traefik/traefik.yml                              # Static Traefik Configuration\r\n        target: /etc/traefik/traefik.yml\r\n        read_only: true\r\n      - type: bind\r\n        source: /etc/traefik/conf.d\r\n        target: /etc/traefik/conf.d\r\n        read_only: true\r\n```\r\n\r\ntraefik.yml\r\n\r\n```yaml\r\nlog:\r\n  filePath: \"/var/log/traefik/traefik.log\"\r\n  level: DEBUG\r\n  maxBackups: 5\r\n\r\naccessLog:\r\n  filePath: \"/var/log/traefik/access.log\"\r\n  bufferingSize: 20\r\n  filters:\r\n    statusCodes:\r\n      - \"204-299\"\r\n      - \"400-499\"\r\n      - \"500-599\"\r\n    retryAttempts: true\r\n```\r\n\n\n### If applicable, please paste the log output in DEBUG level\n\n_No response_\n", "patch": "diff --git a/cmd/traefik/logger.go b/cmd/traefik/logger.go\nindex 4a3a10893a..58b7abbe3d 100644\n--- a/cmd/traefik/logger.go\n+++ b/cmd/traefik/logger.go\n@@ -49,7 +49,7 @@ func getLogWriter(staticConfiguration *static.Configuration) io.Writer {\n \tvar w io.Writer = os.Stderr\n \n \tif staticConfiguration.Log != nil && len(staticConfiguration.Log.FilePath) > 0 {\n-\t\t_, _ = os.Create(staticConfiguration.Log.FilePath)\n+\t\t_, _ = os.OpenFile(staticConfiguration.Log.FilePath, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0o666)\n \t\tw = &lumberjack.Logger{\n \t\t\tFilename:   staticConfiguration.Log.FilePath,\n \t\t\tMaxSize:    staticConfiguration.Log.MaxSize,\n", "instance_id": "traefik__traefik-10756", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: log files are not persisted on container restart in Traefik v3, unlike in v2, and the expectation is that logs should be appended rather than overwritten. The user provides relevant configuration details (Docker Compose and Traefik YAML files) and specifies the version of Traefik being used. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention whether this behavior is observed under specific conditions (e.g., particular host OS, filesystem, or container runtime settings). Additionally, there are no examples of log output or specific scenarios where the issue manifests, which could help in understanding the problem's scope. Edge cases, such as file permissions or behavior during concurrent access, are not addressed. Despite these minor gaps, the goal and context are sufficiently clear for a developer familiar with containerized environments and logging systems.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.25) based on the provided code changes and the nature of the issue. Let's break it down by the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a minimal change in a single file (`logger.go`), specifically modifying the file opening mode from `os.Create` (which truncates the file if it exists) to `os.OpenFile` with the `os.O_APPEND` flag (which appends to the file). This is a small, localized change that does not impact the broader architecture of Traefik or require modifications across multiple modules. The amount of code change is trivial, involving just one line.\n\n2. **Number of Technical Concepts**: Solving this issue requires basic knowledge of file I/O operations in Go, specifically understanding the behavior of `os.Create` versus `os.OpenFile` with append mode. Familiarity with logging libraries like `lumberjack` (used in the codebase for log rotation) is helpful but not critical for this fix. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic file handling are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change itself introduces minimal risk. Potential issues like file permission errors or concurrent access are already managed by the existing `lumberjack.Logger` implementation, and the change does not significantly alter error handling logic. However, a developer should verify that appending works as expected across different filesystems and container environments, though this is not complex.\n\n4. **Overall Complexity**: The fix is straightforward and does not require deep understanding of the Traefik codebase or its architecture. It is a simple bug fix that addresses a specific behavior (file truncation on restart) with a well-known solution (using append mode). The impact is limited to logging behavior, with no performance or system-level considerations.\n\nGiven these factors, the problem falls into the easy category (0.2-0.4). It requires understanding basic file I/O logic in Go and making a simple, isolated modification. The score of 0.25 reflects that while the fix is easy, it still requires a minimal level of investigation to confirm the root cause (file truncation) and ensure the solution works in a containerized environment.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Option to use native Kubernetes load-balacing at global level\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you expect to see?\n\nCurrently, Traefik forwards incoming traffic directly to pods, bypassing native Kubernetes services. However, when utilizing service meshes to manage cluster traffic from Ingress to pods, it is necessary to forward the traffic to meshed Kubernetes services. Currently, the option to achieve this behavior involves setting `nativeLB` at the Ingressroute level. It would be beneficial if there were an option to specify this at the provider level, especially when dealing with large production clusters, eliminating the need to consistently set this parameter at the route level.\n", "patch": "diff --git a/docs/content/providers/kubernetes-crd.md b/docs/content/providers/kubernetes-crd.md\nindex f0ef2da39f..2f60a5868d 100644\n--- a/docs/content/providers/kubernetes-crd.md\n+++ b/docs/content/providers/kubernetes-crd.md\n@@ -337,6 +337,30 @@ providers:\n --providers.kubernetescrd.allowexternalnameservices=true\n ```\n \n+### `nativeLBByDefault`\n+\n+_Optional, Default: false_\n+\n+Defines whether to use Native Kubernetes load-balancing mode by default.\n+For more information, please check out the IngressRoute `nativeLB` option [documentation](../routing/providers/kubernetes-crd.md#load-balancing).\n+\n+```yaml tab=\"File (YAML)\"\n+providers:\n+  kubernetesCRD:\n+    nativeLBByDefault: true\n+    # ...\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[providers.kubernetesCRD]\n+  nativeLBByDefault = true\n+  # ...\n+```\n+\n+```bash tab=\"CLI\"\n+--providers.kubernetescrd.nativeLBByDefault=true\n+```\n+\n ## Full Example\n \n For additional information, refer to the [full example](../user-guides/crd-acme/index.md) with Let's Encrypt.\ndiff --git a/docs/content/providers/kubernetes-ingress.md b/docs/content/providers/kubernetes-ingress.md\nindex dc08cde099..f60ed243ac 100644\n--- a/docs/content/providers/kubernetes-ingress.md\n+++ b/docs/content/providers/kubernetes-ingress.md\n@@ -467,6 +467,30 @@ providers:\n --providers.kubernetesingress.allowexternalnameservices=true\n ```\n \n+### `nativeLBByDefault`\n+\n+_Optional, Default: false_\n+\n+Defines whether to use Native Kubernetes load-balancing mode by default.\n+For more information, please check out the `traefik.ingress.kubernetes.io/service.nativelb` [service annotation documentation](../routing/providers/kubernetes-ingress.md#on-service).\n+\n+```yaml tab=\"File (YAML)\"\n+providers:\n+  kubernetesIngress:\n+    nativeLBByDefault: true\n+    # ...\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[providers.kubernetesIngress]\n+  nativeLBByDefault = true\n+  # ...\n+```\n+\n+```bash tab=\"CLI\"\n+--providers.kubernetesingress.nativeLBByDefault=true\n+```\n+\n ### Further\n \n To learn more about the various aspects of the Ingress specification that Traefik supports,\ndiff --git a/docs/content/reference/static-configuration/cli-ref.md b/docs/content/reference/static-configuration/cli-ref.md\nindex d0d339bf14..99065ee630 100644\n--- a/docs/content/reference/static-configuration/cli-ref.md\n+++ b/docs/content/reference/static-configuration/cli-ref.md\n@@ -714,6 +714,9 @@ Kubernetes label selector to use.\n `--providers.kubernetescrd.namespaces`:  \n Kubernetes namespaces.\n \n+`--providers.kubernetescrd.nativelbbydefault`:  \n+Defines whether to use Native Kubernetes load-balancing mode by default. (Default: ```false```)\n+\n `--providers.kubernetescrd.throttleduration`:  \n Ingress refresh throttle duration (Default: ```0```)\n \n@@ -795,6 +798,9 @@ Kubernetes Ingress label selector to use.\n `--providers.kubernetesingress.namespaces`:  \n Kubernetes namespaces.\n \n+`--providers.kubernetesingress.nativelbbydefault`:  \n+Defines whether to use Native Kubernetes load-balancing mode by default. (Default: ```false```)\n+\n `--providers.kubernetesingress.throttleduration`:  \n Ingress refresh throttle duration (Default: ```0```)\n \ndiff --git a/docs/content/reference/static-configuration/env-ref.md b/docs/content/reference/static-configuration/env-ref.md\nindex 5c4242bd58..5d8313abbb 100644\n--- a/docs/content/reference/static-configuration/env-ref.md\n+++ b/docs/content/reference/static-configuration/env-ref.md\n@@ -714,6 +714,9 @@ Kubernetes label selector to use.\n `TRAEFIK_PROVIDERS_KUBERNETESCRD_NAMESPACES`:  \n Kubernetes namespaces.\n \n+`TRAEFIK_PROVIDERS_KUBERNETESCRD_NATIVELBBYDEFAULT`:  \n+Defines whether to use Native Kubernetes load-balancing mode by default. (Default: ```false```)\n+\n `TRAEFIK_PROVIDERS_KUBERNETESCRD_THROTTLEDURATION`:  \n Ingress refresh throttle duration (Default: ```0```)\n \n@@ -795,6 +798,9 @@ Kubernetes Ingress label selector to use.\n `TRAEFIK_PROVIDERS_KUBERNETESINGRESS_NAMESPACES`:  \n Kubernetes namespaces.\n \n+`TRAEFIK_PROVIDERS_KUBERNETESINGRESS_NATIVELBBYDEFAULT`:  \n+Defines whether to use Native Kubernetes load-balancing mode by default. (Default: ```false```)\n+\n `TRAEFIK_PROVIDERS_KUBERNETESINGRESS_THROTTLEDURATION`:  \n Ingress refresh throttle duration (Default: ```0```)\n \ndiff --git a/docs/content/reference/static-configuration/file.toml b/docs/content/reference/static-configuration/file.toml\nindex 9d796c4a62..c3ee80b197 100644\n--- a/docs/content/reference/static-configuration/file.toml\n+++ b/docs/content/reference/static-configuration/file.toml\n@@ -124,6 +124,7 @@\n     allowEmptyServices = true\n     allowExternalNameServices = true\n     disableIngressClassLookup = true\n+    nativeLBByDefault = true\n     [providers.kubernetesIngress.ingressEndpoint]\n       ip = \"foobar\"\n       hostname = \"foobar\"\n@@ -139,6 +140,7 @@\n     ingressClass = \"foobar\"\n     throttleDuration = \"42s\"\n     allowEmptyServices = true\n+    nativeLBByDefault = true\n   [providers.kubernetesGateway]\n     endpoint = \"foobar\"\n     token = \"foobar\"\ndiff --git a/docs/content/reference/static-configuration/file.yaml b/docs/content/reference/static-configuration/file.yaml\nindex 477fa6b0ec..f96e50f992 100644\n--- a/docs/content/reference/static-configuration/file.yaml\n+++ b/docs/content/reference/static-configuration/file.yaml\n@@ -141,6 +141,7 @@ providers:\n     allowEmptyServices: true\n     allowExternalNameServices: true\n     disableIngressClassLookup: true\n+    nativeLBByDefault: true\n   kubernetesCRD:\n     endpoint: foobar\n     token: foobar\n@@ -154,6 +155,7 @@ providers:\n     ingressClass: foobar\n     throttleDuration: 42s\n     allowEmptyServices: true\n+    nativeLBByDefault: true\n   kubernetesGateway:\n     endpoint: foobar\n     token: foobar\ndiff --git a/pkg/provider/kubernetes/crd/fixtures/tcp/with_global_native_service_lb.yml b/pkg/provider/kubernetes/crd/fixtures/tcp/with_global_native_service_lb.yml\nnew file mode 100644\nindex 0000000000..d8832498e3\n--- /dev/null\n+++ b/pkg/provider/kubernetes/crd/fixtures/tcp/with_global_native_service_lb.yml\n@@ -0,0 +1,15 @@\n+apiVersion: traefik.io/v1alpha1\n+kind: IngressRouteTCP\n+metadata:\n+  name: global-native-lb\n+  namespace: default\n+\n+spec:\n+  entryPoints:\n+    - foo\n+\n+  routes:\n+  - match: HostSNI(`foo.com`)\n+    services:\n+    - name: native-svc-tcp\n+      port: 8000\ndiff --git a/pkg/provider/kubernetes/crd/fixtures/udp/with_global_native_service_lb.yml b/pkg/provider/kubernetes/crd/fixtures/udp/with_global_native_service_lb.yml\nnew file mode 100644\nindex 0000000000..5e7dbd2e54\n--- /dev/null\n+++ b/pkg/provider/kubernetes/crd/fixtures/udp/with_global_native_service_lb.yml\n@@ -0,0 +1,14 @@\n+apiVersion: traefik.io/v1alpha1\n+kind: IngressRouteUDP\n+metadata:\n+  name: global-native-lb\n+  namespace: default\n+\n+spec:\n+  entryPoints:\n+    - foo\n+\n+  routes:\n+  - services:\n+    - name: native-svc-udp\n+      port: 8000\ndiff --git a/pkg/provider/kubernetes/crd/fixtures/with_global_native_service_lb.yml b/pkg/provider/kubernetes/crd/fixtures/with_global_native_service_lb.yml\nnew file mode 100644\nindex 0000000000..9b8fa25816\n--- /dev/null\n+++ b/pkg/provider/kubernetes/crd/fixtures/with_global_native_service_lb.yml\n@@ -0,0 +1,16 @@\n+apiVersion: traefik.io/v1alpha1\n+kind: IngressRoute\n+metadata:\n+  name: global-native-lb\n+  namespace: default\n+\n+spec:\n+  entryPoints:\n+    - foo\n+\n+  routes:\n+  - match: Host(`foo.com`)\n+    kind: Rule\n+    services:\n+    - name: native-svc\n+      port: 80\ndiff --git a/pkg/provider/kubernetes/crd/kubernetes.go b/pkg/provider/kubernetes/crd/kubernetes.go\nindex 1559e44e97..298bf884ff 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes.go\n@@ -60,6 +60,7 @@ type Provider struct {\n \tIngressClass              string              `description:\"Value of kubernetes.io/ingress.class annotation to watch for.\" json:\"ingressClass,omitempty\" toml:\"ingressClass,omitempty\" yaml:\"ingressClass,omitempty\" export:\"true\"`\n \tThrottleDuration          ptypes.Duration     `description:\"Ingress refresh throttle duration\" json:\"throttleDuration,omitempty\" toml:\"throttleDuration,omitempty\" yaml:\"throttleDuration,omitempty\" export:\"true\"`\n \tAllowEmptyServices        bool                `description:\"Allow the creation of services without endpoints.\" json:\"allowEmptyServices,omitempty\" toml:\"allowEmptyServices,omitempty\" yaml:\"allowEmptyServices,omitempty\" export:\"true\"`\n+\tNativeLBByDefault         bool                `description:\"Defines whether to use Native Kubernetes load-balancing mode by default.\" json:\"nativeLBByDefault,omitempty\" toml:\"nativeLBByDefault,omitempty\" yaml:\"nativeLBByDefault,omitempty\" export:\"true\"`\n \n \tlastConfiguration safe.Safe\n \ndiff --git a/pkg/provider/kubernetes/crd/kubernetes_http.go b/pkg/provider/kubernetes/crd/kubernetes_http.go\nindex 6e3b1f2a53..e7364482cb 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes_http.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes_http.go\n@@ -55,6 +55,7 @@ func (p *Provider) loadIngressRouteConfiguration(ctx context.Context, client Cli\n \t\t\tallowCrossNamespace:       p.AllowCrossNamespace,\n \t\t\tallowExternalNameServices: p.AllowExternalNameServices,\n \t\t\tallowEmptyServices:        p.AllowEmptyServices,\n+\t\t\tNativeLBByDefault:         p.NativeLBByDefault,\n \t\t}\n \n \t\tfor _, route := range ingressRoute.Spec.Routes {\n@@ -202,6 +203,7 @@ type configBuilder struct {\n \tallowCrossNamespace       bool\n \tallowExternalNameServices bool\n \tallowEmptyServices        bool\n+\tNativeLBByDefault         bool\n }\n \n // buildTraefikService creates the configuration for the traefik service defined in tService,\n@@ -377,20 +379,6 @@ func (c configBuilder) loadServers(parentNamespace string, svc traefikv1alpha1.L\n \t\treturn nil, err\n \t}\n \n-\tif svc.NativeLB {\n-\t\taddress, err := getNativeServiceAddress(*service, *svcPort)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n-\t\t}\n-\n-\t\tprotocol, err := parseServiceProtocol(svc.Scheme, svcPort.Name, svcPort.Port)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\treturn []dynamic.Server{{URL: fmt.Sprintf(\"%s://%s\", protocol, address)}}, nil\n-\t}\n-\n \tvar servers []dynamic.Server\n \tif service.Spec.Type == corev1.ServiceTypeExternalName {\n \t\tif !c.allowExternalNameServices {\n@@ -409,6 +397,24 @@ func (c configBuilder) loadServers(parentNamespace string, svc traefikv1alpha1.L\n \t\t}), nil\n \t}\n \n+\tnativeLB := c.NativeLBByDefault\n+\tif svc.NativeLB != nil {\n+\t\tnativeLB = *svc.NativeLB\n+\t}\n+\tif nativeLB {\n+\t\taddress, err := getNativeServiceAddress(*service, *svcPort)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n+\t\t}\n+\n+\t\tprotocol, err := parseServiceProtocol(svc.Scheme, svcPort.Name, svcPort.Port)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\treturn []dynamic.Server{{URL: fmt.Sprintf(\"%s://%s\", protocol, address)}}, nil\n+\t}\n+\n \tendpoints, endpointsExists, endpointsErr := c.client.GetEndpoints(namespace, sanitizedName)\n \tif endpointsErr != nil {\n \t\treturn nil, endpointsErr\ndiff --git a/pkg/provider/kubernetes/crd/kubernetes_tcp.go b/pkg/provider/kubernetes/crd/kubernetes_tcp.go\nindex f41fe125e7..ea955adae5 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes_tcp.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes_tcp.go\n@@ -237,21 +237,25 @@ func (p *Provider) loadTCPServers(client Client, namespace string, svc traefikv1\n \t\treturn nil, err\n \t}\n \n-\tif svc.NativeLB {\n-\t\taddress, err := getNativeServiceAddress(*service, *svcPort)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n-\t\t}\n-\n-\t\treturn []dynamic.TCPServer{{Address: address}}, nil\n-\t}\n-\n \tvar servers []dynamic.TCPServer\n \tif service.Spec.Type == corev1.ServiceTypeExternalName {\n \t\tservers = append(servers, dynamic.TCPServer{\n \t\t\tAddress: net.JoinHostPort(service.Spec.ExternalName, strconv.Itoa(int(svcPort.Port))),\n \t\t})\n \t} else {\n+\t\tnativeLB := p.NativeLBByDefault\n+\t\tif svc.NativeLB != nil {\n+\t\t\tnativeLB = *svc.NativeLB\n+\t\t}\n+\t\tif nativeLB {\n+\t\t\taddress, err := getNativeServiceAddress(*service, *svcPort)\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n+\t\t\t}\n+\n+\t\t\treturn []dynamic.TCPServer{{Address: address}}, nil\n+\t\t}\n+\n \t\tendpoints, endpointsExists, endpointsErr := client.GetEndpoints(namespace, svc.Name)\n \t\tif endpointsErr != nil {\n \t\t\treturn nil, endpointsErr\ndiff --git a/pkg/provider/kubernetes/crd/kubernetes_udp.go b/pkg/provider/kubernetes/crd/kubernetes_udp.go\nindex 417e90d996..c98d0212ad 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes_udp.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes_udp.go\n@@ -121,21 +121,25 @@ func (p *Provider) loadUDPServers(client Client, namespace string, svc traefikv1\n \t\treturn nil, err\n \t}\n \n-\tif svc.NativeLB {\n-\t\taddress, err := getNativeServiceAddress(*service, *svcPort)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n-\t\t}\n-\n-\t\treturn []dynamic.UDPServer{{Address: address}}, nil\n-\t}\n-\n \tvar servers []dynamic.UDPServer\n \tif service.Spec.Type == corev1.ServiceTypeExternalName {\n \t\tservers = append(servers, dynamic.UDPServer{\n \t\t\tAddress: net.JoinHostPort(service.Spec.ExternalName, strconv.Itoa(int(svcPort.Port))),\n \t\t})\n \t} else {\n+\t\tnativeLB := p.NativeLBByDefault\n+\t\tif svc.NativeLB != nil {\n+\t\t\tnativeLB = *svc.NativeLB\n+\t\t}\n+\t\tif nativeLB {\n+\t\t\taddress, err := getNativeServiceAddress(*service, *svcPort)\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n+\t\t\t}\n+\n+\t\t\treturn []dynamic.UDPServer{{Address: address}}, nil\n+\t\t}\n+\n \t\tendpoints, endpointsExists, endpointsErr := client.GetEndpoints(namespace, svc.Name)\n \t\tif endpointsErr != nil {\n \t\t\treturn nil, endpointsErr\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go\nindex 475d160d44..89718b7452 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroute.go\n@@ -125,7 +125,7 @@ type LoadBalancerSpec struct {\n \t// whether the LB's children are directly the pods IPs or if the only child is the Kubernetes Service clusterIP.\n \t// The Kubernetes Service itself does load-balance to the pods.\n \t// By default, NativeLB is false.\n-\tNativeLB bool `json:\"nativeLB,omitempty\"`\n+\tNativeLB *bool `json:\"nativeLB,omitempty\"`\n }\n \n type ResponseForwarding struct {\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroutetcp.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroutetcp.go\nindex 9ff6897db4..e37050f195 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroutetcp.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressroutetcp.go\n@@ -92,7 +92,7 @@ type ServiceTCP struct {\n \t// whether the LB's children are directly the pods IPs or if the only child is the Kubernetes Service clusterIP.\n \t// The Kubernetes Service itself does load-balance to the pods.\n \t// By default, NativeLB is false.\n-\tNativeLB bool `json:\"nativeLB,omitempty\"`\n+\tNativeLB *bool `json:\"nativeLB,omitempty\"`\n }\n \n // +genclient\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressrouteudp.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressrouteudp.go\nindex 18773f4372..c77559a50a 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressrouteudp.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/ingressrouteudp.go\n@@ -37,7 +37,7 @@ type ServiceUDP struct {\n \t// whether the LB's children are directly the pods IPs or if the only child is the Kubernetes Service clusterIP.\n \t// The Kubernetes Service itself does load-balance to the pods.\n \t// By default, NativeLB is false.\n-\tNativeLB bool `json:\"nativeLB,omitempty\"`\n+\tNativeLB *bool `json:\"nativeLB,omitempty\"`\n }\n \n // +genclient\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go\nindex 547f0de4e8..a17ff54840 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go\n@@ -577,6 +577,11 @@ func (in *LoadBalancerSpec) DeepCopyInto(out *LoadBalancerSpec) {\n \t\t*out = new(int)\n \t\t**out = **in\n \t}\n+\tif in.NativeLB != nil {\n+\t\tin, out := &in.NativeLB, &out.NativeLB\n+\t\t*out = new(bool)\n+\t\t**out = **in\n+\t}\n \treturn\n }\n \n@@ -1333,6 +1338,11 @@ func (in *ServiceTCP) DeepCopyInto(out *ServiceTCP) {\n \t\t*out = new(dynamic.ProxyProtocol)\n \t\t**out = **in\n \t}\n+\tif in.NativeLB != nil {\n+\t\tin, out := &in.NativeLB, &out.NativeLB\n+\t\t*out = new(bool)\n+\t\t**out = **in\n+\t}\n \treturn\n }\n \n@@ -1355,6 +1365,11 @@ func (in *ServiceUDP) DeepCopyInto(out *ServiceUDP) {\n \t\t*out = new(int)\n \t\t**out = **in\n \t}\n+\tif in.NativeLB != nil {\n+\t\tin, out := &in.NativeLB, &out.NativeLB\n+\t\t*out = new(bool)\n+\t\t**out = **in\n+\t}\n \treturn\n }\n \ndiff --git a/pkg/provider/kubernetes/ingress/annotations.go b/pkg/provider/kubernetes/ingress/annotations.go\nindex 134b2eda2d..d3bed8cc00 100644\n--- a/pkg/provider/kubernetes/ingress/annotations.go\n+++ b/pkg/provider/kubernetes/ingress/annotations.go\n@@ -45,7 +45,7 @@ type ServiceIng struct {\n \tServersTransport string          `json:\"serversTransport,omitempty\"`\n \tPassHostHeader   *bool           `json:\"passHostHeader\"`\n \tSticky           *dynamic.Sticky `json:\"sticky,omitempty\" label:\"allowEmpty\"`\n-\tNativeLB         bool            `json:\"nativeLB,omitempty\"`\n+\tNativeLB         *bool           `json:\"nativeLB,omitempty\"`\n }\n \n // SetDefaults sets the default values.\ndiff --git a/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-native-lb-by-default.yml b/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-native-lb-by-default.yml\nnew file mode 100644\nindex 0000000000..26d1cbf594\n--- /dev/null\n+++ b/pkg/provider/kubernetes/ingress/fixtures/Ingress-with-native-lb-by-default.yml\n@@ -0,0 +1,30 @@\n+kind: Ingress\n+apiVersion: networking.k8s.io/v1\n+metadata:\n+  name: global-native-lb\n+  namespace: default\n+spec:\n+  rules:\n+  - host: traefik.tchouk\n+    http:\n+      paths:\n+      - path: /bar\n+        backend:\n+          service:\n+            name: service1\n+            port:\n+              number: 8080\n+        pathType: Prefix\n+\n+---\n+kind: Service\n+apiVersion: v1\n+metadata:\n+  name: service1\n+  namespace: default\n+spec:\n+  ports:\n+    - port: 8080\n+  clusterIP: 10.0.0.1\n+  type: ClusterIP\n+  externalName: traefik.wtf\ndiff --git a/pkg/provider/kubernetes/ingress/kubernetes.go b/pkg/provider/kubernetes/ingress/kubernetes.go\nindex 99a8351f82..f3f4aa23ed 100644\n--- a/pkg/provider/kubernetes/ingress/kubernetes.go\n+++ b/pkg/provider/kubernetes/ingress/kubernetes.go\n@@ -52,6 +52,7 @@ type Provider struct {\n \tAllowEmptyServices        bool                `description:\"Allow creation of services without endpoints.\" json:\"allowEmptyServices,omitempty\" toml:\"allowEmptyServices,omitempty\" yaml:\"allowEmptyServices,omitempty\" export:\"true\"`\n \tAllowExternalNameServices bool                `description:\"Allow ExternalName services.\" json:\"allowExternalNameServices,omitempty\" toml:\"allowExternalNameServices,omitempty\" yaml:\"allowExternalNameServices,omitempty\" export:\"true\"`\n \tDisableIngressClassLookup bool                `description:\"Disables the lookup of IngressClasses.\" json:\"disableIngressClassLookup,omitempty\" toml:\"disableIngressClassLookup,omitempty\" yaml:\"disableIngressClassLookup,omitempty\" export:\"true\"`\n+\tNativeLBByDefault         bool                `description:\"Defines whether to use Native Kubernetes load-balancing mode by default.\" json:\"nativeLBByDefault,omitempty\" toml:\"nativeLBByDefault,omitempty\" yaml:\"nativeLBByDefault,omitempty\" export:\"true\"`\n \n \tlastConfiguration safe.Safe\n \n@@ -571,6 +572,8 @@ func (p *Provider) loadService(client Client, namespace string, backend netv1.In\n \t\treturn nil, err\n \t}\n \n+\tnativeLB := p.NativeLBByDefault\n+\n \tif svcConfig != nil && svcConfig.Service != nil {\n \t\tsvc.LoadBalancer.Sticky = svcConfig.Service.Sticky\n \n@@ -582,19 +585,8 @@ func (p *Provider) loadService(client Client, namespace string, backend netv1.In\n \t\t\tsvc.LoadBalancer.ServersTransport = svcConfig.Service.ServersTransport\n \t\t}\n \n-\t\tif svcConfig.Service.NativeLB {\n-\t\t\taddress, err := getNativeServiceAddress(*service, portSpec)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n-\t\t\t}\n-\n-\t\t\tprotocol := getProtocol(portSpec, portSpec.Name, svcConfig)\n-\n-\t\t\tsvc.LoadBalancer.Servers = []dynamic.Server{\n-\t\t\t\t{URL: fmt.Sprintf(\"%s://%s\", protocol, address)},\n-\t\t\t}\n-\n-\t\t\treturn svc, nil\n+\t\tif svcConfig.Service.NativeLB != nil {\n+\t\t\tnativeLB = *svcConfig.Service.NativeLB\n \t\t}\n \t}\n \n@@ -609,6 +601,20 @@ func (p *Provider) loadService(client Client, namespace string, backend netv1.In\n \t\treturn svc, nil\n \t}\n \n+\tif nativeLB {\n+\t\taddress, err := getNativeServiceAddress(*service, portSpec)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"getting native Kubernetes Service address: %w\", err)\n+\t\t}\n+\n+\t\tprotocol := getProtocol(portSpec, portSpec.Name, svcConfig)\n+\t\tsvc.LoadBalancer.Servers = []dynamic.Server{\n+\t\t\t{URL: fmt.Sprintf(\"%s://%s\", protocol, address)},\n+\t\t}\n+\n+\t\treturn svc, nil\n+\t}\n+\n \tendpoints, endpointsExists, endpointsErr := client.GetEndpoints(namespace, backend.Service.Name)\n \tif endpointsErr != nil {\n \t\treturn nil, endpointsErr\n", "instance_id": "traefik__traefik-10323", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a global configuration option (`nativeLBByDefault`) for enabling native Kubernetes load-balancing at the provider level in Traefik, rather than setting it individually at the route level. It provides context about the current behavior and the desired enhancement, which is helpful. However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define what \"native Kubernetes load-balancing\" entails in terms of technical behavior or implications (e.g., how it interacts with service meshes or impacts traffic routing). Additionally, there are no examples of specific use cases or configurations in the problem description itself (though the code changes provide some context). Edge cases, such as how this global setting interacts with existing per-route configurations or potential conflicts, are not mentioned. Overall, while the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, spanning multiple files and affecting both documentation and core logic in the Traefik codebase. The changes involve adding a new configuration option (`nativeLBByDefault`) to the Kubernetes provider configurations (both CRD and Ingress), updating how the load-balancing behavior is determined (prioritizing per-service settings over the global default), and ensuring compatibility across HTTP, TCP, and UDP protocols. This requires understanding the existing provider architecture and modifying logic in several places, such as changing the `NativeLB` field from a boolean to a pointer to handle explicit overrides.\n\nThe technical concepts involved include familiarity with Kubernetes networking (services, endpoints, and load-balancing), Traefik's provider model, and configuration management across YAML, TOML, and CLI formats. Additionally, the developer must understand how to handle backward compatibility (e.g., ensuring existing configurations without the new setting behave as expected). While the problem does not introduce complex algorithms or system-level changes, it does require careful handling of configuration precedence and potential edge cases, such as conflicts between global and per-route settings, though these are not explicitly detailed in the problem statement.\n\nThe changes impact multiple modules but do not fundamentally alter the system's architecture, and the amount of code change is moderate (mostly adding new fields and conditional logic). Error handling requirements appear minimal, as the focus is on configuration rather than runtime behavior. Overall, this task requires a solid understanding of the codebase and Kubernetes concepts, placing it in the medium difficulty range (0.4-0.6), with a score of 0.55 reflecting the need for cross-file modifications and conceptual clarity in a moderately complex domain.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "CopyObject() with x-amz-metadata-directive REPLACE new version is not readable\n## NOTE\r\n\r\nWhen creating a copy of the same object from a previous version using the `CopyObject` method, setting the `x-amz-metadata-directive` header to `REPLACE` and specifying the adjacent `x-amz-meta-` headers corrupts the copy.\r\n\r\n## Expected Behavior\r\n\r\n`HeadObject` request against the newly created object version must successfully complete, returning all the associated metadata.\r\n\r\n## Current Behavior\r\n\r\nAny of the following attempts to perform a `HeadObject` operation against the newly created version of the file fails with HTTP 500 error:\r\n\r\n```\r\nminio-1         | RemoteHost: 192.168.0.1\r\nminio-1         | Host: localhost:9000\r\nminio-1         | UserAgent: Boto3/1.34.79 md/Botocore#1.34.79 ua/2.0 os/linux#5.15.146.1-microsoft-standard-WSL2 md/arch#x86_64 lang/python#3.10.12 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.34.79\r\nminio-1         | Error: file is corrupted (cmd.StorageErr)\r\nminio-1         |        6: internal/logger/logger.go:259:logger.LogIf()\r\nminio-1         |        5: cmd/logging.go:90:cmd.internalLogIf()\r\nminio-1         |        4: cmd/api-errors.go:2533:cmd.toAPIError()\r\nminio-1         |        3: cmd/object-handlers.go:1073:cmd.objectAPIHandlers.headObjectHandler()\r\nminio-1         |        2: cmd/object-handlers.go:1237:cmd.objectAPIHandlers.HeadObjectHandler()\r\nminio-1         |        1: net/http/server.go:2136:http.HandlerFunc.ServeHTTP()\r\n```\r\n\r\n## Steps to Reproduce (for bugs)\r\n\r\n1. Use the compose configuration below and wait until the server has started up.\r\n2. Select a Python interpreter, install a `boto3` library and run the following snippet:\r\n\r\n```python\r\nimport boto3\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.DEBUG)\r\n\r\nkey = \"key\"\r\nbucket = \"data\"\r\n\r\nclient = boto3.client(\r\n    \"s3\",\r\n    endpoint_url=\"http://localhost:9000\",\r\n    aws_access_key_id=\"minio-root\",\r\n    aws_secret_access_key=\"minio-root\",\r\n    aws_session_token=None,\r\n    config=boto3.session.Config(signature_version=\"s3v4\"),\r\n    verify=False,\r\n)\r\n\r\nlogging.info(f\"creating an object; {key=}\")\r\nresponse = client.put_object(\r\n    Bucket=bucket,\r\n    Key=key,\r\n    Body=b\"key-data\",\r\n    Metadata={\r\n        \"metadata-key\": \"metadata-value\",\r\n    },\r\n)\r\noriginal_version_id = response[\"VersionId\"]\r\n\r\nlogging.info(f\"creating an object copy; {key=}, {original_version_id=}\")\r\nresponse = client.copy_object(\r\n    Bucket=bucket,\r\n    Key=key,\r\n    CopySource={\r\n        \"Bucket\": bucket,\r\n        \"Key\": key,\r\n        \"VersionId\": original_version_id,\r\n    },\r\n    Metadata={\r\n        \"metadata-key\": \"other-metadata-value\",\r\n    },\r\n    MetadataDirective=\"REPLACE\",\r\n)\r\ncopy_version_id = response[\"VersionId\"]\r\n\r\nlogging.info(f\"heading an object; {key=}, {copy_version_id=}\")\r\nclient.head_object(\r\n    Bucket=bucket,\r\n    Key=key,\r\n    VersionId=copy_version_id,\r\n)\r\n```\r\n\r\n## Context\r\n\r\nI want to create a copy of an existing object version, overriding its metadata.\r\n\r\n## Regression\r\n\r\nNo\r\n\r\n## Your Environment\r\n\r\n* Version used: `DEVELOPMENT.2024-04-06T05-26-02Z (commit-id=9d63bb1b418f6c1bbcc8434fff5d8aba810ee5d7)`\r\n* Server setup and configuration:\r\n```yaml\r\nversion: \"3.8\"\r\nservices:\r\n  minio:\r\n    image: \"bitnami/minio:2024.4.6\"\r\n    environment:\r\n      MINIO_ROOT_USER: minio-root\r\n      MINIO_ROOT_PASSWORD: minio-root\r\n      MINIO_DEFAULT_BUCKETS: data\r\n    ports:\r\n      - \"9000:9000\"\r\n      - \"9001:9001\"\r\n\r\n  minio-config:\r\n    image: \"bitnami/minio-client:2024.3.30\"\r\n    depends_on:\r\n      - minio\r\n    environment:\r\n      MINIO_SERVER_HOST: minio\r\n      MINIO_SERVER_ROOT_USER: minio-root\r\n      MINIO_SERVER_ROOT_PASSWORD: minio-root\r\n    command:\r\n      - /bin/bash\r\n      - -ce\r\n      - |\r\n        /opt/bitnami/scripts/minio-client/setup.sh\r\n        sleep 10\r\n        mc version enable minio/data\r\n```\r\n\r\n* Operating System and version (`uname -a`): `Linux NV-F4SHWL3 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux`\r\n\n", "patch": "diff --git a/cmd/erasure-object.go b/cmd/erasure-object.go\nindex ada14422e805b..4b40750e3fc8b 100644\n--- a/cmd/erasure-object.go\n+++ b/cmd/erasure-object.go\n@@ -147,11 +147,12 @@ func (er erasureObjects) CopyObject(ctx context.Context, srcBucket, srcObject, d\n \t\tmodTime = dstOpts.MTime\n \t\tfi.ModTime = dstOpts.MTime\n \t}\n+\t// check inline before overwriting metadata.\n+\tinlineData := fi.InlineData()\n \n \tfi.Metadata = srcInfo.UserDefined\n \tsrcInfo.UserDefined[\"etag\"] = srcInfo.ETag\n \n-\tinlineData := fi.InlineData()\n \tfreeVersionID := fi.TierFreeVersionID()\n \tfreeVersionMarker := fi.TierFreeVersion()\n \n", "instance_id": "minio__minio-19468", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue, including the expected behavior, current behavior, steps to reproduce, and relevant context. It includes a Python script to demonstrate the bug and logs showing the error (HTTP 500). The goal is well-defined: fixing a bug in the `CopyObject` method when using `x-amz-metadata-directive` as `REPLACE`, which results in a corrupted object copy that cannot be read via `HeadObject`. However, there are minor ambiguities or missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., specific metadata values or object sizes that might exacerbate the issue) or constraints on the solution (e.g., performance requirements or backward compatibility). Additionally, while the context of wanting to override metadata is provided, there is no deeper explanation of why the corruption occurs or hints at the root cause, which could help in understanding the problem fully. Overall, it is clear enough to start working on but lacks some finer details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of the code change appears to be relatively small, as shown in the diff, which involves a single file (`erasure-object.go`) and a minor modification (reordering a check for inline data before overwriting metadata). This suggests the fix itself is straightforward. However, understanding the root cause and ensuring the fix does not introduce regressions requires a moderate understanding of the MinIO codebase, specifically the `CopyObject` functionality and how metadata and inline data are handled in the erasure coding layer. The technical concepts involved include familiarity with Go, object storage semantics (e.g., S3-compatible APIs like `CopyObject` and `HeadObject`), and potentially erasure coding internals. While the code change is small, the impact could be significant since it affects object integrity and metadata handling, which are critical to the system's reliability. Edge cases, though not explicitly mentioned in the problem statement, are likely to include scenarios with different metadata configurations, large objects, or concurrent operations, which would require careful testing. Overall, this problem requires a moderate level of expertise to analyze and validate the fix, but it does not seem to involve deep architectural changes or highly complex logic, placing it in the 0.4-0.6 range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Goroutine panic and mutex leak in `getInspectData`\n### Description\n\nA goroutine panic can happen in `getInspectData` if the inspected container is being concurrently removed. Goroutine panics while holding container's mutex so subsequent operations that require the mutex (like inspecting the affected container) hang indefinitely. The container is stuck in \"Removal In Progress\" state.\n\nAFAIU the scenario is that:\n* goroutine a: `cleanupContainer` marks container as dead and releases the mutex ([here](https://github.com/moby/moby/blob/v27.4.1/daemon/delete.go#L134))\n* goroutine b: `getInspectData` is called while the mutex is hold ([here](https://github.com/moby/moby/blob/v27.4.1/daemon/inspect.go#L60))\n* goroutine b: `getInspectData` performs `container.RWLayer` `nil` check ([here](https://github.com/moby/moby/blob/v27.4.1/daemon/inspect.go#L192))\n* goroutine a: `cleanupContainer` removes the layer and sets `container.RWLayer` to `nil` **without acquiring the mutex** ([here](https://github.com/moby/moby/blob/v27.4.1/daemon/delete.go#L144))\n* goroutine b: `getInspectData` calls `container.RWLayer.Metadata()` trying to dereference nil, the goroutine panics, the mutex is leaked ([here](https://github.com/moby/moby/blob/v27.4.1/daemon/inspect.go#L199))\n\nI found that a similar (the same?) issue was discovered before and fixed in https://github.com/moby/moby/pull/36242. The fix does not work in this scenario because `container.RWLayer` can be modified right after the `nil` check by `cleanupContainer`.\n\nThe fix I tried that works is to acquire the mutex for setting `container.RWLayer = nil` in `cleanupContainer`. Simply copying `container.RWLayer` to a local variable prior to the `nil` check does not work, as go seem to be optimizing out the local reference..., anyway, the mutex approach seem to be the right one. I'll provide a PR in a moment.\n\nPanic stacktrace:\n```\n2025/01/05 02:35:53 http: panic serving @: runtime error: invalid memory address or nil pointer dereference\ngoroutine 487 [running]:\nnet/http.(*conn).serve.func1()\n        /usr/local/go/src/net/http/server.go:1903 +0xbe\npanic({0x61447a2a4820?, 0x61447bc57760?})\n        /usr/local/go/src/runtime/panic.go:770 +0x132\ngithub.com/docker/docker/layer.(*referencedRWLayer).Metadata(0xc00a366dc0?)\n        <autogenerated>:1 +0x17\ngithub.com/docker/docker/daemon.(*Daemon).getInspectData(0xc0001ef688, 0xc00058e008, 0xc00a158a08)\n        /root/build-deb/engine/daemon/inspect.go:199 +0xf38\ngithub.com/docker/docker/daemon.(*Daemon).ContainerInspectCurrent(0xc0001ef688, {0x61447a757060, 0xc00153d2f0}, {0xc00976a326?, 0xc0057aade0?}, 0x0)\n        /root/build-deb/engine/daemon/inspect.go:60 +0xce\ngithub.com/docker/docker/daemon.(*Daemon).ContainerInspect(0xc0001ef688, {0x61447a757060, 0xc00153d2f0}, {0xc00976a326, 0x40}, 0x0, {0xc00976a316?, 0x614477c896e5?})\n        /root/build-deb/engine/daemon/inspect.go:32 +0xac\ngithub.com/docker/docker/api/server/router/container.(*containerRouter).getContainersByName(0xc002143080, {0x61447a757060, 0xc00153d2f0}, {0x61447a74fe00, 0xc0018cc7e0}, 0xc00509d170?, 0xc00153cd80)\n        /root/build-deb/engine/api/server/router/container/inspect.go:15 +0xcb\ngithub.com/docker/docker/api/server/middleware.(*ExperimentalMiddleware).WrapHandler.ExperimentalMiddleware.WrapHandler.func1({0x61447a757060, 0xc00153d2f0}, {0x61447a74fe00, 0xc0018cc7e0}, 0xc006b58120, 0xc00153cd80)\n        /root/build-deb/engine/api/server/middleware/experimental.go:26 +0xa3\ngithub.com/docker/docker/api/server/middleware.(*VersionMiddleware).WrapHandler.VersionMiddleware.WrapHandler.func1({0x61447a757060, 0xc00153d290}, {0x61447a74fe00, 0xc0018cc7e0}, 0xc006b58120, 0xc00153cd80)\n        /root/build-deb/engine/api/server/middleware/version.go:84 +0x2b3\ngithub.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1({0x61447a757060, 0xc00153d290}, {0x61447a74fe00, 0xc0018cc7e0}, 0xc006b58120, 0xc00153cd80)\n        /root/build-deb/engine/pkg/authorization/middleware.go:59 +0x34a\ngithub.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1({0x61447a74fe00, 0xc0018cc7e0}, 0xc0009cdb00)\n        /root/build-deb/engine/api/server/server.go:56 +0x1ae\nnet/http.HandlerFunc.ServeHTTP(0x61447a757060?, {0x61447a74fe00?, 0xc0018cc7e0?}, 0x614479f47050?)\n        /usr/local/go/src/net/http/server.go:2171 +0x29\ngo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP(0xc000bb4b00, {0x61447a7487f0, 0xc006b9c620}, 0xc0009ccd80, {0x61447a72ad80, 0xc002196180})\n        /root/build-deb/engine/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:229 +0x1243\ngo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1({0x61447a7487f0?, 0xc006b9c620?}, 0x614479f3f201?)\n        /root/build-deb/engine/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:81 +0x35\nnet/http.HandlerFunc.ServeHTTP(0x61447a757060?, {0x61447a7487f0?, 0xc006b9c620?}, 0x614479f3f238?)\n        /usr/local/go/src/net/http/server.go:2171 +0x29\nnet/http.HandlerFunc.ServeHTTP(0xc000829e60?, {0x61447a7487f0?, 0xc006b9c620?}, 0x614477d751cf?)\n        /usr/local/go/src/net/http/server.go:2171 +0x29\ngithub.com/gorilla/mux.(*Router).ServeHTTP(0xc00039e9c0, {0x61447a7487f0, 0xc006b9c620}, 0xc009c93e60)\n        /root/build-deb/engine/vendor/github.com/gorilla/mux/mux.go:212 +0x1e2\nnet/http.serverHandler.ServeHTTP({0xc000f58030?}, {0x61447a7487f0?, 0xc006b9c620?}, 0x6?)\n        /usr/local/go/src/net/http/server.go:3142 +0x8e\nnet/http.(*conn).serve(0xc000bfa750, {0x61447a757060, 0xc000fccf90})\n        /usr/local/go/src/net/http/server.go:2044 +0x5e8\ncreated by net/http.(*Server).Serve in goroutine 611\n        /usr/local/go/src/net/http/server.go:3290 +0x4b4\n```\n\n### Reproduce\n\nTo trigger this race condition reliably:\n\n* run dockerd with an artificial delay right before `graphDriverData, err := container.RWLayer.Metadata()` (it's the use that is done right after `nil` check) in `getInspectData`, e.g. by inserting a loop (`time.Sleep` would also work, but obtained stack would be different from what I seen, as generated assembly would change a bit - without `<autogenerated>:1` frame)  \n  ```\n  (...)\n  for i := 0; i < 1000000000; i++ {\n  }\n  graphDriverData, err := rwLayer.Metadata()\n  (...)\n  ```\n* start a container: `docker run --rm -t -i --name test ubuntu:24.04`\n* in background inspect it in a loop: `while true; do docker inspect test; done`\n* exit the container\n\nThe panic happens reliably with exactly the same stack. Commands that require container lock, like `docker inspect test`, hung.\n\n\n### Expected behavior\n\nNo panic and no mutex leak.\n\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           27.4.1\n API version:       1.47\n Go version:        go1.22.10\n Git commit:        b9d17ea\n Built:             Tue Dec 17 15:45:46 2024\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          27.4.1\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.22.10\n  Git commit:       c710b88\n  Built:            Tue Dec 17 15:45:46 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.24\n  GitCommit:        88bf19b2105c8b17560993bee28a01ddc2f97182\n runc:\n  Version:          1.2.2\n  GitCommit:        v1.2.2-0-g7cb3632\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    27.4.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.19.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.32.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 10\n  Running: 9\n  Paused: 0\n  Stopped: 1\n Images: 1093\n Server Version: 27.4.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 88bf19b2105c8b17560993bee28a01ddc2f97182\n runc version: v1.2.2-0-g7cb3632\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-1018-oem\n Operating System: Ubuntu 24.04.1 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 128\n Total Memory: 125.6GiB\n Name: hostname\n ID: a97209c3-4df2-4440-8e3b-9300990077bb\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_\n", "patch": "diff --git a/daemon/delete.go b/daemon/delete.go\nindex f93a89aa1858d..244f7acfa9032 100644\n--- a/daemon/delete.go\n+++ b/daemon/delete.go\n@@ -125,6 +125,10 @@ func (daemon *Daemon) cleanupContainer(container *container.Container, config ba\n \tcontainer.Lock()\n \tcontainer.Dead = true\n \n+\t// Copy RWLayer for releasing and clear the reference while holding the container lock.\n+\trwLayer := container.RWLayer\n+\tcontainer.RWLayer = nil\n+\n \t// Save container state to disk. So that if error happens before\n \t// container meta file got removed from disk, then a restart of\n \t// docker should not make a dead container alive.\n@@ -135,13 +139,17 @@ func (daemon *Daemon) cleanupContainer(container *container.Container, config ba\n \n \t// When container creation fails and `RWLayer` has not been created yet, we\n \t// do not call `ReleaseRWLayer`\n-\tif container.RWLayer != nil {\n-\t\tif err := daemon.imageService.ReleaseLayer(container.RWLayer); err != nil {\n+\tif rwLayer != nil {\n+\t\tif err := daemon.imageService.ReleaseLayer(rwLayer); err != nil {\n+\t\t\t// Restore the reference on error as it possibly was not released.\n+\t\t\tcontainer.Lock()\n+\t\t\tcontainer.RWLayer = rwLayer\n+\t\t\tcontainer.Unlock()\n+\n \t\t\terr = errors.Wrapf(err, \"container %s\", container.ID)\n \t\t\tcontainer.SetRemovalError(err)\n \t\t\treturn err\n \t\t}\n-\t\tcontainer.RWLayer = nil\n \t} else {\n \t\tif daemon.UsesSnapshotter() {\n \t\t\tls := daemon.containerdClient.LeasesService()\n", "instance_id": "moby__moby-49228", "clarity": 3, "difficulty": 0.75, "clarity_explanation": "The problem statement is exceptionally detailed and comprehensive. It clearly describes the issue of a goroutine panic and mutex leak in the `getInspectData` function due to a race condition with `cleanupContainer`. The description includes a step-by-step explanation of the race condition scenario, references to specific lines of code in the GitHub repository, and a reproduction guide to trigger the issue reliably. Additionally, it provides a stack trace of the panic, expected behavior, and detailed environment information (Docker version, system info). The problem statement also acknowledges a previous attempt to fix a similar issue and explains why it failed, demonstrating thorough analysis. There are no significant ambiguities, and all critical details regarding the problem's context, impact, and reproduction are provided. The only minor omission is a lack of explicit mention of potential edge cases beyond the described race condition, but this does not detract from the overall clarity given the depth of information provided.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the problem involves a race condition in a concurrent system (Docker daemon), which requires a deep understanding of Go's concurrency model, including goroutines and mutexes. Solving this issue demands knowledge of how mutexes are used to prevent data races and the subtleties of Go's memory model and optimizations (e.g., the mention of local variable optimization by the Go compiler). Second, the code changes, while localized to a single file (`daemon/delete.go`), impact critical functionality in a large and complex codebase (Moby/Docker). The modification involves ensuring proper mutex locking to prevent a nil pointer dereference, which requires understanding the interaction between `getInspectData` and `cleanupContainer` functions across different modules. Third, the problem touches on advanced concepts such as resource cleanup, container lifecycle management, and error handling in a distributed system context, which are non-trivial to grasp without significant experience. Fourth, while the provided fix is relatively small in terms of lines of code, it requires careful consideration of edge cases (e.g., failure to release a layer leading to restoration of the reference) and potential performance implications of additional locking. The need to handle errors gracefully while maintaining the integrity of the container state adds further complexity. Overall, this problem requires a strong grasp of concurrent programming, familiarity with Docker's internals, and precise handling of synchronization primitives, justifying a difficulty score of 0.75. It does not reach the \"Very Hard\" range (0.8-1.0) as it does not involve system-level redesign or highly intricate domain-specific knowledge beyond Docker's container management.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Draft RFC link in pkg/archive outdated\n### Description\n\nI noticed the link to a Draft RFC in the file https://github.com/moby/moby/blob/master/pkg/archive/archive.go#L163 is outdated because the Draft RFC expired.\r\n\r\nI suggest to change\r\n```\r\nhttps://tools.ietf.org/id/draft-kucherawy-dispatch-zstd-00.html#rfc.section.2\r\n```\r\nto\r\n```\r\nhttps://datatracker.ietf.org/doc/html/rfc8878#section-3\r\n```\r\n\r\nBut i am not sure if `datatracker.ietf.org` is the preferred page for RFCs.\n\n### Reproduce\n\nTry access\r\n```\r\nhttps://tools.ietf.org/id/draft-kucherawy-dispatch-zstd-00.html#rfc.section.2\r\n```\r\n\r\nWill result in a 404 error\n\n### Expected behavior\n\nThe link to the RFC should be valid.\n\n### docker version\n\n```bash\nunrelated\n```\n\n\n### docker info\n\n```bash\nunrelated\n```\n\n\n### Additional Info\n\n_No response_\n", "patch": "diff --git a/pkg/archive/archive.go b/pkg/archive/archive.go\nindex 43133a0950a1b..3418cfc50e053 100644\n--- a/pkg/archive/archive.go\n+++ b/pkg/archive/archive.go\n@@ -159,7 +159,7 @@ func magicNumberMatcher(m []byte) matcher {\n // zstdMatcher detects zstd compression algorithm.\n // Zstandard compressed data is made of one or more frames.\n // There are two frame formats defined by Zstandard: Zstandard frames and Skippable frames.\n-// See https://tools.ietf.org/id/draft-kucherawy-dispatch-zstd-00.html#rfc.section.2 for more details.\n+// See https://datatracker.ietf.org/doc/html/rfc8878#section-3 for more details.\n func zstdMatcher() matcher {\n \treturn func(source []byte) bool {\n \t\tif bytes.HasPrefix(source, zstdMagic) {\n", "instance_id": "moby__moby-47850", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: an outdated link to a Draft RFC in a specific file (`archive.go`) that results in a 404 error. The goal (updating the link to a valid RFC reference) and the expected behavior (a valid link) are explicitly stated. The reproduction steps are straightforward, and the proposed solution (changing the URL) is provided. However, there is a minor ambiguity regarding whether `datatracker.ietf.org` is the preferred source for RFC links, as the submitter expresses uncertainty about this. This introduces a small gap in the requirements, as it leaves open the question of whether further validation or discussion is needed to confirm the correct link format or source. Additionally, no edge cases or broader implications of the link update are discussed, though they may not be relevant for this specific issue. Overall, the statement is valid and clear but lacks minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a trivial update to a single line of code\u2014a URL in a comment within the `archive.go` file. The scope of the change is extremely limited, affecting only a documentation comment with no impact on the actual logic, functionality, or architecture of the codebase. No deep understanding of the codebase, programming language features, libraries, algorithms, or domain-specific knowledge is required beyond basic familiarity with editing a file. There are no edge cases or error handling considerations mentioned or implied, as the change is purely cosmetic. The only minor complexity lies in confirming whether the proposed URL (`datatracker.ietf.org`) is the correct or preferred source for RFC references, but this is more of a procedural or editorial concern than a technical one. Given the simplicity of the task, I assign a difficulty score of 0.1, placing it in the \"Very Easy\" range (0.0-0.2), as it requires only a basic modification with minimal effort or technical depth.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Sending multiple images to Gemma3 on Mac causes an EOF\n### What is the issue?\n\nOllama crashes with a GGML assertion failure when attempting to run inference on Apple Silicon Mac using Metal GPU acceleration when given multiple images.\n\nThe issue appears to be a type mismatch in the GGML library when using the Metal framework. The code is expecting a tensor of type F32 (32-bit floating point), but received a different type, causing an assertion failure.\n\n### Relevant log output\n\n```shell\nggml-metal.m:3253: GGML_ASSERT(src1->type == GGML_TYPE_F32) failed\nSIGABRT: abort\nPC=0x18cedea60 m=61 sigcode=0\n```\n\n```\ngithub.com/ollama/ollama/ml/backend/ggml.Context.Compute({0x1400037c000, 0x600001cf4cc0, 0x141260e80, 0x0, 0x2000}, {0x1408d5131c0, 0x1, 0x141260e80?})\n        /Users/runner/work/ollama/ollama/ml/backend/ggml/ggml.go:497 +0x9c fp=0x14000599b60 sp=0x14000599ad0 pc=0x10485919c\n```\n\n### Steps to Reproduce\n\n1. Run Ollama with Metal GPU acceleration enabled (default on macOS)\n2. `ollama run gemma3:27b \"What is in these two images? ./photo.jpg ./other.jpg\"`\n3. Process crashes with the above error\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.6.0\nSupport multiple images with Gemma 3\nJust what the title says. I have been wondering for a long time why llama3.2-vision does not support it on ollama and now Gemma 3. If it is not possible, maybe someone can explain why. Thanks.\n", "patch": "diff --git a/model/input/input.go b/model/input/input.go\nindex 0cb3f3f4137..30bdcf065cb 100644\n--- a/model/input/input.go\n+++ b/model/input/input.go\n@@ -15,6 +15,12 @@ type Input struct {\n \t// stored in Multimodal, used for caching and comparing\n \t// equality.\n \tMultimodalHash uint64\n+\n+\t// SameBatch forces the following number of tokens to be processed\n+\t// in a single batch, breaking and extending batches as needed.\n+\t// Useful for things like images that must be processed in one\n+\t// shot.\n+\tSameBatch int\n }\n \n // MultimodalIndex is a multimodal element (such as an image)\ndiff --git a/model/model.go b/model/model.go\nindex fadea3246e8..53e47add9b7 100644\n--- a/model/model.go\n+++ b/model/model.go\n@@ -60,7 +60,7 @@ type MultimodalProcessor interface {\n \t// This function is also responsible for updating MultimodalHash for any Multimodal\n \t// that is modified to ensure that there is a unique hash value that accurately\n \t// represents the contents.\n-\tPostTokenize(ml.Context, []input.Input) ([]input.Input, error)\n+\tPostTokenize([]input.Input) ([]input.Input, error)\n }\n \n // Base implements the common fields and methods for all models\ndiff --git a/model/models/gemma3/model.go b/model/models/gemma3/model.go\nindex 24193f15f21..32ad80f435e 100644\n--- a/model/models/gemma3/model.go\n+++ b/model/models/gemma3/model.go\n@@ -2,10 +2,9 @@ package gemma3\n \n import (\n \t\"bytes\"\n-\t\"encoding/binary\"\n-\t\"hash/fnv\"\n \t\"image\"\n \t\"math\"\n+\t\"slices\"\n \n \t\"github.com/ollama/ollama/kvcache\"\n \t\"github.com/ollama/ollama/ml\"\n@@ -112,36 +111,23 @@ func (m *Model) EncodeMultimodal(ctx ml.Context, multimodalData []byte) (any, er\n \treturn visionOutputs, nil\n }\n \n-type imageToken struct {\n-\tembedding ml.Tensor\n-\tindex     int\n-}\n-\n-func (m *Model) PostTokenize(ctx ml.Context, inputs []input.Input) ([]input.Input, error) {\n+func (m *Model) PostTokenize(inputs []input.Input) ([]input.Input, error) {\n \tvar result []input.Input\n-\tfnvHash := fnv.New64a()\n \n \tfor _, inp := range inputs {\n \t\tif inp.Multimodal == nil {\n \t\t\tresult = append(result, inp)\n \t\t} else {\n-\t\t\timageInputs := []input.Input{\n-\t\t\t\t{Token: 108},    // \"\\n\\n\"\n-\t\t\t\t{Token: 255999}, // \"<start_of_image>\"\"\n-\t\t\t}\n-\t\t\tresult = append(result, imageInputs...)\n-\n-\t\t\t// add image embeddings\n \t\t\tinputMultimodal := inp.Multimodal.(ml.Tensor)\n \n-\t\t\tfor i := range inputMultimodal.Dim(1) {\n-\t\t\t\tfnvHash.Reset()\n-\t\t\t\tbinary.Write(fnvHash, binary.NativeEndian, inp.MultimodalHash)\n-\t\t\t\tfnvHash.Write([]byte{byte(i)})\n+\t\t\tresult = append(result,\n+\t\t\t\tinput.Input{Token: 108, SameBatch: inputMultimodal.Dim(1) + 3},               // \"\\n\\n\"\n+\t\t\t\tinput.Input{Token: 255999},                                                   // \"<start_of_image>\"\"\n+\t\t\t\tinput.Input{Multimodal: inputMultimodal, MultimodalHash: inp.MultimodalHash}, // image data is on the first placeholder\n+\t\t\t)\n \n-\t\t\t\timageToken := imageToken{embedding: inputMultimodal, index: i}\n-\t\t\t\tresult = append(result, input.Input{Multimodal: imageToken, MultimodalHash: fnvHash.Sum64()})\n-\t\t\t}\n+\t\t\t// add image token placeholders\n+\t\t\tresult = append(result, slices.Repeat([]input.Input{{Token: 0}}, inputMultimodal.Dim(1)-1)...)\n \n \t\t\tresult = append(result,\n \t\t\t\tinput.Input{Token: 256000}, // <end_of_image>\ndiff --git a/model/models/gemma3/model_text.go b/model/models/gemma3/model_text.go\nindex 7a88c09218b..567f65a5e74 100644\n--- a/model/models/gemma3/model_text.go\n+++ b/model/models/gemma3/model_text.go\n@@ -171,53 +171,20 @@ func (l *TextLayer) Forward(ctx ml.Context, layer int, hiddenState, positionIDs,\n \treturn hiddenState.Add(ctx, residual)\n }\n \n-func setImageEmbeddings(ctx ml.Context, hiddenState ml.Tensor, multimodal []input.MultimodalIndex) []int {\n-\tvar embedding ml.Tensor\n-\tvar src, dst, length int\n-\tvar except []int\n-\n-\tfor _, image := range multimodal {\n-\t\timageToken := image.Multimodal.(imageToken)\n-\t\timageSrc := imageToken.index\n-\t\timageDst := image.Index\n-\n-\t\tif embedding == nil {\n-\t\t\tembedding = imageToken.embedding\n-\t\t\tsrc = imageSrc\n-\t\t\tdst = imageDst\n-\t\t\tlength = 1\n-\t\t} else if embedding == imageToken.embedding && imageSrc+1 == src && imageDst+1 == dst {\n-\t\t\tsrc = imageSrc\n-\t\t\tdst = imageDst\n-\t\t\tlength++\n-\t\t} else if embedding == imageToken.embedding && src+length == imageSrc && dst+length == imageDst {\n-\t\t\tlength++\n-\t\t} else {\n-\t\t\tvisionOutputs := embedding.View(ctx, src*embedding.Stride(1), length*embedding.Dim(0))\n-\t\t\tctx.Forward(visionOutputs.Copy(ctx, hiddenState.View(ctx, dst*hiddenState.Stride(1), length*hiddenState.Dim(0))))\n-\n-\t\t\tembedding = imageToken.embedding\n-\t\t\tsrc = imageSrc\n-\t\t\tdst = imageDst\n-\t\t\tlength = 1\n-\t\t}\n-\n-\t\texcept = append(except, imageDst)\n-\t}\n-\n-\tif embedding != nil {\n-\t\tvisionOutputs := embedding.View(ctx, src*embedding.Stride(1), length*embedding.Dim(0))\n-\t\tctx.Forward(visionOutputs.Copy(ctx, hiddenState.View(ctx, dst*hiddenState.Stride(1), length*hiddenState.Dim(0))))\n-\t}\n-\n-\treturn except\n-}\n-\n func (m *TextModel) Forward(ctx ml.Context, inputs, positions, outputs ml.Tensor, opts input.Options, cache kvcache.Cache) ml.Tensor {\n \thiddenState := m.TokenEmbedding.Forward(ctx, inputs)\n \thiddenState = hiddenState.Scale(ctx, math.Sqrt(float64(m.TextOptions.hiddenSize)))\n \n-\texcept := setImageEmbeddings(ctx, hiddenState, opts.Multimodal)\n+\t// set image embeddings\n+\tvar except []int\n+\tfor _, image := range opts.Multimodal {\n+\t\tvisionOutputs := image.Multimodal.(ml.Tensor)\n+\t\tctx.Forward(visionOutputs.Copy(ctx, hiddenState.View(ctx, image.Index*hiddenState.Stride(1), visionOutputs.Dim(0)*visionOutputs.Dim(1))))\n+\n+\t\tfor i := range visionOutputs.Dim(1) {\n+\t\t\texcept = append(except, image.Index+i)\n+\t\t}\n+\t}\n \n \tfor i, layer := range m.Layers {\n \t\t// gemma alternates between the sliding window (local) and causal (global)\ndiff --git a/model/models/mllama/model.go b/model/models/mllama/model.go\nindex 071d77ac7a6..fa4d570cac1 100644\n--- a/model/models/mllama/model.go\n+++ b/model/models/mllama/model.go\n@@ -106,17 +106,17 @@ func (m *Model) EncodeMultimodal(ctx ml.Context, multimodalData []byte) (any, er\n \treturn m.Projector.Forward(ctx, crossAttentionStates), nil\n }\n \n-func (m *Model) PostTokenize(ctx ml.Context, inputs []input.Input) ([]input.Input, error) {\n+func (m *Model) PostTokenize(inputs []input.Input) ([]input.Input, error) {\n \tvar images []input.Input\n \tfnvHash := fnv.New64a()\n \n \tfor i := range inputs {\n \t\tif inputs[i].Multimodal == nil {\n \t\t\tif len(images) > 0 {\n-\t\t\t\tinputs[i].Multimodal = images[0].Multimodal\n+\t\t\t\tinputs[i].Multimodal = []ml.Tensor{images[0].Multimodal.(ml.Tensor)}\n \t\t\t\tinputs[i].MultimodalHash = images[0].MultimodalHash\n \t\t\t\tfor j := 1; j < len(images); j++ {\n-\t\t\t\t\tinputs[i].Multimodal = inputs[i].Multimodal.(ml.Tensor).Concat(ctx, images[j].Multimodal.(ml.Tensor), 3)\n+\t\t\t\t\tinputs[i].Multimodal = append(inputs[i].Multimodal.([]ml.Tensor), images[0].Multimodal.(ml.Tensor))\n \t\t\t\t\tfnvHash.Reset()\n \t\t\t\t\tbinary.Write(fnvHash, binary.NativeEndian, inputs[i].MultimodalHash)\n \t\t\t\t\tbinary.Write(fnvHash, binary.NativeEndian, inputs[j].MultimodalHash)\n@@ -138,7 +138,10 @@ func (m *Model) PostTokenize(ctx ml.Context, inputs []input.Input) ([]input.Inpu\n func (m *Model) Forward(ctx ml.Context, opts input.Options) (ml.Tensor, error) {\n \tvar crossAttentionStates ml.Tensor\n \tif len(opts.Multimodal) > 0 {\n-\t\tcrossAttentionStates = opts.Multimodal[len(opts.Multimodal)-1].Multimodal.(ml.Tensor)\n+\t\timages := opts.Multimodal[len(opts.Multimodal)-1].Multimodal.([]ml.Tensor)\n+\t\tif len(images) > 0 {\n+\t\t\tcrossAttentionStates = images[len(images)-1]\n+\t\t}\n \t}\n \n \tinputs, err := ctx.Input().FromIntSlice(opts.Inputs, len(opts.Inputs))\ndiff --git a/runner/ollamarunner/runner.go b/runner/ollamarunner/runner.go\nindex d6339a6157b..d4c24556c59 100644\n--- a/runner/ollamarunner/runner.go\n+++ b/runner/ollamarunner/runner.go\n@@ -34,10 +34,14 @@ import (\n \t_ \"github.com/ollama/ollama/model/models\"\n )\n \n+type contextList struct {\n+\tlist []ml.Context\n+}\n+\n type Sequence struct {\n-\t// ctx for allocating tensors that last the lifetime of the sequence, such as\n+\t// ctxs are used for allocating tensors that last the lifetime of the sequence, such as\n \t// multimodal embeddings\n-\tctx ml.Context\n+\tctxs *contextList\n \n \t// batch index\n \tiBatch int\n@@ -99,9 +103,8 @@ func (s *Server) NewSequence(prompt string, images []llm.ImageData, params NewSe\n \ts.ready.Wait()\n \n \tstartTime := time.Now()\n-\tctx := s.model.Backend().NewContext()\n \n-\tinputs, err := s.inputs(ctx, prompt, images)\n+\tinputs, ctxs, err := s.inputs(prompt, images)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"failed to process inputs: %w\", err)\n \t} else if len(inputs) == 0 {\n@@ -127,7 +130,7 @@ func (s *Server) NewSequence(prompt string, images []llm.ImageData, params NewSe\n \t// TODO(jessegross): Ingest cached history for grammar\n \n \treturn &Sequence{\n-\t\tctx:                 ctx,\n+\t\tctxs:                ctxs,\n \t\tinputs:              inputs,\n \t\tnumPromptInputs:     len(inputs),\n \t\tstartProcessingTime: startTime,\n@@ -146,7 +149,7 @@ func (s *Server) NewSequence(prompt string, images []llm.ImageData, params NewSe\n // inputs processes the prompt and images into a list of inputs\n // by splitting the prompt on [img-<n>] tags, tokenizing text and\n // decoding images\n-func (s *Server) inputs(ctx ml.Context, prompt string, images []llm.ImageData) ([]input.Input, error) {\n+func (s *Server) inputs(prompt string, images []llm.ImageData) ([]input.Input, *contextList, error) {\n \tvar inputs []input.Input\n \tvar parts []string\n \tvar matches [][]string\n@@ -161,12 +164,19 @@ func (s *Server) inputs(ctx ml.Context, prompt string, images []llm.ImageData) (\n \t\tparts = []string{prompt}\n \t}\n \n+\tvar contexts contextList\n+\truntime.AddCleanup(&contexts, func(ctxs []ml.Context) {\n+\t\tfor _, ctx := range ctxs {\n+\t\t\tctx.Close()\n+\t\t}\n+\t}, contexts.list)\n+\n \tpostTokenize := false\n \tfor i, part := range parts {\n \t\t// text - tokenize\n \t\ttokens, err := s.model.(model.TextProcessor).Encode(part, i == 0)\n \t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\t\treturn nil, nil, err\n \t\t}\n \n \t\tfor _, t := range tokens {\n@@ -186,12 +196,14 @@ func (s *Server) inputs(ctx ml.Context, prompt string, images []llm.ImageData) (\n \t\t\t}\n \n \t\t\tif imageIndex < 0 {\n-\t\t\t\treturn nil, fmt.Errorf(\"invalid image index: %d\", n)\n+\t\t\t\treturn nil, nil, fmt.Errorf(\"invalid image index: %d\", n)\n \t\t\t}\n \n+\t\t\tctx := s.model.Backend().NewContext()\n+\t\t\tcontexts.list = append(contexts.list, ctx)\n \t\t\timageEmbeddings, err := multimodalProcessor.EncodeMultimodal(ctx, images[imageIndex].Data)\n \t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n+\t\t\t\treturn nil, nil, err\n \t\t\t}\n \n \t\t\ts.multimodalHash.Reset()\n@@ -205,13 +217,13 @@ func (s *Server) inputs(ctx ml.Context, prompt string, images []llm.ImageData) (\n \n \tif visionModel && postTokenize {\n \t\tvar err error\n-\t\tinputs, err = multimodalProcessor.PostTokenize(ctx, inputs)\n+\t\tinputs, err = multimodalProcessor.PostTokenize(inputs)\n \t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\t\treturn nil, nil, err\n \t\t}\n \t}\n \n-\treturn inputs, nil\n+\treturn inputs, &contexts, nil\n }\n \n type Server struct {\n@@ -306,7 +318,6 @@ func (s *Server) removeSequence(seqIndex int, reason string) {\n \tclose(seq.responses)\n \tclose(seq.embedding)\n \tseq.cache.InUse = false\n-\tseq.ctx.Close()\n \ts.seqs[seqIndex] = nil\n \ts.seqsSem.Release(1)\n }\n@@ -352,6 +363,8 @@ func (s *Server) processBatch() error {\n \t\t\tseq.cache.Inputs = []input.Input{}\n \t\t}\n \n+\t\tbatchSize := s.batchSize\n+\n \t\tfor j, inp := range seq.inputs {\n \t\t\tif int32(len(seq.cache.Inputs)+len(seq.pendingInputs)+1) > s.cache.numCtx {\n \t\t\t\tif len(seq.pendingInputs) == 0 {\n@@ -364,7 +377,15 @@ func (s *Server) processBatch() error {\n \t\t\t\t}\n \t\t\t}\n \n-\t\t\tif j >= s.batchSize {\n+\t\t\t// If we are required to put following inputs into a single batch then extend the\n+\t\t\t// batch size. Since we are only extending the size the minimum amount possible, this\n+\t\t\t// will cause a break if we have pending inputs.\n+\t\t\tminBatch := 1 + inp.SameBatch\n+\t\t\tif minBatch > batchSize {\n+\t\t\t\tbatchSize = minBatch\n+\t\t\t}\n+\n+\t\t\tif len(seq.pendingInputs)+minBatch > batchSize {\n \t\t\t\tbreak\n \t\t\t}\n \ndiff --git a/server/prompt.go b/server/prompt.go\nindex d053f2a8df5..5b5b958f103 100644\n--- a/server/prompt.go\n+++ b/server/prompt.go\n@@ -26,7 +26,6 @@ func chatPrompt(ctx context.Context, m *Model, tokenize tokenizeFunc, opts *api.\n \tvar system []api.Message\n \n \tisMllama := checkMllamaModelFamily(m)\n-\tisGemma3 := checkGemma3ModelFamily(m)\n \n \tvar imageNumTokens int\n \t// TODO: Ideally we would compute this from the projector metadata but some pieces are implementation dependent\n@@ -41,7 +40,7 @@ func chatPrompt(ctx context.Context, m *Model, tokenize tokenizeFunc, opts *api.\n \tn := len(msgs) - 1\n \t// in reverse, find all messages that fit into context window\n \tfor i := n; i >= 0; i-- {\n-\t\tif (isMllama || isGemma3) && len(msgs[i].Images) > 1 {\n+\t\tif isMllama && len(msgs[i].Images) > 1 {\n \t\t\treturn \"\", nil, errTooManyImages\n \t\t}\n \n@@ -158,12 +157,3 @@ func checkMllamaModelFamily(m *Model) bool {\n \t}\n \treturn false\n }\n-\n-func checkGemma3ModelFamily(m *Model) bool {\n-\tfor _, arch := range m.Config.ModelFamilies {\n-\t\tif arch == \"gemma3\" {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n-}\n", "instance_id": "ollama__ollama-9747", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Ollama crashes with a GGML assertion failure on Apple Silicon Macs when processing multiple images with Metal GPU acceleration for the Gemma3 model. It provides relevant log output, steps to reproduce, and context about the environment (OS, GPU, CPU, and Ollama version). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for handling multiple images (e.g., should it process them sequentially or in a batch?), nor does it mention specific constraints or edge cases beyond the crash itself. Additionally, while the log points to a type mismatch in GGML, the exact root cause (e.g., why the tensor type differs) is not fully clarified, leaving some room for interpretation. Overall, the statement is valid and clear enough to understand the goal but lacks comprehensive details on edge cases and desired outcomes.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes spans multiple files and modules (e.g., `input.go`, `model.go`, `gemma3/model.go`, `runner.go`), indicating a need to understand and modify interactions across different parts of the codebase, particularly in how inputs are processed and batched for multimodal data like images. The changes involve significant logic modifications, such as introducing batch control with `SameBatch` to ensure images are processed in a single batch, and refactoring how embeddings are handled in the Gemma3 model, which impacts core functionality.\n\nSecond, the number of technical concepts required is substantial. Solving this requires knowledge of Go programming, familiarity with machine learning tensor operations (via the GGML library), understanding of GPU acceleration with Metal on macOS, and domain-specific knowledge of multimodal models (e.g., handling image embeddings alongside text tokens). Additionally, the changes touch on memory management (context allocation and cleanup) and batch processing logic, which are moderately complex.\n\nThird, while the problem statement does not explicitly mention edge cases beyond the crash with multiple images, the code changes suggest potential challenges in handling varying numbers of images, batch size limits, and ensuring compatibility with other models (e.g., mllama). Error handling logic has been modified (e.g., context cleanup, input validation), but the complexity of these edge cases appears moderate, as they are not fully explored in the statement.\n\nFinally, the impact on the system's architecture is notable but not fundamental. The changes improve input processing for multimodal data without redesigning the core system, though they require careful integration to avoid introducing new bugs or performance issues, especially given the platform-specific nature of the problem (Apple Silicon with Metal). Given the deep understanding of the codebase and domain required, along with the complexity of modifications, I assign a difficulty score of 0.75, reflecting a challenging problem that demands significant expertise but does not reach the extreme complexity of system-level redesign or highly intricate algorithms.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\u89e3\u538b\u5e26\u5bc6\u7801\u7684RAR\u548c7Z\u65f6\u63d0\u793a\u5bc6\u7801\u9519\u8bef\n### Please make sure of the following things\n\n- [x] I have read the [documentation](https://alist.nn.ci).\n\u6211\u5df2\u7ecf\u9605\u8bfb\u4e86[\u6587\u6863](https://alist.nn.ci)\u3002\n\n- [x] I'm sure there are no duplicate issues or discussions.\n\u6211\u786e\u5b9a\u6ca1\u6709\u91cd\u590d\u7684issue\u6216\u8ba8\u8bba\u3002\n\n- [x] I'm sure it's due to `AList` and not something else(such as [Network](https://alist.nn.ci/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host) ,`Dependencies` or `Operational`).\n\u6211\u786e\u5b9a\u662f`AList`\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5176\u4ed6\u539f\u56e0\uff08\u4f8b\u5982[\u7f51\u7edc](https://alist.nn.ci/zh/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host)\uff0c`\u4f9d\u8d56`\u6216`\u64cd\u4f5c`\uff09\u3002\n\n- [x] I'm sure this issue is not fixed in the latest version.\n\u6211\u786e\u5b9a\u8fd9\u4e2a\u95ee\u9898\u5728\u6700\u65b0\u7248\u672c\u4e2d\u6ca1\u6709\u88ab\u4fee\u590d\u3002\n\n\n### AList Version / AList \u7248\u672c\n\nV3.42.0\n\n### Driver used / \u4f7f\u7528\u7684\u5b58\u50a8\u9a71\u52a8\n\nOnedrive and local\n\n### Describe the bug / \u95ee\u9898\u63cf\u8ff0\n\n\u5982\u56fe\u6240\u793a\uff0c\u89e3\u538bRAR\u65f6\u4f1a\u63d0\u793awrong archive password\u5bc6\u7801\u9519\u8bef\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u7684\u5bc6\u7801\u662f\u6b63\u786e\u7684\u3002\n\n![Image](https://github.com/user-attachments/assets/7a940524-6fdf-4f9c-90d9-456add38170f)\n\n\u4f9d\u7136\u5982\u56fe\u6240\u793a\uff0c\u89e3\u538b7Z\u65f6\u4e5f\u4f1a\u62a5\u9519\uff0c\u800c\u4e14\u6ca1\u6709\u586b\u5199\u5bc6\u7801\u7684\u673a\u4f1a\u3002\n\n![Image](https://github.com/user-attachments/assets/da285d00-0c42-4457-8e9b-b637b888afbf)\n\n\u76ee\u524d\u4ec5\u53d1\u73b0RAR\u548c7Z\u6709\u95ee\u9898\u3002\n\n### Reproduction / \u590d\u73b0\u94fe\u63a5\n\n\u4e0a\u4f20\u4efb\u610f\u5e26\u5bc6\u7801\u7684RAR\u548c7Z\u540e\u9009\u62e9\u89e3\u538b\u5373\u53ef\u3002\n\n### Config / \u914d\u7f6e\n\n\u6211\u786e\u5b9a\u6211\u62e5\u6709\u5bf9\u5e94\u6743\u9650\u3002\n\n![Image](https://github.com/user-attachments/assets/712002ab-b2c2-4883-ab0b-1f988f4cd668)\n\n### Logs / \u65e5\u5fd7\n\n_No response_\n", "patch": "diff --git a/go.mod b/go.mod\nindex bdbb1c8edc9..80004710dd0 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -85,7 +85,7 @@ require (\n \tgithub.com/blevesearch/go-faiss v1.0.20 // indirect\n \tgithub.com/blevesearch/zapx/v16 v16.1.5 // indirect\n \tgithub.com/bodgit/plumbing v1.3.0 // indirect\n-\tgithub.com/bodgit/sevenzip v1.6.0 // indirect\n+\tgithub.com/bodgit/sevenzip v1.6.0\n \tgithub.com/bodgit/windows v1.0.1 // indirect\n \tgithub.com/bytedance/sonic/loader v0.1.1 // indirect\n \tgithub.com/charmbracelet/x/ansi v0.2.3 // indirect\n@@ -106,14 +106,14 @@ require (\n \tgithub.com/kr/text v0.2.0 // indirect\n \tgithub.com/matoous/go-nanoid/v2 v2.1.0 // indirect\n \tgithub.com/microcosm-cc/bluemonday v1.0.27\n-\tgithub.com/nwaples/rardecode/v2 v2.0.0-beta.4.0.20241112120701-034e449c6e78 // indirect\n+\tgithub.com/nwaples/rardecode/v2 v2.0.0-beta.4.0.20241112120701-034e449c6e78\n \tgithub.com/sorairolake/lzip-go v0.3.5 // indirect\n \tgithub.com/taruti/bytepool v0.0.0-20160310082835-5e3a9ea56543 // indirect\n \tgithub.com/therootcompany/xz v1.0.1 // indirect\n \tgithub.com/ulikunitz/xz v0.5.12 // indirect\n \tgithub.com/xhofe/115-sdk-go v0.1.3\n \tgithub.com/yuin/goldmark v1.7.8\n-\tgo4.org v0.0.0-20230225012048-214862532bf5 // indirect\n+\tgo4.org v0.0.0-20230225012048-214862532bf5\n \tresty.dev/v3 v3.0.0-beta.2 // indirect\n )\n \ndiff --git a/internal/archive/all.go b/internal/archive/all.go\nindex 18167933b1c..63206cb89ed 100644\n--- a/internal/archive/all.go\n+++ b/internal/archive/all.go\n@@ -3,5 +3,7 @@ package archive\n import (\n \t_ \"github.com/alist-org/alist/v3/internal/archive/archives\"\n \t_ \"github.com/alist-org/alist/v3/internal/archive/iso9660\"\n+\t_ \"github.com/alist-org/alist/v3/internal/archive/rardecode\"\n+\t_ \"github.com/alist-org/alist/v3/internal/archive/sevenzip\"\n \t_ \"github.com/alist-org/alist/v3/internal/archive/zip\"\n )\ndiff --git a/internal/archive/archives/archives.go b/internal/archive/archives/archives.go\nindex 6d48624fa2e..0a42cd0c512 100644\n--- a/internal/archive/archives/archives.go\n+++ b/internal/archive/archives/archives.go\n@@ -16,14 +16,18 @@ import (\n type Archives struct {\n }\n \n-func (*Archives) AcceptedExtensions() []string {\n+func (Archives) AcceptedExtensions() []string {\n \treturn []string{\n-\t\t\".br\", \".bz2\", \".gz\", \".lz4\", \".lz\", \".sz\", \".s2\", \".xz\", \".zz\", \".zst\", \".tar\", \".rar\", \".7z\",\n+\t\t\".br\", \".bz2\", \".gz\", \".lz4\", \".lz\", \".sz\", \".s2\", \".xz\", \".zz\", \".zst\", \".tar\",\n \t}\n }\n \n-func (*Archives) GetMeta(ss *stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n-\tfsys, err := getFs(ss, args)\n+func (Archives) AcceptedMultipartExtensions() map[string]tool.MultipartExtension {\n+\treturn map[string]tool.MultipartExtension{}\n+}\n+\n+func (Archives) GetMeta(ss []*stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n+\tfsys, err := getFs(ss[0], args)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -47,8 +51,8 @@ func (*Archives) GetMeta(ss *stream.SeekableStream, args model.ArchiveArgs) (mod\n \t}, nil\n }\n \n-func (*Archives) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n-\tfsys, err := getFs(ss, args.ArchiveArgs)\n+func (Archives) List(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n+\tfsys, err := getFs(ss[0], args.ArchiveArgs)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -69,8 +73,8 @@ func (*Archives) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([\n \t})\n }\n \n-func (*Archives) Extract(ss *stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n-\tfsys, err := getFs(ss, args.ArchiveArgs)\n+func (Archives) Extract(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n+\tfsys, err := getFs(ss[0], args.ArchiveArgs)\n \tif err != nil {\n \t\treturn nil, 0, err\n \t}\n@@ -85,8 +89,8 @@ func (*Archives) Extract(ss *stream.SeekableStream, args model.ArchiveInnerArgs)\n \treturn file, stat.Size(), nil\n }\n \n-func (*Archives) Decompress(ss *stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n-\tfsys, err := getFs(ss, args.ArchiveArgs)\n+func (Archives) Decompress(ss []*stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n+\tfsys, err := getFs(ss[0], args.ArchiveArgs)\n \tif err != nil {\n \t\treturn err\n \t}\n@@ -133,5 +137,5 @@ func (*Archives) Decompress(ss *stream.SeekableStream, outputPath string, args m\n var _ tool.Tool = (*Archives)(nil)\n \n func init() {\n-\ttool.RegisterTool(&Archives{})\n+\ttool.RegisterTool(Archives{})\n }\ndiff --git a/internal/archive/iso9660/iso9660.go b/internal/archive/iso9660/iso9660.go\nindex e9cb3f538ec..be107d7b4c4 100644\n--- a/internal/archive/iso9660/iso9660.go\n+++ b/internal/archive/iso9660/iso9660.go\n@@ -14,19 +14,23 @@ import (\n type ISO9660 struct {\n }\n \n-func (t *ISO9660) AcceptedExtensions() []string {\n+func (ISO9660) AcceptedExtensions() []string {\n \treturn []string{\".iso\"}\n }\n \n-func (t *ISO9660) GetMeta(ss *stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n+func (ISO9660) AcceptedMultipartExtensions() map[string]tool.MultipartExtension {\n+\treturn map[string]tool.MultipartExtension{}\n+}\n+\n+func (ISO9660) GetMeta(ss []*stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n \treturn &model.ArchiveMetaInfo{\n \t\tComment:   \"\",\n \t\tEncrypted: false,\n \t}, nil\n }\n \n-func (t *ISO9660) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n-\timg, err := getImage(ss)\n+func (ISO9660) List(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n+\timg, err := getImage(ss[0])\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -48,8 +52,8 @@ func (t *ISO9660) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) (\n \treturn ret, nil\n }\n \n-func (t *ISO9660) Extract(ss *stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n-\timg, err := getImage(ss)\n+func (ISO9660) Extract(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n+\timg, err := getImage(ss[0])\n \tif err != nil {\n \t\treturn nil, 0, err\n \t}\n@@ -63,8 +67,8 @@ func (t *ISO9660) Extract(ss *stream.SeekableStream, args model.ArchiveInnerArgs\n \treturn io.NopCloser(obj.Reader()), obj.Size(), nil\n }\n \n-func (t *ISO9660) Decompress(ss *stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n-\timg, err := getImage(ss)\n+func (ISO9660) Decompress(ss []*stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n+\timg, err := getImage(ss[0])\n \tif err != nil {\n \t\treturn err\n \t}\n@@ -92,5 +96,5 @@ func (t *ISO9660) Decompress(ss *stream.SeekableStream, outputPath string, args\n var _ tool.Tool = (*ISO9660)(nil)\n \n func init() {\n-\ttool.RegisterTool(&ISO9660{})\n+\ttool.RegisterTool(ISO9660{})\n }\ndiff --git a/internal/archive/rardecode/rardecode.go b/internal/archive/rardecode/rardecode.go\nnew file mode 100644\nindex 00000000000..cd31d1a40e0\n--- /dev/null\n+++ b/internal/archive/rardecode/rardecode.go\n@@ -0,0 +1,140 @@\n+package rardecode\n+\n+import (\n+\t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n+\t\"github.com/alist-org/alist/v3/internal/errs\"\n+\t\"github.com/alist-org/alist/v3/internal/model\"\n+\t\"github.com/alist-org/alist/v3/internal/stream\"\n+\t\"github.com/nwaples/rardecode/v2\"\n+\t\"io\"\n+\t\"os\"\n+\tstdpath \"path\"\n+\t\"strings\"\n+)\n+\n+type RarDecoder struct{}\n+\n+func (RarDecoder) AcceptedExtensions() []string {\n+\treturn []string{\".rar\"}\n+}\n+\n+func (RarDecoder) AcceptedMultipartExtensions() map[string]tool.MultipartExtension {\n+\treturn map[string]tool.MultipartExtension{\n+\t\t\".part1.rar\": {\".part%d.rar\", 2},\n+\t}\n+}\n+\n+func (RarDecoder) GetMeta(ss []*stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n+\tl, err := list(ss, args.Password)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\t_, tree := tool.GenerateMetaTreeFromFolderTraversal(l)\n+\treturn &model.ArchiveMetaInfo{\n+\t\tComment:   \"\",\n+\t\tEncrypted: false,\n+\t\tTree:      tree,\n+\t}, nil\n+}\n+\n+func (RarDecoder) List(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n+\treturn nil, errs.NotSupport\n+}\n+\n+func (RarDecoder) Extract(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n+\treader, err := getReader(ss, args.Password)\n+\tif err != nil {\n+\t\treturn nil, 0, err\n+\t}\n+\tinnerPath := strings.TrimPrefix(args.InnerPath, \"/\")\n+\tfor {\n+\t\tvar header *rardecode.FileHeader\n+\t\theader, err = reader.Next()\n+\t\tif err == io.EOF {\n+\t\t\tbreak\n+\t\t}\n+\t\tif err != nil {\n+\t\t\treturn nil, 0, err\n+\t\t}\n+\t\tif header.Name == innerPath {\n+\t\t\tif header.IsDir {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\treturn io.NopCloser(reader), header.UnPackedSize, nil\n+\t\t}\n+\t}\n+\treturn nil, 0, errs.ObjectNotFound\n+}\n+\n+func (RarDecoder) Decompress(ss []*stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n+\treader, err := getReader(ss, args.Password)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tif args.InnerPath == \"/\" {\n+\t\tfor {\n+\t\t\tvar header *rardecode.FileHeader\n+\t\t\theader, err = reader.Next()\n+\t\t\tif err == io.EOF {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tname := header.Name\n+\t\t\tif header.IsDir {\n+\t\t\t\tname = name + \"/\"\n+\t\t\t}\n+\t\t\terr = decompress(reader, header, name, outputPath)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tinnerPath := strings.TrimPrefix(args.InnerPath, \"/\")\n+\t\tinnerBase := stdpath.Base(innerPath)\n+\t\tcreatedBaseDir := false\n+\t\tfor {\n+\t\t\tvar header *rardecode.FileHeader\n+\t\t\theader, err = reader.Next()\n+\t\t\tif err == io.EOF {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tname := header.Name\n+\t\t\tif header.IsDir {\n+\t\t\t\tname = name + \"/\"\n+\t\t\t}\n+\t\t\tif name == innerPath {\n+\t\t\t\terr = _decompress(reader, header, outputPath, up)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t} else if strings.HasPrefix(name, innerPath+\"/\") {\n+\t\t\t\ttargetPath := stdpath.Join(outputPath, innerBase)\n+\t\t\t\tif !createdBaseDir {\n+\t\t\t\t\terr = os.Mkdir(targetPath, 0700)\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\treturn err\n+\t\t\t\t\t}\n+\t\t\t\t\tcreatedBaseDir = true\n+\t\t\t\t}\n+\t\t\t\trestPath := strings.TrimPrefix(name, innerPath+\"/\")\n+\t\t\t\terr = decompress(reader, header, restPath, targetPath)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+var _ tool.Tool = (*RarDecoder)(nil)\n+\n+func init() {\n+\ttool.RegisterTool(RarDecoder{})\n+}\ndiff --git a/internal/archive/rardecode/utils.go b/internal/archive/rardecode/utils.go\nnew file mode 100644\nindex 00000000000..5790ec58a22\n--- /dev/null\n+++ b/internal/archive/rardecode/utils.go\n@@ -0,0 +1,225 @@\n+package rardecode\n+\n+import (\n+\t\"fmt\"\n+\t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n+\t\"github.com/alist-org/alist/v3/internal/errs\"\n+\t\"github.com/alist-org/alist/v3/internal/model\"\n+\t\"github.com/alist-org/alist/v3/internal/stream\"\n+\t\"github.com/nwaples/rardecode/v2\"\n+\t\"io\"\n+\t\"io/fs\"\n+\t\"os\"\n+\tstdpath \"path\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"time\"\n+)\n+\n+type VolumeFile struct {\n+\tstream.SStreamReadAtSeeker\n+\tname string\n+}\n+\n+func (v *VolumeFile) Name() string {\n+\treturn v.name\n+}\n+\n+func (v *VolumeFile) Size() int64 {\n+\treturn v.SStreamReadAtSeeker.GetRawStream().GetSize()\n+}\n+\n+func (v *VolumeFile) Mode() fs.FileMode {\n+\treturn 0644\n+}\n+\n+func (v *VolumeFile) ModTime() time.Time {\n+\treturn v.SStreamReadAtSeeker.GetRawStream().ModTime()\n+}\n+\n+func (v *VolumeFile) IsDir() bool {\n+\treturn false\n+}\n+\n+func (v *VolumeFile) Sys() any {\n+\treturn nil\n+}\n+\n+func (v *VolumeFile) Stat() (fs.FileInfo, error) {\n+\treturn v, nil\n+}\n+\n+func (v *VolumeFile) Close() error {\n+\treturn nil\n+}\n+\n+type VolumeFs struct {\n+\tparts map[string]*VolumeFile\n+}\n+\n+func (v *VolumeFs) Open(name string) (fs.File, error) {\n+\tfile, ok := v.parts[name]\n+\tif !ok {\n+\t\treturn nil, fs.ErrNotExist\n+\t}\n+\treturn file, nil\n+}\n+\n+func makeOpts(ss []*stream.SeekableStream) (string, rardecode.Option, error) {\n+\tif len(ss) == 1 {\n+\t\treader, err := stream.NewReadAtSeeker(ss[0], 0)\n+\t\tif err != nil {\n+\t\t\treturn \"\", nil, err\n+\t\t}\n+\t\tfileName := \"file.rar\"\n+\t\tfsys := &VolumeFs{parts: map[string]*VolumeFile{\n+\t\t\tfileName: {SStreamReadAtSeeker: reader, name: fileName},\n+\t\t}}\n+\t\treturn fileName, rardecode.FileSystem(fsys), nil\n+\t} else {\n+\t\tparts := make(map[string]*VolumeFile, len(ss))\n+\t\tfor i, s := range ss {\n+\t\t\treader, err := stream.NewReadAtSeeker(s, 0)\n+\t\t\tif err != nil {\n+\t\t\t\treturn \"\", nil, err\n+\t\t\t}\n+\t\t\tfileName := fmt.Sprintf(\"file.part%d.rar\", i+1)\n+\t\t\tparts[fileName] = &VolumeFile{SStreamReadAtSeeker: reader, name: fileName}\n+\t\t}\n+\t\treturn \"file.part1.rar\", rardecode.FileSystem(&VolumeFs{parts: parts}), nil\n+\t}\n+}\n+\n+type WrapReader struct {\n+\tfiles []*rardecode.File\n+}\n+\n+func (r *WrapReader) Files() []tool.SubFile {\n+\tret := make([]tool.SubFile, 0, len(r.files))\n+\tfor _, f := range r.files {\n+\t\tret = append(ret, &WrapFile{File: f})\n+\t}\n+\treturn ret\n+}\n+\n+type WrapFile struct {\n+\t*rardecode.File\n+}\n+\n+func (f *WrapFile) Name() string {\n+\tif f.File.IsDir {\n+\t\treturn f.File.Name + \"/\"\n+\t}\n+\treturn f.File.Name\n+}\n+\n+func (f *WrapFile) FileInfo() fs.FileInfo {\n+\treturn &WrapFileInfo{File: f.File}\n+}\n+\n+type WrapFileInfo struct {\n+\t*rardecode.File\n+}\n+\n+func (f *WrapFileInfo) Name() string {\n+\treturn stdpath.Base(f.File.Name)\n+}\n+\n+func (f *WrapFileInfo) Size() int64 {\n+\treturn f.File.UnPackedSize\n+}\n+\n+func (f *WrapFileInfo) ModTime() time.Time {\n+\treturn f.File.ModificationTime\n+}\n+\n+func (f *WrapFileInfo) IsDir() bool {\n+\treturn f.File.IsDir\n+}\n+\n+func (f *WrapFileInfo) Sys() any {\n+\treturn nil\n+}\n+\n+func list(ss []*stream.SeekableStream, password string) (*WrapReader, error) {\n+\tfileName, fsOpt, err := makeOpts(ss)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\topts := []rardecode.Option{fsOpt}\n+\tif password != \"\" {\n+\t\topts = append(opts, rardecode.Password(password))\n+\t}\n+\tfiles, err := rardecode.List(fileName, opts...)\n+\t// rardecode\u8f93\u51fa\u6587\u4ef6\u5217\u8868\u7684\u987a\u5e8f\u4e0d\u4e00\u5b9a\u662f\u7236\u76ee\u5f55\u5728\u524d\uff0c\u5b50\u76ee\u5f55\u5728\u540e\n+\t// \u7236\u8def\u5f84\u7684\u957f\u5ea6\u4e00\u5b9a\u6bd4\u5b50\u8def\u5f84\u77ed\uff0c\u6392\u5e8f\u540e\u7684files\u53ef\u4fdd\u8bc1\u7236\u8def\u5f84\u5728\u524d\n+\tsort.Slice(files, func(i, j int) bool {\n+\t\treturn len(files[i].Name) < len(files[j].Name)\n+\t})\n+\tif err != nil {\n+\t\treturn nil, filterPassword(err)\n+\t}\n+\treturn &WrapReader{files: files}, nil\n+}\n+\n+func getReader(ss []*stream.SeekableStream, password string) (*rardecode.Reader, error) {\n+\tfileName, fsOpt, err := makeOpts(ss)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\topts := []rardecode.Option{fsOpt}\n+\tif password != \"\" {\n+\t\topts = append(opts, rardecode.Password(password))\n+\t}\n+\trc, err := rardecode.OpenReader(fileName, opts...)\n+\tif err != nil {\n+\t\treturn nil, filterPassword(err)\n+\t}\n+\tss[0].Closers.Add(rc)\n+\treturn &rc.Reader, nil\n+}\n+\n+func decompress(reader *rardecode.Reader, header *rardecode.FileHeader, filePath, outputPath string) error {\n+\ttargetPath := outputPath\n+\tdir, base := stdpath.Split(filePath)\n+\tif dir != \"\" {\n+\t\ttargetPath = stdpath.Join(targetPath, dir)\n+\t\terr := os.MkdirAll(targetPath, 0700)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\tif base != \"\" {\n+\t\terr := _decompress(reader, header, targetPath, func(_ float64) {})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+func _decompress(reader *rardecode.Reader, header *rardecode.FileHeader, targetPath string, up model.UpdateProgress) error {\n+\tf, err := os.OpenFile(stdpath.Join(targetPath, stdpath.Base(header.Name)), os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tdefer func() { _ = f.Close() }()\n+\t_, err = io.Copy(f, &stream.ReaderUpdatingProgress{\n+\t\tReader: &stream.SimpleReaderWithSize{\n+\t\t\tReader: reader,\n+\t\t\tSize:   header.UnPackedSize,\n+\t\t},\n+\t\tUpdateProgress: up,\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\n+\n+func filterPassword(err error) error {\n+\tif err != nil && strings.Contains(err.Error(), \"password\") {\n+\t\treturn errs.WrongArchivePassword\n+\t}\n+\treturn err\n+}\ndiff --git a/internal/archive/sevenzip/sevenzip.go b/internal/archive/sevenzip/sevenzip.go\nnew file mode 100644\nindex 00000000000..281699664f8\n--- /dev/null\n+++ b/internal/archive/sevenzip/sevenzip.go\n@@ -0,0 +1,72 @@\n+package sevenzip\n+\n+import (\n+\t\"io\"\n+\t\"strings\"\n+\n+\t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n+\t\"github.com/alist-org/alist/v3/internal/errs\"\n+\t\"github.com/alist-org/alist/v3/internal/model\"\n+\t\"github.com/alist-org/alist/v3/internal/stream\"\n+)\n+\n+type SevenZip struct{}\n+\n+func (SevenZip) AcceptedExtensions() []string {\n+\treturn []string{\".7z\"}\n+}\n+\n+func (SevenZip) AcceptedMultipartExtensions() map[string]tool.MultipartExtension {\n+\treturn map[string]tool.MultipartExtension{\n+\t\t\".7z.001\": {\".7z.%.3d\", 2},\n+\t}\n+}\n+\n+func (SevenZip) GetMeta(ss []*stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n+\treader, err := getReader(ss, args.Password)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\t_, tree := tool.GenerateMetaTreeFromFolderTraversal(&WrapReader{Reader: reader})\n+\treturn &model.ArchiveMetaInfo{\n+\t\tComment:   \"\",\n+\t\tEncrypted: args.Password != \"\",\n+\t\tTree:      tree,\n+\t}, nil\n+}\n+\n+func (SevenZip) List(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n+\treturn nil, errs.NotSupport\n+}\n+\n+func (SevenZip) Extract(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n+\treader, err := getReader(ss, args.Password)\n+\tif err != nil {\n+\t\treturn nil, 0, err\n+\t}\n+\tinnerPath := strings.TrimPrefix(args.InnerPath, \"/\")\n+\tfor _, file := range reader.File {\n+\t\tif file.Name == innerPath {\n+\t\t\tr, e := file.Open()\n+\t\t\tif e != nil {\n+\t\t\t\treturn nil, 0, e\n+\t\t\t}\n+\t\t\treturn r, file.FileInfo().Size(), nil\n+\t\t}\n+\t}\n+\treturn nil, 0, errs.ObjectNotFound\n+}\n+\n+func (SevenZip) Decompress(ss []*stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n+\treader, err := getReader(ss, args.Password)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn tool.DecompressFromFolderTraversal(&WrapReader{Reader: reader}, outputPath, args, up)\n+}\n+\n+var _ tool.Tool = (*SevenZip)(nil)\n+\n+func init() {\n+\ttool.RegisterTool(SevenZip{})\n+}\ndiff --git a/internal/archive/sevenzip/utils.go b/internal/archive/sevenzip/utils.go\nnew file mode 100644\nindex 00000000000..624ba1879c8\n--- /dev/null\n+++ b/internal/archive/sevenzip/utils.go\n@@ -0,0 +1,61 @@\n+package sevenzip\n+\n+import (\n+\t\"errors\"\n+\t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n+\t\"github.com/alist-org/alist/v3/internal/errs\"\n+\t\"github.com/alist-org/alist/v3/internal/stream\"\n+\t\"github.com/bodgit/sevenzip\"\n+\t\"io\"\n+\t\"io/fs\"\n+)\n+\n+type WrapReader struct {\n+\tReader *sevenzip.Reader\n+}\n+\n+func (r *WrapReader) Files() []tool.SubFile {\n+\tret := make([]tool.SubFile, 0, len(r.Reader.File))\n+\tfor _, f := range r.Reader.File {\n+\t\tret = append(ret, &WrapFile{f: f})\n+\t}\n+\treturn ret\n+}\n+\n+type WrapFile struct {\n+\tf *sevenzip.File\n+}\n+\n+func (f *WrapFile) Name() string {\n+\treturn f.f.Name\n+}\n+\n+func (f *WrapFile) FileInfo() fs.FileInfo {\n+\treturn f.f.FileInfo()\n+}\n+\n+func (f *WrapFile) Open() (io.ReadCloser, error) {\n+\treturn f.f.Open()\n+}\n+\n+func getReader(ss []*stream.SeekableStream, password string) (*sevenzip.Reader, error) {\n+\treaderAt, err := stream.NewMultiReaderAt(ss)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tsr, err := sevenzip.NewReaderWithPassword(readerAt, readerAt.Size(), password)\n+\tif err != nil {\n+\t\treturn nil, filterPassword(err)\n+\t}\n+\treturn sr, nil\n+}\n+\n+func filterPassword(err error) error {\n+\tif err != nil {\n+\t\tvar e *sevenzip.ReadError\n+\t\tif errors.As(err, &e) && e.Encrypted {\n+\t\t\treturn errs.WrongArchivePassword\n+\t\t}\n+\t}\n+\treturn err\n+}\ndiff --git a/internal/archive/tool/base.go b/internal/archive/tool/base.go\nindex 08e96614f51..8f5b10d96b7 100644\n--- a/internal/archive/tool/base.go\n+++ b/internal/archive/tool/base.go\n@@ -6,10 +6,16 @@ import (\n \t\"io\"\n )\n \n+type MultipartExtension struct {\n+\tPartFileFormat  string\n+\tSecondPartIndex int\n+}\n+\n type Tool interface {\n \tAcceptedExtensions() []string\n-\tGetMeta(ss *stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error)\n-\tList(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error)\n-\tExtract(ss *stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error)\n-\tDecompress(ss *stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error\n+\tAcceptedMultipartExtensions() map[string]MultipartExtension\n+\tGetMeta(ss []*stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error)\n+\tList(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error)\n+\tExtract(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error)\n+\tDecompress(ss []*stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error\n }\ndiff --git a/internal/archive/tool/helper.go b/internal/archive/tool/helper.go\nnew file mode 100644\nindex 00000000000..8f71900ac56\n--- /dev/null\n+++ b/internal/archive/tool/helper.go\n@@ -0,0 +1,201 @@\n+package tool\n+\n+import (\n+\t\"io\"\n+\t\"io/fs\"\n+\t\"os\"\n+\tstdpath \"path\"\n+\t\"strings\"\n+\n+\t\"github.com/alist-org/alist/v3/internal/model\"\n+\t\"github.com/alist-org/alist/v3/internal/stream\"\n+)\n+\n+type SubFile interface {\n+\tName() string\n+\tFileInfo() fs.FileInfo\n+\tOpen() (io.ReadCloser, error)\n+}\n+\n+type CanEncryptSubFile interface {\n+\tIsEncrypted() bool\n+\tSetPassword(password string)\n+}\n+\n+type ArchiveReader interface {\n+\tFiles() []SubFile\n+}\n+\n+func GenerateMetaTreeFromFolderTraversal(r ArchiveReader) (bool, []model.ObjTree) {\n+\tencrypted := false\n+\tdirMap := make(map[string]*model.ObjectTree)\n+\tdirMap[\".\"] = &model.ObjectTree{}\n+\tfor _, file := range r.Files() {\n+\t\tif encrypt, ok := file.(CanEncryptSubFile); ok && encrypt.IsEncrypted() {\n+\t\t\tencrypted = true\n+\t\t}\n+\n+\t\tname := strings.TrimPrefix(file.Name(), \"/\")\n+\t\tvar dir string\n+\t\tvar dirObj *model.ObjectTree\n+\t\tisNewFolder := false\n+\t\tif !file.FileInfo().IsDir() {\n+\t\t\t// \u5148\u5c06 \u6587\u4ef6 \u6dfb\u52a0\u5230 \u6240\u5728\u7684\u6587\u4ef6\u5939\n+\t\t\tdir = stdpath.Dir(name)\n+\t\t\tdirObj = dirMap[dir]\n+\t\t\tif dirObj == nil {\n+\t\t\t\tisNewFolder = true\n+\t\t\t\tdirObj = &model.ObjectTree{}\n+\t\t\t\tdirObj.IsFolder = true\n+\t\t\t\tdirObj.Name = stdpath.Base(dir)\n+\t\t\t\tdirObj.Modified = file.FileInfo().ModTime()\n+\t\t\t\tdirMap[dir] = dirObj\n+\t\t\t}\n+\t\t\tdirObj.Children = append(\n+\t\t\t\tdirObj.Children, &model.ObjectTree{\n+\t\t\t\t\tObject: *MakeModelObj(file.FileInfo()),\n+\t\t\t\t},\n+\t\t\t)\n+\t\t} else {\n+\t\t\tdir = strings.TrimSuffix(name, \"/\")\n+\t\t\tdirObj = dirMap[dir]\n+\t\t\tif dirObj == nil {\n+\t\t\t\tisNewFolder = true\n+\t\t\t\tdirObj = &model.ObjectTree{}\n+\t\t\t\tdirMap[dir] = dirObj\n+\t\t\t}\n+\t\t\tdirObj.IsFolder = true\n+\t\t\tdirObj.Name = stdpath.Base(dir)\n+\t\t\tdirObj.Modified = file.FileInfo().ModTime()\n+\t\t\tdirObj.Children = make([]model.ObjTree, 0)\n+\t\t}\n+\t\tif isNewFolder {\n+\t\t\t// \u5c06 \u6587\u4ef6\u5939 \u6dfb\u52a0\u5230 \u7236\u6587\u4ef6\u5939\n+\t\t\tdir = stdpath.Dir(dir)\n+\t\t\tpDirObj := dirMap[dir]\n+\t\t\tif pDirObj != nil {\n+\t\t\t\tpDirObj.Children = append(pDirObj.Children, dirObj)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\tfor {\n+\t\t\t\t//\t\u8003\u8651\u538b\u7f29\u5305\u4ec5\u8bb0\u5f55\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u4e0d\u8bb0\u5f55\u6587\u4ef6\u5939\n+\t\t\t\tpDirObj = &model.ObjectTree{}\n+\t\t\t\tpDirObj.IsFolder = true\n+\t\t\t\tpDirObj.Name = stdpath.Base(dir)\n+\t\t\t\tpDirObj.Modified = file.FileInfo().ModTime()\n+\t\t\t\tdirMap[dir] = pDirObj\n+\t\t\t\tpDirObj.Children = append(pDirObj.Children, dirObj)\n+\t\t\t\tdir = stdpath.Dir(dir)\n+\t\t\t\tif dirMap[dir] != nil {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\tdirObj = pDirObj\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn encrypted, dirMap[\".\"].GetChildren()\n+}\n+\n+func MakeModelObj(file os.FileInfo) *model.Object {\n+\treturn &model.Object{\n+\t\tName:     file.Name(),\n+\t\tSize:     file.Size(),\n+\t\tModified: file.ModTime(),\n+\t\tIsFolder: file.IsDir(),\n+\t}\n+}\n+\n+type WrapFileInfo struct {\n+\tmodel.Obj\n+}\n+\n+func DecompressFromFolderTraversal(r ArchiveReader, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n+\tvar err error\n+\tfiles := r.Files()\n+\tif args.InnerPath == \"/\" {\n+\t\tfor i, file := range files {\n+\t\t\tname := file.Name()\n+\t\t\terr = decompress(file, name, outputPath, args.Password)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tup(float64(i+1) * 100.0 / float64(len(files)))\n+\t\t}\n+\t} else {\n+\t\tinnerPath := strings.TrimPrefix(args.InnerPath, \"/\")\n+\t\tinnerBase := stdpath.Base(innerPath)\n+\t\tcreatedBaseDir := false\n+\t\tfor _, file := range files {\n+\t\t\tname := file.Name()\n+\t\t\tif name == innerPath {\n+\t\t\t\terr = _decompress(file, outputPath, args.Password, up)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t} else if strings.HasPrefix(name, innerPath+\"/\") {\n+\t\t\t\ttargetPath := stdpath.Join(outputPath, innerBase)\n+\t\t\t\tif !createdBaseDir {\n+\t\t\t\t\terr = os.Mkdir(targetPath, 0700)\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\treturn err\n+\t\t\t\t\t}\n+\t\t\t\t\tcreatedBaseDir = true\n+\t\t\t\t}\n+\t\t\t\trestPath := strings.TrimPrefix(name, innerPath+\"/\")\n+\t\t\t\terr = decompress(file, restPath, targetPath, args.Password)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+func decompress(file SubFile, filePath, outputPath, password string) error {\n+\ttargetPath := outputPath\n+\tdir, base := stdpath.Split(filePath)\n+\tif dir != \"\" {\n+\t\ttargetPath = stdpath.Join(targetPath, dir)\n+\t\terr := os.MkdirAll(targetPath, 0700)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\tif base != \"\" {\n+\t\terr := _decompress(file, targetPath, password, func(_ float64) {})\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+func _decompress(file SubFile, targetPath, password string, up model.UpdateProgress) error {\n+\tif encrypt, ok := file.(CanEncryptSubFile); ok && encrypt.IsEncrypted() {\n+\t\tencrypt.SetPassword(password)\n+\t}\n+\trc, err := file.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tdefer func() { _ = rc.Close() }()\n+\tf, err := os.OpenFile(stdpath.Join(targetPath, file.FileInfo().Name()), os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tdefer func() { _ = f.Close() }()\n+\t_, err = io.Copy(f, &stream.ReaderUpdatingProgress{\n+\t\tReader: &stream.SimpleReaderWithSize{\n+\t\t\tReader: rc,\n+\t\t\tSize:   file.FileInfo().Size(),\n+\t\t},\n+\t\tUpdateProgress: up,\n+\t})\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\ndiff --git a/internal/archive/tool/utils.go b/internal/archive/tool/utils.go\nindex 822ee894fd1..aa92cb1d792 100644\n--- a/internal/archive/tool/utils.go\n+++ b/internal/archive/tool/utils.go\n@@ -5,19 +5,28 @@ import (\n )\n \n var (\n-\tTools = make(map[string]Tool)\n+\tTools               = make(map[string]Tool)\n+\tMultipartExtensions = make(map[string]MultipartExtension)\n )\n \n func RegisterTool(tool Tool) {\n \tfor _, ext := range tool.AcceptedExtensions() {\n \t\tTools[ext] = tool\n \t}\n+\tfor mainFile, ext := range tool.AcceptedMultipartExtensions() {\n+\t\tMultipartExtensions[mainFile] = ext\n+\t\tTools[mainFile] = tool\n+\t}\n }\n \n-func GetArchiveTool(ext string) (Tool, error) {\n+func GetArchiveTool(ext string) (*MultipartExtension, Tool, error) {\n \tt, ok := Tools[ext]\n \tif !ok {\n-\t\treturn nil, errs.UnknownArchiveFormat\n+\t\treturn nil, nil, errs.UnknownArchiveFormat\n+\t}\n+\tpartExt, ok := MultipartExtensions[ext]\n+\tif !ok {\n+\t\treturn nil, t, nil\n \t}\n-\treturn t, nil\n+\treturn &partExt, t, nil\n }\ndiff --git a/internal/archive/zip/utils.go b/internal/archive/zip/utils.go\nindex aa51b88eb93..59f4ed51378 100644\n--- a/internal/archive/zip/utils.go\n+++ b/internal/archive/zip/utils.go\n@@ -2,8 +2,13 @@ package zip\n \n import (\n \t\"bytes\"\n+\t\"io\"\n+\t\"io/fs\"\n+\tstdpath \"path\"\n+\t\"strings\"\n+\n+\t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n \t\"github.com/alist-org/alist/v3/internal/errs\"\n-\t\"github.com/alist-org/alist/v3/internal/model\"\n \t\"github.com/alist-org/alist/v3/internal/stream\"\n \t\"github.com/saintfish/chardet\"\n \t\"github.com/yeka/zip\"\n@@ -16,65 +21,62 @@ import (\n \t\"golang.org/x/text/encoding/unicode\"\n \t\"golang.org/x/text/encoding/unicode/utf32\"\n \t\"golang.org/x/text/transform\"\n-\t\"io\"\n-\t\"os\"\n-\tstdpath \"path\"\n-\t\"strings\"\n )\n \n-func toModelObj(file os.FileInfo) *model.Object {\n-\treturn &model.Object{\n-\t\tName:     decodeName(file.Name()),\n-\t\tSize:     file.Size(),\n-\t\tModified: file.ModTime(),\n-\t\tIsFolder: file.IsDir(),\n-\t}\n+type WrapReader struct {\n+\tReader *zip.Reader\n }\n \n-func decompress(file *zip.File, filePath, outputPath, password string) error {\n-\ttargetPath := outputPath\n-\tdir, base := stdpath.Split(filePath)\n-\tif dir != \"\" {\n-\t\ttargetPath = stdpath.Join(targetPath, dir)\n-\t\terr := os.MkdirAll(targetPath, 0700)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tif base != \"\" {\n-\t\terr := _decompress(file, targetPath, password, func(_ float64) {})\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+func (r *WrapReader) Files() []tool.SubFile {\n+\tret := make([]tool.SubFile, 0, len(r.Reader.File))\n+\tfor _, f := range r.Reader.File {\n+\t\tret = append(ret, &WrapFile{f: f})\n \t}\n-\treturn nil\n+\treturn ret\n }\n \n-func _decompress(file *zip.File, targetPath, password string, up model.UpdateProgress) error {\n-\tif file.IsEncrypted() {\n-\t\tfile.SetPassword(password)\n-\t}\n-\trc, err := file.Open()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer rc.Close()\n-\tf, err := os.OpenFile(stdpath.Join(targetPath, decodeName(file.FileInfo().Name())), os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n-\tif err != nil {\n-\t\treturn err\n+type WrapFileInfo struct {\n+\tfs.FileInfo\n+}\n+\n+func (f *WrapFileInfo) Name() string {\n+\treturn decodeName(f.FileInfo.Name())\n+}\n+\n+type WrapFile struct {\n+\tf *zip.File\n+}\n+\n+func (f *WrapFile) Name() string {\n+\treturn decodeName(f.f.Name)\n+}\n+\n+func (f *WrapFile) FileInfo() fs.FileInfo {\n+\treturn &WrapFileInfo{FileInfo: f.f.FileInfo()}\n+}\n+\n+func (f *WrapFile) Open() (io.ReadCloser, error) {\n+\treturn f.f.Open()\n+}\n+\n+func (f *WrapFile) IsEncrypted() bool {\n+\treturn f.f.IsEncrypted()\n+}\n+\n+func (f *WrapFile) SetPassword(password string) {\n+\tf.f.SetPassword(password)\n+}\n+\n+func getReader(ss []*stream.SeekableStream) (*zip.Reader, error) {\n+\tif len(ss) > 1 && stdpath.Ext(ss[1].GetName()) == \".z01\" {\n+\t\t// FIXME: Incorrect parsing method for standard multipart zip format\n+\t\tss = append(ss[1:], ss[0])\n \t}\n-\tdefer f.Close()\n-\t_, err = io.Copy(f, &stream.ReaderUpdatingProgress{\n-\t\tReader: &stream.SimpleReaderWithSize{\n-\t\t\tReader: rc,\n-\t\t\tSize:   file.FileInfo().Size(),\n-\t\t},\n-\t\tUpdateProgress: up,\n-\t})\n+\treader, err := stream.NewMultiReaderAt(ss)\n \tif err != nil {\n-\t\treturn err\n+\t\treturn nil, err\n \t}\n-\treturn nil\n+\treturn zip.NewReader(reader, reader.Size())\n }\n \n func filterPassword(err error) error {\ndiff --git a/internal/archive/zip/zip.go b/internal/archive/zip/zip.go\nindex 9dc8cc7638f..6e23570c73c 100644\n--- a/internal/archive/zip/zip.go\n+++ b/internal/archive/zip/zip.go\n@@ -2,7 +2,6 @@ package zip\n \n import (\n \t\"io\"\n-\t\"os\"\n \tstdpath \"path\"\n \t\"strings\"\n \n@@ -10,106 +9,37 @@ import (\n \t\"github.com/alist-org/alist/v3/internal/errs\"\n \t\"github.com/alist-org/alist/v3/internal/model\"\n \t\"github.com/alist-org/alist/v3/internal/stream\"\n-\t\"github.com/yeka/zip\"\n )\n \n type Zip struct {\n }\n \n-func (*Zip) AcceptedExtensions() []string {\n-\treturn []string{\".zip\"}\n+func (Zip) AcceptedExtensions() []string {\n+\treturn []string{}\n }\n \n-func (*Zip) GetMeta(ss *stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n-\treader, err := stream.NewReadAtSeeker(ss, 0)\n-\tif err != nil {\n-\t\treturn nil, err\n+func (Zip) AcceptedMultipartExtensions() map[string]tool.MultipartExtension {\n+\treturn map[string]tool.MultipartExtension{\n+\t\t\".zip\":     {\".z%.2d\", 1},\n+\t\t\".zip.001\": {\".zip.%.3d\", 2},\n \t}\n-\tzipReader, err := zip.NewReader(reader, ss.GetSize())\n+}\n+\n+func (Zip) GetMeta(ss []*stream.SeekableStream, args model.ArchiveArgs) (model.ArchiveMeta, error) {\n+\tzipReader, err := getReader(ss)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\tencrypted := false\n-\tdirMap := make(map[string]*model.ObjectTree)\n-\tdirMap[\".\"] = &model.ObjectTree{}\n-\tfor _, file := range zipReader.File {\n-\t\tif file.IsEncrypted() {\n-\t\t\tencrypted = true\n-\t\t}\n-\n-\t\tname := strings.TrimPrefix(decodeName(file.Name), \"/\")\n-\t\tvar dir string\n-\t\tvar dirObj *model.ObjectTree\n-\t\tisNewFolder := false\n-\t\tif !file.FileInfo().IsDir() {\n-\t\t\t// \u5148\u5c06 \u6587\u4ef6 \u6dfb\u52a0\u5230 \u6240\u5728\u7684\u6587\u4ef6\u5939\n-\t\t\tdir = stdpath.Dir(name)\n-\t\t\tdirObj = dirMap[dir]\n-\t\t\tif dirObj == nil {\n-\t\t\t\tisNewFolder = true\n-\t\t\t\tdirObj = &model.ObjectTree{}\n-\t\t\t\tdirObj.IsFolder = true\n-\t\t\t\tdirObj.Name = stdpath.Base(dir)\n-\t\t\t\tdirObj.Modified = file.ModTime()\n-\t\t\t\tdirMap[dir] = dirObj\n-\t\t\t}\n-\t\t\tdirObj.Children = append(\n-\t\t\t\tdirObj.Children, &model.ObjectTree{\n-\t\t\t\t\tObject: *toModelObj(file.FileInfo()),\n-\t\t\t\t},\n-\t\t\t)\n-\t\t} else {\n-\t\t\tdir = strings.TrimSuffix(name, \"/\")\n-\t\t\tdirObj = dirMap[dir]\n-\t\t\tif dirObj == nil {\n-\t\t\t\tisNewFolder = true\n-\t\t\t\tdirObj = &model.ObjectTree{}\n-\t\t\t\tdirMap[dir] = dirObj\n-\t\t\t}\n-\t\t\tdirObj.IsFolder = true\n-\t\t\tdirObj.Name = stdpath.Base(dir)\n-\t\t\tdirObj.Modified = file.ModTime()\n-\t\t\tdirObj.Children = make([]model.ObjTree, 0)\n-\t\t}\n-\t\tif isNewFolder {\n-\t\t\t// \u5c06 \u6587\u4ef6\u5939 \u6dfb\u52a0\u5230 \u7236\u6587\u4ef6\u5939\n-\t\t\tdir = stdpath.Dir(dir)\n-\t\t\tpDirObj := dirMap[dir]\n-\t\t\tif pDirObj != nil {\n-\t\t\t\tpDirObj.Children = append(pDirObj.Children, dirObj)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tfor {\n-\t\t\t\t//\t\u8003\u8651\u538b\u7f29\u5305\u4ec5\u8bb0\u5f55\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u4e0d\u8bb0\u5f55\u6587\u4ef6\u5939\n-\t\t\t\tpDirObj = &model.ObjectTree{}\n-\t\t\t\tpDirObj.IsFolder = true\n-\t\t\t\tpDirObj.Name = stdpath.Base(dir)\n-\t\t\t\tpDirObj.Modified = file.ModTime()\n-\t\t\t\tdirMap[dir] = pDirObj\n-\t\t\t\tpDirObj.Children = append(pDirObj.Children, dirObj)\n-\t\t\t\tdir = stdpath.Dir(dir)\n-\t\t\t\tif dirMap[dir] != nil {\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t\tdirObj = pDirObj\n-\t\t\t}\n-\t\t}\n-\t}\n-\n+\tencrypted, tree := tool.GenerateMetaTreeFromFolderTraversal(&WrapReader{Reader: zipReader})\n \treturn &model.ArchiveMetaInfo{\n \t\tComment:   zipReader.Comment,\n \t\tEncrypted: encrypted,\n-\t\tTree:      dirMap[\".\"].GetChildren(),\n+\t\tTree:      tree,\n \t}, nil\n }\n \n-func (*Zip) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n-\treader, err := stream.NewReadAtSeeker(ss, 0)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tzipReader, err := zip.NewReader(reader, ss.GetSize())\n+func (Zip) List(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) ([]model.Obj, error) {\n+\tzipReader, err := getReader(ss)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -134,13 +64,13 @@ func (*Zip) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]mode\n \t\t\t\tif dir == nil && len(strs) == 2 {\n \t\t\t\t\tdir = &model.Object{\n \t\t\t\t\t\tName:     strs[0],\n-\t\t\t\t\t\tModified: ss.ModTime(),\n+\t\t\t\t\t\tModified: ss[0].ModTime(),\n \t\t\t\t\t\tIsFolder: true,\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\tcontinue\n \t\t\t}\n-\t\t\tret = append(ret, toModelObj(file.FileInfo()))\n+\t\t\tret = append(ret, tool.MakeModelObj(&WrapFileInfo{FileInfo: file.FileInfo()}))\n \t\t}\n \t\tif len(ret) == 0 && dir != nil {\n \t\t\tret = append(ret, dir)\n@@ -157,7 +87,7 @@ func (*Zip) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]mode\n \t\t\t\tcontinue\n \t\t\t}\n \t\t\texist = true\n-\t\t\tret = append(ret, toModelObj(file.FileInfo()))\n+\t\t\tret = append(ret, tool.MakeModelObj(&WrapFileInfo{file.FileInfo()}))\n \t\t}\n \t\tif !exist {\n \t\t\treturn nil, errs.ObjectNotFound\n@@ -166,12 +96,8 @@ func (*Zip) List(ss *stream.SeekableStream, args model.ArchiveInnerArgs) ([]mode\n \t}\n }\n \n-func (*Zip) Extract(ss *stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n-\treader, err := stream.NewReadAtSeeker(ss, 0)\n-\tif err != nil {\n-\t\treturn nil, 0, err\n-\t}\n-\tzipReader, err := zip.NewReader(reader, ss.GetSize())\n+func (Zip) Extract(ss []*stream.SeekableStream, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n+\tzipReader, err := getReader(ss)\n \tif err != nil {\n \t\treturn nil, 0, err\n \t}\n@@ -191,58 +117,16 @@ func (*Zip) Extract(ss *stream.SeekableStream, args model.ArchiveInnerArgs) (io.\n \treturn nil, 0, errs.ObjectNotFound\n }\n \n-func (*Zip) Decompress(ss *stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n-\treader, err := stream.NewReadAtSeeker(ss, 0)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tzipReader, err := zip.NewReader(reader, ss.GetSize())\n+func (Zip) Decompress(ss []*stream.SeekableStream, outputPath string, args model.ArchiveInnerArgs, up model.UpdateProgress) error {\n+\tzipReader, err := getReader(ss)\n \tif err != nil {\n \t\treturn err\n \t}\n-\tif args.InnerPath == \"/\" {\n-\t\tfor i, file := range zipReader.File {\n-\t\t\tname := decodeName(file.Name)\n-\t\t\terr = decompress(file, name, outputPath, args.Password)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tup(float64(i+1) * 100.0 / float64(len(zipReader.File)))\n-\t\t}\n-\t} else {\n-\t\tinnerPath := strings.TrimPrefix(args.InnerPath, \"/\")\n-\t\tinnerBase := stdpath.Base(innerPath)\n-\t\tcreatedBaseDir := false\n-\t\tfor _, file := range zipReader.File {\n-\t\t\tname := decodeName(file.Name)\n-\t\t\tif name == innerPath {\n-\t\t\t\terr = _decompress(file, outputPath, args.Password, up)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\tbreak\n-\t\t\t} else if strings.HasPrefix(name, innerPath+\"/\") {\n-\t\t\t\ttargetPath := stdpath.Join(outputPath, innerBase)\n-\t\t\t\tif !createdBaseDir {\n-\t\t\t\t\terr = os.Mkdir(targetPath, 0700)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n-\t\t\t\t\tcreatedBaseDir = true\n-\t\t\t\t}\n-\t\t\t\trestPath := strings.TrimPrefix(name, innerPath+\"/\")\n-\t\t\t\terr = decompress(file, restPath, targetPath, args.Password)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+\treturn tool.DecompressFromFolderTraversal(&WrapReader{Reader: zipReader}, outputPath, args, up)\n }\n \n var _ tool.Tool = (*Zip)(nil)\n \n func init() {\n-\ttool.RegisterTool(&Zip{})\n+\ttool.RegisterTool(Zip{})\n }\ndiff --git a/internal/driver/driver.go b/internal/driver/driver.go\nindex 05f0fe24576..9e9440b6700 100644\n--- a/internal/driver/driver.go\n+++ b/internal/driver/driver.go\n@@ -79,13 +79,13 @@ type Remove interface {\n type Put interface {\n \t// Put a file (provided as a FileStreamer) into the driver\n \t// Besides the most basic upload functionality, the following features also need to be implemented:\n-\t// 1. Canceling (when `<-ctx.Done()` returns), by the following methods:\n+\t// 1. Canceling (when `<-ctx.Done()` returns), which can be supported by the following methods:\n \t//   (1) Use request methods that carry context, such as the following:\n \t//      a. http.NewRequestWithContext\n \t//      b. resty.Request.SetContext\n \t//      c. s3manager.Uploader.UploadWithContext\n \t//      d. utils.CopyWithCtx\n-\t//   (2) Use a `driver.ReaderWithCtx` or a `driver.NewLimitedUploadStream`\n+\t//   (2) Use a `driver.ReaderWithCtx` or `driver.NewLimitedUploadStream`\n \t//   (3) Use `utils.IsCanceled` to check if the upload has been canceled during the upload process,\n \t//       this is typically applicable to chunked uploads.\n \t// 2. Submit upload progress (via `up`) in real-time. There are three recommended ways as follows:\ndiff --git a/internal/fs/archive.go b/internal/fs/archive.go\nindex 3913182702c..b056decf9a2 100644\n--- a/internal/fs/archive.go\n+++ b/internal/fs/archive.go\n@@ -4,17 +4,6 @@ import (\n \t\"context\"\n \tstderrors \"errors\"\n \t\"fmt\"\n-\t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n-\t\"github.com/alist-org/alist/v3/internal/conf\"\n-\t\"github.com/alist-org/alist/v3/internal/driver\"\n-\t\"github.com/alist-org/alist/v3/internal/errs\"\n-\t\"github.com/alist-org/alist/v3/internal/model\"\n-\t\"github.com/alist-org/alist/v3/internal/op\"\n-\t\"github.com/alist-org/alist/v3/internal/stream\"\n-\t\"github.com/alist-org/alist/v3/internal/task\"\n-\t\"github.com/pkg/errors\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"github.com/xhofe/tache\"\n \t\"io\"\n \t\"math/rand\"\n \t\"mime\"\n@@ -25,6 +14,17 @@ import (\n \t\"strconv\"\n \t\"strings\"\n \t\"time\"\n+\n+\t\"github.com/alist-org/alist/v3/internal/conf\"\n+\t\"github.com/alist-org/alist/v3/internal/driver\"\n+\t\"github.com/alist-org/alist/v3/internal/errs\"\n+\t\"github.com/alist-org/alist/v3/internal/model\"\n+\t\"github.com/alist-org/alist/v3/internal/op\"\n+\t\"github.com/alist-org/alist/v3/internal/stream\"\n+\t\"github.com/alist-org/alist/v3/internal/task\"\n+\t\"github.com/pkg/errors\"\n+\tlog \"github.com/sirupsen/logrus\"\n+\t\"github.com/xhofe/tache\"\n )\n \n type ArchiveDownloadTask struct {\n@@ -37,7 +37,6 @@ type ArchiveDownloadTask struct {\n \tdstStorage   driver.Driver\n \tSrcStorageMp string\n \tDstStorageMp string\n-\tTool         tool.Tool\n }\n \n func (t *ArchiveDownloadTask) GetName() string {\n@@ -67,33 +66,39 @@ func (t *ArchiveDownloadTask) RunWithoutPushUploadTask() (*ArchiveContentUploadT\n \tif t.srcStorage == nil {\n \t\tt.srcStorage, err = op.GetStorageByMountPath(t.SrcStorageMp)\n \t}\n-\tl, srcObj, err := op.Link(t.Ctx(), t.srcStorage, t.SrcObjPath, model.LinkArgs{\n+\tsrcObj, tool, ss, err := op.GetArchiveToolAndStream(t.Ctx(), t.srcStorage, t.SrcObjPath, model.LinkArgs{\n \t\tHeader: http.Header{},\n \t})\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\tfs := stream.FileStream{\n-\t\tObj: srcObj,\n-\t\tCtx: t.Ctx(),\n-\t}\n-\tss, err := stream.NewSeekableStream(fs, l)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n \tdefer func() {\n-\t\tif err := ss.Close(); err != nil {\n-\t\t\tlog.Errorf(\"failed to close file streamer, %v\", err)\n+\t\tvar e error\n+\t\tfor _, s := range ss {\n+\t\t\te = stderrors.Join(e, s.Close())\n+\t\t}\n+\t\tif e != nil {\n+\t\t\tlog.Errorf(\"failed to close file streamer, %v\", e)\n \t\t}\n \t}()\n \tvar decompressUp model.UpdateProgress\n \tif t.CacheFull {\n-\t\tt.SetTotalBytes(srcObj.GetSize())\n+\t\tvar total, cur int64 = 0, 0\n+\t\tfor _, s := range ss {\n+\t\t\ttotal += s.GetSize()\n+\t\t}\n+\t\tt.SetTotalBytes(total)\n \t\tt.status = \"getting src object\"\n-\t\t_, err = ss.CacheFullInTempFileAndUpdateProgress(t.SetProgress)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\tfor _, s := range ss {\n+\t\t\t_, err = s.CacheFullInTempFileAndUpdateProgress(func(p float64) {\n+\t\t\t\tt.SetProgress((float64(cur) + float64(s.GetSize())*p/100.0) / float64(total))\n+\t\t\t})\n+\t\t\tcur += s.GetSize()\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n \t\t}\n+\t\tt.SetProgress(100.0)\n \t\tdecompressUp = func(_ float64) {}\n \t} else {\n \t\tdecompressUp = t.SetProgress\n@@ -103,7 +108,7 @@ func (t *ArchiveDownloadTask) RunWithoutPushUploadTask() (*ArchiveContentUploadT\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\terr = t.Tool.Decompress(ss, dir, t.ArchiveInnerArgs, decompressUp)\n+\terr = tool.Decompress(ss, dir, t.ArchiveInnerArgs, decompressUp)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -344,11 +349,6 @@ func archiveDecompress(ctx context.Context, srcObjPath, dstDirPath string, args\n \t\t\treturn nil, err\n \t\t}\n \t}\n-\text := stdpath.Ext(srcObjActualPath)\n-\tt, err := tool.GetArchiveTool(ext)\n-\tif err != nil {\n-\t\treturn nil, errors.WithMessagef(err, \"failed get [%s] archive tool\", ext)\n-\t}\n \ttaskCreator, _ := ctx.Value(\"user\").(*model.User)\n \ttsk := &ArchiveDownloadTask{\n \t\tTaskExtension: task.TaskExtension{\n@@ -361,7 +361,6 @@ func archiveDecompress(ctx context.Context, srcObjPath, dstDirPath string, args\n \t\tDstDirPath:            dstDirActualPath,\n \t\tSrcStorageMp:          srcStorage.GetStorage().MountPath,\n \t\tDstStorageMp:          dstStorage.GetStorage().MountPath,\n-\t\tTool:                  t,\n \t}\n \tif ctx.Value(conf.NoTaskKey) != nil {\n \t\tuploadTask, err := tsk.RunWithoutPushUploadTask()\ndiff --git a/internal/op/archive.go b/internal/op/archive.go\nindex a241838c9c4..e9a738f98f4 100644\n--- a/internal/op/archive.go\n+++ b/internal/op/archive.go\n@@ -3,6 +3,7 @@ package op\n import (\n \t\"context\"\n \tstderrors \"errors\"\n+\t\"fmt\"\n \t\"io\"\n \tstdpath \"path\"\n \t\"strings\"\n@@ -54,21 +55,76 @@ func GetArchiveMeta(ctx context.Context, storage driver.Driver, path string, arg\n \treturn meta, err\n }\n \n-func getArchiveToolAndStream(ctx context.Context, storage driver.Driver, path string, args model.LinkArgs) (model.Obj, tool.Tool, *stream.SeekableStream, error) {\n+func GetArchiveToolAndStream(ctx context.Context, storage driver.Driver, path string, args model.LinkArgs) (model.Obj, tool.Tool, []*stream.SeekableStream, error) {\n \tl, obj, err := Link(ctx, storage, path, args)\n \tif err != nil {\n \t\treturn nil, nil, nil, errors.WithMessagef(err, \"failed get [%s] link\", path)\n \t}\n-\text := stdpath.Ext(obj.GetName())\n-\tt, err := tool.GetArchiveTool(ext)\n+\tbaseName, ext, found := strings.Cut(obj.GetName(), \".\")\n+\tif !found {\n+\t\tif l.MFile != nil {\n+\t\t\t_ = l.MFile.Close()\n+\t\t}\n+\t\tif l.RangeReadCloser != nil {\n+\t\t\t_ = l.RangeReadCloser.Close()\n+\t\t}\n+\t\treturn nil, nil, nil, errors.Errorf(\"failed get archive tool: the obj does not have an extension.\")\n+\t}\n+\tpartExt, t, err := tool.GetArchiveTool(\".\" + ext)\n \tif err != nil {\n-\t\treturn nil, nil, nil, errors.WithMessagef(err, \"failed get [%s] archive tool\", ext)\n+\t\tvar e error\n+\t\tpartExt, t, e = tool.GetArchiveTool(stdpath.Ext(obj.GetName()))\n+\t\tif e != nil {\n+\t\t\tif l.MFile != nil {\n+\t\t\t\t_ = l.MFile.Close()\n+\t\t\t}\n+\t\t\tif l.RangeReadCloser != nil {\n+\t\t\t\t_ = l.RangeReadCloser.Close()\n+\t\t\t}\n+\t\t\treturn nil, nil, nil, errors.WithMessagef(stderrors.Join(err, e), \"failed get archive tool: %s\", ext)\n+\t\t}\n \t}\n \tss, err := stream.NewSeekableStream(stream.FileStream{Ctx: ctx, Obj: obj}, l)\n \tif err != nil {\n+\t\tif l.MFile != nil {\n+\t\t\t_ = l.MFile.Close()\n+\t\t}\n+\t\tif l.RangeReadCloser != nil {\n+\t\t\t_ = l.RangeReadCloser.Close()\n+\t\t}\n \t\treturn nil, nil, nil, errors.WithMessagef(err, \"failed get [%s] stream\", path)\n \t}\n-\treturn obj, t, ss, nil\n+\tret := []*stream.SeekableStream{ss}\n+\tif partExt == nil {\n+\t\treturn obj, t, ret, nil\n+\t} else {\n+\t\tindex := partExt.SecondPartIndex\n+\t\tdir := stdpath.Dir(path)\n+\t\tfor {\n+\t\t\tp := stdpath.Join(dir, baseName+fmt.Sprintf(partExt.PartFileFormat, index))\n+\t\t\tvar o model.Obj\n+\t\t\tl, o, err = Link(ctx, storage, p, args)\n+\t\t\tif err != nil {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tss, err = stream.NewSeekableStream(stream.FileStream{Ctx: ctx, Obj: o}, l)\n+\t\t\tif err != nil {\n+\t\t\t\tif l.MFile != nil {\n+\t\t\t\t\t_ = l.MFile.Close()\n+\t\t\t\t}\n+\t\t\t\tif l.RangeReadCloser != nil {\n+\t\t\t\t\t_ = l.RangeReadCloser.Close()\n+\t\t\t\t}\n+\t\t\t\tfor _, s := range ret {\n+\t\t\t\t\t_ = s.Close()\n+\t\t\t\t}\n+\t\t\t\treturn nil, nil, nil, errors.WithMessagef(err, \"failed get [%s] stream\", path)\n+\t\t\t}\n+\t\t\tret = append(ret, ss)\n+\t\t\tindex++\n+\t\t}\n+\t\treturn obj, t, ret, nil\n+\t}\n }\n \n func getArchiveMeta(ctx context.Context, storage driver.Driver, path string, args model.ArchiveMetaArgs) (model.Obj, *model.ArchiveMetaProvider, error) {\n@@ -94,13 +150,17 @@ func getArchiveMeta(ctx context.Context, storage driver.Driver, path string, arg\n \t\t\treturn obj, archiveMetaProvider, err\n \t\t}\n \t}\n-\tobj, t, ss, err := getArchiveToolAndStream(ctx, storage, path, args.LinkArgs)\n+\tobj, t, ss, err := GetArchiveToolAndStream(ctx, storage, path, args.LinkArgs)\n \tif err != nil {\n \t\treturn nil, nil, err\n \t}\n \tdefer func() {\n-\t\tif err := ss.Close(); err != nil {\n-\t\t\tlog.Errorf(\"failed to close file streamer, %v\", err)\n+\t\tvar e error\n+\t\tfor _, s := range ss {\n+\t\t\te = stderrors.Join(e, s.Close())\n+\t\t}\n+\t\tif e != nil {\n+\t\t\tlog.Errorf(\"failed to close file streamer, %v\", e)\n \t\t}\n \t}()\n \tmeta, err := t.GetMeta(ss, args.ArchiveArgs)\n@@ -114,9 +174,9 @@ func getArchiveMeta(ctx context.Context, storage driver.Driver, path string, arg\n \tif !storage.Config().NoCache {\n \t\tExpiration := time.Minute * time.Duration(storage.GetStorage().CacheExpiration)\n \t\tarchiveMetaProvider.Expiration = &Expiration\n-\t} else if ss.Link.MFile == nil {\n+\t} else if ss[0].Link.MFile == nil {\n \t\t// alias\u3001crypt \u9a71\u52a8\n-\t\tarchiveMetaProvider.Expiration = ss.Link.Expiration\n+\t\tarchiveMetaProvider.Expiration = ss[0].Link.Expiration\n \t}\n \treturn obj, archiveMetaProvider, err\n }\n@@ -188,13 +248,17 @@ func _listArchive(ctx context.Context, storage driver.Driver, path string, args\n \t\t\treturn obj, files, err\n \t\t}\n \t}\n-\tobj, t, ss, err := getArchiveToolAndStream(ctx, storage, path, args.LinkArgs)\n+\tobj, t, ss, err := GetArchiveToolAndStream(ctx, storage, path, args.LinkArgs)\n \tif err != nil {\n \t\treturn nil, nil, err\n \t}\n \tdefer func() {\n-\t\tif err := ss.Close(); err != nil {\n-\t\t\tlog.Errorf(\"failed to close file streamer, %v\", err)\n+\t\tvar e error\n+\t\tfor _, s := range ss {\n+\t\t\te = stderrors.Join(e, s.Close())\n+\t\t}\n+\t\tif e != nil {\n+\t\t\tlog.Errorf(\"failed to close file streamer, %v\", e)\n \t\t}\n \t}()\n \tfiles, err := t.List(ss, args.ArchiveInnerArgs)\n@@ -378,8 +442,8 @@ func driverExtract(ctx context.Context, storage driver.Driver, path string, args\n }\n \n type streamWithParent struct {\n-\trc     io.ReadCloser\n-\tparent *stream.SeekableStream\n+\trc      io.ReadCloser\n+\tparents []*stream.SeekableStream\n }\n \n func (s *streamWithParent) Read(p []byte) (int, error) {\n@@ -387,24 +451,31 @@ func (s *streamWithParent) Read(p []byte) (int, error) {\n }\n \n func (s *streamWithParent) Close() error {\n-\terr1 := s.rc.Close()\n-\terr2 := s.parent.Close()\n-\treturn stderrors.Join(err1, err2)\n+\terr := s.rc.Close()\n+\tfor _, ss := range s.parents {\n+\t\terr = stderrors.Join(err, ss.Close())\n+\t}\n+\treturn err\n }\n \n func InternalExtract(ctx context.Context, storage driver.Driver, path string, args model.ArchiveInnerArgs) (io.ReadCloser, int64, error) {\n-\t_, t, ss, err := getArchiveToolAndStream(ctx, storage, path, args.LinkArgs)\n+\t_, t, ss, err := GetArchiveToolAndStream(ctx, storage, path, args.LinkArgs)\n \tif err != nil {\n \t\treturn nil, 0, err\n \t}\n \trc, size, err := t.Extract(ss, args)\n \tif err != nil {\n-\t\tif e := ss.Close(); e != nil {\n+\t\tvar e error\n+\t\tfor _, s := range ss {\n+\t\t\te = stderrors.Join(e, s.Close())\n+\t\t}\n+\t\tif e != nil {\n \t\t\tlog.Errorf(\"failed to close file streamer, %v\", e)\n+\t\t\terr = stderrors.Join(err, e)\n \t\t}\n \t\treturn nil, 0, err\n \t}\n-\treturn &streamWithParent{rc: rc, parent: ss}, size, nil\n+\treturn &streamWithParent{rc: rc, parents: ss}, size, nil\n }\n \n func ArchiveDecompress(ctx context.Context, storage driver.Driver, srcPath, dstDirPath string, args model.ArchiveDecompressArgs, lazyCache ...bool) error {\ndiff --git a/internal/stream/limit.go b/internal/stream/limit.go\nindex 3b32a55ff6d..14d0efd0f3e 100644\n--- a/internal/stream/limit.go\n+++ b/internal/stream/limit.go\n@@ -139,7 +139,7 @@ type RateLimitRangeReadCloser struct {\n \tLimiter Limiter\n }\n \n-func (rrc RateLimitRangeReadCloser) RangeRead(ctx context.Context, httpRange http_range.Range) (io.ReadCloser, error) {\n+func (rrc *RateLimitRangeReadCloser) RangeRead(ctx context.Context, httpRange http_range.Range) (io.ReadCloser, error) {\n \trc, err := rrc.RangeReadCloserIF.RangeRead(ctx, httpRange)\n \tif err != nil {\n \t\treturn nil, err\ndiff --git a/internal/stream/stream.go b/internal/stream/stream.go\nindex 1c94715f95a..f6b045a0238 100644\n--- a/internal/stream/stream.go\n+++ b/internal/stream/stream.go\n@@ -14,6 +14,7 @@ import (\n \t\"github.com/alist-org/alist/v3/pkg/http_range\"\n \t\"github.com/alist-org/alist/v3/pkg/utils\"\n \t\"github.com/sirupsen/logrus\"\n+\t\"go4.org/readerutil\"\n )\n \n type FileStream struct {\n@@ -159,6 +160,10 @@ var _ model.FileStreamer = (*FileStream)(nil)\n //var _ seekableStream = (*FileStream)(nil)\n \n // for most internal stream, which is either RangeReadCloser or MFile\n+// Any functionality implemented based on SeekableStream should implement a Close method,\n+// whose only purpose is to close the SeekableStream object. If such functionality has\n+// additional resources that need to be closed, they should be added to the Closer property of\n+// the SeekableStream object and be closed together when the SeekableStream object is closed.\n type SeekableStream struct {\n \tFileStream\n \tLink *model.Link\n@@ -196,7 +201,7 @@ func NewSeekableStream(fs FileStream, link *model.Link) (*SeekableStream, error)\n \t\t\treturn &ss, nil\n \t\t}\n \t\tif ss.Link.RangeReadCloser != nil {\n-\t\t\tss.rangeReadCloser = RateLimitRangeReadCloser{\n+\t\t\tss.rangeReadCloser = &RateLimitRangeReadCloser{\n \t\t\t\tRangeReadCloserIF: ss.Link.RangeReadCloser,\n \t\t\t\tLimiter:           ServerDownloadLimit,\n \t\t\t}\n@@ -208,7 +213,7 @@ func NewSeekableStream(fs FileStream, link *model.Link) (*SeekableStream, error)\n \t\t\tif err != nil {\n \t\t\t\treturn nil, err\n \t\t\t}\n-\t\t\trrc = RateLimitRangeReadCloser{\n+\t\t\trrc = &RateLimitRangeReadCloser{\n \t\t\t\tRangeReadCloserIF: rrc,\n \t\t\t\tLimiter:           ServerDownloadLimit,\n \t\t\t}\n@@ -364,7 +369,7 @@ type RangeReadReadAtSeeker struct {\n \tss        *SeekableStream\n \tmasterOff int64\n \treaders   []*readerCur\n-\t*headCache\n+\theadCache *headCache\n }\n \n type headCache struct {\n@@ -406,7 +411,7 @@ func (c *headCache) read(p []byte) (n int, err error) {\n \t}\n \treturn\n }\n-func (r *headCache) close() error {\n+func (r *headCache) Close() error {\n \tfor i := range r.bufs {\n \t\tr.bufs[i] = nil\n \t}\n@@ -419,6 +424,7 @@ func (r *RangeReadReadAtSeeker) InitHeadCache() {\n \t\treader := r.readers[0]\n \t\tr.readers = r.readers[1:]\n \t\tr.headCache = &headCache{readerCur: reader}\n+\t\tr.ss.Closers.Add(r.headCache)\n \t}\n }\n \n@@ -449,6 +455,18 @@ func NewReadAtSeeker(ss *SeekableStream, offset int64, forceRange ...bool) (SStr\n \treturn r, nil\n }\n \n+func NewMultiReaderAt(ss []*SeekableStream) (readerutil.SizeReaderAt, error) {\n+\treaders := make([]readerutil.SizeReaderAt, 0, len(ss))\n+\tfor _, s := range ss {\n+\t\tra, err := NewReadAtSeeker(s, 0)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\treaders = append(readers, io.NewSectionReader(ra, 0, s.GetSize()))\n+\t}\n+\treturn readerutil.NewMultiReaderAt(readers...), nil\n+}\n+\n func (r *RangeReadReadAtSeeker) GetRawStream() *SeekableStream {\n \treturn r.ss\n }\n@@ -559,9 +577,6 @@ func (r *RangeReadReadAtSeeker) Read(p []byte) (n int, err error) {\n }\n \n func (r *RangeReadReadAtSeeker) Close() error {\n-\tif r.headCache != nil {\n-\t\t_ = r.headCache.close()\n-\t}\n \treturn r.ss.Close()\n }\n \ndiff --git a/server/handles/archive.go b/server/handles/archive.go\nindex 4ec933e17b0..550bc3cec9c 100644\n--- a/server/handles/archive.go\n+++ b/server/handles/archive.go\n@@ -1,10 +1,11 @@\n package handles\n \n import (\n+\t\"encoding/json\"\n \t\"fmt\"\n+\t\"github.com/alist-org/alist/v3/internal/task\"\n \t\"net/url\"\n \tstdpath \"path\"\n-\t\"strings\"\n \n \t\"github.com/alist-org/alist/v3/internal/archive/tool\"\n \t\"github.com/alist-org/alist/v3/internal/conf\"\n@@ -208,14 +209,30 @@ func FsArchiveList(c *gin.Context) {\n \t})\n }\n \n+type StringOrArray []string\n+\n+func (s *StringOrArray) UnmarshalJSON(data []byte) error {\n+\tvar value string\n+\tif err := json.Unmarshal(data, &value); err == nil {\n+\t\t*s = []string{value}\n+\t\treturn nil\n+\t}\n+\tvar sliceValue []string\n+\tif err := json.Unmarshal(data, &sliceValue); err != nil {\n+\t\treturn err\n+\t}\n+\t*s = sliceValue\n+\treturn nil\n+}\n+\n type ArchiveDecompressReq struct {\n-\tSrcDir        string `json:\"src_dir\" form:\"src_dir\"`\n-\tDstDir        string `json:\"dst_dir\" form:\"dst_dir\"`\n-\tName          string `json:\"name\" form:\"name\"`\n-\tArchivePass   string `json:\"archive_pass\" form:\"archive_pass\"`\n-\tInnerPath     string `json:\"inner_path\" form:\"inner_path\"`\n-\tCacheFull     bool   `json:\"cache_full\" form:\"cache_full\"`\n-\tPutIntoNewDir bool   `json:\"put_into_new_dir\" form:\"put_into_new_dir\"`\n+\tSrcDir        string        `json:\"src_dir\" form:\"src_dir\"`\n+\tDstDir        string        `json:\"dst_dir\" form:\"dst_dir\"`\n+\tName          StringOrArray `json:\"name\" form:\"name\"`\n+\tArchivePass   string        `json:\"archive_pass\" form:\"archive_pass\"`\n+\tInnerPath     string        `json:\"inner_path\" form:\"inner_path\"`\n+\tCacheFull     bool          `json:\"cache_full\" form:\"cache_full\"`\n+\tPutIntoNewDir bool          `json:\"put_into_new_dir\" form:\"put_into_new_dir\"`\n }\n \n func FsArchiveDecompress(c *gin.Context) {\n@@ -229,41 +246,51 @@ func FsArchiveDecompress(c *gin.Context) {\n \t\tcommon.ErrorResp(c, errs.PermissionDenied, 403)\n \t\treturn\n \t}\n-\tsrcPath, err := user.JoinPath(stdpath.Join(req.SrcDir, req.Name))\n-\tif err != nil {\n-\t\tcommon.ErrorResp(c, err, 403)\n-\t\treturn\n+\tsrcPaths := make([]string, 0, len(req.Name))\n+\tfor _, name := range req.Name {\n+\t\tsrcPath, err := user.JoinPath(stdpath.Join(req.SrcDir, name))\n+\t\tif err != nil {\n+\t\t\tcommon.ErrorResp(c, err, 403)\n+\t\t\treturn\n+\t\t}\n+\t\tsrcPaths = append(srcPaths, srcPath)\n \t}\n \tdstDir, err := user.JoinPath(req.DstDir)\n \tif err != nil {\n \t\tcommon.ErrorResp(c, err, 403)\n \t\treturn\n \t}\n-\tt, err := fs.ArchiveDecompress(c, srcPath, dstDir, model.ArchiveDecompressArgs{\n-\t\tArchiveInnerArgs: model.ArchiveInnerArgs{\n-\t\t\tArchiveArgs: model.ArchiveArgs{\n-\t\t\t\tLinkArgs: model.LinkArgs{\n-\t\t\t\t\tHeader:  c.Request.Header,\n-\t\t\t\t\tType:    c.Query(\"type\"),\n-\t\t\t\t\tHttpReq: c.Request,\n+\ttasks := make([]task.TaskExtensionInfo, 0, len(srcPaths))\n+\tfor _, srcPath := range srcPaths {\n+\t\tt, e := fs.ArchiveDecompress(c, srcPath, dstDir, model.ArchiveDecompressArgs{\n+\t\t\tArchiveInnerArgs: model.ArchiveInnerArgs{\n+\t\t\t\tArchiveArgs: model.ArchiveArgs{\n+\t\t\t\t\tLinkArgs: model.LinkArgs{\n+\t\t\t\t\t\tHeader:  c.Request.Header,\n+\t\t\t\t\t\tType:    c.Query(\"type\"),\n+\t\t\t\t\t\tHttpReq: c.Request,\n+\t\t\t\t\t},\n+\t\t\t\t\tPassword: req.ArchivePass,\n \t\t\t\t},\n-\t\t\t\tPassword: req.ArchivePass,\n+\t\t\t\tInnerPath: utils.FixAndCleanPath(req.InnerPath),\n \t\t\t},\n-\t\t\tInnerPath: utils.FixAndCleanPath(req.InnerPath),\n-\t\t},\n-\t\tCacheFull:     req.CacheFull,\n-\t\tPutIntoNewDir: req.PutIntoNewDir,\n-\t})\n-\tif err != nil {\n-\t\tif errors.Is(err, errs.WrongArchivePassword) {\n-\t\t\tcommon.ErrorResp(c, err, 202)\n-\t\t} else {\n-\t\t\tcommon.ErrorResp(c, err, 500)\n+\t\t\tCacheFull:     req.CacheFull,\n+\t\t\tPutIntoNewDir: req.PutIntoNewDir,\n+\t\t})\n+\t\tif e != nil {\n+\t\t\tif errors.Is(e, errs.WrongArchivePassword) {\n+\t\t\t\tcommon.ErrorResp(c, e, 202)\n+\t\t\t} else {\n+\t\t\t\tcommon.ErrorResp(c, e, 500)\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\t\tif t != nil {\n+\t\t\ttasks = append(tasks, t)\n \t\t}\n-\t\treturn\n \t}\n \tcommon.SuccessResp(c, gin.H{\n-\t\t\"task\": getTaskInfo(t),\n+\t\t\"task\": getTaskInfos(tasks),\n \t})\n }\n \n@@ -376,7 +403,7 @@ func ArchiveInternalExtract(c *gin.Context) {\n func ArchiveExtensions(c *gin.Context) {\n \tvar ext []string\n \tfor key := range tool.Tools {\n-\t\text = append(ext, strings.TrimPrefix(key, \".\"))\n+\t\text = append(ext, key)\n \t}\n \tcommon.SuccessResp(c, ext)\n }\n", "instance_id": "AlistGo__alist-8184", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: there is a bug in the AList application where decompressing password-protected RAR and 7Z archives results in a \"wrong archive password\" error, even when the correct password is provided. Additionally, for 7Z files, there is no opportunity to input a password. The goal is evident\u2014fix the password handling for RAR and 7Z archive decompression. The statement includes relevant details such as the version of AList (V3.42.0), the drivers used (OneDrive and local), and reproduction steps (upload a password-protected RAR or 7Z file and attempt to decompress it). However, there are minor ambiguities: it lacks specific details about the expected behavior for password input (e.g., should there be a UI prompt for 7Z files?), and it does not mention any specific edge cases or constraints (e.g., archive size limits, multi-part archives, or specific encryption methods). Despite these minor gaps, the problem is valid and mostly clear, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files and modules within the AList codebase (e.g., `internal/archive/rardecode`, `internal/archive/sevenzip`, `internal/archive/zip`, `internal/fs/archive.go`, and others). The changes involve refactoring the archive handling logic to support multi-part archives and proper password management for RAR and 7Z formats, which requires understanding interactions between different components like stream handling, file system operations, and archive metadata processing. Second, the number of technical concepts involved is substantial, including Go-specific features (e.g., interfaces, struct methods), third-party libraries (`nwaples/rardecode`, `bodgit/sevenzip`, `yeka/zip`), file streaming and seeking mechanisms, and archive format specifics (e.g., multi-part file handling, encryption). Third, the modifications impact core functionality (archive decompression), which could affect the system's behavior if not handled carefully, especially with the introduction of new error handling logic for password validation. Finally, while some edge cases like multi-part archives are addressed in the code changes, others (e.g., very large archives, corrupted files) are not explicitly mentioned, but the complexity of handling password errors and file streaming suggests non-trivial edge case considerations. A score of 0.65 reflects the need for a deep understanding of the codebase and archive handling, combined with complex modifications across multiple files, though it does not reach the \"Very Hard\" range as it does not involve system-level or highly domain-specific challenges beyond archive processing.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\u4e2d\u56fd\u79fb\u52a8\u4e91\u76d8\u5bb6\u5ead\u4e91\u4e00\u4e2a\u6587\u4ef6\u5939\u5185\u53ea\u80fd\u663e\u793a200\u4e2a\u6587\u4ef6\n### Please make sure of the following things\n\n- [X] I have read the [documentation](https://alist.nn.ci).\n\u6211\u5df2\u7ecf\u9605\u8bfb\u4e86[\u6587\u6863](https://alist.nn.ci)\u3002\n\n- [X] I'm sure there are no duplicate issues or discussions.\n\u6211\u786e\u5b9a\u6ca1\u6709\u91cd\u590d\u7684issue\u6216\u8ba8\u8bba\u3002\n\n- [X] I'm sure it's due to `AList` and not something else(such as [Network](https://alist.nn.ci/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host) ,`Dependencies` or `Operational`).\n\u6211\u786e\u5b9a\u662f`AList`\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5176\u4ed6\u539f\u56e0\uff08\u4f8b\u5982[\u7f51\u7edc](https://alist.nn.ci/zh/faq/howto.html#tls-handshake-timeout-read-connection-reset-by-peer-dns-lookup-failed-connect-connection-refused-client-timeout-exceeded-while-awaiting-headers-no-such-host)\uff0c`\u4f9d\u8d56`\u6216`\u64cd\u4f5c`\uff09\u3002\n\n- [X] I'm sure this issue is not fixed in the latest version.\n\u6211\u786e\u5b9a\u8fd9\u4e2a\u95ee\u9898\u5728\u6700\u65b0\u7248\u672c\u4e2d\u6ca1\u6709\u88ab\u4fee\u590d\u3002\n\n\n### AList Version / AList \u7248\u672c\n\nv3.41.0\n\n### Driver used / \u4f7f\u7528\u7684\u5b58\u50a8\u9a71\u52a8\n\n\u4e2d\u56fd\u79fb\u52a8\u4e91\u76d8\uff0c\u5bb6\u5ead\u4e91\n\n### Describe the bug / \u95ee\u9898\u63cf\u8ff0\n\n\u5728\u4e2d\u56fd\u79fb\u52a8\u4e91\u76d8\u5bb6\u5ead\u4e91\u4e2d\uff0c\u6d4b\u8bd5\u6587\u4ef6\u5939\u91cc\u9762\u4e00\u5171\u6709225\u4e2a\u6587\u4ef6(001.jpg-225.jpg)\uff0c\u5728alist\u5c31\u53ea\u80fd\u663e\u793a200\u4e2a\u6587\u4ef6(026.jpg-225.jpg)\uff0c\u5269\u4f59\u7684\u6587\u4ef6\u65e0\u6cd5\u663e\u793a\uff0c\u4f7f\u7528\u6700\u65b0\u7684AList Beta Version\uff0c\u8fdb\u884c\u6d4b\u8bd5\u7ed3\u679c\u4f9d\u65e7\u53ea\u80fd\u663e\u793a200\u4e2a\u6587\u4ef6\r\n![Screenshot_2024-12-26-10-35-19-277_com chinamobile mcloud-edit](https://github.com/user-attachments/assets/1a39af48-1b29-408c-9a33-2b5c7bfc12bd)\r\n![Screenshot_2024-12-26-10-41-12-738_com android chrome](https://github.com/user-attachments/assets/89bd64ae-d395-4467-98ee-f54da5038968)\r\n\n\n### Reproduction / \u590d\u73b0\u94fe\u63a5\n\n\u56e0\u4e3a\u94fe\u63a5\u91cc\u9762\u6709\u4e00\u4e9b\u654f\u611f\u6587\u4ef6\uff0c\u6240\u4ee5\u65e0\u6cd5\u63d0\u4f9b\u6d4b\u8bd5\u94fe\u63a5\uff0c\u6211\u8bd5\u7740\u91cd\u65b0\u5b89\u88c5\uff0c\u4f9d\u7136\u53ea\u663e\u793a\u524d200\u4e2a\u6587\u4ef6\n\n### Config / \u914d\u7f6e\n\n{\r\n  \"force\": false,\r\n  \"site_url\": \"\",\r\n  \"cdn\": \"\",\r\n  \"jwt_secret\": \"JomQBeFWWt7QYVhF\",\r\n  \"token_expires_in\": 48,\r\n  \"database\": {\r\n    \"type\": \"sqlite3\",\r\n    \"host\": \"\",\r\n    \"port\": 0,\r\n    \"user\": \"\",\r\n    \"password\": \"\",\r\n    \"name\": \"\",\r\n    \"db_file\": \"data/data.db\",\r\n    \"table_prefix\": \"x_\",\r\n    \"ssl_mode\": \"\",\r\n    \"dsn\": \"\"\r\n  },\r\n  \"meilisearch\": {\r\n    \"host\": \"http://localhost:7700\",\r\n    \"api_key\": \"\",\r\n    \"index_prefix\": \"\"\r\n  },\r\n  \"scheme\": {\r\n    \"address\": \"0.0.0.0\",\r\n    \"http_port\": 5244,\r\n    \"https_port\": -1,\r\n    \"force_https\": false,\r\n    \"cert_file\": \"\",\r\n    \"key_file\": \"\",\r\n    \"unix_file\": \"\",\r\n    \"unix_file_perm\": \"\"\r\n  },\r\n  \"temp_dir\": \"data/temp\",\r\n  \"bleve_dir\": \"data/bleve\",\r\n  \"dist_dir\": \"\",\r\n  \"log\": {\r\n    \"enable\": true,\r\n    \"name\": \"data/log/log.log\",\r\n    \"max_size\": 50,\r\n    \"max_backups\": 30,\r\n    \"max_age\": 28,\r\n    \"compress\": false\r\n  },\r\n  \"delayed_start\": 0,\r\n  \"max_connections\": 0,\r\n  \"tls_insecure_skip_verify\": true,\r\n  \"tasks\": {\r\n    \"download\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 1,\r\n      \"task_persistant\": false\r\n    },\r\n    \"transfer\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 2,\r\n      \"task_persistant\": false\r\n    },\r\n    \"upload\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 0,\r\n      \"task_persistant\": false\r\n    },\r\n    \"copy\": {\r\n      \"workers\": 5,\r\n      \"max_retry\": 2,\r\n      \"task_persistant\": false\r\n    }\r\n  },\r\n  \"cors\": {\r\n    \"allow_origins\": [\r\n      \"*\"\r\n    ],\r\n    \"allow_methods\": [\r\n      \"*\"\r\n    ],\r\n    \"allow_headers\": [\r\n      \"*\"\r\n    ]\r\n  },\r\n  \"s3\": {\r\n    \"enable\": false,\r\n    \"port\": 5246,\r\n    \"ssl\": false\r\n  },\r\n  \"ftp\": {\r\n    \"enable\": false,\r\n    \"listen\": \":5221\",\r\n    \"find_pasv_port_attempts\": 50,\r\n    \"active_transfer_port_non_20\": false,\r\n    \"idle_timeout\": 900,\r\n    \"connection_timeout\": 30,\r\n    \"disable_active_mode\": false,\r\n    \"default_transfer_binary\": false,\r\n    \"enable_active_conn_ip_check\": true,\r\n    \"enable_pasv_conn_ip_check\": true\r\n  },\r\n  \"sftp\": {\r\n    \"enable\": false,\r\n    \"listen\": \":5222\"\r\n  }\n\n### Logs / \u65e5\u5fd7\n\n_No response_\n", "patch": "diff --git a/drivers/139/util.go b/drivers/139/util.go\nindex ccb6a912f32..d0b4d3b46a7 100644\n--- a/drivers/139/util.go\n+++ b/drivers/139/util.go\n@@ -252,7 +252,7 @@ func (d *Yun139) familyGetFiles(catalogID string) ([]model.Obj, error) {\n \t\t\t}\n \t\t\tfiles = append(files, &f)\n \t\t}\n-\t\tif 100*pageNum > resp.Data.TotalCount {\n+\t\tif resp.Data.TotalCount == 0 {\n \t\t\tbreak\n \t\t}\n \t\tpageNum++\n", "instance_id": "AlistGo__alist-7748", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: in the context of the AList application using the \"\u4e2d\u56fd\u79fb\u52a8\u4e91\u76d8\u5bb6\u5ead\u4e91\" (China Mobile Cloud Disk Family Cloud) driver, only 200 out of 225 files in a folder are displayed. The user provides version details (v3.41.0), screenshots for evidence, and confirms that the issue persists in the latest beta version. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention whether this is a limitation of the cloud service API, a bug in AList's implementation, or a configuration issue. Additionally, there are no specific details about expected behavior (e.g., should pagination be implemented to show all files?) or constraints (e.g., API rate limits or maximum page sizes). While the issue is valid and understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is minimal and localized to a single file (`drivers/139/util.go`) and a specific function (`familyGetFiles`). The modification alters a loop termination condition from checking if `100*pageNum > resp.Data.TotalCount` to `resp.Data.TotalCount == 0`, which suggests a simple fix to handle pagination or file listing logic. This does not impact the broader system architecture or require changes across multiple modules. The amount of code change is very small (a single line).\n\n2. **Number of Technical Concepts:** Solving this issue requires understanding basic concepts such as API pagination, loop control in Go, and potentially the behavior of the China Mobile Cloud Disk API. These are relatively straightforward for a developer familiar with Go and API integrations. No advanced algorithms, design patterns, or domain-specific knowledge beyond typical cloud storage driver implementation are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases (e.g., folders with zero files, API failures, or rate limiting), and the code change does not introduce new error handling logic. However, a developer might need to consider whether the new condition (`resp.Data.TotalCount == 0`) adequately handles all scenarios, such as partial responses or API inconsistencies. These considerations are minor and do not significantly increase the difficulty.\n\n4. **Overall Assessment:** The issue appears to be a straightforward bug fix related to pagination logic in the file listing functionality of a specific driver. It requires minimal code changes and a basic understanding of the driver\u2019s interaction with the cloud API. There is no indication of complex interactions with other parts of the AList codebase or significant performance considerations. Therefore, a difficulty score of 0.25 reflects the simplicity of the task while acknowledging the need for some domain context (understanding the specific cloud driver behavior).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Swap position of checkout-commit and checkout-branch menu items\nWe recently added a new option to check out a commit's branch from within the commits, reflog, and sub-commits panels:\n\n![Image](https://github.com/user-attachments/assets/0a5cf3f2-6803-4709-ae5a-e4addc061012)\n\nAfter using it for some time, I find it annoying that the default option has changed. I rarely find myself wanting to check out a branch from the commits panel, and it's rarer still to want to check out a branch from the reflog and sub-commits panel. Although there may be use cases for this, it is jarring that something you can always do (checkout the commit) is harder to do than something that you can sometimes do (checkout the branch).\n\nWe've also had a user [complain](https://github.com/jesseduffield/lazygit/pull/4117) about their muscle-memory being broken by the recent change, and I have also fallen victim to this. I don't think that the new branch checkout option is sufficiently useful to dislodge the existing keybinding, so let's swap them.\n", "patch": "diff --git a/pkg/gui/controllers/helpers/refs_helper.go b/pkg/gui/controllers/helpers/refs_helper.go\nindex 1aa4d8dc3a2..02af116b54e 100644\n--- a/pkg/gui/controllers/helpers/refs_helper.go\n+++ b/pkg/gui/controllers/helpers/refs_helper.go\n@@ -278,7 +278,17 @@ func (self *RefsHelper) CreateCheckoutMenu(commit *models.Commit) error {\n \t})\n \n \thash := commit.Hash\n-\tvar menuItems []*types.MenuItem\n+\n+\tmenuItems := []*types.MenuItem{\n+\t\t{\n+\t\t\tLabelColumns: []string{fmt.Sprintf(self.c.Tr.Actions.CheckoutCommitAsDetachedHead, utils.ShortHash(hash))},\n+\t\t\tOnPress: func() error {\n+\t\t\t\tself.c.LogAction(self.c.Tr.Actions.CheckoutCommit)\n+\t\t\t\treturn self.CheckoutRef(hash, types.CheckoutRefOptions{})\n+\t\t\t},\n+\t\t\tKey: 'd',\n+\t\t},\n+\t}\n \n \tif len(branches) > 0 {\n \t\tmenuItems = append(menuItems, lo.Map(branches, func(branch *models.Branch, index int) *types.MenuItem {\n@@ -304,15 +314,6 @@ func (self *RefsHelper) CreateCheckoutMenu(commit *models.Commit) error {\n \t\t})\n \t}\n \n-\tmenuItems = append(menuItems, &types.MenuItem{\n-\t\tLabelColumns: []string{fmt.Sprintf(self.c.Tr.Actions.CheckoutCommitAsDetachedHead, utils.ShortHash(hash))},\n-\t\tOnPress: func() error {\n-\t\t\tself.c.LogAction(self.c.Tr.Actions.CheckoutCommit)\n-\t\t\treturn self.CheckoutRef(hash, types.CheckoutRefOptions{})\n-\t\t},\n-\t\tKey: 'd',\n-\t})\n-\n \treturn self.c.Menu(types.CreateMenuOptions{\n \t\tTitle: self.c.Tr.Actions.CheckoutBranchOrCommit,\n \t\tItems: menuItems,\n", "instance_id": "jesseduffield__lazygit-4238", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the goal of swapping the positions of the \"checkout-commit\" and \"checkout-branch\" menu items in a user interface for a Git tool. It provides context about why the change is needed (user feedback and muscle memory issues) and includes a visual reference (though not accessible in text form). The intent and desired outcome are evident, i.e., to prioritize the more commonly used action (checkout commit) over the less common one (checkout branch). However, there are minor ambiguities: the statement does not explicitly define the expected behavior after the swap (e.g., which keybinding should be assigned to which action post-swap) beyond implying a change in default or priority. Additionally, it lacks details on potential edge cases, such as how the menu should behave if no branches are associated with a commit, though this is partially inferable from the code changes. Overall, the problem is valid and mostly clear but misses some minor clarifying details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). The task involves a straightforward modification to a single file (refs_helper.go) and a single function (CreateCheckoutMenu). The code change is minimal, consisting of reordering menu items by moving the \"checkout commit as detached head\" option to the top of the menu list, which requires only basic understanding of Go syntax and slice manipulation. The scope is limited, with no impact on the broader codebase architecture or interactions between modules. The technical concepts involved are basic\u2014understanding structs, slices, and function closures in Go, along with minimal domain knowledge of Git concepts (commits and branches). There are no complex algorithms, design patterns, or libraries to comprehend. Regarding edge cases, the problem statement and code changes do not introduce new ones beyond what already exists in the codebase (e.g., handling commits with no associated branches, which is already managed in the code). Error handling remains unchanged. Overall, this is a simple UI adjustment that any junior developer with basic Go knowledge could implement, justifying a difficulty score of 0.15.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incorrect parsing of unicode escape codes\n**Describe the bug**\r\nIn the latest release (v0.43.1), some icons (e.g. nuxt.config.ts which uses the escape sequence `\\uf1106`, and tailwind.config.ts which uses the escape sequence `\\uf13ff`) are showing the incorrect icons (e.g. for nuxt.config.ts `\\uf110`+6 shown as `icon`6 instead of \\f1106 and for tailwind.config.ts `\\uf13f`+f shown as `icon`f instead of `\\uf13ff`).\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. `git init`\r\n2. `touch nuxt.config.ts tailwind.config.ts`\r\n3. `lazygit`\r\n4. See error\r\n\r\n**Expected behavior**\r\nExpected to see the proper icons\r\n\r\n**Version info:**\r\ncommit=v0.43.1, build date=2024-07-13T12:12:22Z, build source=termux, version=0.43.1, os=android, arch=arm64, git version=2.45.2\r\ngit version 2.45.2\n", "patch": "diff --git a/pkg/gui/presentation/icons/file_icons.go b/pkg/gui/presentation/icons/file_icons.go\nindex 0b4159f06dd..bd7bd08138c 100644\n--- a/pkg/gui/presentation/icons/file_icons.go\n+++ b/pkg/gui/presentation/icons/file_icons.go\n@@ -21,7 +21,7 @@ var nameIconMap = map[string]IconProperties{\n \t\".bash_profile\":              {Icon: \"\\ue615\", Color: 113},     // \ue615\n \t\".bashprofile\":               {Icon: \"\\ue615\", Color: 113},     // \ue615\n \t\".bashrc\":                    {Icon: \"\\ue795\", Color: 113},     // \ue795\n-\t\".dockerignore\":              {Icon: \"\\uf0868\", Color: 68},     // \udb82\udc68\n+\t\".dockerignore\":              {Icon: \"\\U000f0868\", Color: 68},  // \udb82\udc68\n \t\".ds_store\":                  {Icon: \"\\ue615\", Color: 239},     // \ue615\n \t\".editorconfig\":              {Icon: \"\\ue652\", Color: 255},     // \ue652\n \t\".env\":                       {Icon: \"\\uf462\", Color: 227},     // \uf462\n@@ -44,7 +44,7 @@ var nameIconMap = map[string]IconProperties{\n \t\".mailmap\":                   {Icon: \"\\U000f02a2\", Color: 202}, // \udb80\udea2\n \t\".npmignore\":                 {Icon: \"\\ue71e\", Color: 197},     // \ue71e\n \t\".npmrc\":                     {Icon: \"\\ue71e\", Color: 197},     // \ue71e\n-\t\".nuxtrc\":                    {Icon: \"\\uf1106\", Color: 42},     // \udb84\udd06\n+\t\".nuxtrc\":                    {Icon: \"\\U000f1106\", Color: 42},  // \udb84\udd06\n \t\".nvmrc\":                     {Icon: \"\\ue718\", Color: 71},      // \ue718\n \t\".prettierignore\":            {Icon: \"\\ue6b4\", Color: 33},      // \ue6b4\n \t\".prettierrc\":                {Icon: \"\\ue6b4\", Color: 33},      // \ue6b4\n@@ -76,21 +76,21 @@ var nameIconMap = map[string]IconProperties{\n \t\"cantorrc\":                   {Icon: \"\\uf373\", Color: 32},      // \uf373\n \t\"Cargo.lock\":                 {Icon: \"\\ue7a8\", Color: 216},     // \ue7a8\n \t\"Cargo.toml\":                 {Icon: \"\\ue7a8\", Color: 216},     // \ue7a8\n-\t\"checkhealth\":                {Icon: \"\\uf04d9\", Color: 75},     // \udb81\udcd9\n+\t\"checkhealth\":                {Icon: \"\\U000f04d9\", Color: 75},  // \udb81\udcd9\n \t\"cmakelists.txt\":             {Icon: \"\\ue615\", Color: 66},      // \ue615\n \t\"commit_editmsg\":             {Icon: \"\\ue702\", Color: 196},     // \ue702\n \t\"COMMIT_EDITMSG\":             {Icon: \"\\ue702\", Color: 239},     // \ue702\n-\t\"commitlint.config.js\":       {Icon: \"\\uf0718\", Color: 30},     // \udb81\udf18\n-\t\"commitlint.config.ts\":       {Icon: \"\\uf0718\", Color: 30},     // \udb81\udf18\n+\t\"commitlint.config.js\":       {Icon: \"\\U000f0718\", Color: 30},  // \udb81\udf18\n+\t\"commitlint.config.ts\":       {Icon: \"\\U000f0718\", Color: 30},  // \udb81\udf18\n \t\"compose.yaml\":               {Icon: \"\\uf308\", Color: 68},      // \uf308\n \t\"compose.yml\":                {Icon: \"\\uf308\", Color: 68},      // \uf308\n \t\"config\":                     {Icon: \"\\ue5fc\", Color: 241},     // \ue5fc\n-\t\"containerfile\":              {Icon: \"\\uf0868\", Color: 68},     // \udb82\udc68\n+\t\"containerfile\":              {Icon: \"\\U000f0868\", Color: 68},  // \udb82\udc68\n \t\"copying\":                    {Icon: \"\\ue60a\", Color: 185},     // \ue60a\n \t\"copying.lesser\":             {Icon: \"\\ue60a\", Color: 185},     // \ue60a\n \t\"docker-compose.yaml\":        {Icon: \"\\uf308\", Color: 68},      // \uf308\n \t\"docker-compose.yml\":         {Icon: \"\\uf308\", Color: 68},      // \uf308\n-\t\"dockerfile\":                 {Icon: \"\\uf0868\", Color: 68},     // \udb82\udc68\n+\t\"dockerfile\":                 {Icon: \"\\U000f0868\", Color: 68},  // \udb82\udc68\n \t\"Dockerfile\":                 {Icon: \"\\uf308\", Color: 68},      // \uf308\n \t\"ds_store\":                   {Icon: \"\\uf179\", Color: 15},      // \uf179\n \t\"eslint.config.cjs\":          {Icon: \"\\ue655\", Color: 56},      // \ue655\n@@ -148,10 +148,10 @@ var nameIconMap = map[string]IconProperties{\n \t\"mpv.conf\":                   {Icon: \"\\uf36e\", Color: 53},      // \uf36e\n \t\"node_modules\":               {Icon: \"\\ue718\", Color: 197},     // \ue718\n \t\"npmignore\":                  {Icon: \"\\ue71e\", Color: 197},     // \ue71e\n-\t\"nuxt.config.cjs\":            {Icon: \"\\uf1106\", Color: 42},     // \udb84\udd06\n-\t\"nuxt.config.js\":             {Icon: \"\\uf1106\", Color: 42},     // \udb84\udd06\n-\t\"nuxt.config.mjs\":            {Icon: \"\\uf1106\", Color: 42},     // \udb84\udd06\n-\t\"nuxt.config.ts\":             {Icon: \"\\uf1106\", Color: 42},     // \udb84\udd06\n+\t\"nuxt.config.cjs\":            {Icon: \"\\U000f1106\", Color: 42},  // \udb84\udd06\n+\t\"nuxt.config.js\":             {Icon: \"\\U000f1106\", Color: 42},  // \udb84\udd06\n+\t\"nuxt.config.mjs\":            {Icon: \"\\U000f1106\", Color: 42},  // \udb84\udd06\n+\t\"nuxt.config.ts\":             {Icon: \"\\U000f1106\", Color: 42},  // \udb84\udd06\n \t\"package.json\":               {Icon: \"\\ue71e\", Color: 197},     // \ue71e\n \t\"package-lock.json\":          {Icon: \"\\ue71e\", Color: 52},      // \ue71e\n \t\"PKGBUILD\":                   {Icon: \"\\uf303\", Color: 38},      // \uf303\n@@ -165,23 +165,23 @@ var nameIconMap = map[string]IconProperties{\n \t\"PrusaSlicer.ini\":            {Icon: \"\\uf351\", Color: 202},     // \uf351\n \t\"py.typed\":                   {Icon: \"\\ue606\", Color: 214},     // \ue606\n \t\"QtProject.conf\":             {Icon: \"\\uf375\", Color: 77},      // \uf375\n-\t\"R\":                          {Icon: \"\\uf07d4\", Color: 25},     // \udb81\udfd4\n-\t\"robots.txt\":                 {Icon: \"\\uf06a9\", Color: 60},     // \udb81\udea9\n+\t\"R\":                          {Icon: \"\\U000f07d4\", Color: 25},  // \udb81\udfd4\n+\t\"robots.txt\":                 {Icon: \"\\U000f06a9\", Color: 60},  // \udb81\udea9\n \t\"rubydoc\":                    {Icon: \"\\ue73b\", Color: 160},     // \ue73b\n \t\"settings.gradle\":            {Icon: \"\\ue660\", Color: 24},      // \ue660\n \t\"svelte.config.js\":           {Icon: \"\\ue697\", Color: 196},     // \ue697\n \t\"sxhkdrc\":                    {Icon: \"\\uf355\", Color: 236},     // \uf355\n \t\"sym-lib-table\":              {Icon: \"\\uf34c\", Color: 231},     // \uf34c\n-\t\"tailwind.config.js\":         {Icon: \"\\uf13ff\", Color: 45},     // \udb84\udfff\n-\t\"tailwind.config.mjs\":        {Icon: \"\\uf13ff\", Color: 45},     // \udb84\udfff\n-\t\"tailwind.config.ts\":         {Icon: \"\\uf13ff\", Color: 45},     // \udb84\udfff\n+\t\"tailwind.config.js\":         {Icon: \"\\U000f13ff\", Color: 45},  // \udb84\udfff\n+\t\"tailwind.config.mjs\":        {Icon: \"\\U000f13ff\", Color: 45},  // \udb84\udfff\n+\t\"tailwind.config.ts\":         {Icon: \"\\U000f13ff\", Color: 45},  // \udb84\udfff\n \t\"tmux.conf\":                  {Icon: \"\\uebc8\", Color: 34},      // \uebc8\n \t\"tmux.conf.local\":            {Icon: \"\\uebc8\", Color: 34},      // \uebc8\n \t\"tsconfig.json\":              {Icon: \"\\ue69d\", Color: 74},      // \ue69d\n \t\"unlicense\":                  {Icon: \"\\ue60a\", Color: 185},     // \ue60a\n \t\"vagrantfile$\":               {Icon: \"\\uf2b8\", Color: 27},      // \uf2b8\n-\t\"vlcrc\":                      {Icon: \"\\uf057c\", Color: 208},    // \udb81\udd7c\n-\t\"webpack\":                    {Icon: \"\\uf072b\", Color: 74},     // \udb81\udf2b\n+\t\"vlcrc\":                      {Icon: \"\\U000f057c\", Color: 208}, // \udb81\udd7c\n+\t\"webpack\":                    {Icon: \"\\U000f072b\", Color: 74},  // \udb81\udf2b\n \t\"weston.ini\":                 {Icon: \"\\uf367\", Color: 214},     // \uf367\n \t\"workspace\":                  {Icon: \"\\ue63a\", Color: 113},     // \ue63a\n \t\"xmobarrc.hs\":                {Icon: \"\\uf35e\", Color: 203},     // \uf35e\n@@ -207,7 +207,7 @@ var extIconMap = map[string]IconProperties{\n \t\".app\":            {Icon: \"\\ueae8\", Color: 124},     // \ueae8\n \t\".apple\":          {Icon: \"\\uf179\", Color: 15},      // \uf179\n \t\".applescript\":    {Icon: \"\\uf179\", Color: 66},      // \uf179\n-\t\".asc\":            {Icon: \"\\uf099d\", Color: 242},    // \udb82\udd9d\n+\t\".asc\":            {Icon: \"\\U000f099d\", Color: 242}, // \udb82\udd9d\n \t\".ass\":            {Icon: \"\\U000f0a16\", Color: 214}, // \udb82\ude16\n \t\".astro\":          {Icon: \"\\ue6b3\", Color: 197},     // \ue6b3\n \t\".avif\":           {Icon: \"\\uf1c5\", Color: 140},     // \uf1c5\n@@ -320,7 +320,7 @@ var extIconMap = map[string]IconProperties{\n \t\".exe\":            {Icon: \"\\uf17a\", Color: 81},      // \uf17a\n \t\".ex\":             {Icon: \"\\ue62d\", Color: 140},     // \ue62d\n \t\".exs\":            {Icon: \"\\ue62d\", Color: 140},     // \ue62d\n-\t\".f3d\":            {Icon: \"\\uf0eeb\", Color: 101},    // \udb83\udeeb\n+\t\".f3d\":            {Icon: \"\\U000f0eeb\", Color: 101}, // \udb83\udeeb\n \t\".f90\":            {Icon: \"\\U000f121a\", Color: 97},  // \udb84\ude1a\n \t\".fbx\":            {Icon: \"\\U000f01a7\", Color: 102}, // \udb80\udda7\n \t\".fcbak\":          {Icon: \"\\uf336\", Color: 160},     // \uf336\n@@ -380,7 +380,7 @@ var extIconMap = map[string]IconProperties{\n \t\".hs\":             {Icon: \"\\ue777\", Color: 140},     // \ue777\n \t\".htm\":            {Icon: \"\\uf13b\", Color: 196},     // \uf13b\n \t\".html\":           {Icon: \"\\uf13b\", Color: 196},     // \uf13b\n-\t\".huff\":           {Icon: \"\\uf0858\", Color: 56},     // \udb82\udc58\n+\t\".huff\":           {Icon: \"\\U000f0858\", Color: 56},  // \udb82\udc58\n \t\".hurl\":           {Icon: \"\\uf0ec\", Color: 198},     // \uf0ec\n \t\".hx\":             {Icon: \"\\ue666\", Color: 208},     // \ue666\n \t\".hxx\":            {Icon: \"\\uf0fd\", Color: 140},     // \uf0fd\n@@ -425,7 +425,7 @@ var extIconMap = map[string]IconProperties{\n \t\".jsx\":            {Icon: \"\\ue7ba\", Color: 45},      // \ue7ba\n \t\".jwmrc\":          {Icon: \"\\uf35b\", Color: 32},      // \uf35b\n \t\".jxl\":            {Icon: \"\\uf1c5\", Color: 241},     // \uf1c5\n-\t\".kbx\":            {Icon: \"\\uf0bc4\", Color: 243},    // \udb82\udfc4\n+\t\".kbx\":            {Icon: \"\\U000f0bc4\", Color: 243}, // \udb82\udfc4\n \t\".kdb\":            {Icon: \"\\uf23e\", Color: 71},      // \uf23e\n \t\".kdbx\":           {Icon: \"\\uf23e\", Color: 71},      // \uf23e\n \t\".kdenlive\":       {Icon: \"\\uf33c\", Color: 110},     // \uf33c\n@@ -472,12 +472,12 @@ var extIconMap = map[string]IconProperties{\n \t\".magnet\":         {Icon: \"\\uf076\", Color: 124},     // \uf076\n \t\".makefile\":       {Icon: \"\\ue779\", Color: 66},      // \ue779\n \t\".markdown\":       {Icon: \"\\uf48a\", Color: 74},      // \uf48a\n-\t\".material\":       {Icon: \"\\uf0509\", Color: 163},    // \udb81\udd09\n-\t\".md5\":            {Icon: \"\\uf0565\", Color: 103},    // \udb81\udd65\n+\t\".material\":       {Icon: \"\\U000f0509\", Color: 163}, // \udb81\udd09\n+\t\".md5\":            {Icon: \"\\U000f0565\", Color: 103}, // \udb81\udd65\n \t\".md\":             {Icon: \"\\uf48a\", Color: 74},      // \uf48a\n \t\".mdx\":            {Icon: \"\\uf48a\", Color: 74},      // \uf48a\n \t\".m\":              {Icon: \"\\ue61e\", Color: 111},     // \ue61e\n-\t\".mint\":           {Icon: \"\\uf032a\", Color: 108},    // \udb80\udf2a\n+\t\".mint\":           {Icon: \"\\U000f032a\", Color: 108}, // \udb80\udf2a\n \t\".mjs\":            {Icon: \"\\ue74e\", Color: 185},     // \ue74e\n \t\".mkd\":            {Icon: \"\\uf48a\", Color: 74},      // \uf48a\n \t\".mk\":             {Icon: \"\\ue795\", Color: 241},     // \ue795\n@@ -535,10 +535,10 @@ var extIconMap = map[string]IconProperties{\n \t\".properties\":     {Icon: \"\\ue60b\", Color: 185},     // \ue60b\n \t\".ps1\":            {Icon: \"\\ue795\", Color: 241},     // \ue795\n \t\".psb\":            {Icon: \"\\ue7b8\", Color: 74},      // \ue7b8\n-\t\".psd1\":           {Icon: \"\\uf0a0a\", Color: 68},     // \udb82\ude0a\n+\t\".psd1\":           {Icon: \"\\U000f0a0a\", Color: 68},  // \udb82\ude0a\n \t\".psd\":            {Icon: \"\\ue7b8\", Color: 241},     // \ue7b8\n-\t\".psm1\":           {Icon: \"\\uf0a0a\", Color: 68},     // \udb82\ude0a\n-\t\".pub\":            {Icon: \"\\uf0dd6\", Color: 222},    // \udb83\uddd6\n+\t\".psm1\":           {Icon: \"\\U000f0a0a\", Color: 68},  // \udb82\ude0a\n+\t\".pub\":            {Icon: \"\\U000f0dd6\", Color: 222}, // \udb83\uddd6\n \t\".pxd\":            {Icon: \"\\ue606\", Color: 39},      // \ue606\n \t\".pxi\":            {Icon: \"\\ue606\", Color: 39},      // \ue606\n \t\".pxm\":            {Icon: \"\\uf1c5\", Color: 241},     // \uf1c5\n@@ -570,7 +570,7 @@ var extIconMap = map[string]IconProperties{\n \t\".rlib\":           {Icon: \"\\ue7a8\", Color: 216},     // \ue7a8\n \t\".rmd\":            {Icon: \"\\uf48a\", Color: 74},      // \uf48a\n \t\".rpm\":            {Icon: \"\\ue7bb\", Color: 52},      // \ue7bb\n-\t\".rproj\":          {Icon: \"\\uf05c6\", Color: 29},     // \udb81\uddc6\n+\t\".rproj\":          {Icon: \"\\U000f05c6\", Color: 29},  // \udb81\uddc6\n \t\".rs\":             {Icon: \"\\ue7a8\", Color: 216},     // \ue7a8\n \t\".rspec\":          {Icon: \"\\ue21e\", Color: 160},     // \ue21e\n \t\".rspec_parallel\": {Icon: \"\\ue21e\", Color: 160},     // \ue21e\n@@ -584,13 +584,13 @@ var extIconMap = map[string]IconProperties{\n \t\".scad\":           {Icon: \"\\uf34e\", Color: 220},     // \uf34e\n \t\".scala\":          {Icon: \"\\ue737\", Color: 74},      // \ue737\n \t\".sc\":             {Icon: \"\\ue737\", Color: 167},     // \ue737\n-\t\".scm\":            {Icon: \"\\uf0627\", Color: 255},    // \udb81\ude27\n+\t\".scm\":            {Icon: \"\\U000f0627\", Color: 255}, // \udb81\ude27\n \t\".scss\":           {Icon: \"\\ue749\", Color: 204},     // \ue749\n-\t\".sha1\":           {Icon: \"\\uf0565\", Color: 103},    // \udb81\udd65\n-\t\".sha224\":         {Icon: \"\\uf0565\", Color: 103},    // \udb81\udd65\n-\t\".sha256\":         {Icon: \"\\uf0565\", Color: 103},    // \udb81\udd65\n-\t\".sha384\":         {Icon: \"\\uf0565\", Color: 103},    // \udb81\udd65\n-\t\".sha512\":         {Icon: \"\\uf0565\", Color: 103},    // \udb81\udd65\n+\t\".sha1\":           {Icon: \"\\U000f0565\", Color: 103}, // \udb81\udd65\n+\t\".sha224\":         {Icon: \"\\U000f0565\", Color: 103}, // \udb81\udd65\n+\t\".sha256\":         {Icon: \"\\U000f0565\", Color: 103}, // \udb81\udd65\n+\t\".sha384\":         {Icon: \"\\U000f0565\", Color: 103}, // \udb81\udd65\n+\t\".sha512\":         {Icon: \"\\U000f0565\", Color: 103}, // \udb81\udd65\n \t\".shell\":          {Icon: \"\\ue795\", Color: 239},     // \ue795\n \t\".sh\":             {Icon: \"\\ue795\", Color: 239},     // \ue795\n \t\".sig\":            {Icon: \"\\u03bb\", Color: 166},     // \u03bb\n@@ -631,10 +631,10 @@ var extIconMap = map[string]IconProperties{\n \t\".swift\":          {Icon: \"\\ue755\", Color: 208},     // \ue755\n \t\".tar\":            {Icon: \"\\uf410\", Color: 241},     // \uf410\n \t\".taz\":            {Icon: \"\\uf410\", Color: 241},     // \uf410\n-\t\".tbc\":            {Icon: \"\\uf06d3\", Color: 25},     // \udb81\uded3\n+\t\".tbc\":            {Icon: \"\\U000f06d3\", Color: 25},  // \udb81\uded3\n \t\".tbz2\":           {Icon: \"\\uf410\", Color: 241},     // \uf410\n \t\".tbz\":            {Icon: \"\\uf410\", Color: 241},     // \uf410\n-\t\".tcl\":            {Icon: \"\\uf06d3\", Color: 25},     // \udb81\uded3\n+\t\".tcl\":            {Icon: \"\\U000f06d3\", Color: 25},  // \udb81\uded3\n \t\".templ\":          {Icon: \"\\ueac4\", Color: 178},     // \ueac4\n \t\".terminal\":       {Icon: \"\\uf489\", Color: 34},      // \uf489\n \t\".test.js\":        {Icon: \"\\uf499\", Color: 185},     // \uf499\n@@ -680,7 +680,7 @@ var extIconMap = map[string]IconProperties{\n \t\".wav\":            {Icon: \"\\uf001\", Color: 241},     // \uf001\n \t\".webmanifest\":    {Icon: \"\\ue60b\", Color: 185},     // \ue60b\n \t\".webm\":           {Icon: \"\\uf03d\", Color: 241},     // \uf03d\n-\t\".webpack\":        {Icon: \"\\uf072b\", Color: 74},     // \udb81\udf2b\n+\t\".webpack\":        {Icon: \"\\U000f072b\", Color: 74},  // \udb81\udf2b\n \t\".webp\":           {Icon: \"\\uf1c5\", Color: 241},     // \uf1c5\n \t\".windows\":        {Icon: \"\\uf17a\", Color: 81},      // \uf17a\n \t\".wma\":            {Icon: \"\\uf001\", Color: 39},      // \uf001\n@@ -690,7 +690,7 @@ var extIconMap = map[string]IconProperties{\n \t\".wrz\":            {Icon: \"\\U000f01a7\", Color: 102}, // \udb80\udda7\n \t\".wvc\":            {Icon: \"\\uf001\", Color: 39},      // \uf001\n \t\".wv\":             {Icon: \"\\uf001\", Color: 39},      // \uf001\n-\t\".xaml\":           {Icon: \"\\uf0673\", Color: 56},     // \udb81\ude73\n+\t\".xaml\":           {Icon: \"\\U000f0673\", Color: 56},  // \udb81\ude73\n \t\".xcf\":            {Icon: \"\\uf338\", Color: 240},     // \uf338\n \t\".xcplayground\":   {Icon: \"\\ue755\", Color: 166},     // \ue755\n \t\".xcstrings\":      {Icon: \"\\U000f05ca\", Color: 31},  // \udb81\uddca\n", "instance_id": "jesseduffield__lazygit-3975", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: incorrect parsing of Unicode escape codes leading to improper display of icons for specific configuration files (e.g., nuxt.config.ts and tailwind.config.ts). It provides steps to reproduce the issue and specifies the expected behavior (proper icon display). However, there are minor ambiguities and missing details. For instance, it does not explicitly clarify the root cause of the parsing issue (e.g., whether it's a rendering issue in the terminal or a code-level parsing error). Additionally, while it mentions specific escape sequences, it lacks detailed examples of what the incorrect and correct outputs look like beyond a vague description (e.g., \"icon6\" instead of the proper icon). Edge cases or constraints, such as terminal compatibility or specific Unicode ranges, are not mentioned. Overall, the statement is valid and clear enough to understand the goal but misses some finer details that could aid in a comprehensive diagnosis.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward fix with minimal code changes. The issue is related to how Unicode escape sequences are represented in the Go source code. The code changes provided show a simple modification across multiple lines in a single file, replacing hardcoded Unicode escape sequences (e.g., \"\\uf1106\") with the proper Go syntax for Unicode code points (e.g., \"\\U000f1106\"). This does not require deep understanding of the codebase architecture, complex logic, or advanced technical concepts beyond basic knowledge of Go's string literal syntax for Unicode. The scope of the change is limited to a single file (file_icons.go) and involves no interaction with other modules or system architecture. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as the fix is purely syntactic. The amount of code change is moderate in terms of lines edited, but the nature of the change is repetitive and trivial, akin to a bulk find-and-replace operation. Overall, this is a very easy task that a junior developer with basic Go knowledge could handle.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Not launching in Korean environment with default `language: 'auto'` config\n**Describe the bug**\r\n\r\n\r\nAfter upgrading to version 0.43.0, lazygit does not launch with the default language setting \u2018auto\u2019.\r\n\r\n\r\nI have resolved the issue temporarily by setting the language to 'ko' or 'en' in the config file as follows:\r\n\r\n```yaml\r\ngui:\r\n    language: ko\r\n```\r\n(config file : `~/Library/Application\\ Support/lazygit/config.yml`)\r\n\r\n**To Reproduce**\r\n\r\n\r\n1. Install lazygit `0.43.0` (using homebrew)\r\n2. Open a terminal and run lazygit.\r\n\r\nerror\r\n```bash\r\n2024/07/13 16:35:26 open translations/ko-KR.json: file does not exist\r\n```\r\n\r\n**Expected behavior**\r\n\r\nlazygit should launch.\r\n\r\n\r\n**Screenshots**\r\n\r\n<img width=\"1097\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a3a2dd83-30b3-48f0-a6a4-6ea0826fefa1\">\r\n\r\n\r\n\r\n**Version info:**\r\n_Run `lazygit --version` and paste the result here_\r\n```\r\ncommit=, build date=, build source=homebrew, version=0.43.0, os=darwin, arch=arm64, git version=2.39.3 (Apple Git-146)\r\n```\r\n_Run `git --version` and paste the result here_\r\n```\r\ngit version 2.39.3 (Apple Git-146)\r\n```\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\r\n**Note:** please try updating to the latest version or [manually building](https://github.com/jesseduffield/lazygit/#manual) the latest `master` to see if the issue still occurs.\r\n\r\n<!--\r\nIf you want to try and debug this issue yourself, you can run `lazygit --debug` in one terminal panel and `lazygit --logs` in another to view the logs.\r\n-->\r\n\n", "patch": "diff --git a/pkg/i18n/i18n.go b/pkg/i18n/i18n.go\nindex f9febbe01ec..3848d42cf4c 100644\n--- a/pkg/i18n/i18n.go\n+++ b/pkg/i18n/i18n.go\n@@ -24,7 +24,7 @@ func NewTranslationSetFromConfig(log *logrus.Entry, configLanguage string) (*Tra\n \t\tlanguage := detectLanguage(jibber_jabber.DetectIETF)\n \t\tfor _, languageCode := range languageCodes {\n \t\t\tif strings.HasPrefix(language, languageCode) {\n-\t\t\t\treturn newTranslationSet(log, language)\n+\t\t\t\treturn newTranslationSet(log, languageCode)\n \t\t\t}\n \t\t}\n \n", "instance_id": "jesseduffield__lazygit-3744", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: lazygit fails to launch in a Korean environment with the default 'auto' language setting after upgrading to version 0.43.0. It provides steps to reproduce, the error message, expected behavior, and version information, which are helpful for understanding the context. However, there are minor ambiguities and missing details. For instance, it does not explicitly state whether the issue occurs only in Korean environments or if other languages are affected. Additionally, while the error message points to a missing translation file (`ko-KR.json`), the problem statement does not clarify if the file is expected to exist or if the detection logic is at fault. These gaps leave some room for interpretation, preventing a perfect clarity score. Overall, the description is valid and actionable but lacks exhaustive detail on edge cases or broader impact.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal, confined to a single line in a single file (`pkg/i18n/i18n.go`). It modifies the language detection logic to use the base language code (`languageCode`) instead of the full detected language string (`language`). This is a small, localized fix that does not impact the broader architecture or require changes across multiple modules.\n\n2. **Technical Concepts Involved**: The solution requires basic understanding of string manipulation (using `strings.HasPrefix`) and familiarity with internationalization (i18n) concepts, such as language code detection and translation file loading. The code also references a library (`jibber_jabber`) for language detection, but the change itself does not require deep knowledge of this library. These concepts are relatively straightforward for a developer with moderate experience in Go.\n\n3. **Edge Cases and Error Handling**: The problem statement highlights a specific error (`file does not exist` for `ko-KR.json`), and the fix addresses this by ensuring the language code used matches an expected format. However, the problem statement and code change do not explicitly address other potential edge cases, such as unsupported languages or fallback behavior if no matching language code is found. The error handling logic in the codebase is not modified in the provided diff, suggesting that the fix relies on existing mechanisms. This keeps the complexity low.\n\n4. **Overall Complexity**: The bug fix is a simple adjustment to the language detection logic, likely addressing a mismatch between the detected language format (`ko-KR`) and the expected format for translation files (`ko`). It does not require deep understanding of the lazygit codebase, complex algorithms, or performance optimizations. The impact is limited to the i18n module, and the change is unlikely to introduce significant side effects.\n\nGiven these points, a difficulty score of 0.25 reflects the ease of the task. It requires understanding some code logic and making a simple modification, fitting well within the \"Easy\" category. A more experienced developer would likely resolve this in under an hour, while a less experienced one might take slightly longer to grasp the context of language detection.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
