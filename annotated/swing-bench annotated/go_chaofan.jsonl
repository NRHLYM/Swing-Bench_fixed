{"problem_statement": "Potentially invalid use of #ifdef\nIn https://github.com/gopacket/gopacket/commit/94ad652445d73076cfbf84a5b6d3674828565dd2, code was added to check for 64-bit time. The following code was added in order to check for the existence of a type:\r\n\r\n```c\r\n#ifdef __time64_t\r\n```\r\n\r\nThe problem with this, as far as I'm aware, is that you cannot use `ifdef` to check whether a type has been defined. See further discussion here[^1]. Was there a particular reason this code was added? It breaks builds on 32-bit systems with 64-bit time enabled.\r\n\r\n[^1]: https://stackoverflow.com/questions/6772802/in-c-c-is-there-a-directive-similar-to-ifndef-for-typedefs.\n", "patch": "diff --git a/pcap/pcap_unix.go b/pcap/pcap_unix.go\nindex 8a38384b..f270638a 100644\n--- a/pcap/pcap_unix.go\n+++ b/pcap/pcap_unix.go\n@@ -117,7 +117,7 @@ int pcap_tstamp_type_name_to_val(const char* t) {\n \t\t#define gopacket_time_secs_t __time_t\n \t\t#define gopacket_time_usecs_t __suseconds_t\n \t#else\n-\t\t#ifdef __time64_t\n+\t\t#if defined(__USE_TIME64_REDIRECTS) || (__TIMESIZE == 32 && __USE_TIME_BITS64)\n \t\t\t#define gopacket_time_secs_t __time64_t\n \t\t\t#define gopacket_time_usecs_t __suseconds64_t\n \t\t#else\n", "instance_id": "gopacket__gopacket-90", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue with the use of `#ifdef` to check for a type definition (`__time64_t`), which is not a valid approach in C/C++ as highlighted by the referenced Stack Overflow discussion. It also specifies the impact, i.e., breaking builds on 32-bit systems with 64-bit time enabled. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or constraints for the fix (e.g., what should happen on systems with 64-bit time enabled), nor does it provide detailed context about the target environments or specific build errors encountered. Additionally, while the issue is described, there are no examples or test cases to illustrate the problem or validate the solution. Overall, the goal is understandable, but some minor details are missing to make it fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code change is localized to a single file (`pcap_unix.go`) and involves modifying a single conditional compilation directive. The diff shows a straightforward replacement of `#ifdef __time64_t` with a more appropriate condition (`#if defined(__USE_TIME64_REDIRECTS) || (__TIMESIZE == 32 && __USE_TIME_BITS64)`). There is no indication of broader impact on the system's architecture or interactions with other modules, and the amount of code change is minimal.\n\n2. **Technical Concepts Involved:** Solving this requires a basic understanding of C preprocessor directives and how they are used for conditional compilation. Familiarity with system-specific macros (like `__USE_TIME64_REDIRECTS` and `__TIMESIZE`) and their implications on 32-bit vs. 64-bit time handling is necessary, but these are relatively standard concepts for anyone working with low-level system programming or cross-platform code. No advanced algorithms, design patterns, or complex libraries are involved.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases beyond the general issue of 32-bit systems with 64-bit time enabled. The code change itself does not introduce new error handling logic; it simply adjusts the condition for defining time-related types. The complexity of edge cases appears low, as the fix targets a specific build configuration issue rather than runtime behavior.\n\n4. **Overall Complexity:** This is a bug fix that requires understanding a specific limitation of the C preprocessor and applying a targeted solution. It does not demand deep knowledge of the broader codebase or intricate logic. The primary challenge lies in knowing the correct macros to use for detecting 64-bit time support, which is a moderate but not overly complex task for someone with experience in systems programming.\n\nGiven these considerations, a difficulty score of 0.30 reflects the need for some specific knowledge of C compilation quirks and system macros, but the limited scope and straightforward nature of the change keep it in the easier spectrum of problems.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "all: fix ST1000, ST102[012] errors\n```\r\n$ staticcheck -checks=\"ST1000,ST1020,ST1021,ST1022\" ./...\r\ndoc.go:5:1: package comment should be of the form \"Package gonum ...\" (ST1000)\r\ndsp/window/window_complex.go:9:1: comment on exported function RectangularComplex should be of the form \"RectangularComplex ...\" (ST1020)\r\nfloats/floats.go:636:1: comment on exported function Reverse should be of the form \"Reverse ...\" (ST1020)\r\ngraph/formats/dot/internal/errors/doc.go:5:1: package comment should be of the form \"Package errors ...\" (ST1000)\r\ngraph/path/internal/testgraphs/doc.go:5:1: package comment should be of the form \"Package testgraphs ...\" (ST1000)\r\nintegrate/quad/quad.go:21:1: comment on exported type FixedLocationSingler should be of the form \"FixedLocationSingler ...\" (with optional leading article) (ST1021)\r\nlapack/lapack.go:224:1: comment on exported type MaximizeNormXJob should be of the form \"MaximizeNormXJob ...\" (with optional leading article) (ST1021)\r\nlapack/testlapack/dgesc2.go:1:1: package comment should be of the form \"Package testlapack ...\" (ST1000)\r\nmat/lu.go:240:1: comment on exported method Pivot should be of the form \"Pivot ...\" (ST1020)\r\nmathext/internal/amos/amos.go:520:1: comment on exported function Zbknu should be of the form \"Zbknu ...\" (ST1020)\r\nmathext/internal/amos/amos.go:1218:1: comment on exported function Zkscl should be of the form \"Zkscl ...\" (ST1020)\r\nmathext/internal/amos/amos.go:1391:1: comment on exported function Zacai should be of the form \"Zacai ...\" (ST1020)\r\nmathext/internal/amos/amos.go:1516:1: comment on exported function Zasyi should be of the form \"Zasyi ...\" (ST1020)\r\nmathext/internal/amos/amos.go:1719:1: comment on exported function Zmlri should be of the form \"Zmlri ...\" (ST1020)\r\nmathext/internal/cephes/ndtri.go:23:1: comment on exported var P0 should be of the form \"P0 ...\" (ST1022)\r\nmathext/internal/cephes/ndtri.go:44:1: comment on exported var P1 should be of the form \"P1 ...\" (ST1022)\r\nmathext/internal/cephes/ndtri.go:70:1: comment on exported var P2 should be of the form \"P2 ...\" (ST1022)\r\noptimize/convex/lp/simplex.go:5:1: package comment should be of the form \"Package lp ...\" (ST1000)\r\nspatial/r2/box.go:34:1: comment on exported method Empty should be of the form \"Empty ...\" (ST1020)\r\nspatial/r3/box.go:24:1: comment on exported method Empty should be of the form \"Empty ...\" (ST1020)\r\n```\n", "patch": "diff --git a/dsp/window/window_complex.go b/dsp/window/window_complex.go\nindex 404d5232c..9ee0bae9d 100644\n--- a/dsp/window/window_complex.go\n+++ b/dsp/window/window_complex.go\n@@ -6,8 +6,8 @@ package window\n \n import \"math\"\n \n-// Rectangular modifies seq in place by the Rectangular window and returns\n-// the result.\n+// RectangularComplex modifies seq in place by the Rectangular window and\n+// returns the result.\n // See https://en.wikipedia.org/wiki/Window_function#Rectangular_window and\n // https://www.recordingblogs.com/wiki/rectangular-window for details.\n //\ndiff --git a/floats/floats.go b/floats/floats.go\nindex 3dd351d00..68c4e65c7 100644\n--- a/floats/floats.go\n+++ b/floats/floats.go\n@@ -633,9 +633,9 @@ func Prod(s []float64) float64 {\n \treturn prod\n }\n \n-// Deprecated: This function simply calls [slices.Reverse].\n-//\n // Reverse reverses the order of elements in the slice.\n+//\n+// Deprecated: This function simply calls [slices.Reverse].\n func Reverse(s []float64) {\n \tslices.Reverse(s)\n }\ndiff --git a/graph/formats/dot/internal/errors/doc.go b/graph/formats/dot/internal/errors/doc.go\nindex 960624236..58cab6320 100644\n--- a/graph/formats/dot/internal/errors/doc.go\n+++ b/graph/formats/dot/internal/errors/doc.go\n@@ -2,5 +2,5 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// Package error provides generated internal error functions for DOT parsing.\n+// Package errors provides generated internal error functions for DOT parsing.\n package errors // import \"gonum.org/v1/gonum/graph/formats/dot/internal/errors\"\ndiff --git a/integrate/quad/quad.go b/integrate/quad/quad.go\nindex 121dd9489..b4fa0cf71 100644\n--- a/integrate/quad/quad.go\n+++ b/integrate/quad/quad.go\n@@ -18,9 +18,11 @@ type FixedLocationer interface {\n \tFixedLocations(x, weight []float64, min, max float64)\n }\n \n-// FixedLocationSingle returns the location and weight for element k in a\n-// fixed quadrature rule with n total samples and integral bounds from min to max.\n+// FixedLocationSingler wraps the FixedLocationSingle method.\n type FixedLocationSingler interface {\n+\t// FixedLocationSingle returns the location and weight for\n+\t// element k in a fixed quadrature rule with n total samples\n+\t// and integral bounds from min to max.\n \tFixedLocationSingle(n, k int, min, max float64) (x, weight float64)\n }\n \ndiff --git a/lapack/lapack.go b/lapack/lapack.go\nindex 60b5d0d36..60ef1c244 100644\n--- a/lapack/lapack.go\n+++ b/lapack/lapack.go\n@@ -221,7 +221,7 @@ const (\n \tEVSelected EVHowMany = 'S' // Compute selected right and/or left eigenvectors.\n )\n \n-// MaximizeNormX specifies the heuristic method for computing a contribution to\n+// MaximizeNormXJob specifies the heuristic method for computing a contribution to\n // the reciprocal Dif-estimate in Dlatdf.\n type MaximizeNormXJob byte\n \ndiff --git a/mat/lu.go b/mat/lu.go\nindex 18ed3dab6..b530ada7e 100644\n--- a/mat/lu.go\n+++ b/mat/lu.go\n@@ -237,6 +237,8 @@ func (lu *LU) RowPivots(dst []int) []int {\n \treturn dst\n }\n \n+// Pivot returns the row pivots of the receiver.\n+//\n // Deprecated: Use RowPivots instead.\n func (lu *LU) Pivot(dst []int) []int {\n \treturn lu.RowPivots(dst)\ndiff --git a/optimize/convex/lp/simplex.go b/optimize/convex/lp/simplex.go\nindex 5fd0dd867..b03d300cd 100644\n--- a/optimize/convex/lp/simplex.go\n+++ b/optimize/convex/lp/simplex.go\n@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// package lp implements routines for solving linear programs.\n+// Package lp implements routines for solving linear programs.\n package lp\n \n import (\ndiff --git a/spatial/r2/box.go b/spatial/r2/box.go\nindex 6c988b3b5..d3bc52230 100644\n--- a/spatial/r2/box.go\n+++ b/spatial/r2/box.go\n@@ -31,7 +31,7 @@ func (a Box) Center() Vec {\n \treturn Scale(0.5, Add(a.Min, a.Max))\n }\n \n-// IsEmpty returns true if a Box's volume is zero\n+// Empty returns true if a Box's volume is zero\n // or if a Min component is greater than its Max component.\n func (a Box) Empty() bool {\n \treturn a.Min.X >= a.Max.X || a.Min.Y >= a.Max.Y\ndiff --git a/spatial/r3/box.go b/spatial/r3/box.go\nindex b7552af65..86cf6eb4a 100644\n--- a/spatial/r3/box.go\n+++ b/spatial/r3/box.go\n@@ -21,7 +21,7 @@ func NewBox(x0, y0, z0, x1, y1, z1 float64) Box {\n \t}\n }\n \n-// IsEmpty returns true if a Box's volume is zero\n+// Empty returns true if a Box's volume is zero\n // or if a Min component is greater than its Max component.\n func (a Box) Empty() bool {\n \treturn a.Min.X >= a.Max.X || a.Min.Y >= a.Max.Y || a.Min.Z >= a.Max.Z\n", "instance_id": "gonum__gonum-1974", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to fix specific static analysis errors (ST1000, ST1020, ST1021, ST1022) identified by a staticcheck tool in a Go codebase. It provides a detailed list of the errors, including the file locations and the specific issues with comments and documentation formatting. However, it lacks explicit instructions on how to address each error beyond the error messages themselves, and there are no examples of correct formatting or detailed guidelines for the expected style. Additionally, edge cases or potential conflicts in documentation style are not mentioned. While the goal is understandable to someone familiar with Go and static analysis tools, minor ambiguities remain for those who might not be familiar with the specific formatting rules or the codebase's documentation conventions.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves straightforward modifications to documentation comments across multiple files in a Go codebase. The scope of changes is limited to updating package and function/type comments to adhere to a specific format as flagged by staticcheck (e.g., changing \"package error\" to \"Package errors\" or renaming function comments to match the function name). The changes are localized to individual lines in multiple files, with no impact on the system's architecture or logic, and the overall amount of code change is minimal. The technical concepts required are basic\u2014understanding Go documentation conventions and the staticcheck tool's output. No complex algorithms, design patterns, or domain-specific knowledge are needed. There are no significant edge cases or error handling requirements beyond ensuring the comments match the expected format. This task is repetitive and clerical in nature, requiring minimal problem-solving or deep understanding of the codebase beyond surface-level familiarity with Go syntax and documentation standards.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "'--dns exec' show output in real time?\n### Welcome\n\n- [X] Yes, I've searched similar issues on GitHub and didn't find any.\n\n### How do you use lego?\n\nBinary\n\n### Detailed Description\n\nI'm working on a shell script to use with `--dns exec` (to manually add/remove DNS records), and running into issues that take significantly longer than they should to debug, because the part of `lego` which runs my script appears to be gathering the script's output in memory and then dumping it to the output stream all at once. The process of adding/removing DNS records can take up to two minutes per name, so if I'm testing with a certificate having multiple hostnames on it (i.e. `name1.domain.xyz`, `'name2.domain.xyz`, and `name3.domain.xyz`), I see no output at all for several minutes and then get a dump all at once at the end.\r\n\r\nI'm looking at `providers/dns/exec/exec.go`, but I don't know enough about the exec module to know if this is even possible, so ... is there a way to make the code print each line of input, as soon as the external script prints it?\n", "patch": "diff --git a/providers/dns/exec/exec.go b/providers/dns/exec/exec.go\nindex a07cba0a63..23fdaf3849 100644\n--- a/providers/dns/exec/exec.go\n+++ b/providers/dns/exec/exec.go\n@@ -2,6 +2,7 @@\n package exec\n \n import (\n+\t\"bufio\"\n \t\"context\"\n \t\"errors\"\n \t\"fmt\"\n@@ -117,10 +118,27 @@ func (d *DNSProvider) run(ctx context.Context, command, domain, token, keyAuth s\n \n \tcmd := exec.CommandContext(ctx, d.config.Program, args...)\n \n-\toutput, err := cmd.CombinedOutput()\n-\tif len(output) > 0 {\n-\t\tlog.Println(string(output))\n+\tstdout, err := cmd.StdoutPipe()\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"create pipe: %w\", err)\n+\t}\n+\n+\tcmd.Stderr = cmd.Stdout\n+\n+\terr = cmd.Start()\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"start command: %w\", err)\n \t}\n \n-\treturn err\n+\tscanner := bufio.NewScanner(stdout)\n+\tfor scanner.Scan() {\n+\t\tlog.Println(scanner.Text())\n+\t}\n+\n+\terr = cmd.Wait()\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"wait command: %w\", err)\n+\t}\n+\n+\treturn nil\n }\n", "instance_id": "go-acme__lego-2166", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the user wants real-time output from an external script executed by the `--dns exec` feature in the `lego` tool, rather than having the output buffered and dumped all at once. The goal (real-time output), context (script execution for DNS record management), and the specific file of interest (`exec.go`) are mentioned. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what \"real-time output\" means in terms of expected behavior (e.g., line-by-line printing with specific formatting). Additionally, there are no examples of the script output or specific constraints on how the output should be handled (e.g., handling very large outputs or partial lines). Edge cases, such as script failures or unexpected output formats, are not addressed. Overall, while the intent is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are localized to a single file (`exec.go`) and specifically to the `run` method of the `DNSProvider` struct. The modification replaces the use of `CombinedOutput()` with a streaming approach using `StdoutPipe()` and `bufio.Scanner` to read and log output line by line. The change is relatively small (about 20 lines of diff) and does not impact the broader architecture of the system or require modifications across multiple modules.\n\n2. **Technical Concepts Involved**: Solving this requires understanding Go's `os/exec` package, specifically how to handle command output in real-time using pipes (`StdoutPipe`) and scanning input streams (`bufio.Scanner`). These are intermediate-level concepts in Go, but not overly complex for someone with moderate experience in the language. No advanced algorithms, design patterns, or domain-specific knowledge (beyond basic process execution) are required.\n\n3. **Edge Cases and Error Handling**: The code change introduces basic error handling for pipe creation, command start, and wait operations, which is an improvement over the original code. However, the problem statement does not explicitly mention specific edge cases (e.g., handling partial lines, very large outputs, or script hangs), and the provided solution does not fully address these either. The complexity of edge cases appears minimal at this stage, though a more robust solution might need to consider them.\n\n4. **Overall Complexity**: The task involves a straightforward bug fix or feature enhancement\u2014switching from buffered to streaming output. It requires understanding the existing code logic in `exec.go` and making a targeted modification. While it is not a trivial typo fix, it does not demand deep architectural knowledge or extensive debugging of complex interactions.\n\nA score of 0.35 reflects that this is slightly more involved than the simplest of tasks (e.g., changing a constant) due to the need to handle streaming I/O and error conditions, but it remains within the realm of an easy problem for a developer familiar with Go's standard library.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Publish dive to ghcr.io\n**What happened**:\n\nI tried out:\n\n```\ndocker pull ghcr.io/wagoodman/dive\n```\n\nand got this response:\n\n```\nError response from daemon: Head \"https://ghcr.io/v2/wagoodman/dive/manifests/latest\": denied\n```\n\n**What you expected to happen**:\n\nThe image to be pulled.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n\n```\ndocker pull ghcr.io/wagoodman/dive\n```\n\n**Anything else we need to know?**:\n\nOpening https://ghcr.io/wagoodman/dive in a browser seems to indicate these packages have been removed or possibly become private.\n\n**Environment**:\n- OS version - macOS 15.3.2\n- Docker version (if applicable):\n```\nClient:\n Version:           28.0.1\n API version:       1.48\n Go version:        go1.23.6\n Git commit:        068a01e\n Built:             Wed Feb 26 10:38:16 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.39.0 (184744)\n Engine:\n  Version:          28.0.1\n  API version:      1.48 (minimum version 1.24)\n  Go version:       go1.23.6\n  Git commit:       bbd0a17\n  Built:            Wed Feb 26 10:40:57 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.25\n  GitCommit:        bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc:\n  Version:          1.2.4\n  GitCommit:        v1.2.4-0-g6c52b3f\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n", "patch": "diff --git a/.github/workflows/release.yaml b/.github/workflows/release.yaml\nindex bc132916..d10a1678 100644\n--- a/.github/workflows/release.yaml\n+++ b/.github/workflows/release.yaml\n@@ -83,9 +83,6 @@ jobs:\n       contents: write\n       # for pushing container images\n       packages: write\n-    env:\n-      REGISTRY: ghcr.io\n-      IMAGE_NAME: ${{ github.repository }}\n     steps:\n       - uses: actions/checkout@v4\n         with:\n@@ -94,12 +91,6 @@ jobs:\n       - name: Bootstrap environment\n         uses: ./.github/actions/bootstrap\n \n-      - name: Set up QEMU\n-        uses: docker/setup-qemu-action@v3\n-\n-      - name: Set up Docker Buildx\n-        uses: docker/setup-buildx-action@v3\n-\n       - name: Tag release\n         run: |\n           git tag ${{ github.event.inputs.version }}\n@@ -110,7 +101,7 @@ jobs:\n       - name: Login to container registry\n         uses: docker/login-action@v3\n         with:\n-          registry: ${{ env.REGISTRY }}\n+          registry: docker.io\n           username: ${{ github.repository_owner }}\n           password: ${{ secrets.GITHUB_TOKEN }}\n \ndiff --git a/.goreleaser.yaml b/.goreleaser.yaml\nindex e227b8ec..2b12c2df 100644\n--- a/.goreleaser.yaml\n+++ b/.goreleaser.yaml\n@@ -8,6 +8,11 @@ release:\n   # If set to true, will not auto-publish the release. This is done to allow us to review the changelog before publishing.\n   draft: false\n \n+env:\n+  # required to support multi architecture docker builds\n+  - DOCKER_CLI_EXPERIMENTAL=enabled\n+  - CGO_ENABLED=0\n+\n builds:\n   - binary: dive\n     env:\n@@ -20,15 +25,15 @@ builds:\n       - amd64\n       - arm64\n       - ppc64le\n-    ldflags: -s -w -X main.version={{.Version}} -X main.commit={{.Commit}} -X main.buildTime={{.Date}}`.\n+    ldflags: -s -w -X main.version={{.Version}} -X main.commit={{.Commit}} -X main.buildTime={{.Date}}\n \n brews:\n   - repository:\n       owner: wagoodman\n       name: homebrew-dive\n       token: \"{{.Env.TAP_GITHUB_TOKEN}}\"\n-    homepage: \"https://github.com/wagoodman/dive/\"\n-    description: \"A tool for exploring layers in a docker image\"\n+    homepage: &project_url \"https://github.com/wagoodman/dive/\"\n+    description: &description \"A tool for exploring layers in a docker image\"\n \n archives:\n   - format: tar.gz\n@@ -39,51 +44,96 @@ archives:\n nfpms:\n   - license: MIT\n     maintainer: Alex Goodman\n-    homepage: https://github.com/wagoodman/dive/\n-    description: \"A tool for exploring layers in a docker image\"\n+    homepage: *project_url\n+    description: *description\n     formats:\n       - rpm\n       - deb\n \n dockers:\n-  - id: docker-amd64\n+  # docker.io amd64\n+  - &dockerhub_amd64\n+    id: docker-amd64\n     ids:\n       - dive\n     use: buildx\n     goarch: amd64\n     image_templates:\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:latest-amd64'\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:{{ .Version }}-amd64'\n+      - docker.io/wagoodman/dive:v{{.Version}}-amd64\n     build_flag_templates:\n       - \"--build-arg=DOCKER_CLI_VERSION={{.Env.DOCKER_CLI_VERSION}}\"\n+      - \"--platform=linux/amd64\"\n       - \"--label=org.opencontainers.image.created={{.Date}}\"\n       - \"--label=org.opencontainers.image.title={{.ProjectName}}\"\n-      - \"--label=org.opencontainers.image.revision={{.FullCommit}}\"\n-      - \"--label=org.opencontainers.image.version={{.Version}}\"\n+      - \"--label=org.opencontainers.image.description=A tool for exploring layers in a docker image\"\n+      - \"--label=org.opencontainers.image.url={{.GitURL}}\"\n       - \"--label=org.opencontainers.image.source={{.GitURL}}\"\n-      - \"--platform=linux/amd64\"\n-  - id: docker-arm64\n+      - \"--label=org.opencontainers.image.version={{.Version}}\"\n+      - \"--label=org.opencontainers.image.revision={{.FullCommit}}\"\n+      - \"--label=org.opencontainers.image.licenses=MIT\"\n+      - \"--label=org.opencontainers.image.authors=Alex Goodman <@wagoodman>\"\n+\n+  # docker.io arm64\n+  - &dockerhub_arm64\n+    id: docker-arm64\n     ids:\n       - dive\n     use: buildx\n     goarch: arm64\n     image_templates:\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:latest-arm64'\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:{{ .Version }}-arm64'\n+      - docker.io/wagoodman/dive:v{{.Version}}-arm64\n     build_flag_templates:\n       - \"--build-arg=DOCKER_CLI_VERSION={{.Env.DOCKER_CLI_VERSION}}\"\n+      - \"--platform=linux/arm64/v8\"\n       - \"--label=org.opencontainers.image.created={{.Date}}\"\n       - \"--label=org.opencontainers.image.title={{.ProjectName}}\"\n-      - \"--label=org.opencontainers.image.revision={{.FullCommit}}\"\n-      - \"--label=org.opencontainers.image.version={{.Version}}\"\n+      - \"--label=org.opencontainers.image.description=A tool for exploring layers in a docker image\"\n+      - \"--label=org.opencontainers.image.url={{.GitURL}}\"\n       - \"--label=org.opencontainers.image.source={{.GitURL}}\"\n-      - \"--platform=linux/arm64/v8\"\n-docker_manifests:\n-  - name_template: '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:latest'\n+      - \"--label=org.opencontainers.image.version={{.Version}}\"\n+      - \"--label=org.opencontainers.image.revision={{.FullCommit}}\"\n+      - \"--label=org.opencontainers.image.licenses=MIT\"\n+      - \"--label=org.opencontainers.image.authors=Alex Goodman <@wagoodman>\"\n+\n+  # ghcr.io amd64\n+  - id: ghcr-amd64\n+    <<: *dockerhub_amd64\n     image_templates:\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:latest-amd64'\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:latest-arm64'\n-  - name_template: '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:{{ .Version }}'\n+      - ghcr.io/wagoodman/dive:v{{.Version}}-amd64\n+\n+  # ghcr.io arm64\n+  - id: ghcr-arm64\n+    <<: *dockerhub_arm64\n     image_templates:\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:{{ .Version }}-amd64'\n-      - '{{ envOrDefault \"REGISTRY\" \"docker.io\" }}/wagoodman/dive:{{ .Version }}-arm64'\n+      - ghcr.io/wagoodman/dive:v{{.Version}}-arm64\n+\n+docker_manifests:\n+  # docker.io manifests\n+  - name_template: docker.io/wagoodman/dive:latest\n+    image_templates: &dockerhub_images\n+      - docker.io/wagoodman/dive:v{{.Version}}-amd64\n+      - docker.io/wagoodman/dive:v{{.Version}}-arm64\n+\n+  - name_template: docker.io/wagoodman/dive:v{{.Major}}\n+    image_templates: *dockerhub_images\n+\n+  - name_template: docker.io/wagoodman/dive:v{{.Major}}.{{.Minor}}\n+    image_templates: *dockerhub_images\n+\n+  - name_template: docker.io/wagoodman/dive:v{{.Version}}\n+    image_templates: *dockerhub_images\n+\n+  # ghcr.io manifests\n+  - name_template: ghcr.io/wagoodman/dive:latest\n+    image_templates: &ghcr_images\n+      - ghcr.io/wagoodman/dive:v{{.Version}}-amd64\n+      - ghcr.io/wagoodman/dive:v{{.Version}}-arm64\n+\n+  - name_template: ghcr.io/wagoodman/dive:v{{.Major}}\n+    image_templates: *ghcr_images\n+\n+  - name_template: ghcr.io/wagoodman/dive:v{{.Major}}.{{.Minor}}\n+    image_templates: *ghcr_images\n+\n+  - name_template: ghcr.io/wagoodman/dive:v{{.Version}}\n+    image_templates: *ghcr_images\ndiff --git a/Makefile b/Makefile\nindex 69d6f46d..6cbe1678 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -1,7 +1,6 @@\n BIN = dive\n TEMP_DIR = ./.tmp\n PWD := ${CURDIR}\n-REGISTRY ?= docker.io\n SHELL = /bin/bash -o pipefail\n TEST_IMAGE = busybox:latest\n \n@@ -213,7 +212,7 @@ ci-test-docker-image:\n \t\t--rm \\\n \t\t-t \\\n \t\t-v /var/run/docker.sock:/var/run/docker.sock \\\n-\t\t'${REGISTRY}/wagoodman/dive:latest-amd64' \\\n+\t\t'docker.io/wagoodman/dive:latest-amd64' \\\n \t\t\t'${TEST_IMAGE}' \\\n \t\t\t--ci\n \ndiff --git a/README.md b/README.md\nindex 9d839cab..a13c42e4 100644\n--- a/README.md\n+++ b/README.md\n@@ -192,6 +192,8 @@ nix-env -iA nixpkgs.dive\n **Docker**\n ```bash\n docker pull docker.io/wagoodman/dive\n+# or alternatively\n+docker pull ghcr.io/wagoodman/dive\n ```\n \n When running you'll need to include the Docker socket file:\n", "instance_id": "wagoodman__dive-577", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the Docker image `ghcr.io/wagoodman/dive` cannot be pulled due to access denial, suggesting it has been removed or made private. The goal of making the image accessible again is implied, and steps to reproduce the issue are provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly state the desired resolution (e.g., whether to make the image public again, move it to another registry, or provide alternative access). Additionally, there are no mentions of constraints or specific requirements for the solution, such as compatibility with existing workflows or user expectations. While the issue is valid and the context is provided (including environment details), the lack of explicit goals or edge case considerations prevents it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes primarily involve configuration files (`.github/workflows/release.yaml`, `.goreleaser.yaml`, `Makefile`, and `README.md`) rather than core application logic. The modifications are focused on updating Docker image registries (from `ghcr.io` to `docker.io` or supporting both) and adjusting build and release workflows. While the changes span multiple files, they are relatively straightforward and do not impact the system's architecture or require deep understanding of the codebase's interactions. The amount of code change is moderate but mostly involves repetitive updates to image paths and manifest configurations.\n\n2. **Number of Technical Concepts**: Solving this problem requires understanding of Docker image registries, GitHub Actions workflows, and GoReleaser configuration for multi-architecture builds. These concepts are not overly complex for someone with moderate experience in DevOps or CI/CD pipelines. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond familiarity with containerization and release automation tools.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not introduce significant error handling logic. However, there are implicit considerations, such as ensuring compatibility with existing users who pull from either registry and handling potential authentication issues with the new registry setup. These are relatively minor and do not add substantial complexity.\n\n4. **Overall Complexity**: The task involves understanding the intent behind the registry switch and ensuring consistency across configuration files. It requires some logic to interpret the intent (e.g., supporting dual registries) and apply consistent changes, but it does not demand deep architectural changes or advanced technical expertise. The primary challenge lies in ensuring the release pipeline works seamlessly for both `docker.io` and `ghcr.io`, which is a manageable task for someone with basic to intermediate experience in CI/CD and Docker.\n\nThus, a score of 0.35 reflects an \"Easy\" problem that requires moderate effort to understand and modify configuration files but does not pose significant technical challenges or require extensive codebase knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "panic: string field contains invalid UTF-8 [recovered]\n### What happened?\n\nHi,\r\n\r\nSyncthing crashes with \"panic: string field contains invalid UTF-8 [recovered]\" since a few days. I'm running Syncthing on Debian 12 and Linux Mint 22 64 bit PCs and one Rapspberry Pi OS (32 bit), that is used for encrypted storing and broadcasting the synced directories.The crash happens on all machines but the Pi.\r\n\r\nThe setup worked for about 3 years without problem. I used ISO-8859-15 for filename encoding before the crash happened and converted the filenames to UTF-8 now in case that it could cause the problem. But the crash still happened.\r\n\r\nI tried `syncthing -reset-database` and `syncthing --verbose` and enabling debugging in the config.xml, but it didn't help or didn't give me hints how to resolv the issue by myselv.\r\n\r\nBy changing the contents of a synced directory it seems that Syncthing crashes when scanning a subdirectory name that contains an \"\u00fc\".\r\n\r\nWould be great, if it can be fixed or if you could give a hint what changes I should do locally (eg. configuration).\r\n\r\nThank you,\r\n Hauke\n\n### Syncthing version\n\nv1.29.1\n\n### Platform & operating system\n\nDebian 12 AMD64, Linux Mint 22 AMD64\n\n### Browser version\n\n_No response_\n\n### Relevant log output\n\n```shell\n[EEZTP] 2025/01/10 15:03:54 WARNING: Aborted scan due to an unexpected error: context canceled\r\npanic: string field contains invalid UTF-8 [recovered]\r\n[monitor] 2025/01/10 15:03:54 WARNING: Panic detected, writing to \"/home/test/.local/state/syncthing/panic-20250110-150354.log\"\r\n[monitor] 2025/01/10 15:03:54 WARNING: Please check for existing issues with similar panic message at https://github.com/syncthing/syncthing/issues/\r\n[monitor] 2025/01/10 15:03:54 WARNING: If no issue with similar panic message exists, please create a new issue with the panic log attached\r\n[monitor] 2025/01/10 15:03:54 INFO: Syncthing exited: exit status 2\r\n[monitor] 2025/01/10 15:03:55 WARNING: 4 restarts in 7.119028171s; not retrying further\n```\n\n", "patch": "diff --git a/internal/gen/bep/bep.pb.go b/internal/gen/bep/bep.pb.go\nindex 7671339a46d..686fc6c0417 100644\n--- a/internal/gen/bep/bep.pb.go\n+++ b/internal/gen/bep/bep.pb.go\n@@ -877,7 +877,7 @@ type FileInfo struct {\n \tVersion       *Vector       `protobuf:\"bytes,9,opt,name=version,proto3\" json:\"version,omitempty\"`\n \tSequence      int64         `protobuf:\"varint,10,opt,name=sequence,proto3\" json:\"sequence,omitempty\"`\n \tBlocks        []*BlockInfo  `protobuf:\"bytes,16,rep,name=blocks,proto3\" json:\"blocks,omitempty\"`\n-\tSymlinkTarget string        `protobuf:\"bytes,17,opt,name=symlink_target,json=symlinkTarget,proto3\" json:\"symlink_target,omitempty\"`\n+\tSymlinkTarget []byte        `protobuf:\"bytes,17,opt,name=symlink_target,json=symlinkTarget,proto3\" json:\"symlink_target,omitempty\"`\n \tBlocksHash    []byte        `protobuf:\"bytes,18,opt,name=blocks_hash,json=blocksHash,proto3\" json:\"blocks_hash,omitempty\"`\n \tEncrypted     []byte        `protobuf:\"bytes,19,opt,name=encrypted,proto3\" json:\"encrypted,omitempty\"`\n \tType          FileInfoType  `protobuf:\"varint,2,opt,name=type,proto3,enum=bep.FileInfoType\" json:\"type,omitempty\"`\n@@ -983,11 +983,11 @@ func (x *FileInfo) GetBlocks() []*BlockInfo {\n \treturn nil\n }\n \n-func (x *FileInfo) GetSymlinkTarget() string {\n+func (x *FileInfo) GetSymlinkTarget() []byte {\n \tif x != nil {\n \t\treturn x.SymlinkTarget\n \t}\n-\treturn \"\"\n+\treturn nil\n }\n \n func (x *FileInfo) GetBlocksHash() []byte {\n@@ -2047,7 +2047,7 @@ var file_bep_bep_proto_rawDesc = []byte{\n \t0x28, 0x0b, 0x32, 0x0e, 0x2e, 0x62, 0x65, 0x70, 0x2e, 0x42, 0x6c, 0x6f, 0x63, 0x6b, 0x49, 0x6e,\n \t0x66, 0x6f, 0x52, 0x06, 0x62, 0x6c, 0x6f, 0x63, 0x6b, 0x73, 0x12, 0x25, 0x0a, 0x0e, 0x73, 0x79,\n \t0x6d, 0x6c, 0x69, 0x6e, 0x6b, 0x5f, 0x74, 0x61, 0x72, 0x67, 0x65, 0x74, 0x18, 0x11, 0x20, 0x01,\n-\t0x28, 0x09, 0x52, 0x0d, 0x73, 0x79, 0x6d, 0x6c, 0x69, 0x6e, 0x6b, 0x54, 0x61, 0x72, 0x67, 0x65,\n+\t0x28, 0x0c, 0x52, 0x0d, 0x73, 0x79, 0x6d, 0x6c, 0x69, 0x6e, 0x6b, 0x54, 0x61, 0x72, 0x67, 0x65,\n \t0x74, 0x12, 0x1f, 0x0a, 0x0b, 0x62, 0x6c, 0x6f, 0x63, 0x6b, 0x73, 0x5f, 0x68, 0x61, 0x73, 0x68,\n \t0x18, 0x12, 0x20, 0x01, 0x28, 0x0c, 0x52, 0x0a, 0x62, 0x6c, 0x6f, 0x63, 0x6b, 0x73, 0x48, 0x61,\n \t0x73, 0x68, 0x12, 0x1c, 0x0a, 0x09, 0x65, 0x6e, 0x63, 0x72, 0x79, 0x70, 0x74, 0x65, 0x64, 0x18,\ndiff --git a/internal/gen/dbproto/structs.pb.go b/internal/gen/dbproto/structs.pb.go\nindex 3a803347d01..48f588b98e5 100644\n--- a/internal/gen/dbproto/structs.pb.go\n+++ b/internal/gen/dbproto/structs.pb.go\n@@ -34,7 +34,7 @@ type FileInfoTruncated struct {\n \tModifiedBy    uint64            `protobuf:\"varint,12,opt,name=modified_by,json=modifiedBy,proto3\" json:\"modified_by,omitempty\"`\n \tVersion       *bep.Vector       `protobuf:\"bytes,9,opt,name=version,proto3\" json:\"version,omitempty\"`\n \tSequence      int64             `protobuf:\"varint,10,opt,name=sequence,proto3\" json:\"sequence,omitempty\"`\n-\tSymlinkTarget string            `protobuf:\"bytes,17,opt,name=symlink_target,json=symlinkTarget,proto3\" json:\"symlink_target,omitempty\"`\n+\tSymlinkTarget []byte            `protobuf:\"bytes,17,opt,name=symlink_target,json=symlinkTarget,proto3\" json:\"symlink_target,omitempty\"`\n \tBlocksHash    []byte            `protobuf:\"bytes,18,opt,name=blocks_hash,json=blocksHash,proto3\" json:\"blocks_hash,omitempty\"`\n \tEncrypted     []byte            `protobuf:\"bytes,19,opt,name=encrypted,proto3\" json:\"encrypted,omitempty\"`\n \tType          bep.FileInfoType  `protobuf:\"varint,2,opt,name=type,proto3,enum=bep.FileInfoType\" json:\"type,omitempty\"`\n@@ -133,11 +133,11 @@ func (x *FileInfoTruncated) GetSequence() int64 {\n \treturn 0\n }\n \n-func (x *FileInfoTruncated) GetSymlinkTarget() string {\n+func (x *FileInfoTruncated) GetSymlinkTarget() []byte {\n \tif x != nil {\n \t\treturn x.SymlinkTarget\n \t}\n-\treturn \"\"\n+\treturn nil\n }\n \n func (x *FileInfoTruncated) GetBlocksHash() []byte {\n@@ -760,7 +760,7 @@ var file_dbproto_structs_proto_rawDesc = []byte{\n \t0x72, 0x73, 0x69, 0x6f, 0x6e, 0x12, 0x1a, 0x0a, 0x08, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x63,\n \t0x65, 0x18, 0x0a, 0x20, 0x01, 0x28, 0x03, 0x52, 0x08, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x63,\n \t0x65, 0x12, 0x25, 0x0a, 0x0e, 0x73, 0x79, 0x6d, 0x6c, 0x69, 0x6e, 0x6b, 0x5f, 0x74, 0x61, 0x72,\n-\t0x67, 0x65, 0x74, 0x18, 0x11, 0x20, 0x01, 0x28, 0x09, 0x52, 0x0d, 0x73, 0x79, 0x6d, 0x6c, 0x69,\n+\t0x67, 0x65, 0x74, 0x18, 0x11, 0x20, 0x01, 0x28, 0x0c, 0x52, 0x0d, 0x73, 0x79, 0x6d, 0x6c, 0x69,\n \t0x6e, 0x6b, 0x54, 0x61, 0x72, 0x67, 0x65, 0x74, 0x12, 0x1f, 0x0a, 0x0b, 0x62, 0x6c, 0x6f, 0x63,\n \t0x6b, 0x73, 0x5f, 0x68, 0x61, 0x73, 0x68, 0x18, 0x12, 0x20, 0x01, 0x28, 0x0c, 0x52, 0x0a, 0x62,\n \t0x6c, 0x6f, 0x63, 0x6b, 0x73, 0x48, 0x61, 0x73, 0x68, 0x12, 0x1c, 0x0a, 0x09, 0x65, 0x6e, 0x63,\ndiff --git a/lib/model/folder_sendrecv.go b/lib/model/folder_sendrecv.go\nindex ceb74d130d2..73e3ef184d6 100644\n--- a/lib/model/folder_sendrecv.go\n+++ b/lib/model/folder_sendrecv.go\n@@ -743,7 +743,7 @@ func (f *sendReceiveFolder) handleSymlink(file protocol.FileInfo, snap *db.Snaps\n \t\tl.Debugf(\"need symlink\\n\\t%v\\n\\t%v\", file, curFile)\n \t}\n \n-\tif file.SymlinkTarget == \"\" {\n+\tif len(file.SymlinkTarget) == 0 {\n \t\t// Index entry from a Syncthing predating the support for including\n \t\t// the link target in the index entry. We log this as an error.\n \t\tf.newPullError(file.Name, errIncompatibleSymlink)\n@@ -758,7 +758,7 @@ func (f *sendReceiveFolder) handleSymlink(file protocol.FileInfo, snap *db.Snaps\n \t// We declare a function that acts on only the path name, so\n \t// we can pass it to InWritableDir.\n \tcreateLink := func(path string) error {\n-\t\tif err := f.mtimefs.CreateSymlink(file.SymlinkTarget, path); err != nil {\n+\t\tif err := f.mtimefs.CreateSymlink(string(file.SymlinkTarget), path); err != nil {\n \t\t\treturn err\n \t\t}\n \t\treturn f.setPlatformData(&file, path)\ndiff --git a/lib/protocol/bep_fileinfo.go b/lib/protocol/bep_fileinfo.go\nindex d8ed11b5ede..fa5f78d5d1b 100644\n--- a/lib/protocol/bep_fileinfo.go\n+++ b/lib/protocol/bep_fileinfo.go\n@@ -66,7 +66,7 @@ type FileInfo struct {\n \tVersion       Vector\n \tSequence      int64\n \tBlocks        []BlockInfo\n-\tSymlinkTarget string\n+\tSymlinkTarget []byte\n \tBlocksHash    []byte\n \tEncrypted     []byte\n \tPlatform      PlatformData\n@@ -187,7 +187,7 @@ type FileInfoWithoutBlocks interface {\n \tGetVersion() *bep.Vector\n \tGetSequence() int64\n \t// GetBlocks() []*bep.BlockInfo // not included\n-\tGetSymlinkTarget() string\n+\tGetSymlinkTarget() []byte\n \tGetBlocksHash() []byte\n \tGetEncrypted() []byte\n \tGetType() FileInfoType\n@@ -469,7 +469,7 @@ func (f FileInfo) isEquivalent(other FileInfo, comp FileInfoComparison) bool {\n \tcase FileInfoTypeFile:\n \t\treturn f.Size == other.Size && ModTimeEqual(f.ModTime(), other.ModTime(), comp.ModTimeWindow) && (comp.IgnoreBlocks || f.BlocksEqual(other))\n \tcase FileInfoTypeSymlink:\n-\t\treturn f.SymlinkTarget == other.SymlinkTarget\n+\t\treturn bytes.Equal(f.SymlinkTarget, other.SymlinkTarget)\n \tcase FileInfoTypeDirectory:\n \t\treturn true\n \t}\ndiff --git a/lib/scanner/walk.go b/lib/scanner/walk.go\nindex d9d6f6e79cb..e76ce9651fc 100644\n--- a/lib/scanner/walk.go\n+++ b/lib/scanner/walk.go\n@@ -740,7 +740,7 @@ func CreateFileInfo(fi fs.FileInfo, name string, filesystem fs.Filesystem, scanO\n \t\tif err != nil {\n \t\t\treturn protocol.FileInfo{}, err\n \t\t}\n-\t\tf.SymlinkTarget = target\n+\t\tf.SymlinkTarget = []byte(target)\n \t\tf.NoPermissions = true // Symlinks don't have permissions of their own\n \t\treturn f, nil\n \t}\ndiff --git a/proto/bep/bep.proto b/proto/bep/bep.proto\nindex c1dac02be9d..4cac5059700 100644\n--- a/proto/bep/bep.proto\n+++ b/proto/bep/bep.proto\n@@ -101,7 +101,7 @@ message FileInfo {\n   Vector version = 9;\n   int64 sequence = 10;\n   repeated BlockInfo blocks = 16;\n-  string symlink_target = 17;\n+  bytes symlink_target = 17;\n   bytes blocks_hash = 18;\n   bytes encrypted = 19;\n   FileInfoType type = 2;\ndiff --git a/proto/dbproto/structs.proto b/proto/dbproto/structs.proto\nindex bc49b407903..f8ff2eaa416 100644\n--- a/proto/dbproto/structs.proto\n+++ b/proto/dbproto/structs.proto\n@@ -15,7 +15,7 @@ message FileInfoTruncated {\n   bep.Vector version = 9;\n   int64 sequence = 10;\n   reserved 16; // blocks\n-  string symlink_target = 17;\n+  bytes symlink_target = 17;\n   bytes blocks_hash = 18;\n   bytes encrypted = 19;\n   bep.FileInfoType type = 2;\n", "instance_id": "syncthing__syncthing-9914", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: Syncthing crashes with a \"panic: string field contains invalid UTF-8\" error on certain systems when scanning directories with specific characters (e.g., \"\u00fc\"). The user provides relevant context, including the operating systems affected, Syncthing version, and steps taken to troubleshoot (e.g., converting filenames to UTF-8, resetting the database). The log output is also included, which helps in identifying the nature of the crash. However, there are minor ambiguities: the problem statement does not explicitly confirm whether the issue is tied to symlink handling (though the code changes suggest this), and it lacks detailed reproduction steps or specific file structures that trigger the crash. Additionally, edge cases or specific filename encodings causing the issue are not fully detailed. Overall, the statement is valid and mostly clear but misses some minor specifics that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes spans multiple files (e.g., protocol buffer definitions, generated code, and application logic in Go), requiring a good understanding of how these components interact within the Syncthing codebase. The changes involve modifying the data type of `SymlinkTarget` from `string` to `[]byte` to handle potential invalid UTF-8 data, which indicates a need to understand Go's handling of strings and byte slices, as well as protocol buffer serialization/deserialization. The technical concepts involved include UTF-8 encoding issues, filesystem interactions (symlink creation), and protocol buffer mechanics, which are moderately complex but not overly advanced. The changes also require careful consideration of backward compatibility and potential edge cases, such as how existing data with invalid UTF-8 is handled or how different filesystems represent symlink targets. However, the impact on the system's architecture is limited, as this is more of a bug fix than a major refactoring, and the amount of code change is relatively small. The problem does not require deep domain-specific knowledge beyond filesystem and encoding issues, nor does it involve complex algorithms or performance optimizations. Therefore, a score of 0.55 reflects a medium difficulty level, requiring a solid understanding of multiple concepts and careful handling of edge cases across several files, but not reaching the level of a hard or very hard problem.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Lower the log level of \"already connected to this device\" to DEBUG (is: INFO)\n### Feature description\n\nLower the logging level for `XXX already connected to this device` messages in the log from `INFO` to `DEBUG`.\n\n### Problem or use case\n\nThere are many lines in the logs coming from an expected race condition when two devices connect. See for instance https://forum.syncthing.net/t/connected-to-already-connected-device/13190 for an example of a related question.\r\n\r\nSince they are expected and do not indicate an error they are not useful to see in a normal, production log. The sheer number of these messages makes it difficult to find actually useful ones.\n\n### Alternatives or workarounds\n\nFilter out the messages if the logging interface allows it - this will not help with `syncthing*.log` ones for instance on Windows.\n", "patch": "diff --git a/lib/connections/service.go b/lib/connections/service.go\nindex 5b69f2f99a6..0a09f6c7ced 100644\n--- a/lib/connections/service.go\n+++ b/lib/connections/service.go\n@@ -283,7 +283,11 @@ func (s *service) handleConns(ctx context.Context) error {\n \t\t}\n \n \t\tif err := s.connectionCheckEarly(remoteID, c); err != nil {\n-\t\t\tl.Infof(\"Connection from %s at %s (%s) rejected: %v\", remoteID, c.RemoteAddr(), c.Type(), err)\n+\t\t\tif errors.Is(err, errDeviceAlreadyConnected) {\n+\t\t\t\tl.Debugf(\"Connection from %s at %s (%s) rejected: %v\", remoteID, c.RemoteAddr(), c.Type(), err)\n+\t\t\t} else {\n+\t\t\t\tl.Infof(\"Connection from %s at %s (%s) rejected: %v\", remoteID, c.RemoteAddr(), c.Type(), err)\n+\t\t\t}\n \t\t\tc.Close()\n \t\t\tcontinue\n \t\t}\n", "instance_id": "syncthing__syncthing-9722", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to lower the logging level of a specific message from INFO to DEBUG to reduce log noise in production environments. The goal is well-defined, and the use case is explained with a reference to a forum discussion for context, which helps in understanding the motivation behind the change. However, there are minor ambiguities or missing details. For instance, the problem statement does not explicitly mention the specific file or module where the change is needed, nor does it discuss potential side effects of changing the log level (e.g., impact on debugging or monitoring tools that rely on INFO-level logs). Additionally, while the context of the race condition is mentioned, it lacks specifics on how often this occurs or under what conditions, which could be relevant for assessing the broader impact. Overall, the statement is valid and clear but misses some minor details that could enhance comprehension.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward and localized code modification. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes**: The change is confined to a single file (`service.go`) and involves a small, targeted modification\u2014adding a conditional check to switch the log level from `l.Infof` to `l.Debugf` for a specific error condition (`errDeviceAlreadyConnected`). The diff shows only a few lines of code changed, with no impact on the broader system architecture or interactions between modules.\n\n2. **Number of Technical Concepts**: The required knowledge is minimal. The developer needs to understand basic Go syntax, conditional logic, and the logging mechanism used in the codebase (likely a custom or standard logging library with levels like INFO and DEBUG). No advanced language features, algorithms, design patterns, or domain-specific knowledge are required beyond basic error handling and logging concepts.\n\n3. **Edge Cases and Error Handling**: The problem statement does not mention specific edge cases, and the code change itself does not introduce new error handling logic beyond distinguishing one specific error type. The modification is simple and does not appear to risk introducing new issues or require complex validation.\n\n4. **Overall Complexity**: This is a trivial task that a junior developer with basic familiarity in Go could handle. It requires minimal understanding of the codebase beyond the immediate context of the logging statement and does not involve performance considerations, architectural changes, or deep debugging.\n\nGiven these points, a difficulty score of 0.15 reflects the very easy nature of the task, requiring only a basic code modification with no significant challenges or risks.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Private CA Resolver and Cloudflare DNS cannot co-exist\n### Welcome!\r\n\r\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\r\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\r\n\r\n### What did you do?\r\n\r\nAdd Cloudflare DNS resolver after running private step CA HTTP resolver\r\n\r\nIf I comment out LEGO_CA_CERTIFICATES and LEGO_CA_SERVER_NAME env vars then Cloudflare DNS works, but step CA can no longer issue certs\r\n\r\n### What did you see instead?\r\n\r\n```\r\ntime=\"2024-04-07T19:34:34Z\" level=error msg=\"Unable to obtain ACME certificate for domains \\\"jellyfin.real-address.com,jf.real-address.com\\\": cannot get ACME client get directory at 'https://acme-v02.api.letsencrypt.org/directory': Get \\\"https://acme-v02.api.letsencrypt.org/directory\\\": tls: failed to verify certificate: x509: certificate is valid for acme-v02-1.api.letsencrypt.org, acme-v02-2.api.letsencrypt.org, acme-v02-3.api.letsencrypt.org, acme-v02-4.api.letsencrypt.org, acme-v02-5.api.letsencrypt.org, acme-v02.api.letsencrypt.org, incident.letsencrypt.org, not step-ca\" ACME CA=\"https://acme-v02.api.letsencrypt.org/directory\" rule=\"Host(`jellyfin.real-address.com`, `jf.real-address.com`)\" routerName=jellyfin-real-address-https@docker providerName=cloudflare.acme\r\n```\r\n\r\n### What version of Traefik are you using?\r\n\r\nv2.11.0\r\n\r\n### What is your environment & configuration?\r\n\r\n```yaml\r\n## Jellyfin Labels\r\n\r\nx-nio_io_labels: &nio_io_labels\r\n  ## HTTPS Service\r\n  traefik.http.routers.jellyfin-https.service: jellyfin-https\r\n  traefik.http.routers.jellyfin-https.rule: >-\r\n    Host(`jellyfin-${DEVICE_IP:-ip_addr}.nip.io`, `jf-${DEVICE_IP:-ip_addr}.nip.io`, `jellyfin.nip.io`, `jf.nip.io`)\r\n  traefik.http.routers.jellyfin-https.entrypoints: websecure\r\n  traefik.http.routers.jellyfin-https.middlewares: jellyfin-mw\r\n  traefik.http.routers.jellyfin-https.tls: true\r\n  traefik.http.routers.jellyfin-https.tls.certresolver: step-ca\r\n  traefik.http.services.jellyfin-https.loadbalancer.server.port: 8096\r\n  # HTTP Service Allowed!\r\n  traefik.http.routers.jellyfin-http.service: jellyfin-http\r\n  traefik.http.routers.jellyfin-http.rule: >-\r\n    Host(`jellyfin-${DEVICE_IP:-ip_addr}.nip.io`, `jf-${DEVICE_IP:-ip_addr}.nip.io`, `jellyfin.nip.io`, `jf.nip.io`)\r\n  traefik.http.routers.jellyfin-http.entrypoints: web\r\n  traefik.http.routers.jellyfin-http.middlewares: jellyfin-mw\r\n  traefik.http.services.jellyfin-http.loadbalancer.server.port: 8096\r\n\r\n## UDP\r\n\r\nx-udp_labels: &udp_labels\r\n  traefik.udp.routers.jellyfin-client-discovery.entrypoints: client_discovery\r\n  traefik.udp.routers.jellyfin-client-discovery.service: jellyfin-client-discovery\r\n  traefik.udp.services.jellyfin-client-discovery.loadbalancer.server.port: 7359\r\n\r\n## Recommended by Jellyfin Docs\r\n\r\nx-recommended_middleware_labels: &recommended_middleware_labels\r\n  traefik.http.middlewares.jellyfin-mw.headers.customResponseHeaders.X-Robots-Tag: noindex,nofollow,nosnippet,noarchive,notranslate,noimageindex\r\n  traefik.http.middlewares.jellyfin-mw.headers.frameDeny: 'true'\r\n  traefik.http.middlewares.jellyfin-mw.headers.contentTypeNosniff: 'true'\r\n  traefik.http.middlewares.jellyfin-mw.headers.browserXSSFilter: 'true'\r\n  traefik.http.services.jellyfin.loadBalancer.passHostHeader: 'true'\r\n\r\nx-real_address_labels: &real_address_labels\r\n  # HTTPS Service\r\n  traefik.http.routers.jellyfin-real-address-https.service: jellyfin-real-address-https\r\n  traefik.http.routers.jellyfin-real-address-https.rule: Host(`jellyfin.real-address.com`, `jf.real-address.com`)\r\n  traefik.http.routers.jellyfin-real-address-https.entrypoints: websecure\r\n  traefik.http.routers.jellyfin-real-address-https.middlewares: jellyfin-mw\r\n  traefik.http.routers.jellyfin-real-address-https.tls: true\r\n  traefik.http.routers.jellyfin-real-address-https.tls.certresolver: cloudflare\r\n  traefik.http.services.jellyfin-real-address-https.loadbalancer.server.port: 8096\r\n  # HTTP to HTTPS Redirect\r\n  traefik.http.routers.jellyfin-real-address-http.rule: Host(`jellyfin.real-address.com`, `jf.real-address.com`)\r\n  traefik.http.routers.jellyfin-real-address-http.entrypoints: web\r\n  traefik.http.routers.jellyfin-real-address-http.middlewares: https_redirect\r\n```\r\n\r\nTraefik ENV Vars\r\n```yaml\r\nservices:\r\n  traefik:\r\n    image: traefik:latest\r\n    container_name: traefik\r\n    environment:\r\n      # LEGO_CA_CERTIFICATES: /shared_files/ca_certs/root_ca.crt\r\n      # LEGO_CA_SERVER_NAME: step-ca\r\n      SSL_CERT_FILE: /shared_files/ca_certs/root_ca.crt\r\n      CLOUDFLARE_DNS_API_TOKEN: $CLOUDFLARE_DNS_API_TOKEN\r\n```\r\nAdd more configuration information here.\r\n```yaml\r\n# Static Conf\r\n# Configure your CertificateResolver here...\r\n# ---\r\ncertificatesResolvers:\r\n  step-ca:\r\n    acme:\r\n      email: redacted@gmail.com\r\n      storage: /shared_files/acme/acme.json\r\n      caServer: \"https://192.168.1.66:55888/acme/acme/directory\"\r\n      # caServer: \"https://step-ca:55888/acme/acme/directory\"\r\n      httpChallenge:\r\n        entryPoint: web\r\n  cloudflare:\r\n    acme:\r\n      email: redacted@gmail.com\r\n      storage: /shared_files/acme/cloudflare_dns_acme.json\r\n      caServer: \"https://acme-v02.api.letsencrypt.org/directory\"\r\n      dnsChallenge:\r\n        provider: cloudflare\r\n        delayBeforeCheck: 10\r\n```\r\n\r\n### If applicable, please paste the log output in DEBUG level\r\n\r\n_No response_\n", "patch": "diff --git a/docs/content/https/acme.md b/docs/content/https/acme.md\nindex b73e6cc2f2..daaaa21000 100644\n--- a/docs/content/https/acme.md\n+++ b/docs/content/https/acme.md\n@@ -709,6 +709,109 @@ certificatesResolvers:\n # ...\n ```\n \n+### `caCertificates`\n+\n+_Optional, Default=[]_\n+\n+The `caCertificates` option specifies the paths to PEM encoded CA Certificates that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\n+\n+```yaml tab=\"File (YAML)\"\n+certificatesResolvers:\n+  myresolver:\n+    acme:\n+      # ...\n+      caCertificates:\n+        - path/certificates1.pem\n+        - path/certificates2.pem\n+      # ...\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[certificatesResolvers.myresolver.acme]\n+  # ...\n+  caCertificates = [ \"path/certificates1.pem\", \"path/certificates2.pem\" ]\n+  # ...\n+```\n+\n+```bash tab=\"CLI\"\n+# ...\n+--certificatesresolvers.myresolver.acme.caCertificates=\"path/certificates1.pem,path/certificates2.pem\"\n+# ...\n+```\n+\n+??? note \"LEGO Environment Variable\"\n+\n+    It can be defined globally by using the environment variable `LEGO_CA_CERTIFICATES`.\n+    This environment variable is neither a fallback nor an override of the configuration option.\n+\n+### `caSystemCertPool`\n+\n+_Optional, Default=false_\n+\n+The `caSystemCertPool` option defines if the certificates pool must use a copy of the system cert pool.\n+\n+```yaml tab=\"File (YAML)\"\n+certificatesResolvers:\n+  myresolver:\n+    acme:\n+      # ...\n+      caSystemCertPool: true\n+      # ...\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[certificatesResolvers.myresolver.acme]\n+  # ...\n+  caSystemCertPool = true\n+  # ...\n+```\n+\n+```bash tab=\"CLI\"\n+# ...\n+--certificatesresolvers.myresolver.acme.caSystemCertPool=true\n+# ...\n+```\n+\n+??? note \"LEGO Environment Variable\"\n+\n+    It can be defined globally by using the environment variable `LEGO_CA_SYSTEM_CERT_POOL`.\n+    `LEGO_CA_SYSTEM_CERT_POOL` is ignored if `LEGO_CA_CERTIFICATES` is not set or empty.\n+    This environment variable is neither a fallback nor an override of the configuration option.\n+\n+### `caServerName`\n+\n+_Optional, Default=\"\"_\n+\n+The `caServerName` option specifies the CA server name that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\n+\n+```yaml tab=\"File (YAML)\"\n+certificatesResolvers:\n+  myresolver:\n+    acme:\n+      # ...\n+      caServerName: \"my-server\"\n+      # ...\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[certificatesResolvers.myresolver.acme]\n+  # ...\n+  caServerName = \"my-server\"\n+  # ...\n+```\n+\n+```bash tab=\"CLI\"\n+# ...\n+--certificatesresolvers.myresolver.acme.caServerName=\"my-server\"\n+# ...\n+```\n+\n+??? note \"LEGO Environment Variable\"\n+\n+    It can be defined globally by using the environment variable `LEGO_CA_SERVER_NAME`.\n+    `LEGO_CA_SERVER_NAME` is ignored if `LEGO_CA_CERTIFICATES` is not set or empty.\n+    This environment variable is neither a fallback nor an override of the configuration option.\n+\n ## Fallback\n \n If Let's Encrypt is not reachable, the following certificates will apply:\ndiff --git a/docs/content/reference/static-configuration/cli-ref.md b/docs/content/reference/static-configuration/cli-ref.md\nindex a2171dfa8d..b0c02c5653 100644\n--- a/docs/content/reference/static-configuration/cli-ref.md\n+++ b/docs/content/reference/static-configuration/cli-ref.md\n@@ -57,9 +57,18 @@ Activate API directly on the entryPoint named traefik. (Default: ```false```)\n `--certificatesresolvers.<name>`:  \n Certificates resolvers configuration. (Default: ```false```)\n \n+`--certificatesresolvers.<name>.acme.cacertificates`:  \n+Specify the paths to PEM encoded CA Certificates that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\n+\n `--certificatesresolvers.<name>.acme.caserver`:  \n CA server to use. (Default: ```https://acme-v02.api.letsencrypt.org/directory```)\n \n+`--certificatesresolvers.<name>.acme.caservername`:  \n+Specify the CA server name that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\n+\n+`--certificatesresolvers.<name>.acme.casystemcertpool`:  \n+Define if the certificates pool must use a copy of the system cert pool. (Default: ```false```)\n+\n `--certificatesresolvers.<name>.acme.certificatesduration`:  \n Certificates' duration in hours. (Default: ```2160```)\n \ndiff --git a/docs/content/reference/static-configuration/env-ref.md b/docs/content/reference/static-configuration/env-ref.md\nindex b7cd5ee3c5..2f4c19971a 100644\n--- a/docs/content/reference/static-configuration/env-ref.md\n+++ b/docs/content/reference/static-configuration/env-ref.md\n@@ -57,9 +57,18 @@ Activate API directly on the entryPoint named traefik. (Default: ```false```)\n `TRAEFIK_CERTIFICATESRESOLVERS_<NAME>`:  \n Certificates resolvers configuration. (Default: ```false```)\n \n+`TRAEFIK_CERTIFICATESRESOLVERS_<NAME>_ACME_CACERTIFICATES`:  \n+Specify the paths to PEM encoded CA Certificates that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\n+\n `TRAEFIK_CERTIFICATESRESOLVERS_<NAME>_ACME_CASERVER`:  \n CA server to use. (Default: ```https://acme-v02.api.letsencrypt.org/directory```)\n \n+`TRAEFIK_CERTIFICATESRESOLVERS_<NAME>_ACME_CASERVERNAME`:  \n+Specify the CA server name that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\n+\n+`TRAEFIK_CERTIFICATESRESOLVERS_<NAME>_ACME_CASYSTEMCERTPOOL`:  \n+Define if the certificates pool must use a copy of the system cert pool. (Default: ```false```)\n+\n `TRAEFIK_CERTIFICATESRESOLVERS_<NAME>_ACME_CERTIFICATESDURATION`:  \n Certificates' duration in hours. (Default: ```2160```)\n \ndiff --git a/docs/content/reference/static-configuration/file.toml b/docs/content/reference/static-configuration/file.toml\nindex daccb6dee1..f92cd37631 100644\n--- a/docs/content/reference/static-configuration/file.toml\n+++ b/docs/content/reference/static-configuration/file.toml\n@@ -441,6 +441,9 @@\n       storage = \"foobar\"\n       keyType = \"foobar\"\n       certificatesDuration = 42\n+      caCertificates = [\"foobar\", \"foobar\"]\n+      caSystemCertPool = true\n+      caServerName = \"foobar\"\n       [certificatesResolvers.CertificateResolver0.acme.eab]\n         kid = \"foobar\"\n         hmacEncoded = \"foobar\"\n@@ -461,6 +464,9 @@\n       storage = \"foobar\"\n       keyType = \"foobar\"\n       certificatesDuration = 42\n+      caCertificates = [\"foobar\", \"foobar\"]\n+      caSystemCertPool = true\n+      caServerName = \"foobar\"\n       [certificatesResolvers.CertificateResolver1.acme.eab]\n         kid = \"foobar\"\n         hmacEncoded = \"foobar\"\ndiff --git a/docs/content/reference/static-configuration/file.yaml b/docs/content/reference/static-configuration/file.yaml\nindex 2919824c3a..12b5efccc0 100644\n--- a/docs/content/reference/static-configuration/file.yaml\n+++ b/docs/content/reference/static-configuration/file.yaml\n@@ -483,6 +483,11 @@ certificatesResolvers:\n         kid: foobar\n         hmacEncoded: foobar\n       certificatesDuration: 42\n+      caCertificates:\n+        - foobar\n+        - foobar\n+      caSystemCertPool: true\n+      caServerName: foobar\n       dnsChallenge:\n         provider: foobar\n         delayBeforeCheck: 42s\n@@ -505,6 +510,11 @@ certificatesResolvers:\n         kid: foobar\n         hmacEncoded: foobar\n       certificatesDuration: 42\n+      caCertificates:\n+        - foobar\n+        - foobar\n+      caSystemCertPool: true\n+      caServerName: foobar\n       dnsChallenge:\n         provider: foobar\n         delayBeforeCheck: 42s\ndiff --git a/pkg/provider/acme/provider.go b/pkg/provider/acme/provider.go\nindex 7df870c595..912b4073eb 100644\n--- a/pkg/provider/acme/provider.go\n+++ b/pkg/provider/acme/provider.go\n@@ -6,9 +6,13 @@ import (\n \t\"crypto/x509\"\n \t\"errors\"\n \t\"fmt\"\n+\t\"net\"\n+\t\"net/http\"\n \t\"net/url\"\n+\t\"os\"\n \t\"reflect\"\n \t\"sort\"\n+\t\"strconv\"\n \t\"strings\"\n \t\"sync\"\n \t\"time\"\n@@ -43,6 +47,10 @@ type Configuration struct {\n \tEAB                  *EAB   `description:\"External Account Binding to use.\" json:\"eab,omitempty\" toml:\"eab,omitempty\" yaml:\"eab,omitempty\"`\n \tCertificatesDuration int    `description:\"Certificates' duration in hours.\" json:\"certificatesDuration,omitempty\" toml:\"certificatesDuration,omitempty\" yaml:\"certificatesDuration,omitempty\" export:\"true\"`\n \n+\tCACertificates   []string `description:\"Specify the paths to PEM encoded CA Certificates that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\" json:\"caCertificates,omitempty\" toml:\"caCertificates,omitempty\" yaml:\"caCertificates,omitempty\"`\n+\tCASystemCertPool bool     `description:\"Define if the certificates pool must use a copy of the system cert pool.\" json:\"caSystemCertPool,omitempty\" toml:\"caSystemCertPool,omitempty\" yaml:\"caSystemCertPool,omitempty\" export:\"true\"`\n+\tCAServerName     string   `description:\"Specify the CA server name that can be used to authenticate an ACME server with an HTTPS certificate not issued by a CA in the system-wide trusted root list.\" json:\"caServerName,omitempty\" toml:\"caServerName,omitempty\" yaml:\"caServerName,omitempty\" export:\"true\"`\n+\n \tDNSChallenge  *DNSChallenge  `description:\"Activate DNS-01 Challenge.\" json:\"dnsChallenge,omitempty\" toml:\"dnsChallenge,omitempty\" yaml:\"dnsChallenge,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n \tHTTPChallenge *HTTPChallenge `description:\"Activate HTTP-01 Challenge.\" json:\"httpChallenge,omitempty\" toml:\"httpChallenge,omitempty\" yaml:\"httpChallenge,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n \tTLSChallenge  *TLSChallenge  `description:\"Activate TLS-ALPN-01 Challenge.\" json:\"tlsChallenge,omitempty\" toml:\"tlsChallenge,omitempty\" yaml:\"tlsChallenge,omitempty\" label:\"allowEmpty\" file:\"allowEmpty\" export:\"true\"`\n@@ -261,6 +269,11 @@ func (p *Provider) getClient() (*lego.Client, error) {\n \tconfig.Certificate.KeyType = GetKeyType(ctx, p.KeyType)\n \tconfig.UserAgent = fmt.Sprintf(\"containous-traefik/%s\", version.Version)\n \n+\tconfig.HTTPClient, err = p.createHTTPClient()\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"creating HTTP client: %w\", err)\n+\t}\n+\n \tclient, err := lego.NewClient(config)\n \tif err != nil {\n \t\treturn nil, err\n@@ -340,6 +353,64 @@ func (p *Provider) getClient() (*lego.Client, error) {\n \treturn p.client, nil\n }\n \n+func (p *Provider) createHTTPClient() (*http.Client, error) {\n+\ttlsConfig, err := p.createClientTLSConfig()\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"creating client TLS config: %w\", err)\n+\t}\n+\n+\treturn &http.Client{\n+\t\tTimeout: 2 * time.Minute,\n+\t\tTransport: &http.Transport{\n+\t\t\tProxy: http.ProxyFromEnvironment,\n+\t\t\tDialContext: (&net.Dialer{\n+\t\t\t\tTimeout:   30 * time.Second,\n+\t\t\t\tKeepAlive: 30 * time.Second,\n+\t\t\t}).DialContext,\n+\t\t\tTLSHandshakeTimeout:   30 * time.Second,\n+\t\t\tResponseHeaderTimeout: 30 * time.Second,\n+\t\t\tTLSClientConfig:       tlsConfig,\n+\t\t},\n+\t}, nil\n+}\n+\n+func (p *Provider) createClientTLSConfig() (*tls.Config, error) {\n+\tif len(p.CACertificates) > 0 || p.CAServerName != \"\" {\n+\t\tcertPool, err := lego.CreateCertPool(p.CACertificates, p.CASystemCertPool)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"creating cert pool with custom certificates: %w\", err)\n+\t\t}\n+\n+\t\treturn &tls.Config{\n+\t\t\tServerName: p.CAServerName,\n+\t\t\tRootCAs:    certPool,\n+\t\t}, nil\n+\t}\n+\n+\t// Compatibility layer with the lego.\n+\t// https://github.com/go-acme/lego/blob/834a9089f143e3407b3f5c8b93a0e285ba231fe2/lego/client_config.go#L24-L34\n+\t// https://github.com/go-acme/lego/blob/834a9089f143e3407b3f5c8b93a0e285ba231fe2/lego/client_config.go#L97-L113\n+\n+\tserverName := os.Getenv(\"LEGO_CA_SERVER_NAME\")\n+\tcustomCACertsPath := os.Getenv(\"LEGO_CA_CERTIFICATES\")\n+\n+\tif customCACertsPath == \"\" && serverName == \"\" {\n+\t\treturn nil, nil\n+\t}\n+\n+\tuseSystemCertPool, _ := strconv.ParseBool(os.Getenv(\"LEGO_CA_SYSTEM_CERT_POOL\"))\n+\n+\tcertPool, err := lego.CreateCertPool(strings.Split(customCACertsPath, string(os.PathListSeparator)), useSystemCertPool)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"creating cert pool: %w\", err)\n+\t}\n+\n+\treturn &tls.Config{\n+\t\tServerName: serverName,\n+\t\tRootCAs:    certPool,\n+\t}, nil\n+}\n+\n func (p *Provider) initAccount(ctx context.Context) (*Account, error) {\n \tif p.account == nil || len(p.account.Email) == 0 {\n \t\tvar err error\n@@ -424,8 +495,7 @@ func (p *Provider) watchNewDomains(ctx context.Context) {\n \n \t\t\t\t\t\tif len(route.TLS.Domains) > 0 {\n \t\t\t\t\t\t\tdomains := deleteUnnecessaryDomains(ctxRouter, route.TLS.Domains)\n-\t\t\t\t\t\t\tfor i := range len(domains) {\n-\t\t\t\t\t\t\t\tdomain := domains[i]\n+\t\t\t\t\t\t\tfor _, domain := range domains {\n \t\t\t\t\t\t\t\tsafe.Go(func() {\n \t\t\t\t\t\t\t\t\tdom, cert, err := p.resolveCertificate(ctx, domain, traefiktls.DefaultTLSStoreName)\n \t\t\t\t\t\t\t\t\tif err != nil {\n@@ -461,8 +531,7 @@ func (p *Provider) watchNewDomains(ctx context.Context) {\n \n \t\t\t\t\t\tif len(route.TLS.Domains) > 0 {\n \t\t\t\t\t\t\tdomains := deleteUnnecessaryDomains(ctxRouter, route.TLS.Domains)\n-\t\t\t\t\t\t\tfor i := range len(domains) {\n-\t\t\t\t\t\t\t\tdomain := domains[i]\n+\t\t\t\t\t\t\tfor _, domain := range domains {\n \t\t\t\t\t\t\t\tsafe.Go(func() {\n \t\t\t\t\t\t\t\t\tdom, cert, err := p.resolveCertificate(ctx, domain, traefiktls.DefaultTLSStoreName)\n \t\t\t\t\t\t\t\t\tif err != nil {\n", "instance_id": "traefik__traefik-10816", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the inability of Private CA Resolver (step-ca) and Cloudflare DNS to coexist in a Traefik setup due to certificate validation issues. The goal is evident\u2014resolving the conflict between using a private CA and a public ACME server like Let's Encrypt with Cloudflare DNS. The input (configuration files and environment variables) and output (error logs) are provided, along with the context of the Traefik version and environment setup. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or constraints when both resolvers are active, nor does it mention specific edge cases or scenarios (e.g., what happens with mixed domains or certificate renewals). Additionally, while the error log points to a certificate validation issue, the exact root cause or desired resolution (e.g., whether to prioritize one resolver or allow both to function simultaneously) is not fully articulated. Thus, while the problem is valid and mostly clear, it lacks some critical details and examples that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is moderate but impactful, as seen in the diff, which involves updates to multiple documentation files and a core provider module (`pkg/provider/acme/provider.go`) in Traefik. The changes introduce new configuration options (`caCertificates`, `caSystemCertPool`, `caServerName`) to handle custom CA certificates for ACME servers, requiring modifications to the TLS configuration and HTTP client setup. This indicates a need to understand and modify the interaction between Traefik's certificate resolution logic and the underlying `lego` library for ACME operations, which is non-trivial.\n\nSecond, the number of technical concepts involved is significant. Solving this requires knowledge of TLS certificate validation, x509 certificate pools, HTTP client configuration in Go, and the specifics of ACME protocols (e.g., how CA servers are authenticated). Additionally, familiarity with Traefik's architecture, particularly its certificate resolver mechanism, and the `lego` library's behavior with custom CA certificates is necessary. The code changes also involve environment variable handling and compatibility layers, adding to the complexity.\n\nThird, while the problem statement does not explicitly mention edge cases, the nature of the issue (certificate validation across different CA servers) implies potential challenges such as handling invalid or expired CA certificates, misconfigured server names, or conflicts between system and custom certificate pools. The code changes address some of these by introducing configurable options, but implementing and testing these for robustness would require careful consideration of error handling and edge scenarios.\n\nFinally, the impact on the system's architecture is moderate but significant, as it affects how Traefik interacts with ACME servers for certificate issuance\u2014a core functionality. While not a complete refactoring, the changes touch critical components and require a deep understanding of the codebase. Given these factors, a difficulty score of 0.65 reflects the hard nature of the problem, requiring substantial technical expertise and careful implementation, though it does not reach the \"Very Hard\" level as it does not involve system-level redesign or highly intricate domain-specific challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "with mincache enabled `HEAD` request returns different `Content-Length` then matching `GET` on a KMS setup\nI have enabled server-side encryption on my bucket and uploaded `Audi cabriolet 01.jpg` of 2,971,514 bytes to the `test` bucket. When I try to copy the file using `mc`, then I got the following error: `Input reader closed pre-maturely. Expected '2972986' bytes, but only received '2971514' bytes.`\r\n\r\nI ran the `mc` command using the `--debug` flag and noticed that the `HEAD` request returns a different length then the `GET` request. This is the `HEAD` request:\r\n```\r\nHEAD /test/audi/Audi%20cabriolet%2001.jpg HTTP/1.1\r\nHost: tenant-rdk-hl.tenant-rdk:9000\r\nUser-Agent: MinIO (linux; amd64) minio-go/v7.0.70 __debug_bin2996553503/DEVELOPMENT.GOGET\r\nAuthorization: AWS4-HMAC-SHA256 Credential=l5m7mQiwUi2GCSH3/20240612/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=**REDACTED**\r\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\r\nX-Amz-Date: 20240612T160723Z\r\n```\r\n`HEAD` response:\r\n```\r\nHTTP/1.1 200 OK\r\nContent-Length: 2972986\r\nAccept-Ranges: bytes\r\nContent-Type: image/jpeg\r\nDate: Wed, 12 Jun 2024 16:07:23 GMT\r\nEtag: \"efd738945e94f4dbb8588b788b2ea7c1\"\r\nLast-Modified: Wed, 12 Jun 2024 15:21:24 GMT\r\nServer: MinIO\r\nStrict-Transport-Security: max-age=31536000; includeSubDomains\r\nVary: Origin\r\nVary: Accept-Encoding\r\nX-Amz-Id-2: d5a265f7265c149d31b79b2568737f9e0d97aab546838b9090d7403265887733\r\nX-Amz-Request-Id: 17D84D79C009F04A\r\nX-Content-Type-Options: nosniff\r\nX-Xss-Protection: 1; mode=block\r\n```\r\nThen it performs the actual `GET` and that request looks like:\r\n```\r\nGET /test/audi/Audi%20cabriolet%2001.jpg HTTP/1.1\r\nHost: tenant-rdk-hl.tenant-rdk:9000\r\nUser-Agent: MinIO (linux; amd64) minio-go/v7.0.70 __debug_bin2996553503/DEVELOPMENT.GOGET\r\nAccept-Encoding: identity\r\nAuthorization: AWS4-HMAC-SHA256 Credential=l5m7mQiwUi2GCSH3/20240612/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=**REDACTED**\r\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\r\nX-Amz-Date: 20240612T160904Z\r\n```\r\nAnd the response is:\r\n```\r\nHTTP/1.1 200 OK\r\nContent-Length: 2971514\r\nAccept-Ranges: bytes\r\nContent-Type: image/jpeg\r\nDate: Wed, 12 Jun 2024 16:09:04 GMT\r\nEtag: \"efd738945e94f4dbb8588b788b2ea7c1\"\r\nLast-Modified: Wed, 12 Jun 2024 15:21:24 GMT\r\nServer: MinIO\r\nStrict-Transport-Security: max-age=31536000; includeSubDomains\r\nVary: Origin\r\nVary: Accept-Encoding\r\nX-Amz-Id-2: d5a265f7265c149d31b79b2568737f9e0d97aab546838b9090d7403265887733\r\nX-Amz-Request-Id: 17D84D916E7CC820\r\nX-Amz-Server-Side-Encryption: aws:kms\r\nX-Amz-Server-Side-Encryption-Aws-Kms-Key-Id: arn:aws:kms:tenant-rdk-key\r\nX-Content-Type-Options: nosniff\r\nX-Xss-Protection: 1; mode=block\r\n```\r\nThis seems to be a bug in Minio, because both the `HEAD` and `GET` requests should return the same content length. I never have seen this issue without encryption.\r\n\n", "patch": "diff --git a/cmd/object-handlers.go b/cmd/object-handlers.go\nindex 4f6b5143ffe33..e0100249c1ccb 100644\n--- a/cmd/object-handlers.go\n+++ b/cmd/object-handlers.go\n@@ -959,7 +959,6 @@ func (api objectAPIHandlers) headObjectHandler(ctx context.Context, objectAPI Ob\n \t\t// No need to check cache for encrypted objects.\n \t\tcachedResult = false\n \t}\n-\tvar update bool\n \tif cachedResult {\n \t\trc := &cache.CondCheck{}\n \t\th := r.Header.Clone()\n@@ -968,7 +967,7 @@ func (api objectAPIHandlers) headObjectHandler(ctx context.Context, objectAPI Ob\n \t\t}\n \t\trc.Init(bucket, object, h)\n \n-\t\tci, err := globalCacheConfig.Get(rc)\n+\t\tci, _ := globalCacheConfig.Get(rc)\n \t\tif ci != nil {\n \t\t\ttgs, ok := ci.Metadata[xhttp.AmzObjectTagging]\n \t\t\tif ok {\n@@ -1027,9 +1026,6 @@ func (api objectAPIHandlers) headObjectHandler(ctx context.Context, objectAPI Ob\n \t\t\t\treturn\n \t\t\t}\n \t\t}\n-\t\tif errors.Is(err, cache.ErrKeyMissing) {\n-\t\t\tupdate = true\n-\t\t}\n \t}\n \n \topts.FastGetObjInfo = true\n@@ -1053,10 +1049,6 @@ func (api objectAPIHandlers) headObjectHandler(ctx context.Context, objectAPI Ob\n \t\t\t}\n \t\t}\n \t}\n-\tif _, ok := crypto.IsEncrypted(objInfo.UserDefined); ok {\n-\t\t// Never store encrypted objects in cache.\n-\t\tupdate = false\n-\t}\n \n \tif objInfo.UserTags != \"\" {\n \t\t// Set this such that authorization policies can be applied on the object tags.\n@@ -1137,24 +1129,6 @@ func (api objectAPIHandlers) headObjectHandler(ctx context.Context, objectAPI Ob\n \t\treturn\n \t}\n \n-\tif update {\n-\t\tasize, err := objInfo.GetActualSize()\n-\t\tif err != nil {\n-\t\t\tasize = objInfo.Size\n-\t\t}\n-\n-\t\tdefer globalCacheConfig.Set(&cache.ObjectInfo{\n-\t\t\tKey:          objInfo.Name,\n-\t\t\tBucket:       objInfo.Bucket,\n-\t\t\tETag:         objInfo.ETag,\n-\t\t\tModTime:      objInfo.ModTime,\n-\t\t\tExpires:      objInfo.ExpiresStr(),\n-\t\t\tCacheControl: objInfo.CacheControl,\n-\t\t\tSize:         asize,\n-\t\t\tMetadata:     cleanReservedKeys(objInfo.UserDefined),\n-\t\t})\n-\t}\n-\n \t// Validate pre-conditions if any.\n \tif checkPreconditions(ctx, w, r, objInfo, opts) {\n \t\treturn\n", "instance_id": "minio__minio-19929", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a discrepancy in `Content-Length` between `HEAD` and `GET` requests in a MinIO setup with server-side encryption enabled using KMS. It provides specific details such as the file size, bucket name, and HTTP request/response logs, which help in understanding the context and reproducing the issue. However, there are minor ambiguities and missing details. For instance, it does not explicitly state the expected behavior beyond \"both requests should return the same content length,\" nor does it mention specific edge cases or constraints related to encryption or caching mechanisms that might be relevant. Additionally, there are no examples of expected output or detailed steps to reproduce beyond the provided logs. While the issue is valid and the goal is clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is relatively focused, primarily affecting a single file (`object-handlers.go`) and specifically the `headObjectHandler` function. However, the changes involve removing logic related to caching and updating object metadata for encrypted objects, which suggests a need to understand the interaction between MinIO's caching mechanism and server-side encryption (SSE) with KMS. This requires knowledge of specific technical concepts such as HTTP request handling, object metadata management, encryption in storage systems, and MinIO's internal caching logic (`globalCacheConfig`). The removal of cache updates for encrypted objects indicates a nuanced understanding of how encryption impacts content length calculations and caching behavior, which is not trivial. Additionally, while the problem statement does not explicitly mention edge cases, the nature of the issue (discrepancy in content length with encryption) implies potential edge cases around different encryption configurations, object sizes, or concurrent access scenarios that might need consideration. Error handling logic appears to be modified (e.g., ignoring errors from `globalCacheConfig.Get`), which adds a layer of complexity in ensuring robustness. Overall, solving this requires a deep understanding of MinIO's architecture, particularly around encryption and caching, making it a challenging task but not at the extreme end of difficulty since the changes are localized and do not involve a complete system refactor or advanced distributed systems concepts.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Docker rootless and outgoing IPv6\n### Description\r\n\r\nWhen using IPv6 in rootless mode in containers to reach any IPv6 address, this does not work.\r\n\r\nRunning ping6 fails, curl too:\r\n```\r\ndocker@pcserver2023:~> docker run -it --rm bash:4.4\r\nbash-4.4# ping6 -c 3 www.heise.de\r\nPING www.heise.de (2a02:2e0:3fe:1001:7777:772e:2:85): 56 data bytes\r\n\r\n--- www.heise.de ping statistics ---\r\n3 packets transmitted, 0 packets received, 100% packet loss\r\nbash-4.4# wget --no-check-certificate --spider https://[2a02:2e0:3fe:1001:7777:772e:2:85]/index.html\r\nConnecting to [2a02:2e0:3fe:1001:7777:772e:2:85] ([2a02:2e0:3fe:1001:7777:772e:2:85]:443)\r\nwget: can't connect to remote host: Network unreachable\r\n```\r\n\r\nNo matter what i have tried so far, i was not able to make outgoing ipv6 work (Think i have followed all instructions to make firewalls and docker work).\r\nStrange but with slirp4netns at least connecting to my IPv6 from the internet (to my ipv6 adress of the hosts nic) is working perfectly.\r\n\r\nUnfortunatelly my service is relying on the described functionality (Nextcloud AIO) - and i do only have IPv6 available.\r\n\r\nDoes anyone know any way to make outgoing IPv6 work on rootless?\r\n\r\nbtw. didn't get pasta, bypass4netns working, maybe some advice how to work around problems with slirp4netns and ipv6?\r\n\r\n### Reproduce\r\n\r\n1. fresh linux, enabled cgroupsv2. Set up Docker rootless as user docker.\r\nHere using Network Pasta/implicit for testing, but its the same with slirp4netns:\r\n```\r\ndocker@pcserver2023:~> curl -fsSL https://get.docker.com/rootless | sh\r\n# Installing stable version 27.1.0\r\n# Executing docker rootless install script, commit: 1ce4e39\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 70.4M  100 70.4M    0     0  2314k      0  0:00:31  0:00:31 --:--:-- 1052k\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 19.7M  100 19.7M    0     0  2292k      0  0:00:08  0:00:08 --:--:-- 2759k\r\n+ PATH=/home/docker/bin:/home/docker/bin:/home/docker/bin:/home/docker/bin:/usr/local/bin:/usr/bin:/bin\r\n+ /home/docker/bin/dockerd-rootless-setuptool.sh install\r\n[INFO] Creating /home/docker/.config/systemd/user/docker.service\r\n[INFO] starting systemd service docker.service\r\n+ systemctl --user start docker.service\r\n+ sleep 3\r\n+ systemctl --user --no-pager --full status docker.service\r\n\u25cf docker.service - Docker Application Container Engine (Rootless)\r\n     Loaded: loaded (/home/docker/.config/systemd/user/docker.service; enabled; vendor preset: disabled)\r\n    Drop-In: /home/docker/.config/systemd/user/docker.service.d\r\n             \u2514\u2500override.conf\r\n     Active: active (running) since Tue 2024-07-23 14:36:02 CEST; 3s ago\r\n       Docs: https://docs.docker.com/go/rootless/\r\n   Main PID: 934 (rootlesskit)\r\n      Tasks: 45\r\n     Memory: 67.1M\r\n        CPU: 385ms\r\n     CGroup: /user.slice/user-1001.slice/user@1001.service/app.slice/docker.service\r\n             \u251c\u2500 934 rootlesskit --state-dir=/run/user/1001/dockerd-rootless --net=pasta --mtu=1500 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=implicit --copy-up=/etc --copy-up=/run --propagation=rslave --ipv6 /home/docker/bin/dockerd-rootless.sh\r\n             \u251c\u2500 940 /proc/self/exe --state-dir=/run/user/1001/dockerd-rootless --net=pasta --mtu=1500 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=implicit --copy-up=/etc --copy-up=/run --propagation=rslave --ipv6 /home/docker/bin/dockerd-rootless.sh\r\n             \u251c\u2500 955 pasta --foreground --stderr --ns-ifname=tap0 --mtu=1500 --no-dhcp --no-ra --address=10.0.2.100 --netmask=24 --gateway=10.0.2.2 --dns-forward=10.0.2.3 --no-map-gw --tcp-ports=auto --udp-ports=auto 940\r\n             \u251c\u2500 967 dockerd\r\n             \u2514\u2500 992 containerd --config /run/user/1001/docker/containerd/containerd.toml\r\n+ DOCKER_HOST=unix:///run/user/1001/docker.sock\r\n+ /home/docker/bin/docker version\r\nClient:\r\n Version:           27.1.0\r\n API version:       1.46\r\n Go version:        go1.21.12\r\n Git commit:        6312585\r\n Built:             Fri Jul 19 17:41:56 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          27.1.0\r\n  API version:      1.46 (minimum version 1.24)\r\n  Go version:       go1.21.12\r\n  Git commit:       a21b1a2\r\n  Built:            Fri Jul 19 17:43:33 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          v1.7.20\r\n  GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353\r\n runc:\r\n  Version:          1.1.13\r\n  GitCommit:        v1.1.13-0-g58aa920\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          2.0.2\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    pasta\r\n  StateDir:         /run/user/1001/dockerd-rootless\r\n+ systemctl --user enable docker.service\r\n[INFO] Installed docker.service successfully.\r\n[INFO] To control docker.service, run: `systemctl --user (start|stop|restart) docker.service`\r\n[INFO] To run docker.service on system startup, run: `sudo loginctl enable-linger docker`\r\n\r\n[INFO] CLI context \"rootless\" already exists\r\n[INFO] Using CLI context \"rootless\"\r\nCurrent context is now \"rootless\"\r\nWarning: DOCKER_HOST environment variable overrides the active context. To use \"rootless\", either set the global --context flag, or unset DOCKER_HOST environment variable.\r\n\r\n[INFO] Make sure the following environment variable(s) are set (or add them to ~/.bashrc):\r\nexport PATH=/home/docker/bin:$PATH\r\n\r\n[INFO] Some applications may require the following environment variable too:\r\nexport DOCKER_HOST=unix:///run/user/1001/docker.sock\r\n```\r\ndaemon.json:\r\n```\r\n{\r\n \"experimental\": true,\r\n \"ipv6\": true,\r\n \"fixed-cidr-v6\": \"fdff:6785:1::/48\",\r\n \"iptables\": true,\r\n \"ip6tables\": true,\r\n \"log-opts\": {\r\n  \"max-size\": \"10m\",\r\n  \"max-file\": \"5\"\r\n }\r\n}\r\n```\r\n\r\nnetwork:\r\n```\r\ndocker@pcserver2023:~> docker network inspect bridge\r\n[\r\n    {\r\n        \"Name\": \"bridge\",\r\n        \"Id\": \"61278e5b24b18a0b6897834cdad253851e5133389928e82a9fe1ebb9debda930\",\r\n        \"Created\": \"2024-07-23T14:45:08.276573789+02:00\",\r\n        \"Scope\": \"local\",\r\n        \"Driver\": \"bridge\",\r\n        \"EnableIPv6\": true,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": null,\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"172.17.0.0/16\",\r\n                    \"Gateway\": \"172.17.0.1\"\r\n                },\r\n                {\r\n                    \"Subnet\": \"fdff:6785:1::/48\",\r\n                    \"Gateway\": \"fdff:6785:1::1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": false,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {},\r\n        \"Options\": {\r\n            \"com.docker.network.bridge.default_bridge\": \"true\",\r\n            \"com.docker.network.bridge.enable_icc\": \"true\",\r\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\r\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\r\n            \"com.docker.network.bridge.name\": \"docker0\",\r\n            \"com.docker.network.driver.mtu\": \"1500\"\r\n        },\r\n        \"Labels\": {}\r\n    }\r\n]\r\n```\r\n\r\n### Expected behavior\r\n\r\nshould work as ipv4 does\r\ndocker run -it --rm bash:4.4\r\n```\r\nbash-4.4# ping -c 3 www.heise.de\r\nPING www.heise.de (193.99.144.85): 56 data bytes\r\n64 bytes from 193.99.144.85: seq=0 ttl=254 time=326.905 ms\r\n64 bytes from 193.99.144.85: seq=1 ttl=254 time=14.479 ms\r\n64 bytes from 193.99.144.85: seq=2 ttl=254 time=14.441 ms\r\n\r\n--- www.heise.de ping statistics ---\r\n3 packets transmitted, 3 packets received, 0% packet loss\r\nround-trip min/avg/max = 14.441/118.608/326.905 ms\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           27.1.0\r\n API version:       1.46\r\n Go version:        go1.21.12\r\n Git commit:        6312585\r\n Built:             Fri Jul 19 17:41:56 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          27.1.0\r\n  API version:      1.46 (minimum version 1.24)\r\n  Go version:       go1.21.12\r\n  Git commit:       a21b1a2\r\n  Built:            Fri Jul 19 17:43:33 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          v1.7.20\r\n  GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353\r\n runc:\r\n  Version:          1.1.13\r\n  GitCommit:        v1.1.13-0-g58aa920\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          2.0.2\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    pasta\r\n  StateDir:         /run/user/1001/dockerd-rootless\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    27.1.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.16.2\r\n    Path:     /usr/lib/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.27.0\r\n    Path:     /home/docker/.docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 35\r\n Server Version: 27.1.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8fc6bcff51318944179630522a095cc9dbf9f353\r\n runc version: v1.1.13-0-g58aa920\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  rootless\r\n  cgroupns\r\n Kernel Version: 5.14.21-150500.55.68-default\r\n Operating System: openSUSE Leap 15.5\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 30.79GiB\r\n Name: pcserver2023\r\n ID: 45699224-ea9c-4865-8dea-a53bb20b788c\r\n Docker Root Dir: /home/docker/.local/share/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nSame setup with rootful works as expected, but has very different network devices (docker0), which are not existent on the host in rootless.\n", "patch": "diff --git a/Dockerfile b/Dockerfile\nindex 0ff37fc6c1519..622f4b4443cee 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -356,7 +356,7 @@ FROM base AS rootlesskit-src\n WORKDIR /usr/src/rootlesskit\n RUN git init . && git remote add origin \"https://github.com/rootless-containers/rootlesskit.git\"\n # When updating, also update vendor.mod and hack/dockerfile/install/rootlesskit.installer accordingly.\n-ARG ROOTLESSKIT_VERSION=v2.0.2\n+ARG ROOTLESSKIT_VERSION=v2.3.1\n RUN git fetch -q --depth 1 origin \"${ROOTLESSKIT_VERSION}\" +refs/tags/*:refs/tags/* && git checkout -q FETCH_HEAD\n \n FROM base AS rootlesskit-build\ndiff --git a/hack/dockerfile/install/rootlesskit.installer b/hack/dockerfile/install/rootlesskit.installer\nindex de6846ae17fa2..3c3e96189b499 100755\n--- a/hack/dockerfile/install/rootlesskit.installer\n+++ b/hack/dockerfile/install/rootlesskit.installer\n@@ -1,7 +1,7 @@\n #!/bin/sh\n \n # When updating, also update vendor.mod and Dockerfile accordingly.\n-: \"${ROOTLESSKIT_VERSION:=v2.0.2}\"\n+: \"${ROOTLESSKIT_VERSION:=v2.3.1}\"\n \n install_rootlesskit() {\n \tcase \"$1\" in\n", "instance_id": "moby__moby-48479", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: outgoing IPv6 connectivity does not work in Docker rootless mode, while IPv4 works as expected. It provides detailed reproduction steps, including the setup process, configuration files, and expected versus actual behavior with examples of failed commands (e.g., `ping6` and `wget`). Additionally, it includes relevant system and Docker version information, which aids in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the desired solution or constraints (e.g., whether the fix should be in Docker, rootlesskit, or a network driver like `pasta` or `slirp4netns`). It also lacks clarity on specific edge cases or environmental factors (e.g., specific kernel configurations or network setups beyond the provided info) that might affect the issue. While the issue is well-documented, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the issue involves understanding and potentially modifying a complex part of the Docker ecosystem\u2014rootless mode networking with IPv6 support. The code changes provided are minimal, merely updating the version of `rootlesskit` from v2.0.2 to v2.3.1, which suggests the solution might be leveraging an existing fix or feature in a newer version. However, evaluating and confirming this requires deep knowledge of the `rootlesskit` project, its changelog, and its interaction with Docker's networking stack, including drivers like `pasta` and `slirp4netns`. Second, the problem demands familiarity with multiple technical concepts: Linux networking (IPv6, cgroupsv2), Docker's rootless architecture, and low-level network virtualization tools. Third, while the code change itself is trivial, the underlying issue likely involves debugging complex network interactions, potentially requiring modifications or configurations beyond the provided diff if the version update does not fully resolve the issue. Edge cases, such as specific host network configurations, firewall rules, or kernel versions, are implied but not fully specified, adding to the complexity of ensuring a robust solution. Finally, the impact of a fix could affect the broader system behavior in rootless Docker setups, necessitating careful testing. Given these factors, a difficulty score of 0.75 reflects the need for significant expertise and effort to fully address the problem, even if the provided code change is a starting point.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Race condition in api/embed when generating embeddings with multiple inputs\n### What is the issue?\n\nThere seems to be a race condition between the `processBatch/decode` execution and the `Server.embeddings` [block](https://github.com/ollama/ollama/blob/main/llama/runner/runner.go#L784) that generates the json response from the embedding buffer received as a result.\n\nCalling the `api/embed` endpoint repeatedly should trigger the issue for one of the executions. The race condition happens between the inputs sent for a given request, as far as I've been able to verify, but I think it could happen between concurrent requests to the api/embed endpoint as well.\n\nWhen the issue manifests itself, part of of the json array in the embedding response for one of the inputs is overwritten by zeroes, which seems to be caused by a concurrent call to `memset` within llama cpp code when processing the other input.\n\nExample payload to reproduce:\n\n```\n{\"input\": [\"dove\", \"pigeon\"], \"model\": \"llama3.2:3b\"}\n```\n\nI think the issue happens with any model, and the probability of the race condition seems to vary according to model size (and processing time?). Reproduced with these models:\n\n- llama3.1:8b\n- llama3.2:3b\n- deepseek-r1:14b\n\n### What did you expect to happen?\n\nThe json arrays returned from the api/embed call should not be partially overwritten with zero values.\n\n### Additional details:\n\nI'm using a CPU backend, and have not tested this in another hardware configuration in detail. The issue also happens a in Mac M1, but I'm not sure if the code paths involved are the same as with the current box I'm using right now (intel core ultra 9 185H / Linux) -- will be able to check about the mac only later and update.\n\nThe issue seems to happen only when input 1 and input 2 are processed in separate batches. This happens when `processBatch` processes input 1, returns, and is called again by the `run` method, and processes input 2 separately. When both inputs are processed in the same batch, in a single `Decode` call, the issue does not manifest.\n\nThe flow that causes the issue starts in `EmbedHandler` triggering two calls to the `embeddings` endpoint in parallel (by triggering two goroutines). After that, the concurrent flows are roughly the ones below (RC marks the step that happen in parallel/concurrently when the issue manifests):\n\nInput 1: `embeddings (add seq and wait) -> processBatch -> Decode -> ... -> RC: embeddings (resume and encode json response)`\nInput 2: `embeddings (add seq and wait) -> processBatch -> Decode -> llama_decode -> llama_decode_internal -> llama_output_reserve -> ggml_backend_buffer_clear -> ggml_backend_cpu_buffer_clear -> RC: memset`\n\nThe last steps for each input happen in parallel and act on the same buffer (buffer being used to encode json response for input 1 flow is the same as memset is zeroing out for input 2), causing memset to clear the buffer for processing Input 2 that is being used to return the embedding result for Input 1.\n\n\n### OS\n\nLinux\n\n### GPU\n\n_No response_\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.5.7\n", "patch": "diff --git a/llama/llama.go b/llama/llama.go\nindex 18790a95df3..7cb4857f0de 100644\n--- a/llama/llama.go\n+++ b/llama/llama.go\n@@ -243,21 +243,25 @@ func (c *Context) KvCacheDefrag() {\n \n // Get the embeddings for a sequence id\n func (c *Context) GetEmbeddingsSeq(seqId int) []float32 {\n-\tembeddings := unsafe.Pointer(C.llama_get_embeddings_seq(c.c, C.int(seqId)))\n-\tif embeddings == nil {\n+\te := unsafe.Pointer(C.llama_get_embeddings_seq(c.c, C.int(seqId)))\n+\tif e == nil {\n \t\treturn nil\n \t}\n \n-\treturn unsafe.Slice((*float32)(embeddings), c.Model().NEmbd())\n+\tembeddings := make([]float32, c.Model().NEmbd())\n+\t_ = copy(embeddings, unsafe.Slice((*float32)(e), c.Model().NEmbd()))\n+\treturn embeddings\n }\n \n func (c *Context) GetEmbeddingsIth(i int) []float32 {\n-\tembeddings := unsafe.Pointer(C.llama_get_embeddings_ith(c.c, C.int32_t(i)))\n-\tif embeddings == nil {\n+\te := unsafe.Pointer(C.llama_get_embeddings_ith(c.c, C.int32_t(i)))\n+\tif e == nil {\n \t\treturn nil\n \t}\n \n-\treturn unsafe.Slice((*float32)(embeddings), c.Model().NEmbd())\n+\tembeddings := make([]float32, c.Model().NEmbd())\n+\t_ = copy(embeddings, unsafe.Slice((*float32)(e), c.Model().NEmbd()))\n+\treturn embeddings\n }\n \n type ModelParams struct {\n", "instance_id": "ollama__ollama-8714", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue, including the context of the race condition in the `api/embed` endpoint, the expected behavior, and steps to reproduce the issue with example payloads. It also includes specific details about the hardware and software environment, as well as a breakdown of the concurrent flows leading to the race condition. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define the expected input/output format for the API endpoint beyond mentioning JSON arrays, nor does it fully clarify the scope of hardware configurations affected (e.g., whether this is CPU-specific or could occur on GPUs). Additionally, while edge cases like separate batch processing are mentioned, other potential edge cases (e.g., very large inputs or specific model configurations) are not discussed. Overall, the statement is valid and clear but lacks some minor details that would make it fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the problem involves a race condition, which inherently requires a deep understanding of concurrency in Go, including goroutines and memory management, as well as familiarity with the underlying C bindings (via `unsafe.Pointer`) to the llama.cpp library. The code changes, while localized to a single file (`llama/llama.go`) and relatively small in terms of lines of code, address a critical issue by modifying how embeddings are retrieved and copied to prevent buffer overwrites during concurrent access. This requires understanding the interaction between Go and C code, as well as the memory model of the llama.cpp backend.\n\nSecond, the scope of the change, though limited to two functions, has a significant impact on the correctness of the system, as it addresses a race condition that affects the integrity of API responses. Understanding the problem requires knowledge of the broader codebase, particularly how embeddings are processed in batches and how concurrency is managed in the `EmbedHandler`.\n\nThird, the technical concepts involved are moderately complex, including Go's concurrency model, unsafe pointers for C interoperability, and memory buffer management. Additionally, domain-specific knowledge of machine learning model embeddings and the llama.cpp library's internals (e.g., `memset` behavior) is necessary to fully grasp the issue.\n\nFinally, while the problem statement highlights the specific condition under which the race condition occurs (separate batch processing), handling edge cases and ensuring robust error handling for concurrent access to shared buffers could add further complexity. The solution must be tested across different hardware configurations and model sizes, as mentioned in the problem statement, which introduces additional challenges in validation.\n\nOverall, I rate this as 0.65 because it requires a deep understanding of concurrency and C-Go interoperability, involves a critical fix with system-wide implications, and demands careful handling of shared resources, but it does not reach the \"Very Hard\" level as the changes are localized and do not involve extensive architectural refactoring or advanced domain-specific algorithms.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "act favors secrets from `.secrets` over values from `-s`\n### Bug report info\n\n```plain text\nact version:            0.2.57\r\nGOOS:                   darwin\r\nGOARCH:                 arm64\r\nNumCPU:                 10\r\nDocker host:            DOCKER_HOST environment variable is not set\r\nSockets found:\r\n\t/var/run/docker.sock\r\nConfig files:           \r\n\t/Users/jsoref/.actrc:\r\n\t\t#-P ubuntu-latest=node:12.20.1-buster-slim\r\n\t\t#-P ubuntu-20.04=node:12.20.1-buster-slim\r\n\t\t#-P ubuntu-18.04=node:12.20.1-buster-slim\r\n\t\t-P ubuntu-latest=catthehacker/ubuntu:act-latest\r\n\t\t-P ubuntu-22.04=catthehacker/ubuntu:act-22.04\r\n\t\t-P ubuntu-20.04=catthehacker/ubuntu:act-20.04\r\n\t\t-P ubuntu-18.04=catthehacker/ubuntu:act-18.04\r\n\t\t-P ubuntu-16.04=catthehacker/ubuntu:act-16.04\r\n\t\t-P self-hosted=catthehacker/ubuntu:act-latest\r\n\t\t-P ubuntu-latest-4cpu=ubuntu:act-latest\r\n\t\t-P ubuntu-latest-8cpu=ubuntu:act-latest\r\nBuild info:\r\n\tGo version:            go1.21.5\r\n\tModule path:           command-line-arguments\r\n\tMain version:          \r\n\tMain path:             \r\n\tMain checksum:         \r\n\tBuild settings:\r\n\t\t-buildmode:           exe\r\n\t\t-compiler:            gc\r\n\t\t-ldflags:             -X main.version=0.2.57\r\n\t\tDefaultGODEBUG:       panicnil=1\r\n\t\tCGO_ENABLED:          1\r\n\t\tCGO_CFLAGS:           \r\n\t\tCGO_CPPFLAGS:         \r\n\t\tCGO_CXXFLAGS:         \r\n\t\tCGO_LDFLAGS:          \r\n\t\tGOARCH:               arm64\r\n\t\tGOOS:                 darwin\r\nDocker Engine:\r\n\tEngine version:        23.0.6\r\n\tEngine runtime:        runc\r\n\tCgroup version:        1\r\n\tCgroup driver:         cgroupfs\r\n\tStorage driver:        overlay2\r\n\tRegistry URI:          https://index.docker.io/v1/\r\n\tOS:                    Alpine Linux v3.18\r\n\tOS type:               linux\r\n\tOS version:            3.18.5\r\n\tOS arch:               aarch64\r\n\tOS kernel:             6.1.64-0-virt\r\n\tOS CPU:                2\r\n\tOS memory:             1973 MB\r\n\tSecurity options:\r\n\t\tname=seccomp,profile=builtin\n```\n\n\n### Command used with act\n\n```sh\n(~/code/nektos/act/dist/local/act --use-new-action-cache -j spelling -s \"GITHUB_TOKEN=$GH_TOKEN\" 2>&1)\n```\n\n\n### Describe issue\n\nUsing a fine-grained token that allows contents:read for a private repository, act (using `--use-new-action-cache`) isn't able to retrieve my public action.\n\n### Link to GitHub repository\n\n_No response_\n\n### Workflow content\n\n```yml\nname: Check Spelling\r\n\r\n# Comment management is handled through a secondary job, for details see:\r\n# https://github.com/check-spelling/check-spelling/wiki/Feature%3A-Restricted-Permissions\r\n#\r\n# `jobs.comment-push` runs when a push is made to a repository and the `jobs.spelling` job needs to make a comment\r\n#   (in odd cases, it might actually run just to collapse a comment, but that's fairly rare)\r\n#   it needs `contents: write` in order to add a comment.\r\n#\r\n# `jobs.comment-pr` runs when a pull_request is made to a repository and the `jobs.spelling` job needs to make a comment\r\n#   or collapse a comment (in the case where it had previously made a comment and now no longer needs to show a comment)\r\n#   it needs `pull-requests: write` in order to manipulate those comments.\r\n\r\n# Updating pull request branches is managed via comment handling.\r\n# For details, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Update-expect-list\r\n#\r\n# These elements work together to make it happen:\r\n#\r\n# `on.issue_comment`\r\n#   This event listens to comments by users asking to update the metadata.\r\n#\r\n# `jobs.update`\r\n#   This job runs in response to an issue_comment and will push a new commit\r\n#   to update the spelling metadata.\r\n#\r\n# `with.experimental_apply_changes_via_bot`\r\n#   Tells the action to support and generate messages that enable it\r\n#   to make a commit to update the spelling metadata.\r\n#\r\n# `with.ssh_key`\r\n#   In order to trigger workflows when the commit is made, you can provide a\r\n#   secret (typically, a write-enabled github deploy key).\r\n#\r\n#   For background, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Update-with-deploy-key\r\n\r\n# Sarif reporting\r\n#\r\n# Access to Sarif reports is generally restricted (by GitHub) to members of the repository.\r\n#\r\n# Requires enabling `security-events: write`\r\n# and configuring the action with `use_sarif: 1`\r\n#\r\n#   For information on the feature, see: https://github.com/check-spelling/check-spelling/wiki/Feature:-Sarif-output\r\n\r\n# Minimal workflow structure:\r\n#\r\n# on:\r\n#   push:\r\n#     ...\r\n#   pull_request_target:\r\n#     ...\r\n# jobs:\r\n#   # you only want the spelling job, all others should be omitted\r\n#   spelling:\r\n#     # remove `security-events: write` and `use_sarif: 1`\r\n#     # remove `experimental_apply_changes_via_bot: 1`\r\n#     ... otherwise adjust the `with:` as you wish\r\n\r\non:\r\n  push:\r\n    branches:\r\n    - \"**\"\r\n    tags-ignore:\r\n    - \"**\"\r\n  pull_request_target:\r\n    branches:\r\n    - \"**\"\r\n    types:\r\n    - 'opened'\r\n    - 'reopened'\r\n    - 'synchronize'\r\n  issue_comment:\r\n    types:\r\n    - 'created'\r\n\r\njobs:\r\n  spelling:\r\n    name: Check Spelling\r\n    permissions:\r\n      contents: read\r\n      pull-requests: read\r\n      actions: read\r\n    outputs:\r\n      followup: ${{ steps.spelling.outputs.followup }}\r\n    runs-on: ubuntu-latest\r\n    if: ${{ contains(github.event_name, 'pull_request') || github.event_name == 'push' }}\r\n    concurrency:\r\n      group: spelling-${{ github.event.pull_request.number || github.ref }}\r\n      # note: If you use only_check_changed_files, you do not want cancel-in-progress\r\n      cancel-in-progress: true\r\n    steps:\r\n    - name: check-spelling\r\n      id: spelling\r\n      uses: check-spelling/check-spelling@prerelease\r\n      with:\r\n        config: .github/actions/spelling-data\r\n        suppress_push_for_open_pull_request: ${{ github.actor != 'dependabot[bot]' && 1 }}\r\n        checkout: true\r\n        check_file_names: 1\r\n        spell_check_this: check-spelling/spell-check-this@prerelease\r\n        post_comment: 0\r\n        use_magic_file: 1\r\n        report-timing: 1\r\n        warnings: bad-regex,binary-file,deprecated-feature,ignored-expect-variant,large-file,limited-references,no-newline-at-eof,noisy-file,non-alpha-in-dictionary,token-is-substring,unexpected-line-ending,whitespace-in-dictionary,minified-file,unsupported-configuration,no-files-to-check\r\n        experimental_apply_changes_via_bot: 1\r\n        use_sarif: ${{ (!github.event.pull_request || (github.event.pull_request.head.repo.full_name == github.repository)) && 1 }}\r\n        extra_dictionary_limit: 20\r\n        extra_dictionaries:\r\n          cspell:software-terms/dict/softwareTerms.txt\r\n\r\n  comment-push:\r\n    name: Report (Push)\r\n    # If your workflow isn't running on push, you can remove this job\r\n    runs-on: ubuntu-latest\r\n    needs: spelling\r\n    permissions:\r\n      actions: read\r\n      contents: write\r\n    if: (success() || failure()) && needs.spelling.outputs.followup && github.event_name == 'push'\r\n    steps:\r\n    - name: comment\r\n      uses: check-spelling/check-spelling@prerelease\r\n      with:\r\n        config: .github/actions/spelling-data\r\n        checkout: true\r\n        spell_check_this: check-spelling/spell-check-this@prerelease\r\n        task: ${{ needs.spelling.outputs.followup }}\r\n\r\n  comment-pr:\r\n    name: Report (PR)\r\n    # If you workflow isn't running on pull_request*, you can remove this job\r\n    runs-on: ubuntu-latest\r\n    needs: spelling\r\n    permissions:\r\n      actions: read\r\n      contents: read\r\n      pull-requests: write\r\n    if: (success() || failure()) && needs.spelling.outputs.followup && contains(github.event_name, 'pull_request')\r\n    steps:\r\n    - name: comment\r\n      uses: check-spelling/check-spelling@prerelease\r\n      with:\r\n        config: .github/actions/spelling-data\r\n        checkout: true\r\n        spell_check_this: check-spelling/spell-check-this@prerelease\r\n        task: ${{ needs.spelling.outputs.followup }}\r\n        experimental_apply_changes_via_bot: 1\r\n\r\n  update:\r\n    name: Update PR\r\n    permissions:\r\n      contents: write\r\n      pull-requests: write\r\n      actions: read\r\n    runs-on: ubuntu-latest\r\n    if: ${{\r\n        github.event_name == 'issue_comment' &&\r\n        github.event.issue.pull_request &&\r\n        contains(github.event.comment.body, '@check-spelling-bot apply')\r\n      }}\r\n    concurrency:\r\n      group: spelling-update-${{ github.event.issue.number }}\r\n      cancel-in-progress: false\r\n    steps:\r\n    - name: apply spelling updates\r\n      uses: check-spelling/check-spelling@prerelease\r\n      with:\r\n        config: .github/actions/spelling-data\r\n        experimental_apply_changes_via_bot: 1\r\n        checkout: true\r\n        ssh_key: \"${{ secrets.CHECK_SPELLING }}\"\n```\n\n\n### Relevant log output\n\n```sh\n> Error: failed to fetch \"https://github.com/check-spelling/check-spelling\" version \"prerelease\": authentication required\n```\n\n\n### Additional information\n\nhttps://github.com/settings/tokens?type=beta\r\nhttps://docs.github.com/en/rest/authentication/permissions-required-for-fine-grained-personal-access-tokens?apiVersion=2022-11-28\n\nApparently I have an expired GITHUB_TOKEN in `.secrets` which has precedence over the `-s GITHUB_TOKEN=...`. This is undesirable. \n", "patch": "diff --git a/cmd/root.go b/cmd/root.go\nindex 3eca0406b90..44d8cddda75 100644\n--- a/cmd/root.go\n+++ b/cmd/root.go\n@@ -305,7 +305,9 @@ func readEnvs(path string, envs map[string]string) bool {\n \t\t\tlog.Fatalf(\"Error loading from %s: %v\", path, err)\n \t\t}\n \t\tfor k, v := range env {\n-\t\t\tenvs[k] = v\n+\t\t\tif _, ok := envs[k]; !ok {\n+\t\t\t\tenvs[k] = v\n+\t\t\t}\n \t\t}\n \t\treturn true\n \t}\n", "instance_id": "nektos__act-2274", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the tool 'act' prioritizes secrets from a `.secrets` file over command-line provided values (via `-s`), which leads to authentication issues when an expired token is present in `.secrets`. The goal is implied to be changing this precedence to favor command-line inputs over file-based secrets. The statement includes detailed context such as version information, command used, relevant log output, and a description of the issue. However, it lacks explicit specification of the desired behavior (e.g., should command-line arguments always override file-based secrets, or only in specific cases?) and does not mention potential edge cases or constraints (e.g., what happens if both sources are invalid?). Additionally, there are no examples of expected input/output behavior after the fix. These minor ambiguities prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The code change provided is minimal, involving a single conditional check in one file (`cmd/root.go`) to prevent overwriting existing environment variables when loading from a file. This indicates a straightforward bug fix that requires basic understanding of Go's map data structure and control flow. The scope of the change is limited to a single function and does not impact the broader architecture or multiple modules of the codebase. The technical concepts involved are basic (map operations and conditional logic in Go), and there is no indication of complex algorithms, design patterns, or domain-specific knowledge required. While the problem statement does not explicitly mention edge cases, the nature of the change suggests minimal need for additional error handling beyond what is already in place (e.g., handling file read errors, which are already addressed in the surrounding code). The primary challenge lies in understanding the intent of the precedence logic, but the provided diff resolves this directly. Therefore, a difficulty score of 0.25 is appropriate, reflecting a simple bug fix with limited complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Backblaze B2 remote does not always honor `--dry-run` or `--interactive` flags\n#### What is the problem you are having with rclone?\r\n\r\nThe Backblaze B2 remote does not honor the `--dry-run` or `--interactive` flags in `backend` operations.\r\n\r\n#### What is your rclone version (output from `rclone version`)\r\n\r\n```\r\n% rclone version\r\nrclone v1.68.2\r\n- os/version: darwin 14.7.1 (64 bit)\r\n- os/kernel: 23.6.0 (arm64)\r\n- os/type: darwin\r\n- os/arch: arm64 (ARMv8 compatible)\r\n- go/version: go1.23.3\r\n- go/linking: dynamic\r\n- go/tags: cmount\r\n```\r\n\r\n#### Which OS you are using and how many bits (e.g. Windows 7, 64 bit)\r\n\r\nmacOS Sonoma 14.7.1\r\n\r\n#### Which cloud storage system are you using? (e.g. Google Drive)\r\n\r\nBackblaze B2\r\n\r\n#### The command you were trying to run (e.g. `rclone copy /tmp remote:tmp`)\r\n\r\n`rclone backend cleanup-hidden b2:metadaddy-rclone --dry-run`\r\n\r\n#### A log from the command with the `-vv` flag (e.g. output from `rclone -vv copy /tmp remote:tmp`)\r\n\r\nFirst, create some hidden versions...\r\n\r\n```\r\n% rclone copyto -I /Users/ppatterson/Documents/Test\\ Files/hello.txt b2:metadaddy-rclone/test-dry-run.txt\r\n% rclone copyto -I /Users/ppatterson/Documents/Test\\ Files/hello.txt b2:metadaddy-rclone/test-dry-run.txt\r\n% rclone copyto -I /Users/ppatterson/Documents/Test\\ Files/hello.txt b2:metadaddy-rclone/test-dry-run.txt\r\n% rclone --b2-versions ls b2:metadaddy-rclone       \r\n       14 test-dry-run.txt\r\n       14 test-dry-run-v2024-11-19-014429-131.txt\r\n       14 test-dry-run-v2024-11-19-014219-013.txt\r\n```\r\n\r\nNow run `cleanup-hidden` with `--dry-run`:\r\n\r\n```\r\n% rclone backend cleanup-hidden b2:metadaddy-rclone --dry-run -vv                                  \r\n2024/11/18 17:42:32 DEBUG : Setting --fast-list \"true\" from environment variable RCLONE_FAST_LIST=\"true\"\r\n2024/11/18 17:42:32 DEBUG : rclone: Version \"v1.68.2\" starting with parameters [\"rclone\" \"backend\" \"cleanup-hidden\" \"b2:metadaddy-rclone\" \"--dry-run\" \"-vv\"]\r\n2024/11/18 17:42:32 DEBUG : Using config file from \"/Users/ppatterson/.config/rclone/rclone.conf\"\r\n2024/11/18 17:42:32 INFO  : B2 bucket metadaddy-rclone: cleaning bucket \"metadaddy-rclone\" of all hidden files\r\n2024/11/18 17:42:32 DEBUG : test-dry-run.txt: Not deleting current version (id \"4_z91a5cf5973f5dc4f848d0c1b_f10733d4f99d18f52_d20241119_m014219_c004_v0402027_t0044_u01731980539013\") \"upload\" dated 2024-11-18 17:42:19.013 -0800 PST (13.651871s ago)\r\n2024/11/18 17:42:32 DEBUG : test-dry-run.txt: Deleting (id \"4_z91a5cf5973f5dc4f848d0c1b_f118ed7238ae3ae95_d20241119_m014215_c004_v0402026_t0034_u01731980535576\")\r\n2024/11/18 17:42:32 DEBUG : test-dry-run.txt: Deleting (id \"4_z91a5cf5973f5dc4f848d0c1b_f1175cd5b88012b9d_d20241119_m014212_c004_v0402007_t0007_u01731980532933\")\r\n2024/11/18 17:42:32 DEBUG : 11 go routines active\r\n```\r\n\r\nOld versions are gone!\r\n\r\n```\r\n% rclone --b2-versions ls b2:metadaddy-rclone                    \r\n       14 test-dry-run.txt\r\n```\r\n\r\nLooking at the code, `operations.SkipDestructive()` is not called at all in `b2.go`.\r\n\r\nI'll see if I can spin up a quick PR - it looks like it's just a case of adding `operations.SkipDestructive()` and the associated logic in the relevant places.\r\n\r\n<!--- Please keep the note below for others who read your bug report. -->\r\n\r\n#### How to use GitHub\r\n\r\n* Please use the \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.\r\n* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.\r\n* Subscribe to receive notifications on status change and new comments.\r\n\n", "patch": "diff --git a/backend/b2/b2.go b/backend/b2/b2.go\nindex 4d4e754242566..ad58383f9ac49 100644\n--- a/backend/b2/b2.go\n+++ b/backend/b2/b2.go\n@@ -30,6 +30,7 @@ import (\n \t\"github.com/rclone/rclone/fs/fserrors\"\n \t\"github.com/rclone/rclone/fs/fshttp\"\n \t\"github.com/rclone/rclone/fs/hash\"\n+\t\"github.com/rclone/rclone/fs/operations\"\n \t\"github.com/rclone/rclone/fs/walk\"\n \t\"github.com/rclone/rclone/lib/bucket\"\n \t\"github.com/rclone/rclone/lib/encoder\"\n@@ -1318,16 +1319,22 @@ func (f *Fs) purge(ctx context.Context, dir string, oldOnly bool, deleteHidden b\n \t\t\t\t// Check current version of the file\n \t\t\t\tif deleteHidden && object.Action == \"hide\" {\n \t\t\t\t\tfs.Debugf(remote, \"Deleting current version (id %q) as it is a hide marker\", object.ID)\n-\t\t\t\t\ttoBeDeleted <- object\n+\t\t\t\t\tif !operations.SkipDestructive(ctx, object.Name, \"remove hide marker\") {\n+\t\t\t\t\t\ttoBeDeleted <- object\n+\t\t\t\t\t}\n \t\t\t\t} else if deleteUnfinished && object.Action == \"start\" && isUnfinishedUploadStale(object.UploadTimestamp) {\n \t\t\t\t\tfs.Debugf(remote, \"Deleting current version (id %q) as it is a start marker (upload started at %s)\", object.ID, time.Time(object.UploadTimestamp).Local())\n-\t\t\t\t\ttoBeDeleted <- object\n+\t\t\t\t\tif !operations.SkipDestructive(ctx, object.Name, \"remove pending upload\") {\n+\t\t\t\t\t\ttoBeDeleted <- object\n+\t\t\t\t\t}\n \t\t\t\t} else {\n \t\t\t\t\tfs.Debugf(remote, \"Not deleting current version (id %q) %q dated %v (%v ago)\", object.ID, object.Action, time.Time(object.UploadTimestamp).Local(), time.Since(time.Time(object.UploadTimestamp)))\n \t\t\t\t}\n \t\t\t} else {\n \t\t\t\tfs.Debugf(remote, \"Deleting (id %q)\", object.ID)\n-\t\t\t\ttoBeDeleted <- object\n+\t\t\t\tif !operations.SkipDestructive(ctx, object.Name, \"delete\") {\n+\t\t\t\t\ttoBeDeleted <- object\n+\t\t\t\t}\n \t\t\t}\n \t\t\tlast = remote\n \t\t\ttr.Done(ctx, nil)\n@@ -2293,8 +2300,10 @@ func (f *Fs) lifecycleCommand(ctx context.Context, name string, arg []string, op\n \n \t}\n \n+\tskip := operations.SkipDestructive(ctx, name, \"update lifecycle rules\")\n+\n \tvar bucket *api.Bucket\n-\tif newRule.DaysFromHidingToDeleting != nil || newRule.DaysFromUploadingToHiding != nil || newRule.DaysFromStartingToCancelingUnfinishedLargeFiles != nil {\n+\tif !skip && (newRule.DaysFromHidingToDeleting != nil || newRule.DaysFromUploadingToHiding != nil || newRule.DaysFromStartingToCancelingUnfinishedLargeFiles != nil) {\n \t\tbucketID, err := f.getBucketID(ctx, bucketName)\n \t\tif err != nil {\n \t\t\treturn nil, err\n", "instance_id": "rclone__rclone-8195", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear, as it identifies a specific issue with the Backblaze B2 remote in rclone not honoring the `--dry-run` or `--interactive` flags during backend operations like `cleanup-hidden`. The goal is evident: ensure these flags are respected to prevent unintended destructive actions. The statement includes relevant details such as the rclone version, OS, cloud storage system, and a reproducible example with logs, which helps in understanding the issue. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior for all affected operations (only `cleanup-hidden` is demonstrated) and no discussion of potential edge cases or constraints (e.g., how these flags should interact with other backend operations or configurations). Additionally, while the user suggests a solution (`operations.SkipDestructive()`), the problem statement does not fully clarify the scope of operations needing this fix beyond the provided example. Thus, it falls short of being comprehensive but is still mostly clear.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.30) based on the following analysis of the factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are relatively localized, primarily within the `b2.go` file of the rclone backend for Backblaze B2. The modifications involve adding calls to `operations.SkipDestructive()` in specific functions related to file deletion and lifecycle rule updates. The changes are limited to a few lines of code and do not impact the broader system architecture or require extensive refactoring. The diff shows updates in two main areas: the `purge` function for handling deletions and the `lifecycleCommand` function for updating rules, indicating a narrow scope.\n\n2. **Number of Technical Concepts**: Solving this requires a basic understanding of Go programming, familiarity with rclone's codebase structure (specifically the backend module), and knowledge of how rclone handles flags like `--dry-run` and `--interactive` through the `operations.SkipDestructive()` function. These concepts are not particularly complex for someone with moderate experience in Go or rclone development. No advanced algorithms, design patterns, or domain-specific knowledge beyond cloud storage operations are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest a straightforward application of the `SkipDestructive` logic to prevent unintended deletions or modifications. There are no complex error handling requirements introduced by the changes, as the function simply checks context flags to skip operations. Potential edge cases, such as conflicts with other flags or specific B2 bucket configurations, are not addressed in the statement or code, but they do not appear to significantly increase the complexity of the fix.\n\n4. **Overall Complexity**: The task involves understanding a specific bug (flags not being honored) and applying a known solution (`SkipDestructive`) in the appropriate places. This requires some code logic comprehension to identify where destructive operations occur, but it is not a deep or intricate modification. The impact is limited to specific backend operations, and the risk of introducing new issues is low due to the simplicity of the change.\n\nGiven these points, the problem falls into the easy category (0.2-0.4), as it requires minimal changes and a moderate understanding of the codebase. A score of 0.30 reflects that while it is not a trivial fix (like changing a constant), it is still a straightforward bug fix for a developer familiar with Go and rclone.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fatal error after crlt-c kill: gap in the chain between ancients [...] and leveldb [...]\n#### System information\n\nGeth version: 1.15.6-unstable\nCL client & version: nimbus\nOS & Version: OSX\nCommit hash : 881ee4062f93eaa6d0cc95202326caa7af4f88c9\n\n#### Actual behaviour\nAfter killing geth in the init phase with a ctrl-c, it can't restart.\n\nKilled in the init phase like this:\n```\nINFO [03-26|10:54:44.566]  - Cancun:                      @1710338135 (https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/cancun.md)\nINFO [03-26|10:54:44.566] \nINFO [03-26|10:54:44.566] ---------------------------------------------------------------------------------------------------------------------------------------------------------\nINFO [03-26|10:54:44.566] \nINFO [03-26|10:54:44.567] Loaded most recent local block           number=22,130,311 hash=15955c..c22cee age=21s\nINFO [03-26|10:54:44.567] Loaded most recent local finalized block number=22,130,237 hash=b807f1..b0ba95 age=15m9s\nINFO [03-26|10:54:44.567] Loaded last snap-sync pivot marker       number=22,124,002\nWARN [03-26|10:54:44.567] Head state missing, repairing            number=22,130,311 hash=15955c..c22cee snaproot=ce4bcb..5631b6\nINFO [03-26|10:54:44.574] Rewound to block with state              number=22,129,711 hash=9c1ded..575df6\n^C\n```\n\nRestarting fails like this\n```\nINFO [03-26|11:09:38.551] Starting Geth on Ethereum mainnet...\nINFO [03-26|11:09:38.551] Bumping default cache on mainnet         provided=1024 updated=4096\nINFO [03-26|11:09:38.552] Maximum peer count                       ETH=50 total=50\nINFO [03-26|11:09:38.552] Smartcard socket not found, disabling    err=\"stat /run/pcscd/pcscd.comm: no such file or directory\"\nINFO [03-26|11:09:38.554] Set global gas cap                       cap=50,000,000\nINFO [03-26|11:09:38.561] Initializing the KZG library             backend=gokzg\nINFO [03-26|11:09:38.572] Enabling metrics collection\nINFO [03-26|11:09:38.572] Enabling stand-alone metrics HTTP endpoint address=0.0.0.0:6060\nINFO [03-26|11:09:38.572] Starting metrics server                  addr=http://0.0.0.0:6060/debug/metrics\nINFO [03-26|11:09:38.572] Allocated trie memory caches             clean=614.00MiB dirty=1024.00MiB\nINFO [03-26|11:09:38.578] Using pebble as the backing database\nINFO [03-26|11:09:38.578] Allocated cache and file handles         database=/home/user/.ethereum/geth/chaindata cache=2.00GiB handles=524,288\nINFO [03-26|11:09:39.592] Opened ancient database                  database=/home/user/.ethereum/geth/chaindata/ancient/chain readonly=false\nChain metadata\n  databaseVersion: 9 (0x9)\n  headBlockHash: 0x9c1ded300eff7bd9888ef50efe834ae4872138c499bc0cac9237c66359575df6\n  headFastBlockHash: 0x15955ca3b96163347ac361e0f5ed4f030345a95f2d66b5c0f0c7668111c22cee\n  headHeaderHash: 0xa08081168d23b5505909ff2d98547b786e27753748d8787a129bd98e9c01953a\n  lastPivotNumber: 22124002 (0x15195e2)\n  len(snapshotSyncStatus): 282 bytes\n  snapshotDisabled: false\n  snapshotJournal: 0 bytes\n  snapshotRecoveryNumber: 22130119 (0x151adc7)\n  snapshotRoot: 0xce4bcb0e32a40ead20270594392d09540d9b881396fb9198997c337fbb5631b6\n  txIndexTail: 19780311 (0x12dd2d7)\n  SkeletonSyncStatus: {\"Subchains\":[{\"Head\":22130270,\"Tail\":22129712,\"Next\":\"0x9c1ded300eff7bd9888ef50efe834ae4872138c499bc0cac9237c66359575df6\"}],\"Finalized\":22124001}\n\n\nFatal: Failed to register the Ethereum service: gap in the chain between ancients [0 - #22129824] and leveldb [#22130074 - #22130073]\n```\n\n#### Steps to reproduce the behaviour\nAlready happened twice killing geth early, but not sure how reproducible.\n\n", "patch": "diff --git a/ethdb/pebble/pebble.go b/ethdb/pebble/pebble.go\nindex b87ecb259501..969e67af5a3f 100644\n--- a/ethdb/pebble/pebble.go\n+++ b/ethdb/pebble/pebble.go\n@@ -144,7 +144,7 @@ func (l panicLogger) Fatalf(format string, args ...interface{}) {\n \n // New returns a wrapped pebble DB object. The namespace is the prefix that the\n // metrics reporting should use for surfacing internal stats.\n-func New(file string, cache int, handles int, namespace string, readonly bool) (*Database, error) {\n+func New(file string, cache int, handles int, namespace string, readonly bool, ephemeral bool) (*Database, error) {\n \t// Ensure we have some minimal caching and file guarantees\n \tif cache < minCache {\n \t\tcache = minCache\n@@ -185,7 +185,7 @@ func New(file string, cache int, handles int, namespace string, readonly bool) (\n \t\tfn:           file,\n \t\tlog:          logger,\n \t\tquitChan:     make(chan chan error),\n-\t\twriteOptions: &pebble.WriteOptions{Sync: false},\n+\t\twriteOptions: &pebble.WriteOptions{Sync: !ephemeral},\n \t}\n \topt := &pebble.Options{\n \t\t// Pebble has a single combined cache area and the write\ndiff --git a/node/database.go b/node/database.go\nindex e3ccb9106678..b7d0d856cbd6 100644\n--- a/node/database.go\n+++ b/node/database.go\n@@ -36,6 +36,11 @@ type openOptions struct {\n \tCache             int    // the capacity(in megabytes) of the data caching\n \tHandles           int    // number of files to be open simultaneously\n \tReadOnly          bool\n+\n+\t// Ephemeral means that filesystem sync operations should be avoided:\n+\t// data integrity in the face of a crash is not important. This option\n+\t// should typically be used in tests.\n+\tEphemeral bool\n }\n \n // openDatabase opens both a disk-based key-value database such as leveldb or pebble, but also\n@@ -78,7 +83,7 @@ func openKeyValueDatabase(o openOptions) (ethdb.Database, error) {\n \t}\n \tif o.Type == rawdb.DBPebble || existingDb == rawdb.DBPebble {\n \t\tlog.Info(\"Using pebble as the backing database\")\n-\t\treturn newPebbleDBDatabase(o.Directory, o.Cache, o.Handles, o.Namespace, o.ReadOnly)\n+\t\treturn newPebbleDBDatabase(o.Directory, o.Cache, o.Handles, o.Namespace, o.ReadOnly, o.Ephemeral)\n \t}\n \tif o.Type == rawdb.DBLeveldb || existingDb == rawdb.DBLeveldb {\n \t\tlog.Info(\"Using leveldb as the backing database\")\n@@ -86,7 +91,7 @@ func openKeyValueDatabase(o openOptions) (ethdb.Database, error) {\n \t}\n \t// No pre-existing database, no user-requested one either. Default to Pebble.\n \tlog.Info(\"Defaulting to pebble as the backing database\")\n-\treturn newPebbleDBDatabase(o.Directory, o.Cache, o.Handles, o.Namespace, o.ReadOnly)\n+\treturn newPebbleDBDatabase(o.Directory, o.Cache, o.Handles, o.Namespace, o.ReadOnly, o.Ephemeral)\n }\n \n // newLevelDBDatabase creates a persistent key-value database without a freezer\n@@ -102,8 +107,8 @@ func newLevelDBDatabase(file string, cache int, handles int, namespace string, r\n \n // newPebbleDBDatabase creates a persistent key-value database without a freezer\n // moving immutable chain segments into cold storage.\n-func newPebbleDBDatabase(file string, cache int, handles int, namespace string, readonly bool) (ethdb.Database, error) {\n-\tdb, err := pebble.New(file, cache, handles, namespace, readonly)\n+func newPebbleDBDatabase(file string, cache int, handles int, namespace string, readonly bool, ephemeral bool) (ethdb.Database, error) {\n+\tdb, err := pebble.New(file, cache, handles, namespace, readonly, ephemeral)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n", "instance_id": "ethereum__go-ethereum-31519", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a fatal error occurs in Geth (an Ethereum client) when it is killed with Ctrl-C during the initialization phase and fails to restart due to a gap in the chain data between the \"ancients\" and \"leveldb\" databases. The statement provides detailed logs of the actual behavior, system information, and steps to reproduce the issue, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly state the expected behavior (e.g., should Geth recover gracefully after a Ctrl-C kill?), and the reproducibility is uncertain as it mentions \"not sure how reproducible.\" Additionally, there are no specific constraints or requirements for the solution (e.g., performance expectations or specific recovery mechanisms). Edge cases are not explicitly mentioned beyond the general scenario of killing during init. Despite these minor gaps, the problem is valid and provides enough context to infer the goal of fixing the restart failure.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is relatively focused, primarily involving modifications to the database initialization logic in the Pebble backend (as seen in the diff), specifically around filesystem sync operations with the introduction of an \"Ephemeral\" flag. However, these changes impact a critical part of the system\u2014database integrity and recovery\u2014which requires a deep understanding of Geth's architecture, particularly how it handles chain data across different storage layers (\"ancients\" and \"leveldb/pebble\"). Second, the technical concepts involved are moderately complex, including knowledge of database synchronization (e.g., Pebble's WriteOptions and Sync behavior), Ethereum blockchain data structures, and crash recovery mechanisms. Third, while the code changes themselves are small (adding a parameter and toggling sync behavior), the implications are significant, as improper handling could lead to data corruption or further recovery issues. Fourth, edge cases and error handling are critical here, as the problem inherently deals with crash recovery\u2014a scenario rife with potential edge cases (e.g., partial writes, inconsistent state across database layers). The problem statement does not explicitly list these edge cases, but addressing them would require careful consideration of data consistency and state repair logic. Overall, solving this requires a solid grasp of the codebase's internals and careful testing to ensure robustness, justifying a difficulty score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Partials does not work with multiple languages and RSS\n<!--\nPlease do not use the issue queue for questions or troubleshooting. Unless you are certain that your issue is a software defect, use the forum:\n\nhttps://discourse.gohugo.io\n-->\n\n<!-- Please answer these questions before submitting your issue. Thanks! -->\n\n### What version of Hugo are you using (`hugo version`)?\n\n<pre>\nv0.146.3\n</pre>\n\n### Does this issue reproduce with the latest release?\nYes\n\n### Problem\n\nMy site is in Russian and English. The default language is Russian (defaultContentLanguage = \"ru\"). \n\nFiles: \n**/partials/comment.ru.html\n/partials/comment.en.html\n/partials/comment.ru.xml\n/partials/comment.en.xml**\n\nFragment from the template /layouts/_default/single.html:\n<pre>\n{{ partial (print \"comment.\" (default \"ru\" .Lang) \".html\") . }}\n</pre>\n\nIn version 0.145.0 everything was fine, but after the update it always displays only partial en (comment.en.html). Even in RSS I see comment.en.html (instead of comment.ru.xml or comment.en.xml).\n", "patch": "diff --git a/common/paths/pathparser.go b/common/paths/pathparser.go\nindex 4b3feaa1492..c04f2022e4f 100644\n--- a/common/paths/pathparser.go\n+++ b/common/paths/pathparser.go\n@@ -724,7 +724,7 @@ func (p *Path) IsContentData() bool {\n \treturn p.pathType == TypeContentData\n }\n \n-func (p Path) ForBundleType(t Type) *Path {\n+func (p Path) ForType(t Type) *Path {\n \tp.pathType = t\n \treturn &p\n }\ndiff --git a/hugolib/content_map_page.go b/hugolib/content_map_page.go\nindex b501cd9ea94..c51b4a17ad3 100644\n--- a/hugolib/content_map_page.go\n+++ b/hugolib/content_map_page.go\n@@ -180,7 +180,7 @@ func (t *pageTrees) collectAndMarkStaleIdentities(p *paths.Path) []identity.Iden\n \n \tif p.Component() == files.ComponentFolderContent {\n \t\t// It may also be a bundled content resource.\n-\t\tkey := p.ForBundleType(paths.TypeContentResource).Base()\n+\t\tkey := p.ForType(paths.TypeContentResource).Base()\n \t\ttree = t.treeResources\n \t\tnCount = 0\n \t\ttree.ForEeachInDimension(key, doctree.DimensionLanguage.Index(),\n", "instance_id": "gohugoio__hugo-13614", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the partials for multiple languages and RSS feeds are not rendering correctly after a Hugo version update, with the English version always being displayed instead of the appropriate language or format (HTML/XML). The goal is evident\u2014fix the language-specific partial rendering\u2014and the context of the issue (version update, file structure, and template snippet) is provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention expected behavior for RSS feeds (e.g., whether language-specific XML partials should be rendered based on the content language) beyond a general expectation. Additionally, there are no examples of the actual output versus expected output, which could help clarify the issue further. Constraints or edge cases (e.g., behavior with non-default languages or missing partials) are also not specified. Despite these minor gaps, the statement is valid and provides enough information to understand the core issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes appears limited to a specific part of the Hugo codebase, as evidenced by the diff affecting only two files (`pathparser.go` and `content_map_page.go`) with minimal lines changed (a function rename and its usage). This suggests the fix is localized and does not impact the broader system architecture significantly. However, understanding the issue requires familiarity with Hugo's internal handling of paths, content types, and language-specific rendering, which involves moderately complex concepts such as path parsing, content bundling, and resource mapping. The code changes indicate a need to grasp how path types are managed and applied in the context of content resources, which adds a layer of depth. Additionally, while the problem statement does not explicitly mention edge cases, the nature of language and format handling (HTML vs. XML for RSS) implies potential complexities in ensuring correct partial selection across different contexts and configurations (e.g., missing partials, non-default languages, or custom output formats). Error handling does not seem to be a significant focus based on the provided diff, but ensuring robustness in path type handling might require some consideration. Overall, solving this requires understanding multiple concepts and making targeted but non-trivial modifications, justifying a difficulty score of 0.55, which sits in the medium range (0.4-0.6) due to the balance between localized changes and the need for specific domain knowledge within Hugo's codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Trim trailing newlines for normal blockquotes too, to mirror callout blockquotes\n### What version of Hugo are you using (`hugo version`)?\n\n<pre>\nhugo v0.142.0+extended+withdeploy linux/amd64 BuildDate=unknown\n</pre>\n\n### Description\n\nRelated: https://github.com/gohugoio/hugo/issues/13301\n\nCurrently, trailing newlines are removed from blockquotes with callouts, but are kept for normal blockquotes, e.g.\n\n```\n<blockquote>\n\t<p>Example</p>\n\n</blockquote>\n\n<blockquote>\n\t<p>Example</p>\n</blockquote>\n\n```\n\nThis is slightly confusing (in that sense that it's unintuitive why they'd differ), even if easy enough to mitigate by reimplementing the trimming in the `render-blockquotes.html` file, so it'd be nice if both types of blockquotes mirrored each other for more consistent HTML output. It's pretty minor, but without a built-in way to prettify HTML in Hugo (related issue: https://github.com/gohugoio/hugo/issues/7190), it is something users will need to keep in mind when wanting to achieve clean & mostly reproducible output\n", "patch": "diff --git a/markup/goldmark/blockquotes/blockquotes.go b/markup/goldmark/blockquotes/blockquotes.go\nindex 064200d5e9f..539cd187590 100644\n--- a/markup/goldmark/blockquotes/blockquotes.go\n+++ b/markup/goldmark/blockquotes/blockquotes.go\n@@ -69,7 +69,7 @@ func (r *htmlRenderer) renderBlockquote(w util.BufWriter, src []byte, node ast.N\n \t\treturn ast.WalkContinue, nil\n \t}\n \n-\ttext := ctx.PopRenderedString()\n+\ttext := strings.TrimSpace(ctx.PopRenderedString())\n \n \tordinal := ctx.GetAndIncrementOrdinal(ast.KindBlockquote)\n \n@@ -90,7 +90,7 @@ func (r *htmlRenderer) renderBlockquote(w util.BufWriter, src []byte, node ast.N\n \t\t// tag if the first line of the blockquote content does not have a\n \t\t// closing p tag. At some point we might want to move this to the\n \t\t// parser.\n-\t\tbefore, after, found := strings.Cut(strings.TrimSpace(text), \"\\n\")\n+\t\tbefore, after, found := strings.Cut(text, \"\\n\")\n \t\tif found {\n \t\t\tif strings.HasSuffix(before, \"</p>\") {\n \t\t\t\ttext = after\n", "instance_id": "gohugoio__hugo-13306", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the goal of achieving consistent behavior between normal blockquotes and callout blockquotes by trimming trailing newlines in normal blockquotes. It provides context about the current inconsistency and references related issues for additional background. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what \"trailing newlines\" means in all contexts (e.g., multiple newlines or just one), nor does it specify any edge cases or constraints around the HTML structure that might affect the trimming behavior. Additionally, while examples are provided, they are not exhaustive in demonstrating potential variations in input. Overall, the intent is understandable, but some minor clarifications would make it more comprehensive.", "difficulty_explanation": "The difficulty of this problem is on the lower end of the spectrum, as it involves a relatively straightforward modification to the existing codebase. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes**: The changes are localized to a single file (`blockquotes.go`) and involve minimal lines of code. The modification primarily consists of adding `strings.TrimSpace()` to remove whitespace and adjusting a related string operation. There is no impact on the broader system architecture or interactions with other modules, making the scope very narrow.\n\n2. **Number of Technical Concepts**: The solution requires basic familiarity with Go's standard library (specifically the `strings` package) and an understanding of string manipulation. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond basic HTML rendering logic in the context of Hugo's markup processing.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest consideration of multi-line blockquotes (via the `strings.Cut` operation). However, the complexity of handling such cases is low, as the logic is already partially in place, and the modification does not introduce significant new error handling requirements.\n\n4. **Overall Complexity**: The task is a simple bug fix or minor feature enhancement that does not require deep understanding of the Hugo codebase beyond the specific rendering logic for blockquotes. It can be accomplished with basic Go programming skills and minimal debugging.\n\nGiven these points, I rate the difficulty as 0.25, placing it in the \"Easy\" category. It requires understanding some code logic and making a simple modification to achieve the desired consistency in HTML output.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Improve .Page.Fragments.ToHTML\nThe [`Fragments.ToHTML`](https://github.com/gohugoio/hugo/blob/487bb96474363070a9b5b22ce4640f80329e91e4/markup/tableofcontents/tableofcontents.go#L136-L149) method on `Page` takes three arguments:\r\n\r\n```text\r\nfunc (toc *Fragments) ToHTML(startLevel, stopLevel int, ordered bool) template.HTML {\r\n  ...\r\n}\r\n```\r\n\r\nIt would be convenient to pass site and/or page parameters to this method, something like:\r\n\r\n```text\r\n{{ .Fragments.ToHTML (.Param \"toc.startLevel\") (.Param \"toc.endLevel\") true }}\r\n```\r\n\r\nThe above throws an error with JSON and TOML site/page parameters because the values are unmarshaled to `float64` (JSON) or `int64` (TOML). You can work around this with:\r\n\r\n```text\r\n{{ .Fragments.ToHTML (.Param \"toc.startLevel\" | int) (.Param \"toc.endLevel\" | int) true }}\r\n```\r\n\r\nBut it would easier if the method signature were...\r\n\r\n```text\r\nfunc (toc *Fragments) ToHTML(startLevel, stopLevel any, ordered bool) template.HTML {\r\n  ...\r\n}\r\n```\r\n\r\n... with `cast.ToIntE`, etc.\r\n\r\n\r\n\r\n\r\n\n", "patch": "diff --git a/hugolib/page__content.go b/hugolib/page__content.go\nindex b5527a28103..f7579f182f8 100644\n--- a/hugolib/page__content.go\n+++ b/hugolib/page__content.go\n@@ -730,16 +730,15 @@ func (c *cachedContentScope) contentToC(ctx context.Context) (contentTableOfCont\n \t\tisHTML := cp.po.p.m.pageConfig.ContentMediaType.IsHTML()\n \n \t\tif !isHTML {\n-\t\t\tcreateAndSetToC := func(tocProvider converter.TableOfContentsProvider) {\n+\t\t\tcreateAndSetToC := func(tocProvider converter.TableOfContentsProvider) error {\n \t\t\t\tcfg := p.s.ContentSpec.Converters.GetMarkupConfig()\n \t\t\t\tct.tableOfContents = tocProvider.TableOfContents()\n-\t\t\t\tct.tableOfContentsHTML = template.HTML(\n-\t\t\t\t\tct.tableOfContents.ToHTML(\n-\t\t\t\t\t\tcfg.TableOfContents.StartLevel,\n-\t\t\t\t\t\tcfg.TableOfContents.EndLevel,\n-\t\t\t\t\t\tcfg.TableOfContents.Ordered,\n-\t\t\t\t\t),\n+\t\t\t\tct.tableOfContentsHTML, err = ct.tableOfContents.ToHTML(\n+\t\t\t\t\tcfg.TableOfContents.StartLevel,\n+\t\t\t\t\tcfg.TableOfContents.EndLevel,\n+\t\t\t\t\tcfg.TableOfContents.Ordered,\n \t\t\t\t)\n+\t\t\t\treturn err\n \t\t\t}\n \n \t\t\t// If the converter supports doing the parsing separately, we do that.\ndiff --git a/markup/tableofcontents/tableofcontents.go b/markup/tableofcontents/tableofcontents.go\nindex 49a9cdeb79c..560e421b7e8 100644\n--- a/markup/tableofcontents/tableofcontents.go\n+++ b/markup/tableofcontents/tableofcontents.go\n@@ -14,11 +14,13 @@\n package tableofcontents\n \n import (\n+\t\"fmt\"\n \t\"html/template\"\n \t\"sort\"\n \t\"strings\"\n \n \t\"github.com/gohugoio/hugo/common/collections\"\n+\t\"github.com/spf13/cast\"\n )\n \n // Empty is an empty ToC.\n@@ -133,19 +135,30 @@ func (toc *Fragments) addAt(h *Heading, row, level int) {\n }\n \n // ToHTML renders the ToC as HTML.\n-func (toc *Fragments) ToHTML(startLevel, stopLevel int, ordered bool) template.HTML {\n+func (toc *Fragments) ToHTML(startLevel, stopLevel any, ordered bool) (template.HTML, error) {\n \tif toc == nil {\n-\t\treturn \"\"\n+\t\treturn \"\", nil\n \t}\n+\n+\tiStartLevel, err := cast.ToIntE(startLevel)\n+\tif err != nil {\n+\t\treturn \"\", fmt.Errorf(\"startLevel: %w\", err)\n+\t}\n+\n+\tiStopLevel, err := cast.ToIntE(stopLevel)\n+\tif err != nil {\n+\t\treturn \"\", fmt.Errorf(\"stopLevel: %w\", err)\n+\t}\n+\n \tb := &tocBuilder{\n \t\ts:          strings.Builder{},\n \t\th:          toc.Headings,\n-\t\tstartLevel: startLevel,\n-\t\tstopLevel:  stopLevel,\n+\t\tstartLevel: iStartLevel,\n+\t\tstopLevel:  iStopLevel,\n \t\tordered:    ordered,\n \t}\n \tb.Build()\n-\treturn template.HTML(b.s.String())\n+\treturn template.HTML(b.s.String()), nil\n }\n \n func (toc Fragments) walk(fn func(*Heading)) {\n", "instance_id": "gohugoio__hugo-13108", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the goal of improving the `Fragments.ToHTML` method to handle different input types for `startLevel` and `stopLevel` parameters by changing their type from `int` to `any` and using type casting. It provides a specific example of the issue (JSON/TOML unmarshaling to `float64` or `int64`) and suggests a solution using `cast.ToIntE`. However, there are minor ambiguities: the statement does not explicitly mention how errors from type casting should be handled in the broader context of the application, nor does it discuss potential edge cases (e.g., invalid input types or values). Additionally, while the desired method signature change is clear, there is no mention of performance implications or compatibility concerns with existing code. Overall, the problem is valid and mostly clear, but these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting the `Fragments.ToHTML` method signature and its implementation in `markup/tableofcontents/tableofcontents.go`, along with a related update in `hugolib/page__content.go` to handle the new return value (error). The modifications involve a small amount of code across two files and do not impact the broader system architecture.\n\n2. **Technical Concepts Involved**: The solution requires understanding of Go's type system (changing parameter types to `any` and using type assertions/casting with `cast.ToIntE`), basic error handling (adding error returns), and familiarity with the Hugo framework's template system. These concepts are relatively straightforward for a developer with moderate Go experience.\n\n3. **Edge Cases and Error Handling**: The problem introduces error handling for type casting failures, which is evident in the code changes (returning errors with descriptive messages). However, the complexity of edge cases is low\u2014primarily dealing with invalid input types for `startLevel` and `stopLevel`. There are no complex edge cases or performance considerations mentioned or implied.\n\n4. **Overall Complexity**: The task involves simple modifications to function signatures and adding basic error handling logic. It requires understanding the immediate context of the `ToHTML` method but does not necessitate deep knowledge of the entire Hugo codebase or complex interactions between modules.\n\nGiven these points, a difficulty score of 0.3 reflects the simplicity of the changes, the limited scope, and the basic technical concepts involved, making it an easy task for a developer familiar with Go.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Alias Page.Scratch to Page.Store \nThis was prompted by:\r\n<https://discourse.gohugo.io/t/strange-behavior-with-scratch-variable/52288>\r\n\r\nThat topic is not unique. I estimate this comes up every couple of months.\r\n\r\nDeprecating `Page.Scratch` would generate a lot of noise given the number of templates that still use a `Page.Scratch` instead of a variable. This is a hangover from [v0.48.0](https://gohugo.io/news/0.48-relnotes/) and earlier where you could not assign a new value to a variable once it had been initialized. So aliasing one to the other seems like a kinder approach.\r\n\r\nWe would also update the `Page.Scratch` documentation, noting that you should use `Page.Store` instead and that `Page.Scratch` will probably be removed at some point in the future... sort of a soft deprecation.\r\n\r\n\r\nThe shortcode-scoped Scratch would remain as-is.\n", "patch": "diff --git a/common/maps/scratch.go b/common/maps/scratch.go\nindex e9f412540b2..6383772164a 100644\n--- a/common/maps/scratch.go\n+++ b/common/maps/scratch.go\n@@ -28,25 +28,6 @@ type Scratch struct {\n \tmu     sync.RWMutex\n }\n \n-// Scratcher provides a scratching service.\n-type Scratcher interface {\n-\t// Scratch returns a \"scratch pad\" that can be used to store state.\n-\tScratch() *Scratch\n-}\n-\n-type scratcher struct {\n-\ts *Scratch\n-}\n-\n-func (s scratcher) Scratch() *Scratch {\n-\treturn s.s\n-}\n-\n-// NewScratcher creates a new Scratcher.\n-func NewScratcher() Scratcher {\n-\treturn scratcher{s: NewScratch()}\n-}\n-\n // Add will, for single values, add (using the + operator) the addend to the existing addend (if found).\n // Supports numeric values and strings.\n //\ndiff --git a/hugolib/page.go b/hugolib/page.go\nindex 2bc1da044b4..e4c841966b2 100644\n--- a/hugolib/page.go\n+++ b/hugolib/page.go\n@@ -38,7 +38,6 @@ import (\n \t\"github.com/gohugoio/hugo/tpl\"\n \n \t\"github.com/gohugoio/hugo/common/herrors\"\n-\t\"github.com/gohugoio/hugo/common/maps\"\n \t\"github.com/gohugoio/hugo/common/types\"\n \n \t\"github.com/gohugoio/hugo/source\"\n@@ -149,7 +148,7 @@ func (p *pageState) Key() string {\n }\n \n func (p *pageState) resetBuildState() {\n-\tp.Scratcher = maps.NewScratcher()\n+\t// Nothing to do for now.\n }\n \n func (p *pageState) reusePageOutputContent() bool {\ndiff --git a/hugolib/page__common.go b/hugolib/page__common.go\nindex d3b0bd112dd..55465e214ec 100644\n--- a/hugolib/page__common.go\n+++ b/hugolib/page__common.go\n@@ -56,7 +56,6 @@ type pageCommon struct {\n \tstore *maps.Scratch\n \n \t// All of these represents the common parts of a page.Page\n-\tmaps.Scratcher\n \tnavigation.PageMenusProvider\n \tpage.AuthorProvider\n \tpage.AlternativeOutputFormatsProvider\n@@ -113,3 +112,8 @@ type pageCommon struct {\n func (p *pageCommon) Store() *maps.Scratch {\n \treturn p.store\n }\n+\n+// See issue 13016.\n+func (p *pageCommon) Scratch() *maps.Scratch {\n+\treturn p.Store()\n+}\ndiff --git a/hugolib/page__new.go b/hugolib/page__new.go\nindex 9a4972d07fd..9a11fa8892b 100644\n--- a/hugolib/page__new.go\n+++ b/hugolib/page__new.go\n@@ -184,7 +184,6 @@ func (h *HugoSites) doNewPage(m *pageMeta) (*pageState, *paths.Path, error) {\n \t\t\tpageCommon: &pageCommon{\n \t\t\t\tFileProvider:              m,\n \t\t\t\tAuthorProvider:            m,\n-\t\t\t\tScratcher:                 maps.NewScratcher(),\n \t\t\t\tstore:                     maps.NewScratch(),\n \t\t\t\tPositioner:                page.NopPage,\n \t\t\t\tInSectionPositioner:       page.NopPage,\ndiff --git a/resources/page/page.go b/resources/page/page.go\nindex 20525669c1e..ea7f4bf1bd7 100644\n--- a/resources/page/page.go\n+++ b/resources/page/page.go\n@@ -327,7 +327,9 @@ type PageWithoutContent interface {\n \n \t// Scratch returns a Scratch that can be used to store temporary state.\n \t// Note that this Scratch gets reset on server rebuilds. See Store() for a variant that survives.\n-\tmaps.Scratcher\n+\t// Scratch returns a \"scratch pad\" that can be used to store state.\n+\t// Deprecated: From Hugo v0.138.0 this is just an alias for Store.\n+\tScratch() *maps.Scratch\n \n \t// Store returns a Scratch that can be used to store temporary state.\n \t// In contrast to Scratch(), this Scratch is not reset on server rebuilds.\n", "instance_id": "gohugoio__hugo-13017", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent to alias `Page.Scratch` to `Page.Store` as a soft deprecation strategy for `Page.Scratch` in the Hugo framework. It provides context about why this change is necessary, referencing user confusion and historical reasons for the dual naming. The goal of the change (aliasing one method to another) and the scope (specific to `Page.Scratch` while leaving shortcode-scoped Scratch unchanged) are defined. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases or compatibility issues that might arise from this aliasing, such as existing templates or plugins that rely on distinct behaviors of `Scratch` and `Store`. Additionally, while the intent to update documentation is mentioned, specific details about how this will be communicated or enforced are absent. Overall, the statement is valid and clear but lacks comprehensive coverage of potential challenges or side effects, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the relatively straightforward nature of the code changes required. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes involve multiple files (`scratch.go`, `page.go`, `page__common.go`, `page__new.go`, and `page.go` in the resources directory), but the modifications are mostly localized and repetitive. The primary task is to alias `Scratch()` to `Store()` by adding a method that delegates to `Store()` and removing or updating references to the old `Scratcher` interface and its implementations. The changes do not significantly impact the system's architecture; they are more of a refactoring for naming consistency and deprecation signaling. The amount of code change is moderate, involving deletions of obsolete code and additions of aliasing logic, but it does not require deep restructuring.\n\n2. **Number of Technical Concepts:** The problem requires a basic understanding of Go language features such as interfaces, method delegation, and struct composition. Familiarity with the Hugo codebase structure (e.g., how `Page` and `Scratch` are used) is necessary but not overly complex for someone with moderate experience in Go or similar projects. No advanced algorithms, design patterns, or domain-specific knowledge beyond Hugo's templating system are needed. The concept of deprecation and aliasing is straightforward.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not introduce new error handling logic. However, there is an implicit need to ensure backward compatibility with existing templates that use `Page.Scratch`. The aliasing approach mitigates potential breakage, but a developer would need to consider if there are subtle behavioral differences between `Scratch` and `Store` that might surface as edge cases. This aspect adds minor complexity but does not significantly elevate the difficulty.\n\n4. **Overall Assessment:** The task requires understanding some code logic and making simple modifications across a few files. It does not involve complex interactions between modules beyond the immediate `Page` and `Scratch` components. The impact is limited to renaming and aliasing, with no performance or architectural redesign considerations. A score of 0.30 reflects an \"Easy\" problem that a developer with intermediate Go skills and basic familiarity with the Hugo codebase could handle without significant challenges. It is slightly above the \"Very Easy\" range due to the multi-file changes and the need to understand the context of deprecation in a larger framework.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "New CLI command: hugo build\nDoes what `hugo` does now, just to clarify intent of the command. This is purely cosmetic, but keeps things consistent.\n", "patch": "diff --git a/commands/commandeer.go b/commands/commandeer.go\nindex e8cde2114f3..841e8d81cba 100644\n--- a/commands/commandeer.go\n+++ b/commands/commandeer.go\n@@ -499,16 +499,26 @@ func (r *rootCommand) IsTestRun() bool {\n }\n \n func (r *rootCommand) Init(cd *simplecobra.Commandeer) error {\n+\treturn r.initRootCommand(\"\", cd)\n+}\n+\n+func (r *rootCommand) initRootCommand(subCommandName string, cd *simplecobra.Commandeer) error {\n \tcmd := cd.CobraCommand\n-\tcmd.Use = \"hugo [flags]\"\n-\tcmd.Short = \"hugo builds your site\"\n-\tcmd.Long = `hugo is the main command, used to build your Hugo site.\n+\tcommandName := \"hugo\"\n+\tif subCommandName != \"\" {\n+\t\tcommandName = subCommandName\n+\t}\n+\tcmd.Use = fmt.Sprintf(\"%s [flags]\", commandName)\n+\tcmd.Short = fmt.Sprintf(\"%s builds your site\", commandName)\n+\tcmd.Long = `COMMAND_NAME is the main command, used to build your Hugo site.\n \n Hugo is a Fast and Flexible Static Site Generator\n built with love by spf13 and friends in Go.\n \n Complete documentation is available at https://gohugo.io/.`\n \n+\tcmd.Long = strings.ReplaceAll(cmd.Long, \"COMMAND_NAME\", commandName)\n+\n \t// Configure persistent flags\n \tcmd.PersistentFlags().StringVarP(&r.source, \"source\", \"s\", \"\", \"filesystem path to read files relative from\")\n \t_ = cmd.MarkFlagDirname(\"source\")\ndiff --git a/commands/commands.go b/commands/commands.go\nindex e21d743ab90..10ab106e277 100644\n--- a/commands/commands.go\n+++ b/commands/commands.go\n@@ -14,6 +14,8 @@\n package commands\n \n import (\n+\t\"context\"\n+\n \t\"github.com/bep/simplecobra\"\n )\n \n@@ -21,6 +23,7 @@ import (\n func newExec() (*simplecobra.Exec, error) {\n \trootCmd := &rootCommand{\n \t\tcommands: []simplecobra.Commander{\n+\t\t\tnewHugoBuildCmd(),\n \t\t\tnewVersionCmd(),\n \t\t\tnewEnvCommand(),\n \t\t\tnewServerCommand(),\n@@ -38,3 +41,33 @@ func newExec() (*simplecobra.Exec, error) {\n \n \treturn simplecobra.New(rootCmd)\n }\n+\n+func newHugoBuildCmd() simplecobra.Commander {\n+\treturn &hugoBuildCommand{}\n+}\n+\n+// hugoBuildCommand just delegates to the rootCommand.\n+type hugoBuildCommand struct {\n+\trootCmd *rootCommand\n+}\n+\n+func (c *hugoBuildCommand) Commands() []simplecobra.Commander {\n+\treturn nil\n+}\n+\n+func (c *hugoBuildCommand) Name() string {\n+\treturn \"build\"\n+}\n+\n+func (c *hugoBuildCommand) Init(cd *simplecobra.Commandeer) error {\n+\tc.rootCmd = cd.Root.Command.(*rootCommand)\n+\treturn c.rootCmd.initRootCommand(\"build\", cd)\n+}\n+\n+func (c *hugoBuildCommand) PreRun(cd, runner *simplecobra.Commandeer) error {\n+\treturn c.rootCmd.PreRun(cd, runner)\n+}\n+\n+func (c *hugoBuildCommand) Run(ctx context.Context, cd *simplecobra.Commandeer, args []string) error {\n+\treturn c.rootCmd.Run(ctx, cd, args)\n+}\ndiff --git a/docs/content/en/commands/hugo.md b/docs/content/en/commands/hugo.md\nindex cfbe66053fd..badd6d7eacf 100644\n--- a/docs/content/en/commands/hugo.md\n+++ b/docs/content/en/commands/hugo.md\n@@ -70,6 +70,7 @@ hugo [flags]\n \n ### SEE ALSO\n \n+* [hugo build](/commands/hugo_build/)\t - build builds your site\n * [hugo completion](/commands/hugo_completion/)\t - Generate the autocompletion script for the specified shell\n * [hugo config](/commands/hugo_config/)\t - Print the site configuration\n * [hugo convert](/commands/hugo_convert/)\t - Convert your content to different formats\ndiff --git a/docs/content/en/commands/hugo_build.md b/docs/content/en/commands/hugo_build.md\nnew file mode 100644\nindex 00000000000..72686cb9be2\n--- /dev/null\n+++ b/docs/content/en/commands/hugo_build.md\n@@ -0,0 +1,74 @@\n+---\n+title: \"hugo build\"\n+slug: hugo_build\n+url: /commands/hugo_build/\n+---\n+## hugo build\n+\n+build builds your site\n+\n+### Synopsis\n+\n+build is the main command, used to build your Hugo site.\n+\n+Hugo is a Fast and Flexible Static Site Generator\n+built with love by spf13 and friends in Go.\n+\n+Complete documentation is available at https://gohugo.io/.\n+\n+```\n+hugo build [flags]\n+```\n+\n+### Options\n+\n+```\n+  -b, --baseURL string             hostname (and path) to the root, e.g. https://spf13.com/\n+  -D, --buildDrafts                include content marked as draft\n+  -E, --buildExpired               include expired content\n+  -F, --buildFuture                include content with publishdate in the future\n+      --cacheDir string            filesystem path to cache directory\n+      --cleanDestinationDir        remove files from destination not found in static directories\n+      --clock string               set the clock used by Hugo, e.g. --clock 2021-11-06T22:30:00.00+09:00\n+      --config string              config file (default is hugo.yaml|json|toml)\n+      --configDir string           config dir (default \"config\")\n+  -c, --contentDir string          filesystem path to content directory\n+      --debug                      debug output\n+  -d, --destination string         filesystem path to write files to\n+      --disableKinds strings       disable different kind of pages (home, RSS etc.)\n+      --enableGitInfo              add Git revision, date, author, and CODEOWNERS info to the pages\n+  -e, --environment string         build environment\n+      --forceSyncStatic            copy all files when static is changed.\n+      --gc                         enable to run some cleanup tasks (remove unused cache files) after the build\n+  -h, --help                       help for build\n+      --ignoreCache                ignores the cache directory\n+      --ignoreVendorPaths string   ignores any _vendor for module paths matching the given Glob pattern\n+  -l, --layoutDir string           filesystem path to layout directory\n+      --logLevel string            log level (debug|info|warn|error)\n+      --minify                     minify any supported output format (HTML, XML etc.)\n+      --noBuildLock                don't create .hugo_build.lock file\n+      --noChmod                    don't sync permission mode of files\n+      --noTimes                    don't sync modification time of files\n+      --panicOnWarning             panic on first WARNING log\n+      --poll string                set this to a poll interval, e.g --poll 700ms, to use a poll based approach to watch for file system changes\n+      --printI18nWarnings          print missing translations\n+      --printMemoryUsage           print memory usage to screen at intervals\n+      --printPathWarnings          print warnings on duplicate target paths etc.\n+      --printUnusedTemplates       print warnings on unused templates.\n+      --quiet                      build in quiet mode\n+      --renderSegments strings     named segments to render (configured in the segments config)\n+  -M, --renderToMemory             render to memory (mostly useful when running the server)\n+  -s, --source string              filesystem path to read files relative from\n+      --templateMetrics            display metrics about template executions\n+      --templateMetricsHints       calculate some improvement hints when combined with --templateMetrics\n+  -t, --theme strings              themes to use (located in /themes/THEMENAME/)\n+      --themesDir string           filesystem path to themes directory\n+      --trace file                 write trace to file (not useful in general)\n+  -v, --verbose                    verbose output\n+  -w, --watch                      watch filesystem for changes and recreate as needed\n+```\n+\n+### SEE ALSO\n+\n+* [hugo](/commands/hugo/)\t - hugo builds your site\n+\n", "instance_id": "gohugoio__hugo-12891", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: it describes the addition of a new CLI command \"hugo build\" that mirrors the functionality of the existing \"hugo\" command for consistency and clarity of intent. The goal is explicitly stated as cosmetic, which helps in understanding the purpose. However, the statement lacks depth in terms of specific requirements or constraints, such as how the command should interact with existing commands or any specific user scenarios to consider. Additionally, there are no examples or detailed expectations provided for the implementation beyond the general idea of consistency. While the code changes provide context, the problem statement itself does not address potential edge cases or integration challenges, leaving minor ambiguities. Hence, it is rated as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem is rated as Easy with a score of 0.25. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, affecting a few files (`commandeer.go`, `commands.go`, and documentation files). The modifications involve adding a new command (`hugo build`) that delegates to the existing root command logic, requiring minimal new code. The changes do not impact the system's architecture significantly, as they are purely additive and cosmetic, reusing existing functionality. The amount of code change is small, with straightforward updates to command initialization and documentation.\n\n2. **Number of Technical Concepts:** The problem requires understanding of Go programming, specifically the use of the `simplecobra` library for CLI command handling. The concepts involved are basic, such as command initialization, string formatting for command descriptions, and delegation of functionality. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond familiarity with CLI tool development in Go, which is relatively straightforward for an experienced developer.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention any specific edge cases or error conditions, and the code changes do not introduce new error handling logic. Since the new command delegates to the existing root command, any edge cases or errors are likely already handled by the existing codebase. The cosmetic nature of the change further reduces the likelihood of complex edge case considerations.\n\n4. **Overall Complexity:** The task involves simple modifications to add a new command alias without altering core functionality. It requires understanding some code logic to ensure proper delegation and consistency in command naming, but the changes are not complex or far-reaching. This aligns with the \"Easy\" difficulty range (0.2-0.4), as it involves basic feature addition with minimal risk or complexity.\n\nGiven these factors, a score of 0.25 reflects the simplicity of the task, requiring only basic modifications and a moderate understanding of the CLI framework used in the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "transform.Plainify should return template.HTML instead of a string\n<!-- Please answer these questions before submitting your issue. Thanks! -->\r\n\r\n### What version of Hugo are you using (`hugo version`)?\r\n\r\n<pre>\r\n$ hugo version\r\nhugo v0.84.0+extended linux/amd64 BuildDate=unknown\r\n</pre>\r\n\r\n### Does this issue reproduce with the latest release?\r\n\r\nThis is the latest release.\r\n\r\nWhen using `plainify` hugo many times does the wrong thing. I realize that this is a feature, but I wonder if there is a better way to do it. Specifically, if the HTML contains any HTML entities `plainify` returns the plain HTML with the entities but not marked as a HTML safe string. So the `plainify` output for HTML like `<div>This &amp; that</div>` will be `This &amp; that`. Now it's easy to pipe it through `safeHTML` or `markdownify` or `htmlUnescape` to solve the problem, but it seems `plainify` should probably include an automatic `htmlUnescape` or at least mark the output as HTML safe.\r\nThis is a petty kind of thing, but in our code, we always must do `plainify | htmlUnescape` and whenever we forget it the entities are shown on the screen. Seems cumbersome.\n", "patch": "diff --git a/docs/content/en/functions/transform/Plainify.md b/docs/content/en/functions/transform/Plainify.md\nindex 040145170b8..681d41f72e2 100644\n--- a/docs/content/en/functions/transform/Plainify.md\n+++ b/docs/content/en/functions/transform/Plainify.md\n@@ -6,7 +6,7 @@ keywords: []\n action:\n   aliases: [plainify]\n   related: []\n-  returnType: string\n+  returnType: template.HTML\n   signatures: [transform.Plainify INPUT]\n aliases: [/functions/plainify]\n ---\ndiff --git a/tpl/transform/transform.go b/tpl/transform/transform.go\nindex db7703b7fb5..10a91671d54 100644\n--- a/tpl/transform/transform.go\n+++ b/tpl/transform/transform.go\n@@ -54,7 +54,7 @@ func New(deps *deps.Deps) *Namespace {\n \t\t\t\"/tmpl/transform/unmarshal\",\n \t\t\tdynacache.OptionsPartition{Weight: 30, ClearWhen: dynacache.ClearOnChange},\n \t\t),\n-\t\tcacheMath: dynacache.GetOrCreatePartition[string, string](\n+\t\tcacheMath: dynacache.GetOrCreatePartition[string, template.HTML](\n \t\t\tdeps.MemCache,\n \t\t\t\"/tmpl/transform/math\",\n \t\t\tdynacache.OptionsPartition{Weight: 30, ClearWhen: dynacache.ClearNever},\n@@ -65,7 +65,7 @@ func New(deps *deps.Deps) *Namespace {\n // Namespace provides template functions for the \"transform\" namespace.\n type Namespace struct {\n \tcacheUnmarshal *dynacache.Partition[string, *resources.StaleValue[any]]\n-\tcacheMath      *dynacache.Partition[string, string]\n+\tcacheMath      *dynacache.Partition[string, template.HTML]\n \n \tid   atomic.Uint32\n \tdeps *deps.Deps\n@@ -188,18 +188,18 @@ func (ns *Namespace) Markdownify(ctx context.Context, s any) (template.HTML, err\n }\n \n // Plainify returns a copy of s with all HTML tags removed.\n-func (ns *Namespace) Plainify(s any) (string, error) {\n+func (ns *Namespace) Plainify(s any) (template.HTML, error) {\n \tss, err := cast.ToStringE(s)\n \tif err != nil {\n \t\treturn \"\", err\n \t}\n \n-\treturn tpl.StripHTML(ss), nil\n+\treturn template.HTML(tpl.StripHTML(ss)), nil\n }\n \n // ToMath converts a LaTeX string to math in the given format, default MathML.\n // This uses KaTeX to render the math, see https://katex.org/.\n-func (ns *Namespace) ToMath(ctx context.Context, args ...any) (string, error) {\n+func (ns *Namespace) ToMath(ctx context.Context, args ...any) (template.HTML, error) {\n \tif len(args) < 1 {\n \t\treturn \"\", errors.New(\"must provide at least one argument\")\n \t}\n@@ -226,7 +226,7 @@ func (ns *Namespace) ToMath(ctx context.Context, args ...any) (string, error) {\n \tkey := \"tomath/\" + s[:2] + \"/\" + s[2:]\n \tfileCache := ns.deps.ResourceSpec.FileCaches.MiscCache()\n \n-\treturn ns.cacheMath.GetOrCreate(key, func(string) (string, error) {\n+\treturn ns.cacheMath.GetOrCreate(key, func(string) (template.HTML, error) {\n \t\t_, r, err := fileCache.GetOrCreate(key, func() (io.ReadCloser, error) {\n \t\t\tmessage := warpc.Message[warpc.KatexInput]{\n \t\t\t\tHeader: warpc.Header{\n@@ -250,7 +250,9 @@ func (ns *Namespace) ToMath(ctx context.Context, args ...any) (string, error) {\n \t\t\treturn \"\", err\n \t\t}\n \n-\t\treturn hugio.ReadString(r)\n+\t\ts, err := hugio.ReadString(r)\n+\n+\t\treturn template.HTML(s), err\n \t})\n }\n \n", "instance_id": "gohugoio__hugo-12744", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `plainify` function in Hugo. It explains the current behavior (returning plain text with HTML entities unescaped but not marked as HTML safe) and the desired outcome (returning `template.HTML` to mark the output as safe or automatically unescaping entities). However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define whether the goal is solely to change the return type to `template.HTML` or if additional logic (like automatic `htmlUnescape`) is expected. It also lacks specific examples of edge cases or constraints, such as how the function should behave with malformed HTML or nested tags. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, primarily affecting the `Plainify` function and a related `ToMath` function in a single file (`tpl/transform/transform.go`), along with a documentation update. The modifications involve changing return types from `string` to `template.HTML` and updating associated cache types. There is no significant impact on the broader system architecture or interactions between multiple modules, as the change is mostly a type adjustment with minimal logic alteration.\n\n2. **Technical Concepts Involved:** The problem requires basic familiarity with Go's type system, specifically understanding `template.HTML` as a type used in Hugo to mark strings as safe for HTML rendering. It also involves minor knowledge of Hugo's templating system and caching mechanism (`dynacache`). These concepts are straightforward for someone with intermediate Go experience and do not involve complex algorithms, design patterns, or domain-specific knowledge beyond Hugo's templating context.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases or new error handling requirements. The code changes do not introduce additional error handling logic beyond what already exists. However, there is a minor implicit concern about how `template.HTML` might affect downstream rendering if consumers of `Plainify` are not expecting HTML-safe output, but this is not addressed in the changes or statement.\n\n4. **Overall Complexity:** The task is primarily a type conversion with corresponding updates to function signatures and cache definitions. It does not require deep understanding of the codebase beyond the immediate context of the `transform` namespace. The amount of code change is small, and the impact is limited to ensuring type consistency.\n\nGiven these points, a difficulty score of 0.30 reflects a problem that is easy to solve with basic to intermediate knowledge of Go and Hugo's templating system, involving simple modifications without significant complexity or risk of broader impact.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Improve integration with cloud static website hosting\n**Proposal**: Add an option to remap files named `<dir>/index.html` to `<dir>/` during deployment.\r\n\r\n## Background\r\n\r\nTraditional web servers (e.g. Apache, nginx) are typically configured to serve `index.html` files when users visit a directory. However, blob storage systems (e.g. S3, GCS) let you create files with names ending in `/`, and this is sometimes preferred.\r\n\r\nFor example, let\u2019s say you deploy your site to an S3 bucket and CloudFront distribution. The simplest way to do this is to just use an S3 origin for your CloudFront distribution. However, this simple setup doesn\u2019t work with Hugo out of the box\u2014your website root `/` will correctly map to the root `index.html` file, but subdirectories will not work. You will get a 404 error for `dir/`, because CloudFront will look for an S3 object named `dir/` instead of `dir/index.html`.\r\n\r\nThere are some workarounds for this:\r\n\r\n* You can enable \u201cstatic website hosting\u201d for your S3 bucket, set the index document to `index.html`, and configure CloudFront to use an HTTP origin that points to the S3 bucket. However, this prevents you from using the normal authentication & authorization systems that you would otherwise get, so this option only works for sites that are public.\r\n\r\n* You can use a CloudFront function to remap requests to `dir/` to request `dir/index.html`, but this adds complexity and code to what would otherwise be a no-code configuration. See: https://stackoverflow.com/a/76581267\r\n\r\n## Solution\r\n\r\nAdd a new option to the `deployment.targets` section of the configuration called `stripIndexHTML`. This option maps any local file named `<dir>/index.html` to a remote file named `<dir>/`. The root page, `index.html`, is untouched.\r\n\r\nThis is \u201celegant\u201d because this just means that the remote file path matches the canonical URLs that Hugo uses anyways. Your link to `dir/` maps to a remote file named `dir/`.\r\n\r\n## Impact\r\n\r\nFrom a web search, I can see that other people using Hugo have experienced this problem:\r\n\r\n* [discourse.gohugo.io](https://discourse.gohugo.io/t/index-html-pages-in-s3-bucket-arent-loading-through-cloudfront-distribution/29956): \u201cIndex.html pages in S3 bucket aren\u2019t loading through CloudFront distribution\u201d\r\n* [Stack Overflow](https://stackoverflow.com/questions/76853176/hugo-on-s3-with-cloudfront-doesnt-redirect-to-index-html-terraform-config): \u201cHugo on S3 with CloudFront doesn't redirect to index.html.\u201d\r\n* [Stack Overflow](https://stackoverflow.com/questions/58652041/hugo-multilingual-site-deploy-to-aws-s3-error-accessdenied): \u201cHugo multilingual site deploy to aws s3 error \u2018AccessDenied\u2019\u201d\r\n* [Stack Overflow](https://stackoverflow.com/questions/45828017/aws-static-links-fail-without-index-html): \u201cAWS static links fail without \u2018index.html\u2019\u201d\n", "patch": "diff --git a/deploy/deploy.go b/deploy/deploy.go\nindex b2a8a88138b..a69e974b78c 100644\n--- a/deploy/deploy.go\n+++ b/deploy/deploy.go\n@@ -133,10 +133,14 @@ func (d *Deployer) Deploy(ctx context.Context) error {\n \n \t// Load local files from the source directory.\n \tvar include, exclude glob.Glob\n+\tvar mappath func(string) string\n \tif d.target != nil {\n \t\tinclude, exclude = d.target.IncludeGlob, d.target.ExcludeGlob\n+\t\tif d.target.StripIndexHTML {\n+\t\t\tmappath = stripIndexHTML\n+\t\t}\n \t}\n-\tlocal, err := d.walkLocal(d.localFs, d.cfg.Matchers, include, exclude, d.mediaTypes)\n+\tlocal, err := d.walkLocal(d.localFs, d.cfg.Matchers, include, exclude, d.mediaTypes, mappath)\n \tif err != nil {\n \t\treturn err\n \t}\n@@ -483,7 +487,7 @@ func knownHiddenDirectory(name string) bool {\n \n // walkLocal walks the source directory and returns a flat list of files,\n // using localFile.SlashPath as the map keys.\n-func (d *Deployer) walkLocal(fs afero.Fs, matchers []*deployconfig.Matcher, include, exclude glob.Glob, mediaTypes media.Types) (map[string]*localFile, error) {\n+func (d *Deployer) walkLocal(fs afero.Fs, matchers []*deployconfig.Matcher, include, exclude glob.Glob, mediaTypes media.Types, mappath func(string) string) (map[string]*localFile, error) {\n \tretval := map[string]*localFile{}\n \terr := afero.Walk(fs, \"\", func(path string, info os.FileInfo, err error) error {\n \t\tif err != nil {\n@@ -529,6 +533,11 @@ func (d *Deployer) walkLocal(fs afero.Fs, matchers []*deployconfig.Matcher, incl\n \t\t\t\tbreak\n \t\t\t}\n \t\t}\n+\t\t// Apply any additional modifications to the local path, to map it to\n+\t\t// the remote path.\n+\t\tif mappath != nil {\n+\t\t\tslashpath = mappath(slashpath)\n+\t\t}\n \t\tlf, err := newLocalFile(fs, path, slashpath, m, mediaTypes)\n \t\tif err != nil {\n \t\t\treturn err\n@@ -542,6 +551,15 @@ func (d *Deployer) walkLocal(fs afero.Fs, matchers []*deployconfig.Matcher, incl\n \treturn retval, nil\n }\n \n+// stripIndexHTML remaps keys matching \"<dir>/index.html\" to \"<dir>/\".\n+func stripIndexHTML(slashpath string) string {\n+\tconst suffix = \"/index.html\"\n+\tif strings.HasSuffix(slashpath, suffix) {\n+\t\treturn slashpath[:len(slashpath)-len(suffix)+1]\n+\t}\n+\treturn slashpath\n+}\n+\n // walkRemote walks the target bucket and returns a flat list.\n func (d *Deployer) walkRemote(ctx context.Context, bucket *blob.Bucket, include, exclude glob.Glob) (map[string]*blob.ListObject, error) {\n \tretval := map[string]*blob.ListObject{}\ndiff --git a/deploy/deployconfig/deployConfig.go b/deploy/deployconfig/deployConfig.go\nindex b5b8973b2d8..b16b7c627a2 100644\n--- a/deploy/deployconfig/deployConfig.go\n+++ b/deploy/deployconfig/deployConfig.go\n@@ -69,6 +69,11 @@ type Target struct {\n \t// Parsed versions of Include/Exclude.\n \tIncludeGlob glob.Glob `json:\"-\"`\n \tExcludeGlob glob.Glob `json:\"-\"`\n+\n+\t// If true, any local path matching <dir>/index.html will be mapped to the\n+\t// remote path <dir>/. This does not affect the top-level index.html file,\n+\t// since that would result in an empty path.\n+\tStripIndexHTML bool\n }\n \n func (tgt *Target) ParseIncludeExclude() error {\ndiff --git a/docs/content/en/hosting-and-deployment/hugo-deploy.md b/docs/content/en/hosting-and-deployment/hugo-deploy.md\nindex 45b917f1e28..db2448ee7c6 100644\n--- a/docs/content/en/hosting-and-deployment/hugo-deploy.md\n+++ b/docs/content/en/hosting-and-deployment/hugo-deploy.md\n@@ -186,6 +186,15 @@ URL = \"<FILL ME IN>\"\n #include = \"**.html\" # would only include files with \".html\" suffix\n #exclude = \"**.{jpg, png}\" # would exclude files with \".jpg\" or \".png\" suffix\n \n+# Map any file named \"<dir>/index.html\" to the remote file \"<dir>/\". This does\n+# not affect the root \"index.html\" file, and it does not affect matchers below.\n+# This works when deploying to key-value cloud storage systems, such as Amazon\n+# S3 (general purpose buckets, not directory buckets), Google Cloud Storage, and\n+# Azure Blob Storage. This makes it so the canonical URL will match the object\n+# key in cloud storage, except for the root index.html file.\n+#\n+#stripIndexHTML = true\n+\n \n #######################\n [[deployment.matchers]] \n@@ -195,6 +204,7 @@ URL = \"<FILL ME IN>\"\n \n # See https://golang.org/pkg/regexp/syntax/ for pattern syntax.\n # Pattern searching is stopped on first match.\n+# This is not affected by stripIndexHTML, above.\n pattern = \"<FILL ME IN>\"\n \n # If true, Hugo will gzip the file before uploading it to the bucket.\n", "instance_id": "gohugoio__hugo-12608", "clarity": 3, "difficulty": 0.35, "clarity_explanation": "The problem statement is comprehensive and well-articulated. It clearly defines the goal of adding an option to remap files named `<dir>/index.html` to `<dir>/` during deployment to improve integration with cloud static website hosting services like S3 and GCS. The background section provides detailed context about the issue, including why it occurs and existing workarounds, supported by external references to forums and Stack Overflow discussions. The solution is explicitly described with the introduction of a new configuration option `stripIndexHTML`, and its impact is discussed in terms of user experience and compatibility with cloud storage systems. Inputs (configuration settings), outputs (file path mappings), and constraints (e.g., root `index.html` is unaffected) are well-defined. There are no significant ambiguities, and the inclusion of real-world user issues adds to the clarity of the problem's relevance and scope. Therefore, I assign a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are relatively localized, primarily affecting the `deploy/deploy.go` file with modifications to the `walkLocal` function and the addition of a new `stripIndexHTML` helper function. There are also minor updates to the configuration struct in `deployconfig/deployConfig.go` and documentation in `hugo-deploy.md`. The changes do not impact the broader system architecture and are confined to a specific feature within the deployment logic. The amount of code change is small, with less than 50 lines of meaningful additions or modifications.\n\n2. **Number of Technical Concepts**: The solution requires understanding of basic Go programming constructs (e.g., function pointers for path mapping, string manipulation), familiarity with the Hugo deployment module, and knowledge of cloud storage behaviors (e.g., S3 object naming). These concepts are not particularly complex for a developer with moderate experience in Go or web deployment workflows. No advanced algorithms, design patterns, or domain-specific knowledge beyond static website hosting are needed.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement and code changes address a specific edge case (root `index.html` should not be remapped to an empty path), which is handled implicitly by the logic in `stripIndexHTML`. However, there are no explicit mentions of other edge cases (e.g., nested directories, malformed paths, or conflicts with existing mappings) or additional error handling requirements in the code changes. The simplicity of the string suffix check suggests minimal complexity in handling edge cases.\n\n4. **Overall Complexity**: The task involves adding a straightforward feature with a clear mapping rule. It requires understanding the existing deployment logic to integrate the new functionality but does not demand deep architectural changes or performance optimizations. The problem is well-contained and does not introduce significant risk to other parts of the codebase.\n\nGiven these factors, I assign a difficulty score of 0.35, placing it towards the higher end of the \"Easy\" category due to the need for some contextual understanding of Hugo's deployment system and cloud storage nuances, but still within the realm of simple feature addition with minimal complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "create/skeletons: Remove superfluous language code fallback \nhttps://github.com/gohugoio/hugo/blob/b1bf0bff2c8ce1e2cace5010a81911a3dbf8e674/create/skeletons/theme/layouts/_default/baseof.html#L2\r\n\r\nWith v0.112.0 and later, `site.Language.LanguageCode` falls back to `site.Language.Lang`, so the explicit fallback above is not required.\n", "patch": "diff --git a/create/skeletons/theme/layouts/_default/baseof.html b/create/skeletons/theme/layouts/_default/baseof.html\nindex ce9613a2bd1..479c897137a 100644\n--- a/create/skeletons/theme/layouts/_default/baseof.html\n+++ b/create/skeletons/theme/layouts/_default/baseof.html\n@@ -1,5 +1,5 @@\n <!DOCTYPE html>\n-<html lang=\"{{ or site.Language.LanguageCode site.Language.Lang }}\" dir=\"{{ or site.Language.LanguageDirection `ltr` }}\">\n+<html lang=\"{{ or site.Language.LanguageCode }}\" dir=\"{{ or site.Language.LanguageDirection `ltr` }}\">\n <head>\n   {{ partial \"head.html\" . }}\n </head>\n", "instance_id": "gohugoio__hugo-12483", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent to remove a superfluous language code fallback in a specific file due to a change in behavior in Hugo v0.112.0 and later. It provides a direct reference to the GitHub repository and the specific line of code to be modified. However, it lacks additional context or examples about the broader implications of this change, such as whether this fallback removal could affect certain configurations or older versions of Hugo. Edge cases or potential compatibility issues are not mentioned, which could be critical for a complete understanding of the problem. Overall, while the goal is clear, minor details are missing that could help in fully grasping the scope or potential risks of the change.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward modification to a single line of code in a single file. The change is minimal, requiring only the removal of an explicit fallback (`site.Language.Lang`) in a template file, which is now handled by the updated behavior of `site.Language.LanguageCode` in Hugo v0.112.0+. The scope of the code change is extremely limited, with no impact on the broader codebase or system architecture. There are no complex technical concepts, algorithms, or domain-specific knowledge required beyond basic familiarity with Hugo templates and the specific property behavior. Edge cases or error handling are not mentioned in the problem statement, and the provided code change does not introduce or modify any such logic. This task falls into the \"very easy\" category, as it is a simple, localized update with minimal cognitive load or risk.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "server: Browser does not refresh after updating Sass file\nThis worked fine in v0.122.0, and fails with v0.123.0 and later.\r\n\r\n```text\r\ngit clone --single-branch -b hugo-forum-topic-49365 https://github.com/jmooring/hugo-testing hugo-forum-topic-49365\r\ncd hugo-forum-topic-49365\r\nhugo server\r\n```\r\n\r\nWhile the server is running, edit themes/sasstest_theme/assets/scss/index.scss, changing h1.color from red to blue. The site title should change from red to blue.\r\n\r\nReference: <https://discourse.gohugo.io/t/changes-to-sass-stylesheets-are-not-reflected-in-the-browser-via-live-reload/49365>\r\n\n", "patch": "diff --git a/cache/dynacache/dynacache.go b/cache/dynacache/dynacache.go\nindex eab251e5d3a..e79de5a5bb9 100644\n--- a/cache/dynacache/dynacache.go\n+++ b/cache/dynacache/dynacache.go\n@@ -140,16 +140,25 @@ func (c *Cache) DrainEvictedIdentities() []identity.Identity {\n }\n \n // ClearMatching clears all partition for which the predicate returns true.\n-func (c *Cache) ClearMatching(predicate func(k, v any) bool) {\n+func (c *Cache) ClearMatching(predicatePartition func(k string, p PartitionManager) bool, predicateValue func(k, v any) bool) {\n+\tif predicatePartition == nil {\n+\t\tpredicatePartition = func(k string, p PartitionManager) bool { return true }\n+\t}\n+\tif predicateValue == nil {\n+\t\tpanic(\"nil predicateValue\")\n+\t}\n \tg := rungroup.Run[PartitionManager](context.Background(), rungroup.Config[PartitionManager]{\n \t\tNumWorkers: len(c.partitions),\n \t\tHandle: func(ctx context.Context, partition PartitionManager) error {\n-\t\t\tpartition.clearMatching(predicate)\n+\t\t\tpartition.clearMatching(predicateValue)\n \t\t\treturn nil\n \t\t},\n \t})\n \n-\tfor _, p := range c.partitions {\n+\tfor k, p := range c.partitions {\n+\t\tif !predicatePartition(k, p) {\n+\t\t\tcontinue\n+\t\t}\n \t\tg.Enqueue(p)\n \t}\n \n@@ -356,6 +365,7 @@ func GetOrCreatePartition[K comparable, V any](c *Cache, name string, opts Optio\n \t\ttrace:   c.opts.Log.Logger().WithLevel(logg.LevelTrace).WithField(\"partition\", name),\n \t\topts:    opts,\n \t}\n+\n \tc.partitions[name] = partition\n \n \treturn partition\ndiff --git a/commands/commandeer.go b/commands/commandeer.go\nindex f18c3f81311..616a3c867f4 100644\n--- a/commands/commandeer.go\n+++ b/commands/commandeer.go\n@@ -128,6 +128,7 @@ type rootCommand struct {\n \tverbose bool\n \tdebug   bool\n \tquiet   bool\n+\tdevMode bool // Hidden flag.\n \n \trenderToMemory bool\n \n@@ -423,29 +424,33 @@ func (r *rootCommand) PreRun(cd, runner *simplecobra.Commandeer) error {\n func (r *rootCommand) createLogger(running bool) (loggers.Logger, error) {\n \tlevel := logg.LevelWarn\n \n-\tif r.logLevel != \"\" {\n-\t\tswitch strings.ToLower(r.logLevel) {\n-\t\tcase \"debug\":\n-\t\t\tlevel = logg.LevelDebug\n-\t\tcase \"info\":\n-\t\t\tlevel = logg.LevelInfo\n-\t\tcase \"warn\", \"warning\":\n-\t\t\tlevel = logg.LevelWarn\n-\t\tcase \"error\":\n-\t\t\tlevel = logg.LevelError\n-\t\tdefault:\n-\t\t\treturn nil, fmt.Errorf(\"invalid log level: %q, must be one of debug, warn, info or error\", r.logLevel)\n-\t\t}\n+\tif r.devMode {\n+\t\tlevel = logg.LevelTrace\n \t} else {\n-\t\tif r.verbose {\n-\t\t\thugo.Deprecate(\"--verbose\", \"use --logLevel info\", \"v0.114.0\")\n-\t\t\thugo.Deprecate(\"--verbose\", \"use --logLevel info\", \"v0.114.0\")\n-\t\t\tlevel = logg.LevelInfo\n-\t\t}\n+\t\tif r.logLevel != \"\" {\n+\t\t\tswitch strings.ToLower(r.logLevel) {\n+\t\t\tcase \"debug\":\n+\t\t\t\tlevel = logg.LevelDebug\n+\t\t\tcase \"info\":\n+\t\t\t\tlevel = logg.LevelInfo\n+\t\t\tcase \"warn\", \"warning\":\n+\t\t\t\tlevel = logg.LevelWarn\n+\t\t\tcase \"error\":\n+\t\t\t\tlevel = logg.LevelError\n+\t\t\tdefault:\n+\t\t\t\treturn nil, fmt.Errorf(\"invalid log level: %q, must be one of debug, warn, info or error\", r.logLevel)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif r.verbose {\n+\t\t\t\thugo.Deprecate(\"--verbose\", \"use --logLevel info\", \"v0.114.0\")\n+\t\t\t\thugo.Deprecate(\"--verbose\", \"use --logLevel info\", \"v0.114.0\")\n+\t\t\t\tlevel = logg.LevelInfo\n+\t\t\t}\n \n-\t\tif r.debug {\n-\t\t\thugo.Deprecate(\"--debug\", \"use --logLevel debug\", \"v0.114.0\")\n-\t\t\tlevel = logg.LevelDebug\n+\t\t\tif r.debug {\n+\t\t\t\thugo.Deprecate(\"--debug\", \"use --logLevel debug\", \"v0.114.0\")\n+\t\t\t\tlevel = logg.LevelDebug\n+\t\t\t}\n \t\t}\n \t}\n \n@@ -505,10 +510,13 @@ Complete documentation is available at https://gohugo.io/.`\n \n \tcmd.PersistentFlags().BoolVarP(&r.verbose, \"verbose\", \"v\", false, \"verbose output\")\n \tcmd.PersistentFlags().BoolVarP(&r.debug, \"debug\", \"\", false, \"debug output\")\n+\tcmd.PersistentFlags().BoolVarP(&r.devMode, \"devMode\", \"\", false, \"only used for internal testing, flag hidden.\")\n \tcmd.PersistentFlags().StringVar(&r.logLevel, \"logLevel\", \"\", \"log level (debug|info|warn|error)\")\n \t_ = cmd.RegisterFlagCompletionFunc(\"logLevel\", cobra.FixedCompletions([]string{\"debug\", \"info\", \"warn\", \"error\"}, cobra.ShellCompDirectiveNoFileComp))\n \tcmd.Flags().BoolVarP(&r.buildWatch, \"watch\", \"w\", false, \"watch filesystem for changes and recreate as needed\")\n \n+\tcmd.PersistentFlags().MarkHidden(\"devMode\")\n+\n \t// Configure local flags\n \tapplyLocalFlagsBuild(cmd, r)\n \ndiff --git a/hugolib/content_map_page.go b/hugolib/content_map_page.go\nindex aa32b532071..50e1bc35dcb 100644\n--- a/hugolib/content_map_page.go\n+++ b/hugolib/content_map_page.go\n@@ -1084,7 +1084,7 @@ func (h *HugoSites) resolveAndClearStateForIdentities(\n \t\t\t\treturn b\n \t\t\t}\n \n-\t\t\th.MemCache.ClearMatching(shouldDelete)\n+\t\t\th.MemCache.ClearMatching(nil, shouldDelete)\n \n \t\t\treturn ll, nil\n \t\t}); err != nil {\ndiff --git a/hugolib/hugo_sites_build.go b/hugolib/hugo_sites_build.go\nindex 411f90734e7..3beb072e3c7 100644\n--- a/hugolib/hugo_sites_build.go\n+++ b/hugolib/hugo_sites_build.go\n@@ -23,6 +23,7 @@ import (\n \t\"path\"\n \t\"path/filepath\"\n \t\"strings\"\n+\t\"sync\"\n \t\"time\"\n \n \t\"github.com/bep/logg\"\n@@ -46,6 +47,7 @@ import (\n \t\"github.com/gohugoio/hugo/resources/page\"\n \t\"github.com/gohugoio/hugo/resources/page/siteidentities\"\n \t\"github.com/gohugoio/hugo/resources/postpub\"\n+\t\"github.com/gohugoio/hugo/resources/resource\"\n \n \t\"github.com/spf13/afero\"\n \n@@ -758,15 +760,45 @@ func (h *HugoSites) processPartial(ctx context.Context, l logg.LevelLogger, conf\n \t\t\t\t}\n \t\t\t}\n \t\tcase files.ComponentFolderAssets:\n-\t\t\tlogger.Println(\"Asset changed\", pathInfo.Path())\n+\t\t\tp := pathInfo.Path()\n+\t\t\tlogger.Println(\"Asset changed\", p)\n+\n+\t\t\tvar matches []any\n+\t\t\tvar mu sync.Mutex\n+\n+\t\t\th.MemCache.ClearMatching(\n+\t\t\t\tfunc(k string, pm dynacache.PartitionManager) bool {\n+\t\t\t\t\t// Avoid going through everything.\n+\t\t\t\t\treturn strings.HasPrefix(k, \"/res\")\n+\t\t\t\t},\n+\t\t\t\tfunc(k, v any) bool {\n+\t\t\t\t\tif strings.Contains(k.(string), p) {\n+\t\t\t\t\t\tmu.Lock()\n+\t\t\t\t\t\tdefer mu.Unlock()\n+\t\t\t\t\t\tswitch vv := v.(type) {\n+\t\t\t\t\t\tcase resource.Resources:\n+\t\t\t\t\t\t\t// GetMatch/Match.\n+\t\t\t\t\t\t\tfor _, r := range vv {\n+\t\t\t\t\t\t\t\tmatches = append(matches, r)\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\treturn true\n+\t\t\t\t\t\tdefault:\n+\t\t\t\t\t\t\tmatches = append(matches, vv)\n+\t\t\t\t\t\t\treturn true\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t\treturn false\n+\t\t\t\t})\n \n \t\t\tvar hasID bool\n-\t\t\tr, _ := h.ResourceSpec.ResourceCache.Get(context.Background(), dynacache.CleanKey(pathInfo.Base()))\n-\t\t\tidentity.WalkIdentitiesShallow(r, func(level int, rid identity.Identity) bool {\n-\t\t\t\thasID = true\n-\t\t\t\tchanges = append(changes, rid)\n-\t\t\t\treturn false\n-\t\t\t})\n+\t\t\tfor _, r := range matches {\n+\t\t\t\tidentity.WalkIdentitiesShallow(r, func(level int, rid identity.Identity) bool {\n+\t\t\t\t\thasID = true\n+\t\t\t\t\tchanges = append(changes, rid)\n+\t\t\t\t\treturn false\n+\t\t\t\t})\n+\t\t\t}\n \t\t\tif !hasID {\n \t\t\t\tchanges = append(changes, pathInfo)\n \t\t\t}\n", "instance_id": "gohugoio__hugo-12400", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the browser does not refresh after updating a Sass file in a Hugo server setup, which worked in a previous version (v0.122.0) but fails in v0.123.0 and later. It provides a reproducible setup with a GitHub repository and specific steps to replicate the issue, along with the expected behavior (site title color change). However, it lacks critical details about the root cause or specific technical requirements for the fix, such as which part of the Hugo server or live reload mechanism is failing. Additionally, edge cases or specific constraints (e.g., browser compatibility, file types other than Sass) are not mentioned. While the goal is clear, the lack of deeper context or diagnostic information makes it less comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files (`dynacache.go`, `commandeer.go`, `content_map_page.go`, `hugo_sites_build.go`), indicating a need to understand and modify interactions across different parts of the Hugo codebase, particularly in caching and resource handling mechanisms. The changes involve complex logic, such as refining cache clearing with predicate functions and handling asset changes with identity tracking, which requires a deep understanding of Hugo's internal architecture, especially its resource caching (`MemCache`) and live reload system. \n\nSecond, the technical concepts involved are non-trivial: familiarity with Go concurrency (e.g., `sync.Mutex`, `rungroup.Run`), Hugo's custom caching system (`dynacache`), and resource identity management (`identity.Identity`) are necessary. Additionally, the introduction of a hidden `devMode` flag suggests debugging or testing considerations that add to the complexity. \n\nThird, while edge cases are not explicitly mentioned in the problem statement, the code changes imply handling specific scenarios like partial cache clearing based on asset paths and ensuring correct propagation of changes in the live reload system, which could involve subtle bugs or performance issues if not handled carefully. \n\nOverall, solving this requires a solid grasp of the Hugo framework's internals, careful modification of critical components like caching and resource processing, and an understanding of how file changes trigger browser updates. This places the difficulty at 0.75, reflecting a challenging problem that demands significant expertise and codebase familiarity, though it does not reach the extreme complexity of system-level or domain-specific innovations.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Modify embedded figure shortcode to look for page resource\nReference: <https://github.com/gohugoio/hugo/issues/12244>\r\n\r\nWith a shared page resource (e.g., image) on a multilingual single-host site, we do not (by default) duplicate the resource for each language, as designed. In this configuration we automatically enable an embedded image render hook to handle markdown destinations, but the embedded figure shortcode does not use the same logic, resulting in broken images.\r\n\r\nWe need to do this:\r\n\r\n```text\r\n  {{- $u := urls.Parse (.Get \"src\") -}}\r\n  {{- $src := $u.String -}}\r\n  {{- if not $u.IsAbs -}}\r\n    {{- with or (.Page.Resources.Get $u.Path) (resources.Get $u.Path) -}}\r\n      {{- $src = .RelPermalink -}}\r\n    {{- end -}}\r\n  {{- end -}}\r\n```\r\n\r\nThis should be safe regardless of site configuration. We're not touching external destinations, and we fall back to the original value if we can't find a page resource or global resource.\n", "patch": "diff --git a/tpl/tplimpl/embedded/templates/shortcodes/figure.html b/tpl/tplimpl/embedded/templates/shortcodes/figure.html\nindex afe784c19c6..a0b93a437fb 100644\n--- a/tpl/tplimpl/embedded/templates/shortcodes/figure.html\n+++ b/tpl/tplimpl/embedded/templates/shortcodes/figure.html\n@@ -1,29 +1,38 @@\n <figure{{ with .Get \"class\" }} class=\"{{ . }}\"{{ end }}>\n-    {{- if .Get \"link\" -}}\n-        <a href=\"{{ .Get \"link\" }}\"{{ with .Get \"target\" }} target=\"{{ . }}\"{{ end }}{{ with .Get \"rel\" }} rel=\"{{ . }}\"{{ end }}>\n+  {{- if .Get \"link\" -}}\n+    <a href=\"{{ .Get \"link\" }}\"{{ with .Get \"target\" }} target=\"{{ . }}\"{{ end }}{{ with .Get \"rel\" }} rel=\"{{ . }}\"{{ end }}>\n+  {{- end -}}\n+\n+  {{- $u := urls.Parse (.Get \"src\") -}}\n+  {{- $src := $u.String -}}\n+  {{- if not $u.IsAbs -}}\n+    {{- with or (.Page.Resources.Get $u.Path) (resources.Get $u.Path) -}}\n+      {{- $src = .RelPermalink -}}\n     {{- end -}}\n-    <img src=\"{{ .Get \"src\" }}\"\n-         {{- if or (.Get \"alt\") (.Get \"caption\") }}\n-         alt=\"{{ with .Get \"alt\" }}{{ . }}{{ else }}{{ .Get \"caption\" | markdownify| plainify }}{{ end }}\"\n-         {{- end -}}\n-         {{- with .Get \"width\" }} width=\"{{ . }}\"{{ end -}}\n-         {{- with .Get \"height\" }} height=\"{{ . }}\"{{ end -}}\n-         {{- with .Get \"loading\" }} loading=\"{{ . }}\"{{ end -}}\n-    /><!-- Closing img tag -->\n-    {{- if .Get \"link\" }}</a>{{ end -}}\n-    {{- if or (or (.Get \"title\") (.Get \"caption\")) (.Get \"attr\") -}}\n-        <figcaption>\n-            {{ with (.Get \"title\") -}}\n-                <h4>{{ . }}</h4>\n-            {{- end -}}\n-            {{- if or (.Get \"caption\") (.Get \"attr\") -}}<p>\n-                {{- .Get \"caption\" | markdownify -}}\n-                {{- with .Get \"attrlink\" }}\n-                    <a href=\"{{ . }}\">\n-                {{- end -}}\n-                {{- .Get \"attr\" | markdownify -}}\n-                {{- if .Get \"attrlink\" }}</a>{{ end }}</p>\n-            {{- end }}\n-        </figcaption>\n-    {{- end }}\n+  {{- end -}}\n+\n+  <img src=\"{{ $src }}\"\n+    {{- if or (.Get \"alt\") (.Get \"caption\") }}\n+    alt=\"{{ with .Get \"alt\" }}{{ . }}{{ else }}{{ .Get \"caption\" | markdownify| plainify }}{{ end }}\"\n+    {{- end -}}\n+    {{- with .Get \"width\" }} width=\"{{ . }}\"{{ end -}}\n+    {{- with .Get \"height\" }} height=\"{{ . }}\"{{ end -}}\n+    {{- with .Get \"loading\" }} loading=\"{{ . }}\"{{ end -}}\n+  ><!-- Closing img tag -->\n+  {{- if .Get \"link\" }}</a>{{ end -}}\n+  {{- if or (or (.Get \"title\") (.Get \"caption\")) (.Get \"attr\") -}}\n+    <figcaption>\n+      {{ with (.Get \"title\") -}}\n+        <h4>{{ . }}</h4>\n+      {{- end -}}\n+      {{- if or (.Get \"caption\") (.Get \"attr\") -}}<p>\n+        {{- .Get \"caption\" | markdownify -}}\n+        {{- with .Get \"attrlink\" }}\n+          <a href=\"{{ . }}\">\n+        {{- end -}}\n+        {{- .Get \"attr\" | markdownify -}}\n+        {{- if .Get \"attrlink\" }}</a>{{ end }}</p>\n+      {{- end }}\n+    </figcaption>\n+  {{- end }}\n </figure>\n", "instance_id": "gohugoio__hugo-12246", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the embedded figure shortcode in a multilingual Hugo site setup, where shared page resources (like images) are not duplicated per language, leading to broken images. The goal is to modify the shortcode to look for page or global resources and adjust the image source accordingly. The provided code snippet in the problem statement outlines the intended logic, which helps in understanding the required change. However, there are minor ambiguities: the statement does not explicitly discuss potential edge cases (e.g., what happens if the resource path is malformed or if there are permission issues accessing resources) or provide examples of input/output behavior. Additionally, the broader context of how this change integrates with other render hooks or site configurations is not fully detailed, which could lead to minor confusion for someone unfamiliar with Hugo's internals. Overall, it is clear enough to proceed with the implementation but lacks comprehensive details on edge cases and broader impact.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is limited to a single file (`figure.html`), specifically modifying the logic for resolving the image source (`src`) using Hugo's resource lookup functions. The change involves adding a small block of template logic (parsing URLs and conditionally updating the source based on resource availability), which is straightforward for someone familiar with Hugo's templating system. Second, the technical concepts required are relatively basic: understanding Hugo's template syntax, the `urls.Parse` function, and resource lookup methods (`Page.Resources.Get` and `resources.Get`). These are not overly complex for a developer with moderate experience in Go templating or Hugo. Third, the problem does not appear to impact the broader system architecture or require deep modifications across multiple modules\u2014it is a localized fix. However, there is a slight increase in difficulty due to the need to ensure the logic handles relative vs. absolute URLs correctly and falls back gracefully, which introduces minor complexity in error handling. Edge cases, such as malformed URLs or unavailable resources, are not explicitly mentioned but are implicitly handled by falling back to the original `src` value, keeping the complexity low. Overall, this task requires understanding some specific Hugo logic and making a targeted modification, but it does not demand advanced technical knowledge or extensive codebase familiarity, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Possible missing nil checks\n### Bug report criteria\r\n\r\n- [X] This bug report is not security related, security issues should be disclosed privately via [etcd maintainers](mailto:etcd-maintainers@googlegroups.com).\r\n- [X] This is not a support request or question, support requests or questions should be raised in the etcd [discussion forums](https://github.com/etcd-io/etcd/discussions).\r\n- [X] You have read the etcd [bug reporting guidelines](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/reporting_bugs.md).\r\n- [X] Existing open issues along with etcd [frequently asked questions](https://etcd.io/docs/latest/faq) have been checked and this is not a duplicate.\r\n\r\n### What happened?\r\n\r\nAfter having been compared to a nil value at backend.go:515, pointer 'b.lg' is passed as implicit 'this' parameter in call to function 'Error' at backend.go:545 or pointer 'b.lg' is passed as implicit 'this' parameter in call to function 'Fatal' at backend.go:532. \r\n```\r\nif b.lg != nil {\r\n\t\tb.lg.Info(\r\n\t\t\t\"defragmenting\",\r\n\t\t\tzap.String(\"path\", dbp),\r\n\t\t\tzap.Int64(\"current-db-size-bytes\", size1),\r\n\t\t\tzap.String(\"current-db-size\", humanize.Bytes(uint64(size1))),\r\n\t\t\tzap.Int64(\"current-db-size-in-use-bytes\", sizeInUse1),\r\n\t\t\tzap.String(\"current-db-size-in-use\", humanize.Bytes(uint64(sizeInUse1))),\r\n\t\t)\r\n\t}\r\n\r\n\tdefer func() {\r\n\t\t// NOTE: We should exit as soon as possible because that tx\r\n\t\t// might be closed. The inflight request might use invalid\r\n\t\t// tx and then panic as well. The real panic reason might be\r\n\t\t// shadowed by new panic. So, we should fatal here with lock.\r\n\t\tif rerr := recover(); rerr != nil {\r\n\t\t\tb.lg.Fatal(\"unexpected panic during defrag\", zap.Any(\"panic\", rerr))\r\n\t\t}\r\n\t}()\r\n\r\n\t// Commit/stop and then reset current transactions (including the readTx)\r\n\tb.batchTx.unsafeCommit(true)\r\n\tb.batchTx.tx = nil\r\n\r\n\t// gofail: var defragBeforeCopy struct{}\r\n\terr = defragdb(b.db, tmpdb, defragLimit)\r\n\tif err != nil {\r\n\t\ttmpdb.Close()\r\n\t\tif rmErr := os.RemoveAll(tmpdb.Path()); rmErr != nil {\r\n\t\t\tb.lg.Error(\"failed to remove db.tmp after defragmentation completed\", zap.Error(rmErr))\r\n\t\t}\r\n\r\n\t\t// restore the bbolt transactions if defragmentation fails\r\n\t\tb.batchTx.tx = b.unsafeBegin(true)\r\n\t\tb.readTx.tx = b.unsafeBegin(false)\r\n\r\n\t\treturn err\r\n\t}\r\n```\r\nPlease clarify whether verification is required in this case? Do you need to make any changes/corrections to the code?\r\n\r\n### What did you expect to happen?\r\n\r\nIt might be worth adding additional checks for b.lg, if required.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nChecking code\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Etcd version (please run commands below)\r\n\r\nLatest\r\n\r\n### Etcd configuration (command line flags or environment variables)\r\n\r\n<details>\r\n\r\n\r\n\r\n</details>\r\n\r\n\r\n### Etcd debug information (please run commands below, feel free to obfuscate the IP address or FQDN in the output)\r\n\r\n<details>\r\n\r\n\r\n\r\n</details>\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_\n", "patch": "diff --git a/server/storage/backend/backend.go b/server/storage/backend/backend.go\nindex 7f5862c8861..275064f083b 100644\n--- a/server/storage/backend/backend.go\n+++ b/server/storage/backend/backend.go\n@@ -28,6 +28,7 @@ import (\n \t\"go.uber.org/zap\"\n \n \tbolt \"go.etcd.io/bbolt\"\n+\t\"go.etcd.io/etcd/client/pkg/v3/verify\"\n )\n \n var (\n@@ -187,6 +188,11 @@ func newBackend(bcfg BackendConfig) *backend {\n \tif boltOpenOptions != nil {\n \t\t*bopts = *boltOpenOptions\n \t}\n+\n+\tif bcfg.Logger == nil {\n+\t\tbcfg.Logger = zap.NewNop()\n+\t}\n+\n \tbopts.InitialMmapSize = bcfg.mmapSize()\n \tbopts.FreelistType = bcfg.BackendFreelistType\n \tbopts.NoSync = bcfg.UnsafeNoFsync\n@@ -458,6 +464,7 @@ func (b *backend) Defrag() error {\n }\n \n func (b *backend) defrag() error {\n+\tverify.Assert(b.lg != nil, \"the logger should not be nil\")\n \tnow := time.Now()\n \tisDefragActive.Set(1)\n \tdefer isDefragActive.Set(0)\n@@ -499,7 +506,7 @@ func (b *backend) defrag() error {\n \ttmpdb, err := bolt.Open(tdbp, 0o600, &options)\n \tif err != nil {\n \t\ttemp.Close()\n-\t\tif rmErr := os.Remove(temp.Name()); rmErr != nil && b.lg != nil {\n+\t\tif rmErr := os.Remove(temp.Name()); rmErr != nil {\n \t\t\tb.lg.Error(\n \t\t\t\t\"failed to remove temporary file\",\n \t\t\t\tzap.String(\"path\", temp.Name()),\n@@ -512,16 +519,14 @@ func (b *backend) defrag() error {\n \n \tdbp := b.db.Path()\n \tsize1, sizeInUse1 := b.Size(), b.SizeInUse()\n-\tif b.lg != nil {\n-\t\tb.lg.Info(\n-\t\t\t\"defragmenting\",\n-\t\t\tzap.String(\"path\", dbp),\n-\t\t\tzap.Int64(\"current-db-size-bytes\", size1),\n-\t\t\tzap.String(\"current-db-size\", humanize.Bytes(uint64(size1))),\n-\t\t\tzap.Int64(\"current-db-size-in-use-bytes\", sizeInUse1),\n-\t\t\tzap.String(\"current-db-size-in-use\", humanize.Bytes(uint64(sizeInUse1))),\n-\t\t)\n-\t}\n+\tb.lg.Info(\n+\t\t\"defragmenting\",\n+\t\tzap.String(\"path\", dbp),\n+\t\tzap.Int64(\"current-db-size-bytes\", size1),\n+\t\tzap.String(\"current-db-size\", humanize.Bytes(uint64(size1))),\n+\t\tzap.Int64(\"current-db-size-in-use-bytes\", sizeInUse1),\n+\t\tzap.String(\"current-db-size-in-use\", humanize.Bytes(uint64(sizeInUse1))),\n+\t)\n \n \tdefer func() {\n \t\t// NOTE: We should exit as soon as possible because that tx\n@@ -584,19 +589,17 @@ func (b *backend) defrag() error {\n \tdefragSec.Observe(took.Seconds())\n \n \tsize2, sizeInUse2 := b.Size(), b.SizeInUse()\n-\tif b.lg != nil {\n-\t\tb.lg.Info(\n-\t\t\t\"finished defragmenting directory\",\n-\t\t\tzap.String(\"path\", dbp),\n-\t\t\tzap.Int64(\"current-db-size-bytes-diff\", size2-size1),\n-\t\t\tzap.Int64(\"current-db-size-bytes\", size2),\n-\t\t\tzap.String(\"current-db-size\", humanize.Bytes(uint64(size2))),\n-\t\t\tzap.Int64(\"current-db-size-in-use-bytes-diff\", sizeInUse2-sizeInUse1),\n-\t\t\tzap.Int64(\"current-db-size-in-use-bytes\", sizeInUse2),\n-\t\t\tzap.String(\"current-db-size-in-use\", humanize.Bytes(uint64(sizeInUse2))),\n-\t\t\tzap.Duration(\"took\", took),\n-\t\t)\n-\t}\n+\tb.lg.Info(\n+\t\t\"finished defragmenting directory\",\n+\t\tzap.String(\"path\", dbp),\n+\t\tzap.Int64(\"current-db-size-bytes-diff\", size2-size1),\n+\t\tzap.Int64(\"current-db-size-bytes\", size2),\n+\t\tzap.String(\"current-db-size\", humanize.Bytes(uint64(size2))),\n+\t\tzap.Int64(\"current-db-size-in-use-bytes-diff\", sizeInUse2-sizeInUse1),\n+\t\tzap.Int64(\"current-db-size-in-use-bytes\", sizeInUse2),\n+\t\tzap.String(\"current-db-size-in-use\", humanize.Bytes(uint64(sizeInUse2))),\n+\t\tzap.Duration(\"took\", took),\n+\t)\n \treturn nil\n }\n \n", "instance_id": "etcd-io__etcd-19077", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue of potential missing nil checks for the `b.lg` pointer in the `backend.go` file of the etcd project. It provides specific line numbers and code snippets where the issue occurs, which helps in understanding the context. The goal is evident: to ensure that the logger (`b.lg`) is not nil before using it in various logging operations. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss the implications of a nil logger (e.g., whether it could lead to a panic or just silent failures) nor does it specify any constraints or requirements for how the nil check should be handled (e.g., fallback behavior or initialization). Additionally, edge cases or specific scenarios where `b.lg` might be nil are not mentioned, leaving some room for interpretation. Despite these minor gaps, the issue is valid and the intent is understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the Easy category (0.2-0.4). The issue revolves around adding or modifying nil checks for the logger (`b.lg`) in the `backend.go` file, which is a straightforward bug fix. The code changes provided are minimal and localized to a single file, involving the removal of redundant nil checks and the addition of an assertion to ensure the logger is not nil, as well as setting a default no-op logger if none is provided. The scope of the change is small, with no impact on the broader system architecture or interactions between modules. The technical concepts required are basic: understanding Go's pointer semantics, familiarity with the `zap` logging library, and the use of a simple assertion utility (`verify.Assert`). There are no complex algorithms, design patterns, or domain-specific knowledge needed beyond basic Go programming. Edge cases and error handling are minimal, as the primary concern is ensuring the logger is not nil, and the provided solution (using a no-op logger as a fallback) addresses this directly. Overall, this task requires only a basic understanding of the code logic and simple modifications, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
