{"problem_statement": "Logging timestamps should be optional\nlnd always prints timestamps in log messages.\r\nThis is redundant in systemd services (where log messages are automatically timestamped) and should be made optional.\r\nThis [SO post](https://stackoverflow.com/a/48630122/3806468) shows how this can be achieved \n", "patch": "diff --git a/autopilot/log.go b/autopilot/log.go\nindex 34e53c8462..1d86eb5254 100644\n--- a/autopilot/log.go\n+++ b/autopilot/log.go\n@@ -1,7 +1,7 @@\n package autopilot\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/build/config.go b/build/config.go\nnew file mode 100644\nindex 0000000000..66a0b31444\n--- /dev/null\n+++ b/build/config.go\n@@ -0,0 +1,65 @@\n+//go:build !dev\n+// +build !dev\n+\n+package build\n+\n+import \"github.com/btcsuite/btclog/v2\"\n+\n+const (\n+\tcallSiteOff   = \"off\"\n+\tcallSiteShort = \"short\"\n+\tcallSiteLong  = \"long\"\n+)\n+\n+// LogConfig holds logging configuration options.\n+//\n+//nolint:lll\n+type LogConfig struct {\n+\tConsole *LoggerConfig `group:\"console\" namespace:\"console\" description:\"The logger writing to stdout and stderr.\"`\n+\tFile    *LoggerConfig `group:\"file\" namespace:\"file\" description:\"The logger writing to LND's standard log file.\"`\n+}\n+\n+// DefaultLogConfig returns the default logging config options.\n+func DefaultLogConfig() *LogConfig {\n+\treturn &LogConfig{\n+\t\tConsole: &LoggerConfig{\n+\t\t\tCallSite: callSiteOff,\n+\t\t},\n+\t\tFile: &LoggerConfig{\n+\t\t\tCallSite: callSiteOff,\n+\t\t},\n+\t}\n+}\n+\n+// LoggerConfig holds options for a particular logger.\n+//\n+//nolint:lll\n+type LoggerConfig struct {\n+\tDisable      bool   `long:\"disable\" description:\"Disable this logger.\"`\n+\tNoTimestamps bool   `long:\"no-timestamps\" description:\"Omit timestamps from log lines.\"`\n+\tCallSite     string `long:\"call-site\" description:\"Include the call-site of each log line.\" choice:\"off\" choice:\"short\" choice:\"long\"`\n+}\n+\n+// HandlerOptions returns the set of btclog.HandlerOptions that the state of the\n+// config struct translates to.\n+func (cfg *LoggerConfig) HandlerOptions() []btclog.HandlerOption {\n+\topts := []btclog.HandlerOption{\n+\t\t// The default skip depth used by the logging library is 6 but\n+\t\t// since we wrap the logging handlers with another level of\n+\t\t// abstraction with the handlerSet, we increase the skip depth\n+\t\t// to 7 here.\n+\t\tbtclog.WithCallSiteSkipDepth(7),\n+\t}\n+\tif cfg.NoTimestamps {\n+\t\topts = append(opts, btclog.WithNoTimestamp())\n+\t}\n+\n+\tswitch cfg.CallSite {\n+\tcase callSiteShort:\n+\t\topts = append(opts, btclog.WithCallerFlags(btclog.Lshortfile))\n+\tcase callSiteLong:\n+\t\topts = append(opts, btclog.WithCallerFlags(btclog.Llongfile))\n+\t}\n+\n+\treturn opts\n+}\ndiff --git a/build/config_dev.go b/build/config_dev.go\nnew file mode 100644\nindex 0000000000..daa5b1fde2\n--- /dev/null\n+++ b/build/config_dev.go\n@@ -0,0 +1,162 @@\n+//go:build dev\n+// +build dev\n+\n+package build\n+\n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+\n+\tbtclogv1 \"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n+)\n+\n+const (\n+\tresetSeq = \"0\"\n+\tboldSeq  = \"1\"\n+\tfaintSeq = \"2\"\n+\tesc      = '\\x1b'\n+\tcsi      = string(esc) + \"[\"\n+\n+\tcallSiteOff   = \"off\"\n+\tcallSiteShort = \"short\"\n+\tcallSiteLong  = \"long\"\n+)\n+\n+// LogConfig holds logging configuration options.\n+//\n+//nolint:lll\n+type LogConfig struct {\n+\tConsole *consoleLoggerCfg `group:\"console\" namespace:\"console\" description:\"The logger writing to stdout and stderr.\"`\n+\tFile    *LoggerConfig     `group:\"file\" namespace:\"file\" description:\"The logger writing to LND's standard log file.\"`\n+}\n+\n+// DefaultLogConfig returns the default logging config options.\n+func DefaultLogConfig() *LogConfig {\n+\treturn &LogConfig{\n+\t\tConsole: &consoleLoggerCfg{\n+\t\t\tLoggerConfig: LoggerConfig{\n+\t\t\t\tCallSite: callSiteShort,\n+\t\t\t},\n+\t\t},\n+\t\tFile: &LoggerConfig{\n+\t\t\tCallSite: callSiteOff,\n+\t\t},\n+\t}\n+}\n+\n+// LoggerConfig holds options for a particular logger.\n+//\n+//nolint:lll\n+type LoggerConfig struct {\n+\tDisable      bool   `long:\"disable\" description:\"Disable this logger.\"`\n+\tNoTimestamps bool   `long:\"no-timestamps\" description:\"Omit timestamps from log lines.\"`\n+\tCallSite     string `long:\"call-site\" description:\"Include the call-site of each log line.\" choice:\"off\" choice:\"short\" choice:\"long\"`\n+}\n+\n+// HandlerOptions returns the set of btclog.HandlerOptions that the state of the\n+// config struct translates to.\n+func (cfg *LoggerConfig) HandlerOptions() []btclog.HandlerOption {\n+\topts := []btclog.HandlerOption{\n+\t\t// The default skip depth used by the logging library is 6 but\n+\t\t// since we wrap the logging handlers with another level of\n+\t\t// abstraction with the handlerSet, we increase the skip depth\n+\t\t// to 7 here.\n+\t\tbtclog.WithCallSiteSkipDepth(7),\n+\t}\n+\n+\tif cfg.NoTimestamps {\n+\t\topts = append(opts, btclog.WithNoTimestamp())\n+\t}\n+\n+\tswitch cfg.CallSite {\n+\tcase callSiteShort:\n+\t\topts = append(opts, btclog.WithCallerFlags(btclog.Lshortfile))\n+\tcase callSiteLong:\n+\t\topts = append(opts, btclog.WithCallerFlags(btclog.Llongfile))\n+\t}\n+\n+\treturn opts\n+}\n+\n+// consoleLoggerCfg extends the LoggerConfig struct by adding a Color option\n+// which is only available for a console logger.\n+//\n+//nolint:lll\n+type consoleLoggerCfg struct {\n+\tLoggerConfig\n+\tStyle bool `long:\"style\" description:\"If set, the output will be styled with color and fonts\"`\n+}\n+\n+// HandlerOptions returns the set of btclog.HandlerOptions that the state of the\n+// config struct translates to.\n+func (cfg *consoleLoggerCfg) HandlerOptions() []btclog.HandlerOption {\n+\topts := cfg.LoggerConfig.HandlerOptions()\n+\n+\tif !cfg.Style {\n+\t\treturn opts\n+\t}\n+\n+\treturn append(\n+\t\topts, btclog.WithStyledLevel(\n+\t\t\tfunc(l btclogv1.Level) string {\n+\t\t\t\treturn styleString(\n+\t\t\t\t\tfmt.Sprintf(\"[%s]\", l),\n+\t\t\t\t\tboldSeq,\n+\t\t\t\t\tstring(ansiColoSeq(l)),\n+\t\t\t\t)\n+\t\t\t},\n+\t\t),\n+\t\tbtclog.WithStyledCallSite(\n+\t\t\tfunc(file string, line int) string {\n+\t\t\t\tstr := fmt.Sprintf(\"%s:%d\", file, line)\n+\n+\t\t\t\treturn styleString(str, faintSeq)\n+\t\t\t},\n+\t\t),\n+\t\tbtclog.WithStyledKeys(func(key string) string {\n+\t\t\treturn styleString(key, faintSeq)\n+\t\t}),\n+\t)\n+}\n+\n+func styleString(s string, styles ...string) string {\n+\tif len(styles) == 0 {\n+\t\treturn s\n+\t}\n+\n+\tseq := strings.Join(styles, \";\")\n+\tif seq == \"\" {\n+\t\treturn s\n+\t}\n+\n+\treturn fmt.Sprintf(\"%s%sm%s%sm\", csi, seq, s, csi+resetSeq)\n+}\n+\n+type ansiColorSeq string\n+\n+const (\n+\tansiColorSeqDarkTeal  ansiColorSeq = \"38;5;30\"\n+\tansiColorSeqDarkBlue  ansiColorSeq = \"38;5;63\"\n+\tansiColorSeqLightBlue ansiColorSeq = \"38;5;86\"\n+\tansiColorSeqYellow    ansiColorSeq = \"38;5;192\"\n+\tansiColorSeqRed       ansiColorSeq = \"38;5;204\"\n+\tansiColorSeqPink      ansiColorSeq = \"38;5;134\"\n+)\n+\n+func ansiColoSeq(l btclogv1.Level) ansiColorSeq {\n+\tswitch l {\n+\tcase btclog.LevelTrace:\n+\t\treturn ansiColorSeqDarkTeal\n+\tcase btclog.LevelDebug:\n+\t\treturn ansiColorSeqDarkBlue\n+\tcase btclog.LevelWarn:\n+\t\treturn ansiColorSeqYellow\n+\tcase btclog.LevelError:\n+\t\treturn ansiColorSeqRed\n+\tcase btclog.LevelCritical:\n+\t\treturn ansiColorSeqPink\n+\tdefault:\n+\t\treturn ansiColorSeqLightBlue\n+\t}\n+}\ndiff --git a/build/handler_sets.go b/build/handler_sets.go\nnew file mode 100644\nindex 0000000000..d9786a8580\n--- /dev/null\n+++ b/build/handler_sets.go\n@@ -0,0 +1,201 @@\n+package build\n+\n+import (\n+\t\"context\"\n+\t\"log/slog\"\n+\n+\tbtclogv1 \"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n+)\n+\n+// handlerSet is an implementation of Handler that abstracts away multiple\n+// Handlers.\n+type handlerSet struct {\n+\tlevel btclogv1.Level\n+\tset   []btclog.Handler\n+}\n+\n+// newHandlerSet constructs a new HandlerSet.\n+func newHandlerSet(level btclogv1.Level, set ...btclog.Handler) *handlerSet {\n+\th := &handlerSet{\n+\t\tset:   set,\n+\t\tlevel: level,\n+\t}\n+\th.SetLevel(level)\n+\n+\treturn h\n+}\n+\n+// Enabled reports whether the handler handles records at the given level.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (h *handlerSet) Enabled(ctx context.Context, level slog.Level) bool {\n+\tfor _, handler := range h.set {\n+\t\tif !handler.Enabled(ctx, level) {\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\n+\treturn true\n+}\n+\n+// Handle handles the Record.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (h *handlerSet) Handle(ctx context.Context, record slog.Record) error {\n+\tfor _, handler := range h.set {\n+\t\tif err := handler.Handle(ctx, record); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// WithAttrs returns a new Handler whose attributes consist of both the\n+// receiver's attributes and the arguments.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (h *handlerSet) WithAttrs(attrs []slog.Attr) slog.Handler {\n+\tnewSet := &reducedSet{set: make([]slog.Handler, len(h.set))}\n+\tfor i, handler := range h.set {\n+\t\tnewSet.set[i] = handler.WithAttrs(attrs)\n+\t}\n+\n+\treturn newSet\n+}\n+\n+// WithGroup returns a new Handler with the given group appended to the\n+// receiver's existing groups.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (h *handlerSet) WithGroup(name string) slog.Handler {\n+\tnewSet := &reducedSet{set: make([]slog.Handler, len(h.set))}\n+\tfor i, handler := range h.set {\n+\t\tnewSet.set[i] = handler.WithGroup(name)\n+\t}\n+\n+\treturn newSet\n+}\n+\n+// SubSystem creates a new Handler with the given sub-system tag.\n+//\n+// NOTE: this is part of the Handler interface.\n+func (h *handlerSet) SubSystem(tag string) btclog.Handler {\n+\tnewSet := &handlerSet{set: make([]btclog.Handler, len(h.set))}\n+\tfor i, handler := range h.set {\n+\t\tnewSet.set[i] = handler.SubSystem(tag)\n+\t}\n+\n+\treturn newSet\n+}\n+\n+// SetLevel changes the logging level of the Handler to the passed\n+// level.\n+//\n+// NOTE: this is part of the btclog.Handler interface.\n+func (h *handlerSet) SetLevel(level btclogv1.Level) {\n+\tfor _, handler := range h.set {\n+\t\thandler.SetLevel(level)\n+\t}\n+\th.level = level\n+}\n+\n+// Level returns the current logging level of the Handler.\n+//\n+// NOTE: this is part of the btclog.Handler interface.\n+func (h *handlerSet) Level() btclogv1.Level {\n+\treturn h.level\n+}\n+\n+// A compile-time check to ensure that handlerSet implements btclog.Handler.\n+var _ btclog.Handler = (*handlerSet)(nil)\n+\n+// reducedSet is an implementation of the slog.Handler interface which is\n+// itself backed by multiple slog.Handlers. This is used by the handlerSet\n+// WithGroup and WithAttrs methods so that we can apply the WithGroup and\n+// WithAttrs to the underlying handlers in the set. These calls, however,\n+// produce slog.Handlers and not btclog.Handlers. So the reducedSet represents\n+// the resulting set produced.\n+type reducedSet struct {\n+\tset []slog.Handler\n+}\n+\n+// Enabled reports whether the handler handles records at the given level.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (r *reducedSet) Enabled(ctx context.Context, level slog.Level) bool {\n+\tfor _, handler := range r.set {\n+\t\tif !handler.Enabled(ctx, level) {\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\n+\treturn true\n+}\n+\n+// Handle handles the Record.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (r *reducedSet) Handle(ctx context.Context, record slog.Record) error {\n+\tfor _, handler := range r.set {\n+\t\tif err := handler.Handle(ctx, record); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// WithAttrs returns a new Handler whose attributes consist of both the\n+// receiver's attributes and the arguments.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (r *reducedSet) WithAttrs(attrs []slog.Attr) slog.Handler {\n+\tnewSet := &reducedSet{set: make([]slog.Handler, len(r.set))}\n+\tfor i, handler := range r.set {\n+\t\tnewSet.set[i] = handler.WithAttrs(attrs)\n+\t}\n+\n+\treturn newSet\n+}\n+\n+// WithGroup returns a new Handler with the given group appended to the\n+// receiver's existing groups.\n+//\n+// NOTE: this is part of the slog.Handler interface.\n+func (r *reducedSet) WithGroup(name string) slog.Handler {\n+\tnewSet := &reducedSet{set: make([]slog.Handler, len(r.set))}\n+\tfor i, handler := range r.set {\n+\t\tnewSet.set[i] = handler.WithGroup(name)\n+\t}\n+\n+\treturn newSet\n+}\n+\n+// A compile-time check to ensure that handlerSet implements slog.Handler.\n+var _ slog.Handler = (*reducedSet)(nil)\n+\n+// subLogGenerator implements the SubLogCreator backed by a Handler.\n+type subLogGenerator struct {\n+\thandler btclog.Handler\n+}\n+\n+// newSubLogGenerator constructs a new subLogGenerator from a Handler.\n+func newSubLogGenerator(handler btclog.Handler) *subLogGenerator {\n+\treturn &subLogGenerator{\n+\t\thandler: handler,\n+\t}\n+}\n+\n+// Logger returns a new logger for a particular sub-system.\n+//\n+// NOTE: this is part of the SubLogCreator interface.\n+func (b *subLogGenerator) Logger(subsystemTag string) btclog.Logger {\n+\thandler := b.handler.SubSystem(subsystemTag)\n+\n+\treturn btclog.NewSLogger(handler)\n+}\n+\n+// A compile-time check to ensure that handlerSet implements slog.Handler.\n+var _ SubLogCreator = (*subLogGenerator)(nil)\ndiff --git a/build/handlers.go b/build/handlers.go\nnew file mode 100644\nindex 0000000000..9ffbdd70a6\n--- /dev/null\n+++ b/build/handlers.go\n@@ -0,0 +1,23 @@\n+package build\n+\n+import (\n+\t\"os\"\n+\n+\t\"github.com/btcsuite/btclog/v2\"\n+)\n+\n+// NewDefaultLogHandlers returns the standard console logger and rotating log\n+// writer handlers that we generally want to use. It also applies the various\n+// config options to the loggers.\n+func NewDefaultLogHandlers(cfg *LogConfig, rotator *RotatingLogWriter) (\n+\tbtclog.Handler, btclog.Handler) {\n+\n+\tconsoleLogHandler := btclog.NewDefaultHandler(\n+\t\tos.Stdout, cfg.Console.HandlerOptions()...,\n+\t)\n+\tlogFileHandler := btclog.NewDefaultHandler(\n+\t\trotator, cfg.File.HandlerOptions()...,\n+\t)\n+\n+\treturn consoleLogHandler, logFileHandler\n+}\ndiff --git a/build/log.go b/build/log.go\nindex 484b48e422..568f9f618c 100644\n--- a/build/log.go\n+++ b/build/log.go\n@@ -1,11 +1,9 @@\n package build\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"strings\"\n+\t\"os\"\n \n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // LogType is an indicating the type of logging specified by the build flag.\n@@ -62,17 +60,6 @@ func SuportedLogCompressor(logCompressor string) bool {\n \treturn ok\n }\n \n-// LogWriter is a stub type whose behavior can be changed using the build flags\n-// \"stdlog\" and \"nolog\". The default behavior is to write to both stdout and the\n-// RotatorPipe. Passing \"stdlog\" will cause it only to write to stdout, and\n-// \"nolog\" implements Write as a no-op.\n-type LogWriter struct {\n-\t// RotatorPipe is the write-end pipe for writing to the log rotator.  It\n-\t// is written to by the Write method of the LogWriter type. This only\n-\t// needs to be set if neither the stdlog or nolog builds are set.\n-\tRotatorPipe *io.PipeWriter\n-}\n-\n // NewSubLogger constructs a new subsystem log from the current LogWriter\n // implementation. This is primarily intended for use with stdlog, as the actual\n // writer is shared amongst all instantiations.\n@@ -106,8 +93,10 @@ func NewSubLogger(subsystem string,\n \t\t// that they share the same backend, since all output is written\n \t\t// to std out.\n \t\tcase LogTypeStdOut:\n-\t\t\tbackend := btclog.NewBackend(&LogWriter{})\n-\t\t\tlogger := backend.Logger(subsystem)\n+\t\t\tbackend := btclog.NewDefaultHandler(os.Stdout)\n+\t\t\tlogger := btclog.NewSLogger(\n+\t\t\t\tbackend.SubSystem(subsystem),\n+\t\t\t)\n \n \t\t\t// Set the logging level of the stdout logger to use the\n \t\t\t// configured logging level specified by build flags.\n@@ -121,114 +110,3 @@ func NewSubLogger(subsystem string,\n \t// For any other configurations, we'll disable logging.\n \treturn btclog.Disabled\n }\n-\n-// SubLoggers is a type that holds a map of subsystem loggers keyed by their\n-// subsystem name.\n-type SubLoggers map[string]btclog.Logger\n-\n-// LeveledSubLogger provides the ability to retrieve the subsystem loggers of\n-// a logger and set their log levels individually or all at once.\n-type LeveledSubLogger interface {\n-\t// SubLoggers returns the map of all registered subsystem loggers.\n-\tSubLoggers() SubLoggers\n-\n-\t// SupportedSubsystems returns a slice of strings containing the names\n-\t// of the supported subsystems. Should ideally correspond to the keys\n-\t// of the subsystem logger map and be sorted.\n-\tSupportedSubsystems() []string\n-\n-\t// SetLogLevel assigns an individual subsystem logger a new log level.\n-\tSetLogLevel(subsystemID string, logLevel string)\n-\n-\t// SetLogLevels assigns all subsystem loggers the same new log level.\n-\tSetLogLevels(logLevel string)\n-}\n-\n-// ParseAndSetDebugLevels attempts to parse the specified debug level and set\n-// the levels accordingly on the given logger. An appropriate error is returned\n-// if anything is invalid.\n-func ParseAndSetDebugLevels(level string, logger LeveledSubLogger) error {\n-\t// Split at the delimiter.\n-\tlevels := strings.Split(level, \",\")\n-\tif len(levels) == 0 {\n-\t\treturn fmt.Errorf(\"invalid log level: %v\", level)\n-\t}\n-\n-\t// If the first entry has no =, treat is as the log level for all\n-\t// subsystems.\n-\tglobalLevel := levels[0]\n-\tif !strings.Contains(globalLevel, \"=\") {\n-\t\t// Validate debug log level.\n-\t\tif !validLogLevel(globalLevel) {\n-\t\t\tstr := \"the specified debug level [%v] is invalid\"\n-\t\t\treturn fmt.Errorf(str, globalLevel)\n-\t\t}\n-\n-\t\t// Change the logging level for all subsystems.\n-\t\tlogger.SetLogLevels(globalLevel)\n-\n-\t\t// The rest will target specific subsystems.\n-\t\tlevels = levels[1:]\n-\t}\n-\n-\t// Go through the subsystem/level pairs while detecting issues and\n-\t// update the log levels accordingly.\n-\tfor _, logLevelPair := range levels {\n-\t\tif !strings.Contains(logLevelPair, \"=\") {\n-\t\t\tstr := \"the specified debug level contains an \" +\n-\t\t\t\t\"invalid subsystem/level pair [%v]\"\n-\t\t\treturn fmt.Errorf(str, logLevelPair)\n-\t\t}\n-\n-\t\t// Extract the specified subsystem and log level.\n-\t\tfields := strings.Split(logLevelPair, \"=\")\n-\t\tif len(fields) != 2 {\n-\t\t\tstr := \"the specified debug level has an invalid \" +\n-\t\t\t\t\"format [%v] -- use format subsystem1=level1,\" +\n-\t\t\t\t\"subsystem2=level2\"\n-\t\t\treturn fmt.Errorf(str, logLevelPair)\n-\t\t}\n-\t\tsubsysID, logLevel := fields[0], fields[1]\n-\t\tsubLoggers := logger.SubLoggers()\n-\n-\t\t// Validate subsystem.\n-\t\tif _, exists := subLoggers[subsysID]; !exists {\n-\t\t\tstr := \"the specified subsystem [%v] is invalid -- \" +\n-\t\t\t\t\"supported subsystems are %v\"\n-\t\t\treturn fmt.Errorf(\n-\t\t\t\tstr, subsysID, logger.SupportedSubsystems(),\n-\t\t\t)\n-\t\t}\n-\n-\t\t// Validate log level.\n-\t\tif !validLogLevel(logLevel) {\n-\t\t\tstr := \"the specified debug level [%v] is invalid\"\n-\t\t\treturn fmt.Errorf(str, logLevel)\n-\t\t}\n-\n-\t\tlogger.SetLogLevel(subsysID, logLevel)\n-\t}\n-\n-\treturn nil\n-}\n-\n-// validLogLevel returns whether or not logLevel is a valid debug log level.\n-func validLogLevel(logLevel string) bool {\n-\tswitch logLevel {\n-\tcase \"trace\":\n-\t\tfallthrough\n-\tcase \"debug\":\n-\t\tfallthrough\n-\tcase \"info\":\n-\t\tfallthrough\n-\tcase \"warn\":\n-\t\tfallthrough\n-\tcase \"error\":\n-\t\tfallthrough\n-\tcase \"critical\":\n-\t\tfallthrough\n-\tcase \"off\":\n-\t\treturn true\n-\t}\n-\treturn false\n-}\ndiff --git a/build/log_default.go b/build/log_default.go\nindex bbb6d4c390..6e027e0dd0 100644\n--- a/build/log_default.go\n+++ b/build/log_default.go\n@@ -3,17 +3,6 @@\n \n package build\n \n-import \"os\"\n-\n // LoggingType is a log type that writes to both stdout and the log rotator, if\n // present.\n const LoggingType = LogTypeDefault\n-\n-// Write writes the byte slice to both stdout and the log rotator, if present.\n-func (w *LogWriter) Write(b []byte) (int, error) {\n-\tos.Stdout.Write(b)\n-\tif w.RotatorPipe != nil {\n-\t\tw.RotatorPipe.Write(b)\n-\t}\n-\treturn len(b), nil\n-}\ndiff --git a/build/log_nolog.go b/build/log_nolog.go\nindex 32199ada00..75f17bf66b 100644\n--- a/build/log_nolog.go\n+++ b/build/log_nolog.go\n@@ -5,8 +5,3 @@ package build\n \n // LoggingType is a log type that writes no logs.\n const LoggingType = LogTypeNone\n-\n-// Write is a noop.\n-func (w *LogWriter) Write(b []byte) (int, error) {\n-\treturn len(b), nil\n-}\ndiff --git a/build/log_shutdown.go b/build/log_shutdown.go\nindex e5a63410bc..dfabe1cb6f 100644\n--- a/build/log_shutdown.go\n+++ b/build/log_shutdown.go\n@@ -1,7 +1,9 @@\n package build\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"context\"\n+\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // ShutdownLogger wraps an existing logger with a shutdown function which will\n@@ -41,3 +43,16 @@ func (s *ShutdownLogger) Critical(v ...interface{}) {\n \ts.Logger.Info(\"Sending request for shutdown\")\n \ts.shutdown()\n }\n+\n+// CriticalS writes a structured log with the given message and key-value pair\n+// attributes with LevelCritical to the log. It will then call the shutdown\n+// logger's shutdown function to prompt safe shutdown.\n+//\n+// Note: it is part of the btclog.Logger interface.\n+func (s *ShutdownLogger) CriticalS(ctx context.Context, msg string, err error,\n+\tattr ...interface{}) {\n+\n+\ts.Logger.CriticalS(ctx, msg, err, attr...)\n+\ts.Logger.Info(\"Sending request for shutdown\")\n+\ts.shutdown()\n+}\ndiff --git a/build/log_stdlog.go b/build/log_stdlog.go\nindex 34a8c1bf02..461474376d 100644\n--- a/build/log_stdlog.go\n+++ b/build/log_stdlog.go\n@@ -3,13 +3,5 @@\n \n package build\n \n-import \"os\"\n-\n // LoggingType is a log type that only writes to stdout.\n const LoggingType = LogTypeStdOut\n-\n-// Write writes the provided byte slice to stdout.\n-func (w *LogWriter) Write(b []byte) (int, error) {\n-\tos.Stdout.Write(b)\n-\treturn len(b), nil\n-}\ndiff --git a/build/logrotator.go b/build/logrotator.go\nindex aa4683bba2..2ae105fde0 100644\n--- a/build/logrotator.go\n+++ b/build/logrotator.go\n@@ -6,9 +6,7 @@ import (\n \t\"io\"\n \t\"os\"\n \t\"path/filepath\"\n-\t\"sort\"\n \n-\t\"github.com/btcsuite/btclog\"\n \t\"github.com/jrick/logrotate/rotator\"\n \t\"github.com/klauspost/compress/zstd\"\n )\n@@ -16,45 +14,18 @@ import (\n // RotatingLogWriter is a wrapper around the LogWriter that supports log file\n // rotation.\n type RotatingLogWriter struct {\n-\tlogWriter *LogWriter\n+\t// pipe is the write-end pipe for writing to the log rotator.\n+\tpipe *io.PipeWriter\n \n-\tbackendLog *btclog.Backend\n-\n-\tlogRotator *rotator.Rotator\n-\n-\tsubsystemLoggers SubLoggers\n+\trotator *rotator.Rotator\n }\n \n-// A compile time check to ensure RotatingLogWriter implements the\n-// LeveledSubLogger interface.\n-var _ LeveledSubLogger = (*RotatingLogWriter)(nil)\n-\n // NewRotatingLogWriter creates a new file rotating log writer.\n //\n // NOTE: `InitLogRotator` must be called to set up log rotation after creating\n // the writer.\n func NewRotatingLogWriter() *RotatingLogWriter {\n-\tlogWriter := &LogWriter{}\n-\tbackendLog := btclog.NewBackend(logWriter)\n-\treturn &RotatingLogWriter{\n-\t\tlogWriter:        logWriter,\n-\t\tbackendLog:       backendLog,\n-\t\tsubsystemLoggers: SubLoggers{},\n-\t}\n-}\n-\n-// GenSubLogger creates a new sublogger. A shutdown callback function\n-// is provided to be able to shutdown in case of a critical error.\n-func (r *RotatingLogWriter) GenSubLogger(tag string, shutdown func()) btclog.Logger {\n-\tlogger := r.backendLog.Logger(tag)\n-\treturn NewShutdownLogger(logger, shutdown)\n-}\n-\n-// RegisterSubLogger registers a new subsystem logger.\n-func (r *RotatingLogWriter) RegisterSubLogger(subsystem string,\n-\tlogger btclog.Logger) {\n-\n-\tr.subsystemLoggers[subsystem] = logger\n+\treturn &RotatingLogWriter{}\n }\n \n // InitLogRotator initializes the log file rotator to write logs to logFile and\n@@ -68,7 +39,8 @@ func (r *RotatingLogWriter) InitLogRotator(logFile, logCompressor string,\n \tif err != nil {\n \t\treturn fmt.Errorf(\"failed to create log directory: %w\", err)\n \t}\n-\tr.logRotator, err = rotator.New(\n+\n+\tr.rotator, err = rotator.New(\n \t\tlogFile, int64(maxLogFileSize*1024), false, maxLogFiles,\n \t)\n \tif err != nil {\n@@ -94,7 +66,7 @@ func (r *RotatingLogWriter) InitLogRotator(logFile, logCompressor string,\n \t}\n \n \t// Apply the compressor and its file suffix to the log rotator.\n-\tr.logRotator.SetCompressor(c, logCompressors[logCompressor])\n+\tr.rotator.SetCompressor(c, logCompressors[logCompressor])\n \n \t// Run rotator as a goroutine now but make sure we catch any errors\n \t// that happen in case something with the rotation goes wrong during\n@@ -102,75 +74,32 @@ func (r *RotatingLogWriter) InitLogRotator(logFile, logCompressor string,\n \t// create a new logfile for whatever reason).\n \tpr, pw := io.Pipe()\n \tgo func() {\n-\t\terr := r.logRotator.Run(pr)\n+\t\terr := r.rotator.Run(pr)\n \t\tif err != nil {\n \t\t\t_, _ = fmt.Fprintf(os.Stderr,\n \t\t\t\t\"failed to run file rotator: %v\\n\", err)\n \t\t}\n \t}()\n \n-\tr.logWriter.RotatorPipe = pw\n-\treturn nil\n-}\n+\tr.pipe = pw\n \n-// Close closes the underlying log rotator if it has already been created.\n-func (r *RotatingLogWriter) Close() error {\n-\tif r.logRotator != nil {\n-\t\treturn r.logRotator.Close()\n-\t}\n \treturn nil\n }\n \n-// SubLoggers returns all currently registered subsystem loggers for this log\n-// writer.\n-//\n-// NOTE: This is part of the LeveledSubLogger interface.\n-func (r *RotatingLogWriter) SubLoggers() SubLoggers {\n-\treturn r.subsystemLoggers\n-}\n-\n-// SupportedSubsystems returns a sorted string slice of all keys in the\n-// subsystems map, corresponding to the names of the subsystems.\n-//\n-// NOTE: This is part of the LeveledSubLogger interface.\n-func (r *RotatingLogWriter) SupportedSubsystems() []string {\n-\t// Convert the subsystemLoggers map keys to a string slice.\n-\tsubsystems := make([]string, 0, len(r.subsystemLoggers))\n-\tfor subsysID := range r.subsystemLoggers {\n-\t\tsubsystems = append(subsystems, subsysID)\n+// Write writes the byte slice to the log rotator, if present.\n+func (r *RotatingLogWriter) Write(b []byte) (int, error) {\n+\tif r.rotator != nil {\n+\t\treturn r.rotator.Write(b)\n \t}\n \n-\t// Sort the subsystems for stable display.\n-\tsort.Strings(subsystems)\n-\treturn subsystems\n+\treturn len(b), nil\n }\n \n-// SetLogLevel sets the logging level for provided subsystem. Invalid\n-// subsystems are ignored. Uninitialized subsystems are dynamically created as\n-// needed.\n-//\n-// NOTE: This is part of the LeveledSubLogger interface.\n-func (r *RotatingLogWriter) SetLogLevel(subsystemID string, logLevel string) {\n-\t// Ignore invalid subsystems.\n-\tlogger, ok := r.subsystemLoggers[subsystemID]\n-\tif !ok {\n-\t\treturn\n+// Close closes the underlying log rotator if it has already been created.\n+func (r *RotatingLogWriter) Close() error {\n+\tif r.rotator != nil {\n+\t\treturn r.rotator.Close()\n \t}\n \n-\t// Defaults to info if the log level is invalid.\n-\tlevel, _ := btclog.LevelFromString(logLevel)\n-\tlogger.SetLevel(level)\n-}\n-\n-// SetLogLevels sets the log level for all subsystem loggers to the passed\n-// level. It also dynamically creates the subsystem loggers as needed, so it\n-// can be used to initialize the logging system.\n-//\n-// NOTE: This is part of the LeveledSubLogger interface.\n-func (r *RotatingLogWriter) SetLogLevels(logLevel string) {\n-\t// Configure all sub-systems with the new logging level. Dynamically\n-\t// create loggers as needed.\n-\tfor subsystemID := range r.subsystemLoggers {\n-\t\tr.SetLogLevel(subsystemID, logLevel)\n-\t}\n+\treturn nil\n }\ndiff --git a/build/prefix_log.go b/build/prefix_log.go\nindex 926b3f58f3..3db114ffcf 100644\n--- a/build/prefix_log.go\n+++ b/build/prefix_log.go\n@@ -1,18 +1,25 @@\n package build\n \n-import \"github.com/btcsuite/btclog\"\n+import (\n+\t\"context\"\n+\n+\tbtclogv1 \"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n+)\n \n // PrefixLog is a pass-through logger that adds a prefix to every logged line.\n type PrefixLog struct {\n \tlog    btclog.Logger\n \tprefix string\n+\tattr   []any\n }\n \n // NewPrefixLog instantiates a new prefixed logger.\n-func NewPrefixLog(prefix string, log btclog.Logger) *PrefixLog {\n+func NewPrefixLog(prefix string, log btclog.Logger, attrs ...any) *PrefixLog {\n \treturn &PrefixLog{\n-\t\tprefix: prefix,\n \t\tlog:    log,\n+\t\tprefix: prefix,\n+\t\tattr:   attrs,\n \t}\n }\n \n@@ -22,10 +29,16 @@ func (p *PrefixLog) addFormatPrefix(s string) string {\n }\n \n // addArgsPrefix prepends the prefix to a list of arguments.\n-func (p *PrefixLog) addArgsPrefix(args []interface{}) []interface{} {\n+func (p *PrefixLog) addArgsPrefix(args []any) []any {\n \treturn append([]interface{}{p.prefix}, args...)\n }\n \n+// mergeAttr merges the given set of attributes with any attributes that the\n+// logger was initialised with.\n+func (p *PrefixLog) mergeAttr(attrs []any) []any {\n+\treturn append(append([]any{}, attrs...), p.attr...)\n+}\n+\n // Tracef formats message according to format specifier and writes to to log\n // with LevelTrace.\n func (p *PrefixLog) Tracef(format string, params ...interface{}) {\n@@ -98,13 +111,55 @@ func (p *PrefixLog) Critical(v ...interface{}) {\n \tp.log.Critical(p.addArgsPrefix(v)...)\n }\n \n+// TraceS writes a structured log with the given message and key-value pair\n+// attributes with LevelTrace to the log.\n+func (p *PrefixLog) TraceS(ctx context.Context, msg string, attrs ...any) {\n+\tp.log.TraceS(ctx, p.addFormatPrefix(msg), p.mergeAttr(attrs)...)\n+}\n+\n+// DebugS writes a structured log with the given message and key-value pair\n+// attributes with LevelDebug to the log.\n+func (p *PrefixLog) DebugS(ctx context.Context, msg string, attrs ...any) {\n+\tp.log.DebugS(ctx, p.addFormatPrefix(msg), p.mergeAttr(attrs)...)\n+}\n+\n+// InfoS writes a structured log with the given message and key-value pair\n+// attributes with LevelInfo to the log.\n+func (p *PrefixLog) InfoS(ctx context.Context, msg string, attrs ...any) {\n+\tp.log.InfoS(ctx, p.addFormatPrefix(msg), p.mergeAttr(attrs)...)\n+}\n+\n+// WarnS writes a structured log with the given message and key-value pair\n+// attributes with LevelWarn to the log.\n+func (p *PrefixLog) WarnS(ctx context.Context, msg string, err error,\n+\tattrs ...any) {\n+\n+\tp.log.WarnS(ctx, p.addFormatPrefix(msg), err, p.mergeAttr(attrs)...)\n+}\n+\n+// ErrorS writes a structured log with the given message and key-value pair\n+// attributes with LevelError to the log.\n+func (p *PrefixLog) ErrorS(ctx context.Context, msg string, err error,\n+\tattrs ...any) {\n+\n+\tp.log.ErrorS(ctx, p.addFormatPrefix(msg), err, p.mergeAttr(attrs)...)\n+}\n+\n+// CriticalS writes a structured log with the given message and key-value pair\n+// attributes with LevelCritical to the log.\n+func (p *PrefixLog) CriticalS(ctx context.Context, msg string, err error,\n+\tattrs ...any) {\n+\n+\tp.log.CriticalS(ctx, p.addFormatPrefix(msg), err, p.mergeAttr(attrs)...)\n+}\n+\n // Level returns the current logging level.\n-func (p *PrefixLog) Level() btclog.Level {\n+func (p *PrefixLog) Level() btclogv1.Level {\n \treturn p.log.Level()\n }\n \n // SetLevel changes the logging level to the passed level.\n-func (p *PrefixLog) SetLevel(level btclog.Level) {\n+func (p *PrefixLog) SetLevel(level btclogv1.Level) {\n \tp.log.SetLevel(level)\n }\n \ndiff --git a/build/sub_logger.go b/build/sub_logger.go\nnew file mode 100644\nindex 0000000000..2a68e6cd55\n--- /dev/null\n+++ b/build/sub_logger.go\n@@ -0,0 +1,263 @@\n+package build\n+\n+import (\n+\t\"fmt\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"sync\"\n+\n+\t\"github.com/btcsuite/btclog/v2\"\n+)\n+\n+// SubLogCreator can be used to create a new logger for a particular subsystem.\n+type SubLogCreator interface {\n+\t// Logger returns a new logger for a particular subsytem.\n+\tLogger(subsystemTag string) btclog.Logger\n+}\n+\n+// SubLoggerManager manages a set of subsystem loggers. Level updates will be\n+// applied to all the loggers managed by the manager.\n+type SubLoggerManager struct {\n+\tgenLogger SubLogCreator\n+\n+\tloggers SubLoggers\n+\tmu      sync.Mutex\n+}\n+\n+// A compile time check to ensure SubLoggerManager implements the\n+// LeveledSubLogger interface.\n+var _ LeveledSubLogger = (*SubLoggerManager)(nil)\n+\n+// NewSubLoggerManager constructs a new SubLoggerManager.\n+func NewSubLoggerManager(handlers ...btclog.Handler) *SubLoggerManager {\n+\treturn &SubLoggerManager{\n+\t\tloggers: make(SubLoggers),\n+\t\tgenLogger: newSubLogGenerator(\n+\t\t\tnewHandlerSet(btclog.LevelInfo, handlers...),\n+\t\t),\n+\t}\n+}\n+\n+// GenSubLogger creates a new sub-logger and adds it to the set managed by the\n+// SubLoggerManager. A shutdown callback function is provided to be able to shut\n+// down in case of a critical error.\n+func (r *SubLoggerManager) GenSubLogger(subsystem string,\n+\tshutdown func()) btclog.Logger {\n+\n+\t// Create a new logger with the given subsystem tag.\n+\tlogger := r.genLogger.Logger(subsystem)\n+\n+\t// Wrap the new logger in a Shutdown logger so that the shutdown\n+\t// call back is called if a critical log is ever written via this new\n+\t// logger.\n+\tl := NewShutdownLogger(logger, shutdown)\n+\n+\tr.RegisterSubLogger(subsystem, l)\n+\n+\treturn l\n+}\n+\n+// RegisterSubLogger registers the given logger under the given subsystem name.\n+func (r *SubLoggerManager) RegisterSubLogger(subsystem string,\n+\tlogger btclog.Logger) {\n+\n+\t// Add the new logger to the set of loggers managed by the manager.\n+\tr.mu.Lock()\n+\tr.loggers[subsystem] = logger\n+\tr.mu.Unlock()\n+}\n+\n+// SubLoggers returns all currently registered subsystem loggers for this log\n+// writer.\n+//\n+// NOTE: This is part of the LeveledSubLogger interface.\n+func (r *SubLoggerManager) SubLoggers() SubLoggers {\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\treturn r.loggers\n+}\n+\n+// SupportedSubsystems returns a sorted string slice of all keys in the\n+// subsystems map, corresponding to the names of the subsystems.\n+//\n+// NOTE: This is part of the LeveledSubLogger interface.\n+func (r *SubLoggerManager) SupportedSubsystems() []string {\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\t// Convert the subsystemLoggers map keys to a string slice.\n+\tsubsystems := make([]string, 0, len(r.loggers))\n+\tfor subsysID := range r.loggers {\n+\t\tsubsystems = append(subsystems, subsysID)\n+\t}\n+\n+\t// Sort the subsystems for stable display.\n+\tsort.Strings(subsystems)\n+\n+\treturn subsystems\n+}\n+\n+// SetLogLevel sets the logging level for provided subsystem. Invalid\n+// subsystems are ignored. Uninitialized subsystems are dynamically created as\n+// needed.\n+//\n+// NOTE: This is part of the LeveledSubLogger interface.\n+func (r *SubLoggerManager) SetLogLevel(subsystemID string, logLevel string) {\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\tr.setLogLevelUnsafe(subsystemID, logLevel)\n+}\n+\n+// setLogLevelUnsafe sets the logging level for provided subsystem. Invalid\n+// subsystems are ignored. Uninitialized subsystems are dynamically created as\n+// needed.\n+//\n+// NOTE: the SubLoggerManager mutex must be held before calling this method.\n+func (r *SubLoggerManager) setLogLevelUnsafe(subsystemID string,\n+\tlogLevel string) {\n+\n+\t// Ignore invalid subsystems.\n+\tlogger, ok := r.loggers[subsystemID]\n+\tif !ok {\n+\t\treturn\n+\t}\n+\n+\t// Defaults to info if the log level is invalid.\n+\tlevel, _ := btclog.LevelFromString(logLevel)\n+\n+\tlogger.SetLevel(level)\n+}\n+\n+// SetLogLevels sets the log level for all subsystem loggers to the passed\n+// level. It also dynamically creates the subsystem loggers as needed, so it\n+// can be used to initialize the logging system.\n+//\n+// NOTE: This is part of the LeveledSubLogger interface.\n+func (r *SubLoggerManager) SetLogLevels(logLevel string) {\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\t// Configure all sub-systems with the new logging level. Dynamically\n+\t// create loggers as needed.\n+\tfor subsystemID := range r.loggers {\n+\t\tr.setLogLevelUnsafe(subsystemID, logLevel)\n+\t}\n+}\n+\n+// SubLoggers is a type that holds a map of subsystem loggers keyed by their\n+// subsystem name.\n+type SubLoggers map[string]btclog.Logger\n+\n+// LeveledSubLogger provides the ability to retrieve the subsystem loggers of\n+// a logger and set their log levels individually or all at once.\n+type LeveledSubLogger interface {\n+\t// SubLoggers returns the map of all registered subsystem loggers.\n+\tSubLoggers() SubLoggers\n+\n+\t// SupportedSubsystems returns a slice of strings containing the names\n+\t// of the supported subsystems. Should ideally correspond to the keys\n+\t// of the subsystem logger map and be sorted.\n+\tSupportedSubsystems() []string\n+\n+\t// SetLogLevel assigns an individual subsystem logger a new log level.\n+\tSetLogLevel(subsystemID string, logLevel string)\n+\n+\t// SetLogLevels assigns all subsystem loggers the same new log level.\n+\tSetLogLevels(logLevel string)\n+}\n+\n+// ParseAndSetDebugLevels attempts to parse the specified debug level and set\n+// the levels accordingly on the given logger. An appropriate error is returned\n+// if anything is invalid.\n+func ParseAndSetDebugLevels(level string, logger LeveledSubLogger) error {\n+\t// Split at the delimiter.\n+\tlevels := strings.Split(level, \",\")\n+\tif len(levels) == 0 {\n+\t\treturn fmt.Errorf(\"invalid log level: %v\", level)\n+\t}\n+\n+\t// If the first entry has no =, treat is as the log level for all\n+\t// subsystems.\n+\tglobalLevel := levels[0]\n+\tif !strings.Contains(globalLevel, \"=\") {\n+\t\t// Validate debug log level.\n+\t\tif !validLogLevel(globalLevel) {\n+\t\t\tstr := \"the specified debug level [%v] is invalid\"\n+\n+\t\t\treturn fmt.Errorf(str, globalLevel)\n+\t\t}\n+\n+\t\t// Change the logging level for all subsystems.\n+\t\tlogger.SetLogLevels(globalLevel)\n+\n+\t\t// The rest will target specific subsystems.\n+\t\tlevels = levels[1:]\n+\t}\n+\n+\t// Go through the subsystem/level pairs while detecting issues and\n+\t// update the log levels accordingly.\n+\tfor _, logLevelPair := range levels {\n+\t\tif !strings.Contains(logLevelPair, \"=\") {\n+\t\t\tstr := \"the specified debug level contains an \" +\n+\t\t\t\t\"invalid subsystem/level pair [%v]\"\n+\n+\t\t\treturn fmt.Errorf(str, logLevelPair)\n+\t\t}\n+\n+\t\t// Extract the specified subsystem and log level.\n+\t\tfields := strings.Split(logLevelPair, \"=\")\n+\t\tif len(fields) != 2 {\n+\t\t\tstr := \"the specified debug level has an invalid \" +\n+\t\t\t\t\"format [%v] -- use format subsystem1=level1,\" +\n+\t\t\t\t\"subsystem2=level2\"\n+\n+\t\t\treturn fmt.Errorf(str, logLevelPair)\n+\t\t}\n+\t\tsubsysID, logLevel := fields[0], fields[1]\n+\t\tsubLoggers := logger.SubLoggers()\n+\n+\t\t// Validate subsystem.\n+\t\tif _, exists := subLoggers[subsysID]; !exists {\n+\t\t\tstr := \"the specified subsystem [%v] is invalid -- \" +\n+\t\t\t\t\"supported subsystems are %v\"\n+\n+\t\t\treturn fmt.Errorf(\n+\t\t\t\tstr, subsysID, logger.SupportedSubsystems(),\n+\t\t\t)\n+\t\t}\n+\n+\t\t// Validate log level.\n+\t\tif !validLogLevel(logLevel) {\n+\t\t\tstr := \"the specified debug level [%v] is invalid\"\n+\t\t\treturn fmt.Errorf(str, logLevel)\n+\t\t}\n+\n+\t\tlogger.SetLogLevel(subsysID, logLevel)\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validLogLevel returns whether or not logLevel is a valid debug log level.\n+func validLogLevel(logLevel string) bool {\n+\tswitch logLevel {\n+\tcase \"trace\":\n+\t\tfallthrough\n+\tcase \"debug\":\n+\t\tfallthrough\n+\tcase \"info\":\n+\t\tfallthrough\n+\tcase \"warn\":\n+\t\tfallthrough\n+\tcase \"error\":\n+\t\tfallthrough\n+\tcase \"critical\":\n+\t\tfallthrough\n+\tcase \"off\":\n+\t\treturn true\n+\t}\n+\n+\treturn false\n+}\ndiff --git a/chainntnfs/log.go b/chainntnfs/log.go\nindex f21246053e..cc06e3bc63 100644\n--- a/chainntnfs/log.go\n+++ b/chainntnfs/log.go\n@@ -1,7 +1,7 @@\n package chainntnfs\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/chainreg/log.go b/chainreg/log.go\nindex 2670794786..1a8e1148f8 100644\n--- a/chainreg/log.go\n+++ b/chainreg/log.go\n@@ -1,7 +1,7 @@\n package chainreg\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/chanacceptor/log.go b/chanacceptor/log.go\nindex 26e131c438..3f9c925d30 100644\n--- a/chanacceptor/log.go\n+++ b/chanacceptor/log.go\n@@ -1,7 +1,7 @@\n package chanacceptor\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/chanbackup/log.go b/chanbackup/log.go\nindex 34eb9e93ff..6f7147caa4 100644\n--- a/chanbackup/log.go\n+++ b/chanbackup/log.go\n@@ -1,7 +1,7 @@\n package chanbackup\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/chanfitness/log.go b/chanfitness/log.go\nindex 626f7538dc..b729c51f7b 100644\n--- a/chanfitness/log.go\n+++ b/chanfitness/log.go\n@@ -1,7 +1,7 @@\n package chanfitness\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/channeldb/log.go b/channeldb/log.go\nindex 10b1b54d3c..b423154d3e 100644\n--- a/channeldb/log.go\n+++ b/channeldb/log.go\n@@ -1,7 +1,7 @@\n package channeldb\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \tmig \"github.com/lightningnetwork/lnd/channeldb/migration\"\n \t\"github.com/lightningnetwork/lnd/channeldb/migration12\"\ndiff --git a/channeldb/migration/log.go b/channeldb/migration/log.go\nindex 5085596dac..b60e34978f 100644\n--- a/channeldb/migration/log.go\n+++ b/channeldb/migration/log.go\n@@ -1,6 +1,6 @@\n package migration\n \n-import \"github.com/btcsuite/btclog\"\n+import \"github.com/btcsuite/btclog/v2\"\n \n // log is a logger that is initialized as disabled.  This means the package will\n // not perform any logging by default until a logger is set.\ndiff --git a/channeldb/migration12/log.go b/channeldb/migration12/log.go\nindex 1352e52aea..9581808421 100644\n--- a/channeldb/migration12/log.go\n+++ b/channeldb/migration12/log.go\n@@ -1,7 +1,7 @@\n package migration12\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration13/log.go b/channeldb/migration13/log.go\nindex 33ec1812af..ff8169f30e 100644\n--- a/channeldb/migration13/log.go\n+++ b/channeldb/migration13/log.go\n@@ -1,7 +1,7 @@\n package migration13\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration16/log.go b/channeldb/migration16/log.go\nindex cb946854cf..ec134bc186 100644\n--- a/channeldb/migration16/log.go\n+++ b/channeldb/migration16/log.go\n@@ -1,7 +1,7 @@\n package migration16\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration20/log.go b/channeldb/migration20/log.go\nindex da75760d08..23a97de7d0 100644\n--- a/channeldb/migration20/log.go\n+++ b/channeldb/migration20/log.go\n@@ -1,7 +1,7 @@\n package migration20\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled. This means the package\ndiff --git a/channeldb/migration24/log.go b/channeldb/migration24/log.go\nindex a74cb039d4..0424df79e4 100644\n--- a/channeldb/migration24/log.go\n+++ b/channeldb/migration24/log.go\n@@ -1,7 +1,7 @@\n package migration24\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration25/log.go b/channeldb/migration25/log.go\nindex e308d2755a..07b9f60ad7 100644\n--- a/channeldb/migration25/log.go\n+++ b/channeldb/migration25/log.go\n@@ -1,7 +1,7 @@\n package migration25\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration26/log.go b/channeldb/migration26/log.go\nindex b7326a3130..3bcd276ad3 100644\n--- a/channeldb/migration26/log.go\n+++ b/channeldb/migration26/log.go\n@@ -1,7 +1,7 @@\n package migration26\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration27/log.go b/channeldb/migration27/log.go\nindex 70cdb41ec8..c4dd1a1284 100644\n--- a/channeldb/migration27/log.go\n+++ b/channeldb/migration27/log.go\n@@ -1,7 +1,7 @@\n package migration27\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration29/log.go b/channeldb/migration29/log.go\nindex b9ae19a5f5..3563c074f3 100644\n--- a/channeldb/migration29/log.go\n+++ b/channeldb/migration29/log.go\n@@ -1,6 +1,6 @@\n package migration29\n \n-import \"github.com/btcsuite/btclog\"\n+import \"github.com/btcsuite/btclog/v2\"\n \n // log is a logger that is initialized as disabled. This means the package will\n // not perform any logging by default until a logger is set.\ndiff --git a/channeldb/migration30/log.go b/channeldb/migration30/log.go\nindex 1a1672fe6f..8b58b99a62 100644\n--- a/channeldb/migration30/log.go\n+++ b/channeldb/migration30/log.go\n@@ -1,7 +1,7 @@\n package migration30\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration31/log.go b/channeldb/migration31/log.go\nindex 2a863b7c1c..42b6cadfb9 100644\n--- a/channeldb/migration31/log.go\n+++ b/channeldb/migration31/log.go\n@@ -1,7 +1,7 @@\n package migration31\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration32/log.go b/channeldb/migration32/log.go\nindex 98709c28ea..2a921a600d 100644\n--- a/channeldb/migration32/log.go\n+++ b/channeldb/migration32/log.go\n@@ -1,7 +1,7 @@\n package migration32\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration33/log.go b/channeldb/migration33/log.go\nindex e9b271f5df..62b286b28a 100644\n--- a/channeldb/migration33/log.go\n+++ b/channeldb/migration33/log.go\n@@ -1,7 +1,7 @@\n package migration33\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channeldb/migration_01_to_11/log.go b/channeldb/migration_01_to_11/log.go\nindex b169b5afb1..22c3932618 100644\n--- a/channeldb/migration_01_to_11/log.go\n+++ b/channeldb/migration_01_to_11/log.go\n@@ -1,7 +1,7 @@\n package migration_01_to_11\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/channelnotifier/log.go b/channelnotifier/log.go\nindex 5756f69976..6044e84548 100644\n--- a/channelnotifier/log.go\n+++ b/channelnotifier/log.go\n@@ -1,7 +1,7 @@\n package channelnotifier\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/cluster/log.go b/cluster/log.go\nindex 11ced0871c..5551809c12 100644\n--- a/cluster/log.go\n+++ b/cluster/log.go\n@@ -1,7 +1,7 @@\n package cluster\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/config.go b/config.go\nindex 2ad83c1265..a427e4c7d4 100644\n--- a/config.go\n+++ b/config.go\n@@ -21,6 +21,7 @@ import (\n \n \t\"github.com/btcsuite/btcd/btcutil\"\n \t\"github.com/btcsuite/btcd/chaincfg\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \tflags \"github.com/jessevdk/go-flags\"\n \t\"github.com/lightninglabs/neutrino\"\n \t\"github.com/lightningnetwork/lnd/autopilot\"\n@@ -494,9 +495,11 @@ type Config struct {\n \n \tGRPC *GRPCConfig `group:\"grpc\" namespace:\"grpc\"`\n \n-\t// LogWriter is the root logger that all of the daemon's subloggers are\n+\t// SubLogMgr is the root logger that all the daemon's subloggers are\n \t// hooked up to.\n-\tLogWriter *build.RotatingLogWriter\n+\tSubLogMgr  *build.SubLoggerManager\n+\tLogRotator *build.RotatingLogWriter\n+\tLogConfig  *build.LogConfig `group:\"logging\" namespace:\"logging\"`\n \n \t// networkDir is the path to the directory of the currently active\n \t// network. This path will hold the files related to each different\n@@ -714,7 +717,7 @@ func DefaultConfig() Config {\n \t\tMaxChannelFeeAllocation:   htlcswitch.DefaultMaxLinkFeeAllocation,\n \t\tMaxCommitFeeRateAnchors:   lnwallet.DefaultAnchorsCommitMaxFeeRateSatPerVByte,\n \t\tMaxFeeExposure:            uint64(htlcswitch.DefaultMaxFeeExposure.ToSatoshis()),\n-\t\tLogWriter:                 build.NewRotatingLogWriter(),\n+\t\tLogRotator:                build.NewRotatingLogWriter(),\n \t\tDB:                        lncfg.DefaultDB(),\n \t\tCluster:                   lncfg.DefaultCluster(),\n \t\tRPCMiddleware:             lncfg.DefaultRPCMiddleware(),\n@@ -736,6 +739,7 @@ func DefaultConfig() Config {\n \t\t\tServerPingTimeout: defaultGrpcServerPingTimeout,\n \t\t\tClientPingMinWait: defaultGrpcClientPingMinWait,\n \t\t},\n+\t\tLogConfig:         build.DefaultLogConfig(),\n \t\tWtClient:          lncfg.DefaultWtClientCfg(),\n \t\tHTTPHeaderTimeout: DefaultHTTPHeaderTimeout,\n \t}\n@@ -1400,10 +1404,24 @@ func ValidateConfig(cfg Config, interceptor signal.Interceptor, fileParser,\n \t\tlncfg.NormalizeNetwork(cfg.ActiveNetParams.Name),\n \t)\n \n-\t// A log writer must be passed in, otherwise we can't function and would\n-\t// run into a panic later on.\n-\tif cfg.LogWriter == nil {\n-\t\treturn nil, mkErr(\"log writer missing in config\")\n+\tvar (\n+\t\tlogCfg                            = cfg.LogConfig\n+\t\tlogHandlers                       []btclog.Handler\n+\t\tconsoleLogHandler, logFileHandler = build.NewDefaultLogHandlers(\n+\t\t\tlogCfg, cfg.LogRotator,\n+\t\t)\n+\t)\n+\tmaybeAddLogger := func(cmdOptionDisable bool, handler btclog.Handler) {\n+\t\tif !cmdOptionDisable {\n+\t\t\tlogHandlers = append(logHandlers, handler)\n+\t\t}\n+\t}\n+\tswitch build.LoggingType {\n+\tcase build.LogTypeStdOut:\n+\t\tmaybeAddLogger(logCfg.Console.Disable, consoleLogHandler)\n+\tcase build.LogTypeDefault:\n+\t\tmaybeAddLogger(logCfg.Console.Disable, consoleLogHandler)\n+\t\tmaybeAddLogger(logCfg.File.Disable, logFileHandler)\n \t}\n \n \tif !build.SuportedLogCompressor(cfg.LogCompressor) {\n@@ -1411,16 +1429,18 @@ func ValidateConfig(cfg Config, interceptor signal.Interceptor, fileParser,\n \t\t\tcfg.LogCompressor)\n \t}\n \n+\tcfg.SubLogMgr = build.NewSubLoggerManager(logHandlers...)\n+\n \t// Initialize logging at the default logging level.\n-\tSetupLoggers(cfg.LogWriter, interceptor)\n+\tSetupLoggers(cfg.SubLogMgr, interceptor)\n \n \t// Special show command to list supported subsystems and exit.\n \tif cfg.DebugLevel == \"show\" {\n \t\tfmt.Println(\"Supported subsystems\",\n-\t\t\tcfg.LogWriter.SupportedSubsystems())\n+\t\t\tcfg.SubLogMgr.SupportedSubsystems())\n \t\tos.Exit(0)\n \t}\n-\terr = cfg.LogWriter.InitLogRotator(\n+\terr = cfg.LogRotator.InitLogRotator(\n \t\tfilepath.Join(cfg.LogDir, defaultLogFilename),\n \t\tcfg.LogCompressor, cfg.MaxLogFileSize, cfg.MaxLogFiles,\n \t)\n@@ -1430,7 +1450,7 @@ func ValidateConfig(cfg Config, interceptor signal.Interceptor, fileParser,\n \t}\n \n \t// Parse, validate, and set debug log level(s).\n-\terr = build.ParseAndSetDebugLevels(cfg.DebugLevel, cfg.LogWriter)\n+\terr = build.ParseAndSetDebugLevels(cfg.DebugLevel, cfg.SubLogMgr)\n \tif err != nil {\n \t\tstr := \"error parsing debug level: %v\"\n \t\treturn nil, &lncfg.UsageError{Err: mkErr(str, err)}\ndiff --git a/config_builder.go b/config_builder.go\nindex 1c3a842ef1..7cc1a112d2 100644\n--- a/config_builder.go\n+++ b/config_builder.go\n@@ -18,7 +18,7 @@ import (\n \t\"github.com/btcsuite/btcd/chaincfg\"\n \t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n \t\"github.com/btcsuite/btcd/wire\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/btcsuite/btcwallet/chain\"\n \t\"github.com/btcsuite/btcwallet/waddrmgr\"\n \t\"github.com/btcsuite/btcwallet/wallet\"\ndiff --git a/contractcourt/contract_resolver.go b/contractcourt/contract_resolver.go\nindex 5acf800649..691822610a 100644\n--- a/contractcourt/contract_resolver.go\n+++ b/contractcourt/contract_resolver.go\n@@ -7,7 +7,7 @@ import (\n \t\"io\"\n \n \t\"github.com/btcsuite/btcd/wire\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/channeldb\"\n \t\"github.com/lightningnetwork/lnd/fn\"\ndiff --git a/contractcourt/log.go b/contractcourt/log.go\nindex 2e52cc97dd..21844cc3d6 100644\n--- a/contractcourt/log.go\n+++ b/contractcourt/log.go\n@@ -1,7 +1,7 @@\n package contractcourt\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/discovery/log.go b/discovery/log.go\nindex f09e7fb71b..0f885e9067 100644\n--- a/discovery/log.go\n+++ b/discovery/log.go\n@@ -1,7 +1,7 @@\n package discovery\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/docs/release-notes/release-notes-0.19.0.md b/docs/release-notes/release-notes-0.19.0.md\nindex 9ff363c03c..3b20a054e6 100644\n--- a/docs/release-notes/release-notes-0.19.0.md\n+++ b/docs/release-notes/release-notes-0.19.0.md\n@@ -75,6 +75,19 @@\n   config settings to its own dedicated group. The old ones still work but will\n   be removed in a future release.\n  \n+* [Update to use structured \n+  logging](https://github.com/lightningnetwork/lnd/pull/9083). This also \n+  introduces a new `--logging.console.disable` option to disable logs being \n+  written to stdout and a new `--logging.file.disable` option to disable writing \n+  logs to the standard log file. It also adds `--logging.console.no-timestamps`\n+  and `--logging.file.no-timestamps` which can be used to omit timestamps in\n+  log messages for the respective loggers. The new `--logging.console.call-site`\n+  and `--logging.file.call-site` options can be used to include the call-site of\n+  a log line. The options for this include \"off\" (default), \"short\" (source file\n+  name and line number) and \"long\" (full path to source file and line number). \n+  Finally, the new `--logging.console.style` option can be used under the `dev` \n+  build tag to add styling to console logging.\n+ \n ## Breaking Changes\n ## Performance Improvements\n \ndiff --git a/funding/log.go b/funding/log.go\nindex 887d4f7192..357483db40 100644\n--- a/funding/log.go\n+++ b/funding/log.go\n@@ -1,7 +1,7 @@\n package funding\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/go.mod b/go.mod\nindex b1e63f8cff..692e5ad75f 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -9,7 +9,8 @@ require (\n \tgithub.com/btcsuite/btcd/btcutil v1.1.5\n \tgithub.com/btcsuite/btcd/btcutil/psbt v1.1.8\n \tgithub.com/btcsuite/btcd/chaincfg/chainhash v1.1.0\n-\tgithub.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f\n+\tgithub.com/btcsuite/btclog v0.0.0-20241003133417-09c4e92e319c\n+\tgithub.com/btcsuite/btclog/v2 v2.0.0-20241017175713-3428138b75c7\n \tgithub.com/btcsuite/btcwallet v0.16.10-0.20240912233857-ffb143c77cc5\n \tgithub.com/btcsuite/btcwallet/wallet/txauthor v1.3.5\n \tgithub.com/btcsuite/btcwallet/wallet/txrules v1.2.2\ndiff --git a/go.sum b/go.sum\nindex 5c3b97e88a..d0f1882f32 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -89,8 +89,11 @@ github.com/btcsuite/btcd/chaincfg/chainhash v1.0.0/go.mod h1:7SFka0XMvUgj3hfZtyd\n github.com/btcsuite/btcd/chaincfg/chainhash v1.0.1/go.mod h1:7SFka0XMvUgj3hfZtydOrQY2mwhPclbT2snogU7SQQc=\n github.com/btcsuite/btcd/chaincfg/chainhash v1.1.0 h1:59Kx4K6lzOW5w6nFlA0v5+lk/6sjybR934QNHSJZPTQ=\n github.com/btcsuite/btcd/chaincfg/chainhash v1.1.0/go.mod h1:7SFka0XMvUgj3hfZtydOrQY2mwhPclbT2snogU7SQQc=\n-github.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f h1:bAs4lUbRJpnnkd9VhRV3jjAVU7DJVjMaK+IsvSeZvFo=\n github.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f/go.mod h1:TdznJufoqS23FtqVCzL0ZqgP5MqXbb4fg/WgDys70nA=\n+github.com/btcsuite/btclog v0.0.0-20241003133417-09c4e92e319c h1:4HxD1lBUGUddhzgaNgrCPsFWd7cGYNpeFUgd9ZIgyM0=\n+github.com/btcsuite/btclog v0.0.0-20241003133417-09c4e92e319c/go.mod h1:w7xnGOhwT3lmrS4H3b/D1XAXxvh+tbhUm8xeHN2y3TQ=\n+github.com/btcsuite/btclog/v2 v2.0.0-20241017175713-3428138b75c7 h1:3Ct3zN3VCEKVm5nceWBBEKczc+jvTfVyOEG71ob2Yuc=\n+github.com/btcsuite/btclog/v2 v2.0.0-20241017175713-3428138b75c7/go.mod h1:XItGUfVOxotJL8kkuk2Hj3EVow5KCugXl3wWfQ6K0AE=\n github.com/btcsuite/btcutil v0.0.0-20190425235716-9e5f4b9a998d/go.mod h1:+5NJ2+qvTyV9exUAL/rxXi3DcLg2Ts+ymUAY5y4NvMg=\n github.com/btcsuite/btcwallet v0.16.10-0.20240912233857-ffb143c77cc5 h1:zYy233eUBvkF3lq2MUkybEhxhDsrRDSgiToIKN57mtk=\n github.com/btcsuite/btcwallet v0.16.10-0.20240912233857-ffb143c77cc5/go.mod h1:1HJXYbjJzgumlnxOC2+ViR1U+gnHWoOn7WeK5OfY1eU=\ndiff --git a/graph/log.go b/graph/log.go\nindex 7d2f741c66..4e03156688 100644\n--- a/graph/log.go\n+++ b/graph/log.go\n@@ -1,7 +1,7 @@\n package graph\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/htlcswitch/hop/log.go b/htlcswitch/hop/log.go\nindex 4134d1662f..7b412b95d2 100644\n--- a/htlcswitch/hop/log.go\n+++ b/htlcswitch/hop/log.go\n@@ -1,7 +1,7 @@\n package hop\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized with no output filters.  This\ndiff --git a/htlcswitch/link.go b/htlcswitch/link.go\nindex 7ce618574a..3532a23181 100644\n--- a/htlcswitch/link.go\n+++ b/htlcswitch/link.go\n@@ -13,7 +13,7 @@ import (\n \n \t\"github.com/btcsuite/btcd/btcutil\"\n \t\"github.com/btcsuite/btcd/wire\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/channeldb\"\n \t\"github.com/lightningnetwork/lnd/channeldb/models\"\ndiff --git a/htlcswitch/log.go b/htlcswitch/log.go\nindex 79259be91c..f20310f8dd 100644\n--- a/htlcswitch/log.go\n+++ b/htlcswitch/log.go\n@@ -1,7 +1,7 @@\n package htlcswitch\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/htlcswitch/hop\"\n )\ndiff --git a/invoices/log.go b/invoices/log.go\nindex 5e3a0b17be..291a6aa68e 100644\n--- a/invoices/log.go\n+++ b/invoices/log.go\n@@ -1,7 +1,7 @@\n package invoices\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lncfg/db.go b/lncfg/db.go\nindex 4072a51fea..1f7e0ae35d 100644\n--- a/lncfg/db.go\n+++ b/lncfg/db.go\n@@ -7,7 +7,7 @@ import (\n \t\"path/filepath\"\n \t\"time\"\n \n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/kvdb\"\n \t\"github.com/lightningnetwork/lnd/kvdb/etcd\"\n \t\"github.com/lightningnetwork/lnd/kvdb/postgres\"\ndiff --git a/lncfg/log.go b/lncfg/log.go\nindex a22a215daf..f48516eb1f 100644\n--- a/lncfg/log.go\n+++ b/lncfg/log.go\n@@ -1,7 +1,7 @@\n package lncfg\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnd.go b/lnd.go\nindex 8fb38b8f38..f511811950 100644\n--- a/lnd.go\n+++ b/lnd.go\n@@ -149,7 +149,7 @@ func Main(cfg *Config, lisCfg ListenerCfg, implCfg *ImplementationCfg,\n \n \tdefer func() {\n \t\tltndLog.Info(\"Shutdown complete\\n\")\n-\t\terr := cfg.LogWriter.Close()\n+\t\terr := cfg.LogRotator.Close()\n \t\tif err != nil {\n \t\t\tltndLog.Errorf(\"Could not close log rotator: %v\", err)\n \t\t}\ndiff --git a/lnrpc/autopilotrpc/log.go b/lnrpc/autopilotrpc/log.go\nindex 113486d0d9..6eace32443 100644\n--- a/lnrpc/autopilotrpc/log.go\n+++ b/lnrpc/autopilotrpc/log.go\n@@ -1,7 +1,7 @@\n package autopilotrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/chainrpc/log.go b/lnrpc/chainrpc/log.go\nindex 6b68e324aa..d592e55b06 100644\n--- a/lnrpc/chainrpc/log.go\n+++ b/lnrpc/chainrpc/log.go\n@@ -1,7 +1,7 @@\n package chainrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/devrpc/log.go b/lnrpc/devrpc/log.go\nindex 9cae145e52..8e13c1001f 100644\n--- a/lnrpc/devrpc/log.go\n+++ b/lnrpc/devrpc/log.go\n@@ -1,7 +1,7 @@\n package devrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/invoicesrpc/log.go b/lnrpc/invoicesrpc/log.go\nindex f3644fdb51..7f284ad2ea 100644\n--- a/lnrpc/invoicesrpc/log.go\n+++ b/lnrpc/invoicesrpc/log.go\n@@ -1,7 +1,7 @@\n package invoicesrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/neutrinorpc/log.go b/lnrpc/neutrinorpc/log.go\nindex fb2cd79ecf..be5c9ff1b1 100644\n--- a/lnrpc/neutrinorpc/log.go\n+++ b/lnrpc/neutrinorpc/log.go\n@@ -1,7 +1,7 @@\n package neutrinorpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/peersrpc/log.go b/lnrpc/peersrpc/log.go\nindex 3fd59789cb..2196fdfc91 100644\n--- a/lnrpc/peersrpc/log.go\n+++ b/lnrpc/peersrpc/log.go\n@@ -1,7 +1,7 @@\n package peersrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/routerrpc/log.go b/lnrpc/routerrpc/log.go\nindex 5c4ad01d83..a705e2b615 100644\n--- a/lnrpc/routerrpc/log.go\n+++ b/lnrpc/routerrpc/log.go\n@@ -1,7 +1,7 @@\n package routerrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/signrpc/log.go b/lnrpc/signrpc/log.go\nindex 2dc6533435..cce9f7c436 100644\n--- a/lnrpc/signrpc/log.go\n+++ b/lnrpc/signrpc/log.go\n@@ -1,7 +1,7 @@\n package signrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/verrpc/log.go b/lnrpc/verrpc/log.go\nindex 3553b56806..a6f3546dd9 100644\n--- a/lnrpc/verrpc/log.go\n+++ b/lnrpc/verrpc/log.go\n@@ -1,7 +1,7 @@\n package verrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/walletrpc/log.go b/lnrpc/walletrpc/log.go\nindex 1916c549f2..eef49e7d0c 100644\n--- a/lnrpc/walletrpc/log.go\n+++ b/lnrpc/walletrpc/log.go\n@@ -1,7 +1,7 @@\n package walletrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/watchtowerrpc/log.go b/lnrpc/watchtowerrpc/log.go\nindex a71b7827fb..2563affe1c 100644\n--- a/lnrpc/watchtowerrpc/log.go\n+++ b/lnrpc/watchtowerrpc/log.go\n@@ -1,7 +1,7 @@\n package watchtowerrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnrpc/websocket_proxy.go b/lnrpc/websocket_proxy.go\nindex 4a46929657..b333ba1c8e 100644\n--- a/lnrpc/websocket_proxy.go\n+++ b/lnrpc/websocket_proxy.go\n@@ -12,7 +12,7 @@ import (\n \t\"strings\"\n \t\"time\"\n \n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/gorilla/websocket\"\n \t\"golang.org/x/net/context\"\n )\ndiff --git a/lnrpc/wtclientrpc/config.go b/lnrpc/wtclientrpc/config.go\nindex b95fb1197c..81fb5f18f6 100644\n--- a/lnrpc/wtclientrpc/config.go\n+++ b/lnrpc/wtclientrpc/config.go\n@@ -1,7 +1,7 @@\n package wtclientrpc\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/lncfg\"\n \t\"github.com/lightningnetwork/lnd/watchtower/wtclient\"\n )\ndiff --git a/lnwallet/btcwallet/log.go b/lnwallet/btcwallet/log.go\nindex 878db08e64..d0a589fed2 100644\n--- a/lnwallet/btcwallet/log.go\n+++ b/lnwallet/btcwallet/log.go\n@@ -1,7 +1,7 @@\n package btcwallet\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnwallet/chainfee/log.go b/lnwallet/chainfee/log.go\nindex d5d0405782..536f73eff6 100644\n--- a/lnwallet/chainfee/log.go\n+++ b/lnwallet/chainfee/log.go\n@@ -1,7 +1,7 @@\n package chainfee\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnwallet/chancloser/log.go b/lnwallet/chancloser/log.go\nindex 7189ed2a35..46c777aae8 100644\n--- a/lnwallet/chancloser/log.go\n+++ b/lnwallet/chancloser/log.go\n@@ -1,7 +1,7 @@\n package chancloser\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnwallet/chanfunding/log.go b/lnwallet/chanfunding/log.go\nindex 159a96ca1d..4ef34f9852 100644\n--- a/lnwallet/chanfunding/log.go\n+++ b/lnwallet/chanfunding/log.go\n@@ -1,7 +1,7 @@\n package chanfunding\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/lnwallet/channel.go b/lnwallet/channel.go\nindex 3a980df1a3..33360228d5 100644\n--- a/lnwallet/channel.go\n+++ b/lnwallet/channel.go\n@@ -21,7 +21,7 @@ import (\n \t\"github.com/btcsuite/btcd/mempool\"\n \t\"github.com/btcsuite/btcd/txscript\"\n \t\"github.com/btcsuite/btcd/wire\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/davecgh/go-spew/spew\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/chainntnfs\"\ndiff --git a/lnwallet/log.go b/lnwallet/log.go\nindex 7c8ce51984..5fde019d76 100644\n--- a/lnwallet/log.go\n+++ b/lnwallet/log.go\n@@ -1,7 +1,7 @@\n package lnwallet\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/btcsuite/btcwallet/chain\"\n \tbtcwallet \"github.com/btcsuite/btcwallet/wallet\"\n \t\"github.com/btcsuite/btcwallet/wtxmgr\"\ndiff --git a/lnwallet/rpcwallet/log.go b/lnwallet/rpcwallet/log.go\nindex 190aa623ad..21b8a73d68 100644\n--- a/lnwallet/rpcwallet/log.go\n+++ b/lnwallet/rpcwallet/log.go\n@@ -1,7 +1,7 @@\n package rpcwallet\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/log.go b/log.go\nindex a24c39fd40..faa4f15f4d 100644\n--- a/log.go\n+++ b/log.go\n@@ -3,7 +3,8 @@ package lnd\n import (\n \t\"github.com/btcsuite/btcd/connmgr\"\n \t\"github.com/btcsuite/btcd/rpcclient\"\n-\t\"github.com/btcsuite/btclog\"\n+\tbtclogv1 \"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightninglabs/neutrino\"\n \tsphinx \"github.com/lightningnetwork/lightning-onion\"\n \t\"github.com/lightningnetwork/lnd/autopilot\"\n@@ -95,7 +96,7 @@ var (\n \n // genSubLogger creates a logger for a subsystem. We provide an instance of\n // a signal.Interceptor to be able to shutdown in the case of a critical error.\n-func genSubLogger(root *build.RotatingLogWriter,\n+func genSubLogger(root *build.SubLoggerManager,\n \tinterceptor signal.Interceptor) func(string) btclog.Logger {\n \n \t// Create a shutdown function which will request shutdown from our\n@@ -116,7 +117,9 @@ func genSubLogger(root *build.RotatingLogWriter,\n }\n \n // SetupLoggers initializes all package-global logger variables.\n-func SetupLoggers(root *build.RotatingLogWriter, interceptor signal.Interceptor) {\n+//\n+//nolint:lll\n+func SetupLoggers(root *build.SubLoggerManager, interceptor signal.Interceptor) {\n \tgenLogger := genSubLogger(root, interceptor)\n \n \t// Now that we have the proper root logger, we can replace the\n@@ -132,9 +135,9 @@ func SetupLoggers(root *build.RotatingLogWriter, interceptor signal.Interceptor)\n \t// `btcwallet.chain`, which is overwritten by `lnwallet`. To ensure the\n \t// overwriting works, we need to initialize the loggers here so they\n \t// can be overwritten later.\n-\tAddSubLogger(root, \"BTCN\", interceptor, neutrino.UseLogger)\n-\tAddSubLogger(root, \"CMGR\", interceptor, connmgr.UseLogger)\n-\tAddSubLogger(root, \"RPCC\", interceptor, rpcclient.UseLogger)\n+\tAddV1SubLogger(root, \"BTCN\", interceptor, neutrino.UseLogger)\n+\tAddV1SubLogger(root, \"CMGR\", interceptor, connmgr.UseLogger)\n+\tAddV1SubLogger(root, \"RPCC\", interceptor, rpcclient.UseLogger)\n \n \t// Some of the loggers declared in the main lnd package are also used\n \t// in sub packages.\n@@ -149,7 +152,7 @@ func SetupLoggers(root *build.RotatingLogWriter, interceptor signal.Interceptor)\n \tAddSubLogger(root, \"CNCT\", interceptor, contractcourt.UseLogger)\n \tAddSubLogger(root, \"UTXN\", interceptor, contractcourt.UseNurseryLogger)\n \tAddSubLogger(root, \"BRAR\", interceptor, contractcourt.UseBreachLogger)\n-\tAddSubLogger(root, \"SPHX\", interceptor, sphinx.UseLogger)\n+\tAddV1SubLogger(root, \"SPHX\", interceptor, sphinx.UseLogger)\n \tAddSubLogger(root, \"SWPR\", interceptor, sweep.UseLogger)\n \tAddSubLogger(root, \"SGNR\", interceptor, signrpc.UseLogger)\n \tAddSubLogger(root, \"WLKT\", interceptor, walletrpc.UseLogger)\n@@ -174,13 +177,13 @@ func SetupLoggers(root *build.RotatingLogWriter, interceptor signal.Interceptor)\n \tAddSubLogger(root, routerrpc.Subsystem, interceptor, routerrpc.UseLogger)\n \tAddSubLogger(root, chanfitness.Subsystem, interceptor, chanfitness.UseLogger)\n \tAddSubLogger(root, verrpc.Subsystem, interceptor, verrpc.UseLogger)\n-\tAddSubLogger(root, healthcheck.Subsystem, interceptor, healthcheck.UseLogger)\n+\tAddV1SubLogger(root, healthcheck.Subsystem, interceptor, healthcheck.UseLogger)\n \tAddSubLogger(root, chainreg.Subsystem, interceptor, chainreg.UseLogger)\n \tAddSubLogger(root, chanacceptor.Subsystem, interceptor, chanacceptor.UseLogger)\n \tAddSubLogger(root, funding.Subsystem, interceptor, funding.UseLogger)\n \tAddSubLogger(root, cluster.Subsystem, interceptor, cluster.UseLogger)\n \tAddSubLogger(root, rpcperms.Subsystem, interceptor, rpcperms.UseLogger)\n-\tAddSubLogger(root, tor.Subsystem, interceptor, tor.UseLogger)\n+\tAddV1SubLogger(root, tor.Subsystem, interceptor, tor.UseLogger)\n \tAddSubLogger(root, btcwallet.Subsystem, interceptor, btcwallet.UseLogger)\n \tAddSubLogger(root, rpcwallet.Subsystem, interceptor, rpcwallet.UseLogger)\n \tAddSubLogger(root, peersrpc.Subsystem, interceptor, peersrpc.UseLogger)\n@@ -193,7 +196,7 @@ func SetupLoggers(root *build.RotatingLogWriter, interceptor signal.Interceptor)\n \n // AddSubLogger is a helper method to conveniently create and register the\n // logger of one or more sub systems.\n-func AddSubLogger(root *build.RotatingLogWriter, subsystem string,\n+func AddSubLogger(root *build.SubLoggerManager, subsystem string,\n \tinterceptor signal.Interceptor, useLoggers ...func(btclog.Logger)) {\n \n \t// genSubLogger will return a callback for creating a logger instance,\n@@ -206,9 +209,9 @@ func AddSubLogger(root *build.RotatingLogWriter, subsystem string,\n \tSetSubLogger(root, subsystem, logger, useLoggers...)\n }\n \n-// SetSubLogger is a helper method to conveniently register the logger of a sub\n-// system.\n-func SetSubLogger(root *build.RotatingLogWriter, subsystem string,\n+// SetSubLogger is a helper method to conveniently register the logger of a\n+// sub system.\n+func SetSubLogger(root *build.SubLoggerManager, subsystem string,\n \tlogger btclog.Logger, useLoggers ...func(btclog.Logger)) {\n \n \troot.RegisterSubLogger(subsystem, logger)\n@@ -216,3 +219,31 @@ func SetSubLogger(root *build.RotatingLogWriter, subsystem string,\n \t\tuseLogger(logger)\n \t}\n }\n+\n+// AddV1SubLogger is a helper method to conveniently create and register the\n+// logger of one or more sub systems.\n+func AddV1SubLogger(root *build.SubLoggerManager, subsystem string,\n+\tinterceptor signal.Interceptor, useLoggers ...func(btclogv1.Logger)) {\n+\n+\t// genSubLogger will return a callback for creating a logger instance,\n+\t// which we will give to the root logger.\n+\tgenLogger := genSubLogger(root, interceptor)\n+\n+\t// Create and register just a single logger to prevent them from\n+\t// overwriting each other internally.\n+\tlogger := build.NewSubLogger(subsystem, genLogger)\n+\tSetV1SubLogger(root, subsystem, logger, useLoggers...)\n+}\n+\n+// SetV1SubLogger is a helper method to conveniently register the logger of a\n+// sub system. Note that the btclog v2 logger implements the btclog v1 logger\n+// which is why we can pass the v2 logger to the UseLogger call-backs that\n+// expect the v1 logger.\n+func SetV1SubLogger(root *build.SubLoggerManager, subsystem string,\n+\tlogger btclog.Logger, useLoggers ...func(btclogv1.Logger)) {\n+\n+\troot.RegisterSubLogger(subsystem, logger)\n+\tfor _, useLogger := range useLoggers {\n+\t\tuseLogger(logger)\n+\t}\n+}\ndiff --git a/monitoring/log.go b/monitoring/log.go\nindex a35af85230..95f0efb232 100644\n--- a/monitoring/log.go\n+++ b/monitoring/log.go\n@@ -1,7 +1,7 @@\n package monitoring\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/msgmux/log.go b/msgmux/log.go\nindex 63da4791f9..27abbf271c 100644\n--- a/msgmux/log.go\n+++ b/msgmux/log.go\n@@ -1,7 +1,7 @@\n package msgmux\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/netann/log.go b/netann/log.go\nindex 85d8d68c34..4aaa15bc7a 100644\n--- a/netann/log.go\n+++ b/netann/log.go\n@@ -1,7 +1,7 @@\n package netann\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/peer/brontide.go b/peer/brontide.go\nindex a66618f11c..ae4d373cac 100644\n--- a/peer/brontide.go\n+++ b/peer/brontide.go\n@@ -17,7 +17,7 @@ import (\n \t\"github.com/btcsuite/btcd/connmgr\"\n \t\"github.com/btcsuite/btcd/txscript\"\n \t\"github.com/btcsuite/btcd/wire\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/davecgh/go-spew/spew\"\n \t\"github.com/lightningnetwork/lnd/buffer\"\n \t\"github.com/lightningnetwork/lnd/build\"\ndiff --git a/peer/log.go b/peer/log.go\nindex f0d1a6672f..e4b075d19d 100644\n--- a/peer/log.go\n+++ b/peer/log.go\n@@ -1,7 +1,7 @@\n package peer\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/peernotifier/log.go b/peernotifier/log.go\nindex 1178c0f3df..a06051b263 100644\n--- a/peernotifier/log.go\n+++ b/peernotifier/log.go\n@@ -1,7 +1,7 @@\n package peernotifier\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/routing/blindedpath/log.go b/routing/blindedpath/log.go\nindex 979884fbd4..d6b39d7ef4 100644\n--- a/routing/blindedpath/log.go\n+++ b/routing/blindedpath/log.go\n@@ -1,7 +1,7 @@\n package blindedpath\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/routing/chainview/log.go b/routing/chainview/log.go\nindex b411819dd2..5a2bad27fe 100644\n--- a/routing/chainview/log.go\n+++ b/routing/chainview/log.go\n@@ -1,7 +1,7 @@\n package chainview\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/routing/log.go b/routing/log.go\nindex 741ec084ef..ad248bba98 100644\n--- a/routing/log.go\n+++ b/routing/log.go\n@@ -1,7 +1,7 @@\n package routing\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/routing/chainview\"\n )\ndiff --git a/routing/missioncontrol.go b/routing/missioncontrol.go\nindex d848235d88..be8ce7c9f6 100644\n--- a/routing/missioncontrol.go\n+++ b/routing/missioncontrol.go\n@@ -7,7 +7,7 @@ import (\n \t\"time\"\n \n \t\"github.com/btcsuite/btcd/btcutil\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/btcsuite/btcwallet/walletdb\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/channeldb\"\ndiff --git a/routing/payment_session.go b/routing/payment_session.go\nindex 89fb2fd137..2ab9e4fd41 100644\n--- a/routing/payment_session.go\n+++ b/routing/payment_session.go\n@@ -4,7 +4,7 @@ import (\n \t\"fmt\"\n \n \t\"github.com/btcsuite/btcd/btcec/v2\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/channeldb\"\n \t\"github.com/lightningnetwork/lnd/channeldb/models\"\ndiff --git a/rpcperms/interceptor.go b/rpcperms/interceptor.go\nindex cea9670c48..9bbef0414d 100644\n--- a/rpcperms/interceptor.go\n+++ b/rpcperms/interceptor.go\n@@ -7,7 +7,7 @@ import (\n \t\"sync\"\n \t\"sync/atomic\"\n \n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \tgrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\"\n \t\"github.com/lightningnetwork/lnd/lnrpc\"\n \t\"github.com/lightningnetwork/lnd/macaroons\"\ndiff --git a/rpcperms/log.go b/rpcperms/log.go\nindex 209aacce58..76e4c8d409 100644\n--- a/rpcperms/log.go\n+++ b/rpcperms/log.go\n@@ -1,7 +1,7 @@\n package rpcperms\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/rpcserver.go b/rpcserver.go\nindex 6f43adf7fb..76aa057bf1 100644\n--- a/rpcserver.go\n+++ b/rpcserver.go\n@@ -7341,7 +7341,7 @@ func (r *rpcServer) DebugLevel(ctx context.Context,\n \tif req.Show {\n \t\treturn &lnrpc.DebugLevelResponse{\n \t\t\tSubSystems: strings.Join(\n-\t\t\t\tr.cfg.LogWriter.SupportedSubsystems(), \" \",\n+\t\t\t\tr.cfg.SubLogMgr.SupportedSubsystems(), \" \",\n \t\t\t),\n \t\t}, nil\n \t}\n@@ -7350,12 +7350,12 @@ func (r *rpcServer) DebugLevel(ctx context.Context,\n \n \t// Otherwise, we'll attempt to set the logging level using the\n \t// specified level spec.\n-\terr := build.ParseAndSetDebugLevels(req.LevelSpec, r.cfg.LogWriter)\n+\terr := build.ParseAndSetDebugLevels(req.LevelSpec, r.cfg.SubLogMgr)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n-\tsubLoggers := r.cfg.LogWriter.SubLoggers()\n+\tsubLoggers := r.cfg.SubLogMgr.SubLoggers()\n \t// Sort alphabetically by subsystem name.\n \tvar tags []string\n \tfor t := range subLoggers {\ndiff --git a/sample-lnd.conf b/sample-lnd.conf\nindex b863e60bf6..eeb6a9d311 100644\n--- a/sample-lnd.conf\n+++ b/sample-lnd.conf\n@@ -970,6 +970,33 @@\n ; Instructs lnd to encrypt the private key using the wallet's seed.\n ; tor.encryptkey=false\n \n+[logging]\n+\n+; Disable logging to stdout and stderror.\n+; logging.console.disable=false\n+\n+; Don't add timestamps to logs written to stdout and stderr.\n+; logging.console.no-timestamps=false\n+\n+; Include the log call-site in the log line written to stdout\n+; and stderr. Options include 'off', 'short' and 'long'.\n+; Default:\n+;   logging.console.call-site=off\n+; Example:\n+;   logging.console.call-site=short\n+\n+; Disable logging to the standard LND log file.\n+; logging.file.disable=false\n+\n+; Don't add timestamps to logs written to the standard LND log file.\n+; logging.file.no-timestamps=false\n+\n+; Include the log call-site in the log line written the standard LND\n+; log file. Options include 'off', 'short' and 'long'.\n+; Default:\n+;   logging.file.call-site=off\n+; Example:\n+;   logging.file.call-site=short\n \n [watchtower]\n \ndiff --git a/signal/log.go b/signal/log.go\nindex 6bde284b76..4fcbd6d7aa 100644\n--- a/signal/log.go\n+++ b/signal/log.go\n@@ -1,6 +1,6 @@\n package signal\n \n-import \"github.com/btcsuite/btclog\"\n+import \"github.com/btcsuite/btclog/v2\"\n \n // log is a logger that is initialized with no output filters.  This\n // means the package will not perform any logging by default until the caller\ndiff --git a/subrpcserver_config.go b/subrpcserver_config.go\nindex 8ea5b0ab39..a4ee6d1a16 100644\n--- a/subrpcserver_config.go\n+++ b/subrpcserver_config.go\n@@ -6,7 +6,7 @@ import (\n \t\"reflect\"\n \n \t\"github.com/btcsuite/btcd/chaincfg\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/aliasmgr\"\n \t\"github.com/lightningnetwork/lnd/autopilot\"\n \t\"github.com/lightningnetwork/lnd/chainreg\"\ndiff --git a/sweep/log.go b/sweep/log.go\nindex 1725023758..864c9e37a2 100644\n--- a/sweep/log.go\n+++ b/sweep/log.go\n@@ -1,7 +1,7 @@\n package sweep\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/watchtower/log.go b/watchtower/log.go\nindex 8e9062d958..bfdbe5fcab 100644\n--- a/watchtower/log.go\n+++ b/watchtower/log.go\n@@ -1,7 +1,7 @@\n package watchtower\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/watchtower/lookout\"\n \t\"github.com/lightningnetwork/lnd/watchtower/wtclient\"\ndiff --git a/watchtower/lookout/log.go b/watchtower/lookout/log.go\nindex b569b4ae0a..286478614e 100644\n--- a/watchtower/lookout/log.go\n+++ b/watchtower/lookout/log.go\n@@ -1,7 +1,7 @@\n package lookout\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/watchtower/wtclient/client.go b/watchtower/wtclient/client.go\nindex aef0fe505e..6cf5d9114a 100644\n--- a/watchtower/wtclient/client.go\n+++ b/watchtower/wtclient/client.go\n@@ -11,7 +11,7 @@ import (\n \t\"time\"\n \n \t\"github.com/btcsuite/btcd/btcec/v2\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/channeldb\"\n \t\"github.com/lightningnetwork/lnd/keychain\"\ndiff --git a/watchtower/wtclient/log.go b/watchtower/wtclient/log.go\nindex 8d2e37dda3..dbda57bada 100644\n--- a/watchtower/wtclient/log.go\n+++ b/watchtower/wtclient/log.go\n@@ -1,7 +1,7 @@\n package wtclient\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \ndiff --git a/watchtower/wtclient/queue.go b/watchtower/wtclient/queue.go\nindex ee03d288ad..68e4b6c180 100644\n--- a/watchtower/wtclient/queue.go\n+++ b/watchtower/wtclient/queue.go\n@@ -7,7 +7,7 @@ import (\n \t\"sync/atomic\"\n \t\"time\"\n \n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/watchtower/wtdb\"\n )\n \ndiff --git a/watchtower/wtclient/session_negotiator.go b/watchtower/wtclient/session_negotiator.go\nindex ea17c5b47a..4cba7de62c 100644\n--- a/watchtower/wtclient/session_negotiator.go\n+++ b/watchtower/wtclient/session_negotiator.go\n@@ -7,7 +7,7 @@ import (\n \t\"time\"\n \n \t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/keychain\"\n \t\"github.com/lightningnetwork/lnd/lnwire\"\n \t\"github.com/lightningnetwork/lnd/watchtower/blob\"\ndiff --git a/watchtower/wtclient/session_queue.go b/watchtower/wtclient/session_queue.go\nindex 41ae28f2fa..4ee1597637 100644\n--- a/watchtower/wtclient/session_queue.go\n+++ b/watchtower/wtclient/session_queue.go\n@@ -7,7 +7,7 @@ import (\n \t\"time\"\n \n \t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/input\"\n \t\"github.com/lightningnetwork/lnd/keychain\"\n \t\"github.com/lightningnetwork/lnd/lnwire\"\ndiff --git a/watchtower/wtdb/log.go b/watchtower/wtdb/log.go\nindex fa0a64f6ac..474ff08b9b 100644\n--- a/watchtower/wtdb/log.go\n+++ b/watchtower/wtdb/log.go\n@@ -1,7 +1,7 @@\n package wtdb\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n \t\"github.com/lightningnetwork/lnd/watchtower/wtdb/migration1\"\n \t\"github.com/lightningnetwork/lnd/watchtower/wtdb/migration2\"\ndiff --git a/watchtower/wtdb/migration1/log.go b/watchtower/wtdb/migration1/log.go\nindex 1dc1052809..f741ceaace 100644\n--- a/watchtower/wtdb/migration1/log.go\n+++ b/watchtower/wtdb/migration1/log.go\n@@ -1,7 +1,7 @@\n package migration1\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration2/log.go b/watchtower/wtdb/migration2/log.go\nindex fd64d2fcbe..ae1ca59a18 100644\n--- a/watchtower/wtdb/migration2/log.go\n+++ b/watchtower/wtdb/migration2/log.go\n@@ -1,7 +1,7 @@\n package migration2\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration3/log.go b/watchtower/wtdb/migration3/log.go\nindex 2f14ff5e75..07cc2730ea 100644\n--- a/watchtower/wtdb/migration3/log.go\n+++ b/watchtower/wtdb/migration3/log.go\n@@ -1,7 +1,7 @@\n package migration3\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration4/log.go b/watchtower/wtdb/migration4/log.go\nindex 3a609d76d6..eb85aa5c3a 100644\n--- a/watchtower/wtdb/migration4/log.go\n+++ b/watchtower/wtdb/migration4/log.go\n@@ -1,7 +1,7 @@\n package migration4\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration5/log.go b/watchtower/wtdb/migration5/log.go\nindex f0b725a2ef..d92b108ad6 100644\n--- a/watchtower/wtdb/migration5/log.go\n+++ b/watchtower/wtdb/migration5/log.go\n@@ -1,7 +1,7 @@\n package migration5\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration6/log.go b/watchtower/wtdb/migration6/log.go\nindex e43e7d27ec..b15662bba6 100644\n--- a/watchtower/wtdb/migration6/log.go\n+++ b/watchtower/wtdb/migration6/log.go\n@@ -1,7 +1,7 @@\n package migration6\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration7/log.go b/watchtower/wtdb/migration7/log.go\nindex 39f28b6c00..baf5a0990c 100644\n--- a/watchtower/wtdb/migration7/log.go\n+++ b/watchtower/wtdb/migration7/log.go\n@@ -1,7 +1,7 @@\n package migration7\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtdb/migration8/log.go b/watchtower/wtdb/migration8/log.go\nindex ab35682c5a..412b95f8b8 100644\n--- a/watchtower/wtdb/migration8/log.go\n+++ b/watchtower/wtdb/migration8/log.go\n@@ -1,7 +1,7 @@\n package migration8\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n )\n \n // log is a logger that is initialized as disabled.  This means the package will\ndiff --git a/watchtower/wtserver/log.go b/watchtower/wtserver/log.go\nindex 7939bd97f8..3c901b4861 100644\n--- a/watchtower/wtserver/log.go\n+++ b/watchtower/wtserver/log.go\n@@ -1,7 +1,7 @@\n package wtserver\n \n import (\n-\t\"github.com/btcsuite/btclog\"\n+\t\"github.com/btcsuite/btclog/v2\"\n \t\"github.com/lightningnetwork/lnd/build\"\n )\n \n", "instance_id": "lightningnetwork__lnd-9083", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in its intent: it aims to make logging timestamps optional in the LND (Lightning Network Daemon) system, particularly to avoid redundancy in environments like systemd where timestamps are already added. The reference to a Stack Overflow post provides some context on how this can be achieved. However, the statement lacks critical details such as specific requirements for how the optionality should be implemented (e.g., configuration flags, default behavior), expected input/output formats for configuration, and any constraints or edge cases to consider. Additionally, there are no examples or detailed use cases provided to illustrate the desired functionality. Despite these minor ambiguities, the goal is understandable, and the code changes provide further context, leading to a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is extensive, spanning multiple files and modules within the LND codebase, including updates to logging configurations, handler implementations, and integration across various subsystems. This requires a deep understanding of the codebase architecture and how logging is managed throughout the system. Second, the changes involve upgrading to a new version of the `btclog` library (`btclog/v2`), which introduces structured logging and additional features like optional timestamps and call-site information. This necessitates familiarity with the new library's API and its integration into an existing system. Third, the problem touches on several technical concepts, including Go build tags, logging handler customization, and configuration management, which add to the complexity. Finally, while edge cases and error handling are not explicitly mentioned in the problem statement, the code changes suggest the need to handle different logging configurations (e.g., disabling logs, styling for console output) and ensure backward compatibility with existing logging behavior across various build types. The combination of these factors\u2014significant code changes, architectural impact, and multiple technical concepts\u2014results in a difficulty score of 0.65, reflecting a challenging task that requires substantial expertise in Go and the LND codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "panics if profile is not found\nHello! I opened https://github.com/golang/go/issues/67626 for the Go project to track consuming a fix; my guess is there should be an issue on this project to track creation of a fix.\r\n\r\n### What version of pprof are you using?\r\n\r\nI've checked out 9665fa269a300a7e802154f4df8c4e9ee970fc5c from May 20, currently the latest version of the `main` branch.\r\n\r\n### What operating system and processor architecture are you using?\r\n\r\nlinux/amd64\r\n\r\n### What did you do?\r\n\r\n```\r\n$ go run . ./missing\r\n```\r\n\r\n### What did you expect to see?\r\n\r\n```\r\n$ go run . ./missing\r\n./missing: open ./missing: no such file or directory\r\npprof: failed to fetch any source profiles\r\nexit status 2\r\n```\r\n\r\n### What did you see instead?\r\n\r\n```\r\n$ go run . ./missing\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x8906f7]\r\n\r\ngoroutine 19 [running]:\r\ngithub.com/google/pprof/internal/driver.fetch({0x7fffec08a721, 0x9}, 0xffffffffc4653600, 0xffffffffc4653600, {0x7fc040045278, 0xc000080210}, {0xa68860, 0xc0000aa840})\r\n\t/home/rhysh/pprof/internal/driver/fetch.go:514 +0x2f7\r\ngithub.com/google/pprof/internal/driver.grabProfile(0xc0001e28c0, {0x7fffec08a721?, 0x0?}, {0x0?, 0x0?}, {0xa69c80, 0xc000032fd0}, {0x7fc040045278, 0xc000080210}, {0xa68860, ...})\r\n\t/home/rhysh/pprof/internal/driver/fetch.go:334 +0x131\r\ngithub.com/google/pprof/internal/driver.concurrentGrab.func1(0xc000234000)\r\n\t/home/rhysh/pprof/internal/driver/fetch.go:211 +0x86\r\ncreated by github.com/google/pprof/internal/driver.concurrentGrab in goroutine 34\r\n\t/home/rhysh/pprof/internal/driver/fetch.go:209 +0x8f\r\nexit status 2\r\n```\r\n\r\nBisect says \" a5a03c77bf0842e0592d2be8129bca0b2bd3bb60 is the first bad commit\".\r\n\r\nCC @cuishuang, @aalexand\r\n\n", "patch": "diff --git a/internal/driver/fetch.go b/internal/driver/fetch.go\nindex 95204a39..a94ddf6a 100644\n--- a/internal/driver/fetch.go\n+++ b/internal/driver/fetch.go\n@@ -493,7 +493,7 @@ func fetch(source string, duration, timeout time.Duration, ui plugin.UI, tr http\n \tvar f io.ReadCloser\n \n \t// First determine whether the source is a file, if not, it will be treated as a URL.\n-\tif _, openErr := os.Stat(source); openErr == nil {\n+\tif _, err = os.Stat(source); err == nil {\n \t\tif isPerfFile(source) {\n \t\t\tf, err = convertPerfData(source, ui)\n \t\t} else {\n", "instance_id": "google__pprof-864", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the program panics with a segmentation violation when a profile file is not found, instead of providing a graceful error message. The expected behavior (an error message indicating the file is missing) and the actual behavior (a panic with a stack trace) are explicitly provided, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether the fix should handle only the \"file not found\" case or other potential errors during file access. Additionally, there are no explicit constraints or requirements for how the error should be handled beyond the example output. Edge cases, such as invalid file permissions or symbolic links, are not mentioned. Despite these minor gaps, the issue is valid and the intent is clear enough to proceed with a solution.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). The issue involves a straightforward bug fix in the error handling logic of a file operation within the `fetch.go` file. The code change provided is minimal, involving only a single line modification to rename a variable for consistency (though this does not directly address the panic issue, it suggests the area of focus). Solving the core issue\u2014preventing the panic when a file is not found\u2014requires basic understanding of Go's error handling mechanisms and the `os.Stat` function. The scope of the change is limited to a single function or block of code, with no apparent impact on the broader architecture or interactions with other modules. The technical concepts involved are basic (file I/O and error checking in Go), and while some edge cases (e.g., file not found, permission issues) need to be considered, they are not complex to handle. Overall, this is a simple bug fix that a developer with basic Go experience can address with minimal effort.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add check for changed order identifier by ACME Server (in case it is misbehaving)\n### Welcome\n\n- [x] Yes, I've searched similar issues on GitHub and didn't find any.\n\n### How do you use lego?\n\nThrough Traefik\n\n### Effective version of lego\n\nv4.23.1\n\n### Detailed Description\n\nHello,\n\nfirst of all, thank you very much for your effort and project. Lego in combination with Traefik is really great!\n\nWe recently faced a problem where we wanted to issue a wildcard certificate as part of SANs using a private ACME offering of a CAB member. Our Traefik config was like this:\n```yaml\nentryPoints:\n  websecure:\n    address: \":443\"\n    http:\n      tls:\n        certResolver: acmeResolver\n          domains:\n          - main: \"test.example.com\"\n            sans:\n              - \"*.test.example.com\"\n```\n\nWe were wondering, why we would not get a wildcard certificate, even though Traefik would save the certificate with associated wildcard in _acme.json_. Also, no errors by Lego or Traefik were logged at all.\n\nI then tried with certbot which worked as expected and directly with Lego, which showed the same behaviour as Traefik. To debug this, I added dumps in the do() function is sender.go (https://github.com/kiliant/lego/blob/debug-wildcard/acme/api/internal/sender/sender.go#L88).\nIt became evident, that the ACME server truncated the wildcard from the respective (actually immutable) identifier (which they must not do, according to [RFC 8555 Section 7.1.3](https://www.rfc-editor.org/rfc/rfc8555#section-7.1.3)). As Lego unmarshals the response from the ACME server into its acme.Order struct, the Identifiers given by the ACME server are used for CSR creation.\n\nTherefore, identifiers `[{Type:dns Value:test.example.com} {Type:dns Value:*.test.example.com}]` became `[{Type:dns Value:test.example.com} {Type:dns Value:test.example.com}]` and the certificate was issued without wildcard.\n\n[RFC 8555 Section 7.1.3](https://www.rfc-editor.org/rfc/rfc8555#section-7.1.3) states:\n> The elements of the \"authorizations\" and \"identifiers\" arrays are\n   immutable once set.  The server MUST NOT change the contents of\n   either array after they are created.  **If a client observes a change\n   in the contents of either array, then it SHOULD consider the order\n   invalid.**\n\nMaybe it would be appropriate to add a check in acme/api/order.go for this in order to detect a misbehaving ACME Server and propagate an error to the (potentially) calling application? E. g. like this:\n```go\n// Check if the Identifiers changed, which they must not.\n// The elements of the \"authorizations\" and \"identifiers\" arrays are immutable once set. The server MUST NOT\n// change the contents of either array after they are created. If a client observes a change in the contents of\n// either array, then it SHOULD consider the order invalid.\n// https://www.rfc-editor.org/rfc/rfc8555#section-7.1.3\n// Identifier order might be different, thus we need a helper function.\nif !IdentifiersMatch(order.Identifiers, orderReq.Identifiers) {\n\treturn acme.ExtendedOrder{},\n\t\tfmt.Errorf(\"order: Order Identifiers do not match: %+v != %+v (ACME Server != ACME Client). \"+\n\t\t\"The order is considered invalid as per RFC 8555 section 7.1.3\",\n\t\t\torder.Identifiers, identifiers)\n}\n```\nhttps://github.com/kiliant/lego/blob/7fdc90b92b2f674503e4ebfbb559d985351b0fa4/acme/api/order.go#L88-L99\n\nExample Lego output with identifier validation:\n```bash\n./lego.sh\n2025/04/28 10:44:44 [INFO] [test.example.com, *.test.example.com] acme: Obtaining bundled SAN certificate\n2025/04/28 10:44:44 Could not obtain certificates: order: Order Identifiers do not match: [{Type:dns Value:test.example.com} {Type:dns Value:test.example.com}] != [{Type:dns Value:test.example.com} {Type:dns Value:*.test.example.com}] (ACME Server != ACME Client). The order is considered invalid as per RFC 8555 section 7.1.3\n```\n\nBest regards and thanks again\nThomas\n", "patch": "diff --git a/acme/api/identifier.go b/acme/api/identifier.go\nnew file mode 100644\nindex 0000000000..27337ccba8\n--- /dev/null\n+++ b/acme/api/identifier.go\n@@ -0,0 +1,27 @@\n+package api\n+\n+import (\n+\t\"cmp\"\n+\t\"slices\"\n+\n+\t\"github.com/go-acme/lego/v4/acme\"\n+)\n+\n+// compareIdentifiers compares 2 slices of [acme.Identifier].\n+func compareIdentifiers(a, b []acme.Identifier) int {\n+\t// Clones slices to avoid modifying original slices.\n+\tright := slices.Clone(a)\n+\tleft := slices.Clone(b)\n+\n+\tslices.SortStableFunc(right, compareIdentifier)\n+\tslices.SortStableFunc(left, compareIdentifier)\n+\n+\treturn slices.CompareFunc(right, left, compareIdentifier)\n+}\n+\n+func compareIdentifier(right, left acme.Identifier) int {\n+\treturn cmp.Or(\n+\t\tcmp.Compare(right.Type, left.Type),\n+\t\tcmp.Compare(right.Value, left.Value),\n+\t)\n+}\ndiff --git a/acme/api/order.go b/acme/api/order.go\nindex 4d310e0409..dd42fb4451 100644\n--- a/acme/api/order.go\n+++ b/acme/api/order.go\n@@ -5,6 +5,7 @@ import (\n \t\"errors\"\n \t\"fmt\"\n \t\"net\"\n+\t\"slices\"\n \t\"time\"\n \n \t\"github.com/go-acme/lego/v4/acme\"\n@@ -85,6 +86,21 @@ func (o *OrderService) NewWithOptions(domains []string, opts *OrderOptions) (acm\n \t\t}\n \t}\n \n+\t// The elements of the \"authorizations\" and \"identifiers\" arrays are immutable once set.\n+\t// The server MUST NOT change the contents of either array after they are created.\n+\t// If a client observes a change in the contents of either array,\n+\t// then it SHOULD consider the order invalid.\n+\t// https://www.rfc-editor.org/rfc/rfc8555#section-7.1.3\n+\tif compareIdentifiers(orderReq.Identifiers, order.Identifiers) != 0 {\n+\t\t// Sorts identifiers to avoid error message ambiguities about the order of the identifiers.\n+\t\tslices.SortStableFunc(orderReq.Identifiers, compareIdentifier)\n+\t\tslices.SortStableFunc(order.Identifiers, compareIdentifier)\n+\n+\t\treturn acme.ExtendedOrder{},\n+\t\t\tfmt.Errorf(\"order identifiers have been by the ACME server (RFC8555 \u00a77.1.3): %+v != %+v\",\n+\t\t\t\torderReq.Identifiers, order.Identifiers)\n+\t}\n+\n \treturn acme.ExtendedOrder{\n \t\tOrder:    order,\n \t\tLocation: resp.Header.Get(\"Location\"),\n", "instance_id": "go-acme__lego-2520", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue encountered with the ACME server truncating wildcard identifiers in the order response, which violates RFC 8555 Section 7.1.3. The goal of adding a check to detect such misbehavior by the ACME server is evident, and the context of usage (via Traefik) and the specific issue (wildcard certificate not being issued) are well-explained. Additionally, the proposer includes a code snippet for the suggested fix and references the relevant RFC section, which adds to the clarity. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases (e.g., partial mismatches or server responses with additional identifiers) or how the error should be handled beyond logging. Constraints or expected behavior in such scenarios are not specified, which could lead to implementation uncertainties. Overall, it is mostly clear but lacks some minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, primarily affecting the `acme/api/order.go` file with the addition of a validation check for identifier mismatches. A new utility file `identifier.go` is introduced to handle identifier comparison and sorting, but the changes are straightforward and do not impact the broader system architecture. The modifications involve adding a small amount of code (around 40 lines total) across two files, with no significant interaction with other modules beyond the existing order creation logic.\n\n2. **Number of Technical Concepts:** The solution requires understanding of basic Go language features such as slices, sorting (using `slices.SortStableFunc`), and comparison utilities (using `cmp` package). It also involves familiarity with the ACME protocol (specifically RFC 8555 Section 7.1.3) to understand the immutability of identifiers, but this is a relatively narrow and well-documented domain concept. No advanced algorithms, design patterns, or complex libraries are needed, keeping the conceptual load light.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes implicitly handle the primary issue of identifier mismatch by comparing and sorting slices to ensure order-independent matching. However, additional edge cases (e.g., server returning extra identifiers, malformed responses, or partial matches) are not addressed in the proposed solution or problem statement, and the error handling is basic (returning an error with a formatted message). The complexity of edge cases is low since the core issue is straightforward, though a more robust solution might consider these scenarios.\n\n4. **Overall Complexity:** The task requires understanding a specific part of the ACME client logic and making a simple modification to add a validation step. It does not require deep knowledge of the entire codebase or complex refactoring. The implementation is mostly mechanical\u2014adding a comparison function and invoking it during order creation\u2014making it a relatively easy task for someone familiar with Go and basic protocol specifications.\n\nGiven these factors, a difficulty score of 0.35 is appropriate, placing it in the Easy category. It requires slightly more than trivial changes due to the need to understand the ACME protocol context and implement a stable comparison, but it remains a straightforward bug fix with minimal impact on the broader system.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Atlantis does not take into account GitHub checks in the `Expected - Waiting for status to be reported`-state when determining if a pull request is mergeable\n<!--- Please keep this note for the community --->\r\n\r\n### Community Note\r\n\r\n* Please vote on this issue by adding a \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request. Searching for pre-existing feature requests helps us consolidate datapoints for identical requirements into a single place, thank you!\r\n* Please do not leave \"+1\" or other comments that do not add relevant new information or questions, they generate extra noise for issue followers and do not help prioritize the request.\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment.\r\n\r\n<!--- Thank you for keeping this note for the community --->\r\n\r\n---\r\n\r\n<!---\r\nWhen filing a bug, please include the following headings if possible.\r\nAny example text in this template can be deleted.\r\n--->\r\n\r\n### Overview of the Issue\r\n\r\n<!---\r\nPlease describe the issue you are having and how you encountered the problem.\r\n--->\r\n\r\nAtlantis does not take into account GitHub checks in the `Expected - Waiting for status to be reported`-state when determining if a pull request is mergeable. Resolving this issue may however be infeasible given the current state of GitHub's API, see https://github.com/cli/cli/issues/6448. This is really an issue with GitHub's API more than with Atlantis, but I'm putting this issue up anyway for the sake of visibility. It may be possible to list all required checks through the rulesets and branch protection endpoints of the API and check that all of them appear in the status checks rollup of the pull request, but this could get complicated and is essentially reimplementing GitHub's internal logic.\r\n\r\n\r\n### Reproduction Steps\r\n\r\n<!--- \r\nIn order to effectively and quickly resolve the issue, please provide exact steps that allow us the reproduce the problem. If no steps are provided, then it will likely take longer to get the issue resolved.\r\n--->\r\n\r\nCreating a branch protection rule or ruleset that requires a check that does not exist will result in a required check forever stuck in the `Expected - Waiting for status to be reported`-state. The pull request will not be mergeable in this condition. The GitHub GUI will show this, being rendered internally in GitHub, but the GitHub API will not expose it. Consequently Atlantis will not know of the required check and will assume the pull request is mergeable.\r\n\r\nA similar situation can occur if a new required check is added through branch protection or ruleset after a commit was pushed to the pull request. The creation of the check does not trigger its execution, so the check will be stuck in the `Expected - Waiting for status to be reported`-state. Again, this will be shown in the GitHub GUI, but will not be exposed in the GitHub API, the pull request is not mergeable but Atlantis will not know. But in this case, pushing a new commit on the pull request branch or closing and re-opening the pull request will trigger the check (and it will go into one of the states visible in the API, e.g. `PENDING`), after which it will in fact be visible to Atlantis as expected.\r\n\r\n\r\n### Logs\r\n\r\n<!---\r\nProvide log files from Atlantis server\r\n\r\nlogs can be retrieved from the deployment or from atlantis comments by adding `--debug` such as `atlantis plan --debug`\r\n\r\n<details>\r\n  <summary>Logs</summary>\r\n\r\n```\r\nlog output\r\n```\r\n\r\n</details>\r\n--->\r\n\r\n\r\n### Environment details\r\n\r\n<!---\r\nIf not already included, please provide the following:\r\n\r\n- Atlantis version:\r\n- Deployment method: ecs/eks/helm/tf module\r\n- If not running the latest Atlantis version have you tried to reproduce this issue on the latest version: \r\n- Atlantis flags:\r\n\r\nAtlantis server-side config file:\r\n\r\n```yaml\r\n# config file\r\n```\r\n\r\nRepo `atlantis.yaml` file:\r\n\r\n```yaml\r\n# config file\r\n```\r\n\r\nAny other information you can provide about the environment/deployment (efs/nfs, aws/gcp, k8s/fargate, etc)\r\n--->\r\n\r\n\r\n### Additional Context\r\n\r\n<!---\r\nAdditional context on the problem. Docs, links to blogs, or other material that lead you to discover this issue or were helpful in troubleshooting the issue. \r\n\r\nUse a bulleted list to link to tickets\r\n--->\r\n\r\n\n", "patch": "diff --git a/server/events/vcs/github_client.go b/server/events/vcs/github_client.go\nindex a561833e6f..040a85958b 100644\n--- a/server/events/vcs/github_client.go\n+++ b/server/events/vcs/github_client.go\n@@ -18,6 +18,7 @@ import (\n \t\"encoding/base64\"\n \t\"fmt\"\n \t\"net/http\"\n+\t\"strconv\"\n \t\"strings\"\n \t\"time\"\n \n@@ -39,6 +40,40 @@ var (\n \tpullRequestDismissalMessage = *githubv4.NewString(\"Dismissing reviews because of plan changes\")\n )\n \n+type GithubRepoIdCacheEntry struct {\n+\tRepoId     githubv4.Int\n+\tLookupTime time.Time\n+}\n+\n+type GitHubRepoIdCache struct {\n+\tcache map[githubv4.String]GithubRepoIdCacheEntry\n+}\n+\n+func NewGitHubRepoIdCache() GitHubRepoIdCache {\n+\treturn GitHubRepoIdCache{\n+\t\tcache: make(map[githubv4.String]GithubRepoIdCacheEntry),\n+\t}\n+}\n+\n+func (c *GitHubRepoIdCache) Get(key githubv4.String) (githubv4.Int, bool) {\n+\tentry, ok := c.cache[key]\n+\tif !ok {\n+\t\treturn githubv4.Int(0), false\n+\t}\n+\tif time.Since(entry.LookupTime) > time.Hour {\n+\t\tdelete(c.cache, key)\n+\t\treturn githubv4.Int(0), false\n+\t}\n+\treturn entry.RepoId, true\n+}\n+\n+func (c *GitHubRepoIdCache) Set(key githubv4.String, value githubv4.Int) {\n+\tc.cache[key] = GithubRepoIdCacheEntry{\n+\t\tRepoId:     value,\n+\t\tLookupTime: time.Now(),\n+\t}\n+}\n+\n // GithubClient is used to perform GitHub actions.\n type GithubClient struct {\n \tuser                  string\n@@ -47,6 +82,7 @@ type GithubClient struct {\n \tctx                   context.Context\n \tconfig                GithubConfig\n \tmaxCommentsPerCommand int\n+\trepoIdCache           GitHubRepoIdCache\n }\n \n // GithubAppTemporarySecrets holds app credentials obtained from github after creation.\n@@ -109,6 +145,7 @@ func NewGithubClient(hostname string, credentials GithubCredentials, config Gith\n \tif err != nil {\n \t\treturn nil, errors.Wrap(err, \"getting user\")\n \t}\n+\n \treturn &GithubClient{\n \t\tuser:                  user,\n \t\tclient:                client,\n@@ -116,6 +153,7 @@ func NewGithubClient(hostname string, credentials GithubCredentials, config Gith\n \t\tctx:                   context.Background(),\n \t\tconfig:                config,\n \t\tmaxCommentsPerCommand: maxCommentsPerCommand,\n+\t\trepoIdCache:           NewGitHubRepoIdCache(),\n \t}, nil\n }\n \n@@ -410,119 +448,348 @@ func (g *GithubClient) DiscardReviews(repo models.Repo, pull models.PullRequest)\n \treturn nil\n }\n \n-// isRequiredCheck is a helper function to determine if a check is required or not\n-func isRequiredCheck(check string, required []string) bool {\n-\t//in go1.18 can prob replace this with slices.Contains\n-\tfor _, r := range required {\n-\t\tif r == check {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+type PageInfo struct {\n+\tEndCursor   *githubv4.String\n+\tHasNextPage githubv4.Boolean\n+}\n \n-\treturn false\n+type WorkflowFileReference struct {\n+\tPath         githubv4.String\n+\tRepositoryId githubv4.Int\n+\tSha          *githubv4.String\n }\n \n-// GetCombinedStatusMinusApply checks Statuses for PR, excluding atlantis apply. Returns true if all other statuses are not in failure.\n-func (g *GithubClient) GetCombinedStatusMinusApply(logger logging.SimpleLogging, repo models.Repo, pull *github.PullRequest, vcstatusname string) (bool, error) {\n-\tlogger.Debug(\"Checking if GitHub pull request %d has successful status checks\", pull.GetNumber())\n-\t//check combined status api\n-\tstatus, resp, err := g.client.Repositories.GetCombinedStatus(g.ctx, *pull.Head.Repo.Owner.Login, repo.Name, *pull.Head.Ref, nil)\n-\tif resp != nil {\n-\t\tlogger.Debug(\"GET /repos/%v/%v/commits/%s/status returned: %v\", *pull.Head.Repo.Owner.Login, repo.Name, *pull.Head.Ref, resp.StatusCode)\n+func (original WorkflowFileReference) Copy() WorkflowFileReference {\n+\tcopy := WorkflowFileReference{\n+\t\tPath:         original.Path,\n+\t\tRepositoryId: original.RepositoryId,\n+\t\tSha:          new(githubv4.String),\n \t}\n-\tif err != nil {\n-\t\treturn false, errors.Wrap(err, \"getting combined status\")\n+\tif original.Sha != nil {\n+\t\t*copy.Sha = *original.Sha\n \t}\n+\treturn copy\n+}\n \n-\t//iterate over statuses - return false if we find one that isn't \"apply\" and doesn't have state = \"success\"\n-\tfor _, r := range status.Statuses {\n-\t\tif strings.HasPrefix(*r.Context, fmt.Sprintf(\"%s/%s\", vcstatusname, command.Apply.String())) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif *r.State != \"success\" {\n-\t\t\treturn false, nil\n-\t\t}\n+type WorkflowRun struct {\n+\tFile struct {\n+\t\tPath              githubv4.String\n+\t\tRepositoryFileUrl githubv4.String\n+\t\tRepositoryName    githubv4.String\n \t}\n+}\n \n-\t//get required status checks\n-\trequired, resp, err := g.client.Repositories.GetBranchProtection(context.Background(), repo.Owner, repo.Name, *pull.Base.Ref)\n-\tif resp != nil {\n-\t\tlogger.Debug(\"GET /repos/%v/%v/branches/%s/protection returned: %v\", repo.Owner, repo.Name, *pull.Base.Ref, resp.StatusCode)\n+type CheckRun struct {\n+\tName       githubv4.String\n+\tConclusion githubv4.String\n+\t// Not currently used: GitHub API classifies as required if coming from ruleset, even when the ruleset is not enforced!\n+\tIsRequired githubv4.Boolean `graphql:\"isRequired(pullRequestNumber: $number)\"`\n+\tCheckSuite struct {\n+\t\tWorkflowRun *WorkflowRun\n \t}\n-\tif err != nil {\n-\t\treturn false, errors.Wrap(err, \"getting required status checks\")\n+}\n+\n+func (original CheckRun) Copy() CheckRun {\n+\tcopy := CheckRun{\n+\t\tName:       original.Name,\n+\t\tConclusion: original.Conclusion,\n+\t\tIsRequired: original.IsRequired,\n+\t\tCheckSuite: original.CheckSuite,\n \t}\n+\tif original.CheckSuite.WorkflowRun != nil {\n+\t\tcopy.CheckSuite.WorkflowRun = new(WorkflowRun)\n+\t\t*copy.CheckSuite.WorkflowRun = *original.CheckSuite.WorkflowRun\n+\t}\n+\treturn copy\n+}\n \n-\tif required.RequiredStatusChecks == nil {\n-\t\treturn true, nil\n+type StatusContext struct {\n+\tContext githubv4.String\n+\tState   githubv4.String\n+\t// Not currently used: GitHub API classifies as required if coming from ruleset, even when the ruleset is not enforced!\n+\tIsRequired githubv4.Boolean `graphql:\"isRequired(pullRequestNumber: $number)\"`\n+}\n+\n+func (g *GithubClient) LookupRepoId(repo githubv4.String) (githubv4.Int, error) {\n+\t// This function may get many calls for the same repo, and repo names are not often changed\n+\t// Utilize caching to reduce the number of API calls to GitHub\n+\tif repoId, ok := g.repoIdCache.Get(repo); ok {\n+\t\treturn repoId, nil\n \t}\n \n-\t//check check suite/check run api\n-\tchecksuites, resp, err := g.client.Checks.ListCheckSuitesForRef(context.Background(), *pull.Head.Repo.Owner.Login, repo.Name, *pull.Head.Ref, nil)\n-\tif resp != nil {\n-\t\tlogger.Debug(\"GET /repos/%v/%v/commits/%s/check-suites returned: %v\", *pull.Head.Repo.Owner.Login, repo.Name, *pull.Head.Ref, resp.StatusCode)\n+\trepoSplit := strings.Split(string(repo), \"/\")\n+\tif len(repoSplit) != 2 {\n+\t\treturn githubv4.Int(0), fmt.Errorf(\"invalid repository name: %s\", repo)\n \t}\n+\n+\tvar query struct {\n+\t\tRepository struct {\n+\t\t\tDatabaseId githubv4.Int\n+\t\t} `graphql:\"repository(owner: $owner, name: $name)\"`\n+\t}\n+\tvariables := map[string]interface{}{\n+\t\t\"owner\": githubv4.String(repoSplit[0]),\n+\t\t\"name\":  githubv4.String(repoSplit[1]),\n+\t}\n+\n+\terr := g.v4Client.Query(g.ctx, &query, variables)\n+\n \tif err != nil {\n-\t\treturn false, errors.Wrap(err, \"getting check suites for ref\")\n+\t\treturn githubv4.Int(0), errors.Wrap(err, \"getting repository id from GraphQL\")\n \t}\n \n-\t//iterate over check completed check suites - return false if we find one that doesnt have conclusion = \"success\"\n-\tfor _, c := range checksuites.CheckSuites {\n-\t\tif *c.Status == \"completed\" {\n-\t\t\t//iterate over the runs inside the suite\n-\t\t\tsuite, resp, err := g.client.Checks.ListCheckRunsCheckSuite(context.Background(), *pull.Head.Repo.Owner.Login, repo.Name, *c.ID, nil)\n-\t\t\tif resp != nil {\n-\t\t\t\tlogger.Debug(\"GET /repos/%v/%v/check-suites/%d/check-runs returned: %v\", *pull.Head.Repo.Owner.Login, repo.Name, *c.ID, resp.StatusCode)\n-\t\t\t}\n-\t\t\tif err != nil {\n-\t\t\t\treturn false, errors.Wrap(err, \"getting check runs for check suite\")\n-\t\t\t}\n+\tg.repoIdCache.Set(repo, query.Repository.DatabaseId)\n \n-\t\t\tfor _, r := range suite.CheckRuns {\n-\t\t\t\t//check to see if the check is required\n-\t\t\t\tif isRequiredCheck(*r.Name, required.RequiredStatusChecks.Contexts) {\n-\t\t\t\t\tif *c.Conclusion == \"success\" {\n-\t\t\t\t\t\tcontinue\n-\t\t\t\t\t}\n-\t\t\t\t\treturn false, nil\n-\t\t\t\t}\n-\t\t\t\t//ignore checks that arent required\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n+\treturn query.Repository.DatabaseId, nil\n+}\n+\n+func (g *GithubClient) WorkflowRunMatchesWorkflowFileReference(workflowRun WorkflowRun, workflowFileReference WorkflowFileReference) (bool, error) {\n+\t// Unfortunately, the GitHub API doesn't expose the repositoryId for the WorkflowRunFile from the statusCheckRollup.\n+\t// Conversely, it doesn't expose the repository name for the WorkflowFileReference from the RepositoryRuleConnection.\n+\t// Therefore, a second query is required to lookup the association between repositoryId and repositoryName.\n+\trepoId, err := g.LookupRepoId(workflowRun.File.RepositoryName)\n+\tif err != nil {\n+\t\treturn false, err\n \t}\n \n-\treturn true, nil\n+\tif !(repoId == workflowFileReference.RepositoryId && workflowRun.File.Path == workflowFileReference.Path) {\n+\t\treturn false, nil\n+\t} else if workflowFileReference.Sha != nil {\n+\t\treturn strings.Contains(string(workflowRun.File.RepositoryFileUrl), string(*workflowFileReference.Sha)), nil\n+\t} else {\n+\t\treturn true, nil\n+\t}\n }\n \n-// GetPullReviewDecision gets the pull review decision, which takes into account CODEOWNERS\n-func (g *GithubClient) GetPullReviewDecision(repo models.Repo, pull models.PullRequest) (approvalStatus bool, err error) {\n+func (g *GithubClient) GetPullRequestMergeabilityInfo(\n+\trepo models.Repo,\n+\tpull *github.PullRequest,\n+) (\n+\treviewDecision githubv4.String,\n+\trequiredChecks []githubv4.String,\n+\trequiredWorkflows []WorkflowFileReference,\n+\tcheckRuns []CheckRun,\n+\tstatusContexts []StatusContext,\n+\terr error,\n+) {\n \tvar query struct {\n \t\tRepository struct {\n \t\t\tPullRequest struct {\n-\t\t\t\tReviewDecision string\n+\t\t\t\tReviewDecision githubv4.String\n+\t\t\t\tBaseRef        struct {\n+\t\t\t\t\tBranchProtectionRule struct {\n+\t\t\t\t\t\tRequiredStatusChecks []struct {\n+\t\t\t\t\t\t\tContext githubv4.String\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t\tRules struct {\n+\t\t\t\t\t\tPageInfo PageInfo\n+\t\t\t\t\t\tNodes    []struct {\n+\t\t\t\t\t\t\tType              githubv4.String\n+\t\t\t\t\t\t\tRepositoryRuleset struct {\n+\t\t\t\t\t\t\t\tEnforcement githubv4.String\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tParameters struct {\n+\t\t\t\t\t\t\t\tRequiredStatusChecksParameters struct {\n+\t\t\t\t\t\t\t\t\tRequiredStatusChecks []struct {\n+\t\t\t\t\t\t\t\t\t\tContext githubv4.String\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t} `graphql:\"... on RequiredStatusChecksParameters\"`\n+\t\t\t\t\t\t\t\tWorkflowsParameters struct {\n+\t\t\t\t\t\t\t\t\tWorkflows []WorkflowFileReference\n+\t\t\t\t\t\t\t\t} `graphql:\"... on WorkflowsParameters\"`\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} `graphql:\"rules(first: 100, after: $ruleCursor)\"`\n+\t\t\t\t}\n+\t\t\t\tCommits struct {\n+\t\t\t\t\tNodes []struct {\n+\t\t\t\t\t\tCommit struct {\n+\t\t\t\t\t\t\tStatusCheckRollup struct {\n+\t\t\t\t\t\t\t\tContexts struct {\n+\t\t\t\t\t\t\t\t\tPageInfo PageInfo\n+\t\t\t\t\t\t\t\t\tNodes    []struct {\n+\t\t\t\t\t\t\t\t\t\tTypename      githubv4.String `graphql:\"__typename\"`\n+\t\t\t\t\t\t\t\t\t\tCheckRun      CheckRun        `graphql:\"... on CheckRun\"`\n+\t\t\t\t\t\t\t\t\t\tStatusContext StatusContext   `graphql:\"... on StatusContext\"`\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t} `graphql:\"contexts(first: 100, after: $contextCursor)\"`\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t} `graphql:\"commits(last: 1)\"`\n \t\t\t} `graphql:\"pullRequest(number: $number)\"`\n \t\t} `graphql:\"repository(owner: $owner, name: $name)\"`\n \t}\n \n \tvariables := map[string]interface{}{\n-\t\t\"owner\":  githubv4.String(repo.Owner),\n-\t\t\"name\":   githubv4.String(repo.Name),\n-\t\t\"number\": githubv4.Int(pull.Num),\n+\t\t\"owner\":         githubv4.String(repo.Owner),\n+\t\t\"name\":          githubv4.String(repo.Name),\n+\t\t\"number\":        githubv4.Int(*pull.Number),\n+\t\t\"ruleCursor\":    (*githubv4.String)(nil),\n+\t\t\"contextCursor\": (*githubv4.String)(nil),\n+\t}\n+\n+\trequiredChecksSet := make(map[githubv4.String]any)\n+\n+pagination:\n+\tfor {\n+\t\terr = g.v4Client.Query(g.ctx, &query, variables)\n+\n+\t\tif err != nil {\n+\t\t\tbreak pagination\n+\t\t}\n+\n+\t\treviewDecision = query.Repository.PullRequest.ReviewDecision\n+\n+\t\tfor _, rule := range query.Repository.PullRequest.BaseRef.BranchProtectionRule.RequiredStatusChecks {\n+\t\t\trequiredChecksSet[rule.Context] = struct{}{}\n+\t\t}\n+\n+\t\tfor _, rule := range query.Repository.PullRequest.BaseRef.Rules.Nodes {\n+\t\t\tif rule.RepositoryRuleset.Enforcement != \"ACTIVE\" {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tswitch rule.Type {\n+\t\t\tcase \"REQUIRED_STATUS_CHECKS\":\n+\t\t\t\tfor _, context := range rule.Parameters.RequiredStatusChecksParameters.RequiredStatusChecks {\n+\t\t\t\t\trequiredChecksSet[context.Context] = struct{}{}\n+\t\t\t\t}\n+\t\t\tcase \"WORKFLOWS\":\n+\t\t\t\tfor _, workflow := range rule.Parameters.WorkflowsParameters.Workflows {\n+\t\t\t\t\trequiredWorkflows = append(requiredWorkflows, workflow.Copy())\n+\t\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t}\n+\n+\t\tif len(query.Repository.PullRequest.Commits.Nodes) == 0 {\n+\t\t\terr = errors.New(\"no commits found on PR\")\n+\t\t\tbreak pagination\n+\t\t}\n+\n+\t\tfor _, context := range query.Repository.PullRequest.Commits.Nodes[0].Commit.StatusCheckRollup.Contexts.Nodes {\n+\t\t\tswitch context.Typename {\n+\t\t\tcase \"CheckRun\":\n+\t\t\t\tcheckRuns = append(checkRuns, context.CheckRun.Copy())\n+\t\t\tcase \"StatusContext\":\n+\t\t\t\tstatusContexts = append(statusContexts, context.StatusContext)\n+\t\t\tdefault:\n+\t\t\t\terr = fmt.Errorf(\"unknown type of status check, %q\", context.Typename)\n+\t\t\t\tbreak pagination\n+\t\t\t}\n+\t\t}\n+\n+\t\tif !query.Repository.PullRequest.BaseRef.Rules.PageInfo.HasNextPage &&\n+\t\t\t!query.Repository.PullRequest.Commits.Nodes[0].Commit.StatusCheckRollup.Contexts.PageInfo.HasNextPage {\n+\t\t\tbreak pagination\n+\t\t}\n+\n+\t\tif query.Repository.PullRequest.BaseRef.Rules.PageInfo.EndCursor != nil {\n+\t\t\tvariables[\"ruleCursor\"] = query.Repository.PullRequest.BaseRef.Rules.PageInfo.EndCursor\n+\t\t}\n+\t\tif query.Repository.PullRequest.Commits.Nodes[0].Commit.StatusCheckRollup.Contexts.PageInfo.EndCursor != nil {\n+\t\t\tvariables[\"contextCursor\"] = query.Repository.PullRequest.Commits.Nodes[0].Commit.StatusCheckRollup.Contexts.PageInfo.EndCursor\n+\t\t}\n \t}\n \n-\terr = g.v4Client.Query(g.ctx, &query, variables)\n \tif err != nil {\n-\t\treturn approvalStatus, errors.Wrap(err, \"getting reviewDecision\")\n+\t\treturn \"\", nil, nil, nil, nil, errors.Wrap(err, \"fetching rulesets, branch protections and status checks from GraphQL\")\n \t}\n \n-\tif query.Repository.PullRequest.ReviewDecision == \"APPROVED\" || len(query.Repository.PullRequest.ReviewDecision) == 0 {\n-\t\treturn true, nil\n+\tfor context := range requiredChecksSet {\n+\t\trequiredChecks = append(requiredChecks, context)\n+\t}\n+\n+\treturn reviewDecision, requiredChecks, requiredWorkflows, checkRuns, statusContexts, nil\n+}\n+\n+func CheckRunPassed(checkRun CheckRun) bool {\n+\treturn checkRun.Conclusion == \"SUCCESS\" || checkRun.Conclusion == \"SKIPPED\" || checkRun.Conclusion == \"NEUTRAL\"\n+}\n+\n+func StatusContextPassed(statusContext StatusContext, vcsstatusname string) bool {\n+\treturn strings.HasPrefix(string(statusContext.Context), fmt.Sprintf(\"%s/%s\", vcsstatusname, command.Apply.String())) ||\n+\t\tstatusContext.State == \"SUCCESS\"\n+}\n+\n+func ExpectedCheckPassed(expectedContext githubv4.String, checkRuns []CheckRun, statusContexts []StatusContext, vcsstatusname string) bool {\n+\tfor _, checkRun := range checkRuns {\n+\t\tif checkRun.Name == expectedContext {\n+\t\t\treturn CheckRunPassed(checkRun)\n+\t\t}\n+\t}\n+\n+\tfor _, statusContext := range statusContexts {\n+\t\tif statusContext.Context == expectedContext {\n+\t\t\treturn StatusContextPassed(statusContext, vcsstatusname)\n+\t\t}\n+\t}\n+\n+\treturn false\n+}\n+\n+func (g *GithubClient) ExpectedWorkflowPassed(expectedWorkflow WorkflowFileReference, checkRuns []CheckRun) (bool, error) {\n+\tfor _, checkRun := range checkRuns {\n+\t\tif checkRun.CheckSuite.WorkflowRun == nil {\n+\t\t\tcontinue\n+\t\t}\n+\t\tmatch, err := g.WorkflowRunMatchesWorkflowFileReference(*checkRun.CheckSuite.WorkflowRun, expectedWorkflow)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif match {\n+\t\t\treturn CheckRunPassed(checkRun), nil\n+\t\t}\n \t}\n \n \treturn false, nil\n }\n \n+// IsMergeableMinusApply checks review decision (which takes into account CODEOWNERS) and required checks for PR (excluding the atlantis apply check).\n+func (g *GithubClient) IsMergeableMinusApply(logger logging.SimpleLogging, repo models.Repo, pull *github.PullRequest, vcsstatusname string) (bool, error) {\n+\tif pull.Number == nil {\n+\t\treturn false, errors.New(\"pull request number is nil\")\n+\t}\n+\treviewDecision, requiredChecks, requiredWorkflows, checkRuns, statusContexts, err := g.GetPullRequestMergeabilityInfo(repo, pull)\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\n+\tnotMergeablePrefix := fmt.Sprintf(\"Pull Request %s/%s:%s is not mergeable\", repo.Owner, repo.Name, strconv.Itoa(*pull.Number))\n+\n+\t// Review decision takes CODEOWNERS into account\n+\t// Empty review decision means review is not required\n+\tif reviewDecision != \"APPROVED\" && len(reviewDecision) != 0 {\n+\t\tlogger.Debug(\"%s: Review Decision: %s\", notMergeablePrefix, reviewDecision)\n+\t\treturn false, nil\n+\t}\n+\n+\t// The statusCheckRollup does not always contain all required checks\n+\t// For example, if a check was made required after the pull request was opened, it would be missing\n+\t// Go through all checks and workflows required by branch protection or rulesets\n+\t// Make sure that they can all be found in the statusCheckRollup and that they all pass\n+\tfor _, requiredCheck := range requiredChecks {\n+\t\tif !ExpectedCheckPassed(requiredCheck, checkRuns, statusContexts, vcsstatusname) {\n+\t\t\tlogger.Debug(\"%s: Expected Required Check: %s\", notMergeablePrefix, requiredCheck)\n+\t\t\treturn false, nil\n+\t\t}\n+\t}\n+\tfor _, requiredWorkflow := range requiredWorkflows {\n+\t\tpassed, err := g.ExpectedWorkflowPassed(requiredWorkflow, checkRuns)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !passed {\n+\t\t\tlogger.Debug(\"%s: Expected Required Workflow: RepositoryId: %d Path: %s\", notMergeablePrefix, requiredWorkflow.RepositoryId, requiredWorkflow.Path)\n+\t\t\treturn false, nil\n+\t\t}\n+\t}\n+\n+\treturn true, nil\n+}\n+\n // PullIsMergeable returns true if the pull request is mergeable.\n func (g *GithubClient) PullIsMergeable(logger logging.SimpleLogging, repo models.Repo, pull models.PullRequest, vcsstatusname string) (bool, error) {\n \tlogger.Debug(\"Checking if GitHub pull request %d is mergeable\", pull.Num)\n@@ -531,7 +798,6 @@ func (g *GithubClient) PullIsMergeable(logger logging.SimpleLogging, repo models\n \t\treturn false, errors.Wrap(err, \"getting pull request\")\n \t}\n \n-\tstate := githubPR.GetMergeableState()\n \t// We map our mergeable check to when the GitHub merge button is clickable.\n \t// This corresponds to the following states:\n \t// clean: No conflicts, all requirements satisfied.\n@@ -541,33 +807,22 @@ func (g *GithubClient) PullIsMergeable(logger logging.SimpleLogging, repo models\n \t// has_hooks: GitHub Enterprise only, if a repo has custom pre-receive\n \t//            hooks. Merging is allowed (green box).\n \t// See: https://github.com/octokit/octokit.net/issues/1763\n-\tif state != \"clean\" && state != \"unstable\" && state != \"has_hooks\" {\n-\t\t//mergeable bypass apply code hidden by feature flag\n+\tswitch githubPR.GetMergeableState() {\n+\tcase \"clean\", \"unstable\", \"has_hooks\":\n+\t\treturn true, nil\n+\tcase \"blocked\":\n \t\tif g.config.AllowMergeableBypassApply {\n \t\t\tlogger.Debug(\"AllowMergeableBypassApply feature flag is enabled - attempting to bypass apply from mergeable requirements\")\n-\t\t\tif state == \"blocked\" {\n-\t\t\t\t//check status excluding atlantis apply\n-\t\t\t\tstatus, err := g.GetCombinedStatusMinusApply(logger, repo, githubPR, vcsstatusname)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn false, errors.Wrap(err, \"getting pull request status\")\n-\t\t\t\t}\n-\n-\t\t\t\t//check to see if pr is approved using reviewDecision\n-\t\t\t\tapproved, err := g.GetPullReviewDecision(repo, pull)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn false, errors.Wrap(err, \"getting pull request reviewDecision\")\n-\t\t\t\t}\n-\n-\t\t\t\t//if all other status checks EXCEPT atlantis/apply are successful, and the PR is approved based on reviewDecision, let it proceed\n-\t\t\t\tif status && approved {\n-\t\t\t\t\treturn true, nil\n-\t\t\t\t}\n+\t\t\tisMergeableMinusApply, err := g.IsMergeableMinusApply(logger, repo, githubPR, vcsstatusname)\n+\t\t\tif err != nil {\n+\t\t\t\treturn false, errors.Wrap(err, \"getting pull request status\")\n \t\t\t}\n+\t\t\treturn isMergeableMinusApply, nil\n \t\t}\n-\n+\t\treturn false, nil\n+\tdefault:\n \t\treturn false, nil\n \t}\n-\treturn true, nil\n }\n \n // GetPullRequest returns the pull request.\n", "instance_id": "runatlantis__atlantis-4193", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: Atlantis does not account for GitHub checks in the \"Expected - Waiting for status to be reported\" state when determining if a pull request is mergeable. It provides a detailed overview of the issue, reproduction steps, and additional context about the limitations of GitHub's API. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior or output for Atlantis when encountering such checks (e.g., should it wait, fail, or ignore?). Additionally, while it mentions the complexity of reimplementing GitHub's internal logic, it lacks specific guidance on the desired solution or constraints (e.g., performance or API rate limits). Edge cases are partially addressed in the reproduction steps, but not exhaustively. Overall, the statement is valid and clear but misses some minor details that would make it comprehensive.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.75, placing it in the \"Hard\" category. This assessment is based on the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are extensive and involve a single file (`github_client.go`) but significantly modify the logic for determining pull request mergeability. The diff shows a complete overhaul of how status checks and workflows are fetched and evaluated, including pagination handling and caching mechanisms. While it does not impact multiple files, the changes are deep and affect a critical part of the system (mergeability logic), which could have downstream effects on the overall behavior of Atlantis. The amount of code added (over 300 lines) and the restructuring of existing functionality indicate a substantial effort.\n\n2. **Number of Technical Concepts**: Solving this problem requires a deep understanding of several complex concepts:\n   - **GitHub API (REST and GraphQL)**: The solution uses both APIs to fetch branch protection rules, rulesets, status checks, and workflow references, requiring familiarity with their structure, limitations, and pagination.\n   - **Caching**: A custom caching mechanism (`GitHubRepoIdCache`) is implemented to reduce API calls, which involves understanding time-based expiration and thread-safety considerations (though not explicitly handled in the code).\n   - **Data Structures and Algorithms**: The code handles complex data structures (e.g., nested structs for GraphQL queries) and implements logic to match workflows and checks, requiring careful parsing and comparison.\n   - **Domain Knowledge**: Understanding GitHub's mergeability rules, branch protection, rulesets, and the nuances of check states (e.g., \"Expected - Waiting for status to be reported\") is critical and non-trivial.\n\n3. **Potential Edge Cases and Error Handling**: The problem and code changes address several edge cases, such as missing commits, inactive rulesets, and checks added after a PR is opened. The solution implements detailed logic to handle pagination for rules and status checks, and it accounts for various check conclusions (e.g., \"SUCCESS\", \"SKIPPED\"). However, error handling is present but not exhaustive\u2014there\u2019s no explicit handling for API rate limits or network failures beyond wrapping errors. The complexity of edge cases (e.g., matching workflows across repositories) adds to the difficulty.\n\n4. **Overall Complexity**: The problem requires a deep understanding of the Atlantis codebase (specifically how it interacts with VCS systems like GitHub) and GitHub's API quirks. The solution involves reimplementing logic that GitHub itself does not expose, as noted in the problem statement, which is inherently challenging. While it does not reach the \"Very Hard\" category (e.g., no distributed systems or performance-critical optimizations), it demands significant expertise in API integration, domain-specific logic, and careful handling of external dependencies.\n\nIn summary, this problem is hard due to the depth of changes, the breadth of technical concepts involved, and the need to navigate complex external APIs and domain rules. It requires a senior-level understanding of both the codebase and GitHub's ecosystem to implement correctly.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Feature request: Config option to limit maximum size of database file\nHello,\n\nIn some cases, bbolt is used by applications like opentelemetry-collector as a kind of persistent queue or cache system. It would be nice if it were possible to configure the maximum size of a bbolt database and return an error when the database needs to grow beyond this size. \n\nThe way I see this working is adding a max size field to the [Options object](https://github.com/etcd-io/bbolt/blob/main/db.go#L1273), and then checking this maximum size against the requested size `sz` in the [grow()](https://github.com/etcd-io/bbolt/blob/main/db.go#L1199) function. If the size requested is larger than the maximum size, return an error similar to what happens when the Truncate call fails. However, it would be good to export this error so that it can be checked by callers to Update/Batch.\n\nI'm happy to do the work but wanted to find out if this is desirable before submitting a PR\n", "patch": "diff --git a/bolt_windows.go b/bolt_windows.go\nindex ec21ecb85..bba0f8809 100644\n--- a/bolt_windows.go\n+++ b/bolt_windows.go\n@@ -67,6 +67,19 @@ func mmap(db *DB, sz int) error {\n \tvar sizelo, sizehi uint32\n \n \tif !db.readOnly {\n+\t\tif db.MaxSize > 0 && sz > db.MaxSize {\n+\t\t\t// The max size only limits future writes; however, we don\u2019t block opening\n+\t\t\t// and mapping the database if it already exceeds the limit.\n+\t\t\tfileSize, err := db.fileSize()\n+\t\t\tif err != nil {\n+\t\t\t\treturn fmt.Errorf(\"could not check existing db file size: %s\", err)\n+\t\t\t}\n+\n+\t\t\tif sz > fileSize {\n+\t\t\t\treturn errors.ErrMaxSizeReached\n+\t\t\t}\n+\t\t}\n+\n \t\t// Truncate the database to the size of the mmap.\n \t\tif err := db.file.Truncate(int64(sz)); err != nil {\n \t\t\treturn fmt.Errorf(\"truncate: %s\", err)\ndiff --git a/db.go b/db.go\nindex 4171983bc..9e379ac34 100644\n--- a/db.go\n+++ b/db.go\n@@ -110,6 +110,12 @@ type DB struct {\n \t// of truncate() and fsync() when growing the data file.\n \tAllocSize int\n \n+\t// MaxSize is the maximum size (in bytes) allowed for the data file.\n+\t// If a caller's attempt to add data results in the need to grow\n+\t// the data file, an error will be returned and the data file will not grow.\n+\t// <=0 means no limit.\n+\tMaxSize int\n+\n \t// Mlock locks database file in memory when set to true.\n \t// It prevents major page faults, however used memory can't be reclaimed.\n \t//\n@@ -191,6 +197,7 @@ func Open(path string, mode os.FileMode, options *Options) (db *DB, err error) {\n \tdb.PreLoadFreelist = options.PreLoadFreelist\n \tdb.FreelistType = options.FreelistType\n \tdb.Mlock = options.Mlock\n+\tdb.MaxSize = options.MaxSize\n \n \t// Set default values for later DB operations.\n \tdb.MaxBatchSize = common.DefaultMaxBatchSize\n@@ -1166,7 +1173,11 @@ func (db *DB) allocate(txid common.Txid, count int) (*common.Page, error) {\n \tvar minsz = int((p.Id()+common.Pgid(count))+1) * db.pageSize\n \tif minsz >= db.datasz {\n \t\tif err := db.mmap(minsz); err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"mmap allocate error: %s\", err)\n+\t\t\tif err == berrors.ErrMaxSizeReached {\n+\t\t\t\treturn nil, err\n+\t\t\t} else {\n+\t\t\t\treturn nil, fmt.Errorf(\"mmap allocate error: %s\", err)\n+\t\t\t}\n \t\t}\n \t}\n \n@@ -1198,6 +1209,11 @@ func (db *DB) grow(sz int) error {\n \t\tsz += db.AllocSize\n \t}\n \n+\tif !db.readOnly && db.MaxSize > 0 && sz > db.MaxSize {\n+\t\tlg.Errorf(\"[GOOS: %s, GOARCH: %s] maximum db size reached, size: %d, db.MaxSize: %d\", runtime.GOOS, runtime.GOARCH, sz, db.MaxSize)\n+\t\treturn berrors.ErrMaxSizeReached\n+\t}\n+\n \t// Truncate and fsync to ensure file size metadata is flushed.\n \t// https://github.com/boltdb/bolt/issues/284\n \tif !db.NoGrowSync && !db.readOnly {\n@@ -1320,6 +1336,9 @@ type Options struct {\n \t// PageSize overrides the default OS page size.\n \tPageSize int\n \n+\t// MaxSize sets the maximum size of the data file. <=0 means no maximum.\n+\tMaxSize int\n+\n \t// NoSync sets the initial value of DB.NoSync. Normally this can just be\n \t// set directly on the DB itself when returned from Open(), but this option\n \t// is useful in APIs which expose Options but not the underlying DB.\n@@ -1343,8 +1362,8 @@ func (o *Options) String() string {\n \t\treturn \"{}\"\n \t}\n \n-\treturn fmt.Sprintf(\"{Timeout: %s, NoGrowSync: %t, NoFreelistSync: %t, PreLoadFreelist: %t, FreelistType: %s, ReadOnly: %t, MmapFlags: %x, InitialMmapSize: %d, PageSize: %d, NoSync: %t, OpenFile: %p, Mlock: %t, Logger: %p}\",\n-\t\to.Timeout, o.NoGrowSync, o.NoFreelistSync, o.PreLoadFreelist, o.FreelistType, o.ReadOnly, o.MmapFlags, o.InitialMmapSize, o.PageSize, o.NoSync, o.OpenFile, o.Mlock, o.Logger)\n+\treturn fmt.Sprintf(\"{Timeout: %s, NoGrowSync: %t, NoFreelistSync: %t, PreLoadFreelist: %t, FreelistType: %s, ReadOnly: %t, MmapFlags: %x, InitialMmapSize: %d, PageSize: %d, MaxSize: %d, NoSync: %t, OpenFile: %p, Mlock: %t, Logger: %p}\",\n+\t\to.Timeout, o.NoGrowSync, o.NoFreelistSync, o.PreLoadFreelist, o.FreelistType, o.ReadOnly, o.MmapFlags, o.InitialMmapSize, o.PageSize, o.MaxSize, o.NoSync, o.OpenFile, o.Mlock, o.Logger)\n \n }\n \ndiff --git a/errors/errors.go b/errors/errors.go\nindex c115289e5..dbebd6330 100644\n--- a/errors/errors.go\n+++ b/errors/errors.go\n@@ -69,6 +69,9 @@ var (\n \t// ErrValueTooLarge is returned when inserting a value that is larger than MaxValueSize.\n \tErrValueTooLarge = errors.New(\"value too large\")\n \n+\t// ErrMaxSizeReached is returned when the configured maximum size of the data file is reached.\n+\tErrMaxSizeReached = errors.New(\"database reached maximum size\")\n+\n \t// ErrIncompatibleValue is returned when trying to create or delete a bucket\n \t// on an existing non-bucket key or when trying to create or delete a\n \t// non-bucket key on an existing bucket key.\n", "instance_id": "etcd-io__bbolt-929", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the feature request to add a configuration option for limiting the maximum size of a bbolt database file. It specifies the goal (limit database size), the general approach (add a max size field to Options and check it in the grow() function), and the desired behavior (return an error if the size exceeds the limit). It also mentions exporting the error for caller checks, which adds clarity to the expected interface. However, there are minor ambiguities and missing details. For instance, it does not explicitly address how the feature should behave in edge cases, such as when the database is already larger than the configured max size at initialization, or whether the limit applies only to growth or also to initial mapping. While the code changes address some of these (e.g., not blocking opening an oversized database), the problem statement itself lacks this precision. Additionally, there are no examples or test cases provided to illustrate the expected behavior. Thus, while the statement is valid and mostly clear, it misses some minor but important details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The modifications are relatively localized, affecting a few key areas in the codebase (Options struct, grow() function, mmap-related logic, and error definitions). The changes span multiple files (db.go, bolt_windows.go, errors.go), but they are straightforward additions or modifications without requiring deep architectural changes or extensive refactoring. The amount of code change is moderate, primarily involving adding a new field, updating string representations, and inserting size checks with error returns.\n\n2. **Technical Concepts Required:** Solving this requires a basic understanding of Go structs, error handling, and file operations (e.g., mmap, truncate). It also involves familiarity with the bbolt database internals, such as how database growth is managed via the grow() function. However, these concepts are not particularly complex for someone with moderate experience in Go or systems programming. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic database file management are needed.\n\n3. **Edge Cases and Error Handling:** The problem introduces a new error condition (ErrMaxSizeReached) and requires handling the case where the database size exceeds the configured limit. The code changes also consider an edge case where an existing database is already larger than the max size (allowing it to open but not grow further). While these aspects add some complexity, they are not overly intricate and are addressed in a straightforward manner in the provided diff. Additional edge cases, such as behavior under concurrent writes or specific OS limitations, are not explicitly mentioned or handled, keeping the difficulty moderate.\n\n4. **Overall Complexity:** The task requires understanding some code logic (e.g., how database growth is triggered and managed) and making simple modifications to enforce a size limit. It does not impact the broader system architecture or require deep knowledge of the entire bbolt codebase. The primary challenge lies in ensuring the size check is applied consistently across relevant code paths (e.g., grow() and mmap()), which is a manageable task.\n\nGiven these considerations, I assign a difficulty score of 0.35, placing it on the higher end of the Easy range due to the need to handle a specific error condition and understand database growth mechanics, but still well below Medium difficulty as the changes are localized and conceptually simple.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add China-based STUN servers\n### Feature description\n\nLet's extend our default STUN server list with a service operating in China.\n\n### Problem or use case\n\nOur default list of STUN servers lacks services located in China. As the other servers may not be reachable through the Great Firewall, this may reduce the connectivity of our Chinese users.\n\n### Alternatives or workarounds\n\nWe could stick with our current list and hope that some of them are reachable.\n", "patch": "diff --git a/lib/config/config.go b/lib/config/config.go\nindex 44f8a3f95f4..8a1cfbe9bcc 100644\n--- a/lib/config/config.go\n+++ b/lib/config/config.go\n@@ -78,8 +78,10 @@ var (\n \t\t\"stun.counterpath.com:3478\",\n \t\t\"stun.counterpath.net:3478\",\n \t\t\"stun.ekiga.net:3478\",\n+\t\t\"stun.hitv.com:3478\",\n \t\t\"stun.ideasip.com:3478\",\n \t\t\"stun.internetcalls.com:3478\",\n+\t\t\"stun.miwifi.com:3478\",\n \t\t\"stun.schlund.de:3478\",\n \t\t\"stun.sipgate.net:10000\",\n \t\t\"stun.sipgate.net:3478\",\n", "instance_id": "syncthing__syncthing-9843", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in its intent to add China-based STUN servers to improve connectivity for Chinese users due to potential restrictions by the Great Firewall. The feature description and use case provide a reasonable context for the change. However, there are minor ambiguities and missing details. For instance, the statement does not specify how the particular STUN servers (stun.hitv.com and stun.miwifi.com) were chosen, whether they have been tested for reliability or accessibility, or if there are any specific requirements or constraints for adding new servers (e.g., latency, ownership, or security considerations). Additionally, there are no examples or expected outcomes described, such as how the addition of these servers should impact connectivity metrics. Overall, while the goal is clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, as it involves a straightforward modification to a single configuration file by adding two new STUN server entries to an existing list. The code change is minimal, requiring no deep understanding of the codebase, architecture, or complex logic. It does not involve interactions between different modules, nor does it impact the system's architecture. There are no specific technical concepts, algorithms, or domain-specific knowledge required beyond basic familiarity with configuration files in Go. Additionally, the problem statement and code changes do not indicate any need for handling edge cases or error conditions related to these new servers (e.g., fallback mechanisms if the servers are unreachable). This task falls into the \"very easy\" category as it is essentially a simple update to a static list.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Error while Peeking first byte ; haproxy in front of Traefik with proxy-protocol\n### Do you want to request a *feature* or report a *bug*?\r\n\r\nBug\r\n\r\n### What did you do?\r\n\r\n<!--\r\n\r\nHOW TO WRITE A GOOD BUG REPORT?\r\n\r\n- Respect the issue template as much as possible.\r\n- The title should be short and descriptive.\r\n- Explain the conditions which led you to report this issue: the context.\r\n- The context should lead to something, an idea or a problem that you\u2019re facing.\r\n- Remain clear and concise.\r\n- Format your messages to help the reader focus on what matters and understand the structure of your message, use Markdown syntax https://help.github.com/articles/github-flavored-markdown\r\n\r\n-->\r\n\r\nI have haproxy in front of **dockerized** Traefik in **swarm-mode** to load-balance requests to servers in a private network, with proxy-protocol to identify source IP addresses.\r\nI perform tcp-checks for each entrypoint I use, but these result in the following errors in Traefik : `\"Error while Peeking first byte: read tcp 192.168.224.3:110->10.1.1.1:52524: read: connection reset by peer\"`\r\n\r\n### What did you expect to see?\r\n\r\nI do not expect to see an error due to haproxy tcp-checks.\r\n\r\n### What did you see instead?\r\n\r\n```bash\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Error while Peeking first byte: read tcp 192.168.224.3:110->10.1.1.1:52524: read: connection reset by peer\"\r\n```\r\n\r\n### Output of `traefik version`: (_What version of Traefik are you using?_)\r\n\r\n```\r\nVersion:      2.2.8\r\nCodename:     chevrotin\r\nGo version:   go1.14.6\r\nBuilt:        2020-07-28T15:46:03Z\r\nOS/Arch:      linux/amd64\r\n```\r\n\r\n### What is your environment & configuration (arguments, toml, provider, platform, ...)?\r\n\r\nhaproxy node is 10.1.1.1\r\n\r\nTraefik node is 10.1.1.10\r\n\r\n*Static conf*\r\n\r\n```yml\r\nentryPoints:\r\n  ## POP3\r\n  0110_pop3:\r\n    address: \":110\"\r\n    proxyProtocol:\r\n      trustedIPs:\r\n      - \"10.0.0.0/8\"\r\n    forwardedHeaders:\r\n      trustedIPs:\r\n      - \"10.0.0.0/8\"\r\n\r\nproviders:\r\n  file:\r\n    watch: true\r\n    directory: /etc/traefik/dynanmic\r\n  docker:\r\n    endpoint: \"tcp://socket:2375\"\r\n    swarmMode: true\r\n    exposedByDefault: false\r\n    network: public\r\n\r\napi:\r\n  dashboard: true\r\n  insecure: false # default: false\r\n  debug: false # default: false\r\n\r\ncertificatesResolvers:\r\n  le:\r\n    acme:\r\n      email: xxx\r\n      storage: /acme.json\r\n      dnsChallenge:\r\n        provider: xxx\r\n        delayBeforeCheck: 10\r\n\r\nlog:\r\n  level: DEBUG\r\n```\r\n\r\nPort 110 is exposed in host mode in the `docker-compose.yml` file:\r\n\r\n```yml\r\n## POP3\r\n      - target: 110\r\n        published: 110\r\n        protocol: tcp\r\n        mode: host\r\n```\r\n\r\n*haproxy conf*\r\n\r\n```haproxy\r\nglobal\r\n\tlog /dev/log\tlocal0\r\n\tlog /dev/log\tlocal1 notice\r\n\tchroot /var/lib/haproxy\r\n\tstats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\r\n\tstats timeout 30s\r\n\tuser haproxy\r\n\tgroup haproxy\r\n\tdaemon\r\n\r\n\t# See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate\r\n        # modern configuration\r\n        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\r\n        ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 no-tlsv12 no-tls-tickets\r\n\r\n        ssl-default-server-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\r\n        ssl-default-server-options no-sslv3 no-tlsv10 no-tlsv11 no-tlsv12 no-tls-tickets\r\n\r\ndefaults\r\n\tlog\tglobal\r\n\tmode\thttp\r\n\toption\thttplog\r\n\r\n        timeout connect 1s\r\n        timeout client 20s\r\n        timeout server 20s\r\n        timeout client-fin 20s\r\n        timeout tunnel 1h\r\n\r\n\terrorfile 400 /etc/haproxy/errors/400.http\r\n\terrorfile 403 /etc/haproxy/errors/403.http\r\n\terrorfile 408 /etc/haproxy/errors/408.http\r\n\terrorfile 500 /etc/haproxy/errors/500.http\r\n\terrorfile 502 /etc/haproxy/errors/502.http\r\n\terrorfile 503 /etc/haproxy/errors/503.http\r\n\terrorfile 504 /etc/haproxy/errors/504.http\r\n\r\nlisten stats\r\n        bind :32700\r\n        stats enable\r\n        stats uri /\r\n        stats hide-version\r\n        stats auth haproxy:password\r\n        stats refresh 1s\r\n\r\n## POP3\r\nfrontend 0110\r\n        bind *:110\r\n        mode tcp\r\n        option tcplog\r\n        default_backend 0110\r\n\r\nbackend 0110\r\n        balance roundrobin\r\n        mode tcp\r\n        option redispatch\r\n        option tcp-check\r\n        tcp-check connect send-proxy\r\n\r\n        server m10 10.1.1.10:110 check send-proxy-v2 fall 3 rise 2\r\n```\r\n\r\n### If applicable, please paste the log output in DEBUG level (`--log.level=DEBUG` switch)\r\n\r\n```\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Trusted connection\" entryPointName=0110_pop3\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Addresses len: 12\" entryPointName=0110_pop3\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Address readed: 12\" entryPointName=0110_pop3\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Use header remote addr\" entryPointName=0110_pop3\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Header parsed &{10.1.1.1:52524 10.1.1.10:110}\" entryPointName=0110_pop3\r\nproxy_app.0.y75ghkvn81b7@m10    | time=\"2020-08-05T13:53:06Z\" level=debug msg=\"Error while Peeking first byte: read tcp 192.168.224.3:110->10.1.1.1:52524: read: connection reset by peer\"\r\n```\r\n\n", "patch": "diff --git a/pkg/server/router/tcp/router.go b/pkg/server/router/tcp/router.go\nindex 4790cd7516..f8d6572651 100644\n--- a/pkg/server/router/tcp/router.go\n+++ b/pkg/server/router/tcp/router.go\n@@ -350,7 +350,7 @@ func clientHelloInfo(br *bufio.Reader) (*clientHello, error) {\n \tif err != nil {\n \t\tvar opErr *net.OpError\n \t\tif !errors.Is(err, io.EOF) && (!errors.As(err, &opErr) || !opErr.Timeout()) {\n-\t\t\tlog.WithoutContext().Errorf(\"Error while Peeking first byte: %s\", err)\n+\t\t\tlog.WithoutContext().Debugf(\"Error while peeking first byte: %s\", err)\n \t\t}\n \t\treturn nil, err\n \t}\n@@ -376,7 +376,7 @@ func clientHelloInfo(br *bufio.Reader) (*clientHello, error) {\n \tconst recordHeaderLen = 5\n \thdr, err = br.Peek(recordHeaderLen)\n \tif err != nil {\n-\t\tlog.Errorf(\"Error while Peeking hello: %s\", err)\n+\t\tlog.WithoutContext().Errorf(\"Error while peeking client hello headers: %s\", err)\n \t\treturn &clientHello{\n \t\t\tpeeked: getPeeked(br),\n \t\t}, nil\n@@ -390,7 +390,7 @@ func clientHelloInfo(br *bufio.Reader) (*clientHello, error) {\n \n \thelloBytes, err := br.Peek(recordHeaderLen + recLen)\n \tif err != nil {\n-\t\tlog.Errorf(\"Error while Hello: %s\", err)\n+\t\tlog.WithoutContext().Errorf(\"Error while peeking client hello bytes: %s\", err)\n \t\treturn &clientHello{\n \t\t\tisTLS:  true,\n \t\t\tpeeked: getPeeked(br),\n@@ -419,7 +419,7 @@ func clientHelloInfo(br *bufio.Reader) (*clientHello, error) {\n func getPeeked(br *bufio.Reader) string {\n \tpeeked, err := br.Peek(br.Buffered())\n \tif err != nil {\n-\t\tlog.Errorf(\"Could not get anything: %s\", err)\n+\t\tlog.WithoutContext().Errorf(\"Error while peeking bytes: %s\", err)\n \t\treturn \"\"\n \t}\n \treturn string(peeked)\n", "instance_id": "traefik__traefik-11254", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the context and the issue encountered. It provides a detailed setup of the environment (haproxy in front of Traefik in Docker Swarm mode with proxy-protocol), the specific error message observed (\"Error while Peeking first byte: read: connection reset by peer\"), and the expected behavior (no errors due to haproxy tcp-checks). The inclusion of configuration files, logs, and version information adds to the clarity. However, there are minor ambiguities: the problem statement does not explicitly define the root cause of the error (whether it's a misconfiguration, a bug in Traefik, or an interaction issue with haproxy). Additionally, it lacks specific details on how the tcp-checks from haproxy are expected to interact with Traefik's proxy-protocol handling, and whether there are specific constraints or edge cases (e.g., network conditions, specific protocols) that should be considered. These missing details prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated as Easy (0.25) based on the provided code changes and the nature of the issue. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are minimal and confined to a single file (`router.go`) within the Traefik codebase. The modifications involve changing log levels (from `Errorf` to `Debugf` in one instance) and improving log message clarity in a few places. There is no impact on the system's architecture, and the changes do not require understanding complex interactions across multiple modules. The amount of code change is very small, limited to log message adjustments.\n\n2. **Number of Technical Concepts**: The changes require only a basic understanding of Go's logging mechanisms and the context of TCP connection handling in Traefik. No advanced language features, algorithms, design patterns, or domain-specific knowledge (beyond basic networking and proxy concepts) are needed. The primary concept involved is log level management, which is straightforward.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement and code changes do not introduce or address new edge cases or error handling logic. The error (\"connection reset by peer\") is already being logged, and the change merely adjusts how it's reported (from error to debug level in one case). There is no indication of complex edge case handling or additional error conditions to consider beyond what's already in the code.\n\n4. **Overall Complexity**: The solution does not address the root cause of the error (e.g., fixing the connection reset issue itself) but rather mitigates its visibility by adjusting log verbosity. This is a surface-level change that does not require deep debugging or architectural modifications. It falls into the Easy category as it involves simple modifications to existing code logic without significant technical depth or impact.\n\nThus, a difficulty score of 0.25 reflects the simplicity of the changes, requiring minimal effort and understanding to implement, akin to a minor bug fix or logging adjustment.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Entrypoint redirections not working with defaultRuleSyntax=v2 in v3.0.0\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nI upgraded to v3 by following the migration guide. For easy migration I changed the default rule syntax to `v2`.\n\n### What did you see instead?\n\nAfter upgrading I noticed that redirections from HTTP to HTTPS no longer work.\n\n### What version of Traefik are you using?\n\nVersion:      3.0.0\r\nCodename:     beaufort\r\nGo version:   go1.22.2\r\nBuilt:        2024-04-29T14:25:59Z\r\nOS/Arch:      linux/amd64\n\n### What is your environment & configuration?\n\nThis is a simple example config:\r\n\r\n```yaml\r\ncore:\r\n  defaultRuleSyntax: v2\r\n\r\nentryPoints:\r\n  web:\r\n    address: :8080\r\n    http:\r\n      redirections:\r\n        entryPoint:\r\n          to: websecure\r\n          scheme: http\r\n\r\n  websecure:\r\n    address: :8443\r\n```\r\n\r\nWhen opening http://localhost:8080 with this config, there is no redirect to http://localhost:8443:\r\n```\r\n$ curl http://localhost:8080 -vvvv\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET / HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.81.0\r\n> Accept: */*\r\n> \r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 404 Not Found\r\n< Content-Type: text/plain; charset=utf-8\r\n< X-Content-Type-Options: nosniff\r\n< Date: Sat, 04 May 2024 05:24:53 GMT\r\n< Content-Length: 19\r\n< \r\n404 page not found\r\n* Connection #0 to host localhost left intact\r\n```\r\n\r\nWithout `core.defaultRuleSyntax` it works as expected:\r\n```\r\n$ curl http://localhost:8080 -vvvv\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET / HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.81.0\r\n> Accept: */*\r\n> \r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 301 Moved Permanently\r\n< Location: http://localhost:8443/\r\n< Date: Sat, 04 May 2024 05:25:40 GMT\r\n< Content-Length: 17\r\n< \r\n* Connection #0 to host localhost left intact\r\nMoved Permanently\r\n```\n\n### If applicable, please paste the log output in DEBUG level\n\n```\r\nWRN pkg/config/static/static_config.go:347 > v2 rules syntax is now deprecated, please use v3 instead...\r\nINF cmd/traefik/traefik.go:100 > Traefik version dev built on I don't remember exactly version=dev\r\nDBG cmd/traefik/traefik.go:107 > Static configuration loaded [json] staticConfiguration={\"core\":{\"defaultRuleSyntax\":\"v2\"},\"entryPoints\":{\"web\":{\"address\":\":8080\",\"forwardedHeaders\":{},\"http\":{\"redirections\":{\"entryPoint\":{\"permanent\":true,\"priority\":9223372036854775806,\"scheme\":\"http\",\"to\":\"websecure\"}}},\"http2\":{\"maxConcurrentStreams\":250},\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\",\"readTimeout\":\"1m0s\"}},\"udp\":{\"timeout\":\"3s\"}},\"websecure\":{\"address\":\":8443\",\"forwardedHeaders\":{},\"http\":{},\"http2\":{\"maxConcurrentStreams\":250},\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\",\"readTimeout\":\"1m0s\"}},\"udp\":{\"timeout\":\"3s\"}}},\"global\":{\"checkNewVersion\":true},\"log\":{\"format\":\"common\",\"level\":\"DEBUG\"},\"providers\":{\"providersThrottleDuration\":\"2s\"},\"serversTransport\":{\"maxIdleConnsPerHost\":200},\"tcpServersTransport\":{\"dialKeepAlive\":\"15s\",\"dialTimeout\":\"30s\"}}\r\nINF cmd/traefik/traefik.go:605 > \r\nStats collection is disabled.\r\nHelp us improve Traefik by turning this feature on :)\r\nMore details on: https://doc.traefik.io/traefik/contributing/data-collection/\r\n\r\nINF pkg/server/configurationwatcher.go:73 > Starting provider aggregator aggregator.ProviderAggregator\r\nDBG pkg/server/server_entrypoint_tcp.go:220 > Starting TCP Server entryPointName=web\r\nDBG pkg/server/server_entrypoint_tcp.go:220 > Starting TCP Server entryPointName=websecure\r\nINF pkg/provider/aggregator/aggregator.go:202 > Starting provider *traefik.Provider\r\nINF pkg/provider/aggregator/aggregator.go:202 > Starting provider *acme.ChallengeTLSALPN\r\nDBG pkg/provider/aggregator/aggregator.go:203 > *traefik.Provider provider configuration config={}\r\nDBG pkg/provider/aggregator/aggregator.go:203 > *acme.ChallengeTLSALPN provider configuration config={}\r\nDBG pkg/server/configurationwatcher.go:227 > Configuration received config={\"http\":{\"middlewares\":{\"redirect-web-to-websecure\":{\"redirectScheme\":{\"permanent\":true,\"port\":\"8443\",\"scheme\":\"http\"}}},\"models\":{\"web\":{},\"websecure\":{}},\"routers\":{\"web-to-websecure\":{\"entryPoints\":[\"web\"],\"middlewares\":[\"redirect-web-to-websecure\"],\"priority\":9223372036854775806,\"rule\":\"HostRegexp(`^.+$`)\",\"service\":\"noop@internal\"}},\"serversTransports\":{\"default\":{\"maxIdleConnsPerHost\":200}},\"services\":{\"noop\":{}}},\"tcp\":{\"serversTransports\":{\"default\":{\"dialKeepAlive\":\"15s\",\"dialTimeout\":\"30s\"}}},\"tls\":{},\"udp\":{}} providerName=internal\r\nDBG pkg/tls/tlsmanager.go:321 > No default certificate, fallback to the internal generated certificate tlsStoreName=default\r\nDBG pkg/middlewares/redirect/redirect_scheme.go:29 > Creating middleware entryPointName=web middlewareName=redirect-web-to-websecure@internal middlewareType=RedirectScheme routerName=web-to-websecure@internal\r\nDBG pkg/middlewares/redirect/redirect_scheme.go:30 > Setting up redirection to http 8443 entryPointName=web middlewareName=redirect-web-to-websecure@internal middlewareType=RedirectScheme routerName=web-to-websecure@internal\r\nDBG pkg/middlewares/recovery/recovery.go:22 > Creating middleware entryPointName=web middlewareName=traefik-internal\r\n```\n", "patch": "diff --git a/pkg/provider/kubernetes/ingress/kubernetes.go b/pkg/provider/kubernetes/ingress/kubernetes.go\nindex f3f4aa23ed..9411cac6d7 100644\n--- a/pkg/provider/kubernetes/ingress/kubernetes.go\n+++ b/pkg/provider/kubernetes/ingress/kubernetes.go\n@@ -277,9 +277,10 @@ func (p *Provider) loadConfigurationFromIngresses(ctx context.Context, client Cl\n \t\t\t}\n \n \t\t\trt := &dynamic.Router{\n-\t\t\t\tRule:     \"PathPrefix(`/`)\",\n-\t\t\t\tPriority: math.MinInt32,\n-\t\t\t\tService:  \"default-backend\",\n+\t\t\t\tRule:       \"PathPrefix(`/`)\",\n+\t\t\t\tRuleSyntax: \"v3\",\n+\t\t\t\tPriority:   math.MinInt32,\n+\t\t\t\tService:    \"default-backend\",\n \t\t\t}\n \n \t\t\tif rtConfig != nil && rtConfig.Router != nil {\ndiff --git a/pkg/provider/traefik/fixtures/api_insecure_with_dashboard.json b/pkg/provider/traefik/fixtures/api_insecure_with_dashboard.json\nindex 38a75d6b3b..c463389928 100644\n--- a/pkg/provider/traefik/fixtures/api_insecure_with_dashboard.json\n+++ b/pkg/provider/traefik/fixtures/api_insecure_with_dashboard.json\n@@ -7,6 +7,7 @@\n         ],\n         \"service\": \"api@internal\",\n         \"rule\": \"PathPrefix(`/api`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775806\n       },\n       \"dashboard\": {\n@@ -19,6 +20,7 @@\n         ],\n         \"service\": \"dashboard@internal\",\n         \"rule\": \"PathPrefix(`/`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775805\n       }\n     },\ndiff --git a/pkg/provider/traefik/fixtures/api_insecure_without_dashboard.json b/pkg/provider/traefik/fixtures/api_insecure_without_dashboard.json\nindex 9a98c0a1d4..6b59286088 100644\n--- a/pkg/provider/traefik/fixtures/api_insecure_without_dashboard.json\n+++ b/pkg/provider/traefik/fixtures/api_insecure_without_dashboard.json\n@@ -7,6 +7,7 @@\n         ],\n         \"service\": \"api@internal\",\n         \"rule\": \"PathPrefix(`/api`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775806\n       }\n     },\ndiff --git a/pkg/provider/traefik/fixtures/full_configuration.json b/pkg/provider/traefik/fixtures/full_configuration.json\nindex ed0ebfba69..24597958db 100644\n--- a/pkg/provider/traefik/fixtures/full_configuration.json\n+++ b/pkg/provider/traefik/fixtures/full_configuration.json\n@@ -7,6 +7,7 @@\n         ],\n         \"service\": \"api@internal\",\n         \"rule\": \"PathPrefix(`/api`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775806\n       },\n       \"dashboard\": {\n@@ -19,6 +20,7 @@\n         ],\n         \"service\": \"dashboard@internal\",\n         \"rule\": \"PathPrefix(`/`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775805\n       },\n       \"debug\": {\n@@ -27,6 +29,7 @@\n         ],\n         \"service\": \"api@internal\",\n         \"rule\": \"PathPrefix(`/debug`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775806\n       },\n       \"ping\": {\n@@ -35,6 +38,7 @@\n         ],\n         \"service\": \"ping@internal\",\n         \"rule\": \"PathPrefix(`/ping`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775807\n       },\n       \"prometheus\": {\n@@ -43,6 +47,7 @@\n         ],\n         \"service\": \"prometheus@internal\",\n         \"rule\": \"PathPrefix(`/metrics`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775807\n       },\n       \"rest\": {\n@@ -51,6 +56,7 @@\n         ],\n         \"service\": \"rest@internal\",\n         \"rule\": \"PathPrefix(`/api/providers`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775807\n       }\n     },\ndiff --git a/pkg/provider/traefik/fixtures/ping_simple.json b/pkg/provider/traefik/fixtures/ping_simple.json\nindex 6f159dce53..9e74ee0e3e 100644\n--- a/pkg/provider/traefik/fixtures/ping_simple.json\n+++ b/pkg/provider/traefik/fixtures/ping_simple.json\n@@ -7,6 +7,7 @@\n         ],\n         \"service\": \"ping@internal\",\n         \"rule\": \"PathPrefix(`/ping`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775807\n       }\n     },\ndiff --git a/pkg/provider/traefik/fixtures/prometheus_simple.json b/pkg/provider/traefik/fixtures/prometheus_simple.json\nindex b636b4569f..b385f6c9d5 100644\n--- a/pkg/provider/traefik/fixtures/prometheus_simple.json\n+++ b/pkg/provider/traefik/fixtures/prometheus_simple.json\n@@ -7,6 +7,7 @@\n         ],\n         \"service\": \"prometheus@internal\",\n         \"rule\": \"PathPrefix(`/metrics`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775807\n       }\n     },\ndiff --git a/pkg/provider/traefik/fixtures/redirection.json b/pkg/provider/traefik/fixtures/redirection.json\nindex 73ae77db34..a62480281f 100644\n--- a/pkg/provider/traefik/fixtures/redirection.json\n+++ b/pkg/provider/traefik/fixtures/redirection.json\n@@ -9,7 +9,8 @@\n           \"redirect-web-to-websecure\"\n         ],\n         \"service\": \"noop@internal\",\n-        \"rule\": \"HostRegexp(`^.+$`)\"\n+        \"rule\": \"HostRegexp(`^.+$`)\",\n+        \"ruleSyntax\": \"v3\"\n       }\n     },\n     \"services\": {\ndiff --git a/pkg/provider/traefik/fixtures/redirection_port.json b/pkg/provider/traefik/fixtures/redirection_port.json\nindex a9e75438a9..05ef94a7e4 100644\n--- a/pkg/provider/traefik/fixtures/redirection_port.json\n+++ b/pkg/provider/traefik/fixtures/redirection_port.json\n@@ -9,7 +9,8 @@\n           \"redirect-web-to-443\"\n         ],\n         \"service\": \"noop@internal\",\n-        \"rule\": \"HostRegexp(`^.+$`)\"\n+        \"rule\": \"HostRegexp(`^.+$`)\",\n+        \"ruleSyntax\": \"v3\"\n       }\n     },\n     \"services\": {\ndiff --git a/pkg/provider/traefik/fixtures/redirection_with_protocol.json b/pkg/provider/traefik/fixtures/redirection_with_protocol.json\nindex 73ae77db34..a62480281f 100644\n--- a/pkg/provider/traefik/fixtures/redirection_with_protocol.json\n+++ b/pkg/provider/traefik/fixtures/redirection_with_protocol.json\n@@ -9,7 +9,8 @@\n           \"redirect-web-to-websecure\"\n         ],\n         \"service\": \"noop@internal\",\n-        \"rule\": \"HostRegexp(`^.+$`)\"\n+        \"rule\": \"HostRegexp(`^.+$`)\",\n+        \"ruleSyntax\": \"v3\"\n       }\n     },\n     \"services\": {\ndiff --git a/pkg/provider/traefik/fixtures/rest_insecure.json b/pkg/provider/traefik/fixtures/rest_insecure.json\nindex e11a84e778..a9e13e02d8 100644\n--- a/pkg/provider/traefik/fixtures/rest_insecure.json\n+++ b/pkg/provider/traefik/fixtures/rest_insecure.json\n@@ -7,6 +7,7 @@\n         ],\n         \"service\": \"rest@internal\",\n         \"rule\": \"PathPrefix(`/api/providers`)\",\n+        \"ruleSyntax\": \"v3\",\n         \"priority\": 9223372036854775807\n       }\n     },\ndiff --git a/pkg/provider/traefik/internal.go b/pkg/provider/traefik/internal.go\nindex 7a78703d07..1f1b09a52a 100644\n--- a/pkg/provider/traefik/internal.go\n+++ b/pkg/provider/traefik/internal.go\n@@ -106,6 +106,7 @@ func (i *Provider) acme(cfg *dynamic.Configuration) {\n \tif len(eps) > 0 {\n \t\trt := &dynamic.Router{\n \t\t\tRule:        \"PathPrefix(`/.well-known/acme-challenge/`)\",\n+\t\t\tRuleSyntax:  \"v3\",\n \t\t\tEntryPoints: eps,\n \t\t\tService:     \"acme-http@internal\",\n \t\t\tPriority:    math.MaxInt,\n@@ -141,6 +142,7 @@ func (i *Provider) redirection(ctx context.Context, cfg *dynamic.Configuration)\n \n \t\trt := &dynamic.Router{\n \t\t\tRule:        \"HostRegexp(`^.+$`)\",\n+\t\t\tRuleSyntax:  \"v3\",\n \t\t\tEntryPoints: []string{name},\n \t\t\tMiddlewares: []string{mdName},\n \t\t\tService:     \"noop@internal\",\n@@ -241,6 +243,7 @@ func (i *Provider) apiConfiguration(cfg *dynamic.Configuration) {\n \t\t\tService:     \"api@internal\",\n \t\t\tPriority:    math.MaxInt - 1,\n \t\t\tRule:        \"PathPrefix(`/api`)\",\n+\t\t\tRuleSyntax:  \"v3\",\n \t\t}\n \n \t\tif i.staticCfg.API.Dashboard {\n@@ -249,6 +252,7 @@ func (i *Provider) apiConfiguration(cfg *dynamic.Configuration) {\n \t\t\t\tService:     \"dashboard@internal\",\n \t\t\t\tPriority:    math.MaxInt - 2,\n \t\t\t\tRule:        \"PathPrefix(`/`)\",\n+\t\t\t\tRuleSyntax:  \"v3\",\n \t\t\t\tMiddlewares: []string{\"dashboard_redirect@internal\", \"dashboard_stripprefix@internal\"},\n \t\t\t}\n \n@@ -270,6 +274,7 @@ func (i *Provider) apiConfiguration(cfg *dynamic.Configuration) {\n \t\t\t\tService:     \"api@internal\",\n \t\t\t\tPriority:    math.MaxInt - 1,\n \t\t\t\tRule:        \"PathPrefix(`/debug`)\",\n+\t\t\t\tRuleSyntax:  \"v3\",\n \t\t\t}\n \t\t}\n \t}\n@@ -292,6 +297,7 @@ func (i *Provider) pingConfiguration(cfg *dynamic.Configuration) {\n \t\t\tService:     \"ping@internal\",\n \t\t\tPriority:    math.MaxInt,\n \t\t\tRule:        \"PathPrefix(`/ping`)\",\n+\t\t\tRuleSyntax:  \"v3\",\n \t\t}\n \t}\n \n@@ -309,6 +315,7 @@ func (i *Provider) restConfiguration(cfg *dynamic.Configuration) {\n \t\t\tService:     \"rest@internal\",\n \t\t\tPriority:    math.MaxInt,\n \t\t\tRule:        \"PathPrefix(`/api/providers`)\",\n+\t\t\tRuleSyntax:  \"v3\",\n \t\t}\n \t}\n \n@@ -326,6 +333,7 @@ func (i *Provider) prometheusConfiguration(cfg *dynamic.Configuration) {\n \t\t\tService:     \"prometheus@internal\",\n \t\t\tPriority:    math.MaxInt,\n \t\t\tRule:        \"PathPrefix(`/metrics`)\",\n+\t\t\tRuleSyntax:  \"v3\",\n \t\t}\n \t}\n \n", "instance_id": "traefik__traefik-10689", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: HTTP to HTTPS redirections fail when using `defaultRuleSyntax=v2` in Traefik v3.0.0. It provides a reproducible example with configuration details, expected behavior, and actual output, which helps in understanding the problem. The logs and version information further aid in contextualizing the issue. However, there are minor ambiguities, such as a lack of explicit mention of specific edge cases or constraints (e.g., whether this issue affects only specific configurations or entry points). Additionally, the problem statement does not fully clarify whether the issue is isolated to the user's environment or is a general bug in Traefik. Despite these minor gaps, the statement is actionable and provides sufficient detail for investigation, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes, while not extensive in terms of lines of code, spans multiple files and involves critical configuration logic in Traefik, a complex reverse proxy system. The changes primarily add a `RuleSyntax: \"v3\"` field to various router configurations, indicating a need to enforce a specific rule syntax version to resolve compatibility issues between v2 and v3 syntaxes. This requires understanding Traefik's internal provider logic and configuration parsing mechanisms, which are non-trivial.\n\nSecond, the number of technical concepts involved is significant. Solving this requires knowledge of Traefik's rule syntax evolution (v2 vs. v3), dynamic configuration handling, and how entry point redirections are processed. Familiarity with Go (the language Traefik is written in) and its struct-based configuration management is necessary to implement and test the fix correctly.\n\nThird, while the problem statement does not explicitly mention edge cases, the nature of the issue (redirection behavior) implies potential complexities in handling various configurations, such as different entry point setups or mixed syntax usage. The code changes must ensure backward compatibility and avoid breaking existing configurations, adding to the difficulty.\n\nFinally, the impact on the codebase, though not architectural, affects core functionality (redirection logic), which is critical to Traefik's operation. A score of 0.65 reflects the need for a deep understanding of the system's internals and careful modification to ensure correctness across different use cases, without requiring a full architectural overhaul or advanced domain-specific knowledge beyond Traefik's domain.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "some TCP services with proxyProtocol.trustedIPs broken in 3.0.0-rc1\n### Welcome!\r\n\r\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\r\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\r\n\r\n### What did you do?\r\n\r\nI expose a mariadb instance from Traefik. It's running on a Nomad cluster, and the dynamic config is the consul catalog. I use a dedicated entrypoint on Traefik. The particularity is that this mariadb is reached through the consul service mesh. The service is exposed using these tags\r\n\r\n```\r\n        \"traefik.enable=true\",\r\n        \"traefik.tcp.routers.mariadb.entrypoints=mariadb\",\r\n        \"traefik.tcp.routers.mariadb.rule=HostSNI(`*`)\"\r\n```\r\n\r\nWith Traefik 3.0.0-beta5, everything is working nicely. With rc1, I can't connect to my mariadb service (timeout. Traefik is responding to the TCP connection, but it doesn't seem to route it through the service mesh, mariadb sees nothing)\r\n\r\n\r\n### What did you see instead?\r\n\r\nTimeout when trying to connect to my mariadb service. Only TCP services seem affected, I expose a lot of other HTTP services through the mesh without any issue.\r\n\r\n### What version of Traefik are you using?\r\n\r\n```\r\nVersion:      3.0.0-rc1\r\nCodename:     cheddar\r\nGo version:   go1.22.0\r\nBuilt:        2024-02-13T13:41:20Z\r\nOS/Arch:      linux/amd64\r\n```\r\n\r\n\r\n### What is your environment & configuration?\r\n\r\nI have this in the static config of Traefik\r\n```yaml\r\nproviders:\r\n  consulCatalog:\r\n    prefix: traefik\r\n    exposedByDefault: False\r\n    connectAware: True\r\n    connectByDefault: True\r\n[...]\r\n```\r\nThe entrypoint is configured like this\r\n```yaml\r\nentryPoints:\r\n  mariadb:\r\n    address: \":3306\"\r\n    proxyProtocol:\r\n      trustedIPs:\r\n        - 10.99.9.1\r\n    transport:\r\n      lifeCycle:\r\n        requestAcceptGraceTimeout: 4\r\n```\r\n\r\n\r\n### If applicable, please paste the log output in DEBUG level\r\n\r\n_No response_\n", "patch": "diff --git a/docs/content/migration/v2.md b/docs/content/migration/v2.md\nindex 0f3114fc30..0a80cf43dc 100644\n--- a/docs/content/migration/v2.md\n+++ b/docs/content/migration/v2.md\n@@ -577,3 +577,35 @@ the maximum user-defined router priority value is:\n \n - `(MaxInt32 - 1000)` for 32-bit platforms,\n - `(MaxInt64 - 1000)` for 64-bit platforms.\n+\n+### <EntryPoint>.Transport.RespondingTimeouts.<Timeout>\n+\n+Starting with `v2.11.1` the following timeout options are deprecated:\n+\n+- `<entryPoint>.transport.respondingTimeouts.readTimeout`\n+- `<entryPoint>.transport.respondingTimeouts.writeTimeout`\n+- `<entryPoint>.transport.respondingTimeouts.idleTimeout`\n+\n+They have been replaced by:\n+\n+- `<entryPoint>.transport.respondingTimeouts.http.readTimeout`\n+- `<entryPoint>.transport.respondingTimeouts.http.writeTimeout`\n+- `<entryPoint>.transport.respondingTimeouts.http.idleTimeout`\n+\n+### <EntryPoint>.Transport.RespondingTimeouts.TCP.LingeringTimeout\n+\n+Starting with `v2.11.1` a new `lingeringTimeout` entryPoints option has been introduced, with a default value of 2s.\n+\n+The lingering timeout defines the maximum duration between each TCP read operation on the connection.\n+As a layer 4 timeout, it applies during HTTP handling but respects the configured HTTP server `readTimeout`.\n+\n+This change avoids Traefik instances with the default configuration hanging while waiting for bytes to be read on the connection.\n+\n+We suggest to adapt this value accordingly to your situation.\n+The new default value is purposely narrowed and can close the connection too early.\n+\n+Increasing the `lingeringTimeout` value could be the solution notably if you are dealing with the following errors:\n+\n+- TCP: `Error while handling TCP connection: readfrom tcp X.X.X.X:X->X.X.X.X:X: read tcp X.X.X.X:X->X.X.X.X:X: i/o timeout`\n+- HTTP: `'499 Client Closed Request' caused by: context canceled`\n+- HTTP: `ReverseProxy read error during body copy: read tcp X.X.X.X:X->X.X.X.X:X: use of closed network connection`\ndiff --git a/docs/content/reference/static-configuration/cli-ref.md b/docs/content/reference/static-configuration/cli-ref.md\nindex 60a8c6513f..e3ae10e2a4 100644\n--- a/docs/content/reference/static-configuration/cli-ref.md\n+++ b/docs/content/reference/static-configuration/cli-ref.md\n@@ -183,15 +183,27 @@ Duration to give active requests a chance to finish before Traefik stops. (Defau\n `--entrypoints.<name>.transport.lifecycle.requestacceptgracetimeout`:  \n Duration to keep accepting requests before Traefik initiates the graceful shutdown procedure. (Default: ```0```)\n \n-`--entrypoints.<name>.transport.respondingtimeouts.idletimeout`:  \n+`--entrypoints.<name>.transport.respondingtimeouts.http.idletimeout`:  \n IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set. (Default: ```180```)\n \n-`--entrypoints.<name>.transport.respondingtimeouts.readtimeout`:  \n+`--entrypoints.<name>.transport.respondingtimeouts.http.readtimeout`:  \n ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set. (Default: ```0```)\n \n-`--entrypoints.<name>.transport.respondingtimeouts.writetimeout`:  \n+`--entrypoints.<name>.transport.respondingtimeouts.http.writetimeout`:  \n WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set. (Default: ```0```)\n \n+`--entrypoints.<name>.transport.respondingtimeouts.idletimeout`:  \n+(Deprecated) IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set. (Default: ```0```)\n+\n+`--entrypoints.<name>.transport.respondingtimeouts.readtimeout`:  \n+(Deprecated) ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set. (Default: ```0```)\n+\n+`--entrypoints.<name>.transport.respondingtimeouts.tcp.lingeringtimeout`:  \n+LingeringTimeout is the maximum duration between each TCP read operation on the connection. If zero, no timeout is set. (Default: ```2```)\n+\n+`--entrypoints.<name>.transport.respondingtimeouts.writetimeout`:  \n+(Deprecated) WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set. (Default: ```0```)\n+\n `--entrypoints.<name>.udp.timeout`:  \n Timeout defines how long to wait on an idle session before releasing the related resources. (Default: ```3```)\n \ndiff --git a/docs/content/reference/static-configuration/env-ref.md b/docs/content/reference/static-configuration/env-ref.md\nindex 849601f63c..9ae961694f 100644\n--- a/docs/content/reference/static-configuration/env-ref.md\n+++ b/docs/content/reference/static-configuration/env-ref.md\n@@ -183,15 +183,27 @@ Duration to give active requests a chance to finish before Traefik stops. (Defau\n `TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_LIFECYCLE_REQUESTACCEPTGRACETIMEOUT`:  \n Duration to keep accepting requests before Traefik initiates the graceful shutdown procedure. (Default: ```0```)\n \n-`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_IDLETIMEOUT`:  \n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_HTTP_IDLETIMEOUT`:  \n IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set. (Default: ```180```)\n \n-`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_READTIMEOUT`:  \n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_HTTP_READTIMEOUT`:  \n ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set. (Default: ```0```)\n \n-`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_WRITETIMEOUT`:  \n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_HTTP_WRITETIMEOUT`:  \n WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set. (Default: ```0```)\n \n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_IDLETIMEOUT`:  \n+(Deprecated) IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set. (Default: ```0```)\n+\n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_READTIMEOUT`:  \n+(Deprecated) ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set. (Default: ```0```)\n+\n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_TCP_LINGERINGTIMEOUT`:  \n+LingeringTimeout is the maximum duration between each TCP read operation on the connection. If zero, no timeout is set. (Default: ```2```)\n+\n+`TRAEFIK_ENTRYPOINTS_<NAME>_TRANSPORT_RESPONDINGTIMEOUTS_WRITETIMEOUT`:  \n+(Deprecated) WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set. (Default: ```0```)\n+\n `TRAEFIK_ENTRYPOINTS_<NAME>_UDP_TIMEOUT`:  \n Timeout defines how long to wait on an idle session before releasing the related resources. (Default: ```3```)\n \ndiff --git a/docs/content/reference/static-configuration/file.toml b/docs/content/reference/static-configuration/file.toml\nindex 42c4e6fdf1..bb402467cd 100644\n--- a/docs/content/reference/static-configuration/file.toml\n+++ b/docs/content/reference/static-configuration/file.toml\n@@ -26,6 +26,12 @@\n         readTimeout = \"42s\"\n         writeTimeout = \"42s\"\n         idleTimeout = \"42s\"\n+        [entryPoints.EntryPoint0.transport.respondingTimeouts.http]\n+          readTimeout = \"42s\"\n+          writeTimeout = \"42s\"\n+          idleTimeout = \"42s\"\n+        [entryPoints.EntryPoint0.transport.respondingTimeouts.tcp]\n+          lingeringTimeout = \"42s\"\n     [entryPoints.EntryPoint0.proxyProtocol]\n       insecure = true\n       trustedIPs = [\"foobar\", \"foobar\"]\ndiff --git a/docs/content/reference/static-configuration/file.yaml b/docs/content/reference/static-configuration/file.yaml\nindex abb8e05d88..d5265aa1d9 100644\n--- a/docs/content/reference/static-configuration/file.yaml\n+++ b/docs/content/reference/static-configuration/file.yaml\n@@ -24,6 +24,12 @@ entryPoints:\n         readTimeout: 42s\n         writeTimeout: 42s\n         idleTimeout: 42s\n+        http:\n+          readTimeout: 42s\n+          writeTimeout: 42s\n+          idleTimeout: 42s\n+        tcp:\n+          lingeringTimeout: 42s\n       keepAliveMaxTime: 42s\n       keepAliveMaxRequests: 42\n     proxyProtocol:\ndiff --git a/docs/content/routing/entrypoints.md b/docs/content/routing/entrypoints.md\nindex 70ce11cf5f..c6a79ba8e7 100644\n--- a/docs/content/routing/entrypoints.md\n+++ b/docs/content/routing/entrypoints.md\n@@ -397,10 +397,11 @@ You can configure Traefik to trust the forwarded headers information (`X-Forward\n \n #### `respondingTimeouts`\n \n-`respondingTimeouts` are timeouts for incoming requests to the Traefik instance.\n-Setting them has no effect for UDP entryPoints.\n+##### `http`\n \n-??? info \"`transport.respondingTimeouts.readTimeout`\"\n+`respondingTimeouts.http` are timeouts for incoming requests to the Traefik instance.\n+\n+??? info \"`transport.respondingTimeouts.http.readTimeout`\"\n \n     _Optional, Default=0s_\n \n@@ -417,7 +418,8 @@ Setting them has no effect for UDP entryPoints.\n         address: \":8888\"\n         transport:\n           respondingTimeouts:\n-            readTimeout: 42\n+            http:\n+              readTimeout: 42\n     ```\n \n     ```toml tab=\"File (TOML)\"\n@@ -425,18 +427,17 @@ Setting them has no effect for UDP entryPoints.\n     [entryPoints]\n       [entryPoints.name]\n         address = \":8888\"\n-        [entryPoints.name.transport]\n-          [entryPoints.name.transport.respondingTimeouts]\n-            readTimeout = 42\n+        [entryPoints.name.transport.respondingTimeouts.http]\n+          readTimeout = 42\n     ```\n \n     ```bash tab=\"CLI\"\n     ## Static configuration\n     --entryPoints.name.address=:8888\n-    --entryPoints.name.transport.respondingTimeouts.readTimeout=42\n+    --entryPoints.name.transport.respondingTimeouts.http.readTimeout=42\n     ```\n \n-??? info \"`transport.respondingTimeouts.writeTimeout`\"\n+??? info \"`transport.respondingTimeouts.http.writeTimeout`\"\n \n     _Optional, Default=0s_\n \n@@ -454,7 +455,8 @@ Setting them has no effect for UDP entryPoints.\n         address: \":8888\"\n         transport:\n           respondingTimeouts:\n-            writeTimeout: 42\n+            http:\n+              writeTimeout: 42\n     ```\n \n     ```toml tab=\"File (TOML)\"\n@@ -462,18 +464,17 @@ Setting them has no effect for UDP entryPoints.\n     [entryPoints]\n       [entryPoints.name]\n         address = \":8888\"\n-        [entryPoints.name.transport]\n-          [entryPoints.name.transport.respondingTimeouts]\n-            writeTimeout = 42\n+        [entryPoints.name.transport.respondingTimeouts.http]\n+          writeTimeout = 42\n     ```\n \n     ```bash tab=\"CLI\"\n     ## Static configuration\n     --entryPoints.name.address=:8888\n-    --entryPoints.name.transport.respondingTimeouts.writeTimeout=42\n+    --entryPoints.name.transport.respondingTimeouts.http.writeTimeout=42\n     ```\n \n-??? info \"`transport.respondingTimeouts.idleTimeout`\"\n+??? info \"`transport.respondingTimeouts.http.idleTimeout`\"\n \n     _Optional, Default=180s_\n \n@@ -490,7 +491,8 @@ Setting them has no effect for UDP entryPoints.\n         address: \":8888\"\n         transport:\n           respondingTimeouts:\n-            idleTimeout: 42\n+            http:\n+              idleTimeout: 42\n     ```\n \n     ```toml tab=\"File (TOML)\"\n@@ -498,15 +500,54 @@ Setting them has no effect for UDP entryPoints.\n     [entryPoints]\n       [entryPoints.name]\n         address = \":8888\"\n-        [entryPoints.name.transport]\n-          [entryPoints.name.transport.respondingTimeouts]\n-            idleTimeout = 42\n+        [entryPoints.name.transport.respondingTimeouts.http]\n+          idleTimeout = 42\n     ```\n \n     ```bash tab=\"CLI\"\n     ## Static configuration\n     --entryPoints.name.address=:8888\n-    --entryPoints.name.transport.respondingTimeouts.idleTimeout=42\n+    --entryPoints.name.transport.respondingTimeouts.http.idleTimeout=42\n+\n+##### `tcp`\n+\n+`respondingTimeouts.tcp` are timeouts for client connections to the Traefik instance.\n+\n+??? info \"`transport.respondingTimeouts.tcp.lingeringTimeout`\"\n+\n+    _Optional, Default=2s_\n+\n+    `lingeringTimeout` is the maximum duration between each TCP read operation on the connection.\n+    As a layer 4 timeout, it also applies during HTTP handling, but respect the configured HTTP server `readTimeout`.\n+\n+    If zero, the lingering is disabled.  \n+    Can be provided in a format supported by [time.ParseDuration](https://golang.org/pkg/time/#ParseDuration) or as raw values (digits).\n+    If no units are provided, the value is parsed assuming seconds.\n+\n+    ```yaml tab=\"File (YAML)\"\n+    ## Static configuration\n+    entryPoints:\n+      name:\n+        address: \":8888\"\n+        transport:\n+          respondingTimeouts:\n+            tcp:\n+              lingeringTimeout: 42\n+    ```\n+\n+    ```toml tab=\"File (TOML)\"\n+    ## Static configuration\n+    [entryPoints]\n+      [entryPoints.name]\n+        address = \":8888\"\n+        [entryPoints.name.transport.respondingTimeouts.tcp]\n+          lingeringTimeout = 42\n+    ```\n+\n+    ```bash tab=\"CLI\"\n+    ## Static configuration\n+    --entryPoints.name.address=:8888\n+    --entryPoints.name.transport.respondingTimeouts.tcp.lingeringTimeout=42\n     ```\n \n #### `lifeCycle`\ndiff --git a/pkg/config/static/static_config.go b/pkg/config/static/static_config.go\nindex dcf9b3dbab..2a47763a03 100644\n--- a/pkg/config/static/static_config.go\n+++ b/pkg/config/static/static_config.go\n@@ -56,6 +56,9 @@ const (\n \t// DefaultUDPTimeout defines how long to wait by default on an idle session,\n \t// before releasing all resources related to that session.\n \tDefaultUDPTimeout = 3 * time.Second\n+\n+\t// defaultLingeringTimeout defines the default maximum duration between each read operation on the connection.\n+\tdefaultLingeringTimeout = 2 * time.Second\n )\n \n // Configuration is the static configuration.\n@@ -118,16 +121,44 @@ func (a *API) SetDefaults() {\n \ta.Dashboard = true\n }\n \n-// RespondingTimeouts contains timeout configurations for incoming requests to the Traefik instance.\n+// RespondingTimeouts contains timeout configurations.\n type RespondingTimeouts struct {\n-\tReadTimeout  ptypes.Duration `description:\"ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set.\" json:\"readTimeout,omitempty\" toml:\"readTimeout,omitempty\" yaml:\"readTimeout,omitempty\" export:\"true\"`\n-\tWriteTimeout ptypes.Duration `description:\"WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set.\" json:\"writeTimeout,omitempty\" toml:\"writeTimeout,omitempty\" yaml:\"writeTimeout,omitempty\" export:\"true\"`\n-\tIdleTimeout  ptypes.Duration `description:\"IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set.\" json:\"idleTimeout,omitempty\" toml:\"idleTimeout,omitempty\" yaml:\"idleTimeout,omitempty\" export:\"true\"`\n+\t// Deprecated: please use `respondingTimeouts.http.readTimeout` instead.\n+\tReadTimeout *ptypes.Duration `description:\"(Deprecated) ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set.\" json:\"readTimeout,omitempty\" toml:\"readTimeout,omitempty\" yaml:\"readTimeout,omitempty\" export:\"true\"`\n+\t// Deprecated: please use `respondingTimeouts.http.writeTimeout` instead.\n+\tWriteTimeout *ptypes.Duration `description:\"(Deprecated) WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set.\" json:\"writeTimeout,omitempty\" toml:\"writeTimeout,omitempty\" yaml:\"writeTimeout,omitempty\" export:\"true\"`\n+\t// Deprecated: please use `respondingTimeouts.http.idleTimeout` instead.\n+\tIdleTimeout *ptypes.Duration `description:\"(Deprecated) IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set.\" json:\"idleTimeout,omitempty\" toml:\"idleTimeout,omitempty\" yaml:\"idleTimeout,omitempty\" export:\"true\"`\n+\n+\tHTTP *HTTPRespondingTimeouts `description:\"Defines the HTTP responding timeouts.\" json:\"http,omitempty\" toml:\"http,omitempty\" yaml:\"http,omitempty\" export:\"true\"`\n+\tTCP  *TCPRespondingTimeouts  `description:\"Defines the TCP responding timeouts.\" json:\"tcp,omitempty\" toml:\"tcp,omitempty\" yaml:\"tcp,omitempty\" export:\"true\"`\n+}\n+\n+// HTTPRespondingTimeouts contains HTTP timeout configurations for incoming requests to the Traefik instance.\n+type HTTPRespondingTimeouts struct {\n+\tReadTimeout  *ptypes.Duration `description:\"ReadTimeout is the maximum duration for reading the entire request, including the body. If zero, no timeout is set.\" json:\"readTimeout,omitempty\" toml:\"readTimeout,omitempty\" yaml:\"readTimeout,omitempty\" export:\"true\"`\n+\tWriteTimeout *ptypes.Duration `description:\"WriteTimeout is the maximum duration before timing out writes of the response. If zero, no timeout is set.\" json:\"writeTimeout,omitempty\" toml:\"writeTimeout,omitempty\" yaml:\"writeTimeout,omitempty\" export:\"true\"`\n+\tIdleTimeout  *ptypes.Duration `description:\"IdleTimeout is the maximum amount duration an idle (keep-alive) connection will remain idle before closing itself. If zero, no timeout is set.\" json:\"idleTimeout,omitempty\" toml:\"idleTimeout,omitempty\" yaml:\"idleTimeout,omitempty\" export:\"true\"`\n+}\n+\n+// TCPRespondingTimeouts contains TCP timeout configurations for client connections to the Traefik instance.\n+type TCPRespondingTimeouts struct {\n+\tLingeringTimeout ptypes.Duration `description:\"LingeringTimeout is the maximum duration between each TCP read operation on the connection. If zero, no timeout is set.\" json:\"lingeringTimeout,omitempty\" toml:\"lingeringTimeout,omitempty\" yaml:\"lingeringTimeout,omitempty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values.\n func (a *RespondingTimeouts) SetDefaults() {\n-\ta.IdleTimeout = ptypes.Duration(DefaultIdleTimeout)\n+\tnoTimeout := ptypes.Duration(0)\n+\tdefaultIdleTimeout := ptypes.Duration(DefaultIdleTimeout)\n+\ta.HTTP = &HTTPRespondingTimeouts{\n+\t\tReadTimeout:  &noTimeout,\n+\t\tWriteTimeout: &noTimeout,\n+\t\tIdleTimeout:  &defaultIdleTimeout,\n+\t}\n+\n+\ta.TCP = &TCPRespondingTimeouts{\n+\t\tLingeringTimeout: ptypes.Duration(defaultLingeringTimeout),\n+\t}\n }\n \n // ForwardingTimeouts contains timeout configurations for forwarding requests to the backend servers.\n@@ -211,6 +242,39 @@ func (c *Configuration) SetEffectiveConfiguration() {\n \t\tc.EntryPoints[\"http\"] = ep\n \t}\n \n+\tfor _, entrypoint := range c.EntryPoints {\n+\t\tif entrypoint.Transport == nil ||\n+\t\t\tentrypoint.Transport.RespondingTimeouts == nil {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\trespondingTimeouts := entrypoint.Transport.RespondingTimeouts\n+\n+\t\tif respondingTimeouts.ReadTimeout != nil &&\n+\t\t\trespondingTimeouts.HTTP != nil &&\n+\t\t\trespondingTimeouts.HTTP.ReadTimeout == nil {\n+\t\t\tlog.WithoutContext().Warnf(\"Option `respondingTimeouts.readTimeout` is deprecated, please use `respondingTimeouts.http.readTimeout` instead.\")\n+\t\t\trespondingTimeouts.HTTP.ReadTimeout = respondingTimeouts.ReadTimeout\n+\t\t\trespondingTimeouts.ReadTimeout = nil\n+\t\t}\n+\n+\t\tif respondingTimeouts.WriteTimeout != nil &&\n+\t\t\trespondingTimeouts.HTTP != nil &&\n+\t\t\trespondingTimeouts.HTTP.WriteTimeout == nil {\n+\t\t\tlog.WithoutContext().Warnf(\"Option `respondingTimeouts.writeTimeout` is deprecated, please use `respondingTimeouts.http.writeTimeout` instead.\")\n+\t\t\trespondingTimeouts.HTTP.WriteTimeout = respondingTimeouts.WriteTimeout\n+\t\t\trespondingTimeouts.WriteTimeout = nil\n+\t\t}\n+\n+\t\tif respondingTimeouts.IdleTimeout != nil &&\n+\t\t\trespondingTimeouts.HTTP != nil &&\n+\t\t\trespondingTimeouts.HTTP.IdleTimeout == nil {\n+\t\t\tlog.WithoutContext().Warnf(\"Option `respondingTimeouts.idleTimeout` is deprecated, please use `respondingTimeouts.http.idleTimeout` instead.\")\n+\t\t\trespondingTimeouts.HTTP.IdleTimeout = respondingTimeouts.IdleTimeout\n+\t\t\trespondingTimeouts.IdleTimeout = nil\n+\t\t}\n+\t}\n+\n \t// Creates the internal traefik entry point if needed\n \tif (c.API != nil && c.API.Insecure) ||\n \t\t(c.Ping != nil && !c.Ping.ManualRouting && c.Ping.EntryPoint == DefaultInternalEntryPointName) ||\n@@ -316,6 +380,31 @@ func (c *Configuration) ValidateConfiguration() error {\n \t\treturn errors.New(\"Nomad provider cannot have both namespace and namespaces options configured\")\n \t}\n \n+\tfor epName, entrypoint := range c.EntryPoints {\n+\t\tif entrypoint.Transport == nil ||\n+\t\t\tentrypoint.Transport.RespondingTimeouts == nil ||\n+\t\t\tentrypoint.Transport.RespondingTimeouts.HTTP == nil {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\trespondingTimeouts := entrypoint.Transport.RespondingTimeouts\n+\n+\t\tif respondingTimeouts.ReadTimeout != nil &&\n+\t\t\trespondingTimeouts.HTTP.ReadTimeout != nil {\n+\t\t\treturn fmt.Errorf(\"entrypoint %q has `readTimeout` option is defined multiple times (`respondingTimeouts.readTimeout` is deprecated)\", epName)\n+\t\t}\n+\n+\t\tif respondingTimeouts.WriteTimeout != nil &&\n+\t\t\trespondingTimeouts.HTTP.WriteTimeout != nil {\n+\t\t\treturn fmt.Errorf(\"entrypoint %q has `writeTimeout` option is defined multiple times (`respondingTimeouts.writeTimeout` is deprecated)\", epName)\n+\t\t}\n+\n+\t\tif respondingTimeouts.IdleTimeout != nil &&\n+\t\t\trespondingTimeouts.HTTP.IdleTimeout != nil {\n+\t\t\treturn fmt.Errorf(\"entrypoint %q has `idleTimeout` option is defined multiple times (`respondingTimeouts.idleTimeout` is deprecated)\", epName)\n+\t\t}\n+\t}\n+\n \treturn nil\n }\n \ndiff --git a/pkg/server/router/tcp/router.go b/pkg/server/router/tcp/router.go\nindex 0d85243988..551bb43e74 100644\n--- a/pkg/server/router/tcp/router.go\n+++ b/pkg/server/router/tcp/router.go\n@@ -9,7 +9,6 @@ import (\n \t\"net\"\n \t\"net/http\"\n \t\"slices\"\n-\t\"time\"\n \n \t\"github.com/go-acme/lego/v4/challenge/tlsalpn01\"\n \t\"github.com/traefik/traefik/v2/pkg/log\"\n@@ -117,17 +116,6 @@ func (r *Router) ServeTCP(conn tcp.WriteCloser) {\n \t\treturn\n \t}\n \n-\t// Remove read/write deadline and delegate this to underlying tcp server (for now only handled by HTTP Server)\n-\terr = conn.SetReadDeadline(time.Time{})\n-\tif err != nil {\n-\t\tlog.WithoutContext().Errorf(\"Error while setting read deadline: %v\", err)\n-\t}\n-\n-\terr = conn.SetWriteDeadline(time.Time{})\n-\tif err != nil {\n-\t\tlog.WithoutContext().Errorf(\"Error while setting write deadline: %v\", err)\n-\t}\n-\n \tconnData, err := tcpmuxer.NewConnData(hello.serverName, conn, hello.protos)\n \tif err != nil {\n \t\tlog.WithoutContext().Errorf(\"Error while reading TCP connection data: %v\", err)\ndiff --git a/pkg/server/server_entrypoint_tcp.go b/pkg/server/server_entrypoint_tcp.go\nindex 1fb371d081..b5aaa1ac0b 100644\n--- a/pkg/server/server_entrypoint_tcp.go\n+++ b/pkg/server/server_entrypoint_tcp.go\n@@ -244,24 +244,15 @@ func (e *TCPEntryPoint) Start(ctx context.Context) {\n \t\t\tpanic(err)\n \t\t}\n \n-\t\tsafe.Go(func() {\n-\t\t\t// Enforce read/write deadlines at the connection level,\n-\t\t\t// because when we're peeking the first byte to determine whether we are doing TLS,\n-\t\t\t// the deadlines at the server level are not taken into account.\n-\t\t\tif e.transportConfiguration.RespondingTimeouts.ReadTimeout > 0 {\n-\t\t\t\terr := writeCloser.SetReadDeadline(time.Now().Add(time.Duration(e.transportConfiguration.RespondingTimeouts.ReadTimeout)))\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tlogger.Errorf(\"Error while setting read deadline: %v\", err)\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tif e.transportConfiguration.RespondingTimeouts.WriteTimeout > 0 {\n-\t\t\t\terr = writeCloser.SetWriteDeadline(time.Now().Add(time.Duration(e.transportConfiguration.RespondingTimeouts.WriteTimeout)))\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tlogger.Errorf(\"Error while setting write deadline: %v\", err)\n-\t\t\t\t}\n-\t\t\t}\n+\t\tif e.transportConfiguration != nil &&\n+\t\t\te.transportConfiguration.RespondingTimeouts != nil &&\n+\t\t\te.transportConfiguration.RespondingTimeouts.TCP != nil &&\n+\t\t\te.transportConfiguration.RespondingTimeouts.TCP.LingeringTimeout > 0 {\n+\t\t\tlingeringTimeout := time.Duration(e.transportConfiguration.RespondingTimeouts.TCP.LingeringTimeout)\n+\t\t\twriteCloser = newLingeringConnection(writeCloser, lingeringTimeout)\n+\t\t}\n \n+\t\tsafe.Go(func() {\n \t\t\te.switcher.ServeTCP(newTrackedConnection(writeCloser, e.tracker))\n \t\t})\n \t}\n@@ -391,6 +382,55 @@ func writeCloser(conn net.Conn) (tcp.WriteCloser, error) {\n \t}\n }\n \n+// lingeringConn represents a writeCloser with lingeringTimeout handling.\n+type lingeringConn struct {\n+\ttcp.WriteCloser\n+\n+\tlingeringTimeout time.Duration\n+\n+\trdlMu sync.RWMutex\n+\t// readDeadline is the current readDeadline set by an upper caller.\n+\t// In case of HTTP, the HTTP go server manipulates deadlines on the connection.\n+\treadDeadline time.Time\n+}\n+\n+// newLingeringConnection returns the given writeCloser augmented with lingeringTimeout handling.\n+func newLingeringConnection(conn tcp.WriteCloser, timeout time.Duration) tcp.WriteCloser {\n+\treturn &lingeringConn{\n+\t\tWriteCloser:      conn,\n+\t\tlingeringTimeout: timeout,\n+\t}\n+}\n+\n+// Read reads data from the connection and postpones the connection readDeadline according to the lingeringTimeout config.\n+// It also ensures that the upper level set readDeadline is enforced.\n+func (l *lingeringConn) Read(b []byte) (int, error) {\n+\tif l.lingeringTimeout > 0 {\n+\t\tdeadline := time.Now().Add(l.lingeringTimeout)\n+\n+\t\tl.rdlMu.RLock()\n+\t\tif !l.readDeadline.IsZero() && deadline.After(l.readDeadline) {\n+\t\t\tdeadline = l.readDeadline\n+\t\t}\n+\t\tl.rdlMu.RUnlock()\n+\n+\t\tif err := l.WriteCloser.SetReadDeadline(deadline); err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t}\n+\n+\treturn l.WriteCloser.Read(b)\n+}\n+\n+// SetReadDeadline sets and save the read deadline.\n+func (l *lingeringConn) SetReadDeadline(t time.Time) error {\n+\tl.rdlMu.Lock()\n+\tl.readDeadline = t\n+\tl.rdlMu.Unlock()\n+\n+\treturn l.WriteCloser.SetReadDeadline(t)\n+}\n+\n // tcpKeepAliveListener sets TCP keep-alive timeouts on accepted\n // connections.\n type tcpKeepAliveListener struct {\n@@ -419,7 +459,7 @@ func (ln tcpKeepAliveListener) Accept() (net.Conn, error) {\n }\n \n func buildProxyProtocolListener(ctx context.Context, entryPoint *static.EntryPoint, listener net.Listener) (net.Listener, error) {\n-\ttimeout := entryPoint.Transport.RespondingTimeouts.ReadTimeout\n+\ttimeout := *entryPoint.Transport.RespondingTimeouts.HTTP.ReadTimeout\n \t// proxyproto use 200ms if ReadHeaderTimeout is set to 0 and not no timeout\n \tif timeout == 0 {\n \t\ttimeout = -1\n@@ -588,9 +628,9 @@ func createHTTPServer(ctx context.Context, ln net.Listener, configuration *stati\n \tserverHTTP := &http.Server{\n \t\tHandler:      handler,\n \t\tErrorLog:     httpServerLogger,\n-\t\tReadTimeout:  time.Duration(configuration.Transport.RespondingTimeouts.ReadTimeout),\n-\t\tWriteTimeout: time.Duration(configuration.Transport.RespondingTimeouts.WriteTimeout),\n-\t\tIdleTimeout:  time.Duration(configuration.Transport.RespondingTimeouts.IdleTimeout),\n+\t\tReadTimeout:  time.Duration(*configuration.Transport.RespondingTimeouts.HTTP.ReadTimeout),\n+\t\tWriteTimeout: time.Duration(*configuration.Transport.RespondingTimeouts.HTTP.WriteTimeout),\n+\t\tIdleTimeout:  time.Duration(*configuration.Transport.RespondingTimeouts.HTTP.IdleTimeout),\n \t}\n \tif debugConnection || (configuration.Transport != nil && (configuration.Transport.KeepAliveMaxTime > 0 || configuration.Transport.KeepAliveMaxRequests > 0)) {\n \t\tserverHTTP.ConnContext = func(ctx context.Context, c net.Conn) context.Context {\n", "instance_id": "traefik__traefik-10569", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: a regression in Traefik 3.0.0-rc1 where TCP services (specifically a MariaDB instance) exposed through a Consul service mesh fail to connect, resulting in timeouts. The user provides relevant context, such as the configuration (static and dynamic), environment (Nomad cluster), and the specific entrypoint setup with `proxyProtocol.trustedIPs`. They also mention that HTTP services are unaffected, narrowing the scope to TCP routing. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement lacks specific error logs or debug output (despite a placeholder for logs), which could help pinpoint the root cause. Additionally, while the configuration snippets are helpful, there\u2019s no mention of specific edge cases or conditions under which the timeout occurs (e.g., specific client behaviors or network conditions). The goal is clear\u2014fix the TCP routing issue\u2014but the lack of exhaustive diagnostic information or explicit test cases slightly reduces clarity.\n", "difficulty_explanation": "\nI assign a difficulty score of 0.75, placing this problem in the \"Hard\" category, due to several factors:\n\n1. **Clarity and Complexity of the Problem Description**: While the issue is mostly clear, diagnosing and resolving a regression in TCP routing through a service mesh involves understanding nuanced interactions between Traefik\u2019s TCP handling, Consul integration, and proxy protocol configurations. The lack of detailed logs or specific failure conditions adds to the initial diagnostic challenge.\n\n2. **Scope and Depth of Code Changes**: The provided code changes are extensive, spanning multiple files and affecting core configuration and server logic in Traefik. The modifications include introducing a new `lingeringTimeout` for TCP connections, deprecating old timeout configurations, and restructuring how timeouts are applied (separating HTTP and TCP timeouts). These changes impact critical areas like `server_entrypoint_tcp.go` and configuration parsing in `static_config.go`, requiring a deep understanding of Traefik\u2019s architecture. While the changes are not system-wide refactors, they touch sensitive areas (connection handling and timeouts) that can affect overall system behavior and stability.\n\n3. **Number of Technical Concepts**: Solving this issue requires familiarity with several advanced concepts:\n   - **Go-specific features**: Handling of `net.Conn` interfaces, timeouts, and custom connection wrappers (e.g., `lingeringConn`).\n   - **Networking and TCP**: Understanding TCP connection lifecycle, lingering timeouts, and proxy protocols.\n   - **Traefik internals**: Knowledge of entrypoint configurations, routing logic, and service mesh integration (Consul).\n   - **Configuration management**: Handling backward compatibility and deprecation of configuration options.\n   These concepts are moderately to highly complex, especially for someone not already familiar with Traefik\u2019s codebase or low-level networking in Go.\n\n4. **Edge Cases and Error Handling**: The problem and code changes implicitly address edge cases related to TCP connection timeouts (e.g., preventing hanging connections with `lingeringTimeout`). The implementation also considers interactions with HTTP timeouts, ensuring they are respected. Error handling is added for configuration validation (e.g., preventing duplicate timeout definitions). However, the problem statement itself does not explicitly mention edge cases like specific client behaviors or network conditions, which the developer must infer and address. The code changes show careful handling of these scenarios, but identifying and testing all possible edge cases (e.g., various network latencies, client disconnects) adds to the difficulty.\n\nOverall, this problem requires a deep understanding of Traefik\u2019s TCP routing and timeout mechanisms, as well as careful implementation to avoid introducing new issues in a critical part of the system. The score of 0.75 reflects the need for significant expertise and the potential impact of the changes, though it falls short of \"Very Hard\" as it does not involve entirely new system design or distributed systems challenges.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "S3 Select causing high memory consumption on >3GiB file\nWhen running a select statement on a gzip json dataset (~3 GiB), the memory usage spikes significantly more than I would expect.\r\n\r\n![image](https://github.com/user-attachments/assets/d9795408-b646-41aa-ad1b-419415e9c332)\r\n\r\n## Expected Behavior\r\n\r\nI would expect the memory consumption of an S3 Select operation to be relative to the limit on the sql command, and that stream data processing is used, so the entire object is not loaded  into memory.\r\n\r\n## Current Behavior\r\nI'm using the mc client to run an S3 Select statement on my ~3GiB gzip json file (11.5 GiB uncompressed). The file is a json array (i.e. not NDJSON), so I query it with the following command:\r\n```\r\nmc sql myminio/mybucket/dataset.json --query \"SELECT * FROM S3Object[*]._1[*] s LIMIT 10\" --json-input type=DOCUMENT --compression GZIP\r\n```\r\n\r\nThe command hangs for a while, before I can see the minio pod is OOMKilled - This is running on a 32GiB box, where all other workloads have been removed, so it is free to use all the resources.\r\n\r\nI tried this on a 64GiB box, which worked, but the memory stayed high after the command returned a response, for longer than I'd expect (as seen in the diagram of memory usage).\r\n\r\n## Possible Solution\r\n\u00af\\_(\u30c4)_/\u00af\r\n\r\n## Steps to Reproduce (for bugs)\r\n1. Setup Minio on a box with 32GiB memory.\r\n2. Create a 3GiB Gzip JSON file, storing a json array of records: i.e. `[{ \"a\": 1, ... }, ...]`, and upload to bucket\r\n3. Run a select command:\r\n```\r\nmc sql myminio/mybucket/dataset.json --query \"SELECT * FROM S3Object[*]._1[*] s LIMIT 10\" --json-input type=DOCUMENT --compression GZIP\r\n```\r\n\r\n## Context\r\nI want to allow users to query parts of the dataset in a language they are familiar with (SQL).\r\n\r\n## Regression\r\nNot sure, but I have seen this issue before which I thought could be related: https://github.com/minio/minio/issues/17235\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Version used (`minio --version`):\r\nminio version DEVELOPMENT.2024-09-09T16-59-28Z (commit-id=0b7aa6af879e088030d63e91a29dabf22fdd3a18)\r\nRuntime: go1.22.7 linux/amd64\r\nLicense: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html\r\nCopyright: 2015-2024 MinIO, Inc.\r\n\r\n* Server setup and configuration: Deployed to an AKS cluster using the bitnami minio helm chart version 14.7.8 with default config, but using nodeSelector and tolerations to force it onto a node with no workloads running.\r\n\r\n* Operating System and version (`uname -a`): Node image version: AKSUbuntu-2204gen2containerd-202409.04.0\r\n\n", "patch": "diff --git a/internal/s3select/json/reader.go b/internal/s3select/json/reader.go\nindex 4285c23fa5211..52eda1404866f 100644\n--- a/internal/s3select/json/reader.go\n+++ b/internal/s3select/json/reader.go\n@@ -26,6 +26,10 @@ import (\n \t\"github.com/bcicen/jstream\"\n )\n \n+// Limit single document size to 10MiB, 10x the AWS limit:\n+// https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html\n+const maxDocumentSize = 10 << 20\n+\n // Reader - JSON record reader for S3Select.\n type Reader struct {\n \targs       *ReaderArgs\n@@ -80,7 +84,7 @@ func (r *Reader) Close() error {\n // NewReader - creates new JSON reader using readCloser.\n func NewReader(readCloser io.ReadCloser, args *ReaderArgs) *Reader {\n \treadCloser = &syncReadCloser{rc: readCloser}\n-\td := jstream.NewDecoder(readCloser, 0).ObjectAsKVS()\n+\td := jstream.NewDecoder(io.LimitReader(readCloser, maxDocumentSize), 0).ObjectAsKVS()\n \treturn &Reader{\n \t\targs:       args,\n \t\tdecoder:    d,\ndiff --git a/internal/s3select/select.go b/internal/s3select/select.go\nindex 6026c9b3c847c..74b8f43ed472d 100644\n--- a/internal/s3select/select.go\n+++ b/internal/s3select/select.go\n@@ -442,6 +442,7 @@ func (s3Select *S3Select) Open(rsc io.ReadSeekCloser) error {\n \t\t\t\ts3Select.recordReader = json.NewPReader(s3Select.progressReader, &s3Select.Input.JSONArgs)\n \t\t\t}\n \t\t} else {\n+\t\t\t// Document mode.\n \t\t\ts3Select.recordReader = json.NewReader(s3Select.progressReader, &s3Select.Input.JSONArgs)\n \t\t}\n \n", "instance_id": "minio__minio-20439", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue of high memory consumption during an S3 Select operation on a large gzip JSON dataset. It provides detailed context, including expected behavior, current behavior, steps to reproduce, and environmental details. The inclusion of a memory usage diagram and specific commands used for testing adds to the clarity. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not explicitly define what \"high memory consumption\" means in quantitative terms beyond the OOM kill on a 32GiB system, nor does it specify acceptable memory usage thresholds. Additionally, while edge cases like large file sizes are implied, there is no mention of other potential edge cases (e.g., malformed JSON, nested structures) or specific error handling requirements. The \"Possible Solution\" section being empty also leaves the intended resolution direction unclear, though this is somewhat mitigated by the provided code changes.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the issue involves understanding and optimizing memory usage in a complex system like MinIO's S3 Select feature, which requires a deep understanding of the codebase architecture, particularly how data streaming and JSON parsing are handled. The code changes provided are relatively small, confined to two files (`json/reader.go` and `select.go`), and involve limiting the input size for JSON document parsing using `io.LimitReader`. However, the impact of these changes is significant as they address a critical performance issue (memory consumption) that can lead to OOM kills, potentially affecting the system's stability and scalability. \n\nFrom a technical concepts perspective, solving this requires familiarity with Go's `io` package, streaming JSON parsing libraries like `jstream`, and S3 Select's internal mechanics for handling compressed and structured data. Additionally, knowledge of memory management in Go (e.g., garbage collection behavior, as hinted by the prolonged high memory usage post-query) and performance optimization techniques is necessary. The problem also implicitly involves understanding domain-specific concepts related to cloud storage and query processing (e.g., S3 Select's behavior with large datasets, differences between DOCUMENT and other input types).\n\nRegarding edge cases and error handling, while the problem statement does not explicitly mention specific cases beyond large file sizes, the nature of the issue suggests potential complexities in handling malformed or deeply nested JSON, partial reads due to size limits, and ensuring proper error propagation when the input exceeds the defined limit. The code change introduces a hard limit (`maxDocumentSize`), which may require additional logic to handle cases where the limit is exceeded gracefully, though this is not fully addressed in the provided diff.\n\nOverall, this problem requires a solid grasp of multiple technical areas, careful consideration of performance implications, and potentially further modifications beyond the provided diff to ensure robustness. A score of 0.65 reflects the challenge of addressing a performance-critical issue in a complex system, balanced by the relatively focused scope of the code changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Update GoDoc for ioutils (AtomicWriteFile) to document expected behavior\n### Description\n\n- Relates to https://github.com/moby/moby/issues/47498#issuecomment-1978920171\r\n\r\nThe `ioutils` utilities have different semantics than Go stdlib file utilities; unlike stdlib, these functions do not depend on the active umask. We should update their GoDoc to document the behavior, so that consumers are aware of their semantics.\r\n\r\nAlternatively, we can change their behavior to take `umassk` into account, but this would be a breaking change, so not sure if we want to go that direction.\r\n\n", "patch": "diff --git a/pkg/ioutils/fswriters.go b/pkg/ioutils/fswriters.go\nindex 82671d8cd55c5..05da97b0e416f 100644\n--- a/pkg/ioutils/fswriters.go\n+++ b/pkg/ioutils/fswriters.go\n@@ -9,6 +9,7 @@ import (\n // NewAtomicFileWriter returns WriteCloser so that writing to it writes to a\n // temporary file and closing it atomically changes the temporary file to\n // destination path. Writing and closing concurrently is not allowed.\n+// NOTE: umask is not considered for the file's permissions.\n func NewAtomicFileWriter(filename string, perm os.FileMode) (io.WriteCloser, error) {\n \tf, err := os.CreateTemp(filepath.Dir(filename), \".tmp-\"+filepath.Base(filename))\n \tif err != nil {\n@@ -26,7 +27,8 @@ func NewAtomicFileWriter(filename string, perm os.FileMode) (io.WriteCloser, err\n \t}, nil\n }\n \n-// AtomicWriteFile atomically writes data to a file named by filename.\n+// AtomicWriteFile atomically writes data to a file named by filename and with the specified permission bits.\n+// NOTE: umask is not considered for the file's permissions.\n func AtomicWriteFile(filename string, data []byte, perm os.FileMode) error {\n \tf, err := NewAtomicFileWriter(filename, perm)\n \tif err != nil {\n", "instance_id": "moby__moby-47664", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to update the GoDoc for the `ioutils` package, specifically for the `AtomicWriteFile` and related functions, to document that these utilities do not consider the active `umask` for file permissions, unlike the Go standard library. It provides context by referencing a related GitHub issue and mentions an alternative approach (changing behavior to consider `umask`), though it does not pursue this path. However, there are minor ambiguities: the statement does not explicitly define what \"expected behavior\" should be documented beyond the `umask` note, nor does it provide examples of how the documentation should look or address potential edge cases related to file permissions or atomic operations. Additionally, it lacks clarity on whether any specific formatting or style guidelines for GoDoc should be followed. Despite these minor gaps, the goal and scope of the task are reasonably well-defined.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a straightforward update to the documentation (GoDoc comments) for two functions in a single file (`fswriters.go`). The code changes are minimal, consisting of adding a single line of documentation to note that `umask` is not considered for file permissions. There is no need for deep understanding of the codebase architecture, complex logic, or interaction with other modules, as the task is purely about updating comments. The technical concepts involved are basic\u2014understanding GoDoc conventions and the concept of `umask` in file permissions, which are not particularly complex for a developer familiar with Go. No edge cases or error handling modifications are required, as the task does not involve altering the functional code. The scope of impact is negligible, with no risk to the system's behavior or architecture. Overall, this is a very easy task that requires minimal effort and expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`if` condition in composite action misbehaves with `uses`\n### Bug report info\r\n\r\n```plain text\r\nact version:            0.2.65\r\nGOOS:                   darwin\r\nGOARCH:                 arm64\r\nNumCPU:                 10\r\nDocker host:            DOCKER_HOST environment variable is not set\r\nSockets found:\r\n        $HOME/.docker/run/docker.sock\r\nConfig files:           \r\n        /Users/.../Library/Application Support/act/actrc:\r\n                -P ubuntu-latest=catthehacker/ubuntu:act-latest\r\n                -P ubuntu-22.04=catthehacker/ubuntu:act-22.04\r\n                -P ubuntu-20.04=catthehacker/ubuntu:act-20.04\r\n                -P ubuntu-18.04=catthehacker/ubuntu:act-18.04\r\n        .actrc:\r\n                --container-architecture=linux/amd64\r\n                --container-daemon-socket -\r\n                -P arc-rs-dind=catthehacker/ubuntu:act-latest\r\nBuild info:\r\n        Go version:            go1.22.5\r\n        Module path:           command-line-arguments\r\n        Main version:          \r\n        Main path:             \r\n        Main checksum:         \r\n        Build settings:\r\n                -buildmode:           exe\r\n                -compiler:            gc\r\n                -ldflags:             -X main.version=0.2.65\r\n                DefaultGODEBUG:       httplaxcontentlength=1,httpmuxgo121=1,tls10server=1,tlsrsakex=1,tlsunsafeekm=1\r\n                CGO_ENABLED:          1\r\n                CGO_CFLAGS:           \r\n                CGO_CPPFLAGS:         \r\n                CGO_CXXFLAGS:         \r\n                CGO_LDFLAGS:          \r\n                GOARCH:               arm64\r\n                GOOS:                 darwin\r\n\r\nError: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\r\n```\r\n\r\n\r\n### Command used with act\r\n\r\n```sh\r\nact pull_request -v \\\r\n  -e .act/event.json \\\r\n  -W .github/workflows/test-workflow.yml\r\n```\r\n\r\n\r\n### Describe issue\r\n\r\nI'm passing a secret as an input to a composite action. In the composite action I want to run a third party action with `uses` based on whether the input is defined or not.\r\n\r\nWhen I use an `if` condition on a shell step it works correctly, however on the step that `uses` the third party action the expression is suddenly evaluated to `false`.\r\n\r\n### Link to GitHub repository\r\n\r\n_No response_\r\n\r\n### Workflow content\r\n\r\n```yml\r\nname: Test Workflow\r\n\r\non:\r\n  pull_request:\r\n\r\njobs:\r\n  test-job:\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n      - if: ${{ secrets.TEST_SECRET == '' }}\r\n        run: echo \"Secret undefined\"\r\n\r\n      - if: ${{ secrets.TEST_SECRET != '' }}\r\n        run: echo \"Secret defined\"\r\n\r\n      - uses: actions/checkout@v4\r\n\r\n      - uses: ./.github/actions/composite-test\r\n        with:\r\n          test-input: ${{ secrets.TEST_SECRET }}\r\n\r\n\r\n# Composite action\r\n\r\nname: Composite test\r\n\r\ninputs:\r\n  test-input:\r\n    description: 'My test input'\r\n    required: false\r\n    default: ''\r\n\r\nruns:\r\n  using: 'composite'\r\n  steps:\r\n    - name: 'Bash, input undefined'\r\n      if: inputs.test-input == ''\r\n      shell: bash\r\n      run: echo \"test-input defined\"\r\n    - name: 'Bash, input defined'\r\n      if: inputs.test-input != ''\r\n      shell: bash\r\n      run: echo \"test-input defined\"\r\n    - name: 'Action, input defined'\r\n      if: inputs.test-input != ''\r\n      uses: actions/checkout@v4\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```sh\r\n[Test Workflow/test-job] [DEBUG] evaluating expression '${{ ***s.TEST_SECRET == '' }}'\r\n[Test Workflow/test-job] [DEBUG] expression '${{ ***s.TEST_SECRET == '' }}' evaluated to 'false'\r\n[Test Workflow/test-job] [DEBUG] Skipping step 'echo \"Secret undefined\"' due to '${{ ***s.TEST_SECRET == '' }}'\r\n...\r\n[Test Workflow/test-job] [DEBUG] evaluating expression '${{ ***s.TEST_SECRET != '' }}'\r\n[Test Workflow/test-job] [DEBUG] expression '${{ ***s.TEST_SECRET != '' }}' evaluated to 'true'\r\n[Test Workflow/test-job] \u2b50 Run Main echo \"Secret defined\"\r\n...\r\n[Test Workflow/test-job] [DEBUG] Wrote command \r\n\r\necho \"Secret defined\"\r\n\r\n to 'workflow/1'\r\n...\r\n| Secret defined\r\n[Test Workflow/test-job]   \u2705  Success - Main echo \"Secret defined\"\r\n...\r\n[Test Workflow/test-job] \u2b50 Run Main actions/checkout@v4\r\n...\r\n[Test Workflow/test-job]   \u2705  Success - Main actions/checkout@v4\r\n...\r\n[Test Workflow/test-job] [DEBUG] expression '${{ ***s.TEST_SECRET }}' rewritten to 'format('{0}', ***s.TEST_SECRET)'\r\n[Test Workflow/test-job] [DEBUG] evaluating expression 'format('{0}', ***s.TEST_SECRET)'\r\n[Test Workflow/test-job] [DEBUG] expression 'format('{0}', ***s.TEST_SECRET)' evaluated to '%!t(string=***)'\r\n...\r\n[Test Workflow/test-job] \u2b50 Run Main ./.github/actions/composite-test\r\n...\r\n[Test Workflow/test-job] [DEBUG] evaluating expression 'inputs.test-input == '''\r\n[Test Workflow/test-job] [DEBUG] expression 'inputs.test-input == ''' evaluated to 'false'\r\n[Test Workflow/test-job] [DEBUG] Skipping step 'Bash, input undefined' due to 'inputs.test-input == '''\r\n...\r\n[Test Workflow/test-job] [DEBUG] evaluating expression 'inputs.test-input != '''\r\n[Test Workflow/test-job] [DEBUG] expression 'inputs.test-input != ''' evaluated to 'true'\r\n[Test Workflow/test-job] \u2b50 Run Main Bash, input defined\r\n...\r\n[Test Workflow/test-job] [DEBUG] Wrote command \r\n\r\necho \"test-input defined\"\r\n\r\n to 'workflow/3-composite-1.sh'\r\n...\r\n| test-input defined\r\n[Test Workflow/test-job]   \u2705  Success - Main Bash, input defined\r\n...\r\n[Test Workflow/test-job] [DEBUG] evaluating expression 'inputs.test-input != '''\r\n[Test Workflow/test-job] [DEBUG] expression 'inputs.test-input != ''' evaluated to 'false'\r\n[Test Workflow/test-job] [DEBUG] Skipping step 'Action, input defined' due to 'inputs.test-input != '''\r\n[Test Workflow/test-job] [DEBUG] Found revision: 7cc4b33435c8f25415bd4e1644374780f0c50e00\r\n[Test Workflow/test-job]   \u2705  Success - Main ./.github/actions/composite-test\r\n...\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nMy `event.json` only has base and head refs defined, nothing else.\n", "patch": "diff --git a/pkg/runner/expression.go b/pkg/runner/expression.go\nindex cfd5d42f858..90643d09119 100644\n--- a/pkg/runner/expression.go\n+++ b/pkg/runner/expression.go\n@@ -108,6 +108,19 @@ var hashfiles string\n \n // NewStepExpressionEvaluator creates a new evaluator\n func (rc *RunContext) NewStepExpressionEvaluator(ctx context.Context, step step) ExpressionEvaluator {\n+\treturn rc.NewStepExpressionEvaluatorExt(ctx, step, false)\n+}\n+\n+// NewStepExpressionEvaluatorExt creates a new evaluator\n+func (rc *RunContext) NewStepExpressionEvaluatorExt(ctx context.Context, step step, rcInputs bool) ExpressionEvaluator {\n+\tghc := rc.getGithubContext(ctx)\n+\tif rcInputs {\n+\t\treturn rc.newStepExpressionEvaluator(ctx, step, ghc, getEvaluatorInputs(ctx, rc, nil, ghc))\n+\t}\n+\treturn rc.newStepExpressionEvaluator(ctx, step, ghc, getEvaluatorInputs(ctx, rc, step, ghc))\n+}\n+\n+func (rc *RunContext) newStepExpressionEvaluator(ctx context.Context, step step, ghc *model.GithubContext, inputs map[string]interface{}) ExpressionEvaluator {\n \t// todo: cleanup EvaluationEnvironment creation\n \tjob := rc.Run.Job()\n \tstrategy := make(map[string]interface{})\n@@ -127,9 +140,6 @@ func (rc *RunContext) NewStepExpressionEvaluator(ctx context.Context, step step)\n \t\t}\n \t}\n \n-\tghc := rc.getGithubContext(ctx)\n-\tinputs := getEvaluatorInputs(ctx, rc, step, ghc)\n-\n \tee := &exprparser.EvaluationEnvironment{\n \t\tGithub:   step.getGithubContext(ctx),\n \t\tEnv:      *step.getEnv(),\ndiff --git a/pkg/runner/step.go b/pkg/runner/step.go\nindex e1bcf9ecfdc..7b761bac6f2 100644\n--- a/pkg/runner/step.go\n+++ b/pkg/runner/step.go\n@@ -261,7 +261,7 @@ func isStepEnabled(ctx context.Context, expr string, step step, stage stepStage)\n \t\tdefaultStatusCheck = exprparser.DefaultStatusCheckSuccess\n \t}\n \n-\trunStep, err := EvalBool(ctx, rc.NewStepExpressionEvaluator(ctx, step), expr, defaultStatusCheck)\n+\trunStep, err := EvalBool(ctx, rc.NewStepExpressionEvaluatorExt(ctx, step, stage == stepStageMain), expr, defaultStatusCheck)\n \tif err != nil {\n \t\treturn false, fmt.Errorf(\"  \\u274C  Error in if-expression: \\\"if: %s\\\" (%s)\", expr, err)\n \t}\n", "instance_id": "nektos__act-2473", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: there is a discrepancy in how `if` conditions are evaluated in a composite action when using `uses` compared to shell steps. The goal is to fix this misbehavior so that the condition evaluates consistently. The statement includes relevant workflow content, logs, and environment details, which help in understanding the context. However, there are minor ambiguities and missing details. For instance, it does not explicitly state the expected behavior for the `uses` step in the composite action (though it can be inferred), and there are no specific examples of what the correct output should be for all cases. Additionally, edge cases or constraints (e.g., specific GitHub Actions versions or environments where this issue occurs) are not mentioned. Overall, while the problem is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is relatively focused, affecting only a couple of files (`expression.go` and `step.go`), but the modifications are critical to the core functionality of expression evaluation in a GitHub Actions runner implementation (likely `act`). Understanding the interaction between the `RunContext`, expression evaluators, and step execution logic requires a deep dive into the codebase's architecture. Second, the technical concepts involved include advanced Go programming (context handling, interfaces, and struct methods), familiarity with GitHub Actions' expression syntax and evaluation rules, and potentially domain-specific knowledge of how composite actions and secrets are handled. Third, while the provided code changes seem to address the issue by introducing a new evaluator method with conditional input handling, validating this fix requires understanding subtle differences in context and input propagation, which adds complexity. Finally, edge cases and error handling are not explicitly mentioned in the problem statement, but the nature of expression evaluation suggests potential challenges with malformed inputs, secret handling, or cross-platform compatibility, which the developer must consider. Overall, this problem requires a solid grasp of the system's internals and careful modification to avoid introducing new bugs, justifying a difficulty score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Boolean inputs in reusable workflows are not proper booleans\n### Bug report info\r\n\r\n```plain text\r\nact version:            0.2.61\r\nGOOS:                   linux\r\nGOARCH:                 amd64\r\nNumCPU:                 16\r\nDocker host:            DOCKER_HOST environment variable is not set\r\nSockets found:\r\n        /var/run/docker.sock\r\nConfig files:\r\nBuild info:\r\n        Go version:            go1.20.14\r\n        Module path:           github.com/nektos/act\r\n        Main version:          (devel)\r\n        Main path:             github.com/nektos/act\r\n        Main checksum:\r\n        Build settings:\r\n                -buildmode:           exe\r\n                -compiler:            gc\r\n                -trimpath:            true\r\n                CGO_ENABLED:          0\r\n                GOARCH:               amd64\r\n                GOOS:                 linux\r\n                GOAMD64:              v1\r\n                vcs:                  git\r\n                vcs.revision:         e8856f0fb00fcdd16eef2325b845f55f5d346f51\r\n                vcs.time:             2023-08-21T16:17:06Z\r\n                vcs.modified:         true\r\nDocker Engine:\r\n        Engine version:        25.0.3\r\n        Engine runtime:        runc\r\n        Cgroup version:        1\r\n        Cgroup driver:         cgroupfs\r\n        Storage driver:        overlay2\r\n        Registry URI:          https://index.docker.io/v1/\r\n        OS:                    Docker Desktop\r\n        OS type:               linux\r\n        OS version:\r\n        OS arch:               x86_64\r\n        OS kernel:             5.15.146.1-microsoft-standard-WSL2\r\n        OS CPU:                16\r\n        OS memory:             32100 MB\r\n        Security options:\r\n                name=seccomp,profile=unconfined\r\n```\r\n\r\n\r\n### Command used with act\r\n\r\n```sh\r\nact -W .github/workflows/act-test-caller.yml\r\n```\r\n\r\n\r\n### Describe issue\r\n\r\nIn GitHub Actions, `boolean` type inputs inside a `workflow_call` are real booleans, and you can use them as such. When a workflow is called with those in Act, they are converted to strings when they shouldn't be.\r\n\r\nThe output from my example below will be `This should not run` because `false` gets cast to `'false'` which actually ends up being truthy.\r\n\r\n### Link to GitHub repository\r\n\r\n_No response_\r\n\r\n### Workflow content\r\n\r\n```yml\r\n# Calling workflow:\r\n---\r\non:\r\n  workflow_dispatch:\r\n\r\njobs:\r\n  test:\r\n    uses: ./.github/workflows/act-test-called.yml\r\n\r\n# Reusable workflow:\r\n---\r\non:\r\n  workflow_call:\r\n    inputs:\r\n      test:\r\n        type: boolean\r\n        default: false\r\n\r\njobs:\r\n  test:\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n      - if: ${{ !inputs.test }}\r\n        run: |\r\n          echo \"This should run\"\r\n      \r\n      - if: ${{ inputs.test }}\r\n        run: |\r\n          echo \"This should not run\"\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```sh\r\n[test/act-test-called.yml/test] \u2b50 Run Main echo \"This should not run\"\r\n[test/act-test-called.yml/test]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/1] user= workdir=\r\n| This should not run\r\n[test/act-test-called.yml/test]   \u2705  Success - Main echo \"This should not run\"\r\n\r\n# Relevant line from debug output\r\n[test/act-test-called.yml/test] [DEBUG] evaluating expression '${{ !inputs.test }}'\r\n[test/act-test-called.yml/test] [DEBUG] expression '${{ !inputs.test }}' evaluated to 'false'\r\n[test/act-test-called.yml/test] [DEBUG] Skipping step 'echo \"This should run\"\r\n' due to '${{ !inputs.test }}'\r\n...\r\n[test/act-test-called.yml/test] [DEBUG] evaluating expression '${{ inputs.test }}'\r\n[test/act-test-called.yml/test] [DEBUG] expression '${{ inputs.test }}' evaluated to '%!t(string=false)'\r\n[test/act-test-called.yml/test] \u2b50 Run Main echo \"This should not run\"\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "patch": "diff --git a/pkg/model/workflow.go b/pkg/model/workflow.go\nindex 7fbecd6c537..b8a39f19143 100644\n--- a/pkg/model/workflow.go\n+++ b/pkg/model/workflow.go\n@@ -116,10 +116,10 @@ func (w *Workflow) WorkflowDispatchConfig() *WorkflowDispatch {\n }\n \n type WorkflowCallInput struct {\n-\tDescription string `yaml:\"description\"`\n-\tRequired    bool   `yaml:\"required\"`\n-\tDefault     string `yaml:\"default\"`\n-\tType        string `yaml:\"type\"`\n+\tDescription string    `yaml:\"description\"`\n+\tRequired    bool      `yaml:\"required\"`\n+\tDefault     yaml.Node `yaml:\"default\"`\n+\tType        string    `yaml:\"type\"`\n }\n \n type WorkflowCallOutput struct {\ndiff --git a/pkg/runner/expression.go b/pkg/runner/expression.go\nindex 2e3152b0993..dca35380d60 100644\n--- a/pkg/runner/expression.go\n+++ b/pkg/runner/expression.go\n@@ -512,7 +512,7 @@ func getEvaluatorInputs(ctx context.Context, rc *RunContext, step step, ghc *mod\n \t\t\tfor k, v := range config.Inputs {\n \t\t\t\tvalue := nestedMapLookup(ghc.Event, \"inputs\", k)\n \t\t\t\tif value == nil {\n-\t\t\t\t\tvalue = v.Default\n+\t\t\t\t\tv.Default.Decode(&value)\n \t\t\t\t}\n \t\t\t\tif v.Type == \"boolean\" {\n \t\t\t\t\tinputs[k] = value == \"true\"\n@@ -531,21 +531,24 @@ func setupWorkflowInputs(ctx context.Context, inputs *map[string]interface{}, rc\n \n \t\tfor name, input := range config.Inputs {\n \t\t\tvalue := rc.caller.runContext.Run.Job().With[name]\n+\n \t\t\tif value != nil {\n-\t\t\t\tif str, ok := value.(string); ok {\n+\t\t\t\tnode := yaml.Node{}\n+\t\t\t\t_ = node.Encode(value)\n+\t\t\t\tif rc.caller.runContext.ExprEval != nil {\n \t\t\t\t\t// evaluate using the calling RunContext (outside)\n-\t\t\t\t\tvalue = rc.caller.runContext.ExprEval.Interpolate(ctx, str)\n+\t\t\t\t\t_ = rc.caller.runContext.ExprEval.EvaluateYamlNode(ctx, &node)\n \t\t\t\t}\n+\t\t\t\t_ = node.Decode(&value)\n \t\t\t}\n \n \t\t\tif value == nil && config != nil && config.Inputs != nil {\n-\t\t\t\tvalue = input.Default\n+\t\t\t\tdef := input.Default\n \t\t\t\tif rc.ExprEval != nil {\n-\t\t\t\t\tif str, ok := value.(string); ok {\n-\t\t\t\t\t\t// evaluate using the called RunContext (inside)\n-\t\t\t\t\t\tvalue = rc.ExprEval.Interpolate(ctx, str)\n-\t\t\t\t\t}\n+\t\t\t\t\t// evaluate using the called RunContext (inside)\n+\t\t\t\t\t_ = rc.ExprEval.EvaluateYamlNode(ctx, &def)\n \t\t\t\t}\n+\t\t\t\t_ = def.Decode(&value)\n \t\t\t}\n \n \t\t\t(*inputs)[name] = value\n", "instance_id": "nektos__act-2349", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the issue. It explains the discrepancy between how boolean inputs are handled in GitHub Actions versus the Act tool, with a specific example in the workflow content and relevant log output to demonstrate the bug. The goal is evident: boolean inputs in reusable workflows are being incorrectly converted to strings, leading to incorrect conditional evaluations. However, there are minor ambiguities, such as the lack of explicit mention of expected behavior for edge cases (e.g., what should happen if the input is not provided or is malformed) and no detailed constraints or requirements for the fix beyond correcting the boolean handling. Additionally, while the logs and workflow examples are helpful, there is no explicit discussion of the broader impact on other parts of the system or potential side effects of the fix. Thus, while the problem is valid and mostly clear, it misses some minor details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily affecting two files (`workflow.go` and `expression.go`) and involving modifications to how input values are decoded and evaluated. However, these changes require a solid understanding of the YAML parsing and expression evaluation logic within the Act tool, which involves interacting with the `yaml.Node` type and custom evaluation mechanisms. The technical concepts involved include familiarity with Go's YAML library, type handling, and context-based evaluation of expressions, which are moderately complex for someone unfamiliar with the codebase or Go's ecosystem. The changes also impact how inputs are processed across workflow calls, which could have subtle downstream effects, requiring careful consideration of compatibility with existing workflows. While the problem statement does not explicitly mention edge cases, the code changes suggest the need to handle default values and type conversions correctly, which introduces some complexity in error handling and testing. Overall, solving this requires understanding multiple concepts and making targeted but non-trivial modifications across a couple of files, placing it in the medium difficulty range of 0.55. It is not overly hard as it does not involve deep architectural changes or advanced domain-specific knowledge, but it is beyond a simple bug fix due to the need for precise handling of data types and evaluation logic.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Copying the commit author adds single ticks in wrong places\n**Describe the bug**\r\nWhen copying author name (and e-mail), Lazygit adds single ticks around the coppied string; however, last ticks lands just before the closing angle bracket.\r\n\r\nSo instead of copying `'Eduard <korchmar.eduard+git@gmail.com>'`, it would end up with `'Eduard <korchmar.eduard+git@gmail.com'>`.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to Commits panel in any repo with commits\r\n2. Press `y` (copy commit attribute) and `a` (copy commit author)\r\n3. Go into any text editor, shell or editable text field inside Lazygit\r\n4. Paste the clipboard content and see the malformed author string.\r\n\r\n**Expected behavior**\r\nAdded ticks surround the entire copied string.\r\n\r\n**Screenshots**\r\n![image](https://github.com/user-attachments/assets/6b72b970-31fe-4e20-9ac4-42347f34a549)\r\n\r\n**Version info:**\r\n_Run `lazygit --version` and paste the result here_\r\ncommit=v0.44.1, build date=2024-09-18T13:19:48Z, build source=binaryRelease, version=0.44.1, os=linux, arch=amd64, git version=2.46.1\r\n_Run `git --version` and paste the result here_\r\ngit version 2.46.1\r\n\r\nThe bug is also observable in manually built binary from the current latest commit.\r\n\r\n**Additional context**\r\nI do not understand why ticks are being added at all, but I assume for portability? I will assume this is intended and useful.\r\n\r\n<!--\r\nIf you want to try and debug this issue yourself, you can run `lazygit --debug` in one terminal panel and `lazygit --logs` in another to view the logs.\r\n-->\r\n\n", "patch": "diff --git a/pkg/commands/git_commands/commit.go b/pkg/commands/git_commands/commit.go\nindex 45582c6bd5f..52a65fb6dbd 100644\n--- a/pkg/commands/git_commands/commit.go\n+++ b/pkg/commands/git_commands/commit.go\n@@ -189,7 +189,7 @@ type Author struct {\n \n func (self *CommitCommands) GetCommitAuthor(commitHash string) (Author, error) {\n \tcmdArgs := NewGitCmd(\"show\").\n-\t\tArg(\"--no-patch\", \"--pretty=format:'%an%x00%ae'\", commitHash).\n+\t\tArg(\"--no-patch\", \"--pretty=format:%an%x00%ae\", commitHash).\n \t\tToArgv()\n \n \toutput, err := self.cmd.New(cmdArgs).DontLog().RunWithOutput()\n", "instance_id": "jesseduffield__lazygit-3936", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the bug, including steps to reproduce, expected behavior, and a screenshot for visual reference. The issue with the placement of single ticks in the copied commit author string is well-articulated, and the context of the bug (e.g., version info, platform) is provided. However, there are minor ambiguities: the problem statement does not explicitly discuss why the ticks are added or whether their presence is a requirement (though the reporter assumes it is for portability). Additionally, there is no mention of potential edge cases, such as how the formatting should handle special characters in author names or emails, or if there are other related formatting issues. These missing details prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). The issue is a simple formatting bug in the commit author string where single ticks are misplaced due to their inclusion in the format string of a Git command. The code change provided is minimal, involving a single line modification in one file (`commit.go`) to remove the ticks from the `--pretty=format` argument. This change does not require deep understanding of the codebase, complex logic, or interaction with multiple modules\u2014it is a straightforward fix to the output format of a command. No advanced technical concepts, algorithms, or design patterns are involved; it only requires basic knowledge of string formatting in Git commands. There are no explicit edge cases or error handling requirements mentioned in the problem statement or evident in the code change, further reducing the complexity. The impact on the system's architecture is negligible, as this is a cosmetic fix. Therefore, I assign a difficulty score of 0.15, reflecting a very easy task that requires minimal effort and expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "externalDiffCommand doesn't work when diffing between 2 refs\n**Is your feature request related to a problem? Please describe.**\r\n`externalDiffCommand` currently works wonder when viewing the diff of each commits, but it doesn't work when we're diffing between 2 refs (with `W` key binding). Instead Lazygit used the default diff view\r\n\r\n**Describe the solution you'd like**\r\n`externalDiffCommand` should also applies to diff view between to refs.\r\n\r\n**Describe alternatives you've considered**\r\nThe default diff view is good enough, but I prefer to use difftastic to view the diff.\r\n\r\n**Additional context**\r\n\r\nNormal diff:\r\n<img width=\"1897\" alt=\"image\" src=\"https://github.com/jesseduffield/lazygit/assets/78085736/110826d9-d985-49f0-b036-5cdade2ccb2a\">\r\n\r\nDiff between branches:\r\n<img width=\"1888\" alt=\"image\" src=\"https://github.com/jesseduffield/lazygit/assets/78085736/81c9041c-1d37-4fe1-9d4c-c268d2dd65ba\">\r\n\r\nI could help with implementing this, but I don't know where to start. Any help would be appreciated.\r\n\r\n<!--\r\nYou may be able to add your desired feature with a custom command. Check out the examples here: https://github.com/jesseduffield/lazygit/wiki/Custom-Commands-Compendium\r\n\r\nIf a custom command does what you want but you still want to see the feature built-in to lazygit, feel free to paste the custom command into the issue to help us better understand the functionality you want.\r\n\r\nWe also encourage you to put up a PR yourself! Who cares if you've never written Go before, neither did any of the existing contributors before their first lazygit PR! Check out the PR tutorial here: https://www.youtube.com/watch?v=kNavnhzZHtk&ab_channel=JesseDuffield\r\n\r\nAlso check out the contributing guide here: https://github.com/jesseduffield/lazygit/blob/master/CONTRIBUTING.md\r\n-->\r\n\n", "patch": "diff --git a/pkg/commands/git_commands/diff.go b/pkg/commands/git_commands/diff.go\nindex 9f81bb91d53..97927991427 100644\n--- a/pkg/commands/git_commands/diff.go\n+++ b/pkg/commands/git_commands/diff.go\n@@ -1,6 +1,10 @@\n package git_commands\n \n-import \"github.com/jesseduffield/lazygit/pkg/commands/oscommands\"\n+import (\n+\t\"fmt\"\n+\n+\t\"github.com/jesseduffield/lazygit/pkg/commands/oscommands\"\n+)\n \n type DiffCommands struct {\n \t*GitCommon\n@@ -13,10 +17,16 @@ func NewDiffCommands(gitCommon *GitCommon) *DiffCommands {\n }\n \n func (self *DiffCommands) DiffCmdObj(diffArgs []string) oscommands.ICmdObj {\n+\textDiffCmd := self.UserConfig.Git.Paging.ExternalDiffCommand\n+\tuseExtDiff := extDiffCmd != \"\"\n+\n \treturn self.cmd.New(\n \t\tNewGitCmd(\"diff\").\n \t\t\tConfig(\"diff.noprefix=false\").\n-\t\t\tArg(\"--submodule\", \"--no-ext-diff\", \"--color\").\n+\t\t\tConfigIf(useExtDiff, \"diff.external=\"+extDiffCmd).\n+\t\t\tArgIfElse(useExtDiff, \"--ext-diff\", \"--no-ext-diff\").\n+\t\t\tArg(\"--submodule\").\n+\t\t\tArg(fmt.Sprintf(\"--color=%s\", self.UserConfig.Git.Paging.ColorArg)).\n \t\t\tArg(diffArgs...).\n \t\t\tDir(self.repoPaths.worktreePath).\n \t\t\tToArgv(),\n", "instance_id": "jesseduffield__lazygit-3519", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `externalDiffCommand` feature in Lazygit works for viewing diffs of individual commits but fails to apply when diffing between two refs (e.g., branches). The goal is to extend the functionality of `externalDiffCommand` to work in this context as well. The statement includes visual examples (screenshots) to illustrate the difference in behavior, which adds clarity. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define the expected behavior when using `externalDiffCommand` for ref diffs (e.g., should it behave identically to commit diffs, or are there specific nuances?). Additionally, edge cases or potential constraints (e.g., compatibility with different external diff tools or performance implications) are not mentioned. While the intent is understandable, these gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are localized to a single file (`diff.go`) and specifically to the `DiffCmdObj` function. The modification involves conditionally applying the `externalDiffCommand` configuration based on user settings, which is a relatively small and focused change (adding a few lines of logic and arguments to an existing command builder). There is no indication of broader architectural impact or the need to modify multiple modules.\n\n2. **Technical Concepts Involved**: Solving this requires basic familiarity with Go (e.g., string manipulation, conditional logic) and an understanding of how Git commands are constructed and executed in Lazygit. The change involves interacting with Git configuration (`diff.external`) and command-line arguments (`--ext-diff` vs `--no-ext-diff`), which are straightforward for someone with moderate experience in Git or command-line tool development. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, such as invalid or unsupported `externalDiffCommand` configurations, compatibility issues with specific diff tools, or performance concerns when diffing large refs. The code change itself does not introduce new error handling logic, though a more robust solution might need to consider these aspects. At the current scope, this does not significantly increase difficulty.\n\n4. **Overall Complexity**: The task requires understanding a small part of the codebase (how diff commands are built) and making a simple modification to extend existing behavior. It does not demand deep knowledge of Lazygit's architecture or complex refactoring. The provided diff already demonstrates a working solution, further reducing the perceived difficulty for implementation.\n\nA score of 0.30 reflects that this is an easy task, slightly above the \"very easy\" range due to the need to understand Git command configuration and ensure the change integrates correctly with existing functionality. It is suitable for developers with basic to intermediate experience in Go and Git tools.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "misleading usage help for rclone serve restic with TLS\n<!--\r\n\r\nWe understand you are having a problem with rclone; we want to help you with that!\r\n\r\n**STOP and READ**\r\n**YOUR POST WILL BE REMOVED IF IT IS LOW QUALITY**:\r\nPlease show the effort you've put into solving the problem and please be specific.\r\nPeople are volunteering their time to help! Low effort posts are not likely to get good answers!\r\n\r\nIf you think you might have found a bug, try to replicate it with the latest beta (or stable).\r\nThe update instructions are available at https://rclone.org/commands/rclone_selfupdate/\r\n\r\nIf you can still replicate it or just got a question then please use the rclone forum:\r\n\r\n    https://forum.rclone.org/\r\n\r\nfor a quick response instead of filing an issue on this repo.\r\n\r\nIf nothing else helps, then please fill in the info below which helps us help you.\r\n\r\n**DO NOT REDACT** any information except passwords/keys/personal info.\r\n\r\nYou should use 3 backticks to begin and end your paste to make it readable.\r\n\r\nMake sure to include a log obtained with '-vv'.\r\n\r\nYou can also use '-vv --log-file bug.log' and a service such as https://pastebin.com or https://gist.github.com/\r\n\r\nThank you\r\n\r\nThe Rclone Developers\r\n\r\n-->\r\n#### What is the problem you are having with rclone?\r\n\r\nI am using rclone (only) as a restic server backend, with TLS.\r\n\"rclone serve restic -h\" says:\r\n\r\n```\r\n`--cert` should be either a PEM encoded certificate or a concatenation\r\nof that with the CA certificate.  `--key` should be the PEM encoded\r\nprivate key and `--client-ca` should be the PEM encoded client \r\ncertificate authority certificate.\r\n(...)\r\n --cert string                     SSL PEM key (concatenation of certificate and CA certificate)\r\n --client-ca string                Client certificate authority to verify clients with\r\n --key string                      SSL PEM Private key\r\n```\r\n\r\nThis is misleading, because the argument of these options must be a filename and not a PEM string!\r\n It should read:\r\n\r\n```\r\n`--cert` should be either a file containing a PEM encoded certificate or a concatenation\r\nof that with the CA certificate.  `--key` should be a file containing the PEM encoded\r\nprivate key and `--client-ca` should be a file containing the PEM encoded client \r\ncertificate authority certificate.\r\n(...)\r\n --cert file                     SSL PEM key (concatenation of certificate and CA certificate)\r\n --client-ca file                Client certificate authority to verify clients with\r\n --key file                      SSL PEM Private key\r\n```\r\n\r\n\r\n#### What is your rclone version (output from `rclone version`)\r\n\r\nrclone v1.60.1-DEV\r\n- os/version: ubuntu 24.04 (64 bit)\r\n- os/kernel: 6.8.0-49-generic (x86_64)\r\n- os/type: linux\r\n- os/arch: amd64\r\n- go/version: go1.22.2\r\n- go/linking: dynamic\r\n- go/tags: none\r\n\r\n#### Which OS you are using and how many bits (e.g. Windows 7, 64 bit)\r\n\r\nUbuntu 24.04 (64 bit)\r\n\r\n\r\n#### Which cloud storage system are you using? (e.g. Google Drive)\r\n\r\nlocal filesystem\r\n\r\n#### The command you were trying to run (e.g. `rclone copy /tmp remote:tmp`)\r\n\r\nrclone serve restic --cert ...\r\n\r\n#### A log from the command with the `-vv` flag (e.g. output from `rclone -vv copy /tmp remote:tmp`)\r\n\r\n\r\n\r\n<!--- Please keep the note below for others who read your bug report. -->\r\n\r\n#### How to use GitHub\r\n\r\n* Please use the \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.\r\n* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.\r\n* Subscribe to receive notifications on status change and new comments.\r\n\nmisleading usage help for rclone serve restic with TLS\n<!--\r\n\r\nWe understand you are having a problem with rclone; we want to help you with that!\r\n\r\n**STOP and READ**\r\n**YOUR POST WILL BE REMOVED IF IT IS LOW QUALITY**:\r\nPlease show the effort you've put into solving the problem and please be specific.\r\nPeople are volunteering their time to help! Low effort posts are not likely to get good answers!\r\n\r\nIf you think you might have found a bug, try to replicate it with the latest beta (or stable).\r\nThe update instructions are available at https://rclone.org/commands/rclone_selfupdate/\r\n\r\nIf you can still replicate it or just got a question then please use the rclone forum:\r\n\r\n    https://forum.rclone.org/\r\n\r\nfor a quick response instead of filing an issue on this repo.\r\n\r\nIf nothing else helps, then please fill in the info below which helps us help you.\r\n\r\n**DO NOT REDACT** any information except passwords/keys/personal info.\r\n\r\nYou should use 3 backticks to begin and end your paste to make it readable.\r\n\r\nMake sure to include a log obtained with '-vv'.\r\n\r\nYou can also use '-vv --log-file bug.log' and a service such as https://pastebin.com or https://gist.github.com/\r\n\r\nThank you\r\n\r\nThe Rclone Developers\r\n\r\n-->\r\n#### What is the problem you are having with rclone?\r\n\r\nI am using rclone (only) as a restic server backend, with TLS.\r\n\"rclone serve restic -h\" says:\r\n\r\n```\r\n`--cert` should be either a PEM encoded certificate or a concatenation\r\nof that with the CA certificate.  `--key` should be the PEM encoded\r\nprivate key and `--client-ca` should be the PEM encoded client \r\ncertificate authority certificate.\r\n(...)\r\n --cert string                     SSL PEM key (concatenation of certificate and CA certificate)\r\n --client-ca string                Client certificate authority to verify clients with\r\n --key string                      SSL PEM Private key\r\n```\r\n\r\nThis is misleading, because the argument of these options must be a filename and not a PEM string!\r\n It should read:\r\n\r\n```\r\n`--cert` should be either a file containing a PEM encoded certificate or a concatenation\r\nof that with the CA certificate.  `--key` should be a file containing the PEM encoded\r\nprivate key and `--client-ca` should be a file containing the PEM encoded client \r\ncertificate authority certificate.\r\n(...)\r\n --cert file                     SSL PEM key (concatenation of certificate and CA certificate)\r\n --client-ca file                Client certificate authority to verify clients with\r\n --key file                      SSL PEM Private key\r\n```\r\n\r\n\r\n#### What is your rclone version (output from `rclone version`)\r\n\r\nrclone v1.60.1-DEV\r\n- os/version: ubuntu 24.04 (64 bit)\r\n- os/kernel: 6.8.0-49-generic (x86_64)\r\n- os/type: linux\r\n- os/arch: amd64\r\n- go/version: go1.22.2\r\n- go/linking: dynamic\r\n- go/tags: none\r\n\r\n#### Which OS you are using and how many bits (e.g. Windows 7, 64 bit)\r\n\r\nUbuntu 24.04 (64 bit)\r\n\r\n\r\n#### Which cloud storage system are you using? (e.g. Google Drive)\r\n\r\nlocal filesystem\r\n\r\n#### The command you were trying to run (e.g. `rclone copy /tmp remote:tmp`)\r\n\r\nrclone serve restic --cert ...\r\n\r\n#### A log from the command with the `-vv` flag (e.g. output from `rclone -vv copy /tmp remote:tmp`)\r\n\r\n\r\n\r\n<!--- Please keep the note below for others who read your bug report. -->\r\n\r\n#### How to use GitHub\r\n\r\n* Please use the \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.\r\n* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.\r\n* Subscribe to receive notifications on status change and new comments.\r\n\n", "patch": "diff --git a/docs/content/rc.md b/docs/content/rc.md\nindex 7a8ef34e4fe00..cc3b8329f72bc 100644\n--- a/docs/content/rc.md\n+++ b/docs/content/rc.md\n@@ -18,29 +18,31 @@ If you just want to run a remote control then see the [rcd](/commands/rclone_rcd\n \n ### --rc\n \n-Flag to start the http server listen on remote requests\n+Flag to start the http server listen on remote requests.\n       \n ### --rc-addr=IP\n \n-IPaddress:Port or :Port to bind server to. (default \"localhost:5572\")\n+IPaddress:Port or :Port to bind server to. (default \"localhost:5572\").\n \n ### --rc-cert=KEY\n-SSL PEM key (concatenation of certificate and CA certificate)\n+\n+SSL PEM key (concatenation of certificate and CA certificate).\n \n ### --rc-client-ca=PATH\n-Client certificate authority to verify clients with\n+\n+Client certificate authority to verify clients with.\n \n ### --rc-htpasswd=PATH\n \n-htpasswd file - if not provided no authentication is done\n+htpasswd file - if not provided no authentication is done.\n \n ### --rc-key=PATH\n \n-SSL PEM Private key\n+TLS PEM private key file.\n \n ### --rc-max-header-bytes=VALUE\n \n-Maximum size of request header (default 4096)\n+Maximum size of request header (default 4096).\n \n ### --rc-min-tls-version=VALUE\n \n@@ -57,15 +59,15 @@ Password for authentication.\n \n ### --rc-realm=VALUE\n \n-Realm for authentication (default \"rclone\")\n+Realm for authentication (default \"rclone\").\n \n ### --rc-server-read-timeout=DURATION\n \n-Timeout for server reading data (default 1h0m0s)\n+Timeout for server reading data (default 1h0m0s).\n \n ### --rc-server-write-timeout=DURATION\n \n-Timeout for server writing data (default 1h0m0s)\n+Timeout for server writing data (default 1h0m0s).\n \n ### --rc-serve\n \ndiff --git a/lib/http/server.go b/lib/http/server.go\nindex 9964c297e39c6..ce13b82b66557 100644\n--- a/lib/http/server.go\n+++ b/lib/http/server.go\n@@ -66,20 +66,21 @@ https.  You will need to supply the ` + \"`--{{ .Prefix }}cert` and `--{{ .Prefix\n If you wish to do client side certificate validation then you will need to\n supply ` + \"`--{{ .Prefix }}client-ca`\" + ` also.\n \n-` + \"`--{{ .Prefix }}cert`\" + ` should be a either a PEM encoded certificate or a concatenation\n-of that with the CA certificate.  ` + \"`--k{{ .Prefix }}ey`\" + ` should be the PEM encoded\n-private key and ` + \"`--{{ .Prefix }}client-ca`\" + ` should be the PEM encoded client\n-certificate authority certificate.\n+` + \"`--{{ .Prefix }}cert`\" + ` must be set to the path of a file containing\n+either a PEM encoded certificate, or a concatenation of that with the CA\n+certificate. ` + \"`--{{ .Prefix }}key`\" + ` must be set to the path of a file\n+with the PEM encoded private key. ` + \"If setting `--{{ .Prefix }}client-ca`\" + `,\n+it should be set to the path of a file with PEM encoded client certificate\n+authority certificates.\n \n ` + \"`--{{ .Prefix }}min-tls-version`\" + ` is minimum TLS version that is acceptable. Valid\n-  values are \"tls1.0\", \"tls1.1\", \"tls1.2\" and \"tls1.3\" (default\n-  \"tls1.0\").\n+values are \"tls1.0\", \"tls1.1\", \"tls1.2\" and \"tls1.3\" (default \"tls1.0\").\n \n ### Socket activation\n \n Instead of the listening addresses specified above, rclone will listen to all\n-FDs passed by the service manager, if any (and ignore any arguments passed by ` +\n-\t\t\"--{{ .Prefix }}addr`\" + `).\n+FDs passed by the service manager, if any (and ignore any arguments passed\n+by ` + \"`--{{ .Prefix }}addr`\" + `).\n \n This allows rclone to be a socket-activated service.\n It can be configured with .socket and .service unit files as described in\n@@ -162,11 +163,11 @@ type Config struct {\n \tServerReadTimeout  time.Duration `config:\"server_read_timeout\"`  // Timeout for server reading data\n \tServerWriteTimeout time.Duration `config:\"server_write_timeout\"` // Timeout for server writing data\n \tMaxHeaderBytes     int           `config:\"max_header_bytes\"`     // Maximum size of request header\n-\tTLSCert            string        `config:\"cert\"`                 // Path to TLS PEM key (concatenation of certificate and CA certificate)\n-\tTLSKey             string        `config:\"key\"`                  // Path to TLS PEM Private key\n-\tTLSCertBody        []byte        `config:\"-\"`                    // TLS PEM key (concatenation of certificate and CA certificate) body, ignores TLSCert\n-\tTLSKeyBody         []byte        `config:\"-\"`                    // TLS PEM Private key body, ignores TLSKey\n-\tClientCA           string        `config:\"client_ca\"`            // Client certificate authority to verify clients with\n+\tTLSCert            string        `config:\"cert\"`                 // Path to TLS PEM public key certificate file (can also include intermediate/CA certificates)\n+\tTLSKey             string        `config:\"key\"`                  // Path to TLS PEM private key file\n+\tTLSCertBody        []byte        `config:\"-\"`                    // TLS PEM public key certificate body (can also include intermediate/CA certificates), ignores TLSCert\n+\tTLSKeyBody         []byte        `config:\"-\"`                    // TLS PEM private key body, ignores TLSKey\n+\tClientCA           string        `config:\"client_ca\"`            // Path to TLS PEM CA file with certificate authorities to verify clients with\n \tMinTLSVersion      string        `config:\"min_tls_version\"`      // MinTLSVersion contains the minimum TLS version that is acceptable.\n \tAllowOrigin        string        `config:\"allow_origin\"`         // AllowOrigin sets the Access-Control-Allow-Origin header\n }\n@@ -177,9 +178,9 @@ func (cfg *Config) AddFlagsPrefix(flagSet *pflag.FlagSet, prefix string) {\n \tflags.DurationVarP(flagSet, &cfg.ServerReadTimeout, prefix+\"server-read-timeout\", \"\", cfg.ServerReadTimeout, \"Timeout for server reading data\", prefix)\n \tflags.DurationVarP(flagSet, &cfg.ServerWriteTimeout, prefix+\"server-write-timeout\", \"\", cfg.ServerWriteTimeout, \"Timeout for server writing data\", prefix)\n \tflags.IntVarP(flagSet, &cfg.MaxHeaderBytes, prefix+\"max-header-bytes\", \"\", cfg.MaxHeaderBytes, \"Maximum size of request header\", prefix)\n-\tflags.StringVarP(flagSet, &cfg.TLSCert, prefix+\"cert\", \"\", cfg.TLSCert, \"TLS PEM key (concatenation of certificate and CA certificate)\", prefix)\n-\tflags.StringVarP(flagSet, &cfg.TLSKey, prefix+\"key\", \"\", cfg.TLSKey, \"TLS PEM Private key\", prefix)\n-\tflags.StringVarP(flagSet, &cfg.ClientCA, prefix+\"client-ca\", \"\", cfg.ClientCA, \"Client certificate authority to verify clients with\", prefix)\n+\tflags.StringVarP(flagSet, &cfg.TLSCert, prefix+\"cert\", \"\", cfg.TLSCert, \"Path to TLS PEM public key certificate file (can also include intermediate/CA certificates)\", prefix)\n+\tflags.StringVarP(flagSet, &cfg.TLSKey, prefix+\"key\", \"\", cfg.TLSKey, \"Path to TLS PEM private key file\", prefix)\n+\tflags.StringVarP(flagSet, &cfg.ClientCA, prefix+\"client-ca\", \"\", cfg.ClientCA, \"Path to TLS PEM CA file with certificate authorities to verify clients with\", prefix)\n \tflags.StringVarP(flagSet, &cfg.BaseURL, prefix+\"baseurl\", \"\", cfg.BaseURL, \"Prefix for URLs - leave blank for root\", prefix)\n \tflags.StringVarP(flagSet, &cfg.MinTLSVersion, prefix+\"min-tls-version\", \"\", cfg.MinTLSVersion, \"Minimum TLS version that is acceptable\", prefix)\n \tflags.StringVarP(flagSet, &cfg.AllowOrigin, prefix+\"allow-origin\", \"\", cfg.AllowOrigin, \"Origin which cross-domain request (CORS) can be executed from\", prefix)\n", "instance_id": "rclone__rclone-8235", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the help text and documentation for the `rclone serve restic` command incorrectly suggest that the `--cert`, `--key`, and `--client-ca` options accept PEM strings directly, when in fact they expect file paths containing PEM-encoded data. The user provides a detailed description of the misleading text and proposes corrected wording, which adds to the clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly confirm whether this issue affects functionality or is purely a documentation error (though the code changes suggest the latter). Additionally, there are no examples of incorrect usage or potential user impact, which could have made the statement more comprehensive. Overall, the goal is clear, but minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range (Very Easy). The issue is purely related to documentation and help text, requiring only textual updates to clarify that the arguments for `--cert`, `--key`, and `--client-ca` are file paths rather than direct PEM strings. The code changes are minimal, involving straightforward modifications to strings in help messages and comments across a few files (`rc.md` and `server.go`). These changes do not impact the system's functionality or architecture, nor do they require deep understanding of the codebase, complex logic, or handling of edge cases. The technical concepts involved are basic\u2014understanding string literals and documentation formatting in Go. There is no need for advanced language features, algorithms, or domain-specific knowledge beyond familiarity with command-line tool documentation. The scope of changes is limited to a small number of lines in two files, with no interaction between modules or risk of introducing bugs. Therefore, a difficulty score of 0.15 is appropriate, reflecting a very easy task that requires minimal effort and expertise.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "DLNA Browse+BrowseMetadata not to spec, angers pedantic client.\n\r\n#### The associated forum post URL from `https://forum.rclone.org`\r\n\r\nIrrelevant.\r\n\r\n#### What is the problem you are having with rclone?\r\n\r\nRclone DLNA server doesn't conform to [spec](https://upnp.org/specs/av/UPnP-av-ContentDirectory-v1-Service.pdf) when replying to a Browse request. Section 2.7.4.2. _Argument Descriptions_ says it should include `NumberReturned` and `TotalMatches`.\r\n\r\n[Current rclone code](https://github.com/rclone/rclone/blob/3f3e4b055e322ed6b8c8686cc20948244d884c21/cmd/serve/dlna/cds.go#L291) only returns `Result`.\r\n\r\nThis was discovered using the [DLNA Digital Media Integration](https://www.home-assistant.io/integrations/dlna_dms) in Home Assistant.\r\n\r\nThis results in a failure to browse and inability to go further. Granted, the client could be more tolerant. But the fix seems trivial and would probably benefit everyone.\r\n\r\n![image](https://github.com/rclone/rclone/assets/28158/cff1fe0a-4cca-4c45-be9c-2ded3442d5b3)\r\n\r\n#### What is your rclone version (output from `rclone version`)\r\n\r\nrclone v1.66.0\r\n- os/version: arch 24.0.1 (64 bit)\r\n- os/kernel: 6.7.12-1-MANJARO (x86_64)\r\n- os/type: linux\r\n- os/arch: amd64\r\n- go/version: go1.22.1\r\n- go/linking: dynamic\r\n- go/tags: none\r\n\r\n#### Which OS you are using and how many bits (e.g. Windows 7, 64 bit)\r\n\r\nSee above.\r\n\r\n#### Which cloud storage system are you using? (e.g. Google Drive)\r\n\r\nNone\r\n\r\n#### The command you were trying to run (e.g. `rclone copy /tmp remote:tmp`)\r\n\r\n`rclone serve dlna --addr 0.0.0.0:8090 -vv /home/arnaud/[...]`\r\n\r\n#### A log from the command with the `-vv` flag (e.g. output from `rclone -vv copy /tmp remote:tmp`)\r\n\r\n```\r\n[...Startup...]\r\n2024/06/01 16:15:11 INFO  : /ctl: 192.168.1.173:51490 POST 200 \"urn:schemas-upnp-org:service:ContentDirectory:1#Browse\"\r\n```\r\n\r\n#### How to use GitHub\r\n\r\n* Please use the \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.\r\n* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.\r\n* Subscribe to receive notifications on status change and new comments.\r\n\n", "patch": "diff --git a/cmd/serve/dlna/cds.go b/cmd/serve/dlna/cds.go\nindex 59572f9265932..be8fdc9c94ba6 100644\n--- a/cmd/serve/dlna/cds.go\n+++ b/cmd/serve/dlna/cds.go\n@@ -289,7 +289,10 @@ func (cds *contentDirectoryService) Handle(action string, argsXML []byte, r *htt\n \t\t\t\treturn nil, err\n \t\t\t}\n \t\t\treturn map[string]string{\n-\t\t\t\t\"Result\": didlLite(string(result)),\n+\t\t\t\t\"TotalMatches\":   \"1\",\n+\t\t\t\t\"NumberReturned\": \"1\",\n+\t\t\t\t\"Result\":         didlLite(string(result)),\n+\t\t\t\t\"UpdateID\":       cds.updateIDString(),\n \t\t\t}, nil\n \t\tdefault:\n \t\t\treturn nil, upnp.Errorf(upnp.ArgumentValueInvalidErrorCode, \"unhandled browse flag: %v\", browse.BrowseFlag)\n", "instance_id": "rclone__rclone-7885", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the Rclone DLNA server not conforming to the UPnP specification for Browse requests. It specifies the missing fields (`NumberReturned` and `TotalMatches`) as per the referenced spec and provides context about the impact (failure to browse in a specific client). The link to the current code and the spec document is helpful, as is the mention of the client (Home Assistant) that exposed the issue. However, there are minor ambiguities: the problem statement does not explicitly define how `NumberReturned` and `TotalMatches` should be calculated or whether there are specific conditions under which these values might differ. Additionally, edge cases or potential variations in the Browse request (e.g., different `BrowseFlag` values or large datasets) are not addressed. While the issue is valid and the goal is clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue requires a straightforward modification to a single function in the `cds.go` file to add two static fields (`TotalMatches` and `NumberReturned`) and an additional field (`UpdateID`) to the response map. The code change is minimal, involving only a few lines, and does not impact the broader architecture or require deep understanding of the codebase beyond the specific function. No complex algorithms, design patterns, or domain-specific knowledge beyond basic familiarity with the UPnP protocol (as referenced in the spec) are needed. However, I note that the provided solution hardcodes the values as \"1\", which might not be correct for all scenarios (e.g., when multiple items are returned). Despite this, the task of adding the fields is trivial, and even implementing dynamic counting logic would only slightly increase the difficulty to the lower end of the 0.2-0.4 range. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code change, further keeping the difficulty low. Overall, this is a very easy fix suitable for a beginner or intermediate developer with basic Go knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add fork readiness indicator to logs\nMany of the CLs print a log periodically when they have been upgraded to a fork, but the fork isn't active. It seems that users appreciate this peace of mind seeing they client is \"ready\". Sure we print the fork info when geth is started up and they can always review that info, but this seems like a nice feature to add.\n\nsomething like\n\n```\nINFO Ready for Prague\n```\n\nevery 3-5 mins?\n", "patch": "diff --git a/core/blockchain.go b/core/blockchain.go\nindex b98c2d43aa83..40c750b8b311 100644\n--- a/core/blockchain.go\n+++ b/core/blockchain.go\n@@ -21,6 +21,7 @@ import (\n \t\"errors\"\n \t\"fmt\"\n \t\"io\"\n+\t\"math\"\n \t\"math/big\"\n \t\"runtime\"\n \t\"slices\"\n@@ -98,6 +99,10 @@ var (\n \terrInvalidNewChain      = errors.New(\"invalid new chain\")\n )\n \n+var (\n+\tforkReadyInterval = 3 * time.Minute\n+)\n+\n const (\n \tbodyCacheLimit     = 256\n \tblockCacheLimit    = 256\n@@ -268,6 +273,8 @@ type BlockChain struct {\n \tprocessor  Processor // Block transaction processor interface\n \tvmConfig   vm.Config\n \tlogger     *tracing.Hooks\n+\n+\tlastForkReadyAlert time.Time // Last time there was a fork readiness print out\n }\n \n // NewBlockChain returns a fully initialised block chain using information\n@@ -1825,6 +1832,9 @@ func (bc *BlockChain) insertChain(chain types.Blocks, setHead bool, makeWitness\n \t\ttrieDiffNodes, trieBufNodes, _ := bc.triedb.Size()\n \t\tstats.report(chain, it.index, snapDiffItems, snapBufItems, trieDiffNodes, trieBufNodes, setHead)\n \n+\t\t// Print confirmation that a future fork is scheduled, but not yet active.\n+\t\tbc.logForkReadiness(block)\n+\n \t\tif !setHead {\n \t\t\t// After merge we expect few side chains. Simply count\n \t\t\t// all blocks the CL gives us for GC processing time\n@@ -1858,6 +1868,7 @@ func (bc *BlockChain) insertChain(chain types.Blocks, setHead bool, makeWitness\n \t\t\t\t\"root\", block.Root())\n \t\t}\n \t}\n+\n \tstats.ignored += it.remaining()\n \treturn witness, it.index, err\n }\n@@ -2455,6 +2466,23 @@ func (bc *BlockChain) reportBlock(block *types.Block, res *ProcessResult, err er\n \tlog.Error(summarizeBadBlock(block, receipts, bc.Config(), err))\n }\n \n+// logForkReadiness will write a log when a future fork is scheduled, but not\n+// active. This is useful so operators know their client is ready for the fork.\n+func (bc *BlockChain) logForkReadiness(block *types.Block) {\n+\tc := bc.Config()\n+\tcurrent, last := c.LatestFork(block.Time()), c.LatestFork(math.MaxUint64)\n+\tt := c.Timestamp(last)\n+\tif t == nil {\n+\t\treturn\n+\t}\n+\tat := time.Unix(int64(*t), 0)\n+\tif current < last && time.Now().After(bc.lastForkReadyAlert.Add(forkReadyInterval)) {\n+\t\tlog.Info(\"Ready for fork activation\", \"fork\", last, \"date\", at.Format(time.RFC822),\n+\t\t\t\"remaining\", time.Until(at).Round(time.Second), \"timestamp\", at.Unix())\n+\t\tbc.lastForkReadyAlert = time.Now()\n+\t}\n+}\n+\n // summarizeBadBlock returns a string summarizing the bad block and other\n // relevant information.\n func summarizeBadBlock(block *types.Block, receipts []*types.Receipt, config *params.ChainConfig, err error) string {\ndiff --git a/params/config.go b/params/config.go\nindex 593c70b1390d..8ba247c447e8 100644\n--- a/params/config.go\n+++ b/params/config.go\n@@ -875,6 +875,23 @@ func (c *ChainConfig) LatestFork(time uint64) forks.Fork {\n \t}\n }\n \n+// Timestamp returns the timestamp associated with the fork or returns nil if\n+// the fork isn't defined or isn't a time-based fork.\n+func (c *ChainConfig) Timestamp(fork forks.Fork) *uint64 {\n+\tswitch {\n+\tcase fork == forks.Osaka:\n+\t\treturn c.OsakaTime\n+\tcase fork == forks.Prague:\n+\t\treturn c.PragueTime\n+\tcase fork == forks.Cancun:\n+\t\treturn c.CancunTime\n+\tcase fork == forks.Shanghai:\n+\t\treturn c.ShanghaiTime\n+\tdefault:\n+\t\treturn nil\n+\t}\n+}\n+\n // isForkBlockIncompatible returns true if a fork scheduled at block s1 cannot be\n // rescheduled to block s2 because head is already past the fork.\n func isForkBlockIncompatible(s1, s2, head *big.Int) bool {\ndiff --git a/params/forks/forks.go b/params/forks/forks.go\nindex 2d44e13b04d8..5d0123b7344f 100644\n--- a/params/forks/forks.go\n+++ b/params/forks/forks.go\n@@ -20,7 +20,7 @@ package forks\n type Fork int\n \n const (\n-\tFrontier = iota\n+\tFrontier Fork = iota\n \tFrontierThawing\n \tHomestead\n \tDAO\n@@ -41,3 +41,35 @@ const (\n \tPrague\n \tOsaka\n )\n+\n+// String implements fmt.Stringer.\n+func (f Fork) String() string {\n+\ts, ok := forkToString[f]\n+\tif !ok {\n+\t\treturn \"Unknown fork\"\n+\t}\n+\treturn s\n+}\n+\n+var forkToString = map[Fork]string{\n+\tFrontier:         \"Frontier\",\n+\tFrontierThawing:  \"Frontier Thawing\",\n+\tHomestead:        \"Homestead\",\n+\tDAO:              \"DAO\",\n+\tTangerineWhistle: \"Tangerine Whistle\",\n+\tSpuriousDragon:   \"Spurious Dragon\",\n+\tByzantium:        \"Byzantium\",\n+\tConstantinople:   \"Constantinople\",\n+\tPetersburg:       \"Petersburg\",\n+\tIstanbul:         \"Istanbul\",\n+\tMuirGlacier:      \"Muir Glacier\",\n+\tBerlin:           \"Berlin\",\n+\tLondon:           \"London\",\n+\tArrowGlacier:     \"Arrow Glacier\",\n+\tGrayGlacier:      \"Gray Glacier\",\n+\tParis:            \"Paris\",\n+\tShanghai:         \"Shanghai\",\n+\tCancun:           \"Cancun\",\n+\tPrague:           \"Prague\",\n+\tOsaka:            \"Osaka\",\n+}\n", "instance_id": "ethereum__go-ethereum-31340", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add a periodic log message indicating readiness for a future fork in a blockchain client (likely Ethereum's Geth). The goal is explicitly stated as adding a log like \"INFO Ready for Prague\" every 3-5 minutes when a fork is scheduled but not yet active. However, there are minor ambiguities and missing details. For instance, it does not specify under what exact conditions the log should be printed (e.g., only for specific forks or all future forks), nor does it clarify how to handle multiple scheduled forks or whether the interval (3-5 minutes) is configurable. Additionally, there are no explicit mentions of edge cases, such as what happens if the system clock changes or if the fork schedule is updated dynamically. Despite these minor gaps, the intent and general requirement are understandable, especially when paired with the provided code changes, which give context to the implementation approach.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, affecting three files (`blockchain.go`, `config.go`, and `forks.go`) in a blockchain codebase (likely Geth). The modifications include adding a new logging function, a helper method for fork timestamps, and string representations for fork types. The changes are mostly additive, with minimal impact on the overall architecture or existing logic. The amount of code changed is moderate, with around 50-60 lines added or modified, which is manageable.\n\n2. **Technical Concepts Involved**: The problem requires understanding basic Go programming constructs (e.g., structs, methods, time handling with `time.Now()` and `time.Unix()`), as well as familiarity with the codebase's configuration and fork scheduling logic. Specific concepts include working with timestamps, intervals for periodic logging, and interacting with the blockchain configuration (`ChainConfig`). These are not overly complex for a developer with moderate Go experience, though some domain knowledge of blockchain forks (e.g., Ethereum hard forks like Prague or Cancun) is helpful for context.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes handle basic scenarios like checking if a fork timestamp exists and ensuring logs are printed only at specified intervals. Potential edge cases, such as system clock changes, dynamic fork schedule updates, or handling multiple forks, are not addressed in the code or problem statement. However, implementing robust error handling for these is not critical to the core functionality and does not significantly increase difficulty.\n\n4. **Overall Complexity**: The task involves straightforward logic\u2014checking if a future fork is scheduled, calculating time until activation, and logging periodically. It does not require deep architectural changes, complex algorithms, or performance optimizations. The primary challenge lies in understanding the existing fork scheduling logic in the codebase, which is manageable with some exploration.\n\nGiven these factors, a difficulty score of 0.35 reflects an \"Easy\" problem that requires moderate understanding of the codebase and simple modifications, with minimal impact on the broader system. It is suitable for a developer with 1-2 years of Go experience and basic familiarity with blockchain concepts.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "config: remove `expand-external-labels` feature flag in v3\n### Proposal\n\nI believe we should make use of the v3 release to finally remove the `expand-external-labels` feature flag and make the behavior \"always on\". The impact was anyway very low in practice, but it could have broken users in rare cases. The v3 release is the perfect opportunity to get this technically breaking change out of the feature flag limbo.\n", "patch": "diff --git a/cmd/prometheus/main.go b/cmd/prometheus/main.go\nindex b84b4edf68a..4a70d63bf86 100644\n--- a/cmd/prometheus/main.go\n+++ b/cmd/prometheus/main.go\n@@ -200,11 +200,10 @@ type flagConfig struct {\n \tmemlimitRatio float64\n \t// These options are extracted from featureList\n \t// for ease of use.\n-\tenableExpandExternalLabels bool\n-\tenablePerStepStats         bool\n-\tenableAutoGOMAXPROCS       bool\n-\tenableAutoGOMEMLIMIT       bool\n-\tenableConcurrentRuleEval   bool\n+\tenablePerStepStats       bool\n+\tenableAutoGOMAXPROCS     bool\n+\tenableAutoGOMEMLIMIT     bool\n+\tenableConcurrentRuleEval bool\n \n \tprometheusURL   string\n \tcorsRegexString string\n@@ -220,9 +219,6 @@ func (c *flagConfig) setFeatureListOptions(logger *slog.Logger) error {\n \t\topts := strings.Split(f, \",\")\n \t\tfor _, o := range opts {\n \t\t\tswitch o {\n-\t\t\tcase \"expand-external-labels\":\n-\t\t\t\tc.enableExpandExternalLabels = true\n-\t\t\t\tlogger.Info(\"Experimental expand-external-labels enabled\")\n \t\t\tcase \"exemplar-storage\":\n \t\t\t\tc.tsdb.EnableExemplarStorage = true\n \t\t\t\tlogger.Info(\"Experimental in-memory exemplar storage enabled\")\n@@ -595,7 +591,7 @@ func main() {\n \n \t// Throw error for invalid config before starting other components.\n \tvar cfgFile *config.Config\n-\tif cfgFile, err = config.LoadFile(cfg.configFile, agentMode, false, promslog.NewNopLogger()); err != nil {\n+\tif cfgFile, err = config.LoadFile(cfg.configFile, agentMode, promslog.NewNopLogger()); err != nil {\n \t\tabsPath, pathErr := filepath.Abs(cfg.configFile)\n \t\tif pathErr != nil {\n \t\t\tabsPath = cfg.configFile\n@@ -1145,7 +1141,7 @@ func main() {\n \t\t\t\tfor {\n \t\t\t\t\tselect {\n \t\t\t\t\tcase <-hup:\n-\t\t\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.enableExpandExternalLabels, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, callback, reloaders...); err != nil {\n+\t\t\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, callback, reloaders...); err != nil {\n \t\t\t\t\t\t\tlogger.Error(\"Error reloading config\", \"err\", err)\n \t\t\t\t\t\t} else if cfg.enableAutoReload {\n \t\t\t\t\t\t\tif currentChecksum, err := config.GenerateChecksum(cfg.configFile); err == nil {\n@@ -1155,7 +1151,7 @@ func main() {\n \t\t\t\t\t\t\t}\n \t\t\t\t\t\t}\n \t\t\t\t\tcase rc := <-webHandler.Reload():\n-\t\t\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.enableExpandExternalLabels, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, callback, reloaders...); err != nil {\n+\t\t\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, callback, reloaders...); err != nil {\n \t\t\t\t\t\t\tlogger.Error(\"Error reloading config\", \"err\", err)\n \t\t\t\t\t\t\trc <- err\n \t\t\t\t\t\t} else {\n@@ -1180,7 +1176,7 @@ func main() {\n \t\t\t\t\t\t}\n \t\t\t\t\t\tlogger.Info(\"Configuration file change detected, reloading the configuration.\")\n \n-\t\t\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.enableExpandExternalLabels, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, callback, reloaders...); err != nil {\n+\t\t\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, callback, reloaders...); err != nil {\n \t\t\t\t\t\t\tlogger.Error(\"Error reloading config\", \"err\", err)\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tchecksum = currentChecksum\n@@ -1210,7 +1206,7 @@ func main() {\n \t\t\t\t\treturn nil\n \t\t\t\t}\n \n-\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.enableExpandExternalLabels, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, func(bool) {}, reloaders...); err != nil {\n+\t\t\t\tif err := reloadConfig(cfg.configFile, cfg.tsdb.EnableExemplarStorage, logger, noStepSubqueryInterval, func(bool) {}, reloaders...); err != nil {\n \t\t\t\t\treturn fmt.Errorf(\"error loading config from %q: %w\", cfg.configFile, err)\n \t\t\t\t}\n \n@@ -1437,7 +1433,7 @@ type reloader struct {\n \treloader func(*config.Config) error\n }\n \n-func reloadConfig(filename string, expandExternalLabels, enableExemplarStorage bool, logger *slog.Logger, noStepSuqueryInterval *safePromQLNoStepSubqueryInterval, callback func(bool), rls ...reloader) (err error) {\n+func reloadConfig(filename string, enableExemplarStorage bool, logger *slog.Logger, noStepSuqueryInterval *safePromQLNoStepSubqueryInterval, callback func(bool), rls ...reloader) (err error) {\n \tstart := time.Now()\n \ttimingsLogger := logger\n \tlogger.Info(\"Loading configuration file\", \"filename\", filename)\n@@ -1453,7 +1449,7 @@ func reloadConfig(filename string, expandExternalLabels, enableExemplarStorage b\n \t\t}\n \t}()\n \n-\tconf, err := config.LoadFile(filename, agentMode, expandExternalLabels, logger)\n+\tconf, err := config.LoadFile(filename, agentMode, logger)\n \tif err != nil {\n \t\treturn fmt.Errorf(\"couldn't load configuration (--config.file=%q): %w\", filename, err)\n \t}\ndiff --git a/cmd/promtool/main.go b/cmd/promtool/main.go\nindex 159fae764d9..6e4eb88437c 100644\n--- a/cmd/promtool/main.go\n+++ b/cmd/promtool/main.go\n@@ -575,7 +575,7 @@ func checkFileExists(fn string) error {\n func checkConfig(agentMode bool, filename string, checkSyntaxOnly bool) ([]string, error) {\n \tfmt.Println(\"Checking\", filename)\n \n-\tcfg, err := config.LoadFile(filename, agentMode, false, promslog.NewNopLogger())\n+\tcfg, err := config.LoadFile(filename, agentMode, promslog.NewNopLogger())\n \tif err != nil {\n \t\treturn nil, err\n \t}\ndiff --git a/cmd/promtool/sd.go b/cmd/promtool/sd.go\nindex 5c00dab03a3..5e005bca8b1 100644\n--- a/cmd/promtool/sd.go\n+++ b/cmd/promtool/sd.go\n@@ -41,7 +41,7 @@ type sdCheckResult struct {\n func CheckSD(sdConfigFiles, sdJobName string, sdTimeout time.Duration, registerer prometheus.Registerer) int {\n \tlogger := promslog.New(&promslog.Config{})\n \n-\tcfg, err := config.LoadFile(sdConfigFiles, false, false, logger)\n+\tcfg, err := config.LoadFile(sdConfigFiles, false, logger)\n \tif err != nil {\n \t\tfmt.Fprintln(os.Stderr, \"Cannot load config\", err)\n \t\treturn failureExitCode\ndiff --git a/config/config.go b/config/config.go\nindex 3f35a195d0b..3eb6898d5cb 100644\n--- a/config/config.go\n+++ b/config/config.go\n@@ -72,7 +72,7 @@ const (\n )\n \n // Load parses the YAML input s into a Config.\n-func Load(s string, expandExternalLabels bool, logger *slog.Logger) (*Config, error) {\n+func Load(s string, logger *slog.Logger) (*Config, error) {\n \tcfg := &Config{}\n \t// If the entire config body is empty the UnmarshalYAML method is\n \t// never called. We thus have to set the DefaultConfig at the entry\n@@ -84,10 +84,6 @@ func Load(s string, expandExternalLabels bool, logger *slog.Logger) (*Config, er\n \t\treturn nil, err\n \t}\n \n-\tif !expandExternalLabels {\n-\t\treturn cfg, nil\n-\t}\n-\n \tb := labels.NewScratchBuilder(0)\n \tcfg.GlobalConfig.ExternalLabels.Range(func(v labels.Label) {\n \t\tnewV := os.Expand(v.Value, func(s string) string {\n@@ -106,17 +102,19 @@ func Load(s string, expandExternalLabels bool, logger *slog.Logger) (*Config, er\n \t\t// Note newV can be blank. https://github.com/prometheus/prometheus/issues/11024\n \t\tb.Add(v.Name, newV)\n \t})\n-\tcfg.GlobalConfig.ExternalLabels = b.Labels()\n+\tif !b.Labels().IsEmpty() {\n+\t\tcfg.GlobalConfig.ExternalLabels = b.Labels()\n+\t}\n \treturn cfg, nil\n }\n \n // LoadFile parses the given YAML file into a Config.\n-func LoadFile(filename string, agentMode, expandExternalLabels bool, logger *slog.Logger) (*Config, error) {\n+func LoadFile(filename string, agentMode bool, logger *slog.Logger) (*Config, error) {\n \tcontent, err := os.ReadFile(filename)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\tcfg, err := Load(string(content), expandExternalLabels, logger)\n+\tcfg, err := Load(string(content), logger)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"parsing YAML file %s: %w\", filename, err)\n \t}\ndiff --git a/docs/configuration/configuration.md b/docs/configuration/configuration.md\nindex 8552e0b16af..70e976978da 100644\n--- a/docs/configuration/configuration.md\n+++ b/docs/configuration/configuration.md\n@@ -79,7 +79,11 @@ global:\n   [ rule_query_offset: <duration> | default = 0s ]\n \n   # The labels to add to any time series or alerts when communicating with\n-  # external systems (federation, remote storage, Alertmanager).\n+  # external systems (federation, remote storage, Alertmanager). \n+  # Environment variable references `${var}` or `$var` are replaced according \n+  # to the values of the current environment variables. \n+  # References to undefined variables are replaced by the empty string.\n+  # The `$` character can be escaped by using `$$`.\n   external_labels:\n     [ <labelname>: <labelvalue> ... ]\n \ndiff --git a/docs/feature_flags.md b/docs/feature_flags.md\nindex a3e2c0b9e95..65eb60eaf17 100644\n--- a/docs/feature_flags.md\n+++ b/docs/feature_flags.md\n@@ -11,15 +11,6 @@ Their behaviour can change in future releases which will be communicated via the\n You can enable them using the `--enable-feature` flag with a comma separated list of features.\n They may be enabled by default in future versions.\n \n-## Expand environment variables in external labels\n-\n-`--enable-feature=expand-external-labels`\n-\n-Replace `${var}` or `$var` in the [`external_labels`](configuration/configuration.md#configuration-file)\n-values according to the values of the current environment variables. References\n-to undefined variables are replaced by the empty string.\n-The `$` character can be escaped by using `$$`.\n-\n ## Exemplars storage\n \n `--enable-feature=exemplar-storage`\n", "instance_id": "prometheus__prometheus-14657", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to remove the `expand-external-labels` feature flag and make the behavior \"always on\" as part of a v3 release. It provides a rationale for the change (low impact in practice, opportunity for a breaking change in v3) and implies the goal of simplifying the codebase by eliminating conditional logic tied to the feature flag. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly describe the expected behavior of \"always on\" (though it can be inferred from the code changes and documentation updates). Additionally, there is no mention of potential edge cases, user impact beyond \"rare cases,\" or specific testing requirements to ensure the change does not introduce regressions. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The changes involve multiple files (`main.go`, `config.go`, `promtool/main.go`, `sd.go`, and documentation files), but the modifications are relatively straightforward. They primarily consist of removing the `expand-external-labels` flag and associated conditional logic, as well as updating function signatures and documentation. The changes do not significantly impact the system's architecture; they are more about cleanup and simplification of existing code. The overall amount of code change is moderate but not extensive.\n\n2. **Number of Technical Concepts:** Solving this problem requires a basic understanding of Go programming, specifically handling command-line flags, configuration parsing, and environment variable expansion. No advanced algorithms, design patterns, or domain-specific knowledge beyond familiarity with Prometheus configuration are needed. The concepts involved are relatively simple for a developer familiar with the codebase.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement mentions that the change could break users in \"rare cases,\" but no specific edge cases are detailed. The code changes do not introduce new error handling logic; they remove conditional checks related to the feature flag. However, a developer would need to consider potential impacts of always expanding external labels (e.g., handling undefined environment variables, which is already addressed in the code). The edge cases appear minimal and are partially mitigated by existing logic.\n\n4. **Overall Complexity:** The task requires understanding the role of the feature flag in the codebase and ensuring its removal does not introduce unintended behavior. While this involves some code logic comprehension and cross-file modifications, it does not require deep architectural changes or complex refactoring. The primary challenge lies in verifying that the \"always on\" behavior works as expected across different use cases, which adds a small layer of complexity but not enough to push this into the \"Medium\" difficulty range.\n\nA score of 0.35 reflects the need for moderate effort in understanding the codebase and making consistent changes across multiple files, but the task remains straightforward for a developer with basic to intermediate experience in Go and familiarity with the Prometheus project.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Overlapping `cascade` rules are applied in nondeterministic order\n<!--\r\nPlease do not use the issue queue for questions or troubleshooting. Unless you are certain that your issue is a software defect, use the forum:\r\n\r\nhttps://discourse.gohugo.io\r\n-->\r\n\r\nIf multiple [`cascade` rules](https://gohugo.io/content-management/front-matter/#cascade) match the same page, they are applied in arbitrary order according to Go map iteration order, resulting in nondeterminstic builds and extremely confusing behavior.\r\n\r\n## Context\r\n\r\nI wanted to create a `/drafts/` section on my site, with the behavior that posts there would be published to production so I could share them for review, but would only be listed locally, to discourage stumbling across them.\r\n\r\nAfter some experimentation, I determined that the following `content/drafts/_index.md` **appeared** to work as desired; I speculated that overlapping cascade rules were applied in a \"first-match-wins\" order.\r\n\r\n```markdown\r\n---\r\ncascade:\r\n- _target:\r\n    environment: development\r\n  build:\r\n    list: always\r\n- build:\r\n    list: never\r\n---\r\n```\r\n\r\nHowever, the above only **mostly** works; I soon discovered that **sometimes** posts would be excluded from the page list in development. \r\n\r\n### The bug\r\n\r\nIt turns out that rules are applied in an arbitrary order, according to [Go map iteration order](https://github.com/gohugoio/hugo/blob/7ee36b3718790f1dc4a2722f925acc6e3fd3caea/hugolib/page__meta.go#L375). In practice -- for my build of hugo -- this results in **mostly** consistent behavior, which is what made the behavior so confusing.\r\n\r\n### Reproducer\r\n\r\nI've [pushed a minimal demo to github](https://github.com/nelhage/hugo-cascade-order):\r\n\r\n```console\r\n$ git clone https://github.com/nelhage/hugo-cascade-order\r\n$ cd hugo-cascade-order\r\n# Render 100 times, and count how many pages appear in the RSS feed each time.\r\n# The correct answer should be 20\r\n$ { for _ in {1..100}; do hugo --quiet --environment development && grep -c '<title>Page' public/post/index.xml; done } | sort | uniq -c | sort -rn\r\n  26 17\r\n  20 18\r\n  18 19\r\n  12 15\r\n  11 16\r\n  10 20\r\n   2 14\r\n   1 12\r\n```\r\n\r\nYou can see that links are generated and included in the page list **most** of the time, but that typically 1-3 pages are dropped. Go deliberately injects entropy into the map iteration order, but apparently in this case it is **mostly** deterministic.\r\n\r\n## Comment\r\n\r\nThe issue can be worked around in user sites by eliminating overlapping `cascade` rules -- e.g. I have worked around using\r\n\r\n```markdown\r\n---\r\ncascade:\r\n- _target:\r\n    environment: development\r\n  build:\r\n    list: always\r\n- _target:\r\n    environment: '{staging,production}'\r\n  build:\r\n    list: never\r\n---\r\n```\r\n\r\nHowever, I still regard this behavior as a bug due to how profoundly confusing and unexpected it is. `cascade:` rules are present in the source as a list, and so IMO it's very reasonable to expect them to be processed in that order.\r\n\r\n<!-- Please answer these questions before submitting your issue. Thanks! -->\r\n\r\n### What version of Hugo are you using (`hugo version`)?\r\n\r\n<pre>\r\n$ hugo version\r\nhugo v0.125.5+extended darwin/arm64 BuildDate=unknown VendorInfo=nixpkgs\r\n</pre>\r\n\r\n### Does this issue reproduce with the latest release?\r\nYes\r\n\n", "patch": "diff --git a/common/hashing/hashing.go b/common/hashing/hashing.go\nindex 18ec8262358..e4535675833 100644\n--- a/common/hashing/hashing.go\n+++ b/common/hashing/hashing.go\n@@ -123,16 +123,24 @@ func HashUint64(vs ...any) uint64 {\n \t\to = elements\n \t}\n \n-\thashOpts := getHashOpts()\n-\tdefer putHashOpts(hashOpts)\n-\n-\thash, err := hashstructure.Hash(o, hashOpts)\n+\thash, err := Hash(o)\n \tif err != nil {\n \t\tpanic(err)\n \t}\n \treturn hash\n }\n \n+// Hash returns a hash from vs.\n+func Hash(vs ...any) (uint64, error) {\n+\thashOpts := getHashOpts()\n+\tdefer putHashOpts(hashOpts)\n+\tvar v any = vs\n+\tif len(vs) == 1 {\n+\t\tv = vs[0]\n+\t}\n+\treturn hashstructure.Hash(v, hashOpts)\n+}\n+\n type keyer interface {\n \tKey() string\n }\ndiff --git a/common/maps/ordered.go b/common/maps/ordered.go\nnew file mode 100644\nindex 00000000000..eaa4d73c611\n--- /dev/null\n+++ b/common/maps/ordered.go\n@@ -0,0 +1,133 @@\n+// Copyright 2024 The Hugo Authors. All rights reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package maps\n+\n+import (\n+\t\"github.com/gohugoio/hugo/common/hashing\"\n+)\n+\n+// Ordered is a map that can be iterated in the order of insertion.\n+// Note that insertion order is not affected if a key is re-inserted into the map.\n+// In a nil map, all operations are no-ops.\n+// This is not thread safe.\n+type Ordered[K comparable, T any] struct {\n+\t// The keys in the order they were added.\n+\tkeys []K\n+\t// The values.\n+\tvalues map[K]T\n+}\n+\n+// NewOrdered creates a new Ordered map.\n+func NewOrdered[K comparable, T any]() *Ordered[K, T] {\n+\treturn &Ordered[K, T]{values: make(map[K]T)}\n+}\n+\n+// Set sets the value for the given key.\n+// Note that insertion order is not affected if a key is re-inserted into the map.\n+func (m *Ordered[K, T]) Set(key K, value T) {\n+\tif m == nil {\n+\t\treturn\n+\t}\n+\t// Check if key already exists.\n+\tif _, found := m.values[key]; !found {\n+\t\tm.keys = append(m.keys, key)\n+\t}\n+\tm.values[key] = value\n+}\n+\n+// Get gets the value for the given key.\n+func (m *Ordered[K, T]) Get(key K) (T, bool) {\n+\tif m == nil {\n+\t\tvar v T\n+\t\treturn v, false\n+\t}\n+\tvalue, found := m.values[key]\n+\treturn value, found\n+}\n+\n+// Delete deletes the value for the given key.\n+func (m *Ordered[K, T]) Delete(key K) {\n+\tif m == nil {\n+\t\treturn\n+\t}\n+\tdelete(m.values, key)\n+\tfor i, k := range m.keys {\n+\t\tif k == key {\n+\t\t\tm.keys = append(m.keys[:i], m.keys[i+1:]...)\n+\t\t\tbreak\n+\t\t}\n+\t}\n+}\n+\n+// Clone creates a shallow copy of the map.\n+func (m *Ordered[K, T]) Clone() *Ordered[K, T] {\n+\tif m == nil {\n+\t\treturn nil\n+\t}\n+\tclone := NewOrdered[K, T]()\n+\tfor _, k := range m.keys {\n+\t\tclone.Set(k, m.values[k])\n+\t}\n+\treturn clone\n+}\n+\n+// Keys returns the keys in the order they were added.\n+func (m *Ordered[K, T]) Keys() []K {\n+\tif m == nil {\n+\t\treturn nil\n+\t}\n+\treturn m.keys\n+}\n+\n+// Values returns the values in the order they were added.\n+func (m *Ordered[K, T]) Values() []T {\n+\tif m == nil {\n+\t\treturn nil\n+\t}\n+\tvar values []T\n+\tfor _, k := range m.keys {\n+\t\tvalues = append(values, m.values[k])\n+\t}\n+\treturn values\n+}\n+\n+// Len returns the number of items in the map.\n+func (m *Ordered[K, T]) Len() int {\n+\tif m == nil {\n+\t\treturn 0\n+\t}\n+\treturn len(m.keys)\n+}\n+\n+// Range calls f sequentially for each key and value present in the map.\n+// If f returns false, range stops the iteration.\n+// TODO(bep) replace with iter.Seq2 when we bump go Go 1.24.\n+func (m *Ordered[K, T]) Range(f func(key K, value T) bool) {\n+\tif m == nil {\n+\t\treturn\n+\t}\n+\tfor _, k := range m.keys {\n+\t\tif !f(k, m.values[k]) {\n+\t\t\treturn\n+\t\t}\n+\t}\n+}\n+\n+// Hash calculates a hash from the values.\n+func (m *Ordered[K, T]) Hash() (uint64, error) {\n+\tif m == nil {\n+\t\treturn 0, nil\n+\t}\n+\treturn hashing.Hash(m.values)\n+}\ndiff --git a/config/allconfig/allconfig.go b/config/allconfig/allconfig.go\nindex 96d10b3bdd7..3c2eddcaba3 100644\n--- a/config/allconfig/allconfig.go\n+++ b/config/allconfig/allconfig.go\n@@ -143,7 +143,7 @@ type Config struct {\n \n \t// The cascade configuration section contains the top level front matter cascade configuration options,\n \t// a slice of page matcher and params to apply to those pages.\n-\tCascade *config.ConfigNamespace[[]page.PageMatcherParamsConfig, map[page.PageMatcher]maps.Params] `mapstructure:\"-\"`\n+\tCascade *config.ConfigNamespace[[]page.PageMatcherParamsConfig, *maps.Ordered[page.PageMatcher, maps.Params]] `mapstructure:\"-\"`\n \n \t// The segments defines segments for the site. Used for partial/segmented builds.\n \tSegments *config.ConfigNamespace[map[string]segments.SegmentConfig, segments.Segments] `mapstructure:\"-\"`\n@@ -766,9 +766,10 @@ type Configs struct {\n }\n \n func (c *Configs) Validate(logger loggers.Logger) error {\n-\tfor p := range c.Base.Cascade.Config {\n+\tc.Base.Cascade.Config.Range(func(p page.PageMatcher, params maps.Params) bool {\n \t\tpage.CheckCascadePattern(logger, p)\n-\t}\n+\t\treturn true\n+\t})\n \treturn nil\n }\n \ndiff --git a/hugolib/content_map_page.go b/hugolib/content_map_page.go\nindex b48504203d7..6927562f127 100644\n--- a/hugolib/content_map_page.go\n+++ b/hugolib/content_map_page.go\n@@ -1387,7 +1387,7 @@ func (sa *sitePagesAssembler) applyAggregates() error {\n \t\t}\n \n \t\t// Handle cascades first to get any default dates set.\n-\t\tvar cascade map[page.PageMatcher]maps.Params\n+\t\tvar cascade *maps.Ordered[page.PageMatcher, maps.Params]\n \t\tif keyPage == \"\" {\n \t\t\t// Home page gets it's cascade from the site config.\n \t\t\tcascade = sa.conf.Cascade.Config\n@@ -1399,7 +1399,7 @@ func (sa *sitePagesAssembler) applyAggregates() error {\n \t\t} else {\n \t\t\t_, data := pw.WalkContext.Data().LongestPrefix(keyPage)\n \t\t\tif data != nil {\n-\t\t\t\tcascade = data.(map[page.PageMatcher]maps.Params)\n+\t\t\t\tcascade = data.(*maps.Ordered[page.PageMatcher, maps.Params])\n \t\t\t}\n \t\t}\n \n@@ -1481,11 +1481,11 @@ func (sa *sitePagesAssembler) applyAggregates() error {\n \t\t\t\tpageResource := rs.r.(*pageState)\n \t\t\t\trelPath := pageResource.m.pathInfo.BaseRel(pageBundle.m.pathInfo)\n \t\t\t\tpageResource.m.resourcePath = relPath\n-\t\t\t\tvar cascade map[page.PageMatcher]maps.Params\n+\t\t\t\tvar cascade *maps.Ordered[page.PageMatcher, maps.Params]\n \t\t\t\t// Apply cascade (if set) to the page.\n \t\t\t\t_, data := pw.WalkContext.Data().LongestPrefix(resourceKey)\n \t\t\t\tif data != nil {\n-\t\t\t\t\tcascade = data.(map[page.PageMatcher]maps.Params)\n+\t\t\t\t\tcascade = data.(*maps.Ordered[page.PageMatcher, maps.Params])\n \t\t\t\t}\n \t\t\t\tif err := pageResource.setMetaPost(cascade); err != nil {\n \t\t\t\t\treturn false, err\n@@ -1549,10 +1549,10 @@ func (sa *sitePagesAssembler) applyAggregatesToTaxonomiesAndTerms() error {\n \t\t\t\tconst eventName = \"dates\"\n \n \t\t\t\tif p.Kind() == kinds.KindTerm {\n-\t\t\t\t\tvar cascade map[page.PageMatcher]maps.Params\n+\t\t\t\t\tvar cascade *maps.Ordered[page.PageMatcher, maps.Params]\n \t\t\t\t\t_, data := pw.WalkContext.Data().LongestPrefix(s)\n \t\t\t\t\tif data != nil {\n-\t\t\t\t\t\tcascade = data.(map[page.PageMatcher]maps.Params)\n+\t\t\t\t\t\tcascade = data.(*maps.Ordered[page.PageMatcher, maps.Params])\n \t\t\t\t\t}\n \t\t\t\t\tif err := p.setMetaPost(cascade); err != nil {\n \t\t\t\t\t\treturn false, err\ndiff --git a/hugolib/page__meta.go b/hugolib/page__meta.go\nindex 07d9d1c0e4e..38fe6db56db 100644\n--- a/hugolib/page__meta.go\n+++ b/hugolib/page__meta.go\n@@ -87,8 +87,8 @@ type pageMetaParams struct {\n \n \t// These are only set in watch mode.\n \tdatesOriginal   pagemeta.Dates\n-\tparamsOriginal  map[string]any                   // contains the original params as defined in the front matter.\n-\tcascadeOriginal map[page.PageMatcher]maps.Params // contains the original cascade as defined in the front matter.\n+\tparamsOriginal  map[string]any                               // contains the original params as defined in the front matter.\n+\tcascadeOriginal *maps.Ordered[page.PageMatcher, maps.Params] // contains the original cascade as defined in the front matter.\n }\n \n // From page front matter.\n@@ -96,10 +96,10 @@ type pageMetaFrontMatter struct {\n \tconfiguredOutputFormats output.Formats // outputs defined in front matter.\n }\n \n-func (m *pageMetaParams) init(preserveOringal bool) {\n-\tif preserveOringal {\n+func (m *pageMetaParams) init(preserveOriginal bool) {\n+\tif preserveOriginal {\n \t\tm.paramsOriginal = xmaps.Clone[maps.Params](m.pageConfig.Params)\n-\t\tm.cascadeOriginal = xmaps.Clone[map[page.PageMatcher]maps.Params](m.pageConfig.CascadeCompiled)\n+\t\tm.cascadeOriginal = m.pageConfig.CascadeCompiled.Clone()\n \t}\n }\n \n@@ -306,22 +306,22 @@ func (p *pageMeta) setMetaPre(pi *contentParseInfo, logger loggers.Logger, conf\n \treturn nil\n }\n \n-func (ps *pageState) setMetaPost(cascade map[page.PageMatcher]maps.Params) error {\n+func (ps *pageState) setMetaPost(cascade *maps.Ordered[page.PageMatcher, maps.Params]) error {\n \tps.m.setMetaPostCount++\n \tvar cascadeHashPre uint64\n \tif ps.m.setMetaPostCount > 1 {\n \t\tcascadeHashPre = hashing.HashUint64(ps.m.pageConfig.CascadeCompiled)\n-\t\tps.m.pageConfig.CascadeCompiled = xmaps.Clone[map[page.PageMatcher]maps.Params](ps.m.cascadeOriginal)\n+\t\tps.m.pageConfig.CascadeCompiled = ps.m.cascadeOriginal.Clone()\n \n \t}\n \n \t// Apply cascades first so they can be overridden later.\n \tif cascade != nil {\n \t\tif ps.m.pageConfig.CascadeCompiled != nil {\n-\t\t\tfor k, v := range cascade {\n-\t\t\t\tvv, found := ps.m.pageConfig.CascadeCompiled[k]\n+\t\t\tcascade.Range(func(k page.PageMatcher, v maps.Params) bool {\n+\t\t\t\tvv, found := ps.m.pageConfig.CascadeCompiled.Get(k)\n \t\t\t\tif !found {\n-\t\t\t\t\tps.m.pageConfig.CascadeCompiled[k] = v\n+\t\t\t\t\tps.m.pageConfig.CascadeCompiled.Set(k, v)\n \t\t\t\t} else {\n \t\t\t\t\t// Merge\n \t\t\t\t\tfor ck, cv := range v {\n@@ -330,7 +330,8 @@ func (ps *pageState) setMetaPost(cascade map[page.PageMatcher]maps.Params) error\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t}\n+\t\t\t\treturn true\n+\t\t\t})\n \t\t\tcascade = ps.m.pageConfig.CascadeCompiled\n \t\t} else {\n \t\t\tps.m.pageConfig.CascadeCompiled = cascade\n@@ -354,16 +355,17 @@ func (ps *pageState) setMetaPost(cascade map[page.PageMatcher]maps.Params) error\n \t}\n \n \t// Cascade is also applied to itself.\n-\tfor m, v := range cascade {\n-\t\tif !m.Matches(ps) {\n-\t\t\tcontinue\n+\tcascade.Range(func(k page.PageMatcher, v maps.Params) bool {\n+\t\tif !k.Matches(ps) {\n+\t\t\treturn true\n \t\t}\n \t\tfor kk, vv := range v {\n \t\t\tif _, found := ps.m.pageConfig.Params[kk]; !found {\n \t\t\t\tps.m.pageConfig.Params[kk] = vv\n \t\t\t}\n \t\t}\n-\t}\n+\t\treturn true\n+\t})\n \n \tif err := ps.setMetaPostParams(); err != nil {\n \t\treturn err\ndiff --git a/resources/page/page_matcher.go b/resources/page/page_matcher.go\nindex f2075273af4..8155be99d98 100644\n--- a/resources/page/page_matcher.go\n+++ b/resources/page/page_matcher.go\n@@ -104,9 +104,9 @@ func CheckCascadePattern(logger loggers.Logger, m PageMatcher) {\n \t}\n }\n \n-func DecodeCascadeConfig(logger loggers.Logger, in any) (*config.ConfigNamespace[[]PageMatcherParamsConfig, map[PageMatcher]maps.Params], error) {\n-\tbuildConfig := func(in any) (map[PageMatcher]maps.Params, any, error) {\n-\t\tcascade := make(map[PageMatcher]maps.Params)\n+func DecodeCascadeConfig(logger loggers.Logger, in any) (*config.ConfigNamespace[[]PageMatcherParamsConfig, *maps.Ordered[PageMatcher, maps.Params]], error) {\n+\tbuildConfig := func(in any) (*maps.Ordered[PageMatcher, maps.Params], any, error) {\n+\t\tcascade := maps.NewOrdered[PageMatcher, maps.Params]()\n \t\tif in == nil {\n \t\t\treturn cascade, []map[string]any{}, nil\n \t\t}\n@@ -134,7 +134,7 @@ func DecodeCascadeConfig(logger loggers.Logger, in any) (*config.ConfigNamespace\n \t\tfor _, cfg := range cfgs {\n \t\t\tm := cfg.Target\n \t\t\tCheckCascadePattern(logger, m)\n-\t\t\tc, found := cascade[m]\n+\t\t\tc, found := cascade.Get(m)\n \t\t\tif found {\n \t\t\t\t// Merge\n \t\t\t\tfor k, v := range cfg.Params {\n@@ -143,18 +143,18 @@ func DecodeCascadeConfig(logger loggers.Logger, in any) (*config.ConfigNamespace\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t} else {\n-\t\t\t\tcascade[m] = cfg.Params\n+\t\t\t\tcascade.Set(m, cfg.Params)\n \t\t\t}\n \t\t}\n \n \t\treturn cascade, cfgs, nil\n \t}\n \n-\treturn config.DecodeNamespace[[]PageMatcherParamsConfig](in, buildConfig)\n+\treturn config.DecodeNamespace[[]PageMatcherParamsConfig, *maps.Ordered[PageMatcher, maps.Params]](in, buildConfig)\n }\n \n // DecodeCascade decodes in which could be either a map or a slice of maps.\n-func DecodeCascade(logger loggers.Logger, in any) (map[PageMatcher]maps.Params, error) {\n+func DecodeCascade(logger loggers.Logger, in any) (*maps.Ordered[PageMatcher, maps.Params], error) {\n \tconf, err := DecodeCascadeConfig(logger, in)\n \tif err != nil {\n \t\treturn nil, err\ndiff --git a/resources/page/pagemeta/page_frontmatter.go b/resources/page/pagemeta/page_frontmatter.go\nindex d5d61260981..21789909ec8 100644\n--- a/resources/page/pagemeta/page_frontmatter.go\n+++ b/resources/page/pagemeta/page_frontmatter.go\n@@ -114,9 +114,9 @@ type PageConfig struct {\n \tContent Source\n \n \t// Compiled values.\n-\tCascadeCompiled      map[page.PageMatcher]maps.Params\n-\tContentMediaType     media.Type `mapstructure:\"-\" json:\"-\"`\n-\tIsFromContentAdapter bool       `mapstructure:\"-\" json:\"-\"`\n+\tCascadeCompiled      *maps.Ordered[page.PageMatcher, maps.Params] `mapstructure:\"-\" json:\"-\"`\n+\tContentMediaType     media.Type                                   `mapstructure:\"-\" json:\"-\"`\n+\tIsFromContentAdapter bool                                         `mapstructure:\"-\" json:\"-\"`\n }\n \n var DefaultPageConfig = PageConfig{\n", "instance_id": "gohugoio__hugo-13295", "clarity": 3, "difficulty": 0.65, "clarity_explanation": "The problem statement is comprehensive and well-articulated. It clearly describes the issue of nondeterministic behavior in the application of `cascade` rules in Hugo due to Go's map iteration order, which leads to inconsistent builds. The goal is explicit: to ensure deterministic processing of cascade rules, ideally in the order they are defined. The context, including the user's intent and the unexpected behavior, is provided with detailed examples and a reproducible demo on GitHub. Additionally, the statement includes a workaround, version information, and a clear explanation of why this is considered a bug despite the workaround. There are no significant ambiguities; the input (front matter configuration), output (consistent page listing behavior), and constraints (order of rule application) are well-defined. The inclusion of a minimal reproducible example and specific code references further enhances clarity. Therefore, this problem statement merits a score of 3 for being valid, clear, and detailed with examples.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, involving multiple files and the introduction of a new data structure (`Ordered` map) to preserve insertion order, which is a non-trivial modification to the existing codebase. The changes span across configuration handling, page metadata processing, and utility functions, requiring a good understanding of Hugo's architecture, particularly how front matter and cascade rules are processed. Second, the technical concepts involved include generics in Go, custom data structure implementation, and hashing mechanisms, which demand a solid grasp of language features and library usage. Third, while the problem statement does not explicitly mention complex edge cases beyond the nondeterministic behavior, the code changes must ensure that the new ordered map implementation handles all existing use cases of cascade rules without introducing regressions, which implies careful consideration of edge cases like rule merging and matching logic. Finally, the impact on the system's architecture is moderate, as it changes how configuration data is stored and iterated, potentially affecting performance and compatibility with existing configurations. Given these factors, a difficulty score of 0.65 is appropriate, reflecting the need for deep understanding and complex modifications, though not reaching the extreme complexity of system-level or domain-specific challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "config/allconfig: Slice of language configs has double the expected elements\nFound while addressing #13199.\r\n\r\n`(allconfig.Configs).Init` builds `(allconfig.Configs).LanguageConfigSlice` by appending one element per language in an indeterminate order. Then it appends to the slice again, one element per language sorted by language weight.\r\n\r\nSo this site configuration:\r\n\r\n```text\r\n[languages.en]\r\ntitle = 'TITLE_EN'\r\nweight = 2\r\n\r\n[languages.de]\r\ntitle = 'TITLE_DE'\r\nweight = 1\r\n\r\n[languages.fr]\r\ntitle = 'TITLE_FR'\r\nweight = 3\r\n```\r\n\r\n...results in a slice with too many elements, ordered like this:\r\n\r\n```text\r\n[en de fr de en fr]\r\n --------\r\n indeterminate order\r\n```\r\n\r\nWe only want the last three so that `(allconfig.Configs).LanguageConfigSlice[0]` returns the config with the lowest weight. \r\n\r\nI looked at our usage of this slice, and from a practical standpoint I think it only affects the output from the `hugo config` command, and even then only under certain conditions.\r\n\r\nBut we should fix this anyway.\r\n\r\n<https://github.com/gohugoio/hugo/blob/master/config/allconfig/allconfig.go#L750-L775>\r\n\r\n\n", "patch": "diff --git a/config/allconfig/allconfig.go b/config/allconfig/allconfig.go\nindex 5fe6edcd16f..4d65d5f2bab 100644\n--- a/config/allconfig/allconfig.go\n+++ b/config/allconfig/allconfig.go\n@@ -751,7 +751,6 @@ func (c *Configs) Init() error {\n \tvar languages langs.Languages\n \tdefaultContentLanguage := c.Base.DefaultContentLanguage\n \tfor k, v := range c.LanguageConfigMap {\n-\t\tc.LanguageConfigSlice = append(c.LanguageConfigSlice, v)\n \t\tlanguageConf := v.Languages[k]\n \t\tlanguage, err := langs.NewLanguage(k, defaultContentLanguage, v.TimeZone, languageConf)\n \t\tif err != nil {\n", "instance_id": "gohugoio__hugo-13202", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `LanguageConfigSlice` in the `Configs` struct contains duplicate elements due to two separate append operations, and the goal is to retain only the sorted elements based on weight. The statement provides a concrete example of the configuration and the resulting incorrect slice order, which helps in understanding the problem. It also references the specific part of the codebase and mentions the practical impact (limited to the `hugo config` command output under certain conditions). However, there are minor ambiguities: it does not explicitly state whether there are any side effects or dependencies on the current ordering elsewhere in the codebase, nor does it mention potential edge cases (e.g., empty language configs or missing weights). These missing details prevent it from being fully comprehensive, hence a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward fix. Analyzing the code change, it simply removes one of the append operations in the `Init` method of the `Configs` struct, ensuring that only the sorted elements (added later in the code, as implied by the problem statement) are retained in `LanguageConfigSlice`. The scope of the change is minimal, confined to a single line in a single file, with no impact on the broader architecture or other modules. The technical concepts required are basic: understanding slice operations in Go and the logic of the `Init` method. There are no complex algorithms, design patterns, or domain-specific knowledge needed. Additionally, the problem statement does not highlight any specific edge cases or error handling requirements, and the code change does not introduce or modify such logic. Given the simplicity of the fix and the limited scope, I assign a difficulty score of 0.15, placing it in the \"Very Easy\" category, as it requires only a basic code modification with minimal understanding of the surrounding codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deprecate .Site.Sites.First in favor of .Site.Sites.Default\nPrior to v0.123.0, `.Site.Sites` returned the sites sorted by weight then language key. With v0.123.0 and later, the site corresponding to the default language is first on the list. \r\n\r\nWith this config...\r\n\r\n```text\r\ndefaultContentLanguage = 'de'\r\n\r\n[languages.en]\r\nweight = 1\r\n\r\n[languages.de]\r\nweight = 2\r\n\r\n[languages.fr]\r\nweight = 3\r\n```\r\n\r\n...`.Site.Sites.First` now returns the second site, which is exactly what it should be doing. But I think we should rename the method to describe what it does.\r\n\r\nIf implemented, need to update:\r\n\r\n- <https://gohugo.io/methods/page/sites/>\r\n- <https://gohugo.io/methods/site/sites/>\r\n\n", "patch": "diff --git a/resources/page/site.go b/resources/page/site.go\nindex df33485eb1a..56f438cb650 100644\n--- a/resources/page/site.go\n+++ b/resources/page/site.go\n@@ -139,8 +139,15 @@ type Site interface {\n // Sites represents an ordered list of sites (languages).\n type Sites []Site\n \n-// First is a convenience method to get the first Site, i.e. the main language.\n+// Deprecated: Use .Sites.Default instead.\n func (s Sites) First() Site {\n+\thugo.Deprecate(\".Sites.First\", \"Use .Sites.Default instead.\", \"v0.127.0\")\n+\treturn s.Default()\n+}\n+\n+// Default is a convenience method to get the site corresponding to the default\n+// content language.\n+func (s Sites) Default() Site {\n \tif len(s) == 0 {\n \t\treturn nil\n \t}\n@@ -165,7 +172,7 @@ func (s *siteWrapper) Key() string {\n \treturn s.s.Language().Lang\n }\n \n-// // Deprecated: Use .Site.Params instead.\n+// Deprecated: Use .Site.Params instead.\n func (s *siteWrapper) Social() map[string]string {\n \treturn s.s.Social()\n }\n", "instance_id": "gohugoio__hugo-12559", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the intent to deprecate `.Site.Sites.First` in favor of `.Site.Sites.Default` to better reflect the behavior introduced in version 0.123.0, where the default language site is prioritized. It provides a configuration example to illustrate the behavior of `.Site.Sites.First` and explains the need for a rename to improve clarity. However, there are minor ambiguities: the statement does not explicitly define what \"default language\" means in all contexts (though it can be inferred from the config), and it lacks details on potential edge cases or backward compatibility concerns beyond the deprecation notice. Additionally, while it mentions updating documentation, it does not specify if there are other parts of the codebase or user-facing features that might be affected. Overall, the goal and context are clear, but some minor details are missing.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward code modification with minimal complexity. The scope of the change is limited to a single file (`site.go`) and involves renaming a method by adding a new `Default()` method and deprecating the existing `First()` method with a simple wrapper that calls the new method. The code change is small, consisting of a few lines to implement the new method and add a deprecation notice. No deep understanding of the broader codebase or architecture is required beyond familiarity with the `Sites` type and its behavior. The technical concepts involved are basic\u2014method renaming, deprecation handling using a utility function (`hugo.Deprecate`), and returning a value from a slice. There are no significant edge cases or error handling requirements beyond the existing check for an empty slice, which is already handled in the code. The primary effort lies in ensuring the deprecation message is clear and updating the referenced documentation, which is outside the scope of coding complexity. Overall, this is a very easy task suitable for a junior developer with basic Go knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add Page method/field to taxonomy object\nWe can get a reference to a _term_ page with:\r\n\r\n```text\r\n{{ $termPage := site.Taxonomies.tags.foo.Page }}\r\n```\r\n\r\nIt would be convenient if we could get a reference to a _taxonomy_ page with:\r\n\r\n```text\r\n{{ $taxonomyPage := site.Taxonomies.tags.Page }}\r\n```\r\n\r\nThat may not be possible, despite the fact we lowercase terms internally (e.g., there shouldn't be a collision between `tags.page` (a term) and `tags.Page` (a method/field).\r\n\r\nThe alternative has always seemed less elegant compared to how you can get a reference to a term page:\r\n\r\n```text\r\n{{ site.GetPage \"/tags\" }}\r\n```\n", "patch": "diff --git a/resources/page/taxonomy.go b/resources/page/taxonomy.go\nindex 7258ec197c0..2732c80402f 100644\n--- a/resources/page/taxonomy.go\n+++ b/resources/page/taxonomy.go\n@@ -112,6 +112,14 @@ func (i Taxonomy) ByCount() OrderedTaxonomy {\n \treturn ia\n }\n \n+// Page returns the taxonomy page or nil if the taxonomy has no terms.\n+func (i Taxonomy) Page() Page {\n+\tfor _, v := range i {\n+\t\treturn v.Page().Parent()\n+\t}\n+\treturn nil\n+}\n+\n // Pages returns the Pages for this taxonomy.\n func (ie OrderedTaxonomyEntry) Pages() Pages {\n \treturn ie.WeightedPages.Pages()\n", "instance_id": "gohugoio__hugo-12363", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a method or field to the taxonomy object to retrieve a reference to a taxonomy page. It provides context by comparing the desired functionality (`site.Taxonomies.tags.Page`) with existing functionality for term pages and an alternative approach using `site.GetPage`. However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes a \"taxonomy page\" in the context of the codebase or how it relates to term pages beyond a parent-child relationship implied in the code change. Additionally, edge cases (e.g., what happens if the taxonomy has no terms or if the parent page cannot be determined) are not addressed in the problem statement, though the code change partially handles this by returning `nil`. Overall, the intent is understandable, but some clarification on terminology and edge case expectations would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. The code change is confined to a single file (`taxonomy.go`) and involves adding a small, straightforward method (`Page()`) to the `Taxonomy` type. The logic in the method is simple: it iterates over the taxonomy entries and returns the parent of the first term's page, or `nil` if no entries exist. This requires basic understanding of the codebase's page structure and the relationship between taxonomy terms and pages, but no complex algorithms, design patterns, or advanced language features are involved. The scope of the change is minimal, with no impact on the broader system architecture or interactions between modules. Edge case handling is implicitly addressed by returning `nil` when no terms are present, and no additional error handling or performance considerations are required. Overall, this task demands only a basic understanding of the existing code and a simple modification, making it suitable for a developer with moderate experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Link the short form syntax of `depends_on` to the defined service for `textDocument/documentHighlight` requests\nWe should highlight the two instances of `db` and `redis` correctly if we receive a `textDocument/documentHighlight` request. We should also return a single element if the cursor is on `web`.\n```YAML\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n  redis:\n    image: redis\n  db:\n    image: postgres\n```\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex cce041e..6a78e5c 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,6 +14,8 @@ All notable changes to the Docker Language Server will be documented in this fil\n     - suggest dependent volume names for the `volumes` attribute ([#133](https://github.com/docker/docker-language-server/issues/133))\n     - suggest dependent config names for the `configs` attribute ([#134](https://github.com/docker/docker-language-server/issues/134))\n     - suggest dependent secret names for the `secrets` attribute ([#135](https://github.com/docker/docker-language-server/issues/135))\n+  - textDocument/documentHighlight\n+    - support highlighting the short form `depends_on` syntax for services ([#70](https://github.com/docker/docker-language-server/issues/70))\n \n ### Fixed\n \ndiff --git a/internal/compose/documentHighlight.go b/internal/compose/documentHighlight.go\nnew file mode 100644\nindex 0000000..b5f0a0d\n--- /dev/null\n+++ b/internal/compose/documentHighlight.go\n@@ -0,0 +1,134 @@\n+package compose\n+\n+import (\n+\t\"github.com/docker/docker-language-server/internal/pkg/document\"\n+\t\"github.com/docker/docker-language-server/internal/tliron/glsp/protocol\"\n+\t\"github.com/goccy/go-yaml/ast\"\n+\t\"github.com/goccy/go-yaml/token\"\n+)\n+\n+func serviceReferences(node *ast.MappingValueNode, dependencyAttributeName string) []*token.Token {\n+\tif servicesNode, ok := node.Value.(*ast.MappingNode); ok {\n+\t\ttokens := []*token.Token{}\n+\t\tfor _, serviceNode := range servicesNode.Values {\n+\t\t\tif serviceAttributes, ok := serviceNode.Value.(*ast.MappingNode); ok {\n+\t\t\t\tfor _, attributeNode := range serviceAttributes.Values {\n+\t\t\t\t\tif attributeNode.Key.GetToken().Value == dependencyAttributeName {\n+\t\t\t\t\t\tif sequenceNode, ok := attributeNode.Value.(*ast.SequenceNode); ok {\n+\t\t\t\t\t\t\tfor _, service := range sequenceNode.Values {\n+\t\t\t\t\t\t\t\ttokens = append(tokens, service.GetToken())\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn tokens\n+\t}\n+\treturn nil\n+}\n+\n+func declarations(node *ast.MappingValueNode, dependencyType string) []*token.Token {\n+\tif s, ok := node.Key.(*ast.StringNode); ok && s.Value == dependencyType {\n+\t\tif servicesNode, ok := node.Value.(*ast.MappingNode); ok {\n+\t\t\ttokens := []*token.Token{}\n+\t\t\tfor _, serviceNode := range servicesNode.Values {\n+\t\t\t\ttokens = append(tokens, serviceNode.Key.GetToken())\n+\t\t\t}\n+\t\t\treturn tokens\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+func DocumentHighlight(doc document.ComposeDocument, position protocol.Position) ([]protocol.DocumentHighlight, error) {\n+\tfile := doc.File()\n+\tif file == nil || len(file.Docs) == 0 {\n+\t\treturn nil, nil\n+\t}\n+\n+\tline := int(position.Line) + 1\n+\tcharacter := int(position.Character) + 1\n+\tif mappingNode, ok := file.Docs[0].Body.(*ast.MappingNode); ok {\n+\t\tfor _, node := range mappingNode.Values {\n+\t\t\tif s, ok := node.Key.(*ast.StringNode); ok {\n+\t\t\t\tswitch s.Value {\n+\t\t\t\tcase \"services\":\n+\t\t\t\t\trefs := serviceReferences(node, \"depends_on\")\n+\t\t\t\t\tdecls := declarations(node, \"services\")\n+\t\t\t\t\thighlights := highlightServiceReferences(refs, decls, line, character)\n+\t\t\t\t\tif len(highlights) > 0 {\n+\t\t\t\t\t\treturn highlights, nil\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil, nil\n+}\n+\n+func highlightServiceReferences(refs, decls []*token.Token, line, character int) []protocol.DocumentHighlight {\n+\tvar match *token.Token\n+\tfor _, reference := range refs {\n+\t\tif inToken(reference, line, character) {\n+\t\t\tmatch = reference\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\tif match == nil {\n+\t\tfor _, declaration := range decls {\n+\t\t\tif inToken(declaration, line, character) {\n+\t\t\t\tmatch = declaration\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif match != nil {\n+\t\thighlights := []protocol.DocumentHighlight{}\n+\t\tfor _, reference := range refs {\n+\t\t\tif reference.Value == match.Value {\n+\t\t\t\thighlights = append(highlights, documentHighlightFromToken(reference, protocol.DocumentHighlightKindRead))\n+\t\t\t}\n+\t\t}\n+\n+\t\tfor _, declaration := range decls {\n+\t\t\tif declaration.Value == match.Value {\n+\t\t\t\thighlights = append(highlights, documentHighlightFromToken(declaration, protocol.DocumentHighlightKindWrite))\n+\t\t\t}\n+\t\t}\n+\t\treturn highlights\n+\t}\n+\treturn nil\n+}\n+\n+func documentHighlightFromToken(t *token.Token, kind protocol.DocumentHighlightKind) protocol.DocumentHighlight {\n+\treturn documentHighlight(\n+\t\tprotocol.UInteger(t.Position.Line)-1,\n+\t\tprotocol.UInteger(t.Position.Column)-1,\n+\t\tprotocol.UInteger(t.Position.Line)-1,\n+\t\tprotocol.UInteger(t.Position.Column+len(t.Value))-1,\n+\t\tkind,\n+\t)\n+}\n+\n+func documentHighlight(startLine, startCharacter, endLine, endCharacter protocol.UInteger, kind protocol.DocumentHighlightKind) protocol.DocumentHighlight {\n+\treturn protocol.DocumentHighlight{\n+\t\tKind: &kind,\n+\t\tRange: protocol.Range{\n+\t\t\tStart: protocol.Position{\n+\t\t\t\tLine:      startLine,\n+\t\t\t\tCharacter: startCharacter,\n+\t\t\t},\n+\t\t\tEnd: protocol.Position{\n+\t\t\t\tLine:      endLine,\n+\t\t\t\tCharacter: endCharacter,\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+func inToken(t *token.Token, line, character int) bool {\n+\treturn t.Position.Line == line && t.Position.Column <= character && character <= t.Position.Column+len(t.Value)\n+}\ndiff --git a/internal/pkg/server/documentHighlight.go b/internal/pkg/server/documentHighlight.go\nindex c85881a..27007fd 100644\n--- a/internal/pkg/server/documentHighlight.go\n+++ b/internal/pkg/server/documentHighlight.go\n@@ -2,6 +2,7 @@ package server\n \n import (\n \t\"github.com/docker/docker-language-server/internal/bake/hcl\"\n+\t\"github.com/docker/docker-language-server/internal/compose\"\n \t\"github.com/docker/docker-language-server/internal/pkg/document\"\n \t\"github.com/docker/docker-language-server/internal/tliron/glsp\"\n \t\"github.com/docker/docker-language-server/internal/tliron/glsp/protocol\"\n@@ -16,6 +17,8 @@ func (s *Server) TextDocumentDocumentHighlight(ctx *glsp.Context, params *protoc\n \tdefer doc.Close()\n \tif doc.LanguageIdentifier() == protocol.DockerBakeLanguage {\n \t\treturn hcl.DocumentHighlight(doc.(document.BakeHCLDocument), params.Position)\n+\t} else if doc.LanguageIdentifier() == protocol.DockerComposeLanguage {\n+\t\treturn compose.DocumentHighlight(doc.(document.ComposeDocument), params.Position)\n \t}\n \treturn nil, nil\n }\n", "instance_id": "docker__docker-language-server-143", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in defining the goal: to implement highlighting for the `depends_on` short form syntax in a YAML file for `textDocument/documentHighlight` requests within the Docker Language Server. It provides a specific example in YAML format and mentions the expected behavior (highlighting instances of `db` and `redis`, and returning a single element for `web`). However, there are minor ambiguities and missing details. For instance, it does not explicitly define what constitutes a \"highlight\" (e.g., whether it includes visual styling or just range selection), nor does it specify how to handle cases where the cursor is on an undefined service or other parts of the YAML file. Additionally, edge cases such as malformed YAML or nested structures are not addressed. While the intent is understandable, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves creating a new file (`documentHighlight.go`) with significant logic (134 lines) and a small modification to an existing file (`server/documentHighlight.go`) to integrate the new functionality, indicating a moderate impact on the codebase. Second, the technical concepts required include parsing YAML using the `goccy/go-yaml` library, understanding the Language Server Protocol (LSP) for `textDocument/documentHighlight`, and implementing logic to traverse an AST (Abstract Syntax Tree) to identify and match service references and declarations. These concepts are moderately complex, requiring a solid understanding of data structures and text processing. Third, the problem involves handling specific cursor positions and token matching, which introduces some complexity in edge case handling (e.g., ensuring correct token boundaries and handling cases where no match is found), though these are not extensively detailed in the problem statement. Overall, while the problem does not require deep architectural changes or advanced domain-specific knowledge, it demands a good grasp of multiple concepts and careful implementation, justifying a score of 0.55 in the medium difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Create a table for AWS Cost Optimization Hub recommendations\n**Is your feature request related to a problem? Please describe.**\nAWS has the Cost Optimization Hub (COH) under Billing and Cost Management to provide recommendations of where to save money.  Example recommendations are around moving to a different EBS volume type, decreasing memory for a Lambda, or purchasing a compute saving plan.  This request is to ask for COH recommendations to be included in a Steampipe table.  This could be from the payor account of an AWS Organization or from all member accounts.\n\nAn example CLI call is with \"aws cost-optimization-hub list-recommendations\"\n\n**Describe the solution you'd like**\nOne additional table, with a row for each account and a JSON structure of each finding and its cost savings potentials.\n\n**Describe alternatives you've considered**\nOur alternative to Pipes is to use a Lambda with boto3 calls to the payor account, or to all accounts, to collect the recommendations and display in a Quicksight report.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n\n", "patch": "diff --git a/aws/plugin.go b/aws/plugin.go\nindex c9ea62e0f..46d1732a0 100644\n--- a/aws/plugin.go\n+++ b/aws/plugin.go\n@@ -179,6 +179,7 @@ func Plugin(ctx context.Context) *plugin.Plugin {\n \t\t\t\"aws_cost_by_tag\":                                              tableAwsCostByTag(ctx),\n \t\t\t\"aws_cost_forecast_daily\":                                      tableAwsCostForecastDaily(ctx),\n \t\t\t\"aws_cost_forecast_monthly\":                                    tableAwsCostForecastMonthly(ctx),\n+\t\t\t\"aws_costoptimizationhub_recommendation\":                       tableAwsCostOptimizationHubRecommendation(ctx),\n \t\t\t\"aws_cost_usage\":                                               tableAwsCostAndUsage(ctx),\n \t\t\t\"aws_dax_cluster\":                                              tableAwsDaxCluster(ctx),\n \t\t\t\"aws_dax_parameter\":                                            tableAwsDaxParameter(ctx),\ndiff --git a/aws/service.go b/aws/service.go\nindex 040916daa..f1b6a6dae 100644\n--- a/aws/service.go\n+++ b/aws/service.go\n@@ -50,6 +50,7 @@ import (\n \t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\n \t\"github.com/aws/aws-sdk-go-v2/service/configservice\"\n \t\"github.com/aws/aws-sdk-go-v2/service/costexplorer\"\n+\t\"github.com/aws/aws-sdk-go-v2/service/costoptimizationhub\"\n \t\"github.com/aws/aws-sdk-go-v2/service/databasemigrationservice\"\n \t\"github.com/aws/aws-sdk-go-v2/service/dax\"\n \t\"github.com/aws/aws-sdk-go-v2/service/directoryservice\"\n@@ -570,6 +571,15 @@ func CostExplorerClient(ctx context.Context, d *plugin.QueryData) (*costexplorer\n \treturn costexplorer.NewFromConfig(*cfg), nil\n }\n \n+func CostOptimizationHubClient(ctx context.Context, d *plugin.QueryData) (*costoptimizationhub.Client, error) {\n+\tcfg, err := getClientForDefaultRegion(ctx, d)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn costoptimizationhub.NewFromConfig(*cfg), nil\n+}\n+\n+\n func DatabaseMigrationClient(ctx context.Context, d *plugin.QueryData) (*databasemigrationservice.Client, error) {\n \tcfg, err := getClientForQueryRegion(ctx, d)\n \tif err != nil {\ndiff --git a/aws/table_aws_costoptimizationhub_recommendation.go b/aws/table_aws_costoptimizationhub_recommendation.go\nnew file mode 100644\nindex 000000000..a3464b407\n--- /dev/null\n+++ b/aws/table_aws_costoptimizationhub_recommendation.go\n@@ -0,0 +1,350 @@\n+package aws\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\n+\t\"github.com/aws/aws-sdk-go-v2/service/costoptimizationhub\"\n+\t\"github.com/aws/aws-sdk-go-v2/service/costoptimizationhub/types\"\n+\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/grpc/proto\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin\"\n+\t\"github.com/turbot/steampipe-plugin-sdk/v5/plugin/transform\"\n+)\n+\n+func tableAwsCostOptimizationHubRecommendation(_ context.Context) *plugin.Table {\n+\treturn &plugin.Table{\n+\t\tName:        \"aws_costoptimizationhub_recommendation\",\n+\t\tDescription: \"AWS Cost Optimization Hub Recommendation\",\n+\t\tList: &plugin.ListConfig{\n+\t\t\tHydrate: listCostOptimizationHubRecommendations,\n+\t\t\tKeyColumns: plugin.KeyColumnSlice{\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"recommendation_account_id\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"action_type\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"implementation_effort\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"recommendation_id\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"resource_region\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"resource_arn\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"resource_id\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"current_resource_type\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"recommended_resource_type\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"restart_needed\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:    \"rollback_possible\",\n+\t\t\t\t\tRequire: plugin.Optional,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tTags: map[string]string{\"service\": \"cost-optimization-hub\", \"action\": \"ListRecommendations\"},\n+\t\t},\n+\t\tColumns: awsGlobalRegionColumns(\n+\t\t\t[]*plugin.Column{\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"recommendation_id\",\n+\t\t\t\t\tDescription: \"The ID for the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"resource_arn\",\n+\t\t\t\t\tDescription: \"The Amazon Resource Name (ARN) for the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"resource_id\",\n+\t\t\t\t\tDescription: \"The resource ID for the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\n+\t\t\t\t// We have a common column named \"account_id\" for all the tables that represents current caller account ID, so renamed it to \"recommendation_account_id\" to avoid ambiguity.\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"recommendation_account_id\",\n+\t\t\t\t\tDescription: \"The account that the recommendation is for.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t\tTransform:   transform.FromField(\"AccountId\"),\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"action_type\",\n+\t\t\t\t\tDescription: \"The type of tasks that can be carried out by this action.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"currency_code\",\n+\t\t\t\t\tDescription: \"The currency code used for the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"current_resource_summary\",\n+\t\t\t\t\tDescription: \"Describes the current resource.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"current_resource_type\",\n+\t\t\t\t\tDescription: \"The current resource type.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"estimated_monthly_cost\",\n+\t\t\t\t\tDescription: \"The estimated monthly cost for the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_DOUBLE,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"estimated_monthly_savings\",\n+\t\t\t\t\tDescription: \"The estimated monthly savings amount for the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_DOUBLE,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"estimated_savings_percentage\",\n+\t\t\t\t\tDescription: \"The estimated savings percentage relative to the total cost over the cost calculation lookback period.\",\n+\t\t\t\t\tType:        proto.ColumnType_DOUBLE,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"implementation_effort\",\n+\t\t\t\t\tDescription: \"The effort required to implement the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"last_refresh_timestamp\",\n+\t\t\t\t\tDescription: \"The time when the recommendation was last generated.\",\n+\t\t\t\t\tType:        proto.ColumnType_TIMESTAMP,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"recommendation_lookback_period_in_days\",\n+\t\t\t\t\tDescription: \"The lookback period that's used to generate the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_INT,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"recommended_resource_summary\",\n+\t\t\t\t\tDescription: \"Describes the recommended resource.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"recommended_resource_type\",\n+\t\t\t\t\tDescription: \"Describes the recommended resource.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"resource_region\",\n+\t\t\t\t\tDescription: \"The Amazon Web Services Region of the resource.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t\tTransform:   transform.FromField(\"Region\"),\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"restart_needed\",\n+\t\t\t\t\tDescription: \"Whether or not implementing the recommendation requires a restart.\",\n+\t\t\t\t\tType:        proto.ColumnType_BOOL,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"rollback_possible\",\n+\t\t\t\t\tDescription: \"Whether or not implementing the recommendation can be rolled back.\",\n+\t\t\t\t\tType:        proto.ColumnType_BOOL,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"source\",\n+\t\t\t\t\tDescription: \"The source of the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"current_resource_details\",\n+\t\t\t\t\tDescription: \"The details for the resource.\",\n+\t\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\t\tHydrate:     getCostOptimizationHubRecommendations,\n+\t\t\t\t\tTransform:   transform.FromField(\"CurrentResourceDetails\"),\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"recommended_resource_details\",\n+\t\t\t\t\tDescription: \"The details about the recommended resource.\",\n+\t\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\t\tHydrate:     getCostOptimizationHubRecommendations,\n+\t\t\t\t\tTransform:   transform.FromField(\"RecommendedResourceDetails\"),\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"tags_src\",\n+\t\t\t\t\tDescription: \"A list of tags assigned to the recommendation.\",\n+\t\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\t},\n+\n+\t\t\t\t// Steampipe standard columns\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"tags\",\n+\t\t\t\t\tDescription: resourceInterfaceDescription(\"tags\"),\n+\t\t\t\t\tType:        proto.ColumnType_JSON,\n+\t\t\t\t\tTransform:   transform.From(costOptimizationRecommendationTurbotTags),\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tName:        \"title\",\n+\t\t\t\t\tDescription: resourceInterfaceDescription(\"title\"),\n+\t\t\t\t\tType:        proto.ColumnType_STRING,\n+\t\t\t\t\tTransform:   transform.FromField(\"RecommendationId\"),\n+\t\t\t\t},\n+\t\t\t}),\n+\t}\n+}\n+\n+//// LIST FUNCTION\n+\n+func listCostOptimizationHubRecommendations(ctx context.Context, d *plugin.QueryData, _ *plugin.HydrateData) (interface{}, error) {\n+\tparams := buildCostOptimizationHubRecommendationInputFromQuals(d.Quals)\n+\n+\t// Create Client\n+\tsvc, err := CostOptimizationHubClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_costoptimizationhub_recommendation.listCostOptimizationHubRecommendations\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\t// Limiting the results\n+\tmaxLimit := int32(1000)\n+\tif d.QueryContext.Limit != nil {\n+\t\tlimit := int32(*d.QueryContext.Limit)\n+\t\tif limit < maxLimit {\n+\t\t\tmaxLimit = limit\n+\t\t}\n+\t}\n+\n+\tinput := &costoptimizationhub.ListRecommendationsInput{\n+\t\tMaxResults: &maxLimit,\n+\t}\n+\n+\tif params != nil {\n+\t\tinput.Filter = params\n+\t}\n+\n+\tpaginator := costoptimizationhub.NewListRecommendationsPaginator(svc, input, func(o *costoptimizationhub.ListRecommendationsPaginatorOptions) {\n+\t\to.Limit = maxLimit\n+\t\to.StopOnDuplicateToken = true\n+\t})\n+\n+\tfor paginator.HasMorePages() {\n+\t\td.WaitForListRateLimit(ctx)\n+\n+\t\toutput, err := paginator.NextPage(ctx)\n+\t\tif err != nil {\n+\t\t\tplugin.Logger(ctx).Error(\"aws_costoptimizationhub_recommendation.listCostOptimizationHubRecommendations\", \"api_error\", err)\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tfor _, item := range output.Items {\n+\t\t\td.StreamListItem(ctx, item)\n+\n+\t\t\t// Context can be cancelled due to manual cancellation or the limit has been hit\n+\t\t\tif d.RowsRemaining(ctx) == 0 {\n+\t\t\t\treturn nil, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil, nil\n+}\n+\n+//// HYDRATE FUNCTIONS\n+\n+func getCostOptimizationHubRecommendations(ctx context.Context, d *plugin.QueryData, h *plugin.HydrateData) (interface{}, error) {\n+\trecommendation := h.Item.(types.Recommendation)\n+\n+\t// Create Client\n+\tsvc, err := CostOptimizationHubClient(ctx, d)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_costoptimizationhub_recommendation.getCostOptimizationHubRecommendations\", \"connection_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\tinput := &costoptimizationhub.GetRecommendationInput{\n+\t\tRecommendationId: recommendation.RecommendationId,\n+\t}\n+\n+\tresult, err := svc.GetRecommendation(ctx, input)\n+\tif err != nil {\n+\t\tplugin.Logger(ctx).Error(\"aws_costoptimizationhub_recommendation.getCostOptimizationHubRecommendations\", \"api_error\", err)\n+\t\treturn nil, err\n+\t}\n+\n+\treturn result, nil\n+}\n+\n+//// TRANSFORM FUNCTIONS\n+\n+func costOptimizationRecommendationTurbotTags(_ context.Context, d *transform.TransformData) (interface{}, error) {\n+\tr := d.HydrateItem.(types.Recommendation)\n+\tvar turbotTagsMap map[string]string\n+\tif r.Tags != nil {\n+\t\tturbotTagsMap = map[string]string{}\n+\t\tfor _, i := range r.Tags {\n+\t\t\tturbotTagsMap[*i.Key] = *i.Value\n+\t\t}\n+\t}\n+\treturn turbotTagsMap, nil\n+}\n+\n+////  Build input parameter fot list API call\n+\n+func buildCostOptimizationHubRecommendationInputFromQuals(quals plugin.KeyColumnQualMap) *types.Filter {\n+\tparam := &types.Filter{}\n+\tfilterQuals := []string{\"recommendation_account_id\", \"action_type\", \"implementation_effort\", \"recommendation_id\", \"resource_region\", \"resource_arn\", \"resource_id\", \"current_resource_type\", \"recommended_resource_type\", \"restart_needed\", \"rollback_possible\"}\n+\n+\tfor _, columnName := range filterQuals {\n+\t\tif quals[columnName] != nil {\n+\t\t\tswitch columnName {\n+\t\t\tcase \"restart_needed\", \"rollback_possible\":\n+\t\t\t\tvalue := getQualsValueByColumn(quals, columnName, \"boolean\")\n+\t\t\t\tval := value.(string) == \"true\"\n+\t\t\t\tif columnName == \"restart_needed\" {\n+\t\t\t\t\tparam.RestartNeeded = &val\n+\t\t\t\t}\n+\t\t\t\tif columnName == \"rollback_possible\" {\n+\t\t\t\t\tparam.RollbackPossible = &val\n+\t\t\t\t}\n+\t\t\tdefault:\n+\t\t\t\tvalue := getQualsValueByColumn(quals, columnName, \"string\")\n+\t\t\t\tswitch columnName {\n+\t\t\t\tcase \"recommendation_account_id\":\n+\t\t\t\t\tparam.AccountIds = []string{fmt.Sprint(value)}\n+\t\t\t\tcase \"recommendation_id\":\n+\t\t\t\t\tparam.RecommendationIds = []string{fmt.Sprint(value)}\n+\t\t\t\tcase \"resource_region\":\n+\t\t\t\t\tparam.Regions = []string{fmt.Sprint(value)}\n+\t\t\t\tcase \"resource_arn\":\n+\t\t\t\t\tparam.ResourceArns = []string{fmt.Sprint(value)}\n+\t\t\t\tcase \"resource_id\":\n+\t\t\t\t\tparam.ResourceIds = []string{fmt.Sprint(value)}\n+\t\t\t\tcase \"implementation_effort\":\n+\t\t\t\t\tparam.ImplementationEfforts = []types.ImplementationEffort{types.ImplementationEffort(value.(string))}\n+\t\t\t\tcase \"action_type\":\n+\t\t\t\t\tparam.ActionTypes = []types.ActionType{types.ActionType(value.(string))}\n+\t\t\t\tcase \"current_resource_type\", \"recommended_resource_type\":\n+\t\t\t\t\tparam.ResourceTypes = []types.ResourceType{types.ResourceType(value.(string))}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn param\n+}\ndiff --git a/docs/tables/aws_costoptimizationhub_recommendation.md b/docs/tables/aws_costoptimizationhub_recommendation.md\nnew file mode 100644\nindex 000000000..ac60785e7\n--- /dev/null\n+++ b/docs/tables/aws_costoptimizationhub_recommendation.md\n@@ -0,0 +1,184 @@\n+---\n+title: \"Steampipe Table: aws_costoptimizationhub_recommendation - Query AWS Cost Optimization Recommendations using SQL\"\n+description: \"Allows users to query AWS Cost Optimization Hub Recommendations to obtain insights on cost-saving opportunities, resource configuration, and associated metadata.\"\n+---\n+\n+# Table: aws_costoptimizationhub_recommendation - Query AWS Cost Optimization Recommendations using SQL\n+\n+The AWS Cost Optimization Hub provides recommendations for reducing costs and optimizing the usage of AWS resources. These recommendations are based on usage patterns, cost analysis, and resource configurations, helping organizations achieve better cost efficiency.\n+\n+## Table Usage Guide\n+\n+The `aws_costoptimizationhub_recommendation` table in Steampipe allows you to query detailed cost optimization recommendations. This table helps DevOps engineers, cost analysts, or financial professionals identify potential savings, understand implementation efforts, and track the effectiveness of recommendations.\n+\n+The schema outlines various attributes of the cost optimization recommendations, including estimated savings, resource details, recommendation types, and implementation efforts. It also provides timestamps for the last refresh and additional metadata such as tags and ARNs.\n+\n+**Important Notes**\n+- This table supports optional quals. Queries with optional quals are optimized to use additional filtering provided by the AWS API function to narrow down the results for better query performance.. Optional quals are supported for the following columns:\n+  - `recommendation_account_id`\n+  - `action_type`\n+  - `implementation_effort`\n+  - `recommendation_id`\n+  - `resource_region`\n+  - `resource_arn`\n+  - `resource_id`\n+  - `current_resource_type`\n+  - `recommended_resource_type`\n+  - `restart_needed`\n+  - `rollback_possible`\n+\n+## Examples\n+\n+### Basic info\n+Retrieve the basic details of cost optimization recommendations, including the resource and estimated savings.\n+\n+```sql+postgres\n+select\n+  recommendation_id,\n+  resource_id,\n+  estimated_monthly_savings,\n+  estimated_savings_percentage,\n+  implementation_effort\n+from\n+  aws_costoptimizationhub_recommendation;\n+```\n+\n+```sql+sqlite\n+select\n+  recommendation_id,\n+  resource_id,\n+  estimated_monthly_savings,\n+  estimated_savings_percentage,\n+  implementation_effort\n+from\n+  aws_costoptimizationhub_recommendation;\n+```\n+\n+### List recommendations with significant savings\n+Identify recommendations where the estimated savings percentage is greater than 50%. This helps prioritize high-impact cost optimization opportunities.\n+\n+```sql+postgres\n+select\n+  recommendation_id,\n+  resource_id,\n+  estimated_monthly_savings,\n+  estimated_savings_percentage\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  estimated_savings_percentage > 50;\n+```\n+\n+```sql+sqlite\n+select\n+  recommendation_id,\n+  resource_id,\n+  estimated_monthly_savings,\n+  estimated_savings_percentage\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  estimated_savings_percentage > 50;\n+```\n+\n+### List recommendations requiring a resource restart\n+Find recommendations that require a restart to implement. This query helps in planning implementation efforts and minimizing downtime.\n+\n+```sql+postgres\n+select\n+  recommendation_id,\n+  resource_id,\n+  implementation_effort,\n+  restart_needed\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  restart_needed = true;\n+```\n+\n+```sql+sqlite\n+select\n+  recommendation_id,\n+  resource_id,\n+  implementation_effort,\n+  restart_needed\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  restart_needed = 1;\n+```\n+\n+### Get recommendations by resource type\n+Filter recommendations based on specific resource types, such as EC2 or RDS, to analyze opportunities for optimizing particular services.\n+\n+```sql+postgres\n+select\n+  recommendation_id,\n+  resource_id,\n+  current_resource_type,\n+  recommended_resource_type\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  current_resource_type = 'EC2';\n+```\n+\n+```sql+sqlite\n+select\n+  recommendation_id,\n+  resource_id,\n+  current_resource_type,\n+  recommended_resource_type\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  current_resource_type = 'EC2';\n+```\n+\n+### List recommendations refreshed in the last 30 days\n+Track recently updated recommendations to stay up-to-date with the latest cost optimization insights.\n+\n+```sql+postgres\n+select\n+  recommendation_id,\n+  resource_id,\n+  last_refresh_timestamp\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  last_refresh_timestamp > now() - interval '30 days';\n+```\n+\n+```sql+sqlite\n+select\n+  recommendation_id,\n+  resource_id,\n+  last_refresh_timestamp\n+from\n+  aws_costoptimizationhub_recommendation\n+where\n+  last_refresh_timestamp > datetime('now','-30 days');\n+```\n+\n+### Get the tags associated with a recommendation\n+Retrieve tags assigned to recommendations to better organize and manage resources.\n+\n+```sql+postgres\n+select\n+  recommendation_id,\n+  resource_id,\n+  jsonb_each_text(tags) as tag\n+from\n+  aws_costoptimizationhub_recommendation;\n+```\n+\n+```sql+sqlite\n+select\n+  recommendation_id,\n+  resource_id,\n+  json_each(tags_src).key as tag_key,\n+  json_each(tags_src).value as tag_value\n+from\n+  aws_costoptimizationhub_recommendation;\n+```\n+\ndiff --git a/go.mod b/go.mod\nindex 4f7ef1e83..afa6da6b2 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -43,6 +43,7 @@ require (\n \tgithub.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider v1.36.3\n \tgithub.com/aws/aws-sdk-go-v2/service/configservice v1.46.4\n \tgithub.com/aws/aws-sdk-go-v2/service/costexplorer v1.37.1\n+\tgithub.com/aws/aws-sdk-go-v2/service/costoptimizationhub v1.4.7\n \tgithub.com/aws/aws-sdk-go-v2/service/databasemigrationservice v1.38.4\n \tgithub.com/aws/aws-sdk-go-v2/service/dax v1.19.4\n \tgithub.com/aws/aws-sdk-go-v2/service/directoryservice v1.24.4\n@@ -118,6 +119,7 @@ require (\n \tgithub.com/aws/aws-sdk-go-v2/service/servicequotas v1.21.4\n \tgithub.com/aws/aws-sdk-go-v2/service/ses v1.22.4\n \tgithub.com/aws/aws-sdk-go-v2/service/sfn v1.26.4\n+\tgithub.com/aws/aws-sdk-go-v2/service/shield v1.25.7\n \tgithub.com/aws/aws-sdk-go-v2/service/simspaceweaver v1.10.4\n \tgithub.com/aws/aws-sdk-go-v2/service/sns v1.29.4\n \tgithub.com/aws/aws-sdk-go-v2/service/sqs v1.31.4\n@@ -132,7 +134,7 @@ require (\n \tgithub.com/aws/aws-sdk-go-v2/service/wafv2 v1.48.2\n \tgithub.com/aws/aws-sdk-go-v2/service/wellarchitected v1.29.4\n \tgithub.com/aws/aws-sdk-go-v2/service/workspaces v1.38.4\n-\tgithub.com/aws/smithy-go v1.20.4\n+\tgithub.com/aws/smithy-go v1.22.1\n \tgithub.com/gocarina/gocsv v0.0.0-20201208093247-67c824bc04d4\n \tgithub.com/goccy/go-yaml v1.11.3\n \tgithub.com/golang/protobuf v1.5.4\n@@ -147,7 +149,6 @@ require (\n require golang.org/x/xerrors v0.0.0-20220907171357-04be3eba64a2 // indirect\n \n require (\n-\tgithub.com/aws/aws-sdk-go-v2/service/shield v1.25.7 // indirect\n \tgithub.com/felixge/httpsnoop v1.0.4 // indirect\n \tgo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.47.0 // indirect\n \tgo.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.47.0 // indirect\ndiff --git a/go.sum b/go.sum\nindex db85cbafe..23f0d205e 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -291,6 +291,8 @@ github.com/aws/aws-sdk-go-v2/service/configservice v1.46.4 h1:bmVA/LmysEu6gOplmz\n github.com/aws/aws-sdk-go-v2/service/configservice v1.46.4/go.mod h1:WCD4Psga99kZmdqPGJ88SURa6UMa4WgqpqzY5vP2ZS0=\n github.com/aws/aws-sdk-go-v2/service/costexplorer v1.37.1 h1:xjhk+io+kPtDOG5RizvHlkGKET3dxRBzorLdPPkpZQc=\n github.com/aws/aws-sdk-go-v2/service/costexplorer v1.37.1/go.mod h1:uLOg0o57AyQQhZGtUKIlcBJOKE53mO9bXKyrM9dFhy4=\n+github.com/aws/aws-sdk-go-v2/service/costoptimizationhub v1.4.7 h1:MGeK6VW2qK2jY5mG0a5VyJ9AFwxjQUumkZcUK/C1UDA=\n+github.com/aws/aws-sdk-go-v2/service/costoptimizationhub v1.4.7/go.mod h1:dD0mbm64tfE2DRlIVEKg0dXb9qyf+qZtNitsR8CvMVM=\n github.com/aws/aws-sdk-go-v2/service/databasemigrationservice v1.38.4 h1:ot9PKavvbeEg3eofQdkpJWrf8DR90S9wx1OirBUComU=\n github.com/aws/aws-sdk-go-v2/service/databasemigrationservice v1.38.4/go.mod h1:hTZS15Gghi40UxU03Cv09Qr2tXgoQrZOSGY6oaNUNAg=\n github.com/aws/aws-sdk-go-v2/service/dax v1.19.4 h1:S3mvtYjRVVsg1R4EuV1LWZUiD72t+pfnBbK8TL7zEmo=\n@@ -487,8 +489,8 @@ github.com/aws/aws-sdk-go-v2/service/wellarchitected v1.29.4 h1:OuFs453KXWTLBkem\n github.com/aws/aws-sdk-go-v2/service/wellarchitected v1.29.4/go.mod h1:MRT/P9Cwn+7xCCVpD1sTvUESiWMAc9hA+FooRsW5fe8=\n github.com/aws/aws-sdk-go-v2/service/workspaces v1.38.4 h1:SvHYikdxmnyptMebU3zFfXbfU96SHzdUX+KXqa6pjYE=\n github.com/aws/aws-sdk-go-v2/service/workspaces v1.38.4/go.mod h1:1XK49PATLHBd7mpKqO91GqRuV7bEsmyQ8Lslvn3fFj4=\n-github.com/aws/smithy-go v1.20.4 h1:2HK1zBdPgRbjFOHlfeQZfpC4r72MOb9bZkiFwggKO+4=\n-github.com/aws/smithy-go v1.20.4/go.mod h1:irrKGvNn1InZwb2d7fkIRNucdfwR8R+Ts3wxYa/cJHg=\n+github.com/aws/smithy-go v1.22.1 h1:/HPHZQ0g7f4eUeK6HKglFz8uwVfZKgoI25rb/J+dnro=\n+github.com/aws/smithy-go v1.22.1/go.mod h1:irrKGvNn1InZwb2d7fkIRNucdfwR8R+Ts3wxYa/cJHg=\n github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\n github.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\n github.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\n", "instance_id": "turbot__steampipe-plugin-aws-2355", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding a Steampipe table for AWS Cost Optimization Hub (COH) recommendations. It specifies the purpose (to include COH recommendations in a table), provides context about AWS COH, and mentions an example CLI call (\"aws cost-optimization-hub list-recommendations\"). The desired solution of having a table with rows for each account and JSON structures for findings and savings is outlined. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the exact input parameters or output format for the table (e.g., specific fields or data types expected), nor does it mention any specific edge cases or constraints (e.g., handling large datasets, rate limits, or permissions). Additionally, while it mentions collecting data from a payor account or all member accounts, it does not clarify how this should be implemented or prioritized. These gaps prevent it from being fully comprehensive, but the intent and scope are reasonably well-defined, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes involves multiple files (e.g., `plugin.go`, `service.go`, and a new file for the table implementation), but the changes are relatively modular and follow a pattern consistent with existing Steampipe plugins for AWS services. The addition of a new table requires integrating with the AWS SDK for the Cost Optimization Hub, which involves understanding the AWS API (`ListRecommendations` and `GetRecommendation`), pagination, and filtering logic, as seen in the code. This requires familiarity with Go, the AWS SDK, and Steampipe's plugin architecture, including concepts like hydrate functions, transform functions, and query data handling. However, the implementation does not appear to significantly impact the broader system architecture or require deep refactoring of existing components. The number of technical concepts involved is moderate, including AWS API interactions, pagination, error handling, and JSON data transformation, but these are not overly complex for someone with experience in Go and AWS SDKs. Edge cases, such as rate limiting (handled via `WaitForListRateLimit`) and large result sets (handled via pagination), are addressed in the code, but the problem statement itself does not explicitly call out complex edge cases or performance considerations. Overall, this task requires a solid understanding of the Steampipe framework and AWS SDK but does not pose an exceptionally high level of complexity or risk, justifying a difficulty score of 0.50.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "User names forbidden by RFC 7617 are allowed\n[RFC 7617](https://tools.ietf.org/html/rfc7617) notes:\r\n\r\n> The user-id and password MUST NOT contain any control characters (see \"CTL\" in [Appendix B.1 of RFC5234](https://tools.ietf.org/html/rfc5234#appendix-B.1)).\r\n>\r\n> Furthermore, a user-id containing a colon character is invalid, as the first colon in a user-pass string separates user-id and password from one another; text after the first colon is part of the password. User-ids containing colons cannot be encoded in user-pass strings.\r\n\r\nWhile Basic authentication is only one of several authentication schemes supported by Miniflux, and both controls and colons are uncommonly used in usernames, it may help to avoid confusion by simply forbidding them.\n", "patch": "diff --git a/internal/validator/user.go b/internal/validator/user.go\nindex a7e05edb5cc..53220148596 100644\n--- a/internal/validator/user.go\n+++ b/internal/validator/user.go\n@@ -6,6 +6,7 @@ package validator // import \"miniflux.app/v2/internal/validator\"\n import (\n \t\"slices\"\n \t\"strings\"\n+\t\"unicode\"\n \n \t\"miniflux.app/v2/internal/locale\"\n \t\"miniflux.app/v2/internal/model\"\n@@ -22,6 +23,10 @@ func ValidateUserCreationWithPassword(store *storage.Storage, request *model.Use\n \t\treturn locale.NewLocalizedError(\"error.user_already_exists\")\n \t}\n \n+\tif err := validateUsername(request.Username); err != nil {\n+\t\treturn err\n+\t}\n+\n \tif err := validatePassword(request.Password); err != nil {\n \t\treturn err\n \t}\n@@ -146,6 +151,23 @@ func validatePassword(password string) *locale.LocalizedError {\n \treturn nil\n }\n \n+// validateUsername return an error if the `username` argument contains\n+// a character that isn't alphanumerical nor `_` and `-`.\n+func validateUsername(username string) *locale.LocalizedError {\n+\tif strings.ContainsFunc(username, func(r rune) bool {\n+\t\tif unicode.IsLetter(r) || unicode.IsNumber(r) {\n+\t\t\treturn false\n+\t\t}\n+\t\tif r == '_' || r == '-' || r == '@' || r == '.' {\n+\t\t\treturn false\n+\t\t}\n+\t\treturn true\n+\t}) {\n+\t\treturn locale.NewLocalizedError(\"error.invalid_username\")\n+\t}\n+\treturn nil\n+}\n+\n func validateTheme(theme string) *locale.LocalizedError {\n \tthemes := model.Themes()\n \tif _, found := themes[theme]; !found {\n", "instance_id": "miniflux__v2-3018", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the goal: to prevent usernames with control characters and colons as per RFC 7617 for Basic authentication in Miniflux. It provides relevant references to the RFC and explains the reasoning behind the restriction (avoiding confusion). However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes a \"control character\" beyond referencing the RFC, nor does it provide examples of invalid usernames. Additionally, it does not mention other potential restrictions or edge cases (e.g., length constraints or other special characters) that might be relevant. While the intent is clear, these omissions prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are confined to a single file (`user.go`) and involve adding a new validation function (`validateUsername`) and integrating it into the user creation process. The changes are localized and do not impact the broader system architecture or require modifications across multiple modules. The amount of code added is small (around 20 lines), making the scope minimal.\n\n2. **Number of Technical Concepts:** The solution requires basic knowledge of Go programming, specifically string manipulation using the `strings` package and Unicode character classification with the `unicode` package. The logic involves straightforward rune-based iteration and conditional checks, which are not complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic RFC compliance are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases beyond control characters and colons, but the code change handles a broader set of invalid characters by restricting usernames to alphanumeric characters, underscores, hyphens, at symbols, and periods. The error handling is simple, returning a localized error message if the validation fails. There are no complex edge cases (e.g., performance considerations for very long usernames or handling of specific Unicode ranges) explicitly required or addressed.\n\n4. **Overall Complexity:** The task involves understanding a small part of the codebase (user validation logic) and implementing a simple feature (username character validation). It does not require deep architectural knowledge or significant debugging of interactions between components.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic modifications and minimal conceptual depth. It is slightly above the \"Very Easy\" range due to the need to understand Unicode handling and integrate the validation into an existing workflow, but it remains an easy problem for a developer with moderate experience in Go.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`LeaseOutput` is slow with postgres backend\n### Issue\r\n\r\n`SendCoins` takes much longer to run with postgres as the db backend, and this itest failure has been shown multiple times,\r\n```\r\nharness_rpc.go:97: \r\n        \tError Trace:\t/home/runner/work/lnd/lnd/lntest/rpc/harness_rpc.go:97\r\n        \t            \t\t\t\t/home/runner/work/lnd/lnd/lntest/rpc/lnd.go:374\r\n        \t            \t\t\t\t/home/runner/work/lnd/lnd/itest/lnd_taproot_test.go:129\r\n        \t            \t\t\t\t/home/runner/work/lnd/lnd/itest/lnd_taproot_test.go:52\r\n        \t            \t\t\t\t/home/runner/work/lnd/lnd/lntest/harness.go:385\r\n        \t            \t\t\t\t/home/runner/work/lnd/lnd/itest/lnd_test.go:139\r\n        \tError:      \tReceived unexpected error:\r\n        \t            \trpc error: code = DeadlineExceeded desc = context deadline exceeded\r\n        \tMessages:   \tAlice: failed to call SendCoins\r\n```\r\n\r\n### Observations\r\nTurns out that when using postgres, leasing outputs take much longer, which is called [here](https://github.com/lightningnetwork/lnd/blob/master/sweep/walletsweep.go#L248) when crafting the tx for `SendCoins`.\r\n- For kvdb backend, the leasing of 100 utxos takes about 1 second.\r\n- For postgres backend, the leasing of 100 utxos takes about 40 seconds.\r\n\r\n### Steps to reproduce\r\n\r\nRunning with `make itest icase=sign_psbt/fund_and_sign_psbt backend=btcd`,\r\n```\r\n2024-06-04 03:37:30.353 [INF] RPCS: [sendcoins] addr=2MzusuQmFdFCBgtns6LjnCnQemeAhRftiku, amt=0 BTC, sat/kw=12500, min_confs=0, send_all=true\r\n2024-06-04 03:37:30.353 [DBG] SWPR: [WithCoinSelectLock] entered the lock\r\n2024-06-04 03:37:30.453 [DBG] SWPR: [WithCoinSelectLock] finished fetching UTXOs\r\n2024-06-04 03:37:30.453 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: 65734f6883a0949f4588589fda166b9339bfb77081cf77dad1ee91b5dc78c000:0\r\n...leasing 98 more...\r\n2024-06-04 03:37:31.616 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: 2fb2947606ac7bf141ae364e8ea8d805344f56a319b679efa4380e6839782bf8:0\r\n2024-06-04 03:37:31.626 [DBG] SWPR: [WithCoinSelectLock] exited the lock\r\n2024-06-04 03:37:31.656 [DBG] SWPR: Creating sweep transaction 6c4bc2d7afeecb395d8bf0654065d7f5acb52b1017e5b4a61442bbd1dd3d7207 for 100 inputs...\r\n```\r\n\r\nRunning with `make itest icase=sign_psbt/fund_and_sign_psbt backend=bitcoind`,\r\n```\r\n2024-06-04 03:46:54.785 [INF] RPCS: [sendcoins] addr=2MzDETUurmNbcnx2mLWtm4tdeARCFUkdQbU, amt=0 BTC, sat/kw=12500, min_confs=0, send_all=true\r\n2024-06-04 03:46:54.786 [DBG] SWPR: [WithCoinSelectLock] entered the lock\r\n2024-06-04 03:46:54.856 [DBG] SWPR: [WithCoinSelectLock] finished fetching UTXOs\r\n2024-06-04 03:46:54.856 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: f4436cede2c43faa7eb16cc2d4577e0825790f685b146eacfbe2a07a1006df00:0\r\n...leasing 98 more...\r\n2024-06-04 03:46:55.960 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: 8dd273ce89622e59cae55a216a33b65d581a0d9ea95a30d74138140f5a0d15ff:0\r\n2024-06-04 03:46:55.970 [DBG] SWPR: [WithCoinSelectLock] exited the lock\r\n2024-06-04 03:46:55.984 [DBG] SWPR: Creating sweep transaction c383561e0096a6099dfe5c9cd57a42a6f393cb7fd281e137e5d989c518db3627 for 100 inputs...\r\n```\r\n\r\nRunning with `make itest icase=sign_psbt/fund_and_sign_psbt backend=bitcoind dbbackend=postgres`,\r\n```\r\n2024-06-04 03:41:21.576 [INF] RPCS: [sendcoins] addr=2N6HM91LmDZP63ViD5MPirXxLScp3gmK2zf, amt=0 BTC, sat/kw=12500, min_confs=0, send_all=true\r\n2024-06-04 03:41:21.577 [DBG] SWPR: [WithCoinSelectLock] entered the lock\r\n2024-06-04 03:41:23.324 [DBG] SWPR: [WithCoinSelectLock] finished fetching UTXOs\r\n2024-06-04 03:41:23.324 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: 38fecbec097297b3e4b1e2f535e4d4707efc8d5668424f4c14e29805aa771801:0\r\n...leasing 98 more...\r\n2024-06-04 03:42:02.574 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: ad3297020e2b81993d2093ae239894a47ad3f67b243fc2f79c0f1fdf808384fc:0\r\n2024-06-04 03:42:03.284 [DBG] SWPR: [WithCoinSelectLock] exited the lock\r\n2024-06-04 03:42:03.772 [DBG] SWPR: Creating sweep transaction e50f98c1f40080ee20139d095945149a16d3334080658598bf4a5eef8ec90a7f for 100 inputs...\r\n```\r\n\r\nRunning with `make itest icase=sign_psbt/fund_and_sign_psbt backend=bitcoind dbbackend=postgres nativesql=true`,\r\n```\r\n2024-06-04 03:53:42.474 [INF] RPCS: [sendcoins] addr=2MuaBevMSc9Gxdi4WGAnuEiRXH2pGAcYNaj, amt=0 BTC, sat/kw=12500, min_confs=0, send_all=true\r\n2024-06-04 03:53:42.475 [DBG] SWPR: [WithCoinSelectLock] entered the lock\r\n2024-06-04 03:53:44.029 [DBG] SWPR: [WithCoinSelectLock] finished fetching UTXOs\r\n2024-06-04 03:53:44.029 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: 1deb82f888c58b2c9e86d0f8773537324a5b1fe2ddd2b3d3a243088cb9990000:0\r\n...leasing 98 more...\r\n2024-06-04 03:54:19.937 [DBG] SWPR: [WithCoinSelectLock] leasing utxo: a4304be189a3075d22f5cabc638af956da871b09ae46ffb48a6dd1c143d084fe:0\r\n2024-06-04 03:54:20.671 [DBG] SWPR: [WithCoinSelectLock] exited the lock\r\n2024-06-04 03:54:21.260 [DBG] SWPR: Creating sweep transaction a6d8019cb0cb3f5bb0603bcfc4676b6eef81f88301933e75bd67673650b4e1c5 for 100 inputs...\r\n```\n", "patch": "diff --git a/docs/release-notes/release-notes-0.18.3.md b/docs/release-notes/release-notes-0.18.3.md\nindex 8ba9eefadf..d60b069ff4 100644\n--- a/docs/release-notes/release-notes-0.18.3.md\n+++ b/docs/release-notes/release-notes-0.18.3.md\n@@ -151,6 +151,11 @@ commitment when the channel was force closed.\n * [Allow](https://github.com/lightningnetwork/lnd/pull/8845) multiple etcd hosts\n   to be specified in db.etcd.host.\n \n+* Improved the internal [`LeaseOutput`\n+  method](https://github.com/lightningnetwork/lnd/pull/8961) to be more\n+  efficient, which improves the performance of related RPC calls such as\n+  `LeaseOutput`, `SendCoins`, and PSBT funding process. \n+\n ## RPC Updates\n \n * [`xImportMissionControl`](https://github.com/lightningnetwork/lnd/pull/8779) \ndiff --git a/go.mod b/go.mod\nindex 0e628d18b0..5abb53852f 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -10,7 +10,7 @@ require (\n \tgithub.com/btcsuite/btcd/btcutil/psbt v1.1.8\n \tgithub.com/btcsuite/btcd/chaincfg/chainhash v1.1.0\n \tgithub.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f\n-\tgithub.com/btcsuite/btcwallet v0.16.10-0.20240718224643-db3a4a2543bd\n+\tgithub.com/btcsuite/btcwallet v0.16.10-0.20240809133323-7d3434c65ae2\n \tgithub.com/btcsuite/btcwallet/wallet/txauthor v1.3.4\n \tgithub.com/btcsuite/btcwallet/wallet/txrules v1.2.1\n \tgithub.com/btcsuite/btcwallet/walletdb v1.4.2\ndiff --git a/go.sum b/go.sum\nindex d556042dd3..e77306ddb8 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -92,8 +92,8 @@ github.com/btcsuite/btcd/chaincfg/chainhash v1.1.0/go.mod h1:7SFka0XMvUgj3hfZtyd\n github.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f h1:bAs4lUbRJpnnkd9VhRV3jjAVU7DJVjMaK+IsvSeZvFo=\n github.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f/go.mod h1:TdznJufoqS23FtqVCzL0ZqgP5MqXbb4fg/WgDys70nA=\n github.com/btcsuite/btcutil v0.0.0-20190425235716-9e5f4b9a998d/go.mod h1:+5NJ2+qvTyV9exUAL/rxXi3DcLg2Ts+ymUAY5y4NvMg=\n-github.com/btcsuite/btcwallet v0.16.10-0.20240718224643-db3a4a2543bd h1:QDb8foTCRoXrfoZVEzSYgSde16MJh4gCtCin8OCS0kI=\n-github.com/btcsuite/btcwallet v0.16.10-0.20240718224643-db3a4a2543bd/go.mod h1:X2xDre+j1QphTRo54y2TikUzeSvreL1t1aMXrD8Kc5A=\n+github.com/btcsuite/btcwallet v0.16.10-0.20240809133323-7d3434c65ae2 h1:qa4Avm7p97JroZZyMJADbEb9u853pjleJYSeitENvLc=\n+github.com/btcsuite/btcwallet v0.16.10-0.20240809133323-7d3434c65ae2/go.mod h1:X2xDre+j1QphTRo54y2TikUzeSvreL1t1aMXrD8Kc5A=\n github.com/btcsuite/btcwallet/wallet/txauthor v1.3.4 h1:poyHFf7+5+RdxNp5r2T6IBRD7RyraUsYARYbp/7t4D8=\n github.com/btcsuite/btcwallet/wallet/txauthor v1.3.4/go.mod h1:GETGDQuyq+VFfH1S/+/7slLM/9aNa4l7P4ejX6dJfb0=\n github.com/btcsuite/btcwallet/wallet/txrules v1.2.1 h1:UZo7YRzdHbwhK7Rhv3PO9bXgTxiOH45edK5qdsdiatk=\ndiff --git a/lnrpc/walletrpc/psbt.go b/lnrpc/walletrpc/psbt.go\nindex 19b1024733..0577519647 100644\n--- a/lnrpc/walletrpc/psbt.go\n+++ b/lnrpc/walletrpc/psbt.go\n@@ -56,7 +56,13 @@ func lockInputs(w lnwallet.WalletController,\n \t\t\t},\n \t\t}\n \n-\t\texpiration, pkScript, value, err := w.LeaseOutput(\n+\t\t// Get the details about this outpoint.\n+\t\tutxo, err := w.FetchOutpointInfo(&lock.Outpoint)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"fetch outpoint info: %w\", err)\n+\t\t}\n+\n+\t\texpiration, err := w.LeaseOutput(\n \t\t\tlock.LockID, lock.Outpoint,\n \t\t\tchanfunding.DefaultLockDuration,\n \t\t)\n@@ -80,8 +86,8 @@ func lockInputs(w lnwallet.WalletController,\n \t\t}\n \n \t\tlock.Expiration = expiration\n-\t\tlock.PkScript = pkScript\n-\t\tlock.Value = int64(value)\n+\t\tlock.PkScript = utxo.PkScript\n+\t\tlock.Value = int64(utxo.Value)\n \t\tlocks[idx] = lock\n \t}\n \ndiff --git a/lnrpc/walletrpc/walletkit_server.go b/lnrpc/walletrpc/walletkit_server.go\nindex e70a1a7712..cc836e809f 100644\n--- a/lnrpc/walletrpc/walletkit_server.go\n+++ b/lnrpc/walletrpc/walletkit_server.go\n@@ -490,7 +490,7 @@ func (w *WalletKit) LeaseOutput(ctx context.Context,\n \t// other concurrent processes attempting to lease the same UTXO.\n \tvar expiration time.Time\n \terr = w.cfg.CoinSelectionLocker.WithCoinSelectLock(func() error {\n-\t\texpiration, _, _, err = w.cfg.Wallet.LeaseOutput(\n+\t\texpiration, err = w.cfg.Wallet.LeaseOutput(\n \t\t\tlockID, *op, duration,\n \t\t)\n \t\treturn err\n@@ -1142,7 +1142,7 @@ func (w *WalletKit) sweepNewInput(op *wire.OutPoint, currentHeight uint32,\n \t//\n \t// We'll gather all of the information required by the UtxoSweeper in\n \t// order to sweep the output.\n-\tutxo, err := w.cfg.Wallet.FetchInputInfo(op)\n+\tutxo, err := w.cfg.Wallet.FetchOutpointInfo(op)\n \tif err != nil {\n \t\treturn err\n \t}\ndiff --git a/lnwallet/btcwallet/btcwallet.go b/lnwallet/btcwallet/btcwallet.go\nindex b3f9b4f9b1..22204dea39 100644\n--- a/lnwallet/btcwallet/btcwallet.go\n+++ b/lnwallet/btcwallet/btcwallet.go\n@@ -1079,36 +1079,20 @@ func (b *BtcWallet) CreateSimpleTx(inputs fn.Set[wire.OutPoint],\n //\n // NOTE: This method requires the global coin selection lock to be held.\n func (b *BtcWallet) LeaseOutput(id wtxmgr.LockID, op wire.OutPoint,\n-\tduration time.Duration) (time.Time, []byte, btcutil.Amount, error) {\n+\tduration time.Duration) (time.Time, error) {\n \n \t// Make sure we don't attempt to double lock an output that's been\n \t// locked by the in-memory implementation.\n \tif b.wallet.LockedOutpoint(op) {\n-\t\treturn time.Time{}, nil, 0, wtxmgr.ErrOutputAlreadyLocked\n+\t\treturn time.Time{}, wtxmgr.ErrOutputAlreadyLocked\n \t}\n \n \tlockedUntil, err := b.wallet.LeaseOutput(id, op, duration)\n \tif err != nil {\n-\t\treturn time.Time{}, nil, 0, err\n+\t\treturn time.Time{}, err\n \t}\n \n-\t// Get the pkScript and value for this lock from the list of all leased\n-\t// outputs.\n-\tallLeases, err := b.wallet.ListLeasedOutputs()\n-\tif err != nil {\n-\t\treturn time.Time{}, nil, 0, err\n-\t}\n-\n-\tfor _, lease := range allLeases {\n-\t\tif lease.Outpoint == op {\n-\t\t\treturn lockedUntil, lease.PkScript,\n-\t\t\t\tbtcutil.Amount(lease.Value), nil\n-\t\t}\n-\t}\n-\n-\t// We MUST find the leased output in the loop above, otherwise something\n-\t// is seriously wrong.\n-\treturn time.Time{}, nil, 0, wtxmgr.ErrUnknownOutput\n+\treturn lockedUntil, nil\n }\n \n // ListLeasedOutputs returns a list of all currently locked outputs.\ndiff --git a/lnwallet/btcwallet/signer.go b/lnwallet/btcwallet/signer.go\nindex 9864164769..97832a7890 100644\n--- a/lnwallet/btcwallet/signer.go\n+++ b/lnwallet/btcwallet/signer.go\n@@ -8,6 +8,7 @@ import (\n \t\"github.com/btcsuite/btcd/btcec/v2/schnorr\"\n \t\"github.com/btcsuite/btcd/btcutil\"\n \t\"github.com/btcsuite/btcd/btcutil/hdkeychain\"\n+\t\"github.com/btcsuite/btcd/btcutil/psbt\"\n \t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n \t\"github.com/btcsuite/btcd/txscript\"\n \t\"github.com/btcsuite/btcd/wire\"\n@@ -18,18 +19,16 @@ import (\n \t\"github.com/lightningnetwork/lnd/lnwallet\"\n )\n \n-// FetchInputInfo queries for the WalletController's knowledge of the passed\n+// FetchOutpointInfo queries for the WalletController's knowledge of the passed\n // outpoint. If the base wallet determines this output is under its control,\n // then the original txout should be returned. Otherwise, a non-nil error value\n // of ErrNotMine should be returned instead.\n //\n // This is a part of the WalletController interface.\n-func (b *BtcWallet) FetchInputInfo(prevOut *wire.OutPoint) (*lnwallet.Utxo,\n+func (b *BtcWallet) FetchOutpointInfo(prevOut *wire.OutPoint) (*lnwallet.Utxo,\n \terror) {\n \n-\tprevTx, txOut, bip32, confirmations, err := b.wallet.FetchInputInfo(\n-\t\tprevOut,\n-\t)\n+\tprevTx, txOut, confirmations, err := b.wallet.FetchOutpointInfo(prevOut)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -51,11 +50,18 @@ func (b *BtcWallet) FetchInputInfo(prevOut *wire.OutPoint) (*lnwallet.Utxo,\n \t\tPkScript:      txOut.PkScript,\n \t\tConfirmations: confirmations,\n \t\tOutPoint:      *prevOut,\n-\t\tDerivation:    bip32,\n \t\tPrevTx:        prevTx,\n \t}, nil\n }\n \n+// FetchDerivationInfo queries for the wallet's knowledge of the passed\n+// pkScript and constructs the derivation info and returns it.\n+func (b *BtcWallet) FetchDerivationInfo(\n+\tpkScript []byte) (*psbt.Bip32Derivation, error) {\n+\n+\treturn b.wallet.FetchDerivationInfo(pkScript)\n+}\n+\n // ScriptForOutput returns the address, witness program and redeem script for a\n // given UTXO. An error is returned if the UTXO does not belong to our wallet or\n // it is not a managed pubKey address.\ndiff --git a/lnwallet/chanfunding/assembler.go b/lnwallet/chanfunding/assembler.go\nindex a694d2a2c4..2fdb87b32d 100644\n--- a/lnwallet/chanfunding/assembler.go\n+++ b/lnwallet/chanfunding/assembler.go\n@@ -44,7 +44,7 @@ type OutputLeaser interface {\n \t// LeaseOutput leases a target output, rendering it unusable for coin\n \t// selection.\n \tLeaseOutput(i wtxmgr.LockID, o wire.OutPoint, d time.Duration) (\n-\t\ttime.Time, []byte, btcutil.Amount, error)\n+\t\ttime.Time, error)\n \n \t// ReleaseOutput releases a target output, allowing it to be used for\n \t// coin selection once again.\ndiff --git a/lnwallet/chanfunding/wallet_assembler.go b/lnwallet/chanfunding/wallet_assembler.go\nindex 4f2aa759b2..f824210e13 100644\n--- a/lnwallet/chanfunding/wallet_assembler.go\n+++ b/lnwallet/chanfunding/wallet_assembler.go\n@@ -525,7 +525,7 @@ func (w *WalletAssembler) ProvisionChannel(r *Request) (Intent, error) {\n \t\tfor _, coin := range selectedCoins {\n \t\t\toutpoint := coin.OutPoint\n \n-\t\t\t_, _, _, err = w.cfg.CoinLeaser.LeaseOutput(\n+\t\t\t_, err = w.cfg.CoinLeaser.LeaseOutput(\n \t\t\t\tLndInternalLockID, outpoint,\n \t\t\t\tDefaultReservationTimeout,\n \t\t\t)\ndiff --git a/lnwallet/interface.go b/lnwallet/interface.go\nindex af5955d8e6..d3b123509b 100644\n--- a/lnwallet/interface.go\n+++ b/lnwallet/interface.go\n@@ -126,8 +126,7 @@ type Utxo struct {\n \tConfirmations int64\n \tPkScript      []byte\n \twire.OutPoint\n-\tDerivation *psbt.Bip32Derivation\n-\tPrevTx     *wire.MsgTx\n+\tPrevTx *wire.MsgTx\n }\n \n // OutputDetail contains additional information on a destination address.\n@@ -227,11 +226,16 @@ type TransactionSubscription interface {\n // behavior of all interface methods in order to ensure identical behavior\n // across all concrete implementations.\n type WalletController interface {\n-\t// FetchInputInfo queries for the WalletController's knowledge of the\n-\t// passed outpoint. If the base wallet determines this output is under\n-\t// its control, then the original txout should be returned.  Otherwise,\n-\t// a non-nil error value of ErrNotMine should be returned instead.\n-\tFetchInputInfo(prevOut *wire.OutPoint) (*Utxo, error)\n+\t// FetchOutpointInfo queries for the WalletController's knowledge of\n+\t// the passed outpoint. If the base wallet determines this output is\n+\t// under its control, then the original txout should be returned.\n+\t// Otherwise, a non-nil error value of ErrNotMine should be returned\n+\t// instead.\n+\tFetchOutpointInfo(prevOut *wire.OutPoint) (*Utxo, error)\n+\n+\t// FetchDerivationInfo queries for the wallet's knowledge of the passed\n+\t// pkScript and constructs the derivation info and returns it.\n+\tFetchDerivationInfo(pkScript []byte) (*psbt.Bip32Derivation, error)\n \n \t// ScriptForOutput returns the address, witness program and redeem\n \t// script for a given UTXO. An error is returned if the UTXO does not\n@@ -409,8 +413,7 @@ type WalletController interface {\n \t//\n \t// NOTE: This method requires the global coin selection lock to be held.\n \tLeaseOutput(id wtxmgr.LockID, op wire.OutPoint,\n-\t\tduration time.Duration) (time.Time, []byte, btcutil.Amount,\n-\t\terror)\n+\t\tduration time.Duration) (time.Time, error)\n \n \t// ReleaseOutput unlocks an output, allowing it to be available for coin\n \t// selection if it remains unspent. The ID should match the one used to\ndiff --git a/lnwallet/mock.go b/lnwallet/mock.go\nindex e6bd0b2e41..faac5fa678 100644\n--- a/lnwallet/mock.go\n+++ b/lnwallet/mock.go\n@@ -45,9 +45,9 @@ func (w *mockWalletController) BackEnd() string {\n \treturn \"mock\"\n }\n \n-// FetchInputInfo will be called to get info about the inputs to the funding\n+// FetchOutpointInfo will be called to get info about the inputs to the funding\n // transaction.\n-func (w *mockWalletController) FetchInputInfo(\n+func (w *mockWalletController) FetchOutpointInfo(\n \tprevOut *wire.OutPoint) (*Utxo, error) {\n \n \tutxo := &Utxo{\n@@ -201,9 +201,9 @@ func (w *mockWalletController) ListTransactionDetails(int32, int32,\n \n // LeaseOutput returns the current time and a nil error.\n func (w *mockWalletController) LeaseOutput(wtxmgr.LockID, wire.OutPoint,\n-\ttime.Duration) (time.Time, []byte, btcutil.Amount, error) {\n+\ttime.Duration) (time.Time, error) {\n \n-\treturn time.Now(), nil, 0, nil\n+\treturn time.Now(), nil\n }\n \n // ReleaseOutput currently does nothing.\n@@ -300,6 +300,14 @@ func (w *mockWalletController) RemoveDescendants(*wire.MsgTx) error {\n \treturn nil\n }\n \n+// FetchDerivationInfo queries for the wallet's knowledge of the passed\n+// pkScript and constructs the derivation info and returns it.\n+func (w *mockWalletController) FetchDerivationInfo(\n+\tpkScript []byte) (*psbt.Bip32Derivation, error) {\n+\n+\treturn nil, nil\n+}\n+\n func (w *mockWalletController) CheckMempoolAcceptance(tx *wire.MsgTx) error {\n \treturn nil\n }\ndiff --git a/lnwallet/rpcwallet/rpcwallet.go b/lnwallet/rpcwallet/rpcwallet.go\nindex c702174163..bf6aa61df3 100644\n--- a/lnwallet/rpcwallet/rpcwallet.go\n+++ b/lnwallet/rpcwallet/rpcwallet.go\n@@ -154,7 +154,7 @@ func (r *RPCKeyRing) SendOutputs(inputs fn.Set[wire.OutPoint],\n \t\t// watch-only wallet if it can map this outpoint into a coin we\n \t\t// own. If not, then we can't continue because our wallet state\n \t\t// is out of sync.\n-\t\tinfo, err := r.WalletController.FetchInputInfo(\n+\t\tinfo, err := r.WalletController.FetchOutpointInfo(\n \t\t\t&txIn.PreviousOutPoint,\n \t\t)\n \t\tif err != nil {\n@@ -289,7 +289,7 @@ func (r *RPCKeyRing) FinalizePsbt(packet *psbt.Packet, _ string) error {\n \t\t// We can only sign this input if it's ours, so we try to map it\n \t\t// to a coin we own. If we can't, then we'll continue as it\n \t\t// isn't our input.\n-\t\tutxo, err := r.FetchInputInfo(&txIn.PreviousOutPoint)\n+\t\tutxo, err := r.FetchOutpointInfo(&txIn.PreviousOutPoint)\n \t\tif err != nil {\n \t\t\tcontinue\n \t\t}\n@@ -906,7 +906,7 @@ func (r *RPCKeyRing) remoteSign(tx *wire.MsgTx, signDesc *input.SignDescriptor,\n \t\t}\n \n \t\ttxIn := tx.TxIn[idx]\n-\t\tinfo, err := r.WalletController.FetchInputInfo(\n+\t\tinfo, err := r.WalletController.FetchOutpointInfo(\n \t\t\t&txIn.PreviousOutPoint,\n \t\t)\n \t\tif err != nil {\n@@ -1015,19 +1015,32 @@ func (r *RPCKeyRing) remoteSign(tx *wire.MsgTx, signDesc *input.SignDescriptor,\n \t\tsignDesc.KeyDesc.PubKey = fullDesc.PubKey\n \t}\n \n+\tvar derivation *psbt.Bip32Derivation\n+\n \t// Make sure we actually know about the input. We either have been\n \t// watching the UTXO on-chain or we have been given all the required\n \t// info in the sign descriptor.\n-\tinfo, err := r.WalletController.FetchInputInfo(&txIn.PreviousOutPoint)\n+\tinfo, err := r.WalletController.FetchOutpointInfo(\n+\t\t&txIn.PreviousOutPoint,\n+\t)\n+\n+\t// If the wallet is aware of this outpoint, we go ahead and fetch the\n+\t// derivation info.\n+\tif err == nil {\n+\t\tderivation, err = r.WalletController.FetchDerivationInfo(\n+\t\t\tinfo.PkScript,\n+\t\t)\n+\t}\n+\n \tswitch {\n-\t// No error, we do have the full UTXO and derivation info available.\n+\t// No error, we do have the full UTXO info available.\n \tcase err == nil:\n \t\tin.WitnessUtxo = &wire.TxOut{\n \t\t\tValue:    int64(info.Value),\n \t\t\tPkScript: info.PkScript,\n \t\t}\n \t\tin.NonWitnessUtxo = info.PrevTx\n-\t\tin.Bip32Derivation = []*psbt.Bip32Derivation{info.Derivation}\n+\t\tin.Bip32Derivation = []*psbt.Bip32Derivation{derivation}\n \n \t// The wallet doesn't know about this UTXO, so it's probably a TX that\n \t// we haven't published yet (e.g. a channel funding TX). So we need to\ndiff --git a/lnwallet/wallet.go b/lnwallet/wallet.go\nindex 90a0bea2d3..6925446a3e 100644\n--- a/lnwallet/wallet.go\n+++ b/lnwallet/wallet.go\n@@ -1646,7 +1646,7 @@ func (l *LightningWallet) handleContributionMsg(req *addContributionMsg) {\n \t\t\t[]*input.Script, 0, len(ourContribution.Inputs),\n \t\t)\n \t\tfor _, txIn := range fundingTx.TxIn {\n-\t\t\t_, err := l.FetchInputInfo(&txIn.PreviousOutPoint)\n+\t\t\t_, err := l.FetchOutpointInfo(&txIn.PreviousOutPoint)\n \t\t\tif err != nil {\n \t\t\t\tcontinue\n \t\t\t}\n@@ -2592,7 +2592,7 @@ func (c *CoinSource) ListCoins(minConfs int32,\n // its outpoint. If the coin isn't under the control of the backing CoinSource,\n // then an error should be returned.\n func (c *CoinSource) CoinFromOutPoint(op wire.OutPoint) (*wallet.Coin, error) {\n-\tinputInfo, err := c.wallet.FetchInputInfo(&op)\n+\tinputInfo, err := c.wallet.FetchOutpointInfo(&op)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -2689,7 +2689,7 @@ func NewWalletPrevOutputFetcher(wc WalletController) *WalletPrevOutputFetcher {\n // passed outpoint. A nil value will be returned if the passed outpoint doesn't\n // exist.\n func (w *WalletPrevOutputFetcher) FetchPrevOutput(op wire.OutPoint) *wire.TxOut {\n-\tutxo, err := w.wc.FetchInputInfo(&op)\n+\tutxo, err := w.wc.FetchOutpointInfo(&op)\n \tif err != nil {\n \t\treturn nil\n \t}\ndiff --git a/rpcserver.go b/rpcserver.go\nindex 7fef873266..005bc38fc9 100644\n--- a/rpcserver.go\n+++ b/rpcserver.go\n@@ -3488,14 +3488,7 @@ func (r *rpcServer) WalletBalance(ctx context.Context,\n \t\treturn nil, err\n \t}\n \tfor _, leasedOutput := range leases {\n-\t\tutxoInfo, err := r.server.cc.Wallet.FetchInputInfo(\n-\t\t\t&leasedOutput.Outpoint,\n-\t\t)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tlockedBalance += utxoInfo.Value\n+\t\tlockedBalance += btcutil.Amount(leasedOutput.Value)\n \t}\n \n \t// Get the current number of non-private anchor channels.\ndiff --git a/sweep/walletsweep.go b/sweep/walletsweep.go\nindex 919e5f1d51..da5e56dee0 100644\n--- a/sweep/walletsweep.go\n+++ b/sweep/walletsweep.go\n@@ -179,7 +179,7 @@ type OutputLeaser interface {\n \t// LeaseOutput leases a target output, rendering it unusable for coin\n \t// selection.\n \tLeaseOutput(i wtxmgr.LockID, o wire.OutPoint, d time.Duration) (\n-\t\ttime.Time, []byte, btcutil.Amount, error)\n+\t\ttime.Time, error)\n \n \t// ReleaseOutput releases a target output, allowing it to be used for\n \t// coin selection once again.\n@@ -284,7 +284,7 @@ func CraftSweepAllTx(feeRate, maxFeeRate chainfee.SatPerKWeight,\n \t\t\tlog.Tracef(\"[WithCoinSelectLock] leasing utxo: %v\",\n \t\t\t\tutxo.OutPoint)\n \n-\t\t\t_, _, _, err = outputLeaser.LeaseOutput(\n+\t\t\t_, err = outputLeaser.LeaseOutput(\n \t\t\t\tchanfunding.LndInternalLockID, utxo.OutPoint,\n \t\t\t\tchanfunding.DefaultLockDuration,\n \t\t\t)\n", "instance_id": "lightningnetwork__lnd-8961", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `LeaseOutput` method is significantly slower with a PostgreSQL backend compared to other database backends like kvdb, leading to performance issues in operations like `SendCoins`. It provides detailed logs and reproduction steps across different configurations, which help in understanding the scope and impact of the issue. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss the root cause of the performance discrepancy (e.g., whether it's due to query inefficiencies, locking mechanisms, or data volume). Additionally, it lacks specifics on expected performance benchmarks or constraints for the fix, and edge cases or specific failure modes beyond the timeout error are not mentioned. Overall, while the goal and context are clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files and modules within the Lightning Network Daemon (LND) codebase, including wallet management, transaction handling, and database interactions, as seen in the modifications to `LeaseOutput` and related interfaces. This requires a deep understanding of the codebase architecture and the interplay between components like `WalletController`, `UtxoSweeper`, and PSBT handling. Second, the technical concepts involved are moderately complex, including Go-specific concurrency patterns (e.g., locks with `WithCoinSelectLock`), Bitcoin transaction handling (e.g., UTXO leasing, outpoint management), and database backend performance optimization (specifically with PostgreSQL). Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential challenges in ensuring correctness during output leasing and release, especially under concurrent access, which could introduce subtle bugs or race conditions if not handled carefully. Finally, the performance optimization aspect adds difficulty, as it requires profiling and potentially refactoring database interactions without breaking existing functionality. The issue does not reach \"Very Hard\" because it does not appear to involve system-level redesign or highly specialized domain knowledge beyond Bitcoin wallet operations. A score of 0.65 reflects the need for significant expertise and careful implementation across multiple components, balanced against the absence of extreme complexity or novelty in the problem space.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "external-dns is ignoring punycode domains\n**What happened**:\n\nexternal-dns is completely ignoring one of my domains, as if it can't find it.\n\n```\nDEBU[0004] Skipping record a.xn--ccka2b6azt.xn--q9jyb4c because no hosted zone matching record DNS Name was detected\nDEBU[0004] Skipping record a.xn--ccka2b6azt.xn--q9jyb4c because no hosted zone matching record DNS Name was detected\n```\n\nMy domain is registered and active within Cloudflare:\n\n![Image](https://github.com/user-attachments/assets/e528f45d-6079-4289-8f64-e93c5037403f)\n\n**What you expected to happen**:\n\nI expect it to find the right zone and create DNS records.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n\nI am not sure how to describe a reproduction scenario, but this is my Ingress manifest (which I use as sources for external-dns):\n<details>\n<summary>ingress.yml</summary>\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: alyxiaminna\n  namespace: alyxiaminna\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: a.xn--ccka2b6azt.xn--q9jyb4c\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: alyxiaminna\n                port:\n                  number: 80\n\n  tls:\n    - hosts:\n        - a.xn--ccka2b6azt.xn--q9jyb4c\n      secretName: alyxiaminna-a-cert\n```\n</details>\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- External-DNS version (use `external-dns --version`): `v0.15.0`\n- DNS provider: Cloudflare\n\n", "patch": "diff --git a/provider/zonefinder.go b/provider/zonefinder.go\nindex 18396000b8..6a07646c18 100644\n--- a/provider/zonefinder.go\n+++ b/provider/zonefinder.go\n@@ -16,7 +16,12 @@ limitations under the License.\n \n package provider\n \n-import \"strings\"\n+import (\n+\t\"strings\"\n+\n+\tlog \"github.com/sirupsen/logrus\"\n+\t\"golang.org/x/net/idna\"\n+)\n \n type ZoneIDName map[string]string\n \n@@ -25,8 +30,13 @@ func (z ZoneIDName) Add(zoneID, zoneName string) {\n }\n \n func (z ZoneIDName) FindZone(hostname string) (suitableZoneID, suitableZoneName string) {\n+\tname, err := idna.Lookup.ToUnicode(hostname)\n+\tif err != nil {\n+\t\tlog.Warnf(\"Failed to convert hostname '%s' to its Unicode form: %v\", hostname, err)\n+\t\tname = hostname\n+\t}\n \tfor zoneID, zoneName := range z {\n-\t\tif hostname == zoneName || strings.HasSuffix(hostname, \".\"+zoneName) {\n+\t\tif name == zoneName || strings.HasSuffix(name, \".\"+zoneName) {\n \t\t\tif suitableZoneName == \"\" || len(zoneName) > len(suitableZoneName) {\n \t\t\t\tsuitableZoneID = zoneID\n \t\t\t\tsuitableZoneName = zoneName\n", "instance_id": "kubernetes-sigs__external-dns-5147", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue: external-dns is ignoring a punycode domain (a domain encoded in ASCII format for internationalization, e.g., xn--ccka2b6azt.xn--q9jyb4c), and the user expects it to find the correct hosted zone and create DNS records. The inclusion of debug logs, environment details (External-DNS version and DNS provider), and the Ingress manifest provides helpful context. However, there are minor ambiguities and missing details that prevent a perfect score. For instance, the reproduction steps are vague (\"I am not sure how to describe a reproduction scenario\"), and there is no mention of specific edge cases or constraints related to punycode handling in external-dns or the Cloudflare provider. Additionally, the problem statement does not clarify whether this issue affects only specific punycode domains or all of them, nor does it provide insight into the expected behavior when punycode conversion fails. Despite these gaps, the intent and core issue are understandable, especially with the provided code changes as context.\n", "difficulty_explanation": "\nI rate the difficulty of this problem as 0.45, placing it in the medium range. Here's the breakdown based on the evaluation factors:\n\n1. **Clarity and Complexity of the Problem Description**: While the problem statement has minor ambiguities, the core issue (ignoring punycode domains) is straightforward. The logic behind punycode handling isn't inherently complex, but it does require specific domain knowledge about internationalized domain names (IDN) and how they are processed in DNS systems.\n\n2. **Scope and Depth of Code Changes**: The provided code change is localized to a single file (`zonefinder.go`) and modifies a specific function (`FindZone`) to convert punycode hostnames to their Unicode form using the `golang.org/x/net/idna` package. The change is minimal (adding a few lines of code) and does not impact the broader system architecture or require modifications across multiple modules. However, understanding the context of zone matching in external-dns and its interaction with DNS providers like Cloudflare adds a slight layer of complexity.\n\n3. **Number of Technical Concepts**: Solving this requires understanding a few specific concepts: (a) punycode and IDN handling, which is a niche but not overly complex topic; (b) the `golang.org/x/net/idna` library for converting between punycode and Unicode; and (c) the logic of zone matching in external-dns. Additionally, familiarity with Go's string manipulation and logging (via `logrus`) is needed. These concepts are not advanced but do require targeted knowledge beyond basic programming.\n\n4. **Potential Edge Cases and Error Handling**: The code change includes basic error handling for punycode conversion failures (logging a warning and falling back to the original hostname). However, the problem statement does not explicitly mention edge cases, such as invalid punycode strings, mixed punycode/Unicode inputs, or provider-specific quirks with Cloudflare. The developer must anticipate these scenarios, but they are not overly complex to handle.\n\nOverall, this problem falls into the medium difficulty range because it requires understanding a specific domain concept (punycode/IDN) and making a targeted code change with basic error handling. It does not involve deep architectural changes or advanced algorithms, nor does it require extensive experience with the codebase. A developer with moderate Go experience and a willingness to research IDN handling can solve this with reasonable effort.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Improve hack/docs to avoid failures when upgrading controller-runtime version\nCurrently, all projects generated for documentation/tutorials are created automatically via commands and scripts under `hack/docs`. This automation helps keep our documentation up to date without needing manual intervention.\n\nWhen we run `make generate`, it triggers `make generate-docs`, which re-generates these example projects and syncs them with the documentation.\n\n### The Problem\n\nWe have a hardcoded version of `controller-runtime` in the following line:\n\nhttps://github.com/kubernetes-sigs/kubebuilder/blob/3ae96043f3673504f18a1e776cc32ee142deaf91/hack/docs/internal/cronjob-tutorial/generate_cronjob.go#L291\n\nThis causes `make generate-docs` to fail when we bump the controller-runtime version in the scaffold logic. For example, updating this constant:\n\nhttps://github.com/kubernetes-sigs/kubebuilder/blob/3ae96043f3673504f18a1e776cc32ee142deaf91/pkg/plugins/golang/v4/scaffolds/init.go#L43\n\nFrom:\nControllerRuntimeVersion = \"v0.20.4\"\n\nTo a newer version may result in errors like:\n\n`ERRO error fixing cronjob_controller.go: string // - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.3/pkg/reconcile not found in /* Copyright 2025 The Kubernetes authors.\n`\n### What Needs to Be Done\n\nWe should improve the logic in `hack/docs` so that it's not tied to a hardcoded controller-runtime version.\n\nWe could either:\n- Remove the need for the manual replacement in the first place\n- OR dynamically use the version from the `ControllerRuntimeVersion` constant in the scaffolding code\n\nThis would prevent future breakages when we upgrade dependencies and keep `make generate-docs` stable and version-aware.\n\n\n", "patch": "diff --git a/hack/docs/internal/cronjob-tutorial/generate_cronjob.go b/hack/docs/internal/cronjob-tutorial/generate_cronjob.go\nindex c7fd20a025e..fef9b97e1df 100644\n--- a/hack/docs/internal/cronjob-tutorial/generate_cronjob.go\n+++ b/hack/docs/internal/cronjob-tutorial/generate_cronjob.go\n@@ -17,6 +17,7 @@ limitations under the License.\n package cronjob\n \n import (\n+\t\"fmt\"\n \t\"os/exec\"\n \t\"path/filepath\"\n \n@@ -24,6 +25,7 @@ import (\n \t\"github.com/spf13/afero\"\n \thackutils \"sigs.k8s.io/kubebuilder/v4/hack/docs/utils\"\n \tpluginutil \"sigs.k8s.io/kubebuilder/v4/pkg/plugin/util\"\n+\t\"sigs.k8s.io/kubebuilder/v4/pkg/plugins/golang/v4/scaffolds\"\n \t\"sigs.k8s.io/kubebuilder/v4/test/e2e/utils\"\n )\n \n@@ -288,7 +290,8 @@ func (sp *Sample) updateController() {\n \n \terr = pluginutil.InsertCode(\n \t\tfilepath.Join(sp.ctx.Dir, \"internal/controller/cronjob_controller.go\"),\n-\t\t`// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.4/pkg/reconcile`, skipGoCycloLint)\n+\t\tfmt.Sprintf(`// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@%s/pkg/reconcile`,\n+\t\t\tscaffolds.ControllerRuntimeVersion), skipGoCycloLint)\n \thackutils.CheckError(\"fixing cronjob_controller.go\", err)\n \n \terr = pluginutil.ReplaceInFile(\n", "instance_id": "kubernetes-sigs__kubebuilder-4713", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear and provides a good overview of the issue at hand. It describes the goal of preventing failures in `make generate-docs` when upgrading the `controller-runtime` version, identifies the specific hardcoded version causing the issue, and links to relevant parts of the codebase. It also suggests two potential solutions: removing the need for manual replacement or dynamically using the version from a constant. However, there are minor ambiguities, such as a lack of detail on how the dynamic version retrieval should interact with other parts of the system or whether there are specific constraints or side effects to consider when implementing either solution. Additionally, edge cases or potential risks of changing the version dynamically are not mentioned. Overall, the statement is valid and clear but misses some finer details that could aid in a complete understanding of the problem scope.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" category. The issue requires a straightforward modification to replace a hardcoded version string with a dynamic reference to the `ControllerRuntimeVersion` constant from the scaffolding code, as shown in the provided code changes. The scope of the change is limited to a single file (`generate_cronjob.go`) and a small code snippet, with no apparent impact on the broader system architecture or multiple modules. The technical concepts involved are basic: understanding Go imports, string formatting, and referencing a constant from another package. No complex algorithms, design patterns, or domain-specific knowledge are required. Edge cases and error handling are not explicitly mentioned in the problem statement, and the provided code change does not introduce new error handling logic, suggesting minimal complexity in this area. Overall, this task requires understanding some code logic and making a simple modification, aligning with a difficulty score of 0.3.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Isolated Workspaces don't seem to be mounting properly\n# Expected Behavior\r\n\r\n[https://github.com/tektoncd/community/blob/main/teps/0029-step-workspaces.md](https://www.google.com/url?sa=D&q=https%3A%2F%2Fgithub.com%2Ftektoncd%2Fcommunity%2Fblob%2Fmain%2Fteps%2F0029-step-workspaces.md)\r\n\r\nWhen we specify the workspace name in a step, only that workspace should have access to the workspace/mount. Further, if you replace a mount path, it should also update the mount path.\r\n\r\n# Actual Behavior\r\n\r\nYaml passes validation, but running find or checking if a dir exists shows that a workspace is mounted to all steps even when specified to a single one.\r\n\r\n# Steps to Reproduce the Problem\r\n\r\n1. Start up a tekton job using the following snippet for the taskSpec:\r\n\r\n```\r\n        taskSpec:\r\n          workspaces:\r\n            - name: output\r\n              mountPath: /job_out\r\n           # Below two workspaces correspond to serviceAccountToken projected Volumes\r\n            - name: gcp-creds\r\n              mountPath: /var/run/secrets/tokens/gcp-creds\r\n            - name: gcp-other-creds\r\n              mountPath: /var/run/secrets/tokens/gcp-other-creds\r\n\r\n          stepTemplate:\r\n            computeResources:\r\n              requests:\r\n                cpu: 20m\r\n                memory: 100M\r\n              limits:\r\n                cpu: 500m\r\n                memory: 1G\r\n            env:\r\n              - name: GOOGLE_CLOUD_PROJECT\r\n                value: project-name\r\n\r\n          steps:\r\n            - name: whoami-privileged-step\r\n              image: used-image\r\n              imagePullPolicy: Always\r\n              workingDir: /job_out\r\n              script: |\r\n                #!/bin/bash\r\n                find /var/run/secrets/tokens/\r\n                find /var/run/secrets/tokens/ -name \"*.json\" -exec cat {} \\;\r\n              env:\r\n                - name: GOOGLE_APPLICATION_CREDENTIALS\r\n                  value: /var/run/secrets/tokens/gcp-creds/credentials.json\r\n\r\n              # Because we specify the workspace here, the other step should not have access to this mount.\r\n              workspaces:\r\n                - name: gcp-creds\r\n                  mountPath: /var/run/secrets/tokens/gcp-creds\r\n\r\n            - name: whoami-other-step\r\n              image: used-image\r\n              imagePullPolicy: Always\r\n              workingDir: /job_out\r\n              script: |\r\n                #!/bin/bash\r\n                find /var/run/secrets/tokens/\r\n                find /var/run/secrets/tokens/ -name \"*.json\" -exec cat {} \\;\r\n              env:\r\n                - name: GOOGLE_APPLICATION_CREDENTIALS\r\n                  value: /var/run/secrets/tokens/gcp-other-creds/other-credentials.json\r\n              workspaces:\r\n                - name: gcp-other-creds\r\n                  mountPath: /var/run/secrets/tokens/gcp-other-creds\r\n```\r\n\r\n2. In logs, we see both steps have access to both sets of creds\r\n\r\n# Additional Info\r\n\r\n- Kubernetes version:\r\n\r\n  **Output of `kubectl version`:**\r\n\r\n```\r\nClient Version: v1.29.8-dispatcher\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.7-gke.1274000\r\n```\r\n\r\n- Tekton Pipeline version:\r\n\r\n  **Output of `tkn version` or `kubectl get pods -n tekton-pipelines -l app=tekton-pipelines-controller -o=jsonpath='{.items[0].metadata.labels.version}'`**\r\n\r\n```\r\nv0.60.2\r\n```\r\n\r\n\r\n<!-- Any other additional information -->\r\n\r\nenable-api-fieldscs is set to alpha\r\n\r\nAs an aside, the docs indicate that this is in beta, but it seems like as of [https://github.com/tektoncd/pipeline/commit/ecb11fc82a647fe706112998c463bb23e7de0028](https://www.google.com/url?sa=D&q=https%3A%2F%2Fgithub.com%2Ftektoncd%2Fpipeline%2Fcommit%2Fecb11fc82a647fe706112998c463bb23e7de0028), it has no flag protection?\r\n\r\n\n", "patch": "diff --git a/pkg/apis/pipeline/v1/merge.go b/pkg/apis/pipeline/v1/merge.go\nindex df413a5456a..b916d8caeb5 100644\n--- a/pkg/apis/pipeline/v1/merge.go\n+++ b/pkg/apis/pipeline/v1/merge.go\n@@ -65,7 +65,18 @@ func MergeStepsWithStepTemplate(template *StepTemplate, steps []Step) ([]Step, e\n \t\tamendConflictingContainerFields(&merged, s)\n \n \t\t// Pass through original step Script, for later conversion.\n-\t\tnewStep := Step{Script: s.Script, OnError: s.OnError, Timeout: s.Timeout, StdoutConfig: s.StdoutConfig, StderrConfig: s.StderrConfig, Results: s.Results, Params: s.Params, Ref: s.Ref, When: s.When}\n+\t\tnewStep := Step{\n+\t\t\tScript:       s.Script,\n+\t\t\tOnError:      s.OnError,\n+\t\t\tTimeout:      s.Timeout,\n+\t\t\tStdoutConfig: s.StdoutConfig,\n+\t\t\tStderrConfig: s.StderrConfig,\n+\t\t\tResults:      s.Results,\n+\t\t\tParams:       s.Params,\n+\t\t\tRef:          s.Ref,\n+\t\t\tWhen:         s.When,\n+\t\t\tWorkspaces:   s.Workspaces,\n+\t\t}\n \t\tnewStep.SetContainerFields(merged)\n \t\tsteps[i] = newStep\n \t}\n", "instance_id": "tektoncd__pipeline-8272", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue with isolated workspaces not mounting properly in Tekton Pipelines. It provides a detailed expected behavior, actual behavior, and steps to reproduce the issue, including a YAML configuration snippet and logs indicating the problem (both steps accessing all credentials). Additionally, it includes relevant version information for Kubernetes and Tekton Pipelines, which is helpful for context. However, there are minor ambiguities and missing details. For instance, the problem statement references a commit and mentions a feature flag (\"enable-api-fieldscs\") set to alpha, but it does not clearly explain how this relates to the issue or whether it is a critical factor in the bug. Furthermore, the expected behavior around updating mount paths is mentioned but not elaborated with specific examples or constraints. Edge cases, such as what happens with overlapping mount paths or invalid workspace configurations, are not addressed. Overall, while the core issue is understandable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of the code change is relatively small, as it involves a single file (`merge.go`) and a straightforward modification to include the `Workspaces` field in the `Step` struct during merging. This suggests a focused fix that does not impact the broader architecture of the Tekton Pipeline system. However, understanding the problem and implementing the solution requires moderate knowledge of Tekton's internals, specifically how step templates and workspaces are merged and mounted in the runtime environment. The developer must also grasp Kubernetes concepts like volume mounts and service account tokens, as well as how Tekton translates YAML configurations into container specifications. The code change itself is simple\u2014adding a field to a struct copy\u2014but validating that this fix correctly isolates workspace mounts across steps may involve debugging complex interactions between steps and their environments. Edge cases, such as conflicting mount paths or misconfigured workspaces, are not explicitly mentioned in the problem statement but could arise during testing or implementation, adding a layer of complexity. Overall, this problem requires understanding multiple concepts (Tekton step merging, Kubernetes mounts) and making a targeted but potentially impactful change, justifying a medium difficulty score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Feature Request] Action to remove/pop entries in the list\n### Checklist\n\n- [x] I have read through the manual page (`man fzf`)\n- [x] I have searched through the existing issues\n- [x] For bug reports, I have checked if the bug is reproducible in the latest version of fzf\n\n### Output of `fzf --version`\n\n0.57.0 (0476a65)\n\n### OS\n\n- [x] Linux\n- [ ] macOS\n- [x] Windows\n- [ ] Etc.\n\n### Shell\n\n- [x] bash\n- [x] zsh\n- [ ] fish\n\n### Problem / Steps to reproduce\n\nI have encounter the use case in which I would like to remove 1 or more items in the list displayed in fzf.\n\nFor simplicity take the following example:\n\n```bash\nprintf 'top\\nmiddle\\nbottom' | fzf --bind 'ctrl-a:execute(operation that makes \"bottom\" invalid for selection)'\n```\n\nWhere the options `top`, `middle` and `bottom` are all valid options. However if you press `ctrl-a` to run some extra processing, then the `bottom` option stops being valid and should not be selected anymore.\n\nMy current solution for this is to add a reload action and filtering out the invalid option(s). However depending on the case reloading and filtering could be expensive/slow or add a lot of extra complexity. My assumption is that removing/filtering out options in fzf is probably faster and easier as the search already does that when fuzzy matching to narrow options.\n\nThe proposed feature is to include a `pop` action (name used for example) that could be used to remove items from the list.\n\n- Without arguments (`pop`) e.g. `--bind 'ctrl-a:execute(foo {1})+pop+execute(something {+f})`\n  - When multiple entries are selected, they all get removed from the list.\n  - When there is no selection (e.g. --no-multi), currently focused item is removed.\n  - Template values (`{1}`, `{n}`, `{+f}`, etc.) should remain unchanged from the start of the --bind flag until the last action in the same --bind flag even if a `pop` action was used.\n- With arguments (`pop(N)`) e.g. `--bind 'ctrl-a:execute(foo {1})+pop(5)+execute(something {+f})`\n  - Argument `N` represents a number, range of numbers `1..2` or space separated list `1 2 3 4..6` that represent the index of the entries to remove.\n  - Templates that represent indexes are valid `{n}` and `{+n}`\n\nThis is just a high level description of how the feature could work but it is open for discussion if the feature is considered.\n\nThanks.\n", "patch": "diff --git a/man/man1/fzf.1 b/man/man1/fzf.1\nindex ba3abaa510f..74d0c05d4d4 100644\n--- a/man/man1/fzf.1\n+++ b/man/man1/fzf.1\n@@ -1601,6 +1601,7 @@ A key or an event can be bound to one or more of the following actions.\n     \\fBdown\\fR                         \\fIctrl\\-j  ctrl\\-n  down\\fR\n     \\fBenable\\-search\\fR                (enable search functionality)\n     \\fBend\\-of\\-line\\fR                  \\fIctrl\\-e  end\\fR\n+    \\fBexclude\\fR                      (exclude the current item or the selected items from the result)\n     \\fBexecute(...)\\fR                 (see below for the details)\n     \\fBexecute\\-silent(...)\\fR          (see below for the details)\n     \\fBfirst\\fR                        (move to the first match; same as \\fBpos(1)\\fR)\ndiff --git a/src/core.go b/src/core.go\nindex 8f4a6d84b07..08d9e868937 100644\n--- a/src/core.go\n+++ b/src/core.go\n@@ -198,10 +198,26 @@ func Run(opts *Options) (int, error) {\n \tinputRevision := revision{}\n \tsnapshotRevision := revision{}\n \tpatternCache := make(map[string]*Pattern)\n+\tdenyMutex := sync.Mutex{}\n+\tdenylist := make(map[int32]struct{})\n+\tclearDenylist := func() {\n+\t\tdenyMutex.Lock()\n+\t\tif len(denylist) > 0 {\n+\t\t\tpatternCache = make(map[string]*Pattern)\n+\t\t}\n+\t\tdenylist = make(map[int32]struct{})\n+\t\tdenyMutex.Unlock()\n+\t}\n \tpatternBuilder := func(runes []rune) *Pattern {\n+\t\tdenyMutex.Lock()\n+\t\tdenylistCopy := make(map[int32]struct{})\n+\t\tfor k, v := range denylist {\n+\t\t\tdenylistCopy[k] = v\n+\t\t}\n+\t\tdenyMutex.Unlock()\n \t\treturn BuildPattern(cache, patternCache,\n \t\t\topts.Fuzzy, opts.FuzzyAlgo, opts.Extended, opts.Case, opts.Normalize, forward, withPos,\n-\t\t\topts.Filter == nil, nth, opts.Delimiter, inputRevision, runes)\n+\t\t\topts.Filter == nil, nth, opts.Delimiter, inputRevision, runes, denylistCopy)\n \t}\n \tmatcher := NewMatcher(cache, patternBuilder, sort, opts.Tac, eventBox, inputRevision)\n \n@@ -301,6 +317,9 @@ func Run(opts *Options) (int, error) {\n \tvar snapshot []*Chunk\n \tvar count int\n \trestart := func(command commandSpec, environ []string) {\n+\t\tif !useSnapshot {\n+\t\t\tclearDenylist()\n+\t\t}\n \t\treading = true\n \t\tchunkList.Clear()\n \t\titemIndex = 0\n@@ -347,7 +366,8 @@ func Run(opts *Options) (int, error) {\n \t\t\t\t\t} else {\n \t\t\t\t\t\treading = reading && evt == EvtReadNew\n \t\t\t\t\t}\n-\t\t\t\t\tif useSnapshot && evt == EvtReadFin {\n+\t\t\t\t\tif useSnapshot && evt == EvtReadFin { // reload-sync\n+\t\t\t\t\t\tclearDenylist()\n \t\t\t\t\t\tuseSnapshot = false\n \t\t\t\t\t}\n \t\t\t\t\tif !useSnapshot {\n@@ -378,9 +398,21 @@ func Run(opts *Options) (int, error) {\n \t\t\t\t\t\tcommand = val.command\n \t\t\t\t\t\tenviron = val.environ\n \t\t\t\t\t\tchanged = val.changed\n+\t\t\t\t\t\tbump := false\n+\t\t\t\t\t\tif len(val.denylist) > 0 && val.revision.compatible(inputRevision) {\n+\t\t\t\t\t\t\tdenyMutex.Lock()\n+\t\t\t\t\t\t\tfor _, itemIndex := range val.denylist {\n+\t\t\t\t\t\t\t\tdenylist[itemIndex] = struct{}{}\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tdenyMutex.Unlock()\n+\t\t\t\t\t\t\tbump = true\n+\t\t\t\t\t\t}\n \t\t\t\t\t\tif val.nth != nil {\n \t\t\t\t\t\t\t// Change nth and clear caches\n \t\t\t\t\t\t\tnth = *val.nth\n+\t\t\t\t\t\t\tbump = true\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tif bump {\n \t\t\t\t\t\t\tpatternCache = make(map[string]*Pattern)\n \t\t\t\t\t\t\tcache.Clear()\n \t\t\t\t\t\t\tinputRevision.bumpMinor()\ndiff --git a/src/options.go b/src/options.go\nindex 2b310612d01..4a6c3b2b1cb 100644\n--- a/src/options.go\n+++ b/src/options.go\n@@ -1603,6 +1603,8 @@ func parseActionList(masked string, original string, prevActions []*action, putA\n \t\t\t}\n \t\tcase \"bell\":\n \t\t\tappendAction(actBell)\n+\t\tcase \"exclude\":\n+\t\t\tappendAction(actExclude)\n \t\tdefault:\n \t\t\tt := isExecuteAction(specLower)\n \t\t\tif t == actIgnore {\ndiff --git a/src/pattern.go b/src/pattern.go\nindex 8919ad876b5..93640cb69af 100644\n--- a/src/pattern.go\n+++ b/src/pattern.go\n@@ -63,6 +63,7 @@ type Pattern struct {\n \trevision      revision\n \tprocFun       map[termType]algo.Algo\n \tcache         *ChunkCache\n+\tdenylist      map[int32]struct{}\n }\n \n var _splitRegex *regexp.Regexp\n@@ -73,7 +74,7 @@ func init() {\n \n // BuildPattern builds Pattern object from the given arguments\n func BuildPattern(cache *ChunkCache, patternCache map[string]*Pattern, fuzzy bool, fuzzyAlgo algo.Algo, extended bool, caseMode Case, normalize bool, forward bool,\n-\twithPos bool, cacheable bool, nth []Range, delimiter Delimiter, revision revision, runes []rune) *Pattern {\n+\twithPos bool, cacheable bool, nth []Range, delimiter Delimiter, revision revision, runes []rune, denylist map[int32]struct{}) *Pattern {\n \n \tvar asString string\n \tif extended {\n@@ -144,6 +145,7 @@ func BuildPattern(cache *ChunkCache, patternCache map[string]*Pattern, fuzzy boo\n \t\trevision:      revision,\n \t\tdelimiter:     delimiter,\n \t\tcache:         cache,\n+\t\tdenylist:      denylist,\n \t\tprocFun:       make(map[termType]algo.Algo)}\n \n \tptr.cacheKey = ptr.buildCacheKey()\n@@ -243,6 +245,9 @@ func parseTerms(fuzzy bool, caseMode Case, normalize bool, str string) []termSet\n \n // IsEmpty returns true if the pattern is effectively empty\n func (p *Pattern) IsEmpty() bool {\n+\tif len(p.denylist) > 0 {\n+\t\treturn false\n+\t}\n \tif !p.extended {\n \t\treturn len(p.text) == 0\n \t}\n@@ -296,14 +301,38 @@ func (p *Pattern) Match(chunk *Chunk, slab *util.Slab) []Result {\n func (p *Pattern) matchChunk(chunk *Chunk, space []Result, slab *util.Slab) []Result {\n \tmatches := []Result{}\n \n+\tif len(p.denylist) == 0 {\n+\t\t// Huge code duplication for minimizing unnecessary map lookups\n+\t\tif space == nil {\n+\t\t\tfor idx := 0; idx < chunk.count; idx++ {\n+\t\t\t\tif match, _, _ := p.MatchItem(&chunk.items[idx], p.withPos, slab); match != nil {\n+\t\t\t\t\tmatches = append(matches, *match)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tfor _, result := range space {\n+\t\t\t\tif match, _, _ := p.MatchItem(result.item, p.withPos, slab); match != nil {\n+\t\t\t\t\tmatches = append(matches, *match)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\treturn matches\n+\t}\n+\n \tif space == nil {\n \t\tfor idx := 0; idx < chunk.count; idx++ {\n+\t\t\tif _, prs := p.denylist[chunk.items[idx].Index()]; prs {\n+\t\t\t\tcontinue\n+\t\t\t}\n \t\t\tif match, _, _ := p.MatchItem(&chunk.items[idx], p.withPos, slab); match != nil {\n \t\t\t\tmatches = append(matches, *match)\n \t\t\t}\n \t\t}\n \t} else {\n \t\tfor _, result := range space {\n+\t\t\tif _, prs := p.denylist[result.item.Index()]; prs {\n+\t\t\t\tcontinue\n+\t\t\t}\n \t\t\tif match, _, _ := p.MatchItem(result.item, p.withPos, slab); match != nil {\n \t\t\t\tmatches = append(matches, *match)\n \t\t\t}\ndiff --git a/src/terminal.go b/src/terminal.go\nindex 9a06ad615ef..60ddaa71e04 100644\n--- a/src/terminal.go\n+++ b/src/terminal.go\n@@ -584,6 +584,7 @@ const (\n \tactShowHeader\n \tactHideHeader\n \tactBell\n+\tactExclude\n )\n \n func (a actionType) Name() string {\n@@ -621,12 +622,14 @@ type placeholderFlags struct {\n }\n \n type searchRequest struct {\n-\tsort    bool\n-\tsync    bool\n-\tnth     *[]Range\n-\tcommand *commandSpec\n-\tenviron []string\n-\tchanged bool\n+\tsort     bool\n+\tsync     bool\n+\tnth      *[]Range\n+\tcommand  *commandSpec\n+\tenviron  []string\n+\tchanged  bool\n+\tdenylist []int32\n+\trevision revision\n }\n \n type previewRequest struct {\n@@ -4751,6 +4754,7 @@ func (t *Terminal) Loop() error {\n \t\tchanged := false\n \t\tbeof := false\n \t\tqueryChanged := false\n+\t\tdenylist := []int32{}\n \n \t\t// Special handling of --sync. Activate the interface on the second tick.\n \t\tif loopIndex == 1 && t.deferActivation() {\n@@ -4907,6 +4911,21 @@ func (t *Terminal) Loop() error {\n \t\t\t\t}\n \t\t\tcase actBell:\n \t\t\t\tt.tui.Bell()\n+\t\t\tcase actExclude:\n+\t\t\t\tif len(t.selected) > 0 {\n+\t\t\t\t\tfor _, item := range t.sortSelected() {\n+\t\t\t\t\t\tdenylist = append(denylist, item.item.Index())\n+\t\t\t\t\t}\n+\t\t\t\t\t// Clear selected items\n+\t\t\t\t\tt.selected = make(map[int32]selectedItem)\n+\t\t\t\t\tt.version++\n+\t\t\t\t} else {\n+\t\t\t\t\titem := t.currentItem()\n+\t\t\t\t\tif item != nil {\n+\t\t\t\t\t\tdenylist = append(denylist, item.Index())\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tchanged = true\n \t\t\tcase actExecute, actExecuteSilent:\n \t\t\t\tt.executeCommand(a.a, false, a.t == actExecuteSilent, false, false, \"\")\n \t\t\tcase actExecuteMulti:\n@@ -6016,7 +6035,7 @@ func (t *Terminal) Loop() error {\n \t\treload := changed || newCommand != nil\n \t\tvar reloadRequest *searchRequest\n \t\tif reload {\n-\t\t\treloadRequest = &searchRequest{sort: t.sort, sync: reloadSync, nth: newNth, command: newCommand, environ: t.environ(), changed: changed}\n+\t\t\treloadRequest = &searchRequest{sort: t.sort, sync: reloadSync, nth: newNth, command: newCommand, environ: t.environ(), changed: changed, denylist: denylist, revision: t.merger.Revision()}\n \t\t}\n \t\tt.mutex.Unlock() // Must be unlocked before touching reqBox\n \n", "instance_id": "junegunn__fzf-4231", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in describing the desired feature: adding an action to remove or exclude items from the list displayed in `fzf`, a command-line fuzzy finder. The goal is well-defined, with a practical use case provided (removing invalid options after some processing). The input and output expectations are implicitly clear through the example and proposed `pop`/`exclude` action behavior. Additionally, the statement includes specific details on how the feature should behave with and without arguments, including template value preservation and index-based removal.\n\nHowever, there are minor ambiguities and missing details that prevent a perfect score. For instance, the problem statement does not explicitly address edge cases such as what happens when all items are excluded, or how the UI should behave after exclusion (e.g., cursor movement). Constraints on the number of items that can be excluded or performance implications of frequent exclusions are also not discussed. While the high-level description is open for discussion, critical implementation details (e.g., whether exclusions persist across reloads or searches) are left vague. Thus, I rate the clarity as \"Mostly Clear\" with a score of 2.\n", "difficulty_explanation": "\nI assess the difficulty of this problem as 0.65, placing it in the \"Hard\" range (0.6-0.8), due to the following factors:\n\n1. **Clarity and Complexity of Problem Description**: While the problem is mostly clear, the implementation requires interpreting the intent behind preserving template values and handling index-based exclusions, which adds a layer of logical complexity. The feature's interaction with existing search and filtering mechanisms in `fzf` also needs careful consideration.\n\n2. **Scope and Depth of Code Changes**: The provided diff shows modifications across multiple files (`core.go`, `options.go`, `pattern.go`, `terminal.go`, and the man page), indicating a non-trivial scope. The changes impact core components like item matching, terminal interaction, and action handling. The code introduces a denylist mechanism to track excluded items, requiring synchronization (mutex usage) and cache invalidation logic, which affects the system's internal state management. While it does not overhaul the architecture, it integrates deeply with existing functionality, increasing the risk of unintended side effects.\n\n3. **Number of Technical Concepts**: Solving this requires understanding several Go-specific concepts and `fzf`-specific internals:\n   - Concurrency and synchronization (mutex usage for denylist).\n   - Data structures (maps for denylist, pattern caching).\n   - Custom pattern matching and filtering logic in `fzf`.\n   - Event-driven terminal UI handling.\n   - Command-line argument parsing and action binding.\n   These concepts are moderately complex, especially for someone unfamiliar with `fzf`'s codebase, and require a solid grasp of Go's concurrency model and the tool's design patterns.\n\n4. **Edge Cases and Error Handling**: The problem statement hints at edge cases like handling multiple selections, no selections, and index-based exclusions, but does not fully specify behavior for scenarios like excluding all items, invalid indices, or interactions with reload actions. The code changes introduce logic to clear the denylist on reloads and handle exclusions without breaking the UI, but additional error handling (e.g., bounds checking for indices in `pop(N)`) may be needed. The complexity of edge cases is moderate, as they involve state consistency and UI responsiveness.\n\nOverall, this task requires a deep understanding of `fzf`'s internals, careful handling of state and concurrency, and integration across multiple modules. It is not at the extreme end of difficulty (e.g., redesigning the core architecture), but it poses significant challenges beyond a medium-level task due to the need for precise modifications in a performance-sensitive tool. Hence, a score of 0.65 reflects the hard but manageable nature of the problem for an experienced developer familiar with Go and command-line tools.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "OpenTelemetry metric name issues\n### Welcome!\n\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\n\n### What did you do?\n\nWhile reviewing the metrics for Traefik v3.1 with OpenTelemetry metrics enabled, I discovered some discrepancies which could be bugs and should be fixed or mentioned in the documentation depending on further analysis results.\r\n\r\nThe setup:\r\n\r\n```mermaid\r\ngraph LR;\r\n    traefik -- otlp --> otel-collector -- prometheus_exporter --> prometheus;\r\n```\r\n\n\n### What did you see instead?\n\nThe list of questionable metrics\r\n\r\n| metric | comment |\r\n| ------ | ------- |\r\n| `http_client_duration_milliseconds_(bucket\\|sum\\|count)` | Only present for `net_peer_name=\"/var/run/docker.sock\"`. As there is no mention in the docs about it, my guess would be that it should be actually `http_client_request_duration_seconds_(bucket\\|sum\\|count)` but is not. |\r\n| `traefik_(entrypoint\\|router\\|service)_request_duration_seconds_milliseconds_(bucket\\|sum\\|count)` | This one is super confusing. Are the values now seconds or milliseconds. The output seems to be affected by the `add_metric_suffixes` setting of the [Prometheus Exporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusexporter/README.md), but as there is already a unit present, it is strange why this changes. |\r\n| `traefik_open_connections_ratio` | the `_ratio` suffix seems also be coming from the [Prometheus Normalization](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/translator/prometheus/README.md) of the otel-collector and also affected by the `add_metric_suffixes` setting |\r\n\r\nMy expectation would have been, that the type and unit suffixes behave the same, as looking at the Metrics documentation page shows identical metric names in the OpenTelementry and Prometheus tabs.\r\n\r\n\n\n### What version of Traefik are you using?\n\n```\r\nVersion:      3.1.3\r\nCodename:     comte\r\nGo version:   go1.23.1\r\nBuilt:        2024-09-16T15:08:54Z\r\nOS/Arch:      linux/amd64\r\n```\n\n### What is your environment & configuration?\n\n```yaml\r\nmetrics:\r\n  otlp:\r\n    http:\r\n      endpoint: \"http://otel-collector:4318/v1/metrics\"\r\n    pushInterval: 10s\r\n    addRoutersLabels: true\r\n```\r\n\r\n\r\notel-collector config\r\n\r\n```yaml\r\nreceivers:\r\n  otlp:\r\n    protocols:\r\n      grpc: null\r\n      http: null\r\nexporters:\r\n  logging:\r\n    verbosity: normal\r\n  prometheus:\r\n    enable_open_metrics: true\r\n    endpoint: 0.0.0.0:8080\r\n    # add_metric_suffixes: false\r\nprocessors:\r\n  batch: null\r\n  resource:\r\n    attributes:\r\n    - action: insert\r\n      from_attribute: container.name\r\n      key: service.name\r\n    - action: upsert\r\n      key: service.instance.id\r\n      value: my.own.static.id\r\n  resourcedetection/docker:\r\n    detectors:\r\n    - env\r\n    - docker\r\n    override: true\r\n    timeout: 2s\r\n  transform:\r\n    metric_statements:\r\n    - context: datapoint\r\n      statements:\r\n      - set(attributes[\"container.id\"], resource.attributes[\"container.id\"])\r\n      - set(attributes[\"container.name\"], resource.attributes[\"container.name\"])\r\n      - set(attributes[\"container.hostname\"], resource.attributes[\"container.hostname\"])\r\n      - set(attributes[\"host.name\"], resource.attributes[\"host.name\"])\r\n      - set(attributes[\"compose.project\"], resource.attributes[\"compose.project\"])\r\n      - set(attributes[\"compose.service\"], resource.attributes[\"compose.service\"])\r\n      - set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])\r\n      - set(attributes[\"service.namespace\"], resource.attributes[\"service.namespace\"])\r\nservice:\r\n  pipelines:\r\n    metrics:\r\n      exporters:\r\n      - prometheus\r\n      - logging\r\n      processors:\r\n      - resourcedetection/docker\r\n      - resource\r\n      - transform\r\n      - batch\r\n      receivers:\r\n      - otlp\r\n```\n\n### If applicable, please paste the log output in DEBUG level\n\n_No response_\n", "patch": "diff --git a/docs/content/migration/v3.md b/docs/content/migration/v3.md\nindex 0f9ed12336..4f2f1d6e6a 100644\n--- a/docs/content/migration/v3.md\n+++ b/docs/content/migration/v3.md\n@@ -180,3 +180,10 @@ please use respectively `acme.dnsChallenge.propagation.delayBeforeChecks` and `a\n In `v3.3`, the `tracing.globalAttributes` option has been deprecated, please use the `tracing.resourceAttributes` option instead.\n The `tracing.globalAttributes` option is misleading as its name does not reflect the operation of adding resource attributes to be sent to the collector,\n and will be removed in the next major version.\n+\n+## v3.3.4\n+\n+### OpenTelemetry Request Duration metric\n+\n+In `v3.3.4`, the OpenTelemetry Request Duration metric (named `traefik_(entrypoint|router|service)_request_duration_seconds`) unit has been changed from milliseconds to seconds.\n+To be consistent with the naming and other metrics providers, the metric now reports the duration in seconds.\ndiff --git a/pkg/metrics/otel.go b/pkg/metrics/otel.go\nindex 2b1b1b5b74..32e48a5d07 100644\n--- a/pkg/metrics/otel.go\n+++ b/pkg/metrics/otel.go\n@@ -132,7 +132,7 @@ func RegisterOpenTelemetry(ctx context.Context, config *types.OTLP) Registry {\n \t\t\t\"How many HTTP requests with TLS processed on an entrypoint, partitioned by TLS Version and TLS cipher Used.\")\n \t\treg.entryPointReqDurationHistogram, _ = NewHistogramWithScale(newOTLPHistogramFrom(meter, entryPointReqDurationName,\n \t\t\t\"How long it took to process the request on an entrypoint, partitioned by status code, protocol, and method.\",\n-\t\t\t\"ms\"), time.Second)\n+\t\t\t\"s\"), time.Second)\n \t\treg.entryPointReqsBytesCounter = newOTLPCounterFrom(meter, entryPointReqsBytesTotalName,\n \t\t\t\"The total size of requests in bytes handled by an entrypoint, partitioned by status code, protocol, and method.\")\n \t\treg.entryPointRespsBytesCounter = newOTLPCounterFrom(meter, entryPointRespsBytesTotalName,\n@@ -146,7 +146,7 @@ func RegisterOpenTelemetry(ctx context.Context, config *types.OTLP) Registry {\n \t\t\t\"How many HTTP requests with TLS are processed on a router, partitioned by service, TLS Version, and TLS cipher Used.\")\n \t\treg.routerReqDurationHistogram, _ = NewHistogramWithScale(newOTLPHistogramFrom(meter, routerReqDurationName,\n \t\t\t\"How long it took to process the request on a router, partitioned by service, status code, protocol, and method.\",\n-\t\t\t\"ms\"), time.Second)\n+\t\t\t\"s\"), time.Second)\n \t\treg.routerReqsBytesCounter = newOTLPCounterFrom(meter, routerReqsBytesTotalName,\n \t\t\t\"The total size of requests in bytes handled by a router, partitioned by status code, protocol, and method.\")\n \t\treg.routerRespsBytesCounter = newOTLPCounterFrom(meter, routerRespsBytesTotalName,\n@@ -160,7 +160,7 @@ func RegisterOpenTelemetry(ctx context.Context, config *types.OTLP) Registry {\n \t\t\t\"How many HTTP requests with TLS processed on a service, partitioned by TLS version and TLS cipher.\")\n \t\treg.serviceReqDurationHistogram, _ = NewHistogramWithScale(newOTLPHistogramFrom(meter, serviceReqDurationName,\n \t\t\t\"How long it took to process the request on a service, partitioned by status code, protocol, and method.\",\n-\t\t\t\"ms\"), time.Second)\n+\t\t\t\"s\"), time.Second)\n \t\treg.serviceRetriesCounter = newOTLPCounterFrom(meter, serviceRetriesTotalName,\n \t\t\t\"How many request retries happened on a service.\")\n \t\treg.serviceServerUpGauge = newOTLPGaugeFrom(meter, serviceServerUpName,\n", "instance_id": "traefik__traefik-11523", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in identifying the issue with OpenTelemetry metric names and units in Traefik v3.1, particularly focusing on discrepancies in metric naming conventions and unit suffixes. The goal is to address inconsistencies between expected and observed metric names, as well as the confusion caused by suffixes like \"_milliseconds\" and \"_ratio\" due to interactions with the OpenTelemetry Collector and Prometheus exporter settings. The statement includes a detailed setup description, configuration files, and a table of questionable metrics with comments, which helps in understanding the context. However, there are minor ambiguities: the problem statement does not explicitly define the desired resolution for all issues (e.g., whether all metrics should be renamed or if documentation updates are sufficient for some cases). Additionally, edge cases or specific constraints (like compatibility with different collector configurations) are not mentioned, which could impact the solution's scope. Overall, it is valid and clear but lacks some finer details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following analysis based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, primarily involving updates to metric unit labels in `otel.go` (changing \"ms\" to \"s\" for consistency in request duration histograms) and a documentation update in `v3.md` to reflect this change. The modifications are confined to a single source file and a documentation file, with no apparent impact on the broader system architecture or interactions between multiple modules. The amount of code change is minimal, focusing on string literals and a small documentation addition.\n\n2. **Number of Technical Concepts:** Solving this issue requires a basic understanding of OpenTelemetry metrics in Go, specifically how histograms and counters are defined and reported using the OpenTelemetry SDK. Familiarity with Traefik's metric system and the interaction with the OpenTelemetry Collector (e.g., Prometheus exporter settings like `add_metric_suffixes`) is necessary but not overly complex. No advanced algorithms, design patterns, or domain-specific knowledge beyond metrics instrumentation are needed. The concepts involved are straightforward for someone with moderate experience in Go and observability tools.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes do not introduce or modify error handling logic. However, there is a minor implicit concern about ensuring consistency across different metric providers and collector configurations, which might require validation but does not appear to be complex. The provided changes do not address potential edge cases like backward compatibility with existing metric consumers expecting \"ms\" units, but this seems to be a documentation-driven mitigation rather than a code-level challenge.\n\n4. **Overall Assessment:** The task involves understanding a specific part of the codebase related to metrics and making simple, targeted modifications to align unit reporting with naming conventions. It does not require deep architectural changes or advanced technical expertise beyond basic familiarity with OpenTelemetry and Go. The impact is low, and the problem can be resolved with minimal risk of introducing bugs or performance issues. Therefore, a difficulty score of 0.30 reflects an \"Easy\" problem that requires some code logic understanding and simple modifications.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Keep URIs in server list\n### Welcome!\r\n\r\n- [X] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any.\r\n- [X] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.\r\n\r\n### What did you expect to see?\r\n\r\nI have two backends that I'd like to load balance between, but both servers are authenticated using a different URI:\r\n\r\n```yaml\r\nservices:\r\n  my-backend:\r\n    loadBalancer:\r\n      servers:\r\n        - url: https://my-first-backend.com/my-auth-token\r\n        - url: https://my-second-backend.com/my-other-different-auth-token\r\n```\r\n\r\nSince Traefik strips the URI, this configuration is impossible. Also, since the URIs are different between these servers, the add-prefix middleware will not work, nor will health checks...\r\n\r\nThe end goal is to have a request come into traefik (`https://traefik:8080`) and then get routed to either backend, as long as it's healthy. In the future, more backends could be added and I'd like Traefik to load balance between them.\r\n\r\nIs there any solution to this with Traefik or should I find a different proxy?\r\n\r\nThe feature request is to be able to keep the URI in the server field. If I could keep the URI, it seems like it would either solve my issue or get me closer to using this...\n", "patch": "diff --git a/docs/content/reference/dynamic-configuration/file.toml b/docs/content/reference/dynamic-configuration/file.toml\nindex 18fc9bc84c..065d3ca16f 100644\n--- a/docs/content/reference/dynamic-configuration/file.toml\n+++ b/docs/content/reference/dynamic-configuration/file.toml\n@@ -59,10 +59,12 @@\n         [[http.services.Service02.loadBalancer.servers]]\n           url = \"foobar\"\n           weight = 42\n+          preservePath = true\n \n         [[http.services.Service02.loadBalancer.servers]]\n           url = \"foobar\"\n           weight = 42\n+          preservePath = true\n         [http.services.Service02.loadBalancer.healthCheck]\n           scheme = \"foobar\"\n           mode = \"foobar\"\ndiff --git a/docs/content/reference/dynamic-configuration/file.yaml b/docs/content/reference/dynamic-configuration/file.yaml\nindex d0b0fc0fb7..08cb9c3851 100644\n--- a/docs/content/reference/dynamic-configuration/file.yaml\n+++ b/docs/content/reference/dynamic-configuration/file.yaml\n@@ -66,8 +66,10 @@ http:\n         servers:\n           - url: foobar\n             weight: 42\n+            preservePath: true\n           - url: foobar\n             weight: 42\n+            preservePath: true\n         healthCheck:\n           scheme: foobar\n           mode: foobar\ndiff --git a/docs/content/reference/dynamic-configuration/kv-ref.md b/docs/content/reference/dynamic-configuration/kv-ref.md\nindex 01de7ee69c..a2f24a8053 100644\n--- a/docs/content/reference/dynamic-configuration/kv-ref.md\n+++ b/docs/content/reference/dynamic-configuration/kv-ref.md\n@@ -256,8 +256,10 @@ THIS FILE MUST NOT BE EDITED BY HAND\n | `traefik/http/services/Service02/loadBalancer/healthCheck/timeout` | `42s` |\n | `traefik/http/services/Service02/loadBalancer/passHostHeader` | `true` |\n | `traefik/http/services/Service02/loadBalancer/responseForwarding/flushInterval` | `42s` |\n+| `traefik/http/services/Service02/loadBalancer/servers/0/preservePath` | `true` |\n | `traefik/http/services/Service02/loadBalancer/servers/0/url` | `foobar` |\n | `traefik/http/services/Service02/loadBalancer/servers/0/weight` | `42` |\n+| `traefik/http/services/Service02/loadBalancer/servers/1/preservePath` | `true` |\n | `traefik/http/services/Service02/loadBalancer/servers/1/url` | `foobar` |\n | `traefik/http/services/Service02/loadBalancer/servers/1/weight` | `42` |\n | `traefik/http/services/Service02/loadBalancer/serversTransport` | `foobar` |\ndiff --git a/docs/content/routing/services/index.md b/docs/content/routing/services/index.md\nindex 8e4cc41f06..b58ac814a5 100644\n--- a/docs/content/routing/services/index.md\n+++ b/docs/content/routing/services/index.md\n@@ -116,12 +116,8 @@ Each service has a load-balancer, even if there is only one server to forward tr\n #### Servers\n \n Servers declare a single instance of your program.\n-The `url` option point to a specific instance.\n \n-!!! info \"\"\n-    Paths in the servers' `url` have no effect.\n-    If you want the requests to be sent to a specific path on your servers,\n-    configure your [`routers`](../routers/index.md) to use a corresponding [middleware](../../middlewares/overview.md) (e.g. the [AddPrefix](../../middlewares/http/addprefix.md) or [ReplacePath](../../middlewares/http/replacepath.md)) middlewares.\n+The `url` option point to a specific instance.\n \n ??? example \"A Service with One Server -- Using the [File Provider](../../providers/file.md)\"\n \n@@ -173,6 +169,34 @@ The `weight` option allows for weighted load balancing on the servers.\n           weight = 1\n     ```\n \n+The `preservePath` option allows to preserve the URL path.\n+\n+!!! info \"Health Check\"\n+\n+    When a [health check](#health-check) is configured for the server, the path is not preserved.\n+\n+??? example \"A Service with One Server and PreservePath -- Using the [File Provider](../../providers/file.md)\"\n+\n+    ```yaml tab=\"YAML\"\n+    ## Dynamic configuration\n+    http:\n+      services:\n+        my-service:\n+          loadBalancer:\n+            servers:\n+              - url: \"http://private-ip-server-1/base\"\n+                preservePath: true\n+    ```\n+\n+    ```toml tab=\"TOML\"\n+    ## Dynamic configuration\n+    [http.services]\n+      [http.services.my-service.loadBalancer]\n+        [[http.services.my-service.loadBalancer.servers]]\n+          url = \"http://private-ip-server-1/base\"\n+          preservePath = true\n+    ```\n+\n #### Load-balancing\n \n For now, only round robin load balancing is supported:\ndiff --git a/pkg/config/dynamic/http_config.go b/pkg/config/dynamic/http_config.go\nindex d347df81e5..e29bdd30a8 100644\n--- a/pkg/config/dynamic/http_config.go\n+++ b/pkg/config/dynamic/http_config.go\n@@ -244,10 +244,11 @@ func (r *ResponseForwarding) SetDefaults() {\n \n // Server holds the server configuration.\n type Server struct {\n-\tURL    string `json:\"url,omitempty\" toml:\"url,omitempty\" yaml:\"url,omitempty\" label:\"-\"`\n-\tWeight *int   `json:\"weight,omitempty\" toml:\"weight,omitempty\" yaml:\"weight,omitempty\" label:\"weight\"`\n-\tScheme string `json:\"-\" toml:\"-\" yaml:\"-\" file:\"-\"`\n-\tPort   string `json:\"-\" toml:\"-\" yaml:\"-\" file:\"-\"`\n+\tURL          string `json:\"url,omitempty\" toml:\"url,omitempty\" yaml:\"url,omitempty\" label:\"-\"`\n+\tWeight       *int   `json:\"weight,omitempty\" toml:\"weight,omitempty\" yaml:\"weight,omitempty\" label:\"weight\" export:\"true\"`\n+\tPreservePath bool   `json:\"preservePath,omitempty\" toml:\"preservePath,omitempty\" yaml:\"preservePath,omitempty\" label:\"-\" export:\"true\"`\n+\tScheme       string `json:\"-\" toml:\"-\" yaml:\"-\" file:\"-\"`\n+\tPort         string `json:\"-\" toml:\"-\" yaml:\"-\" file:\"-\"`\n }\n \n // SetDefaults Default values for a Server.\ndiff --git a/pkg/proxy/fast/builder.go b/pkg/proxy/fast/builder.go\nindex f330d6756d..c53363dad9 100644\n--- a/pkg/proxy/fast/builder.go\n+++ b/pkg/proxy/fast/builder.go\n@@ -68,7 +68,7 @@ func (r *ProxyBuilder) Update(newConfigs map[string]*dynamic.ServersTransport) {\n }\n \n // Build builds a new ReverseProxy with the given configuration.\n-func (r *ProxyBuilder) Build(cfgName string, targetURL *url.URL, passHostHeader bool) (http.Handler, error) {\n+func (r *ProxyBuilder) Build(cfgName string, targetURL *url.URL, passHostHeader, preservePath bool) (http.Handler, error) {\n \tproxyURL, err := r.proxy(&http.Request{URL: targetURL})\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"getting proxy: %w\", err)\n@@ -90,7 +90,7 @@ func (r *ProxyBuilder) Build(cfgName string, targetURL *url.URL, passHostHeader\n \t}\n \n \tpool := r.getPool(cfgName, cfg, tlsConfig, targetURL, proxyURL)\n-\treturn NewReverseProxy(targetURL, proxyURL, r.debug, passHostHeader, responseHeaderTimeout, pool)\n+\treturn NewReverseProxy(targetURL, proxyURL, r.debug, passHostHeader, preservePath, responseHeaderTimeout, pool)\n }\n \n func (r *ProxyBuilder) getPool(cfgName string, config *dynamic.ServersTransport, tlsConfig *tls.Config, targetURL *url.URL, proxyURL *url.URL) *connPool {\ndiff --git a/pkg/proxy/fast/proxy.go b/pkg/proxy/fast/proxy.go\nindex e61a32ad85..388a4d73ce 100644\n--- a/pkg/proxy/fast/proxy.go\n+++ b/pkg/proxy/fast/proxy.go\n@@ -121,11 +121,12 @@ type ReverseProxy struct {\n \n \ttargetURL             *url.URL\n \tpassHostHeader        bool\n+\tpreservePath          bool\n \tresponseHeaderTimeout time.Duration\n }\n \n // NewReverseProxy creates a new ReverseProxy.\n-func NewReverseProxy(targetURL *url.URL, proxyURL *url.URL, debug, passHostHeader bool, responseHeaderTimeout time.Duration, connPool *connPool) (*ReverseProxy, error) {\n+func NewReverseProxy(targetURL, proxyURL *url.URL, debug, passHostHeader, preservePath bool, responseHeaderTimeout time.Duration, connPool *connPool) (*ReverseProxy, error) {\n \tvar proxyAuth string\n \tif proxyURL != nil && proxyURL.User != nil && targetURL.Scheme == \"http\" {\n \t\tusername := proxyURL.User.Username()\n@@ -136,6 +137,7 @@ func NewReverseProxy(targetURL *url.URL, proxyURL *url.URL, debug, passHostHeade\n \treturn &ReverseProxy{\n \t\tdebug:                 debug,\n \t\tpassHostHeader:        passHostHeader,\n+\t\tpreservePath:          preservePath,\n \t\ttargetURL:             targetURL,\n \t\tproxyAuth:             proxyAuth,\n \t\tconnPool:              connPool,\n@@ -207,6 +209,11 @@ func (p *ReverseProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n \n \tu2.Path = u.Path\n \tu2.RawPath = u.RawPath\n+\n+\tif p.preservePath {\n+\t\tu2.Path, u2.RawPath = proxyhttputil.JoinURLPath(p.targetURL, u)\n+\t}\n+\n \tu2.RawQuery = strings.ReplaceAll(u.RawQuery, \";\", \"&\")\n \n \toutReq.SetHost(u2.Host)\ndiff --git a/pkg/proxy/httputil/builder.go b/pkg/proxy/httputil/builder.go\nindex ee88bd0c39..64360517ae 100644\n--- a/pkg/proxy/httputil/builder.go\n+++ b/pkg/proxy/httputil/builder.go\n@@ -38,7 +38,7 @@ func NewProxyBuilder(transportManager TransportManager, semConvMetricsRegistry *\n func (r *ProxyBuilder) Update(_ map[string]*dynamic.ServersTransport) {}\n \n // Build builds a new httputil.ReverseProxy with the given configuration.\n-func (r *ProxyBuilder) Build(cfgName string, targetURL *url.URL, shouldObserve, passHostHeader bool, flushInterval time.Duration) (http.Handler, error) {\n+func (r *ProxyBuilder) Build(cfgName string, targetURL *url.URL, shouldObserve, passHostHeader, preservePath bool, flushInterval time.Duration) (http.Handler, error) {\n \troundTripper, err := r.transportManager.GetRoundTripper(cfgName)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"getting RoundTripper: %w\", err)\n@@ -50,5 +50,5 @@ func (r *ProxyBuilder) Build(cfgName string, targetURL *url.URL, shouldObserve,\n \t\troundTripper = newObservabilityRoundTripper(r.semConvMetricsRegistry, roundTripper)\n \t}\n \n-\treturn buildSingleHostProxy(targetURL, passHostHeader, flushInterval, roundTripper, r.bufferPool), nil\n+\treturn buildSingleHostProxy(targetURL, passHostHeader, preservePath, flushInterval, roundTripper, r.bufferPool), nil\n }\ndiff --git a/pkg/proxy/httputil/proxy.go b/pkg/proxy/httputil/proxy.go\nindex e0401ea1f3..8ed35c23ac 100644\n--- a/pkg/proxy/httputil/proxy.go\n+++ b/pkg/proxy/httputil/proxy.go\n@@ -15,15 +15,17 @@ import (\n \t\"golang.org/x/net/http/httpguts\"\n )\n \n-// StatusClientClosedRequest non-standard HTTP status code for client disconnection.\n-const StatusClientClosedRequest = 499\n+const (\n+\t// StatusClientClosedRequest non-standard HTTP status code for client disconnection.\n+\tStatusClientClosedRequest = 499\n \n-// StatusClientClosedRequestText non-standard HTTP status for client disconnection.\n-const StatusClientClosedRequestText = \"Client Closed Request\"\n+\t// StatusClientClosedRequestText non-standard HTTP status for client disconnection.\n+\tStatusClientClosedRequestText = \"Client Closed Request\"\n+)\n \n-func buildSingleHostProxy(target *url.URL, passHostHeader bool, flushInterval time.Duration, roundTripper http.RoundTripper, bufferPool httputil.BufferPool) http.Handler {\n+func buildSingleHostProxy(target *url.URL, passHostHeader bool, preservePath bool, flushInterval time.Duration, roundTripper http.RoundTripper, bufferPool httputil.BufferPool) http.Handler {\n \treturn &httputil.ReverseProxy{\n-\t\tDirector:      directorBuilder(target, passHostHeader),\n+\t\tDirector:      directorBuilder(target, passHostHeader, preservePath),\n \t\tTransport:     roundTripper,\n \t\tFlushInterval: flushInterval,\n \t\tBufferPool:    bufferPool,\n@@ -31,7 +33,7 @@ func buildSingleHostProxy(target *url.URL, passHostHeader bool, flushInterval ti\n \t}\n }\n \n-func directorBuilder(target *url.URL, passHostHeader bool) func(req *http.Request) {\n+func directorBuilder(target *url.URL, passHostHeader bool, preservePath bool) func(req *http.Request) {\n \treturn func(outReq *http.Request) {\n \t\toutReq.URL.Scheme = target.Scheme\n \t\toutReq.URL.Host = target.Host\n@@ -46,6 +48,11 @@ func directorBuilder(target *url.URL, passHostHeader bool) func(req *http.Reques\n \n \t\toutReq.URL.Path = u.Path\n \t\toutReq.URL.RawPath = u.RawPath\n+\n+\t\tif preservePath {\n+\t\t\toutReq.URL.Path, outReq.URL.RawPath = JoinURLPath(target, u)\n+\t\t}\n+\n \t\t// If a plugin/middleware adds semicolons in query params, they should be urlEncoded.\n \t\toutReq.URL.RawQuery = strings.ReplaceAll(u.RawQuery, \";\", \"&\")\n \t\toutReq.RequestURI = \"\" // Outgoing request should not have RequestURI\n@@ -54,7 +61,7 @@ func directorBuilder(target *url.URL, passHostHeader bool) func(req *http.Reques\n \t\toutReq.ProtoMajor = 1\n \t\toutReq.ProtoMinor = 1\n \n-\t\t// Do not pass client Host header unless optsetter PassHostHeader is set.\n+\t\t// Do not pass client Host header unless option PassHostHeader is set.\n \t\tif !passHostHeader {\n \t\t\toutReq.Host = outReq.URL.Host\n \t\t}\n@@ -106,6 +113,13 @@ func ErrorHandler(w http.ResponseWriter, req *http.Request, err error) {\n \t}\n }\n \n+func statusText(statusCode int) string {\n+\tif statusCode == StatusClientClosedRequest {\n+\t\treturn StatusClientClosedRequestText\n+\t}\n+\treturn http.StatusText(statusCode)\n+}\n+\n // ComputeStatusCode computes the HTTP status code according to the given error.\n func ComputeStatusCode(err error) int {\n \tswitch {\n@@ -127,9 +141,38 @@ func ComputeStatusCode(err error) int {\n \treturn http.StatusInternalServerError\n }\n \n-func statusText(statusCode int) string {\n-\tif statusCode == StatusClientClosedRequest {\n-\t\treturn StatusClientClosedRequestText\n+// JoinURLPath computes the joined path and raw path of the given URLs.\n+// From https://github.com/golang/go/blob/b521ebb55a9b26c8824b219376c7f91f7cda6ec2/src/net/http/httputil/reverseproxy.go#L221\n+func JoinURLPath(a, b *url.URL) (path, rawpath string) {\n+\tif a.RawPath == \"\" && b.RawPath == \"\" {\n+\t\treturn singleJoiningSlash(a.Path, b.Path), \"\"\n \t}\n-\treturn http.StatusText(statusCode)\n+\n+\t// Same as singleJoiningSlash, but uses EscapedPath to determine\n+\t// whether a slash should be added\n+\tapath := a.EscapedPath()\n+\tbpath := b.EscapedPath()\n+\n+\taslash := strings.HasSuffix(apath, \"/\")\n+\tbslash := strings.HasPrefix(bpath, \"/\")\n+\n+\tswitch {\n+\tcase aslash && bslash:\n+\t\treturn a.Path + b.Path[1:], apath + bpath[1:]\n+\tcase !aslash && !bslash:\n+\t\treturn a.Path + \"/\" + b.Path, apath + \"/\" + bpath\n+\t}\n+\treturn a.Path + b.Path, apath + bpath\n+}\n+\n+func singleJoiningSlash(a, b string) string {\n+\taslash := strings.HasSuffix(a, \"/\")\n+\tbslash := strings.HasPrefix(b, \"/\")\n+\tswitch {\n+\tcase aslash && bslash:\n+\t\treturn a + b[1:]\n+\tcase !aslash && !bslash:\n+\t\treturn a + \"/\" + b\n+\t}\n+\treturn a + b\n }\ndiff --git a/pkg/proxy/smart_builder.go b/pkg/proxy/smart_builder.go\nindex 1abadcca74..08b247c53f 100644\n--- a/pkg/proxy/smart_builder.go\n+++ b/pkg/proxy/smart_builder.go\n@@ -45,7 +45,7 @@ func (b *SmartBuilder) Update(newConfigs map[string]*dynamic.ServersTransport) {\n }\n \n // Build builds an HTTP proxy for the given URL using the ServersTransport with the given name.\n-func (b *SmartBuilder) Build(configName string, targetURL *url.URL, shouldObserve, passHostHeader bool, flushInterval time.Duration) (http.Handler, error) {\n+func (b *SmartBuilder) Build(configName string, targetURL *url.URL, shouldObserve, passHostHeader, preservePath bool, flushInterval time.Duration) (http.Handler, error) {\n \tserversTransport, err := b.transportManager.Get(configName)\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"getting ServersTransport: %w\", err)\n@@ -55,7 +55,7 @@ func (b *SmartBuilder) Build(configName string, targetURL *url.URL, shouldObserv\n \t// For the https scheme we cannot guess if the backend communication will use HTTP2,\n \t// thus we check if HTTP/2 is disabled to use the fast proxy implementation when this is possible.\n \tif targetURL.Scheme == \"h2c\" || (targetURL.Scheme == \"https\" && !serversTransport.DisableHTTP2) {\n-\t\treturn b.proxyBuilder.Build(configName, targetURL, shouldObserve, passHostHeader, flushInterval)\n+\t\treturn b.proxyBuilder.Build(configName, targetURL, shouldObserve, passHostHeader, preservePath, flushInterval)\n \t}\n-\treturn b.fastProxyBuilder.Build(configName, targetURL, passHostHeader)\n+\treturn b.fastProxyBuilder.Build(configName, targetURL, passHostHeader, preservePath)\n }\ndiff --git a/pkg/server/service/service.go b/pkg/server/service/service.go\nindex e37ecdf09e..349c031799 100644\n--- a/pkg/server/service/service.go\n+++ b/pkg/server/service/service.go\n@@ -42,7 +42,7 @@ const (\n \n // ProxyBuilder builds reverse proxy handlers.\n type ProxyBuilder interface {\n-\tBuild(cfgName string, targetURL *url.URL, shouldObserve, passHostHeader bool, flushInterval time.Duration) (http.Handler, error)\n+\tBuild(cfgName string, targetURL *url.URL, shouldObserve, passHostHeader, preservePath bool, flushInterval time.Duration) (http.Handler, error)\n \tUpdate(configs map[string]*dynamic.ServersTransport)\n }\n \n@@ -338,7 +338,7 @@ func (m *Manager) getLoadBalancerServiceHandler(ctx context.Context, serviceName\n \t\tqualifiedSvcName := provider.GetQualifiedName(ctx, serviceName)\n \n \t\tshouldObserve := m.observabilityMgr.ShouldAddTracing(qualifiedSvcName) || m.observabilityMgr.ShouldAddMetrics(qualifiedSvcName)\n-\t\tproxy, err := m.proxyBuilder.Build(service.ServersTransport, target, shouldObserve, passHostHeader, flushInterval)\n+\t\tproxy, err := m.proxyBuilder.Build(service.ServersTransport, target, shouldObserve, passHostHeader, server.PreservePath, flushInterval)\n \t\tif err != nil {\n \t\t\treturn nil, fmt.Errorf(\"error building proxy for server URL %s: %w\", server.URL, err)\n \t\t}\n", "instance_id": "traefik__traefik-11193", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal: the user wants Traefik to preserve the URI path in the server configuration for load balancing between backends with different authentication tokens in their URLs. The input (YAML configuration) and desired output (load balancing with preserved URIs) are implied through the description and example. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly address how health checks should behave with preserved paths (though the code changes note an exception for health checks). Additionally, edge cases such as malformed URLs, conflicting middleware, or performance implications of preserving paths are not mentioned. While the intent is clear, these missing details prevent a perfect clarity score. The statement is valid and provides enough context to understand the feature request, but it could benefit from more explicit constraints or examples of complex scenarios.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes spans multiple files and modules within the Traefik codebase, including configuration structs (`http_config.go`), proxy logic (`fast/proxy.go`, `httputil/proxy.go`), and service management (`service.go`), as well as documentation updates. This indicates a need to understand and modify interactions between different components of the system. Second, the technical concepts involved include HTTP proxy behavior, URL path manipulation, and load balancing logic, requiring a solid grasp of Go's `net/http` and `net/url` packages, as well as Traefik's internal architecture for proxying and configuration management. Third, the changes introduce a new configuration option (`preservePath`) and modify request handling logic, which could impact existing functionality (e.g., health checks, as noted in the code) and requires careful consideration of edge cases like URL encoding, path joining, and potential conflicts with other middlewares. While the problem does not involve advanced algorithms or system-level redesign, it demands a deep understanding of Traefik's proxying mechanisms and careful integration to avoid breaking existing behavior. The score of 0.65 reflects the complexity of navigating and modifying a moderately intricate codebase with significant cross-file impact, balanced by the fact that the core logic (path preservation) is relatively straightforward once the architecture is understood.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support for Redis in RateLimiter middleware\n### Do you want to request a *feature* or report a *bug*?\r\n\r\nFeature\r\n\r\n### What did you expect to see?\r\n\r\nCurrently, all rate limiter data is kept in the memory which is a problem when you scale to more instances of Traefik. Many different projects are using our APIs (  custom header with project name as source criterion ). The only way we get it to work is to configure our load balancer to always send traffic for a specific project to the same instance of Traefik, which has its issues.  We would like to use Redis to keep all this data in one place.\r\n\r\nAlso, if there is no reason why it shouldn't be implemented, I would like to contribute to the project.\n", "patch": "diff --git a/docs/content/middlewares/http/ratelimit.md b/docs/content/middlewares/http/ratelimit.md\nindex dc03dc9a37..8759e9255e 100644\n--- a/docs/content/middlewares/http/ratelimit.md\n+++ b/docs/content/middlewares/http/ratelimit.md\n@@ -496,3 +496,718 @@ http:\n     [http.middlewares.test-ratelimit.rateLimit.sourceCriterion]\n       requestHost = true\n ```\n+\n+### `redis`\n+\n+Enables distributed rate limit using `redis` to store the tokens.\n+If not set, Traefik's in-memory storage is used by default.\n+\n+#### `redis.endpoints`\n+\n+_Required, Default=\"127.0.0.1:6379\"_\n+\n+Defines how to connect to the Redis server.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.endpoints=127.0.0.1:6379\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      endpoints:\n+        - \"127.0.0.1:6379\"\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.endpoints=127.0.0.1:6379\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          endpoints:\n+            - \"127.0.0.1:6379\"\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      endpoints = [\"127.0.0.1:6379\"]\n+```\n+\n+#### `redis.username`\n+\n+_Optional, Default=\"\"_\n+\n+Defines the username used to authenticate with the Redis server.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+    - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.username=user\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+   name: test-ratelimit\n+spec:\n+   rateLimit:\n+      # ...\n+      redis:\n+         secret: mysecret\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+   name: mysecret\n+   namespace: default\n+\n+data:\n+   username: dXNlcm5hbWU=\n+   password: cGFzc3dvcmQ=\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.username=user\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          username: user\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      username = \"user\"\n+```\n+\n+#### `redis.password`\n+\n+_Optional, Default=\"\"_\n+\n+Defines the password to authenticate against the Redis server.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+    - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.password=password\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+   name: test-ratelimit\n+spec:\n+   rateLimit:\n+      # ...\n+      redis:\n+         secret: mysecret\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+   name: mysecret\n+   namespace: default\n+\n+data:\n+   username: dXNlcm5hbWU=\n+   password: cGFzc3dvcmQ=\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.password=password\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          password: password\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      password = \"password\"\n+```\n+\n+#### `redis.db`\n+\n+_Optional, Default=0_\n+\n+Defines the database to select after connecting to the Redis.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.db=0\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      db: 0\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.db=0\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          db: 0\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      db = 0\n+```\n+\n+#### `redis.tls`\n+\n+Same as this [config](https://doc.traefik.io/traefik/providers/redis/#tls)\n+\n+_Optional_\n+\n+Defines the TLS configuration used for the secure connection to Redis.\n+\n+##### `redis.tls.ca`\n+\n+_Optional_\n+\n+`ca` is the path to the certificate authority used for the secure connection to Redis,\n+it defaults to the system bundle.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+    - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.ca=path/to/ca.crt\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      tls:\n+        caSecret: mycasercret\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: mycasercret\n+  namespace: default\n+\n+data:\n+  # Must contain a certificate under either a `tls.ca` or a `ca.crt` key. \n+  tls.ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.ca=path/to/ca.crt\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    rateLimit:\n+      # ... \n+      redis:\n+        tls:\n+          ca: path/to/ca.crt\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[providers.redis.tls]\n+  ca = \"path/to/ca.crt\"\n+```\n+\n+##### `redis.tls.cert`\n+\n+_Optional_\n+\n+`cert` is the path to the public certificate used for the secure connection to Redis.\n+When this option is set, the `key` option is required.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.cert=path/to/foo.cert\"\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.key=path/to/foo.key\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+   name: test-ratelimit\n+spec:\n+   rateLimit:\n+      # ...\n+      redis:\n+         tls:\n+           certSecret: mytlscert\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+   name: mytlscert\n+   namespace: default\n+\n+data:\n+   tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=\n+   tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCi0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0=\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.cert=path/to/foo.cert\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.key=path/to/foo.key\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        redis:\n+          tls:\n+            cert: path/to/foo.cert\n+            key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      [http.middlewares.test-ratelimit.rateLimit.redis.tls]\n+        cert = \"path/to/foo.cert\"\n+        key = \"path/to/foo.key\"\n+```\n+\n+##### `redis.tls.key`\n+\n+_Optional_\n+\n+`key` is the path to the private key used for the secure connection to Redis.\n+When this option is set, the `cert` option is required.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.cert=path/to/foo.cert\"\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.key=path/to/foo.key\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+   name: test-ratelimit\n+spec:\n+   rateLimit:\n+      # ...\n+      redis:\n+         tls:\n+            certSecret: mytlscert\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+   name: mytlscert\n+   namespace: default\n+\n+data:\n+   tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=\n+   tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCi0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0=\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.cert=path/to/foo.cert\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.key=path/to/foo.key\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        redis:\n+          tls:\n+            cert: path/to/foo.cert\n+            key: path/to/foo.key\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      [http.middlewares.test-ratelimit.rateLimit.redis.tls]\n+        cert = \"path/to/foo.cert\"\n+        key = \"path/to/foo.key\"\n+```\n+\n+##### `redis.tls.insecureSkipVerify`\n+\n+_Optional, Default=false_\n+\n+If `insecureSkipVerify` is `true`, the TLS connection to Redis accepts any certificate presented by the server regardless of the hostnames it covers.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.insecureSkipVerify=true\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      tls:\n+        insecureSkipVerify: true\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.tls.insecureSkipVerify=true\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          tls:\n+            insecureSkipVerify: true\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      [http.middlewares.test-ratelimit.rateLimit.redis.tls]\n+        insecureSkipVerify = true\n+```\n+\n+#### `redis.poolSize`\n+\n+_Optional, Default=0_\n+\n+Defines the base number of socket connections.\n+\n+If there are not enough connections in the pool, new connections will be allocated beyond `redis.poolSize`. \n+You can limit this using `redis.maxActiveConns`.\n+\n+Zero means 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.poolSize=42\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      poolSize: 42\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.poolSize=42\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          poolSize: 42\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      poolSize = 42\n+```\n+\n+#### `redis.minIdleConns`\n+\n+_Optional, Default=0_\n+\n+Defines the minimum number of idle connections, which is useful when establishing new connections is slow.\n+Zero means that idle connections are not closed.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+    - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.minIdleConns=42\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      minIdleConns: 42\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.minIdleConns=42\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          minIdleConns: 42\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      minIdleConns = 42\n+```\n+\n+#### `redis.maxActiveConns`\n+\n+_Optional, Default=0_\n+\n+Defines the maximum number of connections the pool can allocate at a given time.\n+Zero means no limit.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+    - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.maxActiveConns=42\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      maxActiveConns: 42\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.maxActiveConns=42\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          maxActiveConns: 42\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      maxActiveConns = 42\n+```\n+\n+#### `redis.readTimeout`\n+\n+_Optional, Default=3s_\n+\n+Defines the timeout for socket reads. \n+If reached, commands will fail with a timeout instead of blocking.\n+Zero means no timeout.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.readTimeout=42s\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      readTimeout: 42s\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.readTimeout=42s\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          readTimeout: 42s\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      readTimeout = \"42s\"\n+```\n+\n+#### `redis.writeTimeout`\n+\n+_Optional, Default=3s_\n+\n+Defines the timeout for socket writes. \n+If reached, commands will fail with a timeout instead of blocking. \n+Zero means no timeout.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.writeTimeout=42s\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      writeTimeout: 42s\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.writeTimeout=42s\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          writeTimeout: 42s\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      writeTimeout = \"42s\"\n+```\n+\n+#### `redis.dialTimeout`\n+\n+_Optional, Default=5s_\n+\n+Defines the dial timeout for establishing new connections.\n+Zero means no timeout.\n+\n+```yaml tab=\"Docker & Swarm\"\n+labels:\n+  - \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.dialTimeout=42s\"\n+```\n+\n+```yaml tab=\"Kubernetes\"\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: test-ratelimit\n+spec:\n+  rateLimit:\n+    # ...\n+    redis:\n+      dialTimeout: 42s\n+```\n+\n+```yaml tab=\"Consul Catalog\"\n+- \"traefik.http.middlewares.test-ratelimit.ratelimit.redis.dialTimeout=42s\"\n+```\n+\n+```yaml tab=\"File (YAML)\"\n+http:\n+  middlewares:\n+    test-ratelimit:\n+      rateLimit:\n+        # ...\n+        redis:\n+          dialTimeout: 42s\n+```\n+\n+```toml tab=\"File (TOML)\"\n+[http.middlewares]\n+  [http.middlewares.test-ratelimit.rateLimit]\n+    [http.middlewares.test-ratelimit.rateLimit.redis]\n+      dialTimeout = \"42s\"\n+```\ndiff --git a/docs/content/reference/dynamic-configuration/docker-labels.yml b/docs/content/reference/dynamic-configuration/docker-labels.yml\nindex baf1c4186a..bc8b268f91 100644\n--- a/docs/content/reference/dynamic-configuration/docker-labels.yml\n+++ b/docs/content/reference/dynamic-configuration/docker-labels.yml\n@@ -132,6 +132,20 @@\n - \"traefik.http.middlewares.middleware18.ratelimit.average=42\"\n - \"traefik.http.middlewares.middleware18.ratelimit.burst=42\"\n - \"traefik.http.middlewares.middleware18.ratelimit.period=42s\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.db=42\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.dialtimeout=42s\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.endpoints=foobar, foobar\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.maxactiveconns=42\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.minidleconns=42\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.password=foobar\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.poolsize=42\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.readtimeout=42s\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.tls.ca=foobar\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.tls.cert=foobar\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.tls.insecureskipverify=true\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.tls.key=foobar\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.username=foobar\"\n+- \"traefik.http.middlewares.middleware18.ratelimit.redis.writetimeout=42s\"\n - \"traefik.http.middlewares.middleware18.ratelimit.sourcecriterion.ipstrategy.depth=42\"\n - \"traefik.http.middlewares.middleware18.ratelimit.sourcecriterion.ipstrategy.excludedips=foobar, foobar\"\n - \"traefik.http.middlewares.middleware18.ratelimit.sourcecriterion.ipstrategy.ipv6subnet=42\"\ndiff --git a/docs/content/reference/dynamic-configuration/file.toml b/docs/content/reference/dynamic-configuration/file.toml\nindex a5d45447b0..0c5ae3bbdf 100644\n--- a/docs/content/reference/dynamic-configuration/file.toml\n+++ b/docs/content/reference/dynamic-configuration/file.toml\n@@ -311,6 +311,22 @@\n             depth = 42\n             excludedIPs = [\"foobar\", \"foobar\"]\n             ipv6Subnet = 42\n+        [http.middlewares.Middleware18.rateLimit.redis]\n+          endpoints = [\"foobar\", \"foobar\"]\n+          username = \"foobar\"\n+          password = \"foobar\"\n+          db = 42\n+          poolSize = 42\n+          minIdleConns = 42\n+          maxActiveConns = 42\n+          readTimeout = \"42s\"\n+          writeTimeout = \"42s\"\n+          dialTimeout = \"42s\"\n+          [http.middlewares.Middleware18.rateLimit.redis.tls]\n+            ca = \"foobar\"\n+            cert = \"foobar\"\n+            key = \"foobar\"\n+            insecureSkipVerify = true\n     [http.middlewares.Middleware19]\n       [http.middlewares.Middleware19.redirectRegex]\n         regex = \"foobar\"\ndiff --git a/docs/content/reference/dynamic-configuration/file.yaml b/docs/content/reference/dynamic-configuration/file.yaml\nindex c7daacc11e..7c0766810b 100644\n--- a/docs/content/reference/dynamic-configuration/file.yaml\n+++ b/docs/content/reference/dynamic-configuration/file.yaml\n@@ -360,6 +360,24 @@ http:\n             ipv6Subnet: 42\n           requestHeaderName: foobar\n           requestHost: true\n+        redis:\n+          endpoints:\n+            - foobar\n+            - foobar\n+          tls:\n+            ca: foobar\n+            cert: foobar\n+            key: foobar\n+            insecureSkipVerify: true\n+          username: foobar\n+          password: foobar\n+          db: 42\n+          poolSize: 42\n+          minIdleConns: 42\n+          maxActiveConns: 42\n+          readTimeout: 42s\n+          writeTimeout: 42s\n+          dialTimeout: 42s\n     Middleware19:\n       redirectRegex:\n         regex: foobar\ndiff --git a/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml b/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\nindex 43b14722c1..0b5982e204 100644\n--- a/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n+++ b/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml\n@@ -1790,6 +1790,90 @@ spec:\n                       Period, in combination with Average, defines the actual maximum rate, such as:\n                       r = Average / Period. It defaults to a second.\n                     x-kubernetes-int-or-string: true\n+                  redis:\n+                    description: Redis hold the configs of Redis as bucket in rate\n+                      limiter.\n+                    properties:\n+                      db:\n+                        description: DB defines the Redis database that will be selected\n+                          after connecting to the server.\n+                        type: integer\n+                      dialTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          DialTimeout sets the timeout for establishing new connections.\n+                          Default value is 5 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                      endpoints:\n+                        description: |-\n+                          Endpoints contains either a single address or a seed list of host:port addresses.\n+                          Default value is [\"localhost:6379\"].\n+                        items:\n+                          type: string\n+                        type: array\n+                      maxActiveConns:\n+                        description: |-\n+                          MaxActiveConns defines the maximum number of connections allocated by the pool at a given time.\n+                          Default value is 0, meaning there is no limit.\n+                        type: integer\n+                      minIdleConns:\n+                        description: |-\n+                          MinIdleConns defines the minimum number of idle connections.\n+                          Default value is 0, and idle connections are not closed by default.\n+                        type: integer\n+                      poolSize:\n+                        description: |-\n+                          PoolSize defines the initial number of socket connections.\n+                          If the pool runs out of available connections, additional ones will be created beyond PoolSize.\n+                          This can be limited using MaxActiveConns.\n+                          // Default value is 0, meaning 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n+                        type: integer\n+                      readTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          ReadTimeout defines the timeout for socket read operations.\n+                          Default value is 3 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                      secret:\n+                        description: Secret defines the name of the referenced Kubernetes\n+                          Secret containing Redis credentials.\n+                        type: string\n+                      tls:\n+                        description: |-\n+                          TLS defines TLS-specific configurations, including the CA, certificate, and key,\n+                          which can be provided as a file path or file content.\n+                        properties:\n+                          caSecret:\n+                            description: |-\n+                              CASecret is the name of the referenced Kubernetes Secret containing the CA to validate the server certificate.\n+                              The CA certificate is extracted from key `tls.ca` or `ca.crt`.\n+                            type: string\n+                          certSecret:\n+                            description: |-\n+                              CertSecret is the name of the referenced Kubernetes Secret containing the client certificate.\n+                              The client certificate is extracted from the keys `tls.crt` and `tls.key`.\n+                            type: string\n+                          insecureSkipVerify:\n+                            description: InsecureSkipVerify defines whether the server\n+                              certificates should be validated.\n+                            type: boolean\n+                        type: object\n+                      writeTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          WriteTimeout defines the timeout for socket write operations.\n+                          Default value is 3 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                    type: object\n                   sourceCriterion:\n                     description: |-\n                       SourceCriterion defines what criterion is used to group requests as originating from a common source.\ndiff --git a/docs/content/reference/dynamic-configuration/kv-ref.md b/docs/content/reference/dynamic-configuration/kv-ref.md\nindex e04f5b84ea..c1a0809255 100644\n--- a/docs/content/reference/dynamic-configuration/kv-ref.md\n+++ b/docs/content/reference/dynamic-configuration/kv-ref.md\n@@ -153,6 +153,21 @@ THIS FILE MUST NOT BE EDITED BY HAND\n | `traefik/http/middlewares/Middleware18/rateLimit/average` | `42` |\n | `traefik/http/middlewares/Middleware18/rateLimit/burst` | `42` |\n | `traefik/http/middlewares/Middleware18/rateLimit/period` | `42s` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/db` | `42` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/dialTimeout` | `42s` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/endpoints/0` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/endpoints/1` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/maxActiveConns` | `42` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/minIdleConns` | `42` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/password` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/poolSize` | `42` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/readTimeout` | `42s` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/tls/ca` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/tls/cert` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/tls/insecureSkipVerify` | `true` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/tls/key` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/username` | `foobar` |\n+| `traefik/http/middlewares/Middleware18/rateLimit/redis/writeTimeout` | `42s` |\n | `traefik/http/middlewares/Middleware18/rateLimit/sourceCriterion/ipStrategy/depth` | `42` |\n | `traefik/http/middlewares/Middleware18/rateLimit/sourceCriterion/ipStrategy/excludedIPs/0` | `foobar` |\n | `traefik/http/middlewares/Middleware18/rateLimit/sourceCriterion/ipStrategy/excludedIPs/1` | `foobar` |\ndiff --git a/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml b/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\nindex c5fe517a97..cf0da07197 100644\n--- a/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\n+++ b/docs/content/reference/dynamic-configuration/traefik.io_middlewares.yaml\n@@ -1027,6 +1027,90 @@ spec:\n                       Period, in combination with Average, defines the actual maximum rate, such as:\n                       r = Average / Period. It defaults to a second.\n                     x-kubernetes-int-or-string: true\n+                  redis:\n+                    description: Redis hold the configs of Redis as bucket in rate\n+                      limiter.\n+                    properties:\n+                      db:\n+                        description: DB defines the Redis database that will be selected\n+                          after connecting to the server.\n+                        type: integer\n+                      dialTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          DialTimeout sets the timeout for establishing new connections.\n+                          Default value is 5 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                      endpoints:\n+                        description: |-\n+                          Endpoints contains either a single address or a seed list of host:port addresses.\n+                          Default value is [\"localhost:6379\"].\n+                        items:\n+                          type: string\n+                        type: array\n+                      maxActiveConns:\n+                        description: |-\n+                          MaxActiveConns defines the maximum number of connections allocated by the pool at a given time.\n+                          Default value is 0, meaning there is no limit.\n+                        type: integer\n+                      minIdleConns:\n+                        description: |-\n+                          MinIdleConns defines the minimum number of idle connections.\n+                          Default value is 0, and idle connections are not closed by default.\n+                        type: integer\n+                      poolSize:\n+                        description: |-\n+                          PoolSize defines the initial number of socket connections.\n+                          If the pool runs out of available connections, additional ones will be created beyond PoolSize.\n+                          This can be limited using MaxActiveConns.\n+                          // Default value is 0, meaning 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n+                        type: integer\n+                      readTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          ReadTimeout defines the timeout for socket read operations.\n+                          Default value is 3 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                      secret:\n+                        description: Secret defines the name of the referenced Kubernetes\n+                          Secret containing Redis credentials.\n+                        type: string\n+                      tls:\n+                        description: |-\n+                          TLS defines TLS-specific configurations, including the CA, certificate, and key,\n+                          which can be provided as a file path or file content.\n+                        properties:\n+                          caSecret:\n+                            description: |-\n+                              CASecret is the name of the referenced Kubernetes Secret containing the CA to validate the server certificate.\n+                              The CA certificate is extracted from key `tls.ca` or `ca.crt`.\n+                            type: string\n+                          certSecret:\n+                            description: |-\n+                              CertSecret is the name of the referenced Kubernetes Secret containing the client certificate.\n+                              The client certificate is extracted from the keys `tls.crt` and `tls.key`.\n+                            type: string\n+                          insecureSkipVerify:\n+                            description: InsecureSkipVerify defines whether the server\n+                              certificates should be validated.\n+                            type: boolean\n+                        type: object\n+                      writeTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          WriteTimeout defines the timeout for socket write operations.\n+                          Default value is 3 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                    type: object\n                   sourceCriterion:\n                     description: |-\n                       SourceCriterion defines what criterion is used to group requests as originating from a common source.\ndiff --git a/go.mod b/go.mod\nindex 6e9528a40e..7493f9f505 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -50,6 +50,7 @@ require (\n \tgithub.com/prometheus/client_golang v1.19.1\n \tgithub.com/prometheus/client_model v0.6.1\n \tgithub.com/quic-go/quic-go v0.48.2\n+\tgithub.com/redis/go-redis/v9 v9.7.1\n \tgithub.com/rs/zerolog v1.33.0\n \tgithub.com/sirupsen/logrus v1.9.3\n \tgithub.com/spiffe/go-spiffe/v2 v2.4.0\n@@ -70,6 +71,7 @@ require (\n \tgithub.com/valyala/fasthttp v1.58.0\n \tgithub.com/vulcand/oxy/v2 v2.0.0\n \tgithub.com/vulcand/predicate v1.2.0\n+\tgithub.com/yuin/gopher-lua v1.1.1\n \tgo.opentelemetry.io/collector/pdata v1.10.0\n \tgo.opentelemetry.io/contrib/bridges/otellogrus v0.7.0\n \tgo.opentelemetry.io/contrib/propagators/autoprop v0.53.0\n@@ -301,7 +303,6 @@ require (\n \tgithub.com/prometheus/common v0.55.0 // indirect\n \tgithub.com/prometheus/procfs v0.15.1 // indirect\n \tgithub.com/quic-go/qpack v0.5.1 // indirect\n-\tgithub.com/redis/go-redis/v9 v9.6.1 // indirect\n \tgithub.com/regfish/regfish-dnsapi-go v0.1.1 // indirect\n \tgithub.com/rs/cors v1.7.0 // indirect\n \tgithub.com/sacloud/api-client-go v0.2.10 // indirect\ndiff --git a/go.sum b/go.sum\nindex 6c0789d4cf..5ed12b4284 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -1030,8 +1030,8 @@ github.com/quic-go/quic-go v0.48.2 h1:wsKXZPeGWpMpCGSWqOcqpW2wZYic/8T3aqiOID0/KW\n github.com/quic-go/quic-go v0.48.2/go.mod h1:yBgs3rWBOADpga7F+jJsb6Ybg1LSYiQvwWlLX+/6HMs=\n github.com/rcrowley/go-metrics v0.0.0-20181016184325-3113b8401b8a/go.mod h1:bCqnVzQkZxMG4s8nGwiZ5l3QUCyqpo9Y+/ZMZ9VjZe4=\n github.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475/go.mod h1:bCqnVzQkZxMG4s8nGwiZ5l3QUCyqpo9Y+/ZMZ9VjZe4=\n-github.com/redis/go-redis/v9 v9.6.1 h1:HHDteefn6ZkTtY5fGUE8tj8uy85AHk6zP7CpzIAM0y4=\n-github.com/redis/go-redis/v9 v9.6.1/go.mod h1:0C0c6ycQsdpVNQpxb1njEQIqkx5UcsM8FJCQLgE9+RA=\n+github.com/redis/go-redis/v9 v9.7.1 h1:4LhKRCIduqXqtvCUlaq9c8bdHOkICjDMrr1+Zb3osAc=\n+github.com/redis/go-redis/v9 v9.7.1/go.mod h1:f6zhXITC7JUJIlPEiBOTXxJgPLdZcA93GewI7inzyWw=\n github.com/regfish/regfish-dnsapi-go v0.1.1 h1:TJFtbePHkd47q5GZwYl1h3DIYXmoxdLjW/SBsPtB5IE=\n github.com/regfish/regfish-dnsapi-go v0.1.1/go.mod h1:ubIgXSfqarSnl3XHSn8hIFwFF3h0yrq0ZiWD93Y2VjY=\n github.com/rogpeppe/fastuuid v0.0.0-20150106093220-6724a57986af/go.mod h1:XWv6SoW27p1b0cqNHllgS5HIMJraePCO15w5zCzIWYg=\n@@ -1256,6 +1256,8 @@ github.com/yuin/goldmark v1.1.32/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9de\n github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\n github.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\n github.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\n+github.com/yuin/gopher-lua v1.1.1 h1:kYKnWBjvbNP4XLT3+bPEwAXJx262OhaHDWDVOPjL46M=\n+github.com/yuin/gopher-lua v1.1.1/go.mod h1:GBR0iDaNXjAgGg9zfCvksxSRnQx76gclCIb7kdAd1Pw=\n github.com/yusufpapurcu/wmi v1.2.3 h1:E1ctvB7uKFMOJw3fdOW32DwGE9I7t++CRUEMKvFoFiw=\n github.com/yusufpapurcu/wmi v1.2.3/go.mod h1:SBZ9tNy3G9/m5Oi98Zks0QjeHVDvuK0qfxQmPyzfmi0=\n github.com/zeebo/errs v1.3.0 h1:hmiaKqgYZzcVgRL1Vkc1Mn2914BbzB0IBxs+ebeutGs=\ndiff --git a/integration/fixtures/k8s/01-traefik-crd.yml b/integration/fixtures/k8s/01-traefik-crd.yml\nindex 43b14722c1..0b5982e204 100644\n--- a/integration/fixtures/k8s/01-traefik-crd.yml\n+++ b/integration/fixtures/k8s/01-traefik-crd.yml\n@@ -1790,6 +1790,90 @@ spec:\n                       Period, in combination with Average, defines the actual maximum rate, such as:\n                       r = Average / Period. It defaults to a second.\n                     x-kubernetes-int-or-string: true\n+                  redis:\n+                    description: Redis hold the configs of Redis as bucket in rate\n+                      limiter.\n+                    properties:\n+                      db:\n+                        description: DB defines the Redis database that will be selected\n+                          after connecting to the server.\n+                        type: integer\n+                      dialTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          DialTimeout sets the timeout for establishing new connections.\n+                          Default value is 5 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                      endpoints:\n+                        description: |-\n+                          Endpoints contains either a single address or a seed list of host:port addresses.\n+                          Default value is [\"localhost:6379\"].\n+                        items:\n+                          type: string\n+                        type: array\n+                      maxActiveConns:\n+                        description: |-\n+                          MaxActiveConns defines the maximum number of connections allocated by the pool at a given time.\n+                          Default value is 0, meaning there is no limit.\n+                        type: integer\n+                      minIdleConns:\n+                        description: |-\n+                          MinIdleConns defines the minimum number of idle connections.\n+                          Default value is 0, and idle connections are not closed by default.\n+                        type: integer\n+                      poolSize:\n+                        description: |-\n+                          PoolSize defines the initial number of socket connections.\n+                          If the pool runs out of available connections, additional ones will be created beyond PoolSize.\n+                          This can be limited using MaxActiveConns.\n+                          // Default value is 0, meaning 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n+                        type: integer\n+                      readTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          ReadTimeout defines the timeout for socket read operations.\n+                          Default value is 3 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                      secret:\n+                        description: Secret defines the name of the referenced Kubernetes\n+                          Secret containing Redis credentials.\n+                        type: string\n+                      tls:\n+                        description: |-\n+                          TLS defines TLS-specific configurations, including the CA, certificate, and key,\n+                          which can be provided as a file path or file content.\n+                        properties:\n+                          caSecret:\n+                            description: |-\n+                              CASecret is the name of the referenced Kubernetes Secret containing the CA to validate the server certificate.\n+                              The CA certificate is extracted from key `tls.ca` or `ca.crt`.\n+                            type: string\n+                          certSecret:\n+                            description: |-\n+                              CertSecret is the name of the referenced Kubernetes Secret containing the client certificate.\n+                              The client certificate is extracted from the keys `tls.crt` and `tls.key`.\n+                            type: string\n+                          insecureSkipVerify:\n+                            description: InsecureSkipVerify defines whether the server\n+                              certificates should be validated.\n+                            type: boolean\n+                        type: object\n+                      writeTimeout:\n+                        anyOf:\n+                        - type: integer\n+                        - type: string\n+                        description: |-\n+                          WriteTimeout defines the timeout for socket write operations.\n+                          Default value is 3 seconds.\n+                        pattern: ^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\n+                        x-kubernetes-int-or-string: true\n+                    type: object\n                   sourceCriterion:\n                     description: |-\n                       SourceCriterion defines what criterion is used to group requests as originating from a common source.\ndiff --git a/integration/fixtures/ratelimit/simple_redis.toml b/integration/fixtures/ratelimit/simple_redis.toml\nnew file mode 100644\nindex 0000000000..cf3f08b651\n--- /dev/null\n+++ b/integration/fixtures/ratelimit/simple_redis.toml\n@@ -0,0 +1,39 @@\n+[global]\n+  checkNewVersion = false\n+  sendAnonymousUsage = false\n+\n+[api]\n+  insecure = true\n+\n+[log]\n+  level = \"DEBUG\"\n+  noColor = true\n+\n+[entryPoints]\n+  [entryPoints.web]\n+    address = \":8081\"\n+\n+[providers.file]\n+  filename = \"{{ .SelfFilename }}\"\n+\n+## dynamic configuration ##\n+\n+[http.routers]\n+  [http.routers.router1]\n+    service = \"service1\"\n+    middlewares = [ \"ratelimit\" ]\n+    rule = \"Path(`/`)\"\n+\n+[http.middlewares]\n+  [http.middlewares.ratelimit.rateLimit]\n+    average = 100\n+    burst = 1\n+    [http.middlewares.ratelimit.rateLimit.redis]\n+      endpoints = [\"{{ .RedisEndpoint }}\"]\n+\n+[http.services]\n+  [http.services.service1]\n+    [http.services.service1.loadBalancer]\n+      passHostHeader = true\n+      [[http.services.service1.loadBalancer.servers]]\n+        url = \"http://{{.Server1}}:80\"\ndiff --git a/integration/resources/compose/ratelimit.yml b/integration/resources/compose/ratelimit.yml\nindex 2515681655..5d9d6ec1e2 100644\n--- a/integration/resources/compose/ratelimit.yml\n+++ b/integration/resources/compose/ratelimit.yml\n@@ -2,3 +2,10 @@ version: \"3.8\"\n services:\n   whoami1:\n     image: traefik/whoami\n+\n+  redis:\n+    image: redis:5.0\n+    command:\n+      - redis-server\n+      - --port\n+      - 6379\ndiff --git a/pkg/config/dynamic/middlewares.go b/pkg/config/dynamic/middlewares.go\nindex e1fc3a73b2..501b75c01b 100644\n--- a/pkg/config/dynamic/middlewares.go\n+++ b/pkg/config/dynamic/middlewares.go\n@@ -7,6 +7,7 @@ import (\n \n \tptypes \"github.com/traefik/paerser/types\"\n \t\"github.com/traefik/traefik/v3/pkg/ip\"\n+\t\"github.com/traefik/traefik/v3/pkg/types\"\n )\n \n // ForwardAuthDefaultMaxBodySize is the ForwardAuth.MaxBodySize option default value.\n@@ -566,6 +567,10 @@ type RateLimit struct {\n \t// If several strategies are defined at the same time, an error will be raised.\n \t// If none are set, the default is to use the request's remote address field (as an ipStrategy).\n \tSourceCriterion *SourceCriterion `json:\"sourceCriterion,omitempty\" toml:\"sourceCriterion,omitempty\" yaml:\"sourceCriterion,omitempty\" export:\"true\"`\n+\n+\t// Redis stores the configuration for using Redis as a bucket in the rate-limiting algorithm.\n+\t// If not specified, Traefik will default to an in-memory bucket for the algorithm.\n+\tRedis *Redis `json:\"redis,omitempty\" toml:\"redis,omitempty\" yaml:\"redis,omitempty\" export:\"true\"`\n }\n \n // SetDefaults sets the default values on a RateLimit.\n@@ -576,6 +581,58 @@ func (r *RateLimit) SetDefaults() {\n \n // +k8s:deepcopy-gen=true\n \n+// Redis holds the Redis configuration.\n+type Redis struct {\n+\t// Endpoints contains either a single address or a seed list of host:port addresses.\n+\t// Default value is [\"localhost:6379\"].\n+\tEndpoints []string `json:\"endpoints,omitempty\" toml:\"endpoints,omitempty\" yaml:\"endpoints,omitempty\"`\n+\t// TLS defines TLS-specific configurations, including the CA, certificate, and key,\n+\t// which can be provided as a file path or file content.\n+\tTLS *types.ClientTLS `json:\"tls,omitempty\" toml:\"tls,omitempty\" yaml:\"tls,omitempty\" export:\"true\"`\n+\t// Username defines the username to connect to the Redis server.\n+\tUsername string `json:\"username,omitempty\" toml:\"username,omitempty\" yaml:\"username,omitempty\" loggable:\"false\"`\n+\t// Password defines the password to connect to the Redis server.\n+\tPassword string `json:\"password,omitempty\" toml:\"password,omitempty\" yaml:\"password,omitempty\" loggable:\"false\"`\n+\t// DB defines the Redis database that will be selected after connecting to the server.\n+\tDB int `json:\"db,omitempty\" toml:\"db,omitempty\" yaml:\"db,omitempty\"`\n+\t// PoolSize defines the initial number of socket connections.\n+\t// If the pool runs out of available connections, additional ones will be created beyond PoolSize.\n+\t// This can be limited using MaxActiveConns.\n+\t// Default value is 0, meaning 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n+\tPoolSize int `json:\"poolSize,omitempty\" toml:\"poolSize,omitempty\" yaml:\"poolSize,omitempty\" export:\"true\"`\n+\t// MinIdleConns defines the minimum number of idle connections.\n+\t// Default value is 0, and idle connections are not closed by default.\n+\tMinIdleConns int `json:\"minIdleConns,omitempty\" toml:\"minIdleConns,omitempty\" yaml:\"minIdleConns,omitempty\" export:\"true\"`\n+\t// MaxActiveConns defines the maximum number of connections allocated by the pool at a given time.\n+\t// Default value is 0, meaning there is no limit.\n+\tMaxActiveConns int `json:\"maxActiveConns,omitempty\" toml:\"maxActiveConns,omitempty\" yaml:\"maxActiveConns,omitempty\" export:\"true\"`\n+\t// ReadTimeout defines the timeout for socket read operations.\n+\t// Default value is 3 seconds.\n+\tReadTimeout *ptypes.Duration `json:\"readTimeout,omitempty\" toml:\"readTimeout,omitempty\" yaml:\"readTimeout,omitempty\" export:\"true\"`\n+\t// WriteTimeout defines the timeout for socket write operations.\n+\t// Default value is 3 seconds.\n+\tWriteTimeout *ptypes.Duration `json:\"writeTimeout,omitempty\" toml:\"writeTimeout,omitempty\" yaml:\"writeTimeout,omitempty\" export:\"true\"`\n+\t// DialTimeout sets the timeout for establishing new connections.\n+\t// Default value is 5 seconds.\n+\tDialTimeout *ptypes.Duration `json:\"dialTimeout,omitempty\" toml:\"dialTimeout,omitempty\" yaml:\"dialTimeout,omitempty\" export:\"true\"`\n+}\n+\n+// SetDefaults sets the default values on a RateLimit.\n+func (r *Redis) SetDefaults() {\n+\tr.Endpoints = []string{\"localhost:6379\"}\n+\n+\tdefaultReadTimeout := ptypes.Duration(3 * time.Second)\n+\tr.ReadTimeout = &defaultReadTimeout\n+\n+\tdefaultWriteTimeout := ptypes.Duration(3 * time.Second)\n+\tr.WriteTimeout = &defaultWriteTimeout\n+\n+\tdefaultDialTimeout := ptypes.Duration(5 * time.Second)\n+\tr.DialTimeout = &defaultDialTimeout\n+}\n+\n+// +k8s:deepcopy-gen=true\n+\n // RedirectRegex holds the redirect regex middleware configuration.\n // This middleware redirects a request using regex matching and replacement.\n // More info: https://doc.traefik.io/traefik/v3.3/middlewares/http/redirectregex/#regex\ndiff --git a/pkg/config/dynamic/zz_generated.deepcopy.go b/pkg/config/dynamic/zz_generated.deepcopy.go\nindex 2f0e8dd4d0..2a680897c9 100644\n--- a/pkg/config/dynamic/zz_generated.deepcopy.go\n+++ b/pkg/config/dynamic/zz_generated.deepcopy.go\n@@ -30,6 +30,7 @@ THE SOFTWARE.\n package dynamic\n \n import (\n+\tpaersertypes \"github.com/traefik/paerser/types\"\n \ttls \"github.com/traefik/traefik/v3/pkg/tls\"\n \ttypes \"github.com/traefik/traefik/v3/pkg/types\"\n )\n@@ -1094,6 +1095,11 @@ func (in *RateLimit) DeepCopyInto(out *RateLimit) {\n \t\t*out = new(SourceCriterion)\n \t\t(*in).DeepCopyInto(*out)\n \t}\n+\tif in.Redis != nil {\n+\t\tin, out := &in.Redis, &out.Redis\n+\t\t*out = new(Redis)\n+\t\t(*in).DeepCopyInto(*out)\n+\t}\n \treturn\n }\n \n@@ -1139,6 +1145,47 @@ func (in *RedirectScheme) DeepCopy() *RedirectScheme {\n \treturn out\n }\n \n+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n+func (in *Redis) DeepCopyInto(out *Redis) {\n+\t*out = *in\n+\tif in.Endpoints != nil {\n+\t\tin, out := &in.Endpoints, &out.Endpoints\n+\t\t*out = make([]string, len(*in))\n+\t\tcopy(*out, *in)\n+\t}\n+\tif in.TLS != nil {\n+\t\tin, out := &in.TLS, &out.TLS\n+\t\t*out = new(types.ClientTLS)\n+\t\t**out = **in\n+\t}\n+\tif in.ReadTimeout != nil {\n+\t\tin, out := &in.ReadTimeout, &out.ReadTimeout\n+\t\t*out = new(paersertypes.Duration)\n+\t\t**out = **in\n+\t}\n+\tif in.WriteTimeout != nil {\n+\t\tin, out := &in.WriteTimeout, &out.WriteTimeout\n+\t\t*out = new(paersertypes.Duration)\n+\t\t**out = **in\n+\t}\n+\tif in.DialTimeout != nil {\n+\t\tin, out := &in.DialTimeout, &out.DialTimeout\n+\t\t*out = new(paersertypes.Duration)\n+\t\t**out = **in\n+\t}\n+\treturn\n+}\n+\n+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Redis.\n+func (in *Redis) DeepCopy() *Redis {\n+\tif in == nil {\n+\t\treturn nil\n+\t}\n+\tout := new(Redis)\n+\tin.DeepCopyInto(out)\n+\treturn out\n+}\n+\n // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n func (in *ReplacePath) DeepCopyInto(out *ReplacePath) {\n \t*out = *in\ndiff --git a/pkg/middlewares/ratelimiter/in_memory_limiter.go b/pkg/middlewares/ratelimiter/in_memory_limiter.go\nnew file mode 100644\nindex 0000000000..bec67aa077\n--- /dev/null\n+++ b/pkg/middlewares/ratelimiter/in_memory_limiter.go\n@@ -0,0 +1,72 @@\n+package ratelimiter\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\t\"github.com/mailgun/ttlmap\"\n+\t\"github.com/rs/zerolog\"\n+\t\"golang.org/x/time/rate\"\n+)\n+\n+type inMemoryRateLimiter struct {\n+\trate  rate.Limit // reqs/s\n+\tburst int64\n+\t// maxDelay is the maximum duration we're willing to wait for a bucket reservation to become effective, in nanoseconds.\n+\t// For now it is somewhat arbitrarily set to 1/(2*rate).\n+\tmaxDelay time.Duration\n+\t// Each rate limiter for a given source is stored in the buckets ttlmap.\n+\t// To keep this ttlmap constrained in size,\n+\t// each ratelimiter is \"garbage collected\" when it is considered expired.\n+\t// It is considered expired after it hasn't been used for ttl seconds.\n+\tttl     int\n+\tbuckets *ttlmap.TtlMap // actual buckets, keyed by source.\n+\n+\tlogger *zerolog.Logger\n+}\n+\n+func newInMemoryRateLimiter(rate rate.Limit, burst int64, maxDelay time.Duration, ttl int, logger *zerolog.Logger) (*inMemoryRateLimiter, error) {\n+\tbuckets, err := ttlmap.NewConcurrent(maxSources)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"creating ttlmap: %w\", err)\n+\t}\n+\n+\treturn &inMemoryRateLimiter{\n+\t\trate:     rate,\n+\t\tburst:    burst,\n+\t\tmaxDelay: maxDelay,\n+\t\tttl:      ttl,\n+\t\tlogger:   logger,\n+\t\tbuckets:  buckets,\n+\t}, nil\n+}\n+\n+func (i *inMemoryRateLimiter) Allow(_ context.Context, source string) (*time.Duration, error) {\n+\t// Get bucket which contains limiter information.\n+\tvar bucket *rate.Limiter\n+\tif rlSource, exists := i.buckets.Get(source); exists {\n+\t\tbucket = rlSource.(*rate.Limiter)\n+\t} else {\n+\t\tbucket = rate.NewLimiter(i.rate, int(i.burst))\n+\t}\n+\n+\t// We Set even in the case where the source already exists,\n+\t// because we want to update the expiryTime everytime we get the source,\n+\t// as the expiryTime is supposed to reflect the activity (or lack thereof) on that source.\n+\tif err := i.buckets.Set(source, bucket, i.ttl); err != nil {\n+\t\treturn nil, fmt.Errorf(\"setting buckets: %w\", err)\n+\t}\n+\n+\tres := bucket.Reserve()\n+\tif !res.OK() {\n+\t\treturn nil, nil\n+\t}\n+\n+\tdelay := res.Delay()\n+\tif delay > i.maxDelay {\n+\t\tres.Cancel()\n+\t}\n+\n+\treturn &delay, nil\n+}\ndiff --git a/pkg/middlewares/ratelimiter/lua.go b/pkg/middlewares/ratelimiter/lua.go\nnew file mode 100644\nindex 0000000000..47100629ec\n--- /dev/null\n+++ b/pkg/middlewares/ratelimiter/lua.go\n@@ -0,0 +1,66 @@\n+package ratelimiter\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/redis/go-redis/v9\"\n+)\n+\n+type Rediser interface {\n+\tEval(ctx context.Context, script string, keys []string, args ...interface{}) *redis.Cmd\n+\tEvalSha(ctx context.Context, sha1 string, keys []string, args ...interface{}) *redis.Cmd\n+\tScriptExists(ctx context.Context, hashes ...string) *redis.BoolSliceCmd\n+\tScriptLoad(ctx context.Context, script string) *redis.StringCmd\n+\tDel(ctx context.Context, keys ...string) *redis.IntCmd\n+\n+\tEvalRO(ctx context.Context, script string, keys []string, args ...interface{}) *redis.Cmd\n+\tEvalShaRO(ctx context.Context, sha1 string, keys []string, args ...interface{}) *redis.Cmd\n+}\n+\n+//nolint:dupword\n+var AllowTokenBucketRaw = `\n+local key = KEYS[1]\n+local limit, burst, ttl, t, max_delay = tonumber(ARGV[1]), tonumber(ARGV[2]), tonumber(ARGV[3]), tonumber(ARGV[4]),\n+    tonumber(ARGV[5])\n+\n+local bucket = {\n+    limit = limit,\n+    burst = burst,\n+    tokens = 0,\n+    last = 0\n+}\n+\n+local rl_source = redis.call('hgetall', key)\n+\n+if table.maxn(rl_source) == 4 then\n+    -- Get bucket state from redis\n+    bucket.last = tonumber(rl_source[2])\n+    bucket.tokens = tonumber(rl_source[4])\n+end\n+\n+local last = bucket.last\n+if t < last then\n+    last = t\n+end\n+\n+local elapsed = t - last\n+local delta = bucket.limit * elapsed\n+local tokens = bucket.tokens + delta\n+tokens = math.min(tokens, bucket.burst)\n+tokens = tokens - 1\n+\n+local wait_duration = 0\n+if tokens < 0 then\n+    wait_duration = (tokens * -1) / bucket.limit\n+    if wait_duration > max_delay then\n+        tokens = tokens + 1\n+        tokens = math.min(tokens, burst)\n+    end\n+end\n+\n+redis.call('hset', key, 'last', t, 'tokens', tokens)\n+redis.call('expire', key, ttl)\n+\n+return {tostring(true), tostring(wait_duration),tostring(tokens)}`\n+\n+var AllowTokenBucketScript = redis.NewScript(AllowTokenBucketRaw)\ndiff --git a/pkg/middlewares/ratelimiter/rate_limiter.go b/pkg/middlewares/ratelimiter/rate_limiter.go\nold mode 100644\nnew mode 100755\nindex d22b99a422..5798fd87a4\n--- a/pkg/middlewares/ratelimiter/rate_limiter.go\n+++ b/pkg/middlewares/ratelimiter/rate_limiter.go\n@@ -8,7 +8,7 @@ import (\n \t\"net/http\"\n \t\"time\"\n \n-\t\"github.com/mailgun/ttlmap\"\n+\t\"github.com/rs/zerolog\"\n \t\"github.com/rs/zerolog/log\"\n \t\"github.com/traefik/traefik/v3/pkg/config/dynamic\"\n \t\"github.com/traefik/traefik/v3/pkg/middlewares\"\n@@ -23,24 +23,23 @@ const (\n \tmaxSources = 65536\n )\n \n+type limiter interface {\n+\tAllow(ctx context.Context, token string) (*time.Duration, error)\n+}\n+\n // rateLimiter implements rate limiting and traffic shaping with a set of token buckets;\n // one for each traffic source. The same parameters are applied to all the buckets.\n type rateLimiter struct {\n-\tname  string\n-\trate  rate.Limit // reqs/s\n-\tburst int64\n+\tname string\n+\trate rate.Limit // reqs/s\n \t// maxDelay is the maximum duration we're willing to wait for a bucket reservation to become effective, in nanoseconds.\n \t// For now it is somewhat arbitrarily set to 1/(2*rate).\n-\tmaxDelay time.Duration\n-\t// each rate limiter for a given source is stored in the buckets ttlmap.\n-\t// To keep this ttlmap constrained in size,\n-\t// each ratelimiter is \"garbage collected\" when it is considered expired.\n-\t// It is considered expired after it hasn't been used for ttl seconds.\n-\tttl           int\n+\tmaxDelay      time.Duration\n \tsourceMatcher utils.SourceExtractor\n \tnext          http.Handler\n+\tlogger        *zerolog.Logger\n \n-\tbuckets *ttlmap.TtlMap // actual buckets, keyed by source.\n+\tlimiter limiter\n }\n \n // New returns a rate limiter middleware.\n@@ -60,12 +59,7 @@ func New(ctx context.Context, next http.Handler, config dynamic.RateLimit, name\n \n \tsourceMatcher, err := middlewares.GetSourceExtractor(ctxLog, config.SourceCriterion)\n \tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tbuckets, err := ttlmap.NewConcurrent(maxSources)\n-\tif err != nil {\n-\t\treturn nil, err\n+\t\treturn nil, fmt.Errorf(\"getting source extractor: %w\", err)\n \t}\n \n \tburst := config.Burst\n@@ -109,16 +103,27 @@ func New(ctx context.Context, next http.Handler, config dynamic.RateLimit, name\n \t} else if rtl > 0 {\n \t\tttl += int(1 / rtl)\n \t}\n+\tvar limiter limiter\n+\tif config.Redis != nil {\n+\t\tlimiter, err = newRedisLimiter(ctx, rate.Limit(rtl), burst, maxDelay, ttl, config, logger)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"creating redis limiter: %w\", err)\n+\t\t}\n+\t} else {\n+\t\tlimiter, err = newInMemoryRateLimiter(rate.Limit(rtl), burst, maxDelay, ttl, logger)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"creating in-memory limiter: %w\", err)\n+\t\t}\n+\t}\n \n \treturn &rateLimiter{\n+\t\tlogger:        logger,\n \t\tname:          name,\n \t\trate:          rate.Limit(rtl),\n-\t\tburst:         burst,\n \t\tmaxDelay:      maxDelay,\n \t\tnext:          next,\n \t\tsourceMatcher: sourceMatcher,\n-\t\tbuckets:       buckets,\n-\t\tttl:           ttl,\n+\t\tlimiter:       limiter,\n \t}, nil\n }\n \n@@ -141,38 +146,34 @@ func (rl *rateLimiter) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n \t\tlogger.Info().Msgf(\"ignoring token bucket amount > 1: %d\", amount)\n \t}\n \n-\tvar bucket *rate.Limiter\n-\tif rlSource, exists := rl.buckets.Get(source); exists {\n-\t\tbucket = rlSource.(*rate.Limiter)\n-\t} else {\n-\t\tbucket = rate.NewLimiter(rl.rate, int(rl.burst))\n+\tdelay, err := rl.limiter.Allow(ctx, source)\n+\tif err != nil {\n+\t\trl.logger.Error().Err(err).Msg(\"Could not insert/update bucket\")\n+\t\tobservability.SetStatusErrorf(ctx, \"Could not insert/update bucket\")\n+\t\thttp.Error(rw, \"Could not insert/update bucket\", http.StatusInternalServerError)\n+\t\treturn\n \t}\n \n-\t// We Set even in the case where the source already exists,\n-\t// because we want to update the expiryTime every time we get the source,\n-\t// as the expiryTime is supposed to reflect the activity (or lack thereof) on that source.\n-\tif err := rl.buckets.Set(source, bucket, rl.ttl); err != nil {\n-\t\tlogger.Error().Err(err).Msg(\"Could not insert/update bucket\")\n-\t\tobservability.SetStatusErrorf(req.Context(), \"Could not insert/update bucket\")\n-\t\thttp.Error(rw, \"could not insert/update bucket\", http.StatusInternalServerError)\n+\tif delay == nil {\n+\t\tobservability.SetStatusErrorf(ctx, \"No bursty traffic allowed\")\n+\t\thttp.Error(rw, \"No bursty traffic allowed\", http.StatusTooManyRequests)\n \t\treturn\n \t}\n \n-\tres := bucket.Reserve()\n-\tif !res.OK() {\n-\t\tobservability.SetStatusErrorf(req.Context(), \"No bursty traffic allowed\")\n-\t\thttp.Error(rw, \"No bursty traffic allowed\", http.StatusTooManyRequests)\n+\tif *delay > rl.maxDelay {\n+\t\trl.serveDelayError(ctx, rw, *delay)\n \t\treturn\n \t}\n \n-\tdelay := res.Delay()\n-\tif delay > rl.maxDelay {\n-\t\tres.Cancel()\n-\t\trl.serveDelayError(ctx, rw, delay)\n+\tselect {\n+\tcase <-ctx.Done():\n+\t\tobservability.SetStatusErrorf(ctx, \"Context canceled\")\n+\t\thttp.Error(rw, \"context canceled\", http.StatusInternalServerError)\n \t\treturn\n+\n+\tcase <-time.After(*delay):\n \t}\n \n-\ttime.Sleep(delay)\n \trl.next.ServeHTTP(rw, req)\n }\n \ndiff --git a/pkg/middlewares/ratelimiter/redis_limiter.go b/pkg/middlewares/ratelimiter/redis_limiter.go\nnew file mode 100644\nindex 0000000000..6bdf6c8fca\n--- /dev/null\n+++ b/pkg/middlewares/ratelimiter/redis_limiter.go\n@@ -0,0 +1,118 @@\n+package ratelimiter\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"strconv\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9\"\n+\t\"github.com/rs/zerolog\"\n+\tptypes \"github.com/traefik/paerser/types\"\n+\t\"github.com/traefik/traefik/v3/pkg/config/dynamic\"\n+\t\"golang.org/x/time/rate\"\n+)\n+\n+const redisPrefix = \"rate:\"\n+\n+type redisLimiter struct {\n+\trate     rate.Limit // reqs/s\n+\tburst    int64\n+\tmaxDelay time.Duration\n+\tperiod   ptypes.Duration\n+\tlogger   *zerolog.Logger\n+\tttl      int\n+\tclient   Rediser\n+}\n+\n+func newRedisLimiter(ctx context.Context, rate rate.Limit, burst int64, maxDelay time.Duration, ttl int, config dynamic.RateLimit, logger *zerolog.Logger) (limiter, error) {\n+\toptions := &redis.UniversalOptions{\n+\t\tAddrs:          config.Redis.Endpoints,\n+\t\tUsername:       config.Redis.Username,\n+\t\tPassword:       config.Redis.Password,\n+\t\tDB:             config.Redis.DB,\n+\t\tPoolSize:       config.Redis.PoolSize,\n+\t\tMinIdleConns:   config.Redis.MinIdleConns,\n+\t\tMaxActiveConns: config.Redis.MaxActiveConns,\n+\t}\n+\n+\tif config.Redis.DialTimeout != nil && *config.Redis.DialTimeout > 0 {\n+\t\toptions.DialTimeout = time.Duration(*config.Redis.DialTimeout)\n+\t}\n+\n+\tif config.Redis.ReadTimeout != nil {\n+\t\tif *config.Redis.ReadTimeout > 0 {\n+\t\t\toptions.ReadTimeout = time.Duration(*config.Redis.ReadTimeout)\n+\t\t} else {\n+\t\t\toptions.ReadTimeout = -1\n+\t\t}\n+\t}\n+\n+\tif config.Redis.WriteTimeout != nil {\n+\t\tif *config.Redis.ReadTimeout > 0 {\n+\t\t\toptions.WriteTimeout = time.Duration(*config.Redis.WriteTimeout)\n+\t\t} else {\n+\t\t\toptions.WriteTimeout = -1\n+\t\t}\n+\t}\n+\n+\tif config.Redis.TLS != nil {\n+\t\tvar err error\n+\t\toptions.TLSConfig, err = config.Redis.TLS.CreateTLSConfig(ctx)\n+\t\tif err != nil {\n+\t\t\treturn nil, fmt.Errorf(\"creating TLS config: %w\", err)\n+\t\t}\n+\t}\n+\n+\treturn &redisLimiter{\n+\t\trate:     rate,\n+\t\tburst:    burst,\n+\t\tperiod:   config.Period,\n+\t\tmaxDelay: maxDelay,\n+\t\tlogger:   logger,\n+\t\tttl:      ttl,\n+\t\tclient:   redis.NewUniversalClient(options),\n+\t}, nil\n+}\n+\n+func (r *redisLimiter) Allow(ctx context.Context, source string) (*time.Duration, error) {\n+\tok, delay, err := r.evaluateScript(ctx, source)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"evaluating script: %w\", err)\n+\t}\n+\tif !ok {\n+\t\treturn nil, nil\n+\t}\n+\treturn delay, nil\n+}\n+\n+func (r *redisLimiter) evaluateScript(ctx context.Context, key string) (bool, *time.Duration, error) {\n+\tif r.rate == rate.Inf {\n+\t\treturn true, nil, nil\n+\t}\n+\n+\tparams := []interface{}{\n+\t\tfloat64(r.rate / 1000000),\n+\t\tr.burst,\n+\t\tr.ttl,\n+\t\ttime.Now().UnixMicro(),\n+\t\tr.maxDelay.Microseconds(),\n+\t}\n+\tv, err := AllowTokenBucketScript.Run(ctx, r.client, []string{redisPrefix + key}, params...).Result()\n+\tif err != nil {\n+\t\treturn false, nil, fmt.Errorf(\"running script: %w\", err)\n+\t}\n+\n+\tvalues := v.([]interface{})\n+\tok, err := strconv.ParseBool(values[0].(string))\n+\tif err != nil {\n+\t\treturn false, nil, fmt.Errorf(\"parsing ok value from redis rate lua script: %w\", err)\n+\t}\n+\tdelay, err := strconv.ParseFloat(values[1].(string), 64)\n+\tif err != nil {\n+\t\treturn false, nil, fmt.Errorf(\"parsing delay value from redis rate lua script: %w\", err)\n+\t}\n+\n+\tmicroDelay := time.Duration(delay * float64(time.Microsecond))\n+\treturn ok, &microDelay, nil\n+}\ndiff --git a/pkg/provider/kubernetes/crd/fixtures/with_auth.yml b/pkg/provider/kubernetes/crd/fixtures/with_auth.yml\nindex 16d167e6ce..4a6c16e138 100644\n--- a/pkg/provider/kubernetes/crd/fixtures/with_auth.yml\n+++ b/pkg/provider/kubernetes/crd/fixtures/with_auth.yml\n@@ -8,6 +8,7 @@ data:\n   users: |2\n     dGVzdDokYXByMSRINnVza2trVyRJZ1hMUDZld1RyU3VCa1RycUU4d2ovCnRlc3QyOiRhcHIxJGQ5\n     aHI5SEJCJDRIeHdnVWlyM0hQNEVzZ2dQL1FObzAK\n+\n ---\n apiVersion: v1\n kind: Secret\ndiff --git a/pkg/provider/kubernetes/crd/fixtures/with_ratelimit.yml b/pkg/provider/kubernetes/crd/fixtures/with_ratelimit.yml\nnew file mode 100644\nindex 0000000000..0ada06322f\n--- /dev/null\n+++ b/pkg/provider/kubernetes/crd/fixtures/with_ratelimit.yml\n@@ -0,0 +1,82 @@\n+apiVersion: traefik.io/v1alpha1\n+kind: Middleware\n+metadata:\n+  name: ratelimit\n+  namespace: default\n+\n+spec:\n+  rateLimit:\n+    period: 1m\n+    average: 6\n+    burst: 12\n+    sourceCriterion:\n+      ipStrategy:\n+        excludedIPs:\n+          - 127.0.0.1/32\n+          - 192.168.1.7\n+\n+    redis:\n+      secret: redissecret\n+      endpoints:\n+        - \"127.0.0.1:6379\"\n+      tls:\n+        certSecret: tlssecret\n+        caSecret: casecret\n+      db: 0\n+      poolSize: 42\n+      maxActiveConns: 42\n+      readTimeout: 42s\n+      writeTimeout: 42s\n+      dialTimeout: 42s\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: redissecret\n+  namespace: default\n+data:\n+  username: dXNlcg== # username: user\n+  password: cGFzc3dvcmQ= # password: password\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: casecret\n+  namespace: default\n+\n+data:\n+  ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=\n+\n+---\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: tlssecret\n+  namespace: default\n+\n+data:\n+  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=\n+  tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCi0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0=\n+\n+---\n+apiVersion: traefik.io/v1alpha1\n+kind: IngressRoute\n+metadata:\n+  name: test2.route\n+  namespace: default\n+\n+spec:\n+  entryPoints:\n+    - web\n+\n+  routes:\n+    - match: Host(`foo.com`) && PathPrefix(`/will-be-limited`)\n+      priority: 12\n+      kind: Rule\n+      services:\n+        - name: whoami\n+          port: 80\n+      middlewares:\n+        - name: ratelimit\ndiff --git a/pkg/provider/kubernetes/crd/kubernetes.go b/pkg/provider/kubernetes/crd/kubernetes.go\nindex 5e435346bb..f9a8809c13 100644\n--- a/pkg/provider/kubernetes/crd/kubernetes.go\n+++ b/pkg/provider/kubernetes/crd/kubernetes.go\n@@ -266,7 +266,7 @@ func (p *Provider) loadConfigurationFromCRD(ctx context.Context, client Client)\n \t\t\tcontinue\n \t\t}\n \n-\t\trateLimit, err := createRateLimitMiddleware(middleware.Spec.RateLimit)\n+\t\trateLimit, err := createRateLimitMiddleware(client, middleware.Namespace, middleware.Spec.RateLimit)\n \t\tif err != nil {\n \t\t\tlogger.Error().Err(err).Msg(\"Error while reading rateLimit middleware\")\n \t\t\tcontinue\n@@ -686,7 +686,7 @@ func createCompressMiddleware(compress *traefikv1alpha1.Compress) *dynamic.Compr\n \treturn c\n }\n \n-func createRateLimitMiddleware(rateLimit *traefikv1alpha1.RateLimit) (*dynamic.RateLimit, error) {\n+func createRateLimitMiddleware(client Client, namespace string, rateLimit *traefikv1alpha1.RateLimit) (*dynamic.RateLimit, error) {\n \tif rateLimit == nil {\n \t\treturn nil, nil\n \t}\n@@ -713,9 +713,97 @@ func createRateLimitMiddleware(rateLimit *traefikv1alpha1.RateLimit) (*dynamic.R\n \t\trl.SourceCriterion = rateLimit.SourceCriterion\n \t}\n \n+\tif rateLimit.Redis != nil {\n+\t\trl.Redis = &dynamic.Redis{\n+\t\t\tDB:             rateLimit.Redis.DB,\n+\t\t\tPoolSize:       rateLimit.Redis.PoolSize,\n+\t\t\tMinIdleConns:   rateLimit.Redis.MinIdleConns,\n+\t\t\tMaxActiveConns: rateLimit.Redis.MaxActiveConns,\n+\t\t}\n+\t\trl.Redis.SetDefaults()\n+\n+\t\tif len(rateLimit.Redis.Endpoints) > 0 {\n+\t\t\trl.Redis.Endpoints = rateLimit.Redis.Endpoints\n+\t\t}\n+\n+\t\tif rateLimit.Redis.TLS != nil {\n+\t\t\trl.Redis.TLS = &types.ClientTLS{\n+\t\t\t\tInsecureSkipVerify: rateLimit.Redis.TLS.InsecureSkipVerify,\n+\t\t\t}\n+\n+\t\t\tif len(rateLimit.Redis.TLS.CASecret) > 0 {\n+\t\t\t\tcaSecret, err := loadCASecret(namespace, rateLimit.Redis.TLS.CASecret, client)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load auth ca secret: %w\", err)\n+\t\t\t\t}\n+\t\t\t\trl.Redis.TLS.CA = caSecret\n+\t\t\t}\n+\n+\t\t\tif len(rateLimit.Redis.TLS.CertSecret) > 0 {\n+\t\t\t\tauthSecretCert, authSecretKey, err := loadAuthTLSSecret(namespace, rateLimit.Redis.TLS.CertSecret, client)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load auth secret: %w\", err)\n+\t\t\t\t}\n+\t\t\t\trl.Redis.TLS.Cert = authSecretCert\n+\t\t\t\trl.Redis.TLS.Key = authSecretKey\n+\t\t\t}\n+\t\t}\n+\n+\t\tif rateLimit.Redis.DialTimeout != nil {\n+\t\t\terr := rl.Redis.DialTimeout.Set(rateLimit.Redis.DialTimeout.String())\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t}\n+\n+\t\tif rateLimit.Redis.ReadTimeout != nil {\n+\t\t\terr := rl.Redis.ReadTimeout.Set(rateLimit.Redis.ReadTimeout.String())\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t}\n+\n+\t\tif rateLimit.Redis.WriteTimeout != nil {\n+\t\t\terr := rl.Redis.WriteTimeout.Set(rateLimit.Redis.WriteTimeout.String())\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t}\n+\n+\t\tif rateLimit.Redis.Secret != \"\" {\n+\t\t\tvar err error\n+\t\t\trl.Redis.Username, rl.Redis.Password, err = loadRedisCredentials(namespace, rateLimit.Redis.Secret, client)\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t}\n+\t}\n+\n \treturn rl, nil\n }\n \n+func loadRedisCredentials(namespace, secretName string, k8sClient Client) (string, string, error) {\n+\tsecret, exists, err := k8sClient.GetSecret(namespace, secretName)\n+\tif err != nil {\n+\t\treturn \"\", \"\", fmt.Errorf(\"failed to fetch secret '%s/%s': %w\", namespace, secretName, err)\n+\t}\n+\n+\tif !exists {\n+\t\treturn \"\", \"\", fmt.Errorf(\"secret '%s/%s' not found\", namespace, secretName)\n+\t}\n+\n+\tif secret == nil {\n+\t\treturn \"\", \"\", fmt.Errorf(\"data for secret '%s/%s' must not be nil\", namespace, secretName)\n+\t}\n+\n+\tusername, usernameExists := secret.Data[\"username\"]\n+\tpassword, passwordExists := secret.Data[\"password\"]\n+\tif !usernameExists || !passwordExists {\n+\t\treturn \"\", \"\", fmt.Errorf(\"secret '%s/%s' must contain both username and password keys\", secret.Namespace, secret.Name)\n+\t}\n+\treturn string(username), string(password), nil\n+}\n+\n func createRetryMiddleware(retry *traefikv1alpha1.Retry) (*dynamic.Retry, error) {\n \tif retry == nil {\n \t\treturn nil, nil\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/middleware.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/middleware.go\nindex 29810587f9..91d2fd9621 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/middleware.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/middleware.go\n@@ -170,7 +170,7 @@ type ForwardAuth struct {\n \t// If not set or empty then all request headers are passed.\n \tAuthRequestHeaders []string `json:\"authRequestHeaders,omitempty\"`\n \t// TLS defines the configuration used to secure the connection to the authentication server.\n-\tTLS *ClientTLS `json:\"tls,omitempty\"`\n+\tTLS *ClientTLSWithCAOptional `json:\"tls,omitempty\"`\n \t// AddAuthCookiesToResponse defines the list of cookies to copy from the authentication server response to the response.\n \tAddAuthCookiesToResponse []string `json:\"addAuthCookiesToResponse,omitempty\"`\n \t// HeaderField defines a header field to store the authenticated user.\n@@ -186,16 +186,12 @@ type ForwardAuth struct {\n \tPreserveRequestMethod bool `json:\"preserveRequestMethod,omitempty\"`\n }\n \n-// ClientTLS holds the client TLS configuration.\n-type ClientTLS struct {\n-\t// CASecret is the name of the referenced Kubernetes Secret containing the CA to validate the server certificate.\n-\t// The CA certificate is extracted from key `tls.ca` or `ca.crt`.\n-\tCASecret string `json:\"caSecret,omitempty\"`\n-\t// CertSecret is the name of the referenced Kubernetes Secret containing the client certificate.\n-\t// The client certificate is extracted from the keys `tls.crt` and `tls.key`.\n-\tCertSecret string `json:\"certSecret,omitempty\"`\n-\t// InsecureSkipVerify defines whether the server certificates should be validated.\n-\tInsecureSkipVerify bool `json:\"insecureSkipVerify,omitempty\"`\n+// +k8s:deepcopy-gen=true\n+\n+// ClientTLSWithCAOptional holds the client TLS configuration.\n+// TODO: This has to be removed once the CAOptional option is removed.\n+type ClientTLSWithCAOptional struct {\n+\tClientTLS `json:\",inline\"`\n \n \t// Deprecated: TLS client authentication is a server side option (see https://github.com/golang/go/blob/740a490f71d026bb7d2d13cb8fa2d6d6e0572b70/src/crypto/tls/common.go#L634).\n \tCAOptional *bool `json:\"caOptional,omitempty\"`\n@@ -225,6 +221,65 @@ type RateLimit struct {\n \t// If several strategies are defined at the same time, an error will be raised.\n \t// If none are set, the default is to use the request's remote address field (as an ipStrategy).\n \tSourceCriterion *dynamic.SourceCriterion `json:\"sourceCriterion,omitempty\"`\n+\t// Redis hold the configs of Redis as bucket in rate limiter.\n+\tRedis *Redis `json:\"redis,omitempty\"`\n+}\n+\n+// +k8s:deepcopy-gen=true\n+\n+// Redis contains the configuration for using Redis in middleware.\n+// In a Kubernetes setup, the username and password are stored in a Secret file within the same namespace as the middleware.\n+type Redis struct {\n+\t// Endpoints contains either a single address or a seed list of host:port addresses.\n+\t// Default value is [\"localhost:6379\"].\n+\tEndpoints []string `json:\"endpoints,omitempty\"`\n+\t// TLS defines TLS-specific configurations, including the CA, certificate, and key,\n+\t// which can be provided as a file path or file content.\n+\tTLS *ClientTLS `json:\"tls,omitempty\"`\n+\t// Secret defines the name of the referenced Kubernetes Secret containing Redis credentials.\n+\tSecret string `json:\"secret,omitempty\"`\n+\t// DB defines the Redis database that will be selected after connecting to the server.\n+\tDB int `json:\"db,omitempty\"`\n+\t// PoolSize defines the initial number of socket connections.\n+\t// If the pool runs out of available connections, additional ones will be created beyond PoolSize.\n+\t// This can be limited using MaxActiveConns.\n+\t// // Default value is 0, meaning 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n+\tPoolSize int `json:\"poolSize,omitempty\"`\n+\t// MinIdleConns defines the minimum number of idle connections.\n+\t// Default value is 0, and idle connections are not closed by default.\n+\tMinIdleConns int `json:\"minIdleConns,omitempty\"`\n+\t// MaxActiveConns defines the maximum number of connections allocated by the pool at a given time.\n+\t// Default value is 0, meaning there is no limit.\n+\tMaxActiveConns int `json:\"maxActiveConns,omitempty\"`\n+\t// ReadTimeout defines the timeout for socket read operations.\n+\t// Default value is 3 seconds.\n+\t// +kubebuilder:validation:Pattern=\"^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\"\n+\t// +kubebuilder:validation:XIntOrString\n+\tReadTimeout *intstr.IntOrString `json:\"readTimeout,omitempty\"`\n+\t// WriteTimeout defines the timeout for socket write operations.\n+\t// Default value is 3 seconds.\n+\t// +kubebuilder:validation:Pattern=\"^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\"\n+\t// +kubebuilder:validation:XIntOrString\n+\tWriteTimeout *intstr.IntOrString `json:\"writeTimeout,omitempty\"`\n+\t// DialTimeout sets the timeout for establishing new connections.\n+\t// Default value is 5 seconds.\n+\t// +kubebuilder:validation:Pattern=\"^([0-9]+(ns|us|\u00b5s|ms|s|m|h)?)+$\"\n+\t// +kubebuilder:validation:XIntOrString\n+\tDialTimeout *intstr.IntOrString `json:\"dialTimeout,omitempty\"`\n+}\n+\n+// +k8s:deepcopy-gen=true\n+\n+// ClientTLS holds the client TLS configuration.\n+type ClientTLS struct {\n+\t// CASecret is the name of the referenced Kubernetes Secret containing the CA to validate the server certificate.\n+\t// The CA certificate is extracted from key `tls.ca` or `ca.crt`.\n+\tCASecret string `json:\"caSecret,omitempty\"`\n+\t// CertSecret is the name of the referenced Kubernetes Secret containing the client certificate.\n+\t// The client certificate is extracted from the keys `tls.crt` and `tls.key`.\n+\tCertSecret string `json:\"certSecret,omitempty\"`\n+\t// InsecureSkipVerify defines whether the server certificates should be validated.\n+\tInsecureSkipVerify bool `json:\"insecureSkipVerify,omitempty\"`\n }\n \n // +k8s:deepcopy-gen=true\ndiff --git a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go\nindex ae83e6173d..e9dd2386ba 100644\n--- a/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go\n+++ b/pkg/provider/kubernetes/crd/traefikio/v1alpha1/zz_generated.deepcopy.go\n@@ -146,6 +146,23 @@ func (in *ClientAuth) DeepCopy() *ClientAuth {\n // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n func (in *ClientTLS) DeepCopyInto(out *ClientTLS) {\n \t*out = *in\n+\treturn\n+}\n+\n+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ClientTLS.\n+func (in *ClientTLS) DeepCopy() *ClientTLS {\n+\tif in == nil {\n+\t\treturn nil\n+\t}\n+\tout := new(ClientTLS)\n+\tin.DeepCopyInto(out)\n+\treturn out\n+}\n+\n+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n+func (in *ClientTLSWithCAOptional) DeepCopyInto(out *ClientTLSWithCAOptional) {\n+\t*out = *in\n+\tout.ClientTLS = in.ClientTLS\n \tif in.CAOptional != nil {\n \t\tin, out := &in.CAOptional, &out.CAOptional\n \t\t*out = new(bool)\n@@ -154,12 +171,12 @@ func (in *ClientTLS) DeepCopyInto(out *ClientTLS) {\n \treturn\n }\n \n-// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ClientTLS.\n-func (in *ClientTLS) DeepCopy() *ClientTLS {\n+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ClientTLSWithCAOptional.\n+func (in *ClientTLSWithCAOptional) DeepCopy() *ClientTLSWithCAOptional {\n \tif in == nil {\n \t\treturn nil\n \t}\n-\tout := new(ClientTLS)\n+\tout := new(ClientTLSWithCAOptional)\n \tin.DeepCopyInto(out)\n \treturn out\n }\n@@ -265,7 +282,7 @@ func (in *ForwardAuth) DeepCopyInto(out *ForwardAuth) {\n \t}\n \tif in.TLS != nil {\n \t\tin, out := &in.TLS, &out.TLS\n-\t\t*out = new(ClientTLS)\n+\t\t*out = new(ClientTLSWithCAOptional)\n \t\t(*in).DeepCopyInto(*out)\n \t}\n \tif in.AddAuthCookiesToResponse != nil {\n@@ -1053,6 +1070,11 @@ func (in *RateLimit) DeepCopyInto(out *RateLimit) {\n \t\t*out = new(dynamic.SourceCriterion)\n \t\t(*in).DeepCopyInto(*out)\n \t}\n+\tif in.Redis != nil {\n+\t\tin, out := &in.Redis, &out.Redis\n+\t\t*out = new(Redis)\n+\t\t(*in).DeepCopyInto(*out)\n+\t}\n \treturn\n }\n \n@@ -1066,6 +1088,47 @@ func (in *RateLimit) DeepCopy() *RateLimit {\n \treturn out\n }\n \n+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n+func (in *Redis) DeepCopyInto(out *Redis) {\n+\t*out = *in\n+\tif in.Endpoints != nil {\n+\t\tin, out := &in.Endpoints, &out.Endpoints\n+\t\t*out = make([]string, len(*in))\n+\t\tcopy(*out, *in)\n+\t}\n+\tif in.TLS != nil {\n+\t\tin, out := &in.TLS, &out.TLS\n+\t\t*out = new(ClientTLS)\n+\t\t**out = **in\n+\t}\n+\tif in.ReadTimeout != nil {\n+\t\tin, out := &in.ReadTimeout, &out.ReadTimeout\n+\t\t*out = new(intstr.IntOrString)\n+\t\t**out = **in\n+\t}\n+\tif in.WriteTimeout != nil {\n+\t\tin, out := &in.WriteTimeout, &out.WriteTimeout\n+\t\t*out = new(intstr.IntOrString)\n+\t\t**out = **in\n+\t}\n+\tif in.DialTimeout != nil {\n+\t\tin, out := &in.DialTimeout, &out.DialTimeout\n+\t\t*out = new(intstr.IntOrString)\n+\t\t**out = **in\n+\t}\n+\treturn\n+}\n+\n+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Redis.\n+func (in *Redis) DeepCopy() *Redis {\n+\tif in == nil {\n+\t\treturn nil\n+\t}\n+\tout := new(Redis)\n+\tin.DeepCopyInto(out)\n+\treturn out\n+}\n+\n // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n func (in *ResponseForwarding) DeepCopyInto(out *ResponseForwarding) {\n \t*out = *in\n", "instance_id": "traefik__traefik-10211", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the goal of adding Redis support to the RateLimiter middleware in Traefik to address the issue of in-memory storage limitations when scaling across multiple instances. It identifies the core problem (data not being shared across instances) and proposes a solution (using Redis for centralized storage). However, there are minor ambiguities and missing details. For instance, the statement does not explicitly define the expected behavior of the rate limiter with Redis (e.g., how token buckets should be managed in Redis, consistency guarantees, or performance expectations). Additionally, it lacks mention of specific edge cases (e.g., Redis connection failures, latency issues) or constraints (e.g., Redis version compatibility). While the intent and high-level goal are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and areas of the Traefik codebase, including configuration definitions, middleware logic, Kubernetes CRD updates, and documentation. This requires a broad understanding of the system's architecture, particularly how middlewares interact with storage backends and how configurations are parsed and applied. Second, the technical concepts involved are moderately complex, including Redis client integration (using `go-redis`), Lua scripting for atomic operations in Redis, token bucket rate-limiting algorithms, and TLS configuration for secure connections. Implementing a distributed rate limiter also introduces challenges around consistency and performance, as well as error handling for network issues or Redis unavailability. Third, the changes impact critical functionality (rate limiting) that must work reliably at scale, necessitating careful consideration of edge cases like connection timeouts, Redis failures, and high-latency scenarios, which are not fully addressed in the provided code changes. While the problem does not require advanced domain-specific knowledge beyond typical distributed systems concepts, it demands a deep understanding of Traefik's internals and careful implementation to avoid introducing bugs or performance regressions. A score of 0.65 reflects the complexity of integrating a new storage backend into an existing middleware while ensuring robustness and compatibility with various deployment environments (e.g., Kubernetes, Docker).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Checksum not retrievable when provided as a trailing checksum via the AWS S3 SDK\n## Expected Behavior\r\nWhen putting an object via PutObject using the AWS S3 .NET SDK and requesting a trailing checksum by setting ChecksumAlgorithm (X-Amz-Sdk-Checksum-Algorithm) to \"SHA256\", I expect to be able to retrieve the checksum at a later point from either:\r\n\r\n- GetObjectAttributes (by setting X-Amz-Object-Attributes: Checksum) - the checksum value should be included in the GetObjectAttributesResponse response body\r\n- GetObject (by setting X-Amz-Checksum-Mode: ENABLED) - the checksum value should be included in the X-Amz-Checksum-SHA256 response header\r\n\r\nThe ChecksumAlgorithm should also be included in the ListObjectsV2 result but it too is missing.\r\n\r\nNote: when providing a precalculated checksum to PutObject, the checksum value is able to be retrieved later via GetObject and GetObjectAttributes, however ChecksumAlgoritm is still missing from the ListObjectsV2 result.\r\n\r\n## Current Behavior\r\nWhen I upload a file via PutObject using a trailing checksum, I can see the X-Amz-Trailer request header correctly provided and I also see the trailing checksum correctly appended to the end of the upload request.  The PutObject response includes the X-Amz-Checksum-Sha256 response header as expected.\r\n\r\nHowever, subsequent calls to ListObjectsV2, GetObjectAttributes and GetObject fail to include the checksum value (or the checksum algorithm in the case of ListObjectsV2) in their respective responses.\r\n\r\nRunning the same SDK code against an AWS S3 endpoint yields expected behaviour, that is:\r\n- ListObjectsV2 includes 'ChecksumAlgorithm' in its xml response body\r\n- GetObjectAttributes includes 'Checksum/ChecksumSHA256' in its xml response body\r\n- GetObject includes the 'X-Amz-Checksum-Sha256' response header\r\n\r\n## Environment\r\nVersion: RELEASE.2024-09-13T20-26-02Z (go1.22.7 linux/amd64)\r\nServer setup and configuration: Docker on Ubuntu 20.04.5\r\n\n", "patch": "diff --git a/cmd/erasure-object.go b/cmd/erasure-object.go\nindex de0c4082ff711..6c84c96eb0467 100644\n--- a/cmd/erasure-object.go\n+++ b/cmd/erasure-object.go\n@@ -1329,10 +1329,6 @@ func (er erasureObjects) putObject(ctx context.Context, bucket string, object st\n \t}\n \n \tfi.DataDir = mustGetUUID()\n-\tfi.Checksum = opts.WantChecksum.AppendTo(nil, nil)\n-\tif opts.EncryptFn != nil {\n-\t\tfi.Checksum = opts.EncryptFn(\"object-checksum\", fi.Checksum)\n-\t}\n \tif ckSum := userDefined[ReplicationSsecChecksumHeader]; ckSum != \"\" {\n \t\tif v, err := base64.StdEncoding.DecodeString(ckSum); err == nil {\n \t\t\tfi.Checksum = v\n@@ -1460,7 +1456,13 @@ func (er erasureObjects) putObject(ctx context.Context, bucket string, object st\n \t\t\tactualSize = n\n \t\t}\n \t}\n-\n+\tif fi.Checksum == nil {\n+\t\t// Trailing headers checksums should now be filled.\n+\t\tfi.Checksum = opts.WantChecksum.AppendTo(nil, nil)\n+\t\tif opts.EncryptFn != nil {\n+\t\t\tfi.Checksum = opts.EncryptFn(\"object-checksum\", fi.Checksum)\n+\t\t}\n+\t}\n \tfor i, w := range writers {\n \t\tif w == nil {\n \t\t\tonlineDisks[i] = nil\n@@ -1474,6 +1476,7 @@ func (er erasureObjects) putObject(ctx context.Context, bucket string, object st\n \t\t// No need to add checksum to part. We already have it on the object.\n \t\tpartsMetadata[i].AddObjectPart(1, \"\", n, actualSize, modTime, compIndex, nil)\n \t\tpartsMetadata[i].Versioned = opts.Versioned || opts.VersionSuspended\n+\t\tpartsMetadata[i].Checksum = fi.Checksum\n \t}\n \n \tuserDefined[\"etag\"] = r.MD5CurrentHexString()\ndiff --git a/cmd/object-handlers.go b/cmd/object-handlers.go\nindex 00e74d9c5ae7c..3443ecd41a13c 100644\n--- a/cmd/object-handlers.go\n+++ b/cmd/object-handlers.go\n@@ -1960,6 +1960,7 @@ func (api objectAPIHandlers) PutObjectHandler(w http.ResponseWriter, r *http.Req\n \t\treturn\n \t}\n \topts.IndexCB = idxCb\n+\topts.WantChecksum = hashReader.Checksum()\n \n \tif opts.PreserveETag != \"\" ||\n \t\tr.Header.Get(xhttp.IfMatch) != \"\" ||\ndiff --git a/internal/hash/checksum.go b/internal/hash/checksum.go\nindex 1466cec137551..a169e5acf36c5 100644\n--- a/internal/hash/checksum.go\n+++ b/internal/hash/checksum.go\n@@ -335,6 +335,10 @@ func (c *Checksum) AppendTo(b []byte, parts []byte) []byte {\n \tvar tmp [binary.MaxVarintLen32]byte\n \tn := binary.PutUvarint(tmp[:], uint64(c.Type))\n \tcrc := c.Raw\n+\tif c.Type.Trailing() {\n+\t\t// When we serialize we don't care if it was trailing.\n+\t\tc.Type ^= ChecksumTrailing\n+\t}\n \tif len(crc) != c.Type.RawByteLen() {\n \t\treturn b\n \t}\ndiff --git a/internal/hash/reader.go b/internal/hash/reader.go\nindex f849a69f0fa53..272452f06a23b 100644\n--- a/internal/hash/reader.go\n+++ b/internal/hash/reader.go\n@@ -366,6 +366,14 @@ func (r *Reader) ContentCRC() map[string]string {\n \treturn map[string]string{r.contentHash.Type.String(): r.contentHash.Encoded}\n }\n \n+// Checksum returns the content checksum if set.\n+func (r *Reader) Checksum() *Checksum {\n+\tif !r.contentHash.Type.IsSet() || !r.contentHash.Valid() {\n+\t\treturn nil\n+\t}\n+\treturn &r.contentHash\n+}\n+\n var _ io.Closer = (*Reader)(nil) // compiler check\n \n // Close and release resources.\n", "instance_id": "minio__minio-20456", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the AWS S3 SDK regarding the retrieval of trailing checksums. It outlines the expected behavior (checksum retrieval via GetObjectAttributes, GetObject, and ListObjectsV2) and contrasts it with the current behavior (failure to retrieve checksums in responses). The environment details and comparison with AWS S3 endpoint behavior add useful context. However, there are minor ambiguities: the problem does not explicitly define what constitutes a \"trailing checksum\" in technical terms (e.g., how it is appended or processed), nor does it provide specific examples of request/response payloads to illustrate the issue. Additionally, edge cases or specific constraints (e.g., file size limits, checksum algorithm variations) are not mentioned, which could impact the solution's completeness. Overall, while the goal is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files (erasure-object.go, object-handlers.go, checksum.go, reader.go), indicating a need to understand and modify interactions across different modules of the codebase. The changes involve handling checksum data, particularly for trailing checksums, which requires a deep understanding of the system's object storage and HTTP request handling logic. Second, the technical concepts involved include familiarity with Go's internal hash and checksum mechanisms, HTTP header processing, and object metadata management, as well as domain-specific knowledge of AWS S3 SDK behavior and compatibility requirements. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential complexities in ensuring checksum integrity across different upload scenarios (e.g., precalculated vs. trailing checksums) and encryption contexts, which may require additional error handling or validation logic. Finally, the impact of these changes is significant as they affect core functionality related to data integrity and API responses, necessitating careful testing to avoid regressions. While not at the extreme end of difficulty (e.g., requiring system-level redesign), this problem demands a solid grasp of the codebase and careful implementation, justifying a score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "SHA-1 hash algorithm enabled on SFTP service\n## NOTE\r\nI found that MinIO has added support for [SFTP](https://min.io/docs/minio/linux/developers/file-transfer-protocol.html) however with quick testing with [ssh-audit](https://github.com/jtesta/ssh-audit) told me that SHA-1 hash algorithm looks be enabled by default and I didn't found way to disable it.\r\n\r\n## Expected Behavior\r\nSHA-1 is considered as weak algorithm so it would be better to default to have disabled with possibility to enable it by those who really need it.\r\n\r\n## Current Behavior\r\nssh-audit command output:\r\n```\r\n(kex) diffie-hellman-group14-sha1    -- [fail] using broken SHA-1 hash algorithm\r\n```\r\n\n", "patch": "diff --git a/cmd/sftp-server.go b/cmd/sftp-server.go\nindex 04fa336a86175..e68aeadb5bc7d 100644\n--- a/cmd/sftp-server.go\n+++ b/cmd/sftp-server.go\n@@ -40,6 +40,85 @@ func (s *sftpLogger) Info(tag xsftp.LogType, msg string) {\n \tlogger.Info(msg)\n }\n \n+const (\n+\tkexAlgoDH1SHA1                = \"diffie-hellman-group1-sha1\"\n+\tkexAlgoDH14SHA1               = \"diffie-hellman-group14-sha1\"\n+\tkexAlgoDH14SHA256             = \"diffie-hellman-group14-sha256\"\n+\tkexAlgoDH16SHA512             = \"diffie-hellman-group16-sha512\"\n+\tkexAlgoECDH256                = \"ecdh-sha2-nistp256\"\n+\tkexAlgoECDH384                = \"ecdh-sha2-nistp384\"\n+\tkexAlgoECDH521                = \"ecdh-sha2-nistp521\"\n+\tkexAlgoCurve25519SHA256LibSSH = \"curve25519-sha256@libssh.org\"\n+\tkexAlgoCurve25519SHA256       = \"curve25519-sha256\"\n+\n+\tchacha20Poly1305ID = \"chacha20-poly1305@openssh.com\"\n+\tgcm256CipherID     = \"aes256-gcm@openssh.com\"\n+\taes128cbcID        = \"aes128-cbc\"\n+\ttripledescbcID     = \"3des-cbc\"\n+)\n+\n+// https://cs.opensource.google/go/x/crypto/+/refs/tags/v0.22.0:ssh/common.go;l=46\n+// preferredKexAlgos specifies the default preference for key-exchange\n+// algorithms in preference order. The diffie-hellman-group16-sha512 algorithm\n+// is disabled by default because it is a bit slower than the others.\n+var preferredKexAlgos = []string{\n+\tkexAlgoCurve25519SHA256, kexAlgoCurve25519SHA256LibSSH,\n+\tkexAlgoECDH256, kexAlgoECDH384, kexAlgoECDH521,\n+\tkexAlgoDH14SHA256, kexAlgoDH14SHA1,\n+}\n+\n+// supportedKexAlgos specifies the supported key-exchange algorithms in\n+// preference order.\n+// https://cs.opensource.google/go/x/crypto/+/refs/tags/v0.22.0:ssh/common.go;l=44\n+var supportedKexAlgos = []string{\n+\tkexAlgoCurve25519SHA256, kexAlgoCurve25519SHA256LibSSH,\n+\t// P384 and P521 are not constant-time yet, but since we don't\n+\t// reuse ephemeral keys, using them for ECDH should be OK.\n+\tkexAlgoECDH256, kexAlgoECDH384, kexAlgoECDH521,\n+\tkexAlgoDH14SHA256, kexAlgoDH16SHA512, kexAlgoDH14SHA1,\n+\tkexAlgoDH1SHA1,\n+}\n+\n+// supportedPubKeyAuthAlgos specifies the supported client public key\n+// authentication algorithms. Note that this doesn't include certificate types\n+// since those use the underlying algorithm. This list is sent to the client if\n+// it supports the server-sig-algs extension. Order is irrelevant.\n+// https://cs.opensource.google/go/x/crypto/+/refs/tags/v0.22.0:ssh/common.go;l=142\n+var supportedPubKeyAuthAlgos = []string{\n+\tssh.KeyAlgoED25519,\n+\tssh.KeyAlgoSKED25519, ssh.KeyAlgoSKECDSA256,\n+\tssh.KeyAlgoECDSA256, ssh.KeyAlgoECDSA384, ssh.KeyAlgoECDSA521,\n+\tssh.KeyAlgoRSASHA256, ssh.KeyAlgoRSASHA512, ssh.KeyAlgoRSA,\n+\tssh.KeyAlgoDSA,\n+}\n+\n+// supportedCiphers lists ciphers we support but might not recommend.\n+// https://cs.opensource.google/go/x/crypto/+/refs/tags/v0.22.0:ssh/common.go;l=28\n+var supportedCiphers = []string{\n+\t\"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\",\n+\t\"aes128-gcm@openssh.com\", gcm256CipherID,\n+\tchacha20Poly1305ID,\n+\t\"arcfour256\", \"arcfour128\", \"arcfour\",\n+\taes128cbcID,\n+\ttripledescbcID,\n+}\n+\n+// preferredCiphers specifies the default preference for ciphers.\n+// https://cs.opensource.google/go/x/crypto/+/refs/tags/v0.22.0:ssh/common.go;l=37\n+var preferredCiphers = []string{\n+\t\"aes128-gcm@openssh.com\", gcm256CipherID,\n+\tchacha20Poly1305ID,\n+\t\"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\",\n+}\n+\n+// supportedMACs specifies a default set of MAC algorithms in preference order.\n+// This is based on RFC 4253, section 6.4, but with hmac-md5 variants removed\n+// because they have reached the end of their useful life.\n+// https://cs.opensource.google/go/x/crypto/+/refs/tags/v0.22.0:ssh/common.go;l=85\n+var supportedMACs = []string{\n+\t\"hmac-sha2-256-etm@openssh.com\", \"hmac-sha2-512-etm@openssh.com\", \"hmac-sha2-256\", \"hmac-sha2-512\", \"hmac-sha1\", \"hmac-sha1-96\",\n+}\n+\n func (s *sftpLogger) Error(tag xsftp.LogType, err error) {\n \tswitch tag {\n \tcase xsftp.AcceptNetworkError:\n@@ -53,13 +132,41 @@ func (s *sftpLogger) Error(tag xsftp.LogType, err error) {\n \t}\n }\n \n+func filterAlgos(arg string, want []string, allowed []string) []string {\n+\tvar filteredAlgos []string\n+\tfound := false\n+\tfor _, algo := range want {\n+\t\tif len(algo) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor _, allowedAlgo := range allowed {\n+\t\t\talgo := strings.ToLower(strings.TrimSpace(algo))\n+\t\t\tif algo == allowedAlgo {\n+\t\t\t\tfilteredAlgos = append(filteredAlgos, algo)\n+\t\t\t\tfound = true\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tif !found {\n+\t\t\tlogger.Fatal(fmt.Errorf(\"unknown algorithm %q passed to --sftp=%s\\nValid algorithms: %v\", algo, arg, strings.Join(allowed, \", \")), \"unable to start SFTP server\")\n+\t\t}\n+\t}\n+\tif len(filteredAlgos) == 0 {\n+\t\tlogger.Fatal(fmt.Errorf(\"no valid algorithms passed to --sftp=%s\\nValid algorithms: %v\", arg, strings.Join(allowed, \", \")), \"unable to start SFTP server\")\n+\t}\n+\treturn filteredAlgos\n+}\n+\n func startSFTPServer(args []string) {\n \tvar (\n \t\tport          int\n \t\tpublicIP      string\n \t\tsshPrivateKey string\n \t)\n-\n+\tallowPubKeys := supportedPubKeyAuthAlgos\n+\tallowKexAlgos := preferredKexAlgos\n+\tallowCiphers := preferredCiphers\n+\tallowMACs := supportedMACs\n \tvar err error\n \tfor _, arg := range args {\n \t\ttokens := strings.SplitN(arg, \"=\", 2)\n@@ -82,6 +189,14 @@ func startSFTPServer(args []string) {\n \t\t\tpublicIP = host\n \t\tcase \"ssh-private-key\":\n \t\t\tsshPrivateKey = tokens[1]\n+\t\tcase \"pub-key-algos\":\n+\t\t\tallowPubKeys = filterAlgos(arg, strings.Split(tokens[1], \",\"), supportedPubKeyAuthAlgos)\n+\t\tcase \"kex-algos\":\n+\t\t\tallowKexAlgos = filterAlgos(arg, strings.Split(tokens[1], \",\"), supportedKexAlgos)\n+\t\tcase \"cipher-algos\":\n+\t\t\tallowCiphers = filterAlgos(arg, strings.Split(tokens[1], \",\"), supportedCiphers)\n+\t\tcase \"mac-algos\":\n+\t\t\tallowCiphers = filterAlgos(arg, strings.Split(tokens[1], \",\"), supportedMACs)\n \t\t}\n \t}\n \n@@ -106,6 +221,12 @@ func startSFTPServer(args []string) {\n \t// An SSH server is represented by a ServerConfig, which holds\n \t// certificate details and handles authentication of ServerConns.\n \tsshConfig := &ssh.ServerConfig{\n+\t\tConfig: ssh.Config{\n+\t\t\tKeyExchanges: allowKexAlgos,\n+\t\t\tCiphers:      allowCiphers,\n+\t\t\tMACs:         allowMACs,\n+\t\t},\n+\t\tPublicKeyAuthAlgorithms: allowPubKeys,\n \t\tPasswordCallback: func(c ssh.ConnMetadata, pass []byte) (*ssh.Permissions, error) {\n \t\t\tif globalIAMSys.LDAPConfig.Enabled() {\n \t\t\t\tsa, _, err := globalIAMSys.getServiceAccount(context.Background(), c.User())\ndiff --git a/docs/ftp/README.md b/docs/ftp/README.md\nindex de7b07eec16ee..2c6f4afd38f06 100644\n--- a/docs/ftp/README.md\n+++ b/docs/ftp/README.md\n@@ -167,3 +167,78 @@ Unlike SFTP server, FTP server is insecure by default. To operate under TLS mode\n > certs from the server certificate chain, this is mainly to add simplicity of setup. However if you wish to terminate\n > TLS certificates via a different domain for your FTP servers you may choose the above command line options.\n \n+\n+### Custom Algorithms (SFTP)\n+\n+Custom algorithms can be specified via command line parameters.\n+Algorithms are comma separated. \n+Note that valid values does not in all cases represent default values.\n+\n+`--sftp=pub-key-algos=...` specifies the supported client public key\n+authentication algorithms. Note that this doesn't include certificate types\n+since those use the underlying algorithm. This list is sent to the client if\n+it supports the server-sig-algs extension. Order is irrelevant.\n+\n+Valid values\n+```\n+ssh-ed25519\n+sk-ssh-ed25519@openssh.com\n+sk-ecdsa-sha2-nistp256@openssh.com\n+ecdsa-sha2-nistp256\n+ecdsa-sha2-nistp384\n+ecdsa-sha2-nistp521\n+rsa-sha2-256\n+rsa-sha2-512\n+ssh-rsa\n+ssh-dss\n+```\n+\n+`--sftp=kex-algos=...` specifies the supported key-exchange algorithms in preference order.\n+\n+Valid values: \n+\n+```\n+curve25519-sha256\n+curve25519-sha256@libssh.org\n+ecdh-sha2-nistp256\n+ecdh-sha2-nistp384\n+ecdh-sha2-nistp521\n+diffie-hellman-group14-sha256\n+diffie-hellman-group16-sha512\n+diffie-hellman-group14-sha1\n+diffie-hellman-group1-sha1\n+```\n+\n+`--sftp=cipher-algos=...` specifies the allowed cipher algorithms. \n+If unspecified then a sensible default is used.\n+\n+Valid values: \n+```\n+aes128-ctr\n+aes192-ctr\n+aes256-ctr\n+aes128-gcm@openssh.com\n+aes256-gcm@openssh.com\n+chacha20-poly1305@openssh.com\n+arcfour256\n+arcfour128\n+arcfour\n+aes128-cbc\n+3des-cbc\n+```\n+\n+`--sftp=mac-algos=...` specifies a default set of MAC algorithms in preference order.\n+This is based on RFC 4253, section 6.4, but with hmac-md5 variants removed because they have \n+reached the end of their useful life.\n+\n+Valid values: \n+\n+```\n+hmac-sha2-256-etm@openssh.com\n+hmac-sha2-512-etm@openssh.com\n+hmac-sha2-256\n+hmac-sha2-512\n+hmac-sha1\n+hmac-sha1-96\n+```\n+\n", "instance_id": "minio__minio-19636", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the SHA-1 hash algorithm is enabled by default in the SFTP service of MinIO, which is considered a security weakness. The expected behavior (disabling SHA-1 by default with an option to enable it) and current behavior (SHA-1 enabled as shown by ssh-audit output) are explicitly stated. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify how the configuration to disable or enable algorithms should be implemented (e.g., via environment variables, config files, or command-line arguments). Additionally, there are no examples or test cases provided to validate the solution, and edge cases or potential compatibility issues with clients relying on SHA-1 are not mentioned. Despite these minor gaps, the overall intent and goal of the problem are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is moderate, involving modifications to a single primary file (`sftp-server.go`) with significant additions (around 100 lines of code) and updates to documentation (`README.md`). The changes do not appear to impact the broader system architecture but are focused on the SFTP server's configuration logic. Second, the technical concepts required include a solid understanding of SSH protocols, key exchange algorithms, ciphers, and MACs, as well as familiarity with the Go `x/crypto/ssh` package. While these concepts are not overly complex for someone with experience in security or networking, they do require domain-specific knowledge beyond basic programming. Third, the code changes introduce a filtering mechanism for algorithms, which adds complexity in terms of parsing user input and validating against supported options, though error handling appears straightforward with fatal errors for invalid inputs. Finally, potential edge cases (e.g., handling clients that only support SHA-1, ensuring backward compatibility) are not explicitly addressed in the problem statement or code changes, but the implementation allows for customization, mitigating some risks. Overall, this problem requires understanding multiple concepts and making non-trivial modifications, justifying a difficulty score of 0.55, slightly above the midpoint of the medium range due to the security and configuration aspects.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "containerd integration: Passing a build context via tarball to the `/build` endpoint is broken\nRelated downstream issues:\n\n- https://github.com/kreuzwerker/terraform-provider-docker/issues/534\n- https://github.com/kreuzwerker/terraform-provider-docker/pull/665\n- https://github.com/moby/buildkit/issues/5623\n- https://github.com/pulumi/pulumi-docker/issues/967\n- https://github.com/kurtosis-tech/kurtosis/issues/2613\n\n\n\n### Description\n\nBuilding via the `/build` endpoint with `Version=BuilderBuildkit` fails with:\n```\nfailed to read downloaded context: failed to load cache key: invalid response status 403\n```\n\nWhen passing a build context via a tar without a build session.\n\nIt works with graphdrivers, because the `mobyworker` makes use of the passed `http.RoundTripper`:\nhttps://github.com/moby/moby/blob/8d5d655db0170361d10dbd0acfcf7d171ca29815/builder/builder-next/controller.go#L355\nhttps://github.com/moby/moby/blob/8d5d655db0170361d10dbd0acfcf7d171ca29815/builder/builder-next/controller.go#L193\n\nwhich is used to respond to a fake url echoing the passed build context:\nhttps://github.com/moby/moby/blob/8d5d655db0170361d10dbd0acfcf7d171ca29815/builder/builder-next/builder.go#L294\n\nWith the containerd integration, the ContainerdWorker doesn't use this RoundTripper so it isn't able to access the build context.\n\n### Reproduce\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"os\"\n\n\t\"github.com/docker/docker/api/types\"\n\t\"github.com/docker/docker/client\"\n)\n\nfunc main() {\n\tcli, err := client.NewClientWithOpts(client.FromEnv)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\trd, err := os.Open(\"context.tar\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer rd.Close()\n\n\timageBuildResponse, err := cli.ImageBuild(context.Background(), rd, types.ImageBuildOptions{\n\t\tVersion: types.BuilderBuildKit, // without this, classic builder is used\n\t\tTags:    []string{\"myimage:latest\"},\n\t})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer imageBuildResponse.Body.Close()\n\n\t_, err = io.Copy(os.Stdout, imageBuildResponse.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n\n[context.tar.zip](https://github.com/moby/moby/files/14976968/context.tar.zip)\n(zipping a tar, because github doesn't allow uploading tars \ud83d\ude48)\n\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient:\n Cloud integration: v1.0.35+desktop.13\n Version:           26.0.0\n API version:       1.45\n Go version:        go1.21.8\n Git commit:        2ae903e\n Built:             Wed Mar 20 15:14:46 2024\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.30.0 (145546)\n Engine:\n  Version:          26.0.0\n  API version:      1.45 (minimum version 1.24)\n  Go version:       go1.21.8\n  Git commit:       8b79278\n  Built:            Wed Mar 20 15:18:02 2024\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.6.28\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\n runc:\n  Version:          1.1.12\n  GitCommit:        v1.1.12-0-g51d5e94\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nN/A\n```\n\n\n### Additional Info\n\n_No response_\n", "patch": "diff --git a/builder/builder-next/controller.go b/builder/builder-next/controller.go\nindex cd0e234da094b..bed62eb5438cf 100644\n--- a/builder/builder-next/controller.go\n+++ b/builder/builder-next/controller.go\n@@ -148,7 +148,7 @@ func newSnapshotterController(ctx context.Context, rt http.RoundTripper, opt Opt\n \t}\n \two.Executor = exec\n \n-\tw, err := mobyworker.NewContainerdWorker(ctx, wo, opt.Callbacks)\n+\tw, err := mobyworker.NewContainerdWorker(ctx, wo, opt.Callbacks, rt)\n \tif err != nil {\n \t\treturn nil, err\n \t}\ndiff --git a/builder/builder-next/worker/containerdworker.go b/builder/builder-next/worker/containerdworker.go\nindex 848932d3fa3e3..f4b98d614c101 100644\n--- a/builder/builder-next/worker/containerdworker.go\n+++ b/builder/builder-next/worker/containerdworker.go\n@@ -2,11 +2,14 @@ package worker\n \n import (\n \t\"context\"\n+\tnethttp \"net/http\"\n \n+\t\"github.com/containerd/log\"\n \t\"github.com/docker/docker/builder/builder-next/exporter\"\n \t\"github.com/moby/buildkit/client\"\n \tbkexporter \"github.com/moby/buildkit/exporter\"\n \t\"github.com/moby/buildkit/session\"\n+\t\"github.com/moby/buildkit/source/http\"\n \t\"github.com/moby/buildkit/worker/base\"\n )\n \n@@ -17,11 +20,21 @@ type ContainerdWorker struct {\n }\n \n // NewContainerdWorker instantiates a local worker.\n-func NewContainerdWorker(ctx context.Context, wo base.WorkerOpt, callbacks exporter.BuildkitCallbacks) (*ContainerdWorker, error) {\n+func NewContainerdWorker(ctx context.Context, wo base.WorkerOpt, callbacks exporter.BuildkitCallbacks, rt nethttp.RoundTripper) (*ContainerdWorker, error) {\n \tbw, err := base.NewWorker(ctx, wo)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n+\ths, err := http.NewSource(http.Opt{\n+\t\tCacheAccessor: bw.CacheManager(),\n+\t\tTransport:     rt,\n+\t})\n+\tif err == nil {\n+\t\tbw.SourceManager.Register(hs)\n+\t} else {\n+\t\tlog.G(ctx).Warnf(\"Could not register builder http source: %s\", err)\n+\t}\n+\n \treturn &ContainerdWorker{Worker: bw, callbacks: callbacks}, nil\n }\n \n", "instance_id": "moby__moby-49194", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the containerd integration when passing a build context via tarball to the `/build` endpoint. It provides a detailed description of the error (`failed to read downloaded context: failed to load cache key: invalid response status 403`), references to related downstream issues, and a reproducible code snippet to demonstrate the problem. Additionally, it explains the root cause by comparing the behavior of `mobyworker` (which uses `http.RoundTripper`) and `ContainerdWorker` (which does not), with links to relevant code in the Moby repository. However, there are minor ambiguities: the \"Expected Behavior\" section is empty, leaving it unclear what the exact desired outcome should be beyond fixing the error. Additionally, while the issue is described, there are no explicit mentions of specific edge cases or constraints that might need to be considered in the solution. Overall, the problem is valid and mostly clear, but these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code changes is relatively focused, involving modifications to two files (`controller.go` and `containerdworker.go`) and primarily adding support for `http.RoundTripper` in the `ContainerdWorker`. However, the changes require a deep understanding of the Moby/BuildKit architecture, specifically how build contexts are handled differently between `mobyworker` and `ContainerdWorker`, and how HTTP sources are registered in BuildKit. The technical concepts involved include familiarity with Go's `net/http` package, BuildKit's source management (`http.NewSource`), and containerd integration, which are moderately advanced topics. Additionally, the problem impacts a critical functionality (building images via the `/build` endpoint), and while the provided patch seems straightforward, ensuring correctness likely involves understanding potential side effects on other parts of the system, such as caching and session management. Edge cases are not explicitly mentioned in the problem statement, but handling build contexts via tarballs inherently suggests potential issues like large file handling, network errors, or malformed tarballs, which may require additional error handling logic (as hinted by the logging of failures in the code change). Overall, solving this requires a solid grasp of the codebase and careful consideration of integration points, justifying a difficulty score of 0.65, leaning towards the harder side of the spectrum due to the specialized knowledge required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "build history: orphan records when switching between graphdriver and containerd store\n### Description\r\n\r\nrelates to\r\n* https://github.com/docker/for-mac/issues/7400\r\n* https://github.com/docker/for-win/issues/14288\r\n\r\nWhen switching to another store (graphdriver or containerd), builds from past store cannot be accessed / removed anymore and vice-versa.\r\n\r\n### Reproduce\r\n\r\nStart a dind container with graphdriver store:\r\n\r\n```console\r\ndocker run -d --restart always --privileged --name docker27 -v docker27dt:/var/lib/docker -p 12375:2375 docker:27-dind --debug --host=tcp://0.0.0.0:2375 --tlsverify=false\r\n```\r\n\r\nCreate a docker context and switch to it so buildx can build on it:\r\n\r\n```console\r\ndocker context create docker27 --docker \"host=tcp://localhost:12375,skip-tls-verify=true\"\r\ndocker context use docker27\r\n```\r\n\r\nDo a build with this Dockerfile:\r\n\r\n```Dockerfile\r\nFROM busybox\r\nRUN echo \"Hello, World!\"\r\n```\r\n\r\n```console\r\ndocker buildx build .\r\n```\r\n\r\nYou should see the build on Docker Desktop and open it:\r\n\r\n![450e58d1-d678-4a95-b8db-ab8d1010894a](https://github.com/user-attachments/assets/ae6dc2a3-787d-4e65-a4b4-d404f9ffe4bb)\r\n\r\n![53f6071f-a614-4b92-8204-ed4a1bf864e6](https://github.com/user-attachments/assets/37c03ef7-a796-4d69-9086-7c699e054199)\r\n\r\nNow restart the container with containerd store enabled:\r\n\r\n```console\r\ndocker context use default\r\ndocker rm -f docker27\r\ndocker run -d --restart always --privileged --name docker27 -v docker27dt:/var/lib/docker -p 12375:2375 --env TEST_INTEGRATION_USE_SNAPSHOTTER=1 docker:27-dind --debug --host=tcp://0.0.0.0:2375 --tlsverify=false\r\n```\r\n\r\nYou should see containerd store enabled in the logs:\r\n\r\n```\r\ntime=\"2024-09-30T19:43:01.560448969Z\" level=info msg=\"Starting daemon with containerd snapshotter integration enabled\"\r\n```\r\n\r\nWe still see the build in Docker Dekstop but opening it yield to this error:\r\n\r\n![831e71d2-fc32-461f-9bd4-270469f67df7](https://github.com/user-attachments/assets/663f3a59-e3df-4041-b44c-a407d9274070)\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nAs the blobs for this record are in another store I would expect it not being listed by `ControlClient().ListenBuildHistory(ctx, &controlapi.BuildHistoryRequest{})`.\r\n\r\n### docker version\r\n\r\n```bash\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          27.3.1\r\n  API version:      1.47 (minimum version 1.24)\r\n  Go version:       go1.22.7\r\n  Git commit:       41ca978\r\n  Built:            Fri Sep 20 11:41:02 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.22\r\n  GitCommit:        7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c\r\n runc:\r\n  Version:          1.1.14\r\n  GitCommit:        v1.1.14-0-g2c9f560\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 27.3.1\r\n Storage Driver: overlayfs\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c\r\n runc version: v1.1.14-0-g2c9f560\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 6.10.9-linuxkit\r\n Operating System: Alpine Linux v3.20 (containerized)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 32\r\n Total Memory: 31.3GiB\r\n Name: 0d137b8046d0\r\n ID: af60703c-6a88-4bf5-8dd7-53880e95d58b\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 34\r\n  Goroutines: 69\r\n  System Time: 2024-09-30T19:54:49.198771943Z\r\n  EventsListeners: 0\r\n Username: crazymax\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nI think we have several ways to mitigate this issue:\r\n* Even if the record is listed in the history db, check if it is part of the current content store before sending the event.\r\n* Create a new \"store\" field in the history db bucket and do a migration of all records that would set the db being used (either \"graph\" or \"containerd\") for each of them.\r\n  * Or create a bbolt history db for each store and do the migration?\r\n\r\ncc @tonistiigi \n", "patch": "diff --git a/builder/builder-next/controller.go b/builder/builder-next/controller.go\nindex 4c38642d9d22c..0dea764b7d3f3 100644\n--- a/builder/builder-next/controller.go\n+++ b/builder/builder-next/controller.go\n@@ -43,6 +43,8 @@ import (\n \tcontainerdsnapshot \"github.com/moby/buildkit/snapshot/containerd\"\n \t\"github.com/moby/buildkit/solver\"\n \t\"github.com/moby/buildkit/solver/bboltcachestorage\"\n+\t\"github.com/moby/buildkit/solver/pb\"\n+\t\"github.com/moby/buildkit/util/apicaps\"\n \t\"github.com/moby/buildkit/util/archutil\"\n \t\"github.com/moby/buildkit/util/entitlements\"\n \t\"github.com/moby/buildkit/util/network/netproviders\"\n@@ -55,9 +57,6 @@ import (\n \t\"go.etcd.io/bbolt\"\n \tbolt \"go.etcd.io/bbolt\"\n \t\"go.opentelemetry.io/otel/sdk/trace\"\n-\n-\t\"github.com/moby/buildkit/solver/pb\"\n-\t\"github.com/moby/buildkit/util/apicaps\"\n )\n \n func newController(ctx context.Context, rt http.RoundTripper, opt Opt) (*control.Controller, error) {\n@@ -86,7 +85,7 @@ func newSnapshotterController(ctx context.Context, rt http.RoundTripper, opt Opt\n \t\treturn nil, err\n \t}\n \n-\thistoryDB, historyConf, err := openHistoryDB(opt.Root, opt.BuilderConfig.History)\n+\thistoryDB, historyConf, err := openHistoryDB(opt.Root, \"history_c8d.db\", opt.BuilderConfig.History)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -197,8 +196,8 @@ func newSnapshotterController(ctx context.Context, rt http.RoundTripper, opt Opt\n \t})\n }\n \n-func openHistoryDB(root string, cfg *config.BuilderHistoryConfig) (*bolt.DB, *bkconfig.HistoryConfig, error) {\n-\tdb, err := bbolt.Open(filepath.Join(root, \"history.db\"), 0o600, nil)\n+func openHistoryDB(root string, fn string, cfg *config.BuilderHistoryConfig) (*bolt.DB, *bkconfig.HistoryConfig, error) {\n+\tdb, err := bbolt.Open(filepath.Join(root, fn), 0o600, nil)\n \tif err != nil {\n \t\treturn nil, nil, err\n \t}\n@@ -344,7 +343,7 @@ func newGraphDriverController(ctx context.Context, rt http.RoundTripper, opt Opt\n \t\treturn nil, err\n \t}\n \n-\thistoryDB, historyConf, err := openHistoryDB(opt.Root, opt.BuilderConfig.History)\n+\thistoryDB, historyConf, err := openHistoryDB(opt.Root, \"history.db\", opt.BuilderConfig.History)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n", "instance_id": "moby__moby-48565", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "\nThe problem statement is mostly clear in describing the issue of \"orphan records\" in build history when switching between graphdriver and containerd stores in Docker. It provides detailed steps to reproduce the issue, including specific commands, expected behavior, and visual evidence of the error in Docker Desktop. The goal is evident: builds from a previous store should not be listed if they are inaccessible due to a store switch. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected output format or constraints for the build history API response. Additionally, while potential mitigation strategies are suggested (e.g., checking the content store or adding a \"store\" field to the history DB), they are presented as ideas rather than concrete requirements, leaving room for interpretation. Edge cases, such as partial store migrations or concurrent store switches, are not addressed. Overall, the statement is valid and clear but lacks some precision in requirements and edge case specifications, warranting a score of 2 (Mostly Clear).\n", "difficulty_explanation": "\nI assign a difficulty score of 0.65, placing this problem in the \"Hard\" range (0.6-0.8), due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are relatively focused, modifying a single file (`controller.go`) to use distinct history database filenames for different stores (\"history.db\" for graphdriver and \"history_c8d.db\" for containerd). However, this change hints at a broader impact on the system, as it involves separating build history storage based on the underlying store type. Understanding the interaction between the build history database (using bbolt) and the content store (graphdriver or containerd) is necessary. While the diff is small, implementing a complete solution (as suggested in the problem statement, e.g., checking content store availability or migrating records) could require changes across multiple modules, including the build history API and storage logic. This suggests a moderate-to-high impact on the codebase architecture.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several advanced concepts: Docker's storage drivers (graphdriver vs. containerd snapshotter), BuildKit's architecture (particularly its build history management), and the bbolt database for persistent storage. Additionally, knowledge of Docker's control API (`ControlClient().ListenBuildHistory`) and content store interactions is needed. These are domain-specific and moderately complex concepts, especially for someone not deeply familiar with Docker internals. The code also involves Go-specific features like file path handling and database initialization, though these are relatively straightforward.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but switching between stores introduces several potential issues, such as handling builds in progress during a switch, dealing with corrupted or inaccessible history records, or managing concurrent access to history databases. The proposed solution of using separate database files mitigates some issues but may introduce others, like ensuring consistency during migrations or handling cases where both stores are partially active. Error handling logic will likely need to be added or modified to gracefully handle inaccessible builds, which adds to the complexity.\n\n4. **Overall Complexity**: While the provided code change is simple (modifying a filename parameter), it is likely only a partial solution. A full resolution may involve deeper changes, such as filtering build history based on store availability or implementing a migration mechanism for history records. This requires a deep understanding of BuildKit's internals and careful consideration of performance and consistency, pushing the difficulty into the \"Hard\" category. However, it does not reach \"Very Hard\" (0.8-1.0) as it does not appear to involve system-level redesigns or highly intricate distributed systems logic.\n\nIn summary, this problem requires a solid grasp of Docker/BuildKit internals, moderate architectural changes, and careful handling of potential edge cases, justifying a difficulty score of 0.65.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Model saving broken when parent model name has /\n### What is the issue?\n\nUsing the /save command works for models whose parent doesn't have a slash in the name, but seems broken for models whose parent has a slash.\n\nExample:\n\n~~~sh\n[sultan@wailer ~]$ ollama run phi4\n>>> /set parameter num_ctx 16384\nSet parameter 'num_ctx' to '16384'\n>>> /set parameter temperature 0.5\nSet parameter 'temperature' to '0.5'\n>>> /save phi4-sqk\nCreated new model 'phi4-sqk'\n>>> /bye\n\n[sultan@wailer ~]$ ollama run huihui_ai/phi4-abliterated\n>>> /set parameter num_ctx 16384\nSet parameter 'num_ctx' to '16384'\n>>> /set parameter temperature 0.5\nSet parameter 'temperature' to '0.5'\n>>> /save phi4-sqk\nerror: The model name 'phi4-sqk' is invalid\n~~~\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\n_No response_\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.5.13\n", "patch": "diff --git a/cmd/interactive.go b/cmd/interactive.go\nindex f3489b6525f..d85510d4585 100644\n--- a/cmd/interactive.go\n+++ b/cmd/interactive.go\n@@ -18,6 +18,7 @@ import (\n \t\"github.com/ollama/ollama/envconfig\"\n \t\"github.com/ollama/ollama/readline\"\n \t\"github.com/ollama/ollama/types/errtypes\"\n+\t\"github.com/ollama/ollama/types/model\"\n )\n \n type MultilineState int\n@@ -459,9 +460,16 @@ func generateInteractive(cmd *cobra.Command, opts runOptions) error {\n }\n \n func NewCreateRequest(name string, opts runOptions) *api.CreateRequest {\n+\tparentModel := opts.ParentModel\n+\n+\tmodelName := model.ParseName(parentModel)\n+\tif !modelName.IsValid() {\n+\t\tparentModel = \"\"\n+\t}\n+\n \treq := &api.CreateRequest{\n-\t\tName: name,\n-\t\tFrom: cmp.Or(opts.ParentModel, opts.Model),\n+\t\tModel: name,\n+\t\tFrom:  cmp.Or(parentModel, opts.Model),\n \t}\n \n \tif opts.System != \"\" {\n", "instance_id": "ollama__ollama-9788", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `/save` command fails for models with a parent name containing a slash ('/'). It provides a reproducible example with command-line interactions, which helps in understanding the context and the expected behavior versus the actual error. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes a valid model name or the expected behavior when a parent model name contains a slash. Additionally, there are no mentions of specific edge cases or constraints beyond the provided example. The lack of relevant log output or deeper context about the internal model naming logic also leaves some gaps in fully understanding the root cause without diving into the codebase. Overall, while the issue is clear enough to start investigating, these minor missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows a localized change in a single file (`cmd/interactive.go`) within a specific function (`NewCreateRequest`). The modification involves adding logic to validate the parent model name using a `model.ParseName` function and conditionally setting the `parentModel` to an empty string if invalid. The change is small (a few lines of code) and does not impact the broader system architecture or require modifications across multiple modules.\n\n2. **Technical Concepts Involved**: Solving this requires understanding basic Go programming constructs (e.g., function calls, conditional logic) and familiarity with the project's internal `model.ParseName` utility for validation. There are no complex algorithms, design patterns, or advanced language features involved. The concept of model name validation is straightforward and does not require deep domain-specific knowledge beyond the context of the application.\n\n3. **Edge Cases and Error Handling**: The problem statement highlights a specific failure case (parent model name with a slash), and the code change addresses it by invalidating such names. However, there are no explicit mentions of other edge cases (e.g., other invalid characters, empty names, or case sensitivity) in the problem statement, and the code change does not introduce additional error handling beyond the validation check. The complexity of edge cases appears minimal at this stage.\n\n4. **Overall Complexity**: The task involves a simple bug fix by adding a validation step before using the parent model name. It requires understanding the intent of the `/save` command and the role of the `ParentModel` field, but this can be inferred from the context and the diff. There is no indication of performance considerations, architectural redesign, or deep codebase knowledge required.\n\nGiven these points, a difficulty score of 0.30 reflects an easy problem that requires minimal code changes and a basic understanding of the relevant logic. It is slightly above the lower end of the \"Easy\" range due to the need to understand the model name validation logic and ensure the fix aligns with the intended behavior of the `/save` command.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`RUNNER_ARCH` and `-self-hosted`\n### Bug report info\r\n\r\n```plain text\r\nact --bug-report\r\nact version:            0.2.68\r\nGOOS:                   darwin\r\nGOARCH:                 amd64\r\nNumCPU:                 16\r\nDocker host:            DOCKER_HOST environment variable is not set\r\nSockets found:\r\n        /var/run/docker.sock\r\nConfig files:\r\n        /Users/derrik.fleming/Library/Application Support/act/actrc:\r\n                -P ubuntu-latest=node:16-buster-slim\r\n                -P ubuntu-22.04=node:16-bullseye-slim\r\n                -P ubuntu-20.04=node:16-buster-slim\r\n                -P ubuntu-18.04=node:16-buster-slim\r\nBuild info:\r\n        Go version:            go1.23.3\r\n        Module path:           github.com/nektos/act\r\n        Main version:          (devel)\r\n        Main path:             github.com/nektos/act\r\n        Main checksum:\r\n        Build settings:\r\n                -buildmode:           exe\r\n                -compiler:            gc\r\n                -trimpath:            true\r\n                DefaultGODEBUG:       asynctimerchan=1,gotypesalias=0,httplaxcontentlength=1,httpmuxgo121=1,httpservecontentkeepheaders=1,tls10server=1,tls3des=1,tlskyber=0,tlsrsakex=1,tlsunsafeekm=1,winreadlinkvolume=0,winsymlink=0,x509keypairleaf=0,x509negativeserial=1\r\n                CGO_ENABLED:          1\r\n                GOARCH:               amd64\r\n                GOOS:                 darwin\r\n                GOAMD64:              v1\r\nDocker Engine:\r\n        Engine version:        27.3.1\r\n        Engine runtime:        runc\r\n        Cgroup version:        2\r\n        Cgroup driver:         cgroupfs\r\n        Storage driver:        overlay2\r\n        Registry URI:          https://index.docker.io/v1/\r\n        OS:                    OrbStack\r\n        OS type:               linux\r\n        OS version:\r\n        OS arch:               x86_64\r\n        OS kernel:             6.10.12-orbstack-00282-gd1783374c25e\r\n        OS CPU:                16\r\n        OS memory:             5981 MB\r\n        Security options:\r\n                name=seccomp,profile=builtin\r\n                name=cgroupns\r\n```\r\n\r\n\r\n### Command used with act\r\n\r\n```sh\r\nact -P macos-latest=-self-hosted --container-architecture=darwin/x86_64\r\n```\r\n\r\n\r\n### Describe issue\r\n\r\nSeems like the `--container-architecture` flag has no effect with `-self-hosted`. \r\n\r\nI understand the name doesn't really make sense given that it's not running in a container when using the `-self-hosted` flag, so I'm not surprised that it doesn't work in the same way it would when running in a container. But I am making use of an Action in my Workflow that depends on the architecture, and when `RUNNER_ARCH` is set to the default `amd64` the Action fails.\r\n\r\nI'm looking for a way to be able set `RUNNER_ARCH` when using `-self-hosted`. I've tried setting it via `--env` and on my system, in both cases it seems like it gets overridden by Act.\r\n\r\n### Link to GitHub repository\r\n\r\n_No response_\r\n\r\n### Workflow content\r\n\r\n```yml\r\nname: Build, analyze, format, test, check coverage\r\n\r\non:\r\n  push:\r\n    branches: [ \"main\" ]\r\n  pull_request:\r\n    branches: [ \"main\" ]\r\n\r\njobs:\r\n  build:\r\n    runs-on: macos-latest\r\n\r\n    steps:\r\n      - uses: actions/checkout@v4\r\n\r\n      - uses: Atsumi3/actions-setup-fvm@0.0.3\r\n\r\n      - uses:  kuhnroyal/flutter-fvm-config-action@v2\r\n        id: fvm-config-action\r\n\r\n      - uses: subosito/flutter-action@v2\r\n        with:\r\n          flutter-version: ${{ steps.fvm-config-action.outputs.FLUTTER_VERSION }}\r\n          channel: ${{ steps.fvm-config-action.outputs.FLUTTER_CHANNEL }}\r\n\r\n      - uses: bluefireteam/melos-action@v3\r\n\r\n      - name: Install dependencies\r\n        run: melos get\r\n\r\n      - name: Activate coverde\r\n        run: dart pub global activate coverde\r\n\r\n      - name: Build\r\n        run: melos build\r\n\r\n      - name: Format\r\n        run: melos format:ci\r\n\r\n      - name: Analyze\r\n        run: melos analyze\r\n\r\n      - name: Test\r\n        run: melos test\r\n\r\n      - name: Coverage\r\n        run: melos coverage\r\n\r\n      - name: Check coverage\r\n        run: melos check_coverage\r\n\r\n      - name: Build iOS and Android apps\r\n        run: melos build_apps\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```sh\r\n[Build, analyze, format, test, check coverage/build] \u2b50 Run Main Set action inputs\r\n| Unable to determine Flutter version for channel: stable version: 3.24.3 architecture: amd64\r\n[Build, analyze, format, test, check coverage/build]   \u274c  Failure - Main Set action inputs\r\n[Build, analyze, format, test, check coverage/build] exit status 1\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: PUB-CACHE-KEY=\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: PUB-CACHE-PATH=\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: CHANNEL=\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: VERSION=\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: ARCHITECTURE=\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: CACHE-KEY=\r\n[Build, analyze, format, test, check coverage/build]   \u2699  ::set-output:: CACHE-PATH=\r\n[Build, analyze, format, test, check coverage/build]   \u274c  Failure - Main subosito/flutter-action@v2\r\n[Build, analyze, format, test, check coverage/build] exit status 1\r\n[Build, analyze, format, test, check coverage/build]   \u274c  Failure - Main Atsumi3/actions-setup-fvm@0.0.3\r\n[Build, analyze, format, test, check coverage/build] exit status 1\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "patch": "diff --git a/pkg/container/host_environment.go b/pkg/container/host_environment.go\nindex 0103ebdb2f0..4e9efd18b52 100644\n--- a/pkg/container/host_environment.go\n+++ b/pkg/container/host_environment.go\n@@ -423,6 +423,7 @@ func (*HostEnvironment) JoinPathVariable(paths ...string) string {\n // https://docs.github.com/en/actions/learn-github-actions/contexts#runner-context\n func goArchToActionArch(arch string) string {\n \tarchMapper := map[string]string{\n+\t\t\"amd64\":   \"X64\",\n \t\t\"x86_64\":  \"X64\",\n \t\t\"386\":     \"X86\",\n \t\t\"aarch64\": \"ARM64\",\n", "instance_id": "nektos__act-2580", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `--container-architecture` flag does not affect the `RUNNER_ARCH` environment variable when using the `-self-hosted` option in the `act` tool, leading to failures in workflows that depend on specific architectures. The goal is evident\u2014allow setting or respecting `RUNNER_ARCH` in a self-hosted context. The provided logs, command usage, and workflow content help contextualize the issue. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior (e.g., should `--container-architecture` directly set `RUNNER_ARCH`, or is a different mechanism desired?). Additionally, constraints or limitations of the `-self-hosted` mode are not detailed, and edge cases (e.g., unsupported architectures or conflicts with other flags) are not mentioned. Despite these minor gaps, the issue is understandable with the provided context, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is assessed as Easy (0.25) based on the provided code changes and the nature of the issue. Here's the breakdown by the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows a minimal change in a single file (`host_environment.go`), specifically adding a mapping for `\"amd64\"` to `\"X64\"` in a function that converts Go architecture names to GitHub Actions architecture names. This suggests the fix is localized and does not impact multiple modules or the broader system architecture. The amount of code change is trivial, likely part of a larger solution but not indicative of extensive modifications.\n\n2. **Number of Technical Concepts:** Solving this problem requires basic understanding of Go (mapping data structures), familiarity with environment variables in GitHub Actions (`RUNNER_ARCH`), and how `act` simulates GitHub Actions runners. The concepts involved are straightforward for someone with moderate experience in Go and CI/CD tools. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic runner configuration are needed.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but potential issues could include handling unsupported architectures or conflicts with other environment variables. The provided code change does not address error handling, and it\u2019s unclear if additional logic is needed to fully resolve the issue (e.g., ensuring `RUNNER_ARCH` is set correctly in all self-hosted scenarios). However, these considerations appear manageable and do not significantly elevate the difficulty.\n\n4. **Overall Assessment:** While the full solution might involve additional changes (e.g., respecting `--container-architecture` or allowing manual `RUNNER_ARCH` overrides), the provided diff and the nature of the problem suggest a relatively simple fix. It requires understanding some code logic (how architecture mapping works in `act`) and making minor modifications. There\u2019s no indication of deep architectural impact or complex interactions across the codebase. Thus, a difficulty score of 0.25 (Easy) is appropriate, reflecting a problem that is solvable with basic to intermediate knowledge of the tool and language.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support loading placeholder content from files (for systemd credentials or docker secrets)\nIt would be nice to support loading secrets (e.g. api tokens for DNS auth) from files (as opposed to env vars).\r\n\r\nE.g. using a syntax like `{file./foo/bar/baz.txt}`, which would be replaced with the contents of the file at `/foo/bar/baz.txt`.\r\n\r\nThat would support using [systemd credentials](https://systemd.io/CREDENTIALS/) or [docker secrets](https://docs.docker.com/engine/swarm/secrets/) to provide those secrets.\n", "patch": "diff --git a/modules/caddyhttp/templates/tplcontext.go b/modules/caddyhttp/templates/tplcontext.go\nindex 8ba64200f2b..4c7c86e13d2 100644\n--- a/modules/caddyhttp/templates/tplcontext.go\n+++ b/modules/caddyhttp/templates/tplcontext.go\n@@ -249,6 +249,12 @@ func (c *TemplateContext) executeTemplateInBuffer(tplName string, buf *bytes.Buf\n \n func (c TemplateContext) funcPlaceholder(name string) string {\n \trepl := c.Req.Context().Value(caddy.ReplacerCtxKey).(*caddy.Replacer)\n+\n+\t// For safety, we don't want to allow the file placeholder in\n+\t// templates because it could be used to read arbitrary files\n+\t// if the template contents were not trusted.\n+\trepl = repl.WithoutFile()\n+\n \tvalue, _ := repl.GetString(name)\n \treturn value\n }\ndiff --git a/replacer.go b/replacer.go\nindex 2ad5b8bcbbd..e5d2913e928 100644\n--- a/replacer.go\n+++ b/replacer.go\n@@ -16,6 +16,7 @@ package caddy\n \n import (\n \t\"fmt\"\n+\t\"io\"\n \t\"net/http\"\n \t\"os\"\n \t\"path/filepath\"\n@@ -24,6 +25,8 @@ import (\n \t\"strings\"\n \t\"sync\"\n \t\"time\"\n+\n+\t\"go.uber.org/zap\"\n )\n \n // NewReplacer returns a new Replacer.\n@@ -32,9 +35,10 @@ func NewReplacer() *Replacer {\n \t\tstatic:   make(map[string]any),\n \t\tmapMutex: &sync.RWMutex{},\n \t}\n-\trep.providers = []ReplacerFunc{\n-\t\tglobalDefaultReplacements,\n-\t\trep.fromStatic,\n+\trep.providers = []replacementProvider{\n+\t\tglobalDefaultReplacementProvider{},\n+\t\tfileReplacementProvider{},\n+\t\tReplacerFunc(rep.fromStatic),\n \t}\n \treturn rep\n }\n@@ -46,8 +50,8 @@ func NewEmptyReplacer() *Replacer {\n \t\tstatic:   make(map[string]any),\n \t\tmapMutex: &sync.RWMutex{},\n \t}\n-\trep.providers = []ReplacerFunc{\n-\t\trep.fromStatic,\n+\trep.providers = []replacementProvider{\n+\t\tReplacerFunc(rep.fromStatic),\n \t}\n \treturn rep\n }\n@@ -56,10 +60,25 @@ func NewEmptyReplacer() *Replacer {\n // A default/empty Replacer is not valid;\n // use NewReplacer to make one.\n type Replacer struct {\n-\tproviders []ReplacerFunc\n+\tproviders []replacementProvider\n+\tstatic    map[string]any\n+\tmapMutex  *sync.RWMutex\n+}\n \n-\tstatic   map[string]any\n-\tmapMutex *sync.RWMutex\n+// WithoutFile returns a copy of the current Replacer\n+// without support for the {file.*} placeholder, which\n+// may be unsafe in some contexts.\n+//\n+// EXPERIMENTAL: Subject to change or removal.\n+func (r *Replacer) WithoutFile() *Replacer {\n+\trep := &Replacer{static: r.static}\n+\tfor _, v := range r.providers {\n+\t\tif _, ok := v.(fileReplacementProvider); ok {\n+\t\t\tcontinue\n+\t\t}\n+\t\trep.providers = append(rep.providers, v)\n+\t}\n+\treturn rep\n }\n \n // Map adds mapFunc to the list of value providers.\n@@ -79,7 +98,7 @@ func (r *Replacer) Set(variable string, value any) {\n // the value and whether the variable was known.\n func (r *Replacer) Get(variable string) (any, bool) {\n \tfor _, mapFunc := range r.providers {\n-\t\tif val, ok := mapFunc(variable); ok {\n+\t\tif val, ok := mapFunc.replace(variable); ok {\n \t\t\treturn val, true\n \t\t}\n \t}\n@@ -298,14 +317,52 @@ func ToString(val any) string {\n \t}\n }\n \n-// ReplacerFunc is a function that returns a replacement\n-// for the given key along with true if the function is able\n-// to service that key (even if the value is blank). If the\n-// function does not recognize the key, false should be\n-// returned.\n+// ReplacerFunc is a function that returns a replacement for the\n+// given key along with true if the function is able to service\n+// that key (even if the value is blank). If the function does\n+// not recognize the key, false should be returned.\n type ReplacerFunc func(key string) (any, bool)\n \n-func globalDefaultReplacements(key string) (any, bool) {\n+func (f ReplacerFunc) replace(key string) (any, bool) {\n+\treturn f(key)\n+}\n+\n+// replacementProvider is a type that can provide replacements\n+// for placeholders. Allows for type assertion to determine\n+// which type of provider it is.\n+type replacementProvider interface {\n+\treplace(key string) (any, bool)\n+}\n+\n+// fileReplacementsProvider handles {file.*} replacements,\n+// reading a file from disk and replacing with its contents.\n+type fileReplacementProvider struct{}\n+\n+func (f fileReplacementProvider) replace(key string) (any, bool) {\n+\tif !strings.HasPrefix(key, filePrefix) {\n+\t\treturn nil, false\n+\t}\n+\n+\tfilename := key[len(filePrefix):]\n+\tmaxSize := 1024 * 1024\n+\tbody, err := readFileIntoBuffer(filename, maxSize)\n+\tif err != nil {\n+\t\twd, _ := os.Getwd()\n+\t\tLog().Error(\"placeholder: failed to read file\",\n+\t\t\tzap.String(\"file\", filename),\n+\t\t\tzap.String(\"working_dir\", wd),\n+\t\t\tzap.Error(err))\n+\t\treturn nil, true\n+\t}\n+\treturn string(body), true\n+}\n+\n+// globalDefaultReplacementsProvider handles replacements\n+// that can be used in any context, such as system variables,\n+// time, or environment variables.\n+type globalDefaultReplacementProvider struct{}\n+\n+func (f globalDefaultReplacementProvider) replace(key string) (any, bool) {\n \t// check environment variable\n \tconst envPrefix = \"env.\"\n \tif strings.HasPrefix(key, envPrefix) {\n@@ -347,6 +404,24 @@ func globalDefaultReplacements(key string) (any, bool) {\n \treturn nil, false\n }\n \n+// readFileIntoBuffer reads the file at filePath into a size limited buffer.\n+func readFileIntoBuffer(filename string, size int) ([]byte, error) {\n+\tfile, err := os.Open(filename)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer file.Close()\n+\n+\tbuffer := make([]byte, size)\n+\tn, err := file.Read(buffer)\n+\tif err != nil && err != io.EOF {\n+\t\treturn nil, err\n+\t}\n+\n+\t// slice the buffer to the actual size\n+\treturn buffer[:n], nil\n+}\n+\n // ReplacementFunc is a function that is called when a\n // replacement is being performed. It receives the\n // variable (i.e. placeholder name) and the value that\n@@ -363,3 +438,5 @@ var nowFunc = time.Now\n const ReplacerCtxKey CtxKey = \"replacer\"\n \n const phOpen, phClose, phEscape = '{', '}', '\\\\'\n+\n+const filePrefix = \"file.\"\n", "instance_id": "caddyserver__caddy-5463", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to support loading secrets from files using a specific syntax like `{file./foo/bar/baz.txt}`. It provides context by referencing use cases such as systemd credentials and Docker secrets, which helps in understanding the motivation. However, there are minor ambiguities and missing details. For instance, it does not specify constraints on file paths (e.g., relative vs. absolute paths, allowed directories), file size limits (though the code implies a limit), or expected behavior when files are inaccessible or do not exist. Additionally, there are no explicit examples of input/output or detailed error handling requirements in the statement. While the goal is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes involves modifications across two files (`tplcontext.go` and `replacer.go`), requiring an understanding of the existing `Replacer` mechanism and its integration with templates in a Caddy HTTP module. The changes include adding a new replacement provider for file-based placeholders and ensuring security by disabling file placeholders in untrusted contexts, which shows a moderate level of architectural awareness. Second, the technical concepts involved include Go's interfaces, file I/O operations, string manipulation, and logging with `zap`, along with a need to understand the existing placeholder replacement logic. These are not overly complex for an experienced developer but require careful implementation. Third, the code changes address some edge cases, such as file read errors and size limits (1MB hardcoded), but the problem statement does not explicitly mention others like file permissions or path traversal risks, which the developer must infer. Overall, the task requires a solid understanding of multiple concepts and careful handling of security implications, placing it slightly above medium difficulty at 0.55. It does not reach the hard range as it does not involve deep architectural refactoring or highly complex algorithms.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Interactive staging errors out\n**Describe the bug**\nThe interactive staging, where you can stage by chunks/lines does not appear to work.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Create an empty repository `git init`.\n2. Create a file containing two lines.\n```\nline one\nline two\n```\n3. Commit the file.\n4. Edit the file.\n```\nline one and something\nline two\nline three\n```\n5. Select the file and press enter to go the staging view.\n6. Select the first chunk `line one and something`.\n7. Stage the chunk\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/57222fd7-7d4d-4e30-93ff-9f9498be70ce)\n\nhttps://github.com/user-attachments/assets/ba27853a-90e9-4344-876c-b3b7116d3ae8\n\n**Version info:**\n```bash\n~/g/lazygit-interactive-staging-repro (master)> lazygit --version\ncommit=, build date=, build source=homebrew, version=0.45.2, os=darwin, arch=arm64, git version=2.48.1\n```\n```bash\n~/g/lazygit-interactive-staging-repro (master)> git --version\ngit version 2.48.1\n```\n\n**Additional context**\nOS: MacOS\nTerminal Emulator: Ghostty\nShell: fish\n\nI have used `git add -p` to stage the first chunk.\n```bash\n~/g/lazygit-interactive-staging-repro (master)> git diff\ndiff --git a/test.txt b/test.txt\nindex e5c5c55..9b53c18 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1,2 +1,3 @@\n-line one\n+line one and something\n line two\n+line three\n\n~/g/lazygit-interactive-staging-repro (master)> git add -p test.txt\ndiff --git a/test.txt b/test.txt\nindex e5c5c55..9b53c18 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1,2 +1,3 @@\n-line one\n+line one and something\n line two\n+line three\n(1/1) Stage this hunk [y,n,q,a,d,s,e,p,?]? e\n\n~/g/lazygit-interactive-staging-repro (master)> git diff --cached\ndiff --git a/test.txt b/test.txt\nindex e5c5c55..92186d0 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1,2 +1,2 @@\n-line one\n+line one and something\n line two\n```\n\nI then used `git diff --cached > p.patch` to make patch that should be similar to what I imagine lazygit is using.\n```bash\n~/g/lazygit-interactive-staging-repro (master)> git diff --cached > p.patch\n~/g/lazygit-interactive-staging-repro (master)> cat p.patch\ndiff --git a/test.txt b/test.txt\nindex e5c5c55..92186d0 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1,2 +1,2 @@\n-line one\n+line one and something\n line two\n```\n\nI then reset the staging area with `git restore --staged test.txt`.\n```bash\n~/g/lazygit-interactive-staging-repro (master)> git restore --staged test.txt\n~/g/lazygit-interactive-staging-repro (master)> git diff\ndiff --git a/test.txt b/test.txt\nindex e5c5c55..9b53c18 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1,2 +1,3 @@\n-line one\n+line one and something\n line two\n+line three\n```\n\nFinally I used `git apply --cached p.patch` and the patch was applied succesfully.\n```bash\n~/g/lazygit-interactive-staging-repro (master)> git apply --cached p.patch\n~/g/lazygit-interactive-staging-repro (master)> git diff\ndiff --git a/test.txt b/test.txt\nindex 92186d0..9b53c18 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1,2 +1,3 @@\n line one and something\n line two\n+line three\n```\n", "patch": "diff --git a/pkg/gui/controllers/staging_controller.go b/pkg/gui/controllers/staging_controller.go\nindex c3ea3ca247f..f353cc2154f 100644\n--- a/pkg/gui/controllers/staging_controller.go\n+++ b/pkg/gui/controllers/staging_controller.go\n@@ -1,11 +1,13 @@\n package controllers\n \n import (\n+\t\"fmt\"\n \t\"strings\"\n \n \t\"github.com/jesseduffield/gocui\"\n \t\"github.com/jesseduffield/lazygit/pkg/commands/git_commands\"\n \t\"github.com/jesseduffield/lazygit/pkg/commands/patch\"\n+\t\"github.com/jesseduffield/lazygit/pkg/gui/keybindings\"\n \t\"github.com/jesseduffield/lazygit/pkg/gui/types\"\n )\n \n@@ -185,10 +187,20 @@ func (self *StagingController) TogglePanel() error {\n }\n \n func (self *StagingController) ToggleStaged() error {\n+\tif self.c.AppState.DiffContextSize == 0 {\n+\t\treturn fmt.Errorf(self.c.Tr.Actions.NotEnoughContextToStage,\n+\t\t\tkeybindings.Label(self.c.UserConfig().Keybinding.Universal.IncreaseContextInDiffView))\n+\t}\n+\n \treturn self.applySelectionAndRefresh(self.staged)\n }\n \n func (self *StagingController) DiscardSelection() error {\n+\tif self.c.AppState.DiffContextSize == 0 {\n+\t\treturn fmt.Errorf(self.c.Tr.Actions.NotEnoughContextToDiscard,\n+\t\t\tkeybindings.Label(self.c.UserConfig().Keybinding.Universal.IncreaseContextInDiffView))\n+\t}\n+\n \treset := func() error { return self.applySelectionAndRefresh(true) }\n \n \tif !self.staged && !self.c.UserConfig().Gui.SkipDiscardChangeWarning {\ndiff --git a/pkg/i18n/english.go b/pkg/i18n/english.go\nindex 28e12635626..f92c4099354 100644\n--- a/pkg/i18n/english.go\n+++ b/pkg/i18n/english.go\n@@ -926,6 +926,8 @@ type Actions struct {\n \tUnstageFile                       string\n \tUnstageAllFiles                   string\n \tStageAllFiles                     string\n+\tNotEnoughContextToStage           string\n+\tNotEnoughContextToDiscard         string\n \tIgnoreExcludeFile                 string\n \tIgnoreFileErr                     string\n \tExcludeFile                       string\n@@ -1913,6 +1915,8 @@ func EnglishTranslationSet() *TranslationSet {\n \t\t\tUnstageFile:                     \"Unstage file\",\n \t\t\tUnstageAllFiles:                 \"Unstage all files\",\n \t\t\tStageAllFiles:                   \"Stage all files\",\n+\t\t\tNotEnoughContextToStage:         \"Staging or unstaging changes is not possible with a diff context size of 0. Increase the context using '%s'.\",\n+\t\t\tNotEnoughContextToDiscard:       \"Discarding changes is not possible with a diff context size of 0. Increase the context using '%s'.\",\n \t\t\tIgnoreExcludeFile:               \"Ignore or exclude file\",\n \t\t\tIgnoreFileErr:                   \"Cannot ignore .gitignore\",\n \t\t\tExcludeFile:                     \"Exclude file\",\n", "instance_id": "jesseduffield__lazygit-4235", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear and provides a detailed description of the bug related to interactive staging in the `lazygit` tool. It includes steps to reproduce the issue, version information, screenshots, and additional context about the environment (OS, terminal emulator, shell). The user also demonstrates the expected behavior using `git add -p` and `git apply --cached`, which helps in understanding the desired outcome. However, there are minor ambiguities: the problem statement does not explicitly define what \"errors out\" means (e.g., does it crash, show an error message, or silently fail?). Additionally, it lacks clarity on whether this issue occurs consistently across different environments or is specific to the reported setup (MacOS, Ghostty, fish shell). Edge cases or constraints (e.g., file size, diff complexity) are not mentioned. Overall, while the problem is mostly clear, these minor missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following reasons based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are limited to two files (`staging_controller.go` and `english.go`) and involve small, localized modifications. In `staging_controller.go`, the change adds a check for `DiffContextSize` before staging or discarding changes, along with appropriate error messages. In `english.go`, new translation strings are added for the error messages. The changes do not impact the broader architecture of the system or require understanding complex interactions between modules. The amount of code change is minimal, with only a few lines added.\n\n2. **Number of Technical Concepts:** The solution requires basic knowledge of Go programming, including error handling (`fmt.Errorf`) and working with configuration or state variables (`AppState.DiffContextSize`). It also involves familiarity with the `lazygit` codebase's structure (e.g., controllers, internationalization) and keybinding labels. However, these concepts are relatively straightforward for someone with moderate experience in Go or similar projects. No advanced algorithms, design patterns, or domain-specific knowledge (beyond basic Git operations) are required.\n\n3. **Potential Edge Cases and Error Handling:** The problem and code changes address a specific edge case: when the diff context size is 0, staging or discarding changes should not be allowed. The solution adds error handling to prevent these operations and provides user feedback via translated error messages. The edge case is simple and does not introduce significant complexity. The problem statement does not mention other edge cases (e.g., large files, complex diffs), and the code changes do not address additional error conditions beyond the context size check.\n\n4. **Overall Assessment:** The task involves understanding a small part of the `lazygit` codebase and making simple modifications to add a validation check and corresponding error messages. It does not require deep architectural changes or advanced technical expertise. The primary challenge lies in identifying the root cause of the interactive staging failure (which is somewhat mitigated by the provided reproduction steps and patch comparison) and ensuring the error message is user-friendly. Given the limited scope and straightforward concepts, I assign a difficulty score of 0.30, reflecting an easy problem that requires minimal effort beyond basic logic and code modification.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Line wrap for diff/staging mode (or hunk navigation in files mode)\nHi,\r\nI would find it very useful with a line wrapping key binding/toggle/config param to get the full line content overview in that mode.\r\n\r\nMotivation:\r\nFor me the motivation is that I have files where I am storing text files with very long line content where I store paragraph long text in a single line on many places. Making a modification to such paragraph/line will not too rarely put my changes out of initial view when reviewing and commenting my changes for the commit text. Even so in fullscreen mode for the \"Staged changes\" pane.\r\nI know I have the options to scroll sideways in diff/staging mode or use the files view instead. The reasons I find them insufficient is that scrolling long lines is tedious, and in the files view there is no binding for jumping to next/previous hunk.\r\nIf there was a binding for next/previous hunk I would think that it would also suffice as a solution for me. I also tried changing the git->paging->pager to ydiff, but could see that it does not affect the diff/staging mode, only the files mode.\r\nIf this is enough motivation for you to implement it though is of course up to you, but would be greatly appreciated! :) \n", "patch": "diff --git a/docs/Config.md b/docs/Config.md\nindex e791c257955..c64e56d2edf 100644\n--- a/docs/Config.md\n+++ b/docs/Config.md\n@@ -87,6 +87,11 @@ gui:\n   # - 'top': split the window vertically (side panel on top, main view below)\n   enlargedSideViewLocation: left\n \n+  # If true, wrap lines in the staging view to the width of the view. This\n+  # makes it much easier to work with diffs that have long lines, e.g.\n+  # paragraphs of markdown text.\n+  wrapLinesInStagingView: true\n+\n   # One of 'auto' (default) | 'en' | 'zh-CN' | 'zh-TW' | 'pl' | 'nl' | 'ja' | 'ko' | 'ru'\n   language: auto\n \ndiff --git a/go.mod b/go.mod\nindex 0db99c641e0..d89d03aeab0 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -16,7 +16,7 @@ require (\n \tgithub.com/integrii/flaggy v1.4.0\n \tgithub.com/jesseduffield/generics v0.0.0-20220320043834-727e535cbe68\n \tgithub.com/jesseduffield/go-git/v5 v5.1.2-0.20221018185014-fdd53fef665d\n-\tgithub.com/jesseduffield/gocui v0.3.1-0.20241201093724-68c437bbd543\n+\tgithub.com/jesseduffield/gocui v0.3.1-0.20241223111608-9967d0e928a0\n \tgithub.com/jesseduffield/kill v0.0.0-20220618033138-bfbe04675d10\n \tgithub.com/jesseduffield/lazycore v0.0.0-20221012050358-03d2e40243c5\n \tgithub.com/jesseduffield/minimal/gitignore v0.3.3-0.20211018110810-9cde264e6b1e\n@@ -38,7 +38,7 @@ require (\n \tgithub.com/stretchr/testify v1.8.1\n \tgithub.com/xo/terminfo v0.0.0-20210125001918-ca9a967f8778\n \tgolang.org/x/exp v0.0.0-20220318154914-8dddf5d87bd8\n-\tgolang.org/x/sync v0.9.0\n+\tgolang.org/x/sync v0.10.0\n \tgopkg.in/ozeidan/fuzzy-patricia.v3 v3.0.0\n \tgopkg.in/yaml.v3 v3.0.1\n )\n@@ -75,8 +75,8 @@ require (\n \tgithub.com/xanzy/ssh-agent v0.2.1 // indirect\n \tgolang.org/x/crypto v0.0.0-20220722155217-630584e8d5aa // indirect\n \tgolang.org/x/net v0.7.0 // indirect\n-\tgolang.org/x/sys v0.27.0 // indirect\n-\tgolang.org/x/term v0.26.0 // indirect\n-\tgolang.org/x/text v0.20.0 // indirect\n+\tgolang.org/x/sys v0.28.0 // indirect\n+\tgolang.org/x/term v0.27.0 // indirect\n+\tgolang.org/x/text v0.21.0 // indirect\n \tgopkg.in/warnings.v0 v0.1.2 // indirect\n )\ndiff --git a/go.sum b/go.sum\nindex 23f370bac79..5041d10cc1b 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -188,8 +188,8 @@ github.com/jesseduffield/generics v0.0.0-20220320043834-727e535cbe68 h1:EQP2Tv8T\n github.com/jesseduffield/generics v0.0.0-20220320043834-727e535cbe68/go.mod h1:+LLj9/WUPAP8LqCchs7P+7X0R98HiFujVFANdNaxhGk=\n github.com/jesseduffield/go-git/v5 v5.1.2-0.20221018185014-fdd53fef665d h1:bO+OmbreIv91rCe8NmscRwhFSqkDJtzWCPV4Y+SQuXE=\n github.com/jesseduffield/go-git/v5 v5.1.2-0.20221018185014-fdd53fef665d/go.mod h1:nGNEErzf+NRznT+N2SWqmHnDnF9aLgANB1CUNEan09o=\n-github.com/jesseduffield/gocui v0.3.1-0.20241201093724-68c437bbd543 h1:mizrpmhRsYX6G7pqaLH+Rg9zdQ05S7xYVHTvSuBSX70=\n-github.com/jesseduffield/gocui v0.3.1-0.20241201093724-68c437bbd543/go.mod h1:XtEbqCbn45keRXEu+OMZkjN5gw6AEob59afsgHjokZ8=\n+github.com/jesseduffield/gocui v0.3.1-0.20241223111608-9967d0e928a0 h1:R29+E15wHqTDBfZxmzCLu0x34j5ljsXWT/DhR+2YiOU=\n+github.com/jesseduffield/gocui v0.3.1-0.20241223111608-9967d0e928a0/go.mod h1:XtEbqCbn45keRXEu+OMZkjN5gw6AEob59afsgHjokZ8=\n github.com/jesseduffield/kill v0.0.0-20220618033138-bfbe04675d10 h1:jmpr7KpX2+2GRiE91zTgfq49QvgiqB0nbmlwZ8UnOx0=\n github.com/jesseduffield/kill v0.0.0-20220618033138-bfbe04675d10/go.mod h1:aA97kHeNA+sj2Hbki0pvLslmE4CbDyhBeSSTUUnOuVo=\n github.com/jesseduffield/lazycore v0.0.0-20221012050358-03d2e40243c5 h1:CDuQmfOjAtb1Gms6a1p5L2P8RhbLUq5t8aL7PiQd2uY=\n@@ -424,8 +424,8 @@ golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJ\n golang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.9.0 h1:fEo0HyrW1GIgZdpbhCRO0PkJajUS5H9IFUztCgEo2jQ=\n-golang.org/x/sync v0.9.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n+golang.org/x/sync v0.10.0 h1:3NQrjDixjgGwUOCaF8w2+VYHv0Ve/vGYSbdkTa98gmQ=\n+golang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n golang.org/x/sys v0.0.0-20170407050850-f3918c30c5c2/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n@@ -475,14 +475,14 @@ golang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBc\n golang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.17.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n-golang.org/x/sys v0.27.0 h1:wBqf8DvsY9Y/2P8gAfPDEYNuS30J4lPHJxXSb/nJZ+s=\n-golang.org/x/sys v0.27.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n+golang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=\n+golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\n golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\n golang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=\n golang.org/x/term v0.17.0/go.mod h1:lLRBjIVuehSbZlaOtGMbcMncT+aqLLLmKrsjNrUguwk=\n-golang.org/x/term v0.26.0 h1:WEQa6V3Gja/BhNxg540hBip/kkaYtRg3cxg4oXSw4AU=\n-golang.org/x/term v0.26.0/go.mod h1:Si5m1o57C5nBNQo5z1iq+XDijt21BDBDp2bK0QI8e3E=\n+golang.org/x/term v0.27.0 h1:WP60Sv1nlK1T6SupCHbXzSaN0b9wUmsPoRS9b61A23Q=\n+golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=\n golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n@@ -493,8 +493,8 @@ golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\n golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\n golang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\n-golang.org/x/text v0.20.0 h1:gK/Kv2otX8gz+wn7Rmb3vT96ZwuoxnQlY+HlJVj7Qug=\n-golang.org/x/text v0.20.0/go.mod h1:D4IsuqiFMhST5bX19pQ9ikHC2GsaKyk/oF+pn3ducp4=\n+golang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\n+golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\n golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\n golang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ndiff --git a/pkg/config/user_config.go b/pkg/config/user_config.go\nindex 1148bb94721..36dc02a68a7 100644\n--- a/pkg/config/user_config.go\n+++ b/pkg/config/user_config.go\n@@ -91,6 +91,10 @@ type GuiConfig struct {\n \t// - 'left': split the window horizontally (side panel on the left, main view on the right)\n \t// - 'top': split the window vertically (side panel on top, main view below)\n \tEnlargedSideViewLocation string `yaml:\"enlargedSideViewLocation\"`\n+\t// If true, wrap lines in the staging view to the width of the view. This\n+\t// makes it much easier to work with diffs that have long lines, e.g.\n+\t// paragraphs of markdown text.\n+\tWrapLinesInStagingView bool `yaml:\"wrapLinesInStagingView\"`\n \t// One of 'auto' (default) | 'en' | 'zh-CN' | 'zh-TW' | 'pl' | 'nl' | 'ja' | 'ko' | 'ru'\n \tLanguage string `yaml:\"language\" jsonschema:\"enum=auto,enum=en,enum=zh-TW,enum=zh-CN,enum=pl,enum=nl,enum=ja,enum=ko,enum=ru\"`\n \t// Format used when displaying time e.g. commit time.\n@@ -692,6 +696,7 @@ func GetDefaultConfig() *UserConfig {\n \t\t\tExpandedSidePanelWeight:  2,\n \t\t\tMainPanelSplitMode:       \"flexible\",\n \t\t\tEnlargedSideViewLocation: \"left\",\n+\t\t\tWrapLinesInStagingView:   true,\n \t\t\tLanguage:                 \"auto\",\n \t\t\tTimeFormat:               \"02 Jan 06\",\n \t\t\tShortTimeFormat:          time.Kitchen,\ndiff --git a/pkg/gui/context/patch_explorer_context.go b/pkg/gui/context/patch_explorer_context.go\nindex 46d82f5b4f6..eb79dce8667 100644\n--- a/pkg/gui/context/patch_explorer_context.go\n+++ b/pkg/gui/context/patch_explorer_context.go\n@@ -39,12 +39,13 @@ func NewPatchExplorerContext(\n \t\tmutex:                  &deadlock.Mutex{},\n \t\tgetIncludedLineIndices: getIncludedLineIndices,\n \t\tSimpleContext: NewSimpleContext(NewBaseContext(NewBaseContextOpts{\n-\t\t\tView:             view,\n-\t\t\tWindowName:       windowName,\n-\t\t\tKey:              key,\n-\t\t\tKind:             types.MAIN_CONTEXT,\n-\t\t\tFocusable:        true,\n-\t\t\tHighlightOnFocus: true,\n+\t\t\tView:                       view,\n+\t\t\tWindowName:                 windowName,\n+\t\t\tKey:                        key,\n+\t\t\tKind:                       types.MAIN_CONTEXT,\n+\t\t\tFocusable:                  true,\n+\t\t\tHighlightOnFocus:           true,\n+\t\t\tNeedsRerenderOnWidthChange: types.NEEDS_RERENDER_ON_WIDTH_CHANGE_WHEN_WIDTH_CHANGES,\n \t\t})),\n \t\tSearchTrait: NewSearchTrait(c),\n \t}\n@@ -58,6 +59,8 @@ func NewPatchExplorerContext(\n \t\t}),\n \t)\n \n+\tctx.SetHandleRenderFunc(ctx.OnViewWidthChanged)\n+\n \treturn ctx\n }\n \n@@ -106,13 +109,13 @@ func (self *PatchExplorerContext) FocusSelection() {\n \tstate := self.GetState()\n \tbufferHeight := view.InnerHeight()\n \t_, origin := view.Origin()\n-\tnumLines := view.LinesHeight()\n+\tnumLines := view.ViewLinesHeight()\n \n \tnewOriginY := state.CalculateOrigin(origin, bufferHeight, numLines)\n \n \tview.SetOriginY(newOriginY)\n \n-\tstartIdx, endIdx := state.SelectedRange()\n+\tstartIdx, endIdx := state.SelectedViewRange()\n \t// As far as the view is concerned, we are always selecting a range\n \tview.SetRangeSelectStart(startIdx)\n \tview.SetCursorY(endIdx - newOriginY)\n@@ -140,3 +143,11 @@ func (self *PatchExplorerContext) GetMutex() *deadlock.Mutex {\n func (self *PatchExplorerContext) ModelSearchResults(searchStr string, caseSensitive bool) []gocui.SearchPosition {\n \treturn nil\n }\n+\n+func (self *PatchExplorerContext) OnViewWidthChanged() {\n+\tif state := self.GetState(); state != nil {\n+\t\tstate.OnViewWidthChanged(self.GetView())\n+\t\tself.setContent()\n+\t\tself.RenderAndFocus()\n+\t}\n+}\ndiff --git a/pkg/gui/context/simple_context.go b/pkg/gui/context/simple_context.go\nindex d78db719075..579f975e625 100644\n--- a/pkg/gui/context/simple_context.go\n+++ b/pkg/gui/context/simple_context.go\n@@ -7,6 +7,7 @@ import (\n \n type SimpleContext struct {\n \t*BaseContext\n+\thandleRenderFunc func()\n }\n \n func NewSimpleContext(baseContext *BaseContext) *SimpleContext {\n@@ -54,6 +55,13 @@ func (self *SimpleContext) HandleFocusLost(opts types.OnFocusLostOpts) {\n }\n \n func (self *SimpleContext) HandleRender() {\n+\tif self.handleRenderFunc != nil {\n+\t\tself.handleRenderFunc()\n+\t}\n+}\n+\n+func (self *SimpleContext) SetHandleRenderFunc(f func()) {\n+\tself.handleRenderFunc = f\n }\n \n func (self *SimpleContext) HandleRenderToMain() {\ndiff --git a/pkg/gui/controllers/helpers/confirmation_helper.go b/pkg/gui/controllers/helpers/confirmation_helper.go\nindex 7b0b8ddb23b..f7f6f8720c4 100644\n--- a/pkg/gui/controllers/helpers/confirmation_helper.go\n+++ b/pkg/gui/controllers/helpers/confirmation_helper.go\n@@ -3,12 +3,11 @@ package helpers\n import (\n \tgoContext \"context\"\n \t\"fmt\"\n-\t\"strings\"\n \n \t\"github.com/jesseduffield/lazygit/pkg/gui/style\"\n \t\"github.com/jesseduffield/lazygit/pkg/gui/types\"\n \t\"github.com/jesseduffield/lazygit/pkg/theme\"\n-\t\"github.com/mattn/go-runewidth\"\n+\t\"github.com/jesseduffield/lazygit/pkg/utils\"\n )\n \n type ConfirmationHelper struct {\n@@ -57,61 +56,9 @@ func (self *ConfirmationHelper) DeactivateConfirmationPrompt() {\n \tself.clearConfirmationViewKeyBindings()\n }\n \n-// Temporary hack: we're just duplicating the logic in `gocui.lineWrap`\n func getMessageHeight(wrap bool, message string, width int) int {\n-\treturn len(wrapMessageToWidth(wrap, message, width))\n-}\n-\n-func wrapMessageToWidth(wrap bool, message string, width int) []string {\n-\tlines := strings.Split(message, \"\\n\")\n-\tif !wrap {\n-\t\treturn lines\n-\t}\n-\n-\twrappedLines := make([]string, 0, len(lines))\n-\n-\tfor _, line := range lines {\n-\t\tn := 0\n-\t\toffset := 0\n-\t\tlastWhitespaceIndex := -1\n-\t\tfor i, currChr := range line {\n-\t\t\trw := runewidth.RuneWidth(currChr)\n-\t\t\tn += rw\n-\n-\t\t\tif n > width {\n-\t\t\t\tif currChr == ' ' {\n-\t\t\t\t\twrappedLines = append(wrappedLines, line[offset:i])\n-\t\t\t\t\toffset = i + 1\n-\t\t\t\t\tn = 0\n-\t\t\t\t} else if currChr == '-' {\n-\t\t\t\t\twrappedLines = append(wrappedLines, line[offset:i])\n-\t\t\t\t\toffset = i\n-\t\t\t\t\tn = rw\n-\t\t\t\t} else if lastWhitespaceIndex != -1 && lastWhitespaceIndex+1 != i {\n-\t\t\t\t\tif line[lastWhitespaceIndex] == '-' {\n-\t\t\t\t\t\twrappedLines = append(wrappedLines, line[offset:lastWhitespaceIndex+1])\n-\t\t\t\t\t\toffset = lastWhitespaceIndex + 1\n-\t\t\t\t\t\tn = i - lastWhitespaceIndex\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\twrappedLines = append(wrappedLines, line[offset:lastWhitespaceIndex])\n-\t\t\t\t\t\toffset = lastWhitespaceIndex + 1\n-\t\t\t\t\t\tn = i - lastWhitespaceIndex + 1\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\twrappedLines = append(wrappedLines, line[offset:i])\n-\t\t\t\t\toffset = i\n-\t\t\t\t\tn = rw\n-\t\t\t\t}\n-\t\t\t\tlastWhitespaceIndex = -1\n-\t\t\t} else if currChr == ' ' || currChr == '-' {\n-\t\t\t\tlastWhitespaceIndex = i\n-\t\t\t}\n-\t\t}\n-\n-\t\twrappedLines = append(wrappedLines, line[offset:])\n-\t}\n-\n-\treturn wrappedLines\n+\twrappedLines, _, _ := utils.WrapViewLinesToWidth(wrap, message, width)\n+\treturn len(wrappedLines)\n }\n \n func (self *ConfirmationHelper) getPopupPanelDimensionsForContentHeight(panelWidth, contentHeight int, parentPopupContext types.Context) (int, int, int, int) {\n@@ -329,7 +276,7 @@ func (self *ConfirmationHelper) layoutMenuPrompt(contentWidth int) int {\n \tvar promptLines []string\n \tprompt := self.c.Contexts().Menu.GetPrompt()\n \tif len(prompt) > 0 {\n-\t\tpromptLines = wrapMessageToWidth(true, prompt, contentWidth)\n+\t\tpromptLines, _, _ = utils.WrapViewLinesToWidth(true, prompt, contentWidth)\n \t\tpromptLines = append(promptLines, \"\")\n \t}\n \tself.c.Contexts().Menu.SetPromptLines(promptLines)\ndiff --git a/pkg/gui/controllers/helpers/patch_building_helper.go b/pkg/gui/controllers/helpers/patch_building_helper.go\nindex 63744167f94..cde561fbcf3 100644\n--- a/pkg/gui/controllers/helpers/patch_building_helper.go\n+++ b/pkg/gui/controllers/helpers/patch_building_helper.go\n@@ -91,7 +91,7 @@ func (self *PatchBuildingHelper) RefreshPatchBuildingPanel(opts types.OnFocusOpt\n \n \toldState := context.GetState()\n \n-\tstate := patch_exploring.NewState(diff, selectedLineIdx, oldState, self.c.Log)\n+\tstate := patch_exploring.NewState(diff, selectedLineIdx, context.GetView(), oldState)\n \tcontext.SetState(state)\n \tif state == nil {\n \t\tself.Escape()\ndiff --git a/pkg/gui/controllers/helpers/staging_helper.go b/pkg/gui/controllers/helpers/staging_helper.go\nindex a6b8705179c..69760a1939a 100644\n--- a/pkg/gui/controllers/helpers/staging_helper.go\n+++ b/pkg/gui/controllers/helpers/staging_helper.go\n@@ -63,11 +63,11 @@ func (self *StagingHelper) RefreshStagingPanel(focusOpts types.OnFocusOpts) {\n \tsecondaryContext.GetMutex().Lock()\n \n \tmainContext.SetState(\n-\t\tpatch_exploring.NewState(mainDiff, mainSelectedLineIdx, mainContext.GetState(), self.c.Log),\n+\t\tpatch_exploring.NewState(mainDiff, mainSelectedLineIdx, mainContext.GetView(), mainContext.GetState()),\n \t)\n \n \tsecondaryContext.SetState(\n-\t\tpatch_exploring.NewState(secondaryDiff, secondarySelectedLineIdx, secondaryContext.GetState(), self.c.Log),\n+\t\tpatch_exploring.NewState(secondaryDiff, secondarySelectedLineIdx, secondaryContext.GetView(), secondaryContext.GetState()),\n \t)\n \n \tmainState := mainContext.GetState()\ndiff --git a/pkg/gui/controllers/patch_building_controller.go b/pkg/gui/controllers/patch_building_controller.go\nindex ea181cd054d..7bc0ffb8320 100644\n--- a/pkg/gui/controllers/patch_building_controller.go\n+++ b/pkg/gui/controllers/patch_building_controller.go\n@@ -65,7 +65,7 @@ func (self *PatchBuildingController) GetMouseKeybindings(opts types.KeybindingsO\n func (self *PatchBuildingController) GetOnFocus() func(types.OnFocusOpts) {\n \treturn func(opts types.OnFocusOpts) {\n \t\t// no need to change wrap on the secondary view because it can't be interacted with\n-\t\tself.c.Views().PatchBuilding.Wrap = false\n+\t\tself.c.Views().PatchBuilding.Wrap = self.c.UserConfig().Gui.WrapLinesInStagingView\n \n \t\tself.c.Helpers().PatchBuilding.RefreshPatchBuildingPanel(opts)\n \t}\n@@ -73,6 +73,8 @@ func (self *PatchBuildingController) GetOnFocus() func(types.OnFocusOpts) {\n \n func (self *PatchBuildingController) GetOnFocusLost() func(types.OnFocusLostOpts) {\n \treturn func(opts types.OnFocusLostOpts) {\n+\t\tself.context().SetState(nil)\n+\n \t\tself.c.Views().PatchBuilding.Wrap = true\n \n \t\tif self.c.Git().Patch.PatchBuilder.IsEmpty() {\n@@ -134,13 +136,13 @@ func (self *PatchBuildingController) toggleSelection() error {\n \tif err != nil {\n \t\treturn err\n \t}\n-\tcurrentLineIsStaged := lo.Contains(includedLineIndices, state.GetSelectedLineIdx())\n+\tcurrentLineIsStaged := lo.Contains(includedLineIndices, state.GetSelectedPatchLineIdx())\n \tif currentLineIsStaged {\n \t\ttoggleFunc = self.c.Git().Patch.PatchBuilder.RemoveFileLineRange\n \t}\n \n \t// add range of lines to those set for the file\n-\tfirstLineIdx, lastLineIdx := state.SelectedRange()\n+\tfirstLineIdx, lastLineIdx := state.SelectedPatchRange()\n \n \tif err := toggleFunc(filename, firstLineIdx, lastLineIdx); err != nil {\n \t\t// might actually want to return an error here\ndiff --git a/pkg/gui/controllers/patch_explorer_controller.go b/pkg/gui/controllers/patch_explorer_controller.go\nindex d84ff48a08b..315d392ec8b 100644\n--- a/pkg/gui/controllers/patch_explorer_controller.go\n+++ b/pkg/gui/controllers/patch_explorer_controller.go\n@@ -170,9 +170,9 @@ func (self *PatchExplorerController) GetMouseKeybindings(opts types.KeybindingsO\n }\n \n func (self *PatchExplorerController) HandlePrevLine() error {\n-\tbefore := self.context.GetState().GetSelectedLineIdx()\n+\tbefore := self.context.GetState().GetSelectedViewLineIdx()\n \tself.context.GetState().CycleSelection(false)\n-\tafter := self.context.GetState().GetSelectedLineIdx()\n+\tafter := self.context.GetState().GetSelectedViewLineIdx()\n \n \tif self.context.GetState().SelectingLine() {\n \t\tcheckScrollUp(self.context.GetViewTrait(), self.c.UserConfig(), before, after)\n@@ -182,9 +182,9 @@ func (self *PatchExplorerController) HandlePrevLine() error {\n }\n \n func (self *PatchExplorerController) HandleNextLine() error {\n-\tbefore := self.context.GetState().GetSelectedLineIdx()\n+\tbefore := self.context.GetState().GetSelectedViewLineIdx()\n \tself.context.GetState().CycleSelection(true)\n-\tafter := self.context.GetState().GetSelectedLineIdx()\n+\tafter := self.context.GetState().GetSelectedViewLineIdx()\n \n \tif self.context.GetState().SelectingLine() {\n \t\tcheckScrollDown(self.context.GetViewTrait(), self.c.UserConfig(), before, after)\ndiff --git a/pkg/gui/controllers/staging_controller.go b/pkg/gui/controllers/staging_controller.go\nindex 4ae9a946b45..fbcbc049bdd 100644\n--- a/pkg/gui/controllers/staging_controller.go\n+++ b/pkg/gui/controllers/staging_controller.go\n@@ -118,8 +118,9 @@ func (self *StagingController) GetMouseKeybindings(opts types.KeybindingsOpts) [\n \n func (self *StagingController) GetOnFocus() func(types.OnFocusOpts) {\n \treturn func(opts types.OnFocusOpts) {\n-\t\tself.c.Views().Staging.Wrap = false\n-\t\tself.c.Views().StagingSecondary.Wrap = false\n+\t\twrap := self.c.UserConfig().Gui.WrapLinesInStagingView\n+\t\tself.c.Views().Staging.Wrap = wrap\n+\t\tself.c.Views().StagingSecondary.Wrap = wrap\n \n \t\tself.c.Helpers().Staging.RefreshStagingPanel(opts)\n \t}\n@@ -220,7 +221,7 @@ func (self *StagingController) applySelection(reverse bool) error {\n \t\treturn nil\n \t}\n \n-\tfirstLineIdx, lastLineIdx := state.SelectedRange()\n+\tfirstLineIdx, lastLineIdx := state.SelectedPatchRange()\n \tpatchToApply := patch.\n \t\tParse(state.GetDiff()).\n \t\tTransform(patch.TransformOpts{\n@@ -249,7 +250,7 @@ func (self *StagingController) applySelection(reverse bool) error {\n \t}\n \n \tif state.SelectingRange() {\n-\t\tfirstLine, _ := state.SelectedRange()\n+\t\tfirstLine, _ := state.SelectedViewRange()\n \t\tstate.SelectLine(firstLine)\n \t}\n \n@@ -290,7 +291,7 @@ func (self *StagingController) editHunk() error {\n \t}\n \n \tlineOffset := 3\n-\tlineIdxInHunk := state.GetSelectedLineIdx() - hunkStartIdx\n+\tlineIdxInHunk := state.GetSelectedPatchLineIdx() - hunkStartIdx\n \tif err := self.c.Helpers().Files.EditFileAtLineAndWait(patchFilepath, lineIdxInHunk+lineOffset); err != nil {\n \t\treturn err\n \t}\ndiff --git a/pkg/gui/patch_exploring/state.go b/pkg/gui/patch_exploring/state.go\nindex 4c20b7a51b4..c10807c8c07 100644\n--- a/pkg/gui/patch_exploring/state.go\n+++ b/pkg/gui/patch_exploring/state.go\n@@ -1,15 +1,19 @@\n package patch_exploring\n \n import (\n+\t\"strings\"\n+\n \t\"github.com/jesseduffield/generics/set\"\n+\t\"github.com/jesseduffield/gocui\"\n \t\"github.com/jesseduffield/lazygit/pkg/commands/patch\"\n-\t\"github.com/sirupsen/logrus\"\n+\t\"github.com/jesseduffield/lazygit/pkg/utils\"\n )\n \n // State represents the current state of the patch explorer context i.e. when\n // you're staging a file or you're building a patch from an existing commit\n // this struct holds the info about the diff you're interacting with and what's currently selected.\n type State struct {\n+\t// These are in terms of view lines (wrapped), not patch lines\n \tselectedLineIdx   int\n \trangeStartLineIdx int\n \t// If a range is sticky, it means we expand the range when we move up or down.\n@@ -18,6 +22,11 @@ type State struct {\n \tdiff          string\n \tpatch         *patch.Patch\n \tselectMode    selectMode\n+\n+\t// Array of indices of the wrapped lines indexed by a patch line index\n+\tviewLineIndices []int\n+\t// Array of indices of the original patch lines indexed by a wrapped view line index\n+\tpatchLineIndices []int\n }\n \n // these represent what select mode we're in\n@@ -29,7 +38,7 @@ const (\n \tHUNK\n )\n \n-func NewState(diff string, selectedLineIdx int, oldState *State, log *logrus.Entry) *State {\n+func NewState(diff string, selectedLineIdx int, view *gocui.View, oldState *State) *State {\n \tif oldState != nil && diff == oldState.diff && selectedLineIdx == -1 {\n \t\t// if we're here then we can return the old state. If selectedLineIdx was not -1\n \t\t// then that would mean we were trying to click and potentiall drag a range, which\n@@ -43,6 +52,8 @@ func NewState(diff string, selectedLineIdx int, oldState *State, log *logrus.Ent\n \t\treturn nil\n \t}\n \n+\tviewLineIndices, patchLineIndices := wrapPatchLines(diff, view)\n+\n \trangeStartLineIdx := 0\n \tif oldState != nil {\n \t\trangeStartLineIdx = oldState.rangeStartLineIdx\n@@ -51,6 +62,10 @@ func NewState(diff string, selectedLineIdx int, oldState *State, log *logrus.Ent\n \tselectMode := LINE\n \t// if we have clicked from the outside to focus the main view we'll pass in a non-negative line index so that we can instantly select that line\n \tif selectedLineIdx >= 0 {\n+\t\t// Clamp to the number of wrapped view lines; index might be out of\n+\t\t// bounds if a custom pager is being used which produces more lines\n+\t\tselectedLineIdx = min(selectedLineIdx, len(viewLineIndices)-1)\n+\n \t\tselectMode = RANGE\n \t\trangeStartLineIdx = selectedLineIdx\n \t} else if oldState != nil {\n@@ -58,9 +73,9 @@ func NewState(diff string, selectedLineIdx int, oldState *State, log *logrus.Ent\n \t\tif oldState.selectMode == HUNK {\n \t\t\tselectMode = HUNK\n \t\t}\n-\t\tselectedLineIdx = patch.GetNextChangeIdx(oldState.selectedLineIdx)\n+\t\tselectedLineIdx = viewLineIndices[patch.GetNextChangeIdx(oldState.patchLineIndices[oldState.selectedLineIdx])]\n \t} else {\n-\t\tselectedLineIdx = patch.GetNextChangeIdx(0)\n+\t\tselectedLineIdx = viewLineIndices[patch.GetNextChangeIdx(0)]\n \t}\n \n \treturn &State{\n@@ -70,10 +85,33 @@ func NewState(diff string, selectedLineIdx int, oldState *State, log *logrus.Ent\n \t\trangeStartLineIdx: rangeStartLineIdx,\n \t\trangeIsSticky:     false,\n \t\tdiff:              diff,\n+\t\tviewLineIndices:   viewLineIndices,\n+\t\tpatchLineIndices:  patchLineIndices,\n \t}\n }\n \n-func (s *State) GetSelectedLineIdx() int {\n+func (s *State) OnViewWidthChanged(view *gocui.View) {\n+\tif !view.Wrap {\n+\t\treturn\n+\t}\n+\n+\tselectedPatchLineIdx := s.patchLineIndices[s.selectedLineIdx]\n+\tvar rangeStartPatchLineIdx int\n+\tif s.selectMode == RANGE {\n+\t\trangeStartPatchLineIdx = s.patchLineIndices[s.rangeStartLineIdx]\n+\t}\n+\ts.viewLineIndices, s.patchLineIndices = wrapPatchLines(s.diff, view)\n+\ts.selectedLineIdx = s.viewLineIndices[selectedPatchLineIdx]\n+\tif s.selectMode == RANGE {\n+\t\ts.rangeStartLineIdx = s.viewLineIndices[rangeStartPatchLineIdx]\n+\t}\n+}\n+\n+func (s *State) GetSelectedPatchLineIdx() int {\n+\treturn s.patchLineIndices[s.selectedLineIdx]\n+}\n+\n+func (s *State) GetSelectedViewLineIdx() int {\n \treturn s.selectedLineIdx\n }\n \n@@ -143,8 +181,8 @@ func (s *State) SelectLine(newSelectedLineIdx int) {\n func (s *State) selectLineWithoutRangeCheck(newSelectedLineIdx int) {\n \tif newSelectedLineIdx < 0 {\n \t\tnewSelectedLineIdx = 0\n-\t} else if newSelectedLineIdx > s.patch.LineCount()-1 {\n-\t\tnewSelectedLineIdx = s.patch.LineCount() - 1\n+\t} else if newSelectedLineIdx > len(s.patchLineIndices)-1 {\n+\t\tnewSelectedLineIdx = len(s.patchLineIndices) - 1\n \t}\n \n \ts.selectedLineIdx = newSelectedLineIdx\n@@ -178,12 +216,12 @@ func (s *State) CycleHunk(forward bool) {\n \t\tchange = -1\n \t}\n \n-\thunkIdx := s.patch.HunkContainingLine(s.selectedLineIdx)\n+\thunkIdx := s.patch.HunkContainingLine(s.patchLineIndices[s.selectedLineIdx])\n \tif hunkIdx != -1 {\n \t\tnewHunkIdx := hunkIdx + change\n \t\tif newHunkIdx >= 0 && newHunkIdx < s.patch.HunkCount() {\n \t\t\tstart := s.patch.HunkStartIdx(newHunkIdx)\n-\t\t\ts.selectedLineIdx = s.patch.GetNextChangeIdx(start)\n+\t\t\ts.selectedLineIdx = s.viewLineIndices[s.patch.GetNextChangeIdx(start)]\n \t\t}\n \t}\n }\n@@ -216,16 +254,17 @@ func (s *State) CycleRange(forward bool) {\n \n // returns first and last patch line index of current hunk\n func (s *State) CurrentHunkBounds() (int, int) {\n-\thunkIdx := s.patch.HunkContainingLine(s.selectedLineIdx)\n+\thunkIdx := s.patch.HunkContainingLine(s.patchLineIndices[s.selectedLineIdx])\n \tstart := s.patch.HunkStartIdx(hunkIdx)\n \tend := s.patch.HunkEndIdx(hunkIdx)\n \treturn start, end\n }\n \n-func (s *State) SelectedRange() (int, int) {\n+func (s *State) SelectedViewRange() (int, int) {\n \tswitch s.selectMode {\n \tcase HUNK:\n-\t\treturn s.CurrentHunkBounds()\n+\t\tstart, end := s.CurrentHunkBounds()\n+\t\treturn s.viewLineIndices[start], s.viewLineIndices[end]\n \tcase RANGE:\n \t\tif s.rangeStartLineIdx > s.selectedLineIdx {\n \t\t\treturn s.selectedLineIdx, s.rangeStartLineIdx\n@@ -240,8 +279,13 @@ func (s *State) SelectedRange() (int, int) {\n \t}\n }\n \n+func (s *State) SelectedPatchRange() (int, int) {\n+\tstart, end := s.SelectedViewRange()\n+\treturn s.patchLineIndices[start], s.patchLineIndices[end]\n+}\n+\n func (s *State) CurrentLineNumber() int {\n-\treturn s.patch.LineNumberOfLine(s.selectedLineIdx)\n+\treturn s.patch.LineNumberOfLine(s.patchLineIndices[s.selectedLineIdx])\n }\n \n func (s *State) AdjustSelectedLineIdx(change int) {\n@@ -257,13 +301,13 @@ func (s *State) RenderForLineIndices(includedLineIndices []int) string {\n }\n \n func (s *State) PlainRenderSelected() string {\n-\tfirstLineIdx, lastLineIdx := s.SelectedRange()\n+\tfirstLineIdx, lastLineIdx := s.SelectedPatchRange()\n \treturn s.patch.FormatRangePlain(firstLineIdx, lastLineIdx)\n }\n \n func (s *State) SelectBottom() {\n \ts.DismissHunkSelectMode()\n-\ts.SelectLine(s.patch.LineCount() - 1)\n+\ts.SelectLine(len(s.patchLineIndices) - 1)\n }\n \n func (s *State) SelectTop() {\n@@ -272,15 +316,13 @@ func (s *State) SelectTop() {\n }\n \n func (s *State) CalculateOrigin(currentOrigin int, bufferHeight int, numLines int) int {\n-\tfirstLineIdx, lastLineIdx := s.SelectedRange()\n+\tfirstLineIdx, lastLineIdx := s.SelectedViewRange()\n \n-\treturn calculateOrigin(currentOrigin, bufferHeight, numLines, firstLineIdx, lastLineIdx, s.GetSelectedLineIdx(), s.selectMode)\n+\treturn calculateOrigin(currentOrigin, bufferHeight, numLines, firstLineIdx, lastLineIdx, s.GetSelectedViewLineIdx(), s.selectMode)\n }\n \n-func (s *State) RangeStartLineIdx() (int, bool) {\n-\tif s.selectMode == RANGE {\n-\t\treturn s.rangeStartLineIdx, true\n-\t}\n-\n-\treturn 0, false\n+func wrapPatchLines(diff string, view *gocui.View) ([]int, []int) {\n+\t_, viewLineIndices, patchLineIndices := utils.WrapViewLinesToWidth(\n+\t\tview.Wrap, strings.TrimSuffix(diff, \"\\n\"), view.InnerWidth())\n+\treturn viewLineIndices, patchLineIndices\n }\ndiff --git a/pkg/utils/lines.go b/pkg/utils/lines.go\nindex c70d02ffcef..197b779757e 100644\n--- a/pkg/utils/lines.go\n+++ b/pkg/utils/lines.go\n@@ -3,6 +3,8 @@ package utils\n import (\n \t\"bytes\"\n \t\"strings\"\n+\n+\t\"github.com/mattn/go-runewidth\"\n )\n \n // SplitLines takes a multiline string and splits it on newlines\n@@ -100,3 +102,81 @@ func ScanLinesAndTruncateWhenLongerThanBuffer(maxBufferSize int) func(data []byt\n \t\treturn 0, nil, nil\n \t}\n }\n+\n+// Wrap lines to a given width, and return:\n+// - the wrapped lines\n+// - the line indices of the wrapped lines, indexed by the original line indices\n+// - the line indices of the original lines, indexed by the wrapped line indices\n+// If wrap is false, the text is returned as is.\n+// This code needs to behave the same as `gocui.lineWrap` does.\n+func WrapViewLinesToWidth(wrap bool, text string, width int) ([]string, []int, []int) {\n+\tlines := strings.Split(text, \"\\n\")\n+\tif !wrap {\n+\t\tindices := make([]int, len(lines))\n+\t\tfor i := range lines {\n+\t\t\tindices[i] = i\n+\t\t}\n+\t\treturn lines, indices, indices\n+\t}\n+\n+\twrappedLines := make([]string, 0, len(lines))\n+\twrappedLineIndices := make([]int, 0, len(lines))\n+\toriginalLineIndices := make([]int, 0, len(lines))\n+\n+\tfor originalLineIdx, line := range lines {\n+\t\twrappedLineIndices = append(wrappedLineIndices, len(wrappedLines))\n+\n+\t\t// convert tabs to spaces\n+\t\tfor i := 0; i < len(line); i++ {\n+\t\t\tif line[i] == '\\t' {\n+\t\t\t\tnumSpaces := 4 - (i % 4)\n+\t\t\t\tline = line[:i] + \"    \"[:numSpaces] + line[i+1:]\n+\t\t\t\ti += numSpaces - 1\n+\t\t\t}\n+\t\t}\n+\n+\t\tappendWrappedLine := func(str string) {\n+\t\t\twrappedLines = append(wrappedLines, str)\n+\t\t\toriginalLineIndices = append(originalLineIndices, originalLineIdx)\n+\t\t}\n+\n+\t\tn := 0\n+\t\toffset := 0\n+\t\tlastWhitespaceIndex := -1\n+\t\tfor i, currChr := range line {\n+\t\t\trw := runewidth.RuneWidth(currChr)\n+\t\t\tn += rw\n+\n+\t\t\tif n > width {\n+\t\t\t\tif currChr == ' ' {\n+\t\t\t\t\tappendWrappedLine(line[offset:i])\n+\t\t\t\t\toffset = i + 1\n+\t\t\t\t\tn = 0\n+\t\t\t\t} else if currChr == '-' {\n+\t\t\t\t\tappendWrappedLine(line[offset:i])\n+\t\t\t\t\toffset = i\n+\t\t\t\t\tn = rw\n+\t\t\t\t} else if lastWhitespaceIndex != -1 {\n+\t\t\t\t\tif line[lastWhitespaceIndex] == '-' {\n+\t\t\t\t\t\tappendWrappedLine(line[offset : lastWhitespaceIndex+1])\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tappendWrappedLine(line[offset:lastWhitespaceIndex])\n+\t\t\t\t\t}\n+\t\t\t\t\toffset = lastWhitespaceIndex + 1\n+\t\t\t\t\tn = runewidth.StringWidth(line[offset : i+1])\n+\t\t\t\t} else {\n+\t\t\t\t\tappendWrappedLine(line[offset:i])\n+\t\t\t\t\toffset = i\n+\t\t\t\t\tn = rw\n+\t\t\t\t}\n+\t\t\t\tlastWhitespaceIndex = -1\n+\t\t\t} else if currChr == ' ' || currChr == '-' {\n+\t\t\t\tlastWhitespaceIndex = i\n+\t\t\t}\n+\t\t}\n+\n+\t\tappendWrappedLine(line[offset:])\n+\t}\n+\n+\treturn wrappedLines, wrappedLineIndices, originalLineIndices\n+}\ndiff --git a/schema/config.json b/schema/config.json\nindex ee5726740c1..ee6f37ca508 100644\n--- a/schema/config.json\n+++ b/schema/config.json\n@@ -96,6 +96,11 @@\n           \"description\": \"How the window is split when in half screen mode (i.e. after hitting '+' once).\\nPossible values:\\n- 'left': split the window horizontally (side panel on the left, main view on the right)\\n- 'top': split the window vertically (side panel on top, main view below)\",\n           \"default\": \"left\"\n         },\n+        \"wrapLinesInStagingView\": {\n+          \"type\": \"boolean\",\n+          \"description\": \"If true, wrap lines in the staging view to the width of the view. This\\nmakes it much easier to work with diffs that have long lines, e.g.\\nparagraphs of markdown text.\",\n+          \"default\": true\n+        },\n         \"language\": {\n           \"type\": \"string\",\n           \"enum\": [\ndiff --git a/vendor/github.com/jesseduffield/gocui/gui.go b/vendor/github.com/jesseduffield/gocui/gui.go\nindex 0ea2a737973..7caa174a286 100644\n--- a/vendor/github.com/jesseduffield/gocui/gui.go\n+++ b/vendor/github.com/jesseduffield/gocui/gui.go\n@@ -335,7 +335,7 @@ func (g *Gui) SetView(name string, x0, y0, x1, y1 int, overlaps byte) (*View, er\n \n \tg.Mutexes.ViewsMutex.Lock()\n \n-\tv := newView(name, x0, y0, x1, y1, g.outputMode)\n+\tv := NewView(name, x0, y0, x1, y1, g.outputMode)\n \tv.BgColor, v.FgColor = g.BgColor, g.FgColor\n \tv.SelBgColor, v.SelFgColor = g.SelBgColor, g.SelFgColor\n \tv.Overlaps = overlaps\ndiff --git a/vendor/github.com/jesseduffield/gocui/view.go b/vendor/github.com/jesseduffield/gocui/view.go\nindex 190a653a4c3..5a331b43e64 100644\n--- a/vendor/github.com/jesseduffield/gocui/view.go\n+++ b/vendor/github.com/jesseduffield/gocui/view.go\n@@ -402,8 +402,8 @@ func (l lineType) String() string {\n \treturn str\n }\n \n-// newView returns a new View object.\n-func newView(name string, x0, y0, x1, y1 int, mode OutputMode) *View {\n+// NewView returns a new View object.\n+func NewView(name string, x0, y0, x1, y1 int, mode OutputMode) *View {\n \tv := &View{\n \t\tname:              name,\n \t\tx0:                x0,\n@@ -494,31 +494,15 @@ func (v *View) setRune(x, y int, ch rune, fgColor, bgColor Attribute) {\n \t\tbgColor = v.BgColor\n \t\tch = v.Mask\n \t} else if v.Highlight {\n-\t\tvar ry, rcy int\n-\n-\t\t_, ry, ok := v.realPosition(x, y)\n-\t\tif !ok {\n-\t\t\treturn\n-\t\t}\n-\t\t_, rrcy, ok := v.realPosition(v.cx, v.cy)\n-\t\t// out of bounds is fine\n-\t\tif ok {\n-\t\t\trcy = rrcy\n-\t\t}\n-\n-\t\trangeSelectStart := rcy\n-\t\trangeSelectEnd := rcy\n+\t\trangeSelectStart := v.cy\n+\t\trangeSelectEnd := v.cy\n \t\tif v.rangeSelectStartY != -1 {\n-\t\t\t_, realRangeSelectStart, ok := v.realPosition(0, v.rangeSelectStartY-v.oy)\n-\t\t\tif !ok {\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\trangeSelectStart = min(realRangeSelectStart, rcy)\n-\t\t\trangeSelectEnd = max(realRangeSelectStart, rcy)\n+\t\t\trelativeRangeSelectStart := v.rangeSelectStartY - v.oy\n+\t\t\trangeSelectStart = min(relativeRangeSelectStart, v.cy)\n+\t\t\trangeSelectEnd = max(relativeRangeSelectStart, v.cy)\n \t\t}\n \n-\t\tif ry >= rangeSelectStart && ry <= rangeSelectEnd {\n+\t\tif y >= rangeSelectStart && y <= rangeSelectEnd {\n \t\t\t// this ensures we use the bright variant of a colour upon highlight\n \t\t\tfgColorComponent := fgColor & ^AttrAll\n \t\t\tif fgColorComponent >= AttrIsValidColor && fgColorComponent < AttrIsValidColor+8 {\n@@ -1103,6 +1087,8 @@ func (v *View) updateSearchPositions() {\n \n \t\tif v.searcher.modelSearchResults != nil {\n \t\t\tfor _, result := range v.searcher.modelSearchResults {\n+\t\t\t\t// This code only works when v.Wrap is false.\n+\n \t\t\t\tif result.Y >= len(v.lines) {\n \t\t\t\t\tbreak\n \t\t\t\t}\n@@ -1131,8 +1117,9 @@ func (v *View) updateSearchPositions() {\n \t\t\t\t}\n \t\t\t}\n \t\t} else {\n-\t\t\tfor y, line := range v.lines {\n-\t\t\t\tv.searcher.searchPositions = append(v.searcher.searchPositions, searchPositionsForLine(line, y)...)\n+\t\t\tv.refreshViewLinesIfNeeded()\n+\t\t\tfor y, line := range v.viewLines {\n+\t\t\t\tv.searcher.searchPositions = append(v.searcher.searchPositions, searchPositionsForLine(line.line, y)...)\n \t\t\t}\n \t\t}\n \t}\n@@ -1373,6 +1360,8 @@ func (v *View) ViewBufferLines() []string {\n \tv.writeMutex.Lock()\n \tdefer v.writeMutex.Unlock()\n \n+\tv.refreshViewLinesIfNeeded()\n+\n \tlines := make([]string, len(v.viewLines))\n \tfor i, l := range v.viewLines {\n \t\tstr := lineType(l.line).String()\n@@ -1512,18 +1501,20 @@ func lineWrap(line []cell, columns int) [][]cell {\n \t\t\t\tlines = append(lines, line[offset:i])\n \t\t\t\toffset = i\n \t\t\t\tn = rw\n-\t\t\t} else if lastWhitespaceIndex != -1 && lastWhitespaceIndex+1 != i {\n+\t\t\t} else if lastWhitespaceIndex != -1 {\n \t\t\t\t// if there is a space in the line and the line is not breaking at a space/hyphen\n \t\t\t\tif line[lastWhitespaceIndex].chr == '-' {\n \t\t\t\t\t// if break occurs at hyphen, we'll retain the hyphen\n \t\t\t\t\tlines = append(lines, line[offset:lastWhitespaceIndex+1])\n-\t\t\t\t\toffset = lastWhitespaceIndex + 1\n-\t\t\t\t\tn = i - offset\n \t\t\t\t} else {\n \t\t\t\t\t// if break occurs at space, we'll omit the space\n \t\t\t\t\tlines = append(lines, line[offset:lastWhitespaceIndex])\n-\t\t\t\t\toffset = lastWhitespaceIndex + 1\n-\t\t\t\t\tn = i - offset + 1\n+\t\t\t\t}\n+\t\t\t\t// Either way, continue *after* the break\n+\t\t\t\toffset = lastWhitespaceIndex + 1\n+\t\t\t\tn = 0\n+\t\t\t\tfor _, c := range line[offset : i+1] {\n+\t\t\t\t\tn += runewidth.RuneWidth(c.chr)\n \t\t\t\t}\n \t\t\t} else {\n \t\t\t\t// in this case we're breaking mid-word\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux.go b/vendor/golang.org/x/sys/unix/zerrors_linux.go\nindex ccba391c9fb..6ebc48b3fec 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux.go\n@@ -321,6 +321,9 @@ const (\n \tAUDIT_INTEGRITY_STATUS                      = 0x70a\n \tAUDIT_IPC                                   = 0x517\n \tAUDIT_IPC_SET_PERM                          = 0x51f\n+\tAUDIT_IPE_ACCESS                            = 0x58c\n+\tAUDIT_IPE_CONFIG_CHANGE                     = 0x58d\n+\tAUDIT_IPE_POLICY_LOAD                       = 0x58e\n \tAUDIT_KERNEL                                = 0x7d0\n \tAUDIT_KERNEL_OTHER                          = 0x524\n \tAUDIT_KERN_MODULE                           = 0x532\n@@ -489,6 +492,7 @@ const (\n \tBPF_F_ID                                    = 0x20\n \tBPF_F_NETFILTER_IP_DEFRAG                   = 0x1\n \tBPF_F_QUERY_EFFECTIVE                       = 0x1\n+\tBPF_F_REDIRECT_FLAGS                        = 0x19\n \tBPF_F_REPLACE                               = 0x4\n \tBPF_F_SLEEPABLE                             = 0x10\n \tBPF_F_STRICT_ALIGNMENT                      = 0x1\n@@ -1166,6 +1170,7 @@ const (\n \tEXTA                                        = 0xe\n \tEXTB                                        = 0xf\n \tF2FS_SUPER_MAGIC                            = 0xf2f52010\n+\tFALLOC_FL_ALLOCATE_RANGE                    = 0x0\n \tFALLOC_FL_COLLAPSE_RANGE                    = 0x8\n \tFALLOC_FL_INSERT_RANGE                      = 0x20\n \tFALLOC_FL_KEEP_SIZE                         = 0x1\n@@ -1799,6 +1804,8 @@ const (\n \tLANDLOCK_ACCESS_NET_BIND_TCP                = 0x1\n \tLANDLOCK_ACCESS_NET_CONNECT_TCP             = 0x2\n \tLANDLOCK_CREATE_RULESET_VERSION             = 0x1\n+\tLANDLOCK_SCOPE_ABSTRACT_UNIX_SOCKET         = 0x1\n+\tLANDLOCK_SCOPE_SIGNAL                       = 0x2\n \tLINUX_REBOOT_CMD_CAD_OFF                    = 0x0\n \tLINUX_REBOOT_CMD_CAD_ON                     = 0x89abcdef\n \tLINUX_REBOOT_CMD_HALT                       = 0xcdef0123\n@@ -1924,6 +1931,7 @@ const (\n \tMNT_FORCE                                   = 0x1\n \tMNT_ID_REQ_SIZE_VER0                        = 0x18\n \tMNT_ID_REQ_SIZE_VER1                        = 0x20\n+\tMNT_NS_INFO_SIZE_VER0                       = 0x10\n \tMODULE_INIT_COMPRESSED_FILE                 = 0x4\n \tMODULE_INIT_IGNORE_MODVERSIONS              = 0x1\n \tMODULE_INIT_IGNORE_VERMAGIC                 = 0x2\n@@ -2970,6 +2978,7 @@ const (\n \tRWF_WRITE_LIFE_NOT_SET                      = 0x0\n \tSCHED_BATCH                                 = 0x3\n \tSCHED_DEADLINE                              = 0x6\n+\tSCHED_EXT                                   = 0x7\n \tSCHED_FIFO                                  = 0x1\n \tSCHED_FLAG_ALL                              = 0x7f\n \tSCHED_FLAG_DL_OVERRUN                       = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_386.go b/vendor/golang.org/x/sys/unix/zerrors_linux_386.go\nindex 0c00cb3f3af..c0d45e32050 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_386.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_386.go\n@@ -109,6 +109,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -297,6 +298,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -335,6 +338,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go\nindex dfb364554dd..c731d24f025 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_amd64.go\n@@ -109,6 +109,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -298,6 +299,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -336,6 +339,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go b/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go\nindex d46dcf78abc..680018a4a7c 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_arm.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -303,6 +304,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -341,6 +344,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go\nindex 3af3248a7f2..a63909f308d 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_arm64.go\n@@ -112,6 +112,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -205,6 +206,7 @@ const (\n \tPERF_EVENT_IOC_SET_BPF           = 0x40042408\n \tPERF_EVENT_IOC_SET_FILTER        = 0x40082406\n \tPERF_EVENT_IOC_SET_OUTPUT        = 0x2405\n+\tPOE_MAGIC                        = 0x504f4530\n \tPPPIOCATTACH                     = 0x4004743d\n \tPPPIOCATTCHAN                    = 0x40047438\n \tPPPIOCBRIDGECHAN                 = 0x40047435\n@@ -294,6 +296,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -332,6 +336,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_loong64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_loong64.go\nindex 292bcf0283d..9b0a2573fe3 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_loong64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_loong64.go\n@@ -109,6 +109,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -290,6 +291,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -328,6 +331,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_mips.go b/vendor/golang.org/x/sys/unix/zerrors_linux_mips.go\nindex 782b7110fa1..958e6e0645a 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_mips.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_mips.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x100\n@@ -296,6 +297,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -334,6 +337,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x1029\n \tSO_DONTROUTE                     = 0x10\n \tSO_ERROR                         = 0x1007\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_mips64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_mips64.go\nindex 84973fd9271..50c7f25bd16 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_mips64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_mips64.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x100\n@@ -296,6 +297,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -334,6 +337,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x1029\n \tSO_DONTROUTE                     = 0x10\n \tSO_ERROR                         = 0x1007\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_mips64le.go b/vendor/golang.org/x/sys/unix/zerrors_linux_mips64le.go\nindex 6d9cbc3b274..ced21d66d95 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_mips64le.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_mips64le.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x100\n@@ -296,6 +297,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -334,6 +337,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x1029\n \tSO_DONTROUTE                     = 0x10\n \tSO_ERROR                         = 0x1007\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_mipsle.go b/vendor/golang.org/x/sys/unix/zerrors_linux_mipsle.go\nindex 5f9fedbce02..226c0441902 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_mipsle.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_mipsle.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x100\n@@ -296,6 +297,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -334,6 +337,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x1029\n \tSO_DONTROUTE                     = 0x10\n \tSO_ERROR                         = 0x1007\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_ppc.go b/vendor/golang.org/x/sys/unix/zerrors_linux_ppc.go\nindex bb0026ee0c4..3122737cd46 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_ppc.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_ppc.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x4000\n \tICANON                           = 0x100\n \tIEXTEN                           = 0x400\n@@ -351,6 +352,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -389,6 +392,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64.go\nindex 46120db5c9a..eb5d3467edf 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x4000\n \tICANON                           = 0x100\n \tIEXTEN                           = 0x400\n@@ -355,6 +356,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -393,6 +396,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64le.go b/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64le.go\nindex 5c951634fbe..e921ebc60b7 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64le.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_ppc64le.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x4000\n \tICANON                           = 0x100\n \tIEXTEN                           = 0x400\n@@ -355,6 +356,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -393,6 +396,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_riscv64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_riscv64.go\nindex 11a84d5af20..38ba81c55c1 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_riscv64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_riscv64.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -287,6 +288,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -325,6 +328,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_s390x.go b/vendor/golang.org/x/sys/unix/zerrors_linux_s390x.go\nindex f78c4617cac..71f0400977b 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_s390x.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_s390x.go\n@@ -108,6 +108,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x80084803\n \tHIDIOCGRDESC                     = 0x90044802\n \tHIDIOCGRDESCSIZE                 = 0x80044801\n+\tHIDIOCREVOKE                     = 0x4004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -359,6 +360,8 @@ const (\n \tRTC_WIE_ON                       = 0x700f\n \tRTC_WKALM_RD                     = 0x80287010\n \tRTC_WKALM_SET                    = 0x4028700f\n+\tSCM_DEVMEM_DMABUF                = 0x4f\n+\tSCM_DEVMEM_LINEAR                = 0x4e\n \tSCM_TIMESTAMPING                 = 0x25\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x36\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3a\n@@ -397,6 +400,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x35\n \tSO_COOKIE                        = 0x39\n \tSO_DETACH_REUSEPORT_BPF          = 0x44\n+\tSO_DEVMEM_DMABUF                 = 0x4f\n+\tSO_DEVMEM_DONTNEED               = 0x50\n+\tSO_DEVMEM_LINEAR                 = 0x4e\n \tSO_DOMAIN                        = 0x27\n \tSO_DONTROUTE                     = 0x5\n \tSO_ERROR                         = 0x4\ndiff --git a/vendor/golang.org/x/sys/unix/zerrors_linux_sparc64.go b/vendor/golang.org/x/sys/unix/zerrors_linux_sparc64.go\nindex aeb777c3442..c44a313322c 100644\n--- a/vendor/golang.org/x/sys/unix/zerrors_linux_sparc64.go\n+++ b/vendor/golang.org/x/sys/unix/zerrors_linux_sparc64.go\n@@ -112,6 +112,7 @@ const (\n \tHIDIOCGRAWINFO                   = 0x40084803\n \tHIDIOCGRDESC                     = 0x50044802\n \tHIDIOCGRDESCSIZE                 = 0x40044801\n+\tHIDIOCREVOKE                     = 0x8004480d\n \tHUPCL                            = 0x400\n \tICANON                           = 0x2\n \tIEXTEN                           = 0x8000\n@@ -350,6 +351,8 @@ const (\n \tRTC_WIE_ON                       = 0x2000700f\n \tRTC_WKALM_RD                     = 0x40287010\n \tRTC_WKALM_SET                    = 0x8028700f\n+\tSCM_DEVMEM_DMABUF                = 0x58\n+\tSCM_DEVMEM_LINEAR                = 0x57\n \tSCM_TIMESTAMPING                 = 0x23\n \tSCM_TIMESTAMPING_OPT_STATS       = 0x38\n \tSCM_TIMESTAMPING_PKTINFO         = 0x3c\n@@ -436,6 +439,9 @@ const (\n \tSO_CNX_ADVICE                    = 0x37\n \tSO_COOKIE                        = 0x3b\n \tSO_DETACH_REUSEPORT_BPF          = 0x47\n+\tSO_DEVMEM_DMABUF                 = 0x58\n+\tSO_DEVMEM_DONTNEED               = 0x59\n+\tSO_DEVMEM_LINEAR                 = 0x57\n \tSO_DOMAIN                        = 0x1029\n \tSO_DONTROUTE                     = 0x10\n \tSO_ERROR                         = 0x1007\ndiff --git a/vendor/golang.org/x/sys/unix/ztypes_darwin_amd64.go b/vendor/golang.org/x/sys/unix/ztypes_darwin_amd64.go\nindex d003c3d4378..17c53bd9b33 100644\n--- a/vendor/golang.org/x/sys/unix/ztypes_darwin_amd64.go\n+++ b/vendor/golang.org/x/sys/unix/ztypes_darwin_amd64.go\n@@ -462,11 +462,14 @@ type FdSet struct {\n \n const (\n \tSizeofIfMsghdr    = 0x70\n+\tSizeofIfMsghdr2   = 0xa0\n \tSizeofIfData      = 0x60\n+\tSizeofIfData64    = 0x80\n \tSizeofIfaMsghdr   = 0x14\n \tSizeofIfmaMsghdr  = 0x10\n \tSizeofIfmaMsghdr2 = 0x14\n \tSizeofRtMsghdr    = 0x5c\n+\tSizeofRtMsghdr2   = 0x5c\n \tSizeofRtMetrics   = 0x38\n )\n \n@@ -480,6 +483,20 @@ type IfMsghdr struct {\n \tData    IfData\n }\n \n+type IfMsghdr2 struct {\n+\tMsglen     uint16\n+\tVersion    uint8\n+\tType       uint8\n+\tAddrs      int32\n+\tFlags      int32\n+\tIndex      uint16\n+\tSnd_len    int32\n+\tSnd_maxlen int32\n+\tSnd_drops  int32\n+\tTimer      int32\n+\tData       IfData64\n+}\n+\n type IfData struct {\n \tType       uint8\n \tTypelen    uint8\n@@ -512,6 +529,34 @@ type IfData struct {\n \tReserved2  uint32\n }\n \n+type IfData64 struct {\n+\tType       uint8\n+\tTypelen    uint8\n+\tPhysical   uint8\n+\tAddrlen    uint8\n+\tHdrlen     uint8\n+\tRecvquota  uint8\n+\tXmitquota  uint8\n+\tUnused1    uint8\n+\tMtu        uint32\n+\tMetric     uint32\n+\tBaudrate   uint64\n+\tIpackets   uint64\n+\tIerrors    uint64\n+\tOpackets   uint64\n+\tOerrors    uint64\n+\tCollisions uint64\n+\tIbytes     uint64\n+\tObytes     uint64\n+\tImcasts    uint64\n+\tOmcasts    uint64\n+\tIqdrops    uint64\n+\tNoproto    uint64\n+\tRecvtiming uint32\n+\tXmittiming uint32\n+\tLastchange Timeval32\n+}\n+\n type IfaMsghdr struct {\n \tMsglen  uint16\n \tVersion uint8\n@@ -557,6 +602,21 @@ type RtMsghdr struct {\n \tRmx     RtMetrics\n }\n \n+type RtMsghdr2 struct {\n+\tMsglen      uint16\n+\tVersion     uint8\n+\tType        uint8\n+\tIndex       uint16\n+\tFlags       int32\n+\tAddrs       int32\n+\tRefcnt      int32\n+\tParentflags int32\n+\tReserved    int32\n+\tUse         int32\n+\tInits       uint32\n+\tRmx         RtMetrics\n+}\n+\n type RtMetrics struct {\n \tLocks    uint32\n \tMtu      uint32\ndiff --git a/vendor/golang.org/x/sys/unix/ztypes_darwin_arm64.go b/vendor/golang.org/x/sys/unix/ztypes_darwin_arm64.go\nindex 0d45a941aae..2392226a743 100644\n--- a/vendor/golang.org/x/sys/unix/ztypes_darwin_arm64.go\n+++ b/vendor/golang.org/x/sys/unix/ztypes_darwin_arm64.go\n@@ -462,11 +462,14 @@ type FdSet struct {\n \n const (\n \tSizeofIfMsghdr    = 0x70\n+\tSizeofIfMsghdr2   = 0xa0\n \tSizeofIfData      = 0x60\n+\tSizeofIfData64    = 0x80\n \tSizeofIfaMsghdr   = 0x14\n \tSizeofIfmaMsghdr  = 0x10\n \tSizeofIfmaMsghdr2 = 0x14\n \tSizeofRtMsghdr    = 0x5c\n+\tSizeofRtMsghdr2   = 0x5c\n \tSizeofRtMetrics   = 0x38\n )\n \n@@ -480,6 +483,20 @@ type IfMsghdr struct {\n \tData    IfData\n }\n \n+type IfMsghdr2 struct {\n+\tMsglen     uint16\n+\tVersion    uint8\n+\tType       uint8\n+\tAddrs      int32\n+\tFlags      int32\n+\tIndex      uint16\n+\tSnd_len    int32\n+\tSnd_maxlen int32\n+\tSnd_drops  int32\n+\tTimer      int32\n+\tData       IfData64\n+}\n+\n type IfData struct {\n \tType       uint8\n \tTypelen    uint8\n@@ -512,6 +529,34 @@ type IfData struct {\n \tReserved2  uint32\n }\n \n+type IfData64 struct {\n+\tType       uint8\n+\tTypelen    uint8\n+\tPhysical   uint8\n+\tAddrlen    uint8\n+\tHdrlen     uint8\n+\tRecvquota  uint8\n+\tXmitquota  uint8\n+\tUnused1    uint8\n+\tMtu        uint32\n+\tMetric     uint32\n+\tBaudrate   uint64\n+\tIpackets   uint64\n+\tIerrors    uint64\n+\tOpackets   uint64\n+\tOerrors    uint64\n+\tCollisions uint64\n+\tIbytes     uint64\n+\tObytes     uint64\n+\tImcasts    uint64\n+\tOmcasts    uint64\n+\tIqdrops    uint64\n+\tNoproto    uint64\n+\tRecvtiming uint32\n+\tXmittiming uint32\n+\tLastchange Timeval32\n+}\n+\n type IfaMsghdr struct {\n \tMsglen  uint16\n \tVersion uint8\n@@ -557,6 +602,21 @@ type RtMsghdr struct {\n \tRmx     RtMetrics\n }\n \n+type RtMsghdr2 struct {\n+\tMsglen      uint16\n+\tVersion     uint8\n+\tType        uint8\n+\tIndex       uint16\n+\tFlags       int32\n+\tAddrs       int32\n+\tRefcnt      int32\n+\tParentflags int32\n+\tReserved    int32\n+\tUse         int32\n+\tInits       uint32\n+\tRmx         RtMetrics\n+}\n+\n type RtMetrics struct {\n \tLocks    uint32\n \tMtu      uint32\ndiff --git a/vendor/golang.org/x/sys/unix/ztypes_linux.go b/vendor/golang.org/x/sys/unix/ztypes_linux.go\nindex 8daaf3faf4c..5537148dcbb 100644\n--- a/vendor/golang.org/x/sys/unix/ztypes_linux.go\n+++ b/vendor/golang.org/x/sys/unix/ztypes_linux.go\n@@ -2594,8 +2594,8 @@ const (\n \tSOF_TIMESTAMPING_BIND_PHC     = 0x8000\n \tSOF_TIMESTAMPING_OPT_ID_TCP   = 0x10000\n \n-\tSOF_TIMESTAMPING_LAST = 0x10000\n-\tSOF_TIMESTAMPING_MASK = 0x1ffff\n+\tSOF_TIMESTAMPING_LAST = 0x20000\n+\tSOF_TIMESTAMPING_MASK = 0x3ffff\n \n \tSCM_TSTAMP_SND   = 0x0\n \tSCM_TSTAMP_SCHED = 0x1\n@@ -3541,7 +3541,7 @@ type Nhmsg struct {\n type NexthopGrp struct {\n \tId     uint32\n \tWeight uint8\n-\tResvd1 uint8\n+\tHigh   uint8\n \tResvd2 uint16\n }\n \n@@ -3802,7 +3802,7 @@ const (\n \tETHTOOL_MSG_PSE_GET                       = 0x24\n \tETHTOOL_MSG_PSE_SET                       = 0x25\n \tETHTOOL_MSG_RSS_GET                       = 0x26\n-\tETHTOOL_MSG_USER_MAX                      = 0x2c\n+\tETHTOOL_MSG_USER_MAX                      = 0x2d\n \tETHTOOL_MSG_KERNEL_NONE                   = 0x0\n \tETHTOOL_MSG_STRSET_GET_REPLY              = 0x1\n \tETHTOOL_MSG_LINKINFO_GET_REPLY            = 0x2\n@@ -3842,7 +3842,7 @@ const (\n \tETHTOOL_MSG_MODULE_NTF                    = 0x24\n \tETHTOOL_MSG_PSE_GET_REPLY                 = 0x25\n \tETHTOOL_MSG_RSS_GET_REPLY                 = 0x26\n-\tETHTOOL_MSG_KERNEL_MAX                    = 0x2c\n+\tETHTOOL_MSG_KERNEL_MAX                    = 0x2e\n \tETHTOOL_FLAG_COMPACT_BITSETS              = 0x1\n \tETHTOOL_FLAG_OMIT_REPLY                   = 0x2\n \tETHTOOL_FLAG_STATS                        = 0x4\n@@ -3850,7 +3850,7 @@ const (\n \tETHTOOL_A_HEADER_DEV_INDEX                = 0x1\n \tETHTOOL_A_HEADER_DEV_NAME                 = 0x2\n \tETHTOOL_A_HEADER_FLAGS                    = 0x3\n-\tETHTOOL_A_HEADER_MAX                      = 0x3\n+\tETHTOOL_A_HEADER_MAX                      = 0x4\n \tETHTOOL_A_BITSET_BIT_UNSPEC               = 0x0\n \tETHTOOL_A_BITSET_BIT_INDEX                = 0x1\n \tETHTOOL_A_BITSET_BIT_NAME                 = 0x2\n@@ -4031,11 +4031,11 @@ const (\n \tETHTOOL_A_CABLE_RESULT_UNSPEC             = 0x0\n \tETHTOOL_A_CABLE_RESULT_PAIR               = 0x1\n \tETHTOOL_A_CABLE_RESULT_CODE               = 0x2\n-\tETHTOOL_A_CABLE_RESULT_MAX                = 0x2\n+\tETHTOOL_A_CABLE_RESULT_MAX                = 0x3\n \tETHTOOL_A_CABLE_FAULT_LENGTH_UNSPEC       = 0x0\n \tETHTOOL_A_CABLE_FAULT_LENGTH_PAIR         = 0x1\n \tETHTOOL_A_CABLE_FAULT_LENGTH_CM           = 0x2\n-\tETHTOOL_A_CABLE_FAULT_LENGTH_MAX          = 0x2\n+\tETHTOOL_A_CABLE_FAULT_LENGTH_MAX          = 0x3\n \tETHTOOL_A_CABLE_TEST_NTF_STATUS_UNSPEC    = 0x0\n \tETHTOOL_A_CABLE_TEST_NTF_STATUS_STARTED   = 0x1\n \tETHTOOL_A_CABLE_TEST_NTF_STATUS_COMPLETED = 0x2\n@@ -4200,7 +4200,8 @@ type (\n \t}\n \tPtpSysOffsetExtended struct {\n \t\tSamples uint32\n-\t\tRsv     [3]uint32\n+\t\tClockid int32\n+\t\tRsv     [2]uint32\n \t\tTs      [25][3]PtpClockTime\n \t}\n \tPtpSysOffsetPrecise struct {\n@@ -4399,6 +4400,7 @@ const (\n type LandlockRulesetAttr struct {\n \tAccess_fs  uint64\n \tAccess_net uint64\n+\tScoped     uint64\n }\n \n type LandlockPathBeneathAttr struct {\ndiff --git a/vendor/golang.org/x/sys/windows/syscall_windows.go b/vendor/golang.org/x/sys/windows/syscall_windows.go\nindex 4510bfc3f5c..4a325438685 100644\n--- a/vendor/golang.org/x/sys/windows/syscall_windows.go\n+++ b/vendor/golang.org/x/sys/windows/syscall_windows.go\n@@ -168,6 +168,8 @@ func NewCallbackCDecl(fn interface{}) uintptr {\n //sys\tCreateNamedPipe(name *uint16, flags uint32, pipeMode uint32, maxInstances uint32, outSize uint32, inSize uint32, defaultTimeout uint32, sa *SecurityAttributes) (handle Handle, err error)  [failretval==InvalidHandle] = CreateNamedPipeW\n //sys\tConnectNamedPipe(pipe Handle, overlapped *Overlapped) (err error)\n //sys\tDisconnectNamedPipe(pipe Handle) (err error)\n+//sys   GetNamedPipeClientProcessId(pipe Handle, clientProcessID *uint32) (err error)\n+//sys   GetNamedPipeServerProcessId(pipe Handle, serverProcessID *uint32) (err error)\n //sys\tGetNamedPipeInfo(pipe Handle, flags *uint32, outSize *uint32, inSize *uint32, maxInstances *uint32) (err error)\n //sys\tGetNamedPipeHandleState(pipe Handle, state *uint32, curInstances *uint32, maxCollectionCount *uint32, collectDataTimeout *uint32, userName *uint16, maxUserNameSize uint32) (err error) = GetNamedPipeHandleStateW\n //sys\tSetNamedPipeHandleState(pipe Handle, state *uint32, maxCollectionCount *uint32, collectDataTimeout *uint32) (err error) = SetNamedPipeHandleState\ndiff --git a/vendor/golang.org/x/sys/windows/types_windows.go b/vendor/golang.org/x/sys/windows/types_windows.go\nindex 51311e205ff..9d138de5fed 100644\n--- a/vendor/golang.org/x/sys/windows/types_windows.go\n+++ b/vendor/golang.org/x/sys/windows/types_windows.go\n@@ -176,6 +176,7 @@ const (\n \tWAIT_FAILED    = 0xFFFFFFFF\n \n \t// Access rights for process.\n+\tPROCESS_ALL_ACCESS                = 0xFFFF\n \tPROCESS_CREATE_PROCESS            = 0x0080\n \tPROCESS_CREATE_THREAD             = 0x0002\n \tPROCESS_DUP_HANDLE                = 0x0040\ndiff --git a/vendor/golang.org/x/sys/windows/zsyscall_windows.go b/vendor/golang.org/x/sys/windows/zsyscall_windows.go\nindex 6f5252880ce..01c0716c2c4 100644\n--- a/vendor/golang.org/x/sys/windows/zsyscall_windows.go\n+++ b/vendor/golang.org/x/sys/windows/zsyscall_windows.go\n@@ -280,8 +280,10 @@ var (\n \tprocGetMaximumProcessorCount                             = modkernel32.NewProc(\"GetMaximumProcessorCount\")\n \tprocGetModuleFileNameW                                   = modkernel32.NewProc(\"GetModuleFileNameW\")\n \tprocGetModuleHandleExW                                   = modkernel32.NewProc(\"GetModuleHandleExW\")\n+\tprocGetNamedPipeClientProcessId                          = modkernel32.NewProc(\"GetNamedPipeClientProcessId\")\n \tprocGetNamedPipeHandleStateW                             = modkernel32.NewProc(\"GetNamedPipeHandleStateW\")\n \tprocGetNamedPipeInfo                                     = modkernel32.NewProc(\"GetNamedPipeInfo\")\n+\tprocGetNamedPipeServerProcessId                          = modkernel32.NewProc(\"GetNamedPipeServerProcessId\")\n \tprocGetOverlappedResult                                  = modkernel32.NewProc(\"GetOverlappedResult\")\n \tprocGetPriorityClass                                     = modkernel32.NewProc(\"GetPriorityClass\")\n \tprocGetProcAddress                                       = modkernel32.NewProc(\"GetProcAddress\")\n@@ -1612,7 +1614,7 @@ func DwmSetWindowAttribute(hwnd HWND, attribute uint32, value unsafe.Pointer, si\n }\n \n func CancelMibChangeNotify2(notificationHandle Handle) (errcode error) {\n-\tr0, _, _ := syscall.SyscallN(procCancelMibChangeNotify2.Addr(), uintptr(notificationHandle))\n+\tr0, _, _ := syscall.Syscall(procCancelMibChangeNotify2.Addr(), 1, uintptr(notificationHandle), 0, 0)\n \tif r0 != 0 {\n \t\terrcode = syscall.Errno(r0)\n \t}\n@@ -1652,7 +1654,7 @@ func GetIfEntry(pIfRow *MibIfRow) (errcode error) {\n }\n \n func GetIfEntry2Ex(level uint32, row *MibIfRow2) (errcode error) {\n-\tr0, _, _ := syscall.SyscallN(procGetIfEntry2Ex.Addr(), uintptr(level), uintptr(unsafe.Pointer(row)))\n+\tr0, _, _ := syscall.Syscall(procGetIfEntry2Ex.Addr(), 2, uintptr(level), uintptr(unsafe.Pointer(row)), 0)\n \tif r0 != 0 {\n \t\terrcode = syscall.Errno(r0)\n \t}\n@@ -1660,7 +1662,7 @@ func GetIfEntry2Ex(level uint32, row *MibIfRow2) (errcode error) {\n }\n \n func GetUnicastIpAddressEntry(row *MibUnicastIpAddressRow) (errcode error) {\n-\tr0, _, _ := syscall.SyscallN(procGetUnicastIpAddressEntry.Addr(), uintptr(unsafe.Pointer(row)))\n+\tr0, _, _ := syscall.Syscall(procGetUnicastIpAddressEntry.Addr(), 1, uintptr(unsafe.Pointer(row)), 0, 0)\n \tif r0 != 0 {\n \t\terrcode = syscall.Errno(r0)\n \t}\n@@ -1672,7 +1674,7 @@ func NotifyIpInterfaceChange(family uint16, callback uintptr, callerContext unsa\n \tif initialNotification {\n \t\t_p0 = 1\n \t}\n-\tr0, _, _ := syscall.SyscallN(procNotifyIpInterfaceChange.Addr(), uintptr(family), uintptr(callback), uintptr(callerContext), uintptr(_p0), uintptr(unsafe.Pointer(notificationHandle)))\n+\tr0, _, _ := syscall.Syscall6(procNotifyIpInterfaceChange.Addr(), 5, uintptr(family), uintptr(callback), uintptr(callerContext), uintptr(_p0), uintptr(unsafe.Pointer(notificationHandle)), 0)\n \tif r0 != 0 {\n \t\terrcode = syscall.Errno(r0)\n \t}\n@@ -1684,7 +1686,7 @@ func NotifyUnicastIpAddressChange(family uint16, callback uintptr, callerContext\n \tif initialNotification {\n \t\t_p0 = 1\n \t}\n-\tr0, _, _ := syscall.SyscallN(procNotifyUnicastIpAddressChange.Addr(), uintptr(family), uintptr(callback), uintptr(callerContext), uintptr(_p0), uintptr(unsafe.Pointer(notificationHandle)))\n+\tr0, _, _ := syscall.Syscall6(procNotifyUnicastIpAddressChange.Addr(), 5, uintptr(family), uintptr(callback), uintptr(callerContext), uintptr(_p0), uintptr(unsafe.Pointer(notificationHandle)), 0)\n \tif r0 != 0 {\n \t\terrcode = syscall.Errno(r0)\n \t}\n@@ -2446,6 +2448,14 @@ func GetModuleHandleEx(flags uint32, moduleName *uint16, module *Handle) (err er\n \treturn\n }\n \n+func GetNamedPipeClientProcessId(pipe Handle, clientProcessID *uint32) (err error) {\n+\tr1, _, e1 := syscall.Syscall(procGetNamedPipeClientProcessId.Addr(), 2, uintptr(pipe), uintptr(unsafe.Pointer(clientProcessID)), 0)\n+\tif r1 == 0 {\n+\t\terr = errnoErr(e1)\n+\t}\n+\treturn\n+}\n+\n func GetNamedPipeHandleState(pipe Handle, state *uint32, curInstances *uint32, maxCollectionCount *uint32, collectDataTimeout *uint32, userName *uint16, maxUserNameSize uint32) (err error) {\n \tr1, _, e1 := syscall.Syscall9(procGetNamedPipeHandleStateW.Addr(), 7, uintptr(pipe), uintptr(unsafe.Pointer(state)), uintptr(unsafe.Pointer(curInstances)), uintptr(unsafe.Pointer(maxCollectionCount)), uintptr(unsafe.Pointer(collectDataTimeout)), uintptr(unsafe.Pointer(userName)), uintptr(maxUserNameSize), 0, 0)\n \tif r1 == 0 {\n@@ -2462,6 +2472,14 @@ func GetNamedPipeInfo(pipe Handle, flags *uint32, outSize *uint32, inSize *uint3\n \treturn\n }\n \n+func GetNamedPipeServerProcessId(pipe Handle, serverProcessID *uint32) (err error) {\n+\tr1, _, e1 := syscall.Syscall(procGetNamedPipeServerProcessId.Addr(), 2, uintptr(pipe), uintptr(unsafe.Pointer(serverProcessID)), 0)\n+\tif r1 == 0 {\n+\t\terr = errnoErr(e1)\n+\t}\n+\treturn\n+}\n+\n func GetOverlappedResult(handle Handle, overlapped *Overlapped, done *uint32, wait bool) (err error) {\n \tvar _p0 uint32\n \tif wait {\ndiff --git a/vendor/modules.txt b/vendor/modules.txt\nindex ba86a7a524d..7d5bb258992 100644\n--- a/vendor/modules.txt\n+++ b/vendor/modules.txt\n@@ -172,7 +172,7 @@ github.com/jesseduffield/go-git/v5/utils/merkletrie/filesystem\n github.com/jesseduffield/go-git/v5/utils/merkletrie/index\n github.com/jesseduffield/go-git/v5/utils/merkletrie/internal/frame\n github.com/jesseduffield/go-git/v5/utils/merkletrie/noder\n-# github.com/jesseduffield/gocui v0.3.1-0.20241201093724-68c437bbd543\n+# github.com/jesseduffield/gocui v0.3.1-0.20241223111608-9967d0e928a0\n ## explicit; go 1.12\n github.com/jesseduffield/gocui\n # github.com/jesseduffield/kill v0.0.0-20220618033138-bfbe04675d10\n@@ -313,19 +313,19 @@ golang.org/x/exp/slices\n golang.org/x/net/context\n golang.org/x/net/internal/socks\n golang.org/x/net/proxy\n-# golang.org/x/sync v0.9.0\n+# golang.org/x/sync v0.10.0\n ## explicit; go 1.18\n golang.org/x/sync/errgroup\n-# golang.org/x/sys v0.27.0\n+# golang.org/x/sys v0.28.0\n ## explicit; go 1.18\n golang.org/x/sys/cpu\n golang.org/x/sys/plan9\n golang.org/x/sys/unix\n golang.org/x/sys/windows\n-# golang.org/x/term v0.26.0\n+# golang.org/x/term v0.27.0\n ## explicit; go 1.18\n golang.org/x/term\n-# golang.org/x/text v0.20.0\n+# golang.org/x/text v0.21.0\n ## explicit; go 1.18\n golang.org/x/text/encoding\n golang.org/x/text/encoding/internal/identifier\n", "instance_id": "jesseduffield__lazygit-4098", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the user's need for a line wrapping feature in the diff/staging mode of a Git tool (likely LazyGit, based on the context of the code changes). The motivation is well-articulated, describing the issue with long lines in text files and the inconvenience of horizontal scrolling or using alternative views. The goal is evident: to implement a toggle or configuration for line wrapping in the staging view to improve usability. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define whether the line wrapping should apply to all views or just specific ones (though the code changes clarify this). Additionally, there are no specific requirements for how the wrapping should behave (e.g., word boundaries, performance considerations for large files) or explicit mention of edge cases like extremely long lines or special characters. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes is significant, spanning multiple files and modules in a Go-based TUI (Text User Interface) application, likely LazyGit. The changes involve adding a new configuration option (`wrapLinesInStagingView`), updating the GUI rendering logic in `gocui` (a TUI library), and modifying the patch exploration and staging logic to handle wrapped lines correctly. This requires understanding and modifying interactions between different parts of the codebase, such as view rendering, user configuration, and state management for patch exploration.\n\nSecond, the number of technical concepts involved is substantial. The developer must be familiar with Go, TUI development using `gocui`, text wrapping algorithms (including handling word boundaries and special characters like tabs), and the internal state management of diffs and patches in a Git tool. The code changes also involve updating dependency versions (e.g., `gocui`, `golang.org/x/sync`), which suggests potential compatibility or integration challenges. Additionally, the implementation of line wrapping requires distinguishing between view lines (wrapped) and patch lines (original), as seen in the `patch_exploring` package, which adds complexity to selection and navigation logic.\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes imply the need to handle scenarios like view width changes (triggering re-wrapping), large diffs with long lines, and ensuring that selection and navigation (e.g., next/previous hunk) remain accurate with wrapped content. Error handling modifications are minimal, but the rendering logic must be robust to avoid UI glitches or performance issues with large files.\n\nOverall, this task requires a deep understanding of the LazyGit codebase architecture, particularly its GUI and patch handling components, and involves complex modifications with a moderate impact on user experience. It is not at the highest difficulty level (0.8-1.0) because it does not involve system-level or highly domain-specific challenges, but it is still a challenging feature to implement correctly across the affected modules.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`rclone backend restore` - documentation regression\n<!--\n\nWe understand you are having a problem with rclone; we want to help you with that!\n\n**STOP and READ**\n**YOUR POST WILL BE REMOVED IF IT IS LOW QUALITY**:\nPlease show the effort you've put into solving the problem and please be specific.\nPeople are volunteering their time to help! Low effort posts are not likely to get good answers!\n\nIf you think you might have found a bug, try to replicate it with the latest beta (or stable).\nThe update instructions are available at https://rclone.org/commands/rclone_selfupdate/\n\nIf you can still replicate it or just got a question then please use the rclone forum:\n\n    https://forum.rclone.org/\n\nfor a quick response instead of filing an issue on this repo.\n\nIf nothing else helps, then please fill in the info below which helps us help you.\n\n**DO NOT REDACT** any information except passwords/keys/personal info.\n\nYou should use 3 backticks to begin and end your paste to make it readable.\n\nMake sure to include a log obtained with '-vv'.\n\nYou can also use '-vv --log-file bug.log' and a service such as https://pastebin.com or https://gist.github.com/\n\nThank you\n\nThe Rclone Developers\n\n-->\n\n#### The associated forum post URL from `https://forum.rclone.org`\n\nhttps://forum.rclone.org/t/cant-restore-files-from-aws-glacier-deep-only-directories/39258/10\n\n#### What is the problem you are having with rclone?\n\nThe [rclone documentation](https://rclone.org/s3/#restore) lost the fix in #7077 [at some point](https://github.com/rclone/rclone/commit/e90537b2e93d194848560b83e2c5164641a02595#diff-22ce4ba99f00925bc5402c273e57cb2e66a43b5c659392ff21cc1b85b713fed6R3149).\nThe underlying issue is still the same, so the fix PR just needs reapplying.\n\n#### What is your rclone version (output from `rclone version`)\n\n```\nrclone v1.69.1\n- os/version: ubuntu 22.04 (64 bit)\n- os/kernel: 5.15.0-1071-raspi (aarch64)\n- os/type: linux\n- os/arch: arm64 (ARMv8 compatible)\n- go/version: go1.24.0\n- go/linking: static\n- go/tags: none\n```\n\n#### Which OS you are using and how many bits (e.g. Windows 7, 64 bit)\n\nlinux, 64 bit\n\n#### Which cloud storage system are you using? (e.g. Google Drive)\n\nAWS S3\n\n#### The command you were trying to run (e.g. `rclone copy /tmp remote:tmp`)\n\nrclone backend restore awss3:bucket/path/object\n\n#### A log from the command with the `-vv` flag (e.g. output from `rclone -vv copy /tmp remote:tmp`)\n\n2025/03/09 01:25:01 DEBUG : rclone: Version \"v1.69.1\" starting with parameters [\"rclone\" \"-vv\" \"backend\" \"restore\" \"awss3:bucket/path/object\"]\n2025/03/09 01:25:01 DEBUG : Using config file from \"/root/.config/rclone/rclone.conf\"\n2025/03/09 01:25:01 DEBUG : 5 go routines active\n2025/03/09 01:25:01 NOTICE: Failed to backend: is a file not a directory\n\n<!--- Please keep the note below for others who read your bug report. -->\n\n#### How to use GitHub\n\n* Please use the \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.\n* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.\n* Subscribe to receive notifications on status change and new comments.\n\n", "patch": "diff --git a/backend/s3/s3.go b/backend/s3/s3.go\nindex 89faffcc263a7..d9a6ab026970b 100644\n--- a/backend/s3/s3.go\n+++ b/backend/s3/s3.go\n@@ -5061,7 +5061,7 @@ or from INTELLIGENT-TIERING Archive Access / Deep Archive Access tier to the Fre\n \n Usage Examples:\n \n-    rclone backend restore s3:bucket/path/to/object -o priority=PRIORITY -o lifetime=DAYS\n+    rclone backend restore s3:bucket/path/to/ --include /object -o priority=PRIORITY -o lifetime=DAYS\n     rclone backend restore s3:bucket/path/to/directory -o priority=PRIORITY -o lifetime=DAYS\n     rclone backend restore s3:bucket -o priority=PRIORITY -o lifetime=DAYS\n     rclone backend restore s3:bucket/path/to/directory -o priority=PRIORITY\ndiff --git a/docs/content/s3.md b/docs/content/s3.md\nindex 77a571f032536..b21e46930bfdf 100644\n--- a/docs/content/s3.md\n+++ b/docs/content/s3.md\n@@ -771,7 +771,8 @@ tries to access data from the glacier storage class you will see an error like b\n     2017/09/11 19:07:43 Failed to sync: failed to open source object: Object in GLACIER, restore first: path/to/file\n \n In this case you need to [restore](http://docs.aws.amazon.com/AmazonS3/latest/user-guide/restore-archived-objects.html)\n-the object(s) in question before using rclone.\n+the object(s) in question before accessing object contents.\n+The [restore](#restore) section below shows how to do this with rclone.\n \n Note that rclone only speaks the S3 API it does not speak the Glacier\n Vault API, so rclone cannot directly access Glacier Vaults.\n", "instance_id": "rclone__rclone-8439", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a documentation regression in the `rclone backend restore` command for AWS S3, where a previous fix (from PR #7077) was lost and needs to be reapplied. The goal is to update the documentation to reflect the correct usage of the command. The statement includes relevant links to the forum post, the specific commit where the regression occurred, and logs that show the error. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly describe the expected behavior or output after the fix, nor does it mention any specific edge cases or constraints related to the `restore` command usage. Additionally, while the forum post and logs provide context, the statement could benefit from a more concise summary of the issue and the intended fix. Overall, it is clear enough to understand the task but lacks some finer details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward documentation update with minimal code changes. The scope of the modification is limited to two files (`s3.go` and `s3.md`), and the changes are purely textual, focusing on correcting usage examples and clarifying instructions for the `rclone backend restore` command. There is no impact on the system's architecture, no complex logic to implement, and no interaction with other parts of the codebase. The technical concepts required are negligible, as this task does not involve programming language features, algorithms, or domain-specific knowledge beyond basic familiarity with rclone's documentation format. Edge cases and error handling are not relevant here, as the changes are confined to documentation and do not affect runtime behavior. This task is essentially a simple fix akin to correcting a typo or updating a user guide, requiring minimal effort and understanding of the broader codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "backend: pcloud: support block level sync (upload only)\n<!--\r\n\r\nWelcome :-)\r\n\r\nSo you've got an idea to improve rclone? We love that!\r\nYou'll be glad to hear we've incorporated hundreds of ideas from contributors already.\r\n\r\nProbably the latest beta (or stable) release has your feature, so try to update your rclone.\r\nThe update instructions are available at https://rclone.org/commands/rclone_selfupdate/\r\n\r\nIf it still isn't there, here is a checklist of things to do:\r\n\r\n  1. Search the old issues for your idea and +1 or comment on an existing issue if possible.\r\n  2. Discuss on the forum: https://forum.rclone.org/\r\n  3. Make a feature request issue (this is the right place!).\r\n  4. Be prepared to get involved making the feature :-)\r\n\r\nLooking forward to your great idea!\r\n\r\nThe Rclone Developers\r\n\r\n-->\r\n\r\n#### The associated forum post URL from `https://forum.rclone.org`\r\n\r\n- Block level sync in general was requests in the forum [here](https://forum.rclone.org/t/pcloud-block-level-sync/43455)\r\n- It helps fixing the the rate limit issues discussed [here](https://forum.rclone.org/t/how-to-handle-pcloud-rate-limit/31279/19):\r\n  - The described work around only helps when uploading multiple smaller files by increasing `--transfers`\r\n  - For single large files, I haven't found any workaround.\r\n  - Block level sync seems to be slightly faster, but the main point is: With this, we can resume failed uploads of large files (e.g. due to timeouts caused by rate limit)\r\n\r\n#### What is your current rclone version (output from `rclone version`)?\r\n\r\n- master\r\n\r\n#### What problem are you are trying to solve?\r\n\r\n- Implement object.SetModTime to reduce unecessary re-uploads on mtime mismatch\r\n  - This is an undocumented API feature (reverse-engineered), using server-side copy with same target, only changing metadata.\r\n  - This is necessary for block level support, otherwise uploaded files will have mtime from time of upload.\r\n- Implement new option `upload_chunk_size`\r\n  - Enables chunk-wise uploads of files.\r\n  - Successful uploaded chunks are skipped when upload needs retry, thanks to block-wise hashing capability.\r\n\r\nFocusing on the upload in this issue, block level download can easily be added in follow-up.\r\n\r\n---\r\n\r\ntl;dr\r\n\r\nUploading large files to pcloud using the uploadfile API is error prone, as any connection reset / package loss will cause the full file to be retried. \r\nAdditionally, I can see that pcloud applies some sort of rate limiting, causing large uploads to be throttled down. \r\n(Their \"support\" claims that there is no such rate limit and blames my ISP. But by increasing the `--transfers` with multiple files, I can multiply the throughput by amount of transfers until I reach my ISPs upload lim. Also 32kbps is such a round number, I doubt that its coincidence.)\r\n\r\nIn my case, this renders it almost impossible to upload any file larger than ~200MB via rclone.\r\n\r\nFortunately, pcloud offers a [fileops API](https://docs.pcloud.com/methods/fileops/), which enables chunked uploads on block level \ud83c\udf89\r\nPR #7895  adds an advanced option `upload_chunk_size` to the pcloud backend.\r\nWhen set, files will be uploaded in chunks using the fileops API. Otherwise, current behavior is used (backwards compatibility).\r\n\r\n#### How do you think rclone should be changed to solve that?\r\n\r\nI created a draft PR #7895 that implements it. \r\nFeature is drafty, but works on my machine :). Before I invest more time in writing tests and documentation, I'd appreciate a first review, whether the feature itself / the way I built it goes into the right direction.\r\n\r\n<!--- Please keep the note below for others who read your feature request. -->\r\n\r\n#### How to use GitHub\r\n\r\n* Please use the \ud83d\udc4d [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.\r\n* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.\r\n* Subscribe to receive notifications on status change and new comments.\r\n\n", "patch": "diff --git a/backend/pcloud/api/types.go b/backend/pcloud/api/types.go\nindex c1b5dc2176ff0..6f2d6361bc13f 100644\n--- a/backend/pcloud/api/types.go\n+++ b/backend/pcloud/api/types.go\n@@ -109,6 +109,37 @@ type Hashes struct {\n \tSHA256 string `json:\"sha256\"`\n }\n \n+// FileTruncateResponse is the response from /file_truncate\n+type FileTruncateResponse struct {\n+\tError\n+}\n+\n+// FileCloseResponse is the response from /file_close\n+type FileCloseResponse struct {\n+\tError\n+}\n+\n+// FileOpenResponse is the response from /file_open\n+type FileOpenResponse struct {\n+\tError\n+\tFileid         int64 `json:\"fileid\"`\n+\tFileDescriptor int64 `json:\"fd\"`\n+}\n+\n+// FileChecksumResponse is the response from /file_checksum\n+type FileChecksumResponse struct {\n+\tError\n+\tMD5    string `json:\"md5\"`\n+\tSHA1   string `json:\"sha1\"`\n+\tSHA256 string `json:\"sha256\"`\n+}\n+\n+// FilePWriteResponse is the response from /file_pwrite\n+type FilePWriteResponse struct {\n+\tError\n+\tBytes int64 `json:\"bytes\"`\n+}\n+\n // UploadFileResponse is the response from /uploadfile\n type UploadFileResponse struct {\n \tError\ndiff --git a/backend/pcloud/pcloud.go b/backend/pcloud/pcloud.go\nindex 64a48c25c67cb..dc475cb9db079 100644\n--- a/backend/pcloud/pcloud.go\n+++ b/backend/pcloud/pcloud.go\n@@ -14,6 +14,7 @@ import (\n \t\"net/http\"\n \t\"net/url\"\n \t\"path\"\n+\t\"strconv\"\n \t\"strings\"\n \t\"time\"\n \n@@ -146,7 +147,8 @@ we have to rely on user password authentication for it.`,\n \t\t\tHelp:       \"Your pcloud password.\",\n \t\t\tIsPassword: true,\n \t\t\tAdvanced:   true,\n-\t\t}}...),\n+\t\t},\n+\t\t}...),\n \t})\n }\n \n@@ -161,15 +163,16 @@ type Options struct {\n \n // Fs represents a remote pcloud\n type Fs struct {\n-\tname         string             // name of this remote\n-\troot         string             // the path we are working on\n-\topt          Options            // parsed options\n-\tfeatures     *fs.Features       // optional features\n-\tsrv          *rest.Client       // the connection to the server\n-\tcleanupSrv   *rest.Client       // the connection used for the cleanup method\n-\tdirCache     *dircache.DirCache // Map of directory path to directory id\n-\tpacer        *fs.Pacer          // pacer for API calls\n-\ttokenRenewer *oauthutil.Renew   // renew the token on expiry\n+\tname         string                 // name of this remote\n+\troot         string                 // the path we are working on\n+\topt          Options                // parsed options\n+\tfeatures     *fs.Features           // optional features\n+\tts           *oauthutil.TokenSource // the token source, used to create new clients\n+\tsrv          *rest.Client           // the connection to the server\n+\tcleanupSrv   *rest.Client           // the connection used for the cleanup method\n+\tdirCache     *dircache.DirCache     // Map of directory path to directory id\n+\tpacer        *fs.Pacer              // pacer for API calls\n+\ttokenRenewer *oauthutil.Renew       // renew the token on expiry\n }\n \n // Object describes a pcloud object\n@@ -317,6 +320,7 @@ func NewFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, e\n \t\tname:  name,\n \t\troot:  root,\n \t\topt:   *opt,\n+\t\tts:    ts,\n \t\tsrv:   rest.NewClient(oAuthClient).SetRoot(\"https://\" + opt.Hostname),\n \t\tpacer: fs.NewPacer(ctx, pacer.NewDefault(pacer.MinSleep(minSleep), pacer.MaxSleep(maxSleep), pacer.DecayConstant(decayConstant))),\n \t}\n@@ -326,6 +330,7 @@ func NewFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, e\n \tf.features = (&fs.Features{\n \t\tCaseInsensitive:         false,\n \t\tCanHaveEmptyDirectories: true,\n+\t\tPartialUploads:          true,\n \t}).Fill(ctx, f)\n \tif !canCleanup {\n \t\tf.features.CleanUp = nil\n@@ -333,7 +338,7 @@ func NewFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, e\n \tf.srv.SetErrorHandler(errorHandler)\n \n \t// Renew the token in the background\n-\tf.tokenRenewer = oauthutil.NewRenew(f.String(), ts, func() error {\n+\tf.tokenRenewer = oauthutil.NewRenew(f.String(), f.ts, func() error {\n \t\t_, err := f.readMetaDataForPath(ctx, \"\")\n \t\treturn err\n \t})\n@@ -375,6 +380,56 @@ func NewFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, e\n \treturn f, nil\n }\n \n+// OpenWriterAt opens with a handle for random access writes\n+//\n+// Pass in the remote desired and the size if known.\n+//\n+// It truncates any existing object\n+func (f *Fs) OpenWriterAt(ctx context.Context, remote string, size int64) (fs.WriterAtCloser, error) {\n+\tclient, err := f.newSingleConnClient(ctx)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"create client: %w\", err)\n+\t}\n+\t// init an empty file\n+\tleaf, directoryID, err := f.dirCache.FindPath(ctx, remote, true)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"resolve src: %w\", err)\n+\t}\n+\topenResult, err := fileOpenNew(ctx, client, f, directoryID, leaf)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"open file: %w\", err)\n+\t}\n+\n+\twriter := &writerAt{\n+\t\tctx:    ctx,\n+\t\tclient: client,\n+\t\tfs:     f,\n+\t\tsize:   size,\n+\t\tremote: remote,\n+\t\tfd:     openResult.FileDescriptor,\n+\t\tfileID: openResult.Fileid,\n+\t}\n+\n+\treturn writer, nil\n+}\n+\n+// Create a new http client, accepting keep-alive headers, limited to single connection.\n+// Necessary for pcloud fileops API, as it binds the session to the underlying TCP connection.\n+// File descriptors are only valid within the same connection and auto-closed when the connection is closed,\n+// hence we need a separate client (with single connection) for each fd to avoid all sorts of errors and race conditions.\n+func (f *Fs) newSingleConnClient(ctx context.Context) (*rest.Client, error) {\n+\tbaseClient := fshttp.NewClient(ctx)\n+\tbaseClient.Transport = fshttp.NewTransportCustom(ctx, func(t *http.Transport) {\n+\t\tt.MaxConnsPerHost = 1\n+\t\tt.DisableKeepAlives = false\n+\t})\n+\t// Set our own http client in the context\n+\tctx = oauthutil.Context(ctx, baseClient)\n+\t// create a new oauth client, re-use the token source\n+\toAuthClient := oauth2.NewClient(ctx, f.ts)\n+\treturn rest.NewClient(oAuthClient).SetRoot(\"https://\" + f.opt.Hostname), nil\n+}\n+\n // Return an Object from a path\n //\n // If it can't be found it returns the error fs.ErrorObjectNotFound.\n@@ -1094,9 +1149,42 @@ func (o *Object) ModTime(ctx context.Context) time.Time {\n \n // SetModTime sets the modification time of the local fs object\n func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {\n-\t// Pcloud doesn't have a way of doing this so returning this\n-\t// error will cause the file to be re-uploaded to set the time.\n-\treturn fs.ErrorCantSetModTime\n+\tfilename, directoryID, err := o.fs.dirCache.FindPath(ctx, o.Remote(), true)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tfileID := fileIDtoNumber(o.id)\n+\tfilename = o.fs.opt.Enc.FromStandardName(filename)\n+\topts := rest.Opts{\n+\t\tMethod:           \"PUT\",\n+\t\tPath:             \"/copyfile\",\n+\t\tParameters:       url.Values{},\n+\t\tTransferEncoding: []string{\"identity\"}, // pcloud doesn't like chunked encoding\n+\t\tExtraHeaders: map[string]string{\n+\t\t\t\"Connection\": \"keep-alive\",\n+\t\t},\n+\t}\n+\topts.Parameters.Set(\"fileid\", fileID)\n+\topts.Parameters.Set(\"folderid\", dirIDtoNumber(directoryID))\n+\topts.Parameters.Set(\"toname\", filename)\n+\topts.Parameters.Set(\"tofolderid\", dirIDtoNumber(directoryID))\n+\topts.Parameters.Set(\"ctime\", strconv.FormatInt(modTime.Unix(), 10))\n+\topts.Parameters.Set(\"mtime\", strconv.FormatInt(modTime.Unix(), 10))\n+\n+\tresult := &api.ItemResult{}\n+\terr = o.fs.pacer.CallNoRetry(func() (bool, error) {\n+\t\tresp, err := o.fs.srv.CallJSON(ctx, &opts, nil, result)\n+\t\terr = result.Error.Update(err)\n+\t\treturn shouldRetry(ctx, resp, err)\n+\t})\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"update mtime: copyfile: %w\", err)\n+\t}\n+\tif err := o.setMetaData(&result.Metadata); err != nil {\n+\t\treturn err\n+\t}\n+\n+\treturn nil\n }\n \n // Storable returns a boolean showing whether this object storable\ndiff --git a/backend/pcloud/writer_at.go b/backend/pcloud/writer_at.go\nnew file mode 100644\nindex 0000000000000..ee3380a71e690\n--- /dev/null\n+++ b/backend/pcloud/writer_at.go\n@@ -0,0 +1,216 @@\n+package pcloud\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"crypto/sha1\"\n+\t\"encoding/hex\"\n+\t\"fmt\"\n+\t\"net/url\"\n+\t\"strconv\"\n+\t\"time\"\n+\n+\t\"github.com/rclone/rclone/backend/pcloud/api\"\n+\t\"github.com/rclone/rclone/fs\"\n+\t\"github.com/rclone/rclone/lib/rest\"\n+)\n+\n+// writerAt implements fs.WriterAtCloser, adding the OpenWrtierAt feature to pcloud.\n+type writerAt struct {\n+\tctx    context.Context\n+\tclient *rest.Client\n+\tfs     *Fs\n+\tsize   int64\n+\tremote string\n+\tfd     int64\n+\tfileID int64\n+}\n+\n+// Close implements WriterAt.Close.\n+func (c *writerAt) Close() error {\n+\t// close fd\n+\tif _, err := c.fileClose(c.ctx); err != nil {\n+\t\treturn fmt.Errorf(\"close fd: %w\", err)\n+\t}\n+\n+\t// Avoiding race conditions: Depending on the tcp connection, there might be\n+\t// caching issues when checking the size immediately after write.\n+\t// Hence we try avoiding them by checking the resulting size on a different connection.\n+\tif c.size < 0 {\n+\t\t// Without knowing the size, we cannot do size checks.\n+\t\t// Falling back to a sleep of 1s for sake of hope.\n+\t\ttime.Sleep(1 * time.Second)\n+\t\treturn nil\n+\t}\n+\tsizeOk := false\n+\tsizeLastSeen := int64(0)\n+\tfor retry := 0; retry < 5; retry++ {\n+\t\tfs.Debugf(c.remote, \"checking file size: try %d/5\", retry)\n+\t\tobj, err := c.fs.NewObject(c.ctx, c.remote)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"get uploaded obj: %w\", err)\n+\t\t}\n+\t\tsizeLastSeen = obj.Size()\n+\t\tif obj.Size() == c.size {\n+\t\t\tsizeOk = true\n+\t\t\tbreak\n+\t\t}\n+\t\ttime.Sleep(1 * time.Second)\n+\t}\n+\n+\tif !sizeOk {\n+\t\treturn fmt.Errorf(\"incorrect size after upload: got %d, want %d\", sizeLastSeen, c.size)\n+\t}\n+\n+\treturn nil\n+}\n+\n+// WriteAt implements fs.WriteAt.\n+func (c *writerAt) WriteAt(buffer []byte, offset int64) (n int, err error) {\n+\tcontentLength := len(buffer)\n+\n+\tinSHA1Bytes := sha1.Sum(buffer)\n+\tinSHA1 := hex.EncodeToString(inSHA1Bytes[:])\n+\n+\t// get target hash\n+\toutChecksum, err := c.fileChecksum(c.ctx, offset, int64(contentLength))\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\toutSHA1 := outChecksum.SHA1\n+\n+\tif outSHA1 == \"\" || inSHA1 == \"\" {\n+\t\treturn 0, fmt.Errorf(\"expect both hashes to be filled: src: %q, target: %q\", inSHA1, outSHA1)\n+\t}\n+\n+\t// check hash of buffer, skip if fits\n+\tif inSHA1 == outSHA1 {\n+\t\treturn contentLength, nil\n+\t}\n+\n+\t// upload buffer with offset if necessary\n+\tif _, err := c.filePWrite(c.ctx, offset, buffer); err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn contentLength, nil\n+}\n+\n+// Call pcloud file_open using folderid and name with O_CREAT and O_WRITE flags, see [API Doc.]\n+// [API Doc]: https://docs.pcloud.com/methods/fileops/file_open.html\n+func fileOpenNew(ctx context.Context, c *rest.Client, srcFs *Fs, directoryID, filename string) (*api.FileOpenResponse, error) {\n+\topts := rest.Opts{\n+\t\tMethod:           \"PUT\",\n+\t\tPath:             \"/file_open\",\n+\t\tParameters:       url.Values{},\n+\t\tTransferEncoding: []string{\"identity\"}, // pcloud doesn't like chunked encoding\n+\t\tExtraHeaders: map[string]string{\n+\t\t\t\"Connection\": \"keep-alive\",\n+\t\t},\n+\t}\n+\tfilename = srcFs.opt.Enc.FromStandardName(filename)\n+\topts.Parameters.Set(\"name\", filename)\n+\topts.Parameters.Set(\"folderid\", dirIDtoNumber(directoryID))\n+\topts.Parameters.Set(\"flags\", \"0x0042\") // O_CREAT, O_WRITE\n+\n+\tresult := &api.FileOpenResponse{}\n+\terr := srcFs.pacer.CallNoRetry(func() (bool, error) {\n+\t\tresp, err := c.CallJSON(ctx, &opts, nil, result)\n+\t\terr = result.Error.Update(err)\n+\t\treturn shouldRetry(ctx, resp, err)\n+\t})\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"open new file descriptor: %w\", err)\n+\t}\n+\treturn result, nil\n+}\n+\n+// Call pcloud file_checksum, see [API Doc.]\n+// [API Doc]: https://docs.pcloud.com/methods/fileops/file_checksum.html\n+func (c *writerAt) fileChecksum(\n+\tctx context.Context,\n+\toffset, count int64,\n+) (*api.FileChecksumResponse, error) {\n+\topts := rest.Opts{\n+\t\tMethod:           \"PUT\",\n+\t\tPath:             \"/file_checksum\",\n+\t\tParameters:       url.Values{},\n+\t\tTransferEncoding: []string{\"identity\"}, // pcloud doesn't like chunked encoding\n+\t\tExtraHeaders: map[string]string{\n+\t\t\t\"Connection\": \"keep-alive\",\n+\t\t},\n+\t}\n+\topts.Parameters.Set(\"fd\", strconv.FormatInt(c.fd, 10))\n+\topts.Parameters.Set(\"offset\", strconv.FormatInt(offset, 10))\n+\topts.Parameters.Set(\"count\", strconv.FormatInt(count, 10))\n+\n+\tresult := &api.FileChecksumResponse{}\n+\terr := c.fs.pacer.CallNoRetry(func() (bool, error) {\n+\t\tresp, err := c.client.CallJSON(ctx, &opts, nil, result)\n+\t\terr = result.Error.Update(err)\n+\t\treturn shouldRetry(ctx, resp, err)\n+\t})\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"checksum of fd %d with offset %d and size %d: %w\", c.fd, offset, count, err)\n+\t}\n+\treturn result, nil\n+}\n+\n+// Call pcloud file_pwrite, see [API Doc.]\n+// [API Doc]: https://docs.pcloud.com/methods/fileops/file_pwrite.html\n+func (c *writerAt) filePWrite(\n+\tctx context.Context,\n+\toffset int64,\n+\tbuf []byte,\n+) (*api.FilePWriteResponse, error) {\n+\tcontentLength := int64(len(buf))\n+\topts := rest.Opts{\n+\t\tMethod:           \"PUT\",\n+\t\tPath:             \"/file_pwrite\",\n+\t\tBody:             bytes.NewReader(buf),\n+\t\tContentLength:    &contentLength,\n+\t\tParameters:       url.Values{},\n+\t\tTransferEncoding: []string{\"identity\"}, // pcloud doesn't like chunked encoding\n+\t\tClose:            false,\n+\t\tExtraHeaders: map[string]string{\n+\t\t\t\"Connection\": \"keep-alive\",\n+\t\t},\n+\t}\n+\topts.Parameters.Set(\"fd\", strconv.FormatInt(c.fd, 10))\n+\topts.Parameters.Set(\"offset\", strconv.FormatInt(offset, 10))\n+\n+\tresult := &api.FilePWriteResponse{}\n+\terr := c.fs.pacer.CallNoRetry(func() (bool, error) {\n+\t\tresp, err := c.client.CallJSON(ctx, &opts, nil, result)\n+\t\terr = result.Error.Update(err)\n+\t\treturn shouldRetry(ctx, resp, err)\n+\t})\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"write %d bytes to fd %d with offset %d: %w\", contentLength, c.fd, offset, err)\n+\t}\n+\treturn result, nil\n+}\n+\n+// Call pcloud file_close, see [API Doc.]\n+// [API Doc]: https://docs.pcloud.com/methods/fileops/file_close.html\n+func (c *writerAt) fileClose(ctx context.Context) (*api.FileCloseResponse, error) {\n+\topts := rest.Opts{\n+\t\tMethod:           \"PUT\",\n+\t\tPath:             \"/file_close\",\n+\t\tParameters:       url.Values{},\n+\t\tTransferEncoding: []string{\"identity\"}, // pcloud doesn't like chunked encoding\n+\t\tClose:            true,\n+\t}\n+\topts.Parameters.Set(\"fd\", strconv.FormatInt(c.fd, 10))\n+\n+\tresult := &api.FileCloseResponse{}\n+\terr := c.fs.pacer.CallNoRetry(func() (bool, error) {\n+\t\tresp, err := c.client.CallJSON(ctx, &opts, nil, result)\n+\t\terr = result.Error.Update(err)\n+\t\treturn shouldRetry(ctx, resp, err)\n+\t})\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"close file descriptor: %w\", err)\n+\t}\n+\treturn result, nil\n+}\n", "instance_id": "rclone__rclone-7949", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the goal of implementing block-level sync for uploads to the pCloud backend in rclone, focusing on chunked uploads to handle large files and mitigate rate limiting issues. It provides context about the problem (e.g., inability to upload large files due to connection issues and rate limits) and specifies the desired changes, such as implementing `upload_chunk_size` and `SetModTime`. The associated forum posts and API documentation references add useful background. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for edge cases like partial uploads failing mid-chunk or specific constraints on chunk sizes. Additionally, while the intent of using an undocumented API for `SetModTime` is mentioned, the risks or fallback behaviors for such reverse-engineered features are not discussed. Overall, the statement is valid and clear but lacks comprehensive coverage of edge cases and potential pitfalls, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated at 0.75, placing it in the \"Hard\" category due to several factors. First, the scope of code changes is significant, involving multiple files (`pcloud.go`, `api/types.go`, and a new `writer_at.go`) and adding substantial new functionality like chunked uploads and random-access writing via the `fileops` API. This requires a deep understanding of the rclone codebase, particularly how backends interact with the core filesystem abstraction, as seen in the implementation of `OpenWriterAt` and `writerAt`. Second, the technical concepts involved are complex, including HTTP connection management (e.g., handling single-connection clients to avoid race conditions with file descriptors), API interactions with pCloud's `fileops` endpoints, and hash-based chunk verification (using SHA1 to skip unchanged chunks). Third, the use of an undocumented, reverse-engineered API for `SetModTime` introduces additional complexity and risk, requiring careful error handling and potential fallback logic. Fourth, edge cases and error handling are non-trivial; the code addresses issues like connection resets, size verification retries, and hash mismatches, but the problem statement does not fully specify all scenarios (e.g., network interruptions during chunk uploads or API rate limiting behavior). Finally, the changes impact the backend's architecture by introducing partial upload support, which could affect performance and compatibility. While not at the extreme end of difficulty (e.g., no system-level or distributed systems challenges), this task demands a strong grasp of Go, HTTP client intricacies, and rclone's design, justifying a score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
