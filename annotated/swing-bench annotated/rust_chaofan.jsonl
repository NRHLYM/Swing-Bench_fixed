{"problem_statement": "0.4.37 semver-incompatibly removes trait bounds on `DateTime`\nHello,\r\n\r\nThe following code compile in 0.4.35 but breaks in 0.4.37:\r\n\r\n```rust\r\nuse chrono::TimeZone;\r\nuse chrono::DateTime;\r\n\r\n#[derive(Clone, Debug)]\r\npub struct MyDateTime<T: TimeZone>(DateTime<T>);\r\n\r\nimpl<T: TimeZone> Copy for MyDateTime<T> where T::Offset: Copy {}\r\n```\r\n\r\nWith 0.4.37:\r\n```\r\nerror[E0204]: the trait `Copy` cannot be implemented for this type\r\n --> src/main.rs:7:28\r\n  |\r\n5 | pub struct MyDateTime<T: TimeZone>(chrono::DateTime<T>);\r\n  |                                    ------------------- this field does not implement `Copy`\r\n6 |\r\n7 | impl<T: TimeZone> Copy for MyDateTime<T> where T::Offset: Copy {}\r\n  |                            ^^^^^^^^^^^^^\r\n  |\r\nnote: the `Copy` impl for `DateTime<T>` requires that `T: Copy`\r\n --> src/main.rs:5:36\r\n  |\r\n5 | pub struct MyDateTime<T: TimeZone>(chrono::DateTime<T>);\r\n  |                                    ^^^^^^^^^^^^^^^^^^^\r\nhelp: consider further restricting this bound\r\n  |\r\n7 | impl<T: TimeZone + Copy> Copy for MyDateTime<T> where T::Offset: Copy {}\r\n```\r\nThis is probably due to https://github.com/chronotope/chrono/pull/1492\n", "patch": "diff --git a/CITATION.cff b/CITATION.cff\nindex 0e44adfac8..96cbf16cdd 100644\n--- a/CITATION.cff\n+++ b/CITATION.cff\n@@ -3,8 +3,8 @@ cff-version: 1.2.0\n message: Please cite this crate using these information.\n \n # Version information.\n-date-released: 2024-03-27\n-version: 0.4.37\n+date-released: 2024-04-15\n+version: 0.4.38\n \n # Project information.\n abstract: Date and time library for Rust\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 4a555d21ac..e84c7eb8a7 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"chrono\"\n-version = \"0.4.37\"\n+version = \"0.4.38\"\n description = \"Date and time library for Rust\"\n homepage = \"https://github.com/chronotope/chrono\"\n documentation = \"https://docs.rs/chrono/\"\ndiff --git a/src/datetime/mod.rs b/src/datetime/mod.rs\nindex 366bbe6ad7..28cc64ab79 100644\n--- a/src/datetime/mod.rs\n+++ b/src/datetime/mod.rs\n@@ -46,7 +46,7 @@ mod tests;\n /// There are some constructors implemented here (the `from_*` methods), but\n /// the general-purpose constructors are all via the methods on the\n /// [`TimeZone`](./offset/trait.TimeZone.html) implementations.\n-#[derive(Copy, Clone)]\n+#[derive(Clone)]\n #[cfg_attr(\n     any(feature = \"rkyv\", feature = \"rkyv-16\", feature = \"rkyv-32\", feature = \"rkyv-64\"),\n     derive(Archive, Deserialize, Serialize),\n@@ -1408,6 +1408,15 @@ impl<Tz: TimeZone> Timelike for DateTime<Tz> {\n     }\n }\n \n+// We don't store a field with the `Tz` type, so it doesn't need to influence whether `DateTime` can\n+// be `Copy`. Implement it manually if the two types we do have are `Copy`.\n+impl<Tz: TimeZone> Copy for DateTime<Tz>\n+where\n+    <Tz as TimeZone>::Offset: Copy,\n+    NaiveDateTime: Copy,\n+{\n+}\n+\n impl<Tz: TimeZone, Tz2: TimeZone> PartialEq<DateTime<Tz2>> for DateTime<Tz> {\n     fn eq(&self, other: &DateTime<Tz2>) -> bool {\n         self.datetime == other.datetime\n", "instance_id": "chronotope__chrono-1573", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a breaking change in version 0.4.37 of the `chrono` library due to the removal of trait bounds on `DateTime`, which causes compilation errors in user code that previously worked in version 0.4.35. The provided code snippet and error message help illustrate the problem effectively. Additionally, a reference to the related pull request (PR #1492) provides context for the change. However, the statement lacks explicit mention of the expected resolution or desired behavior (e.g., whether the goal is to revert the change, adjust the trait bounds, or provide a workaround). There are also no details about potential edge cases or broader implications of the change. Thus, while the issue is valid and mostly clear, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of the code changes is relatively small, primarily involving modifications to the `DateTime` struct in a single file (`datetime/mod.rs`). The changes include removing the `Copy` derive and adding a manual `Copy` implementation with specific trait bounds, which is a targeted fix. However, understanding the issue requires familiarity with Rust's trait system, particularly how `Copy` and trait bounds interact with generic types and associated types (`T::Offset`), which adds a moderate level of complexity. Additionally, the problem involves reasoning about backward compatibility and the impact of trait bound changes on downstream users, which requires a nuanced understanding of Rust's type system and library design. While the code change itself is not extensive, and no significant edge cases or error handling are explicitly required beyond the provided fix, the conceptual depth (trait bounds and generics) and the need to understand the broader context of the `chrono` library's design push this slightly above an \"easy\" rating. A score of 0.45 reflects a medium difficulty problem that requires understanding multiple concepts but does not involve extensive architectural changes or complex edge case handling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Consider making serde dependency optional (as a feature)\nFirst: Thanks for writing & publishing argh as libre software! It's been a joy to use and solves the problem without dependencies. Or at least it used to, hence this issue.\r\n\r\nWhere I'm coming from: I'm trying to keep my list of dependencies (and my binary size, but that's secondary) as small as possible for my (rust) programs.\r\n\r\nMy question: Would you consider putting the new serde dependency behind a feature flag? Then I could use argh without having another dependency to audit. Also it looks like my program would save a few bytes in binary size (although in all honesty the binary-size impact of serde seems rather minor, according to [`cargo bloat`](https://github.com/RazrFalcon/cargo-bloat)).\r\n\r\nThanks \ud83d\ude42 \n", "patch": "diff --git a/argh/Cargo.toml b/argh/Cargo.toml\nindex b310dd4..be9b44a 100644\n--- a/argh/Cargo.toml\n+++ b/argh/Cargo.toml\n@@ -20,5 +20,6 @@ once_cell = \"1.10.0\"\n trybuild = \"1.0.63\"\n \n [features]\n-default = [\"help\"]\n+default = [\"help\", \"serde\"]\n help = [\"argh_derive/help\"]\n+serde = [\"argh_shared/serde\"]\ndiff --git a/argh_shared/Cargo.toml b/argh_shared/Cargo.toml\nindex 8e07308..de65044 100644\n--- a/argh_shared/Cargo.toml\n+++ b/argh_shared/Cargo.toml\n@@ -9,4 +9,4 @@ repository = \"https://github.com/google/argh\"\n readme = \"README.md\"\n \n [dependencies]\n-serde = { version = \"1\", features = [\"derive\"] }\n+serde = { version = \"1\", optional = true, features = [\"derive\"] }\ndiff --git a/argh_shared/src/lib.rs b/argh_shared/src/lib.rs\nindex c9306e0..ae44e5b 100644\n--- a/argh_shared/src/lib.rs\n+++ b/argh_shared/src/lib.rs\n@@ -15,7 +15,8 @@ pub struct CommandInfo<'a> {\n }\n \n /// Information about the command line arguments for a given command.\n-#[derive(Debug, Default, PartialEq, Eq, Clone, serde::Serialize)]\n+#[derive(Debug, Default, PartialEq, Eq, Clone)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub struct CommandInfoWithArgs<'a> {\n     /// The name of the command.\n     pub name: &'a str,\n@@ -36,7 +37,8 @@ pub struct CommandInfoWithArgs<'a> {\n }\n \n /// Information about a documented error code.\n-#[derive(Debug, PartialEq, Eq, serde::Serialize)]\n+#[derive(Debug, PartialEq, Eq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub struct ErrorCodeInfo<'a> {\n     /// The code value.\n     pub code: i32,\n@@ -45,7 +47,8 @@ pub struct ErrorCodeInfo<'a> {\n }\n \n /// Information about positional arguments\n-#[derive(Debug, PartialEq, Eq, serde::Serialize)]\n+#[derive(Debug, PartialEq, Eq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub struct PositionalInfo<'a> {\n     /// Name of the argument.\n     pub name: &'a str,\n@@ -63,7 +66,8 @@ pub struct PositionalInfo<'a> {\n /// Dynamic subcommands do not implement\n /// get_args_info(), so the command field\n /// only contains the name and description.\n-#[derive(Debug, Default, PartialEq, Eq, Clone, serde::Serialize)]\n+#[derive(Debug, Default, PartialEq, Eq, Clone)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub struct SubCommandInfo<'a> {\n     /// The subcommand name.\n     pub name: &'a str,\n@@ -72,7 +76,8 @@ pub struct SubCommandInfo<'a> {\n }\n \n /// Information about a flag or option.\n-#[derive(Debug, Default, PartialEq, Eq, serde::Serialize)]\n+#[derive(Debug, Default, PartialEq, Eq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub struct FlagInfo<'a> {\n     /// The kind of flag.\n     pub kind: FlagInfoKind<'a>,\n@@ -92,7 +97,8 @@ pub struct FlagInfo<'a> {\n }\n \n /// The kind of flags.\n-#[derive(Debug, Default, PartialEq, Eq, serde::Serialize)]\n+#[derive(Debug, Default, PartialEq, Eq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub enum FlagInfoKind<'a> {\n     /// switch represents a boolean flag,\n     #[default]\n@@ -104,7 +110,8 @@ pub enum FlagInfoKind<'a> {\n \n /// The optionality defines the requirements related\n /// to the presence of the argument on the command line.\n-#[derive(Debug, Default, PartialEq, Eq, serde::Serialize)]\n+#[derive(Debug, Default, PartialEq, Eq)]\n+#[cfg_attr(feature = \"serde\", derive(serde::Serialize))]\n pub enum Optionality {\n     /// Required indicates the argument is required\n     /// exactly once.\n", "instance_id": "google__argh-188", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent: the user requests that the `serde` dependency in the `argh` library be made optional behind a feature flag to reduce dependencies and binary size for users who do not need serialization support. The goal is straightforward, and the context (minimizing dependencies and binary size) is provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential impacts on users who rely on `serde` by default, nor does it specify expectations for backward compatibility or how the feature flag should behave in different scenarios (e.g., whether it should be enabled by default). Additionally, no examples or test cases are provided to validate the change. Despite these minor gaps, the intent and scope are reasonably clear, especially when paired with the provided code changes, which illustrate the proposed solution. Thus, a score of 2 (Mostly Clear) is appropriate.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" range (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are limited to a few files (`Cargo.toml` for both `argh` and `argh_shared`, and `lib.rs` in `argh_shared`). The modifications involve adding a feature flag for `serde` in the dependency declaration and using conditional compilation attributes (`#[cfg_attr(feature = \"serde\", ...)]`) to make serialization optional. The changes are localized and do not impact the broader architecture of the library. The amount of code change is minimal, focusing on configuration and attribute annotations rather than complex logic.\n\n2. **Number of Technical Concepts:** The solution requires understanding of Rust's feature flags and conditional compilation, which are intermediate-level concepts in Rust but not particularly complex for someone familiar with the language. Knowledge of `Cargo.toml` dependency management and the `serde` crate's derive macros is also needed, but these are common in Rust development. No advanced algorithms, design patterns, or domain-specific knowledge are required.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention specific edge cases, and the code changes do not introduce new error handling logic. However, there is a minor implicit consideration: ensuring that users who do not enable the `serde` feature still have a functional library (i.e., no compilation errors). The provided changes handle this by using `cfg_attr`, which is a standard approach. Additionally, making `serde` optional by default (as seen in the updated `default` feature in `Cargo.toml`) could affect existing users, but this is more of a design decision than a technical edge case requiring complex handling.\n\n4. **Overall Complexity:** The task is straightforward for a Rust developer with moderate experience. It involves applying a well-known pattern (optional dependencies via feature flags) without needing deep understanding of the `argh` library's internals or making significant architectural decisions. The primary challenge might be ensuring that downstream users are not inadvertently broken by the change, but this is more of a documentation or communication issue than a coding challenge.\n\nGiven these factors, a difficulty score of 0.25 reflects the simplicity of the task, requiring only basic to intermediate Rust knowledge and minimal code changes with low risk of introducing complex issues.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add `--release` flag\nI think five steps are necessary:\r\n1. Modify the `TestFuzzWithDeprecations` struct to add the `--release` flag: https://github.com/trailofbits/test-fuzz/blob/42bacfdc5f1b6095862df0e5835b846f3bb5b94e/cargo-test-fuzz/src/bin/cargo_test_fuzz/transition.rs#L26\r\n2. Add a `release` field to this `TestFuzz` struct: https://github.com/trailofbits/test-fuzz/blob/42bacfdc5f1b6095862df0e5835b846f3bb5b94e/cargo-test-fuzz/src/lib.rs#L74\r\n3. Modify this `From` implementation to set the `release` field in the `TestFuzz` struct when that field is set in the `TestFuzzWithDeprecations` struct: https://github.com/trailofbits/test-fuzz/blob/42bacfdc5f1b6095862df0e5835b846f3bb5b94e/cargo-test-fuzz/src/bin/cargo_test_fuzz/transition.rs#L133\r\n4. Modify the `build` function so that it checks whether the `release` field is set, and adds `--release` to the flags passed to `cargo` when it is: https://github.com/trailofbits/test-fuzz/blob/42bacfdc5f1b6095862df0e5835b846f3bb5b94e/cargo-test-fuzz/src/lib.rs#L231\r\n5. Update the \"Usage\" section in the README: https://github.com/trailofbits/test-fuzz/blob/master/README.md#usage\n", "patch": "diff --git a/README.md b/README.md\nindex 54852cb6..c1fa3b60 100644\n--- a/README.md\n+++ b/README.md\n@@ -312,6 +312,7 @@ Options:\n   -p, --package <PACKAGE>         Package containing fuzz target\n       --persistent                Enable persistent mode fuzzing\n       --pretty                    Pretty-print debug output when displaying/replaying\n+      --release                   Build in release mode\n       --replay <OBJECT>           Replay corpus, crashes, hangs, or work queue. By default, an\n                                   uninstrumented fuzz target is used. To replay with instrumentation\n                                   append `-instrumented` to <OBJECT>, e.g., --replay\n", "instance_id": "trailofbits__test-fuzz-517", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in outlining the goal of adding a `--release` flag to the `cargo-test-fuzz` tool. It provides specific steps and links to the relevant parts of the codebase, which helps in understanding the scope of the task. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention how the `--release` flag should interact with other flags or configurations, nor does it discuss potential edge cases or constraints (e.g., compatibility issues with other build options or environments). Additionally, while the steps are detailed, there are no examples or expected outcomes provided to validate the implementation. Thus, while the problem is valid and mostly clear, it lacks some minor but important details that could affect the implementation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes involve multiple files and structs (`TestFuzzWithDeprecations`, `TestFuzz`, and the `build` function), as well as updating documentation in the README. However, the modifications are relatively straightforward\u2014adding a field, updating a conversion implementation, and conditionally passing a flag to a `cargo` command. The changes do not impact the system's architecture significantly and are localized to specific parts of the codebase. The amount of code change appears minimal based on the provided diff and the described steps.\n\n2. **Number of Technical Concepts:** The problem requires basic familiarity with Rust, including struct definitions, field access, and implementing traits like `From`. It also involves understanding how command-line arguments are processed and passed to external tools like `cargo`. These concepts are not particularly complex for someone with moderate Rust experience. No advanced algorithms, design patterns, or domain-specific knowledge (beyond basic Rust tooling) are required.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not mention specific edge cases or error conditions related to the `--release` flag. However, a developer might need to consider minor edge cases, such as ensuring the flag does not conflict with other build configurations or handling cases where `cargo` fails to build in release mode. These considerations are not complex and do not significantly increase the difficulty.\n\n4. **Overall Assessment:** The task requires understanding some code logic and making simple modifications across a few files. It does not involve deep architectural changes or complex interactions within the codebase. The primary challenge lies in ensuring the flag is correctly propagated through the structs and applied to the `cargo` command, which is a relatively simple feature addition. Therefore, a difficulty score of 0.35 reflects the ease of the task while acknowledging the need for minor cross-file coordination and attention to detail.\n\nIn summary, this problem is approachable for a developer with basic to intermediate Rust skills and does not pose significant technical challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Allow async connections to update auth when reconnecting\n#291 will add support for reconnections to the async client. Once that lands, we should give the app an opportunity to update its connection settings (address, auth, etc.), because the original settings may not be valid by the time a reconnection is required.\r\n\r\nAdding something like an `on_reconnect` callback on the builder would allow us to do this without any breaking changes, like so:\r\n\r\n```rust\r\nlet client = rust_socketio::asynchronous::ClientBuilder::new(server)\r\n    .on_reconnect(|| {\r\n        async move {\r\n            // Do something async...\r\n            sleep(Duration::from_secs(5)).await;\r\n\r\n            // Generate and return new config\r\n            let mut settings = ReconnectSettings::new();\r\n            settings.auth(json!({\"token\": \"foo\"}));\r\n            settings\r\n        }\r\n        .boxed()\r\n    })\r\n```\n", "patch": "diff --git a/socketio/src/asynchronous/client/builder.rs b/socketio/src/asynchronous/client/builder.rs\nindex 496f5a4d..44710e19 100644\n--- a/socketio/src/asynchronous/client/builder.rs\n+++ b/socketio/src/asynchronous/client/builder.rs\n@@ -11,8 +11,10 @@ use url::Url;\n use crate::{error::Result, Event, Payload, TransportType};\n \n use super::{\n-    callback::{Callback, DynAsyncAnyCallback, DynAsyncCallback},\n-    client::Client,\n+    callback::{\n+        Callback, DynAsyncAnyCallback, DynAsyncCallback, DynAsyncReconnectSettingsCallback,\n+    },\n+    client::{Client, ReconnectSettings},\n };\n use crate::asynchronous::socket::Socket as InnerSocket;\n \n@@ -21,9 +23,10 @@ use crate::asynchronous::socket::Socket as InnerSocket;\n /// namespace is specified, the default namespace `/` is taken. The `connect` method\n /// acts the `build` method and returns a connected [`Client`].\n pub struct ClientBuilder {\n-    address: String,\n+    pub(crate) address: String,\n     pub(crate) on: HashMap<Event, Callback<DynAsyncCallback>>,\n     pub(crate) on_any: Option<Callback<DynAsyncAnyCallback>>,\n+    pub(crate) on_reconnect: Option<Callback<DynAsyncReconnectSettingsCallback>>,\n     pub(crate) namespace: String,\n     tls_config: Option<TlsConnector>,\n     opening_headers: Option<HeaderMap>,\n@@ -82,6 +85,7 @@ impl ClientBuilder {\n             address: address.into(),\n             on: HashMap::new(),\n             on_any: None,\n+            on_reconnect: None,\n             namespace: \"/\".to_owned(),\n             tls_config: None,\n             opening_headers: None,\n@@ -192,6 +196,42 @@ impl ClientBuilder {\n         self\n     }\n \n+    /// Registers a callback for reconnect events. The event handler must return\n+    /// a [ReconnectSettings] struct with the settings that should be updated.\n+    ///\n+    /// # Example\n+    /// ```rust\n+    /// use rust_socketio::{asynchronous::{ClientBuilder, ReconnectSettings}};\n+    /// use futures_util::future::FutureExt;\n+    /// use serde_json::json;\n+    ///\n+    /// #[tokio::main]\n+    /// async fn main() {\n+    ///     let client = ClientBuilder::new(\"http://localhost:4200/\")\n+    ///         .namespace(\"/admin\")\n+    ///         .on_reconnect(|| {\n+    ///             async {\n+    ///                 let mut settings = ReconnectSettings::new();\n+    ///                 settings.address(\"http://server?test=123\");\n+    ///                 settings.auth(json!({ \"token\": \"abc\" }));\n+    ///                 settings\n+    ///             }.boxed()\n+    ///         })\n+    ///         .connect()\n+    ///         .await;\n+    /// }\n+    /// ```\n+    pub fn on_reconnect<F>(mut self, callback: F) -> Self\n+    where\n+        F: for<'a> std::ops::FnMut() -> BoxFuture<'static, ReconnectSettings>\n+            + 'static\n+            + Send\n+            + Sync,\n+    {\n+        self.on_reconnect = Some(Callback::<DynAsyncReconnectSettingsCallback>::new(callback));\n+        self\n+    }\n+\n     /// Registers a Callback for all [`crate::event::Event::Custom`] and [`crate::event::Event::Message`].\n     ///\n     /// # Example\ndiff --git a/socketio/src/asynchronous/client/callback.rs b/socketio/src/asynchronous/client/callback.rs\nindex 73506fbd..3188b175 100644\n--- a/socketio/src/asynchronous/client/callback.rs\n+++ b/socketio/src/asynchronous/client/callback.rs\n@@ -6,7 +6,7 @@ use std::{\n \n use crate::{Event, Payload};\n \n-use super::client::Client;\n+use super::client::{Client, ReconnectSettings};\n \n /// Internal type, provides a way to store futures and return them in a boxed manner.\n pub(crate) type DynAsyncCallback =\n@@ -16,6 +16,9 @@ pub(crate) type DynAsyncAnyCallback = Box<\n     dyn for<'a> FnMut(Event, Payload, Client) -> BoxFuture<'static, ()> + 'static + Send + Sync,\n >;\n \n+pub(crate) type DynAsyncReconnectSettingsCallback =\n+    Box<dyn for<'a> FnMut() -> BoxFuture<'static, ReconnectSettings> + 'static + Send + Sync>;\n+\n pub(crate) struct Callback<T> {\n     inner: T,\n }\n@@ -77,3 +80,29 @@ impl Callback<DynAsyncAnyCallback> {\n         }\n     }\n }\n+\n+impl Deref for Callback<DynAsyncReconnectSettingsCallback> {\n+    type Target =\n+        dyn for<'a> FnMut() -> BoxFuture<'static, ReconnectSettings> + 'static + Sync + Send;\n+\n+    fn deref(&self) -> &Self::Target {\n+        self.inner.as_ref()\n+    }\n+}\n+\n+impl DerefMut for Callback<DynAsyncReconnectSettingsCallback> {\n+    fn deref_mut(&mut self) -> &mut Self::Target {\n+        self.inner.as_mut()\n+    }\n+}\n+\n+impl Callback<DynAsyncReconnectSettingsCallback> {\n+    pub(crate) fn new<T>(callback: T) -> Self\n+    where\n+        T: for<'a> FnMut() -> BoxFuture<'static, ReconnectSettings> + 'static + Sync + Send,\n+    {\n+        Callback {\n+            inner: Box::new(callback),\n+        }\n+    }\n+}\ndiff --git a/socketio/src/asynchronous/client/client.rs b/socketio/src/asynchronous/client/client.rs\nindex 05b09f74..bece41e7 100644\n--- a/socketio/src/asynchronous/client/client.rs\n+++ b/socketio/src/asynchronous/client/client.rs\n@@ -33,6 +33,33 @@ enum DisconnectReason {\n     Server,\n }\n \n+/// Settings that can be updated before reconnecting to a server\n+#[derive(Default)]\n+pub struct ReconnectSettings {\n+    address: Option<String>,\n+    auth: Option<serde_json::Value>,\n+}\n+\n+impl ReconnectSettings {\n+    pub fn new() -> Self {\n+        Self::default()\n+    }\n+\n+    /// Sets the URL that will be used when reconnecting to the server\n+    pub fn address<T>(&mut self, address: T) -> &mut Self\n+    where\n+        T: Into<String>,\n+    {\n+        self.address = Some(address.into());\n+        self\n+    }\n+\n+    /// Sets the authentication data that will be send in the opening request\n+    pub fn auth(&mut self, auth: serde_json::Value) {\n+        self.auth = Some(auth);\n+    }\n+}\n+\n /// A socket which handles communication with the server. It's initialized with\n /// a specific address as well as an optional namespace to connect to. If `None`\n /// is given the client will connect to the default namespace `\"/\"`.\n@@ -81,7 +108,19 @@ impl Client {\n     }\n \n     pub(crate) async fn reconnect(&mut self) -> Result<()> {\n-        let builder = self.builder.write().await;\n+        let mut builder = self.builder.write().await;\n+\n+        if let Some(config) = builder.on_reconnect.as_mut() {\n+            let reconnect_settings = config().await;\n+            if let Some(address) = reconnect_settings.address {\n+                builder.address = address;\n+            }\n+\n+            if let Some(auth) = reconnect_settings.auth {\n+                self.auth = Some(auth);\n+            }\n+        }\n+\n         let socket = builder.inner_create().await?;\n \n         // New inner socket that can be connected\n@@ -548,7 +587,10 @@ mod test {\n     };\n \n     use crate::{\n-        asynchronous::client::{builder::ClientBuilder, client::Client},\n+        asynchronous::{\n+            client::{builder::ClientBuilder, client::Client},\n+            ReconnectSettings,\n+        },\n         error::Result,\n         packet::{Packet, PacketId},\n         Payload, TransportType,\n@@ -700,13 +742,28 @@ mod test {\n     async fn socket_io_reconnect_integration() -> Result<()> {\n         static CONNECT_NUM: AtomicUsize = AtomicUsize::new(0);\n         static MESSAGE_NUM: AtomicUsize = AtomicUsize::new(0);\n+        static ON_RECONNECT_CALLED: AtomicUsize = AtomicUsize::new(0);\n \n         let url = crate::test::socket_io_restart_server();\n \n-        let socket = ClientBuilder::new(url)\n+        let socket = ClientBuilder::new(url.clone())\n             .reconnect(true)\n             .max_reconnect_attempts(100)\n             .reconnect_delay(100, 100)\n+            .on_reconnect(move || {\n+                let url = url.clone();\n+                async move {\n+                    ON_RECONNECT_CALLED.fetch_add(1, Ordering::Release);\n+\n+                    let mut settings = ReconnectSettings::new();\n+\n+                    // Try setting the address to what we already have, just\n+                    // to test. This is not strictly necessary in real usage.\n+                    settings.address(url.to_string());\n+                    settings\n+                }\n+                .boxed()\n+            })\n             .on(\"open\", |_, socket| {\n                 async move {\n                     CONNECT_NUM.fetch_add(1, Ordering::Release);\n@@ -753,6 +810,10 @@ mod test {\n \n         assert_eq!(load(&CONNECT_NUM), 2, \"should connect twice\");\n         assert_eq!(load(&MESSAGE_NUM), 2, \"should receive two messages\");\n+        assert!(\n+            load(&ON_RECONNECT_CALLED) > 1,\n+            \"should call on_reconnect at least once\"\n+        );\n \n         socket.disconnect().await?;\n         Ok(())\ndiff --git a/socketio/src/asynchronous/mod.rs b/socketio/src/asynchronous/mod.rs\nindex a0a184aa..2976dd9a 100644\n--- a/socketio/src/asynchronous/mod.rs\n+++ b/socketio/src/asynchronous/mod.rs\n@@ -5,3 +5,4 @@ mod socket;\n #[cfg(feature = \"async\")]\n pub use client::builder::ClientBuilder;\n pub use client::client::Client;\n+pub use client::client::ReconnectSettings;\n", "instance_id": "1c3t3a__rust-socketio-405", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of allowing async connections to update authentication settings during reconnection in a socket.io client. It provides a specific example of how the feature should work with an `on_reconnect` callback and mentions the context of a related issue (#291) for reconnections. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected behavior if the callback fails or returns invalid settings, nor does it mention specific constraints or edge cases (e.g., what happens if the new address is unreachable or if the auth data is malformed). Additionally, the problem statement lacks clarity on how often or under what exact conditions the callback should be invoked during reconnection attempts. Despite these minor gaps, the intent and primary requirements are understandable, supported by a code snippet, making it \"Mostly Clear.\"", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes is moderate, involving multiple files (`builder.rs`, `callback.rs`, `client.rs`, and `mod.rs`) but primarily focused on adding a new callback mechanism for reconnection settings. The changes include introducing a new `ReconnectSettings` struct, updating the `ClientBuilder` to support an `on_reconnect` callback, and modifying the reconnection logic to apply updated settings, which requires understanding the async client architecture and callback system. Second, the technical concepts involved include Rust's async programming with `BoxFuture`, trait bounds for callbacks, and handling of shared state (e.g., `RwLock` in the client builder), which are moderately complex but not overly advanced for someone familiar with Rust. Third, the problem does not explicitly address complex edge cases in the statement, but the code changes imply handling scenarios like updating addresses or auth data dynamically, which could introduce subtle issues (e.g., validation of new settings or race conditions during reconnection). Finally, the impact on the codebase is localized to the async client module without major architectural changes. Overall, this task requires a solid understanding of async Rust and the specific library's design, placing it at a medium difficulty level of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "MINOR: Add validation to the delete_resource_config method in GrpcPlacementService and fill in the test case\n### What type of enhancement is this?\r\n\r\nRefactor\r\n\r\n### What does the enhancement do?\r\n\r\nAdd validation to the delete_resource_config method in GrpcPlacementService and fill in the test case.\r\n\r\nTips:\r\nyou can implement the `ValidateExt` trait on `DeleteResourceConfigRequest`, then invoke ` validate_ext ` function to achieve the validation functionality.\r\n\r\n- you can locate the ValidateExt trait in the file: src\\placement-center\\src\\server\\grpc\\validate.rs\r\n- you need to add validation to the delete_resource_config method in this file:  src\\placement-center\\src\\server\\grpc\\service_placement.rs\r\n- you need to fill in the test case in this file: src\\grpc-clients\\tests\\placement_test.rs\r\n\r\n\r\nreferences:\r\n- https://github.com/robustmq/robustmq/pull/548)https://github.com/robustmq/robustmq/pull/548\r\n\r\n\r\n### Implementation challenges\r\n\r\n_No response_\n", "patch": "diff --git a/src/placement-center/src/server/grpc/service_inner.rs b/src/placement-center/src/server/grpc/service_inner.rs\nindex a9edc72e9..df443fbc3 100644\n--- a/src/placement-center/src/server/grpc/service_inner.rs\n+++ b/src/placement-center/src/server/grpc/service_inner.rs\n@@ -233,6 +233,9 @@ impl PlacementCenterService for GrpcPlacementService {\n         request: Request<DeleteResourceConfigRequest>,\n     ) -> Result<Response<DeleteResourceConfigReply>, Status> {\n         let req = request.into_inner();\n+\n+        let _ = req.validate_ext()?;\n+\n         let data = StorageData::new(\n             StorageDataType::ClusterDeleteResourceConfig,\n             DeleteResourceConfigRequest::encode_to_vec(&req),\ndiff --git a/src/placement-center/src/server/grpc/validate.rs b/src/placement-center/src/server/grpc/validate.rs\nindex e2c9eb3a0..3e201285b 100644\n--- a/src/placement-center/src/server/grpc/validate.rs\n+++ b/src/placement-center/src/server/grpc/validate.rs\n@@ -16,9 +16,9 @@ use std::net::{IpAddr, SocketAddr};\n \n use common_base::error::common::CommonError;\n use protocol::placement_center::placement_center_inner::{\n-    ClusterType, DeleteIdempotentDataRequest, ExistsIdempotentDataRequest,\n-    GetResourceConfigRequest, NodeListRequest, RegisterNodeRequest, SetIdempotentDataRequest,\n-    SetResourceConfigRequest, UnRegisterNodeRequest,\n+    ClusterType, DeleteIdempotentDataRequest, DeleteResourceConfigRequest,\n+    ExistsIdempotentDataRequest, GetResourceConfigRequest, NodeListRequest, RegisterNodeRequest,\n+    SetIdempotentDataRequest, SetResourceConfigRequest, UnRegisterNodeRequest,\n };\n use protocol::placement_center::placement_center_mqtt::GetShareSubLeaderRequest;\n use tonic::Status;\n@@ -174,6 +174,13 @@ impl ValidateExt for ExistsIdempotentDataRequest {\n     }\n }\n \n+impl ValidateExt for DeleteResourceConfigRequest {\n+    fn validate_ext(&self) -> Result<(), Status> {\n+        ensure_param_not_empty(\"cluster_name\", &self.cluster_name)?;\n+        Ok(())\n+    }\n+}\n+\n #[cfg(test)]\n mod validate_test {\n     use super::*;\n", "instance_id": "robustmq__robustmq-745", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in terms of the goal, which is to add validation to the `delete_resource_config` method in `GrpcPlacementService` and to fill in a related test case. It provides specific file locations for the implementation and references a helpful trait (`ValidateExt`) and function (`validate_ext`) to achieve the task. Additionally, it points to a relevant pull request for context. However, there are minor ambiguities: the problem statement does not explicitly define what constitutes valid input for the `DeleteResourceConfigRequest` (though the code changes suggest it involves checking for a non-empty `cluster_name`). Edge cases or specific validation rules beyond this are not mentioned, and the test case requirements are vague, lacking details on what scenarios should be tested. Overall, while the intent and general approach are clear, some finer details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this task is relatively low, falling into the \"Easy\" range (0.2-0.4). The scope of the code changes is limited to two files: adding validation logic in `validate.rs` by implementing the `ValidateExt` trait for `DeleteResourceConfigRequest` (a straightforward check for a non-empty field) and invoking this validation in `service_inner.rs` with a single line of code. The changes are minimal and localized, requiring no deep understanding of the broader codebase or system architecture. The technical concepts involved are basic\u2014understanding Rust traits and implementing a simple validation function using an existing utility (`ensure_param_not_empty`). No complex algorithms, design patterns, or domain-specific knowledge are required beyond familiarity with Rust's syntax and the provided framework. Edge cases appear minimal, with the validation focusing on a single field, and no explicit error handling beyond the provided utility is needed. The task of filling in the test case (though not shown in the diff) is implied to be straightforward given the simplicity of the validation. Overall, this is a simple enhancement that a developer with basic Rust knowledge can handle with minimal effort, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "MINOR:  Add validation to the set_idempotent_data method in GrpcPlacementService and fill in the test case\n### What type of enhancement is this?\r\n\r\nRefactor\r\n\r\n### What does the enhancement do?\r\n\r\nAdd validation to the set_idempotent_data method in GrpcPlacementService and fill in the test case. \r\n\r\nTips:\r\n\r\nYou can implement the `ValidateExt` trait on `SetIdempotentDataRequest`, then invoke ` validate_ext ` function to achieve the validation functionality.\r\n\r\n- you can locate the ValidateExt trait in the file: src\\placement-center\\src\\server\\grpc\\validate.rs\r\n- you need to add validation to the set_idempotent_data method in this file:  src\\placement-center\\src\\server\\grpc\\service_placement.rs\r\n- you need to fill in the test case in this file: src\\grpc-clients\\tests\\placement_test.rs\r\n\r\nreferences:\r\n- https://github.com/robustmq/robustmq/pull/548)https://github.com/robustmq/robustmq/pull/548\r\n\r\n### Implementation challenges\r\n\r\n_No response_\n", "patch": "diff --git a/src/placement-center/src/server/grpc/service_placement.rs b/src/placement-center/src/server/grpc/service_placement.rs\nindex 7f2d99092..ddaca4563 100644\n--- a/src/placement-center/src/server/grpc/service_placement.rs\n+++ b/src/placement-center/src/server/grpc/service_placement.rs\n@@ -262,6 +262,7 @@ impl PlacementCenterService for GrpcPlacementService {\n         request: Request<SetIdempotentDataRequest>,\n     ) -> Result<Response<SetIdempotentDataReply>, Status> {\n         let req = request.into_inner();\n+        let _ = req.validate_ext()?;\n         let data = StorageData::new(\n             StorageDataType::ClusterSetIdempotentData,\n             SetIdempotentDataRequest::encode_to_vec(&req),\ndiff --git a/src/placement-center/src/server/grpc/validate.rs b/src/placement-center/src/server/grpc/validate.rs\nindex d2c69668a..61f5a95f4 100644\n--- a/src/placement-center/src/server/grpc/validate.rs\n+++ b/src/placement-center/src/server/grpc/validate.rs\n@@ -14,8 +14,8 @@\n \n use common_base::error::common::CommonError;\n use protocol::placement_center::placement_center_inner::{\n-    DeleteIdempotentDataRequest, RegisterNodeRequest, SetResourceConfigRequest,\n-    UnRegisterNodeRequest,\n+    DeleteIdempotentDataRequest, RegisterNodeRequest, SetIdempotentDataRequest,\n+    SetResourceConfigRequest, UnRegisterNodeRequest,\n };\n use tonic::Status;\n \n@@ -72,3 +72,21 @@ impl ValidateExt for SetResourceConfigRequest {\n         Ok(())\n     }\n }\n+\n+impl ValidateExt for SetIdempotentDataRequest {\n+    fn validate_ext(&self) -> Result<(), Status> {\n+        if self.cluster_name.is_empty() {\n+            return Err(Status::cancelled(\n+                CommonError::ParameterCannotBeNull(\"cluster name\".to_string()).to_string(),\n+            ));\n+        }\n+\n+        if self.producer_id.is_empty() {\n+            return Err(Status::cancelled(\n+                CommonError::ParameterCannotBeNull(\"producer id\".to_string()).to_string(),\n+            ));\n+        }\n+\n+        Ok(())\n+    }\n+}\n", "instance_id": "robustmq__robustmq-592", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in terms of the goal, which is to add validation to the `set_idempotent_data` method in `GrpcPlacementService` and to fill in a related test case. It provides specific file locations for the implementation and references a relevant pull request for context. Additionally, it offers a tip on using the `ValidateExt` trait for validation. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes valid input for the `SetIdempotentDataRequest` (though the code changes imply validation for non-empty `cluster_name` and `producer_id`). It also lacks details on specific edge cases or constraints beyond the implied ones in the code changes. Lastly, the requirements for the test case in `placement_test.rs` are not specified, leaving room for interpretation on what needs to be tested. Overall, while the intent and general approach are clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively small and localized to two files: `service_placement.rs` (adding a single validation call) and `validate.rs` (implementing the `ValidateExt` trait for `SetIdempotentDataRequest`). The changes do not impact the broader system architecture or require understanding complex interactions between modules. The amount of code added is minimal, focusing on straightforward validation logic.\n\n2. **Technical Concepts Involved:** The problem requires understanding Rust's trait system to implement `ValidateExt`, basic error handling using `Result` and `Status` from the `tonic` crate, and familiarity with the project's custom error type (`CommonError`). These concepts are not particularly complex for a developer with moderate Rust experience. No advanced algorithms, design patterns, or domain-specific knowledge are needed beyond the provided context.\n\n3. **Edge Cases and Error Handling:** The code changes address basic validation for non-empty fields (`cluster_name` and `producer_id`), which are simple edge cases. There are no complex error conditions or performance considerations mentioned or implied in the problem statement or code changes. The error handling logic added is straightforward, returning a `Status::cancelled` with a descriptive message.\n\n4. **Overall Complexity:** While the task involves filling in a test case (not shown in the provided diff), the core implementation is simple and does not require deep knowledge of the codebase beyond the specified files. The problem is self-contained and does not pose significant challenges in terms of debugging or unintended side effects.\n\nGiven these points, a difficulty score of 0.30 reflects an easy task that requires understanding some code logic and making simple modifications. It is slightly above the \"Very Easy\" range due to the need to implement a trait and handle basic validation, but it does not approach the complexity of a medium-difficulty problem.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`on_scroll` is not fired when scrolling via operations\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues.\n\n### Is this issue related to iced?\n\n- [X] My hardware is compatible and my graphics drivers are up-to-date.\n\n### What happened?\n\nWhen scrolling via `scroll_to` or `snap_to`, the callback passed through `on_scroll` is not fired, hiding thus vital information about the current offset of the scrollable that can't be queried through other means.\r\n\r\nIf I show a percentage indicator of some scrollable in my app and perform a `scroll_to` operation, I don't know what's the new relative offset.\n\n### What is the expected behavior?\n\n`on_scroll` should fire on every scrolling event.\n\n### Version\n\ncrates.io release\n\n### Operating System\n\nLinux\n\n### Do you have any log output?\n\n_No response_\nscrollable: emit scroll on viewport change\nCurrently, `Scrollable` emits `on_scroll` only when a scroll interaction is performed. However, it is useful to emit `on_scroll` when the `Viewport` changes for any other reason. For example, `on_scroll` was not previously emitted when the `Viewport` is initialised, which made it impossible to get the initial state of the `Viewport`. This is now possible. Another example is that `on_scroll` is now emitted when the `Viewport` is resized e.g. because of a window resize, which can provide the correct `Viewport` to code that gets it from `on_scroll`.\r\n\r\nNote that, when I updated the `unchanged` check, I removed `last_relative_offset`. There's no need to have it if we also have `last_absolute_offset`, because `last_relative_offset` is calculated based on `last_absolute_offset`.\n", "patch": "diff --git a/widget/src/scrollable.rs b/widget/src/scrollable.rs\nindex f8455392c9..af6a3945fd 100644\n--- a/widget/src/scrollable.rs\n+++ b/widget/src/scrollable.rs\n@@ -12,6 +12,7 @@ use crate::core::touch;\n use crate::core::widget;\n use crate::core::widget::operation::{self, Operation};\n use crate::core::widget::tree::{self, Tree};\n+use crate::core::window;\n use crate::core::{\n     self, Background, Clipboard, Color, Element, Layout, Length, Padding,\n     Pixels, Point, Rectangle, Shell, Size, Theme, Vector, Widget,\n@@ -526,7 +527,7 @@ where\n                             content_bounds,\n                         );\n \n-                        let _ = notify_on_scroll(\n+                        let _ = notify_scroll(\n                             state,\n                             &self.on_scroll,\n                             bounds,\n@@ -564,7 +565,7 @@ where\n \n                         state.y_scroller_grabbed_at = Some(scroller_grabbed_at);\n \n-                        let _ = notify_on_scroll(\n+                        let _ = notify_scroll(\n                             state,\n                             &self.on_scroll,\n                             bounds,\n@@ -597,7 +598,7 @@ where\n                             content_bounds,\n                         );\n \n-                        let _ = notify_on_scroll(\n+                        let _ = notify_scroll(\n                             state,\n                             &self.on_scroll,\n                             bounds,\n@@ -635,7 +636,7 @@ where\n \n                         state.x_scroller_grabbed_at = Some(scroller_grabbed_at);\n \n-                        let _ = notify_on_scroll(\n+                        let _ = notify_scroll(\n                             state,\n                             &self.on_scroll,\n                             bounds,\n@@ -759,7 +760,7 @@ where\n                     content_bounds,\n                 );\n \n-                if notify_on_scroll(\n+                if notify_scroll(\n                     state,\n                     &self.on_scroll,\n                     bounds,\n@@ -807,7 +808,7 @@ where\n                                 Some(cursor_position);\n \n                             // TODO: bubble up touch movements if not consumed.\n-                            let _ = notify_on_scroll(\n+                            let _ = notify_scroll(\n                                 state,\n                                 &self.on_scroll,\n                                 bounds,\n@@ -821,6 +822,17 @@ where\n \n                 event::Status::Captured\n             }\n+            Event::Window(window::Event::RedrawRequested(_)) => {\n+                let _ = notify_viewport(\n+                    state,\n+                    &self.on_scroll,\n+                    bounds,\n+                    content_bounds,\n+                    shell,\n+                );\n+\n+                event::Status::Ignored\n+            }\n             _ => event::Status::Ignored,\n         }\n     }\n@@ -1153,8 +1165,23 @@ pub fn scroll_by<T>(id: Id, offset: AbsoluteOffset) -> Task<T> {\n     )))\n }\n \n-/// Returns [`true`] if the viewport actually changed.\n-fn notify_on_scroll<Message>(\n+fn notify_scroll<Message>(\n+    state: &mut State,\n+    on_scroll: &Option<Box<dyn Fn(Viewport) -> Message + '_>>,\n+    bounds: Rectangle,\n+    content_bounds: Rectangle,\n+    shell: &mut Shell<'_, Message>,\n+) -> bool {\n+    if notify_viewport(state, on_scroll, bounds, content_bounds, shell) {\n+        state.last_scrolled = Some(Instant::now());\n+\n+        true\n+    } else {\n+        false\n+    }\n+}\n+\n+fn notify_viewport<Message>(\n     state: &mut State,\n     on_scroll: &Option<Box<dyn Fn(Viewport) -> Message + '_>>,\n     bounds: Rectangle,\n@@ -1167,6 +1194,11 @@ fn notify_on_scroll<Message>(\n         return false;\n     }\n \n+    let Some(on_scroll) = on_scroll else {\n+        state.last_notified = None;\n+        return false;\n+    };\n+\n     let viewport = Viewport {\n         offset_x: state.offset_x,\n         offset_y: state.offset_y,\n@@ -1186,7 +1218,9 @@ fn notify_on_scroll<Message>(\n             (a - b).abs() <= f32::EPSILON || (a.is_nan() && b.is_nan())\n         };\n \n-        if unchanged(last_relative_offset.x, current_relative_offset.x)\n+        if last_notified.bounds == bounds\n+            && last_notified.content_bounds == content_bounds\n+            && unchanged(last_relative_offset.x, current_relative_offset.x)\n             && unchanged(last_relative_offset.y, current_relative_offset.y)\n             && unchanged(last_absolute_offset.x, current_absolute_offset.x)\n             && unchanged(last_absolute_offset.y, current_absolute_offset.y)\n@@ -1195,12 +1229,8 @@ fn notify_on_scroll<Message>(\n         }\n     }\n \n-    if let Some(on_scroll) = on_scroll {\n-        shell.publish(on_scroll(viewport));\n-    }\n-\n+    shell.publish(on_scroll(viewport));\n     state.last_notified = Some(viewport);\n-    state.last_scrolled = Some(Instant::now());\n \n     true\n }\n", "instance_id": "iced-rs__iced-2438", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `on_scroll` callback in the `Scrollable` widget of the `iced` framework does not fire during certain operations like `scroll_to` or `snap_to`, or during viewport initialization and resizing. The expected behavior is explicitly stated as firing `on_scroll` on every scrolling event or viewport change. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify whether there are particular conditions or configurations under which this behavior should be observed, nor does it mention potential edge cases (e.g., rapid successive scrolls or viewport changes). Additionally, while the description provides context about the impact (e.g., inability to track scroll offset for UI elements like percentage indicators), it lacks detailed examples or test cases to illustrate the issue comprehensively. Thus, while the core issue and goal are clear, the statement misses some finer details that would make it fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is relatively focused, primarily within a single file (`scrollable.rs`) and centered around modifying the behavior of the `on_scroll` callback and related viewport notification logic. The changes involve renaming functions, adding a new event handler for window redraw events, and adjusting the logic for when notifications are sent, which requires understanding the existing event handling and state management in the `iced` framework. Second, the technical concepts involved include familiarity with Rust's event-driven programming model, closures, and widget state management, as well as understanding the `iced` library's architecture for UI components like `Scrollable`. These concepts are moderately complex but not overly advanced for someone with experience in Rust and GUI frameworks. Third, the changes impact how viewport updates are communicated, which could have subtle downstream effects on other parts of an application relying on `on_scroll`, though the architectural impact appears limited. Finally, while the problem statement does not explicitly mention edge cases, the code changes suggest potential considerations like avoiding redundant notifications (via the `unchanged` check) and handling viewport resizing, which adds a layer of complexity to ensure correctness. Overall, this task requires a moderate level of understanding and careful modification but does not involve deep architectural refactoring or highly intricate logic, placing it at a difficulty of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "clap_complete generates invalid powershell code when docs have RIGHT_SINGLE_QUOTATION_MARK\n### Please complete the following tasks\n\n- [X] I have searched the [discussions](https://github.com/clap-rs/clap/discussions)\n- [X] I have searched the [open](https://github.com/clap-rs/clap/issues) and [rejected](https://github.com/clap-rs/clap/issues?q=is%3Aissue+label%3AS-wont-fix+is%3Aclosed) issues\n\n### Rust Version\n\nrustc 1.81.0 (eeb90cda1 2024-09-04)\n\n### Clap Version\n\n4.5.21\n\n### Minimal reproducible code\n\n```rust\r\nuse std::io;\r\n\r\nuse clap::{CommandFactory, Parser};\r\nuse clap_complete::{generate, Shell};\r\n\r\nfn main() {\r\n    let cmd = Cmd::parse();\r\n    cmd.run();\r\n}\r\n\r\n#[derive(Parser, Debug)]\r\nstruct Cmd {\r\n    /// This doesn\u2019t work on powershell.\r\n    #[arg(long)]\r\n    shell: Shell,\r\n}\r\n\r\nimpl Cmd {\r\n    fn run(&self) {\r\n        let cmd = &mut Cmd::command();\r\n        generate(self.shell, cmd, \"sample\", &mut io::stdout());\r\n    }\r\n}\r\n```\r\n\n\n### Steps to reproduce the bug with the above code\n\nOn Windows (PowerShell), run:\r\n\r\n```sh\r\ncargo run -- --shell powershell > bug.ps1\r\n```\r\n\r\nThen try to load this file:\r\n\r\n```console\r\n$ . .\\bug.ps1\r\nParserError: C:\\Users\\fnando\\Downloads\\bug.ps1:24\r\nLine |\r\n  24 |  \u2026 --shell', [CompletionResultType]::ParameterName, 'This doesn\u2019t work o \u2026\r\n     |                                                                 ~\r\n     | Missing ')' in method call.\r\n```\n\n### Actual Behaviour\n\nWhen trying to load the generated completion code, I get an exception because it's not properly escaping `\u2019`.\n\n### Expected Behaviour\n\nclap_complete escapes `\u2019`\n\n### Additional Context\n\n_No response_\n\n### Debug Output\n\n```console\r\n$ cargo run -- --shell powershell\r\n[clap_builder::builder::command]Command::_do_parse\r\n[clap_builder::builder::command]Command::_build: name=\"clap_complete_bug\"\r\n[clap_builder::builder::command]Command::_propagate:clap_complete_bug\r\n[clap_builder::builder::command]Command::_check_help_and_version:clap_complete_bug expand_help_tree=false\r\n[clap_builder::builder::command]Command::long_help_exists\r\n[clap_builder::builder::command]Command::_check_help_and_version: Building default --help\r\n[clap_builder::builder::command]Command::_propagate_global_args:clap_complete_bug\r\n[clap_builder::builder::debug_asserts]Command::_debug_asserts\r\n[clap_builder::builder::debug_asserts]Arg::_debug_asserts:shell\r\n[clap_builder::builder::debug_asserts]Arg::_debug_asserts:help\r\n[clap_builder::builder::debug_asserts]Command::_verify_positionals\r\n[clap_builder::parser::parser]Parser::get_matches_with\r\n[clap_builder::parser::parser]Parser::parse\r\n[clap_builder::parser::parser]Parser::get_matches_with: Begin parsing '\"--shell\"'\r\n[clap_builder::parser::parser]Parser::possible_subcommand: arg=Ok(\"--shell\")\r\n[clap_builder::parser::parser]Parser::get_matches_with: sc=None\r\n[clap_builder::parser::parser]Parser::parse_long_arg\r\n[clap_builder::parser::parser]Parser::parse_long_arg: Does it contain '='...\r\n[clap_builder::parser::parser]Parser::parse_long_arg: Found valid arg or flag '--shell <SHELL>'\r\n[clap_builder::parser::parser]Parser::parse_long_arg(\"shell\"): Found an arg with value 'None'\r\n[clap_builder::parser::parser]Parser::parse_opt_value; arg=shell, val=None, has_eq=false\r\n[clap_builder::parser::parser]Parser::parse_opt_value; arg.settings=ArgFlags(1)\r\n[clap_builder::parser::parser]Parser::parse_opt_value; Checking for val...\r\n[clap_builder::parser::parser]Parser::parse_opt_value: More arg vals required...\r\n[clap_builder::parser::parser]Parser::get_matches_with: After parse_long_arg Opt(\"shell\")\r\n[clap_builder::parser::parser]Parser::get_matches_with: Begin parsing '\"powershell\"'\r\n[clap_builder::parser::arg_matcher]ArgMatcher::needs_more_vals: o=shell, pending=1\r\n[clap_builder::parser::arg_matcher]ArgMatcher::needs_more_vals: expected=1, actual=1\r\n[clap_builder::parser::parser]Parser::resolve_pending: id=\"shell\"\r\n[clap_builder::parser::parser]Parser::react action=Set, identifier=Some(Long), source=CommandLine\r\n[clap_builder::parser::parser]Parser::react: cur_idx:=1\r\n[clap_builder::parser::parser]Parser::remove_overrides: id=\"shell\"\r\n[clap_builder::parser::arg_matcher]ArgMatcher::start_custom_arg: id=\"shell\", source=CommandLine\r\n[clap_builder::builder::command]Command::groups_for_arg: id=\"shell\"\r\n[clap_builder::parser::arg_matcher]ArgMatcher::start_custom_arg: id=\"Cmd\", source=CommandLine\r\n[clap_builder::parser::parser]Parser::push_arg_values: [\"powershell\"]\r\n[clap_builder::parser::parser]Parser::add_single_val_to_arg: cur_idx:=2\r\n[clap_builder::parser::arg_matcher]ArgMatcher::needs_more_vals: o=shell, pending=0\r\n[clap_builder::parser::arg_matcher]ArgMatcher::needs_more_vals: expected=1, actual=0\r\n[clap_builder::parser::parser]Parser::react not enough values passed in, leaving it to the validator to complain\r\n[clap_builder::parser::parser]Parser::add_defaults\r\n[clap_builder::parser::parser]Parser::add_defaults:iter:shell:\r\n[clap_builder::parser::parser]Parser::add_default_value: doesn't have conditional defaults\r\n[clap_builder::parser::parser]Parser::add_default_value:iter:shell: doesn't have default vals\r\n[clap_builder::parser::parser]Parser::add_defaults:iter:help:\r\n[clap_builder::parser::parser]Parser::add_default_value: doesn't have conditional defaults\r\n[clap_builder::parser::parser]Parser::add_default_value:iter:help: doesn't have default vals\r\n[clap_builder::parser::validator]Validator::validate\r\n[clap_builder::builder::command]Command::groups_for_arg: id=\"shell\"\r\n[clap_builder::parser::validator]Conflicts::gather_direct_conflicts id=\"shell\", conflicts=[]\r\n[clap_builder::parser::validator]Conflicts::gather_direct_conflicts id=\"Cmd\", conflicts=[]\r\n[clap_builder::parser::validator]Validator::validate_conflicts\r\n[clap_builder::parser::validator]Validator::validate_exclusive\r\n[clap_builder::parser::validator]Validator::validate_conflicts::iter: id=\"shell\"\r\n[clap_builder::parser::validator]Conflicts::gather_conflicts: arg=\"shell\"\r\n[clap_builder::parser::validator]Conflicts::gather_conflicts: conflicts=[]\r\n[clap_builder::parser::validator]Validator::validate_required: required=ChildGraph([Child { id: \"shell\", children: [] }])\r\n[clap_builder::parser::validator]Validator::gather_requires\r\n[clap_builder::parser::validator]Validator::gather_requires:iter:\"shell\"\r\n[clap_builder::parser::validator]Validator::gather_requires:iter:\"Cmd\"\r\n[clap_builder::parser::validator]Validator::gather_requires:iter:\"Cmd\":group\r\n[clap_builder::parser::validator]Validator::validate_required: is_exclusive_present=false\r\n[clap_builder::parser::arg_matcher]ArgMatcher::get_global_values: global_arg_vec=[]\r\n[clap_builder::builder::command]Command::_build: name=\"clap_complete_bug\"\r\n[clap_builder::builder::command]Command::_propagate:clap_complete_bug\r\n[clap_builder::builder::command]Command::_check_help_and_version:clap_complete_bug expand_help_tree=true\r\n[clap_builder::builder::command]Command::long_help_exists\r\n[clap_builder::builder::command]Command::_check_help_and_version: Building default --help\r\n[clap_builder::builder::command]Command::_propagate_global_args:clap_complete_bug\r\n[clap_builder::builder::debug_asserts]Command::_debug_asserts\r\n[clap_builder::builder::debug_asserts]Arg::_debug_asserts:shell\r\n[clap_builder::builder::debug_asserts]Arg::_debug_asserts:help\r\n[clap_builder::builder::debug_asserts]Command::_verify_positionals\r\n[clap_builder::builder::command]Command::_build_bin_names\r\n[ clap_builder::output::usage]Usage::get_required_usage_from: incls=[], matcher=false, incl_last=true\r\n[ clap_builder::output::usage]Usage::get_required_usage_from: unrolled_reqs=[\"shell\"]\r\n[ clap_builder::output::usage]Usage::get_required_usage_from:iter:\"shell\" arg is_present=false\r\n[ clap_builder::output::usage]Usage::get_required_usage_from: ret_val=[StyledStr(\"\\u{1b}[1m--shell\\u{1b}[0m <SHELL>\")]\r\n\r\nusing namespace System.Management.Automation\r\nusing namespace System.Management.Automation.Language\r\n\r\nRegister-ArgumentCompleter -Native -CommandName 'sample' -ScriptBlock {\r\n    param($wordToComplete, $commandAst, $cursorPosition)\r\n\r\n    $commandElements = $commandAst.CommandElements\r\n    $command = @(\r\n        'sample'\r\n        for ($i = 1; $i -lt $commandElements.Count; $i++) {\r\n            $element = $commandElements[$i]\r\n            if ($element -isnot [StringConstantExpressionAst] -or\r\n                $element.StringConstantType -ne [StringConstantType]::BareWord -or\r\n                $element.Value.StartsWith('-') -or\r\n                $element.Value -eq $wordToComplete) {\r\n                break\r\n        }\r\n        $element.Value\r\n    }) -join ';'\r\n\r\n    $completions = @(switch ($command) {\r\n        'sample' {\r\n            [CompletionResult]::new('--shell', '--shell', [CompletionResultType]::ParameterName, 'This doesn\u2019t work on powershell')\r\n            [CompletionResult]::new('-h', '-h', [CompletionResultType]::ParameterName, 'Print help')\r\n            [CompletionResult]::new('--help', '--help', [CompletionResultType]::ParameterName, 'Print help')\r\n            break\r\n        }\r\n    })\r\n\r\n    $completions.Where{ $_.CompletionText -like \"$wordToComplete*\" } |\r\n        Sort-Object -Property ListItemText\r\n}\r\n```\n", "patch": "diff --git a/clap_complete/src/aot/shells/powershell.rs b/clap_complete/src/aot/shells/powershell.rs\nindex 13e4a61f351..e99c9eac734 100644\n--- a/clap_complete/src/aot/shells/powershell.rs\n+++ b/clap_complete/src/aot/shells/powershell.rs\n@@ -59,7 +59,7 @@ Register-ArgumentCompleter -Native -CommandName '{bin_name}' -ScriptBlock {{\n \n // Escape string inside single quotes\n fn escape_string(string: &str) -> String {\n-    string.replace('\\'', \"''\")\n+    string.replace('\\'', \"''\").replace('\u2019', \"'\u2019\")\n }\n \n fn escape_help<T: ToString>(help: Option<&StyledStr>, data: T) -> String {\n", "instance_id": "clap-rs__clap-5892", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-structured. It clearly defines the issue: `clap_complete` generates invalid PowerShell code when documentation strings contain a `RIGHT_SINGLE_QUOTATION_MARK` (\u2019). The goal is explicit\u2014ensure proper escaping of this character in the generated output. The input, output, and constraints are indirectly but sufficiently described through the minimal reproducible code and the steps to reproduce the bug. The expected behavior (proper escaping of the character) and actual behavior (parsing error in PowerShell) are clearly contrasted. Additionally, the inclusion of debug output and a specific error message enhances the clarity. There are no significant ambiguities, and the problem is supported by a concrete example, making it easy to understand the context and requirements. Therefore, I assign a clarity score of 3 (Comprehensive).", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward fix to an existing function in a single file. Let's break it down based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code change is minimal and confined to a single line in the `escape_string` function within `powershell.rs`. It does not impact multiple modules or require understanding complex interactions across the codebase. The modification simply adds an additional character replacement for the `RIGHT_SINGLE_QUOTATION_MARK` (\u2019). There is no architectural impact or significant refactoring required. The amount of code change is trivial\u2014one additional method call in a string replacement chain.\n\n2. **Number of Technical Concepts**: The solution requires basic knowledge of string manipulation in Rust, specifically the `replace` method of the `String` type. No advanced language features, libraries, algorithms, or design patterns are involved. There is also no domain-specific knowledge required beyond understanding the context of character escaping in PowerShell, which is already provided in the problem statement.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention additional edge cases beyond the specific character causing the issue. The code change does not introduce new error handling logic or require consideration of complex edge cases. The fix is a direct mapping of one character to an escaped form, which is unlikely to have unintended side effects in this context.\n\n4. **Overall Complexity**: The task is a simple bug fix that can be implemented and tested quickly. It does not require deep understanding of the `clap_complete` codebase or its architecture, as the issue is isolated to a specific utility function. The fix is mechanical and does not involve performance considerations or intricate logic.\n\nGiven these factors, I assign a difficulty score of 0.15, placing it in the \"Very Easy\" range (0.0-0.2). The problem requires only a basic code modification to address a specific, well-defined issue with minimal impact on the broader system.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Broken link in `default_values_t` error message\n### Please complete the following tasks\n\n- [X] I have searched the [discussions](https://github.com/clap-rs/clap/discussions)\n- [X] I have searched the [open](https://github.com/clap-rs/clap/issues) and [rejected](https://github.com/clap-rs/clap/issues?q=is%3Aissue+label%3AS-wont-fix+is%3Aclosed) issues\n\n### Rust Version\n\nrustc 1.83.0 (90b35a623 2024-11-26)\n\n### Clap Version\n\nv4.5.23\n\n### Minimal reproducible code\n\n```\r\n#[clap(default_values_t = Some(Vec::new()))]\r\nfoo: Option<Vec<String>>,\r\n```\n\n### Steps to reproduce the bug with the above code\n\n```bash\r\ncargo build\r\n```\n\n### Actual Behaviour\n\n```text\r\nerror: #[arg(default_values_t)] can be used only on Vec types\r\n       \r\n       = note: see https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\r\n```\r\n\r\nLink is a 404.\r\n\r\n> The master branch of clap does not contain the path examples/derive_ref/README.md.\r\n\r\nThis link is repeated several times in https://github.com/clap-rs/clap/blob/fde45f9aea766fb8de46e3d46e6575f393c3b6b9/clap_derive/src/item.rs.\n\n### Expected Behaviour\n\nA working link.\n\n### Additional Context\n\n_No response_\n\n### Debug Output\n\n_No response_\n", "patch": "diff --git a/clap_derive/src/item.rs b/clap_derive/src/item.rs\nindex e48b200f704..ab32918e5f4 100644\n--- a/clap_derive/src/item.rs\n+++ b/clap_derive/src/item.rs\n@@ -561,7 +561,7 @@ impl Item {\n                             only on field level\\n\\n= note: {note}\\n\\n\",\n \n                             note = \"see \\\n-                                https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\")\n+                                https://docs.rs/clap/latest/clap/_derive/index.html#arg-attributes\")\n                     };\n \n                     let val = if let Some(expr) = &attr.value {\n@@ -611,7 +611,7 @@ impl Item {\n                             only on field level\\n\\n= note: {note}\\n\\n\",\n \n                             note = \"see \\\n-                                https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\")\n+                                https://docs.rs/clap/latest/clap/_derive/index.html#arg-attributes\")\n                     };\n                     let expr = attr.value_or_abort()?;\n \n@@ -622,7 +622,7 @@ impl Item {\n                             \"#[arg(default_values_t)] can be used only on Vec types\\n\\n= note: {note}\\n\\n\",\n \n                             note = \"see \\\n-                                https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\")\n+                                https://docs.rs/clap/latest/clap/_derive/index.html#arg-attributes\")\n                     }\n                     let inner_type = inner_type(ty);\n \n@@ -689,7 +689,7 @@ impl Item {\n                             only on field level\\n\\n= note: {note}\\n\\n\",\n \n                             note = \"see \\\n-                                https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\")\n+                                https://docs.rs/clap/latest/clap/_derive/index.html#arg-attributes\")\n                     };\n \n                     let val = if let Some(expr) = &attr.value {\n@@ -739,7 +739,7 @@ impl Item {\n                             only on field level\\n\\n= note: {note}\\n\\n\",\n \n                             note = \"see \\\n-                                https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\")\n+                                https://docs.rs/clap/latest/clap/_derive/index.html#arg-attributes\")\n                     };\n                     let expr = attr.value_or_abort()?;\n \n@@ -750,7 +750,7 @@ impl Item {\n                             \"#[arg(default_values_os_t)] can be used only on Vec types\\n\\n= note: {note}\\n\\n\",\n \n                             note = \"see \\\n-                                https://github.com/clap-rs/clap/blob/master/examples/derive_ref/README.md#magic-attributes\")\n+                                https://docs.rs/clap/latest/clap/_derive/index.html#arg-attributes\")\n                     }\n                     let inner_type = inner_type(ty);\n \n", "instance_id": "clap-rs__clap-5843", "clarity": 3, "difficulty": 0.1, "clarity_explanation": "The problem statement is comprehensive and clear. The goal is explicitly defined: to fix a broken link in the error message of the `clap_derive` crate. The input is the current codebase with the outdated link, and the expected output is a working link in the error messages. The problem provides specific details, including the exact file and lines where the issue occurs, a minimal reproducible code snippet, steps to reproduce the bug, and the actual versus expected behavior. There are no ambiguities or missing critical details regarding the task at hand. Additionally, the context of the issue (a 404 link to outdated documentation) is well-explained, and the provided code changes align perfectly with the problem description. The only minor omission is the lack of discussion on potential side effects of changing the link (e.g., ensuring the new link remains relevant across versions), but this does not detract from the overall clarity.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a straightforward fix with minimal code changes. The scope of the modification is limited to a single file (`item.rs`) and consists of replacing an outdated URL with a current one across multiple error message strings. This does not require understanding complex interactions within the codebase, nor does it impact the system's architecture or functionality\u2014it is purely a documentation fix within error messages. The amount of code change is trivial, involving only string replacements in six locations. No advanced technical concepts, algorithms, design patterns, or domain-specific knowledge are required beyond basic familiarity with Rust and string literals. There are no edge cases or error handling requirements mentioned or implied, as the change is purely cosmetic and does not alter the logic or behavior of the code. This task falls into the \"very easy\" category, as it is akin to fixing a typo or updating a constant, requiring only basic text editing skills.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Deprecate/remove pom_parser in favor of nom_parser\n@J-F-Liu I would like to get your opinion on removing pom_parser, or at least deprecating it.\r\nA concrete problem that prompted this issue is described in https://github.com/J-F-Liu/lopdf/issues/125#issuecomment-2275055483.\r\n#248 probably has the same cause, but the PDF link expired, so I can't check that.\r\n\r\nThe main advantages we would get when removing one of the parsers:\r\n- reduced code complexity\r\n- increased maintainability\r\n  - no bugs that are only reproducible with one of the parsers\r\n  - no doubled work when implementing new parser features, like #125 \r\n\r\nIt seems that the nom_parser is the better implementation of the two parser, at least in terms of performance (according to #60 and #208).\r\n\r\n\n", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex 3fd2226..a0754d2 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -30,7 +30,6 @@ log = \"0.4\"\n md-5 = \"0.10\"\n nom = { version = \"7.1\", optional = true }\n nom_locate = { version = \"4.2.0\", optional = true }\n-pom = { version = \"3.2\", optional = true }\n rayon = { version = \"1.6\", optional = true }\n serde = { version = \"1.0\", features = [\"derive\"], optional = true }\n time = { version = \"0.3\", features = [\"formatting\", \"parsing\"] }\n@@ -53,13 +52,8 @@ chrono_time = [\"chrono\"]\n default = [\"chrono_time\", \"nom_parser\", \"rayon\"]\n embed_image = [\"image\"]\n nom_parser = [\"nom\", \"nom_locate\"]\n-pom_parser = [\"pom\"]\n serde = [\"dep:serde\"]\n \n-[[example]]\n-name = \"add_barcode\"\n-required-features = [\"pom_parser\"]\n-\n [[example]]\n name = \"extract_toc\"\n required-features = [\"serde\"]\n@@ -76,9 +70,5 @@ required-features = [\"default\"]\n name = \"rotate\"\n required-features = [\"nom_parser\"]\n \n-[[test]]\n-name = \"modify\"\n-required-features = [\"pom_parser\"]\n-\n [badges]\n travis-ci = { repository = \"J-F-Liu/lopdf\" }\ndiff --git a/README.md b/README.md\nindex 87a3e06..3952fc5 100644\n--- a/README.md\n+++ b/README.md\n@@ -402,7 +402,7 @@ use lopdf::Document;\n \n // For this example to work a parser feature needs to be enabled\n #[cfg(not(feature = \"async\"))]\n-#[cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n+#[cfg(feature = \"nom_parser\")]\n {\n     let mut doc = Document::load(\"assets/example.pdf\").unwrap();\n \n@@ -416,7 +416,7 @@ use lopdf::Document;\n }\n \n #[cfg(feature = \"async\")]\n-#[cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n+#[cfg(feature = \"nom_parser\")]\n {\n     tokio::runtime::Builder::new_current_thread()\n         .build()\ndiff --git a/src/cmap_parser.rs b/src/cmap_parser.rs\ndeleted file mode 100644\nindex 07338df..0000000\n--- a/src/cmap_parser.rs\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-use log::warn;\n-\n-use crate::cmap_section::{CMapParseError, CMapSection};\n-\n-pub(crate) fn parse(_stream_content: &[u8]) -> Result<Vec<CMapSection>, CMapParseError> {\n-    warn!(\n-        \"Unicode cmap parsing is not supported in pom version, \\\n-text extraction might contain missing characters\"\n-    );\n-    Ok(Vec::new())\n-}\ndiff --git a/src/encodings/cmap.rs b/src/encodings/cmap.rs\nindex 8518422..a7787c9 100644\n--- a/src/encodings/cmap.rs\n+++ b/src/encodings/cmap.rs\n@@ -1,5 +1,5 @@\n-use crate::cmap_parser::parse;\n use crate::cmap_section::{CMapParseError, CMapSection, CodeLen, SourceCode};\n+use crate::parser::cmap_parser::parse;\n use crate::parser::ParserInput;\n \n use log::error;\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 50ba714..5a2bd21 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -2,60 +2,47 @@\n #![forbid(unsafe_code)]\n #![deny(clippy::all)]\n \n+pub mod content;\n+pub mod encryption;\n+pub mod filters;\n+pub mod xobject;\n+pub mod xref;\n+\n #[macro_use]\n mod object;\n-mod datetime;\n-pub use crate::object::{Dictionary, Object, ObjectId, Stream, StringFormat};\n-\n-mod common_data_structures;\n mod document;\n mod incremental_document;\n-mod object_stream;\n-#[cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n-pub use object_stream::ObjectStream;\n-pub mod xref;\n-pub use crate::common_data_structures::{decode_text_string, text_string};\n-pub use crate::document::Document;\n-pub use crate::encodings::{encode_utf16_be, encode_utf8};\n-pub use crate::incremental_document::IncrementalDocument;\n \n mod bookmarks;\n-pub use crate::bookmarks::Bookmark;\n-mod outlines;\n-pub use crate::outlines::Outline;\n-mod destinations;\n-pub use crate::destinations::Destination;\n-mod toc;\n-pub use crate::toc::Toc;\n-#[cfg(not(feature = \"nom_parser\"))]\n-#[cfg(feature = \"pom_parser\")]\n-mod cmap_parser;\n-#[cfg(feature = \"nom_parser\")]\n-#[path = \"nom_cmap_parser.rs\"]\n-mod cmap_parser;\n mod cmap_section;\n-pub mod content;\n+mod common_data_structures;\n mod creator;\n+mod datetime;\n+mod destinations;\n mod encodings;\n-pub use encodings::Encoding;\n-pub mod encryption;\n mod error;\n-pub use error::XrefError;\n-pub mod filters;\n-#[cfg(not(feature = \"nom_parser\"))]\n-#[cfg(feature = \"pom_parser\")]\n-mod parser;\n-#[cfg(feature = \"nom_parser\")]\n-#[path = \"nom_parser.rs\"]\n-mod parser;\n-mod parser_aux;\n+mod outlines;\n mod processor;\n-#[cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n-mod reader;\n-#[cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n-pub use reader::Reader;\n mod rc4;\n+mod toc;\n mod writer;\n-pub mod xobject;\n \n-pub use error::{Error, Result};\n+mod object_stream;\n+mod parser;\n+mod parser_aux;\n+mod reader;\n+\n+pub use document::Document;\n+pub use object::{Dictionary, Object, ObjectId, Stream, StringFormat};\n+\n+pub use bookmarks::Bookmark;\n+pub use common_data_structures::{decode_text_string, text_string};\n+pub use destinations::Destination;\n+pub use encodings::{encode_utf16_be, encode_utf8, Encoding};\n+pub use error::{Error, Result, XrefError};\n+pub use incremental_document::IncrementalDocument;\n+pub use object_stream::ObjectStream;\n+pub use outlines::Outline;\n+#[cfg(feature = \"nom_parser\")]\n+pub use reader::Reader;\n+pub use toc::Toc;\ndiff --git a/src/object.rs b/src/object.rs\nindex 3a685bf..cda8ee8 100644\n--- a/src/object.rs\n+++ b/src/object.rs\n@@ -365,8 +365,10 @@ impl Dictionary {\n             Ok(\"MacRomanEncoding\") => Ok(Encoding::OneByteEncoding(&encodings::MAC_ROMAN_ENCODING)),\n             Ok(\"MacExpertEncoding\") => Ok(Encoding::OneByteEncoding(&encodings::MAC_EXPERT_ENCODING)),\n             Ok(\"WinAnsiEncoding\") => Ok(Encoding::OneByteEncoding(&encodings::WIN_ANSI_ENCODING)),\n-            // it is not customary to use PDFDocEncoding in fonts but it is not forbidden by the standard\n-            Ok(\"PDFDocEncoding\") => Ok(Encoding::OneByteEncoding(&encodings::PDF_DOC_ENCODING)),\n+            Ok(\"PDFDocEncoding\") => {\n+                log::warn!(\"PDFDocEncoding is not a valid character encoding for a font\");\n+                Ok(Encoding::OneByteEncoding(&encodings::PDF_DOC_ENCODING))\n+            }\n             Ok(\"Identity-H\") | Ok(\"Identity-V\") => {\n                 let stream = self.get_deref(b\"ToUnicode\", doc)?.as_stream()?;\n                 self.get_encoding_from_to_unicode_cmap(stream)\ndiff --git a/src/object_stream.rs b/src/object_stream.rs\nindex 85ff14c..35036f0 100644\n--- a/src/object_stream.rs\n+++ b/src/object_stream.rs\n@@ -1,5 +1,4 @@\n-#![cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n-\n+#[cfg(feature = \"nom_parser\")]\n use crate::parser::{self, ParserInput};\n use crate::{Error, Object, ObjectId, Result, Stream};\n use std::collections::BTreeMap;\ndiff --git a/src/parser.rs b/src/parser.rs\ndeleted file mode 100644\nindex 742dad9..0000000\n--- a/src/parser.rs\n+++ /dev/null\n@@ -1,407 +0,0 @@\n-use super::{Dictionary, Object, ObjectId, Stream, StringFormat};\n-use crate::content::*;\n-use crate::error::XrefError;\n-use crate::reader::Reader;\n-use crate::xref::*;\n-use crate::{Error, Result};\n-use pom::char_class::{alpha, hex_digit, multispace, oct_digit};\n-use pom::parser::*;\n-use std::cmp::max;\n-use std::str::{self, FromStr};\n-\n-pub(crate) fn eol<'a>() -> Parser<'a, u8, u8> {\n-    (sym(b'\\r') * sym(b'\\n')) | sym(b'\\n') | sym(b'\\r')\n-}\n-\n-fn comment<'a>() -> Parser<'a, u8, ()> {\n-    sym(b'%') * none_of(b\"\\r\\n\").repeat(0..) * eol().discard()\n-}\n-\n-fn white_space<'a>() -> Parser<'a, u8, ()> {\n-    one_of(b\" \\t\\n\\r\\0\\x0C\").repeat(0..).discard()\n-}\n-\n-pub(crate) fn space<'a>() -> Parser<'a, u8, ()> {\n-    (one_of(b\" \\t\\n\\r\\0\\x0C\").repeat(1..).discard() | comment())\n-        .repeat(0..)\n-        .discard()\n-}\n-\n-fn integer<'a>() -> Parser<'a, u8, i64> {\n-    let number = one_of(b\"+-\").opt() + one_of(b\"0123456789\").repeat(1..);\n-    number.collect().convert(str::from_utf8).convert(i64::from_str)\n-}\n-\n-fn real<'a>() -> Parser<'a, u8, f32> {\n-    let number = one_of(b\"+-\").opt()\n-        + ((one_of(b\"0123456789\").repeat(1..) * sym(b'.') - one_of(b\"0123456789\").repeat(0..))\n-            | (sym(b'.') - one_of(b\"0123456789\").repeat(1..)));\n-    number.collect().convert(str::from_utf8).convert(f32::from_str)\n-}\n-\n-pub(crate) fn hex_char<'a>() -> Parser<'a, u8, u8> {\n-    let number = is_a(hex_digit).repeat(2);\n-    number\n-        .collect()\n-        .convert(|v| u8::from_str_radix(str::from_utf8(v).unwrap(), 16))\n-}\n-\n-fn oct_char<'a>() -> Parser<'a, u8, u8> {\n-    let number = is_a(oct_digit).repeat(1..4);\n-    number\n-        .collect()\n-        .convert(|v| u8::from_str_radix(str::from_utf8(v).unwrap(), 8))\n-}\n-\n-pub(crate) fn name<'a>() -> Parser<'a, u8, Vec<u8>> {\n-    sym(b'/') * (none_of(b\" \\t\\n\\r\\x0C()<>[]{}/%#\") | (sym(b'#') * hex_char())).repeat(0..)\n-}\n-\n-fn escape_sequence<'a>() -> Parser<'a, u8, Vec<u8>> {\n-    sym(b'\\\\')\n-        * (sym(b'\\\\').map(|_| vec![b'\\\\'])\n-            | sym(b'(').map(|_| vec![b'('])\n-            | sym(b')').map(|_| vec![b')'])\n-            | sym(b'n').map(|_| vec![b'\\n'])\n-            | sym(b'r').map(|_| vec![b'\\r'])\n-            | sym(b't').map(|_| vec![b'\\t'])\n-            | sym(b'b').map(|_| vec![b'\\x08'])\n-            | sym(b'f').map(|_| vec![b'\\x0C'])\n-            | oct_char().map(|c| vec![c])\n-            | eol().map(|_| vec![])\n-            | empty().map(|_| vec![]))\n-}\n-\n-fn nested_literal_string<'a>(depth: usize) -> Parser<'a, u8, Vec<u8>> {\n-    if depth == 0 {\n-        return Parser::new(move |_: &'a [u8], pos: usize| {\n-            Err(pom::Error::Custom {\n-                message: \"Brackets embedded to deep.\".to_string(),\n-                position: pos,\n-                inner: None,\n-            })\n-        });\n-    }\n-\n-    sym(b'(')\n-        * (none_of(b\"\\\\()\").repeat(1..) | escape_sequence() | call(move || nested_literal_string(depth - 1)))\n-            .repeat(0..)\n-            .map(|segments| {\n-                let mut bytes = segments.into_iter().fold(vec![b'('], |mut bytes, mut segment| {\n-                    bytes.append(&mut segment);\n-                    bytes\n-                });\n-                bytes.push(b')');\n-                bytes\n-            })\n-        - sym(b')')\n-}\n-\n-fn literal_string<'a>() -> Parser<'a, u8, Vec<u8>> {\n-    sym(b'(')\n-        * (none_of(b\"\\\\()\").repeat(1..) | escape_sequence() | nested_literal_string(crate::reader::MAX_BRACKET))\n-            .repeat(0..)\n-            .map(|segments| segments.concat())\n-        - sym(b')')\n-}\n-\n-fn hexadecimal_string<'a>() -> Parser<'a, u8, Vec<u8>> {\n-    sym(b'<') * (white_space() * hex_char()).repeat(0..) - (white_space() * sym(b'>'))\n-}\n-\n-fn array<'a>() -> Parser<'a, u8, Vec<Object>> {\n-    sym(b'[') * space() * call(_direct_object).repeat(0..) - sym(b']')\n-}\n-\n-pub(crate) fn dictionary<'a>() -> Parser<'a, u8, Dictionary> {\n-    let entry = name() - space() + call(_direct_object);\n-    let entries = seq(b\"<<\") * space() * entry.repeat(0..) - seq(b\">>\");\n-    entries.map(|entries| {\n-        entries\n-            .into_iter()\n-            .fold(Dictionary::new(), |mut dict: Dictionary, (key, value)| {\n-                dict.set(key, value);\n-                dict\n-            })\n-    })\n-}\n-\n-fn stream<'a>(reader: &'a Reader) -> Parser<'a, u8, Stream> {\n-    (dictionary() - space() - seq(b\"stream\") - eol())\n-        >> move |dict: Dictionary| {\n-            if let Ok(length) = dict.get(b\"Length\").and_then(|value| {\n-                if let Ok(id) = value.as_reference() {\n-                    return reader.get_object(id).and_then(|value| value.as_i64());\n-                }\n-                value.as_i64()\n-            }) {\n-                if length < 0 {\n-                    println!(\"Warning: invalid stream length {}\", length);\n-                    return empty().map(move |_| Stream::new(dict.clone(), vec![]));\n-                }\n-                let stream = take(length as usize) - eol().opt() - seq(b\"endstream\").expect(\"endstream\");\n-                stream.map(move |data| Stream::new(dict.clone(), data.to_vec()))\n-            } else {\n-                empty().pos().map(move |pos| Stream::with_position(dict.clone(), pos))\n-            }\n-        }\n-}\n-\n-fn object_id<'a>() -> Parser<'a, u8, ObjectId> {\n-    let id: Parser<u8, u32> = one_of(b\"0123456789\")\n-        .repeat(1..)\n-        .convert(|v| u32::from_str(str::from_utf8(&v).unwrap()));\n-    let gen = one_of(b\"0123456789\")\n-        .repeat(1..)\n-        .convert(|v| u16::from_str(str::from_utf8(&v).unwrap()));\n-    id - space() + gen - space()\n-}\n-\n-pub fn direct_object(input: &[u8]) -> Option<Object> {\n-    _direct_object().parse(input).ok()\n-}\n-\n-fn _direct_object<'a>() -> Parser<'a, u8, Object> {\n-    (seq(b\"null\").map(|_| Object::Null)\n-        | seq(b\"true\").map(|_| Object::Boolean(true))\n-        | seq(b\"false\").map(|_| Object::Boolean(false))\n-        | (object_id().map(Object::Reference) - sym(b'R'))\n-        | real().map(Object::Real)\n-        | integer().map(Object::Integer)\n-        | name().map(Object::Name)\n-        | literal_string().map(Object::string_literal)\n-        | hexadecimal_string().map(|bytes| Object::String(bytes, StringFormat::Hexadecimal))\n-        | array().map(Object::Array)\n-        | dictionary().map(Object::Dictionary))\n-        - space()\n-}\n-\n-fn object<'a>(reader: &'a Reader) -> Parser<'a, u8, Object> {\n-    (seq(b\"null\").map(|_| Object::Null)\n-        | seq(b\"true\").map(|_| Object::Boolean(true))\n-        | seq(b\"false\").map(|_| Object::Boolean(false))\n-        | (object_id().map(Object::Reference) - sym(b'R'))\n-        | real().map(Object::Real)\n-        | integer().map(Object::Integer)\n-        | name().map(Object::Name)\n-        | literal_string().map(Object::string_literal)\n-        | hexadecimal_string().map(|bytes| Object::String(bytes, StringFormat::Hexadecimal))\n-        | array().map(Object::Array)\n-        | stream(reader).map(Object::Stream)\n-        | dictionary().map(Object::Dictionary))\n-        - space()\n-}\n-\n-pub fn indirect_object(\n-    input: &[u8], offset: usize, expected_id: Option<ObjectId>, reader: &Reader,\n-) -> Result<(ObjectId, Object)> {\n-    _indirect_object(expected_id, reader)\n-        .parse_at(input, offset)\n-        .map(|(out, _)| out)\n-        .map_err(|_| Error::Parse { offset })\n-}\n-\n-fn _indirect_object<'a>(expected_id: Option<ObjectId>, reader: &'a Reader) -> Parser<'a, u8, (ObjectId, Object)> {\n-    object_id().convert(move |id| match expected_id {\n-        Some(expected_id) if expected_id == id => Ok(id),\n-        Some(_) => Err(()),\n-        None => Ok(id),\n-    }) - seq(b\"obj\")\n-        - space()\n-        + object(reader)\n-        - space()\n-        - seq(b\"endobj\").opt()\n-        - space()\n-}\n-\n-pub fn header(input: &[u8]) -> Option<String> {\n-    (seq(b\"%PDF-\") * none_of(b\"\\r\\n\").repeat(0..).convert(String::from_utf8) - eol() - comment().repeat(0..))\n-        .parse(input)\n-        .ok()\n-}\n-\n-/// Decode CrossReferenceTable\n-fn xref<'a>() -> Parser<'a, u8, Xref> {\n-    let xref_entry = integer().map(|i| max(0, i) as u32) - sym(b' ') + integer().map(|i| i as u16) - sym(b' ')\n-        + one_of(b\"nf\").map(|k| k == b'n')\n-        - take(2);\n-    let xref_section = integer().map(|i| max(0, i) as usize) - sym(b' ') + integer() - sym(b' ').opt() - eol()\n-        + xref_entry.repeat(0..);\n-    let xref = seq(b\"xref\") * sym(b' ').opt() * eol() * xref_section.repeat(1..) - space();\n-    xref.map(|sections| {\n-        sections.into_iter().fold(\n-            Xref::new(0, XrefType::CrossReferenceTable),\n-            |mut xref: Xref, ((start, _count), entries): _| {\n-                for (index, ((offset, generation), is_normal)) in entries.into_iter().enumerate() {\n-                    if is_normal {\n-                        xref.insert((start + index) as u32, XrefEntry::Normal { offset, generation });\n-                    }\n-                }\n-                xref\n-            },\n-        )\n-    })\n-}\n-\n-fn trailer<'a>() -> Parser<'a, u8, Dictionary> {\n-    seq(b\"trailer\") * space() * dictionary() - space()\n-}\n-\n-pub fn xref_and_trailer<'a>(input: &'a [u8], reader: &'a Reader) -> Result<(Xref, Dictionary)> {\n-    _xref_and_trailer(reader)\n-        .parse(input)\n-        .map_err(|_| Error::Xref(XrefError::Parse))\n-}\n-\n-fn _xref_and_trailer<'a>(reader: &'a Reader) -> Parser<'a, u8, (Xref, Dictionary)> {\n-    (xref() + trailer()).convert(|(mut xref, trailer)| -> Result<_> {\n-        xref.size = trailer\n-            .get(b\"Size\")\n-            .and_then(Object::as_i64)\n-            .map_err(|_| Error::Trailer)? as u32;\n-        Ok((xref, trailer))\n-    }) | _indirect_object(None, reader).convert(|(_, obj)| match obj {\n-        Object::Stream(stream) => decode_xref_stream(stream),\n-        _ => Err(Error::Xref(XrefError::Parse)),\n-    })\n-}\n-\n-pub fn xref_start(input: &[u8]) -> Option<i64> {\n-    (seq(b\"startxref\") * white_space() * integer() - white_space() - seq(b\"%%EOF\") - space())\n-        .parse(input)\n-        .ok()\n-}\n-\n-// The following code create parser to parse content stream.\n-\n-fn content_space<'a>() -> Parser<'a, u8, ()> {\n-    is_a(multispace).repeat(0..).discard()\n-}\n-\n-fn operator<'a>() -> Parser<'a, u8, String> {\n-    (is_a(alpha) | one_of(b\"*'\\\"\")).repeat(1..).convert(String::from_utf8)\n-}\n-\n-fn operand<'a>() -> Parser<'a, u8, Object> {\n-    (seq(b\"null\").map(|_| Object::Null)\n-        | seq(b\"true\").map(|_| Object::Boolean(true))\n-        | seq(b\"false\").map(|_| Object::Boolean(false))\n-        | real().map(Object::Real)\n-        | integer().map(Object::Integer)\n-        | name().map(Object::Name)\n-        | literal_string().map(Object::string_literal)\n-        | hexadecimal_string().map(|bytes| Object::String(bytes, StringFormat::Hexadecimal))\n-        | array().map(Object::Array)\n-        | dictionary().map(Object::Dictionary))\n-        - content_space()\n-}\n-\n-fn operation<'a>() -> Parser<'a, u8, Operation> {\n-    let operation = operand().repeat(0..) + operator() - content_space();\n-    operation.map(|(operands, operator)| Operation { operator, operands })\n-}\n-\n-pub fn content(input: &[u8]) -> Option<Content<Vec<Operation>>> {\n-    (content_space() * operation().repeat(0..).map(|operations| Content { operations }))\n-        .parse(input)\n-        .ok()\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-\n-    #[test]\n-    fn parse_real_number() {\n-        let r0 = real().parse(b\"0.12\");\n-        assert_eq!(r0, Ok(0.12));\n-        let r1 = real().parse(b\"-.12\");\n-        assert_eq!(r1, Ok(-0.12));\n-        let r2 = real().parse(b\"10.\");\n-        assert_eq!(r2, Ok(10.0));\n-    }\n-\n-    #[test]\n-    fn parse_string() {\n-        let data = vec![\n-            (\"()\", \"\"),\n-            (\"(text())\", \"text()\"),\n-            (\"(text\\r\\n\\\\\\\\(nested\\\\t\\\\b\\\\f))\", \"text\\r\\n\\\\(nested\\t\\x08\\x0C)\"),\n-            (\"(text\\\\0\\\\53\\\\053\\\\0053)\", \"text\\0++\\x053\"),\n-            (\"(text line\\\\\\n())\", \"text line()\"),\n-        ];\n-\n-        for (input, expected) in data {\n-            assert_eq!(\n-                literal_string().parse(input.as_bytes()),\n-                Ok(expected.as_bytes().to_vec()),\n-                \"input: {:?} output: {:?}\",\n-                input,\n-                expected,\n-            );\n-        }\n-    }\n-\n-    #[test]\n-    fn parse_name() {\n-        let (text, expected) = (b\"/ABC#5f\", b\"ABC\\x5F\");\n-        let result = name().parse(text).ok();\n-        assert_eq!(result, Some(expected.to_vec()));\n-\n-        let (text, expected) = (b\"/#cb#ce#cc#e5\", b\"\\xcb\\xce\\xcc\\xe5\");\n-        let result = name().parse(text).ok();\n-        assert_eq!(result, Some(expected.to_vec()));\n-    }\n-\n-    #[test]\n-    /// Run `cargo test -- --nocapture` to see output\n-    fn parse_content() {\n-        let stream = b\"\n-2 J\n-BT\n-/F1 12 Tf\n-0 Tc\n-0 Tw\n-72.5 712 TD\n-[(Unencoded streams can be read easily) 65 (,) ] TJ\n-0 -14 TD\n-[(b) 20 (ut generally tak) 10 (e more space than \\\\311)] TJ\n-T* (encoded streams.) Tj\n-\t\t\";\n-        let content = content(stream);\n-        println!(\"{:?}\", content);\n-        assert!(content.is_some());\n-    }\n-\n-    #[test]\n-    fn parse_cidsysteminfo_dictionary() {\n-        let stream = b\"<< /Registry (Adobe) /Ordering (UCS) /Supplement 0 >>\";\n-        let expected = dictionary!(\n-            \"Registry\" => Object::string_literal(\"Adobe\"),\n-            \"Ordering\" => Object::string_literal(\"UCS\"),\n-            \"Supplement\" => Object::Integer(0));\n-\n-        let parsed_dict = dictionary().parse(stream);\n-        assert_eq!(parsed_dict, Ok(expected));\n-    }\n-\n-    #[test]\n-    fn parse_font_dictionary() {\n-        // from PDF 1.7 standard example font\n-        let stream = b\"<< /Type /Font\n-    /Subtype /Type0\n-    /BaseFont /HeiseiMin-W5-90ms-RKSJ-H\n-    /Encoding /90ms-RKSJ-H\n-    /DescendantFonts [15 0 R]\n->>\";\n-        let expected = dictionary!(\n-            \"Type\" => \"Font\",\n-            \"Subtype\"=> \"Type0\",\n-            \"BaseFont\" => \"HeiseiMin-W5-90ms-RKSJ-H\",\n-            \"Encoding\" => \"90ms-RKSJ-H\",\n-            \"DescendantFonts\" => Object::Array(vec![Object::Reference((15, 0))]));\n-        let parsed_dict = dictionary().parse(stream);\n-        println!(\"{:#?}\", parsed_dict.clone().unwrap().get(b\"DescendantFonts\"));\n-        assert_eq!(parsed_dict, Ok(expected));\n-    }\n-}\ndiff --git a/src/nom_cmap_parser.rs b/src/parser/cmap_parser.rs\nsimilarity index 100%\nrename from src/nom_cmap_parser.rs\nrename to src/parser/cmap_parser.rs\ndiff --git a/src/nom_parser.rs b/src/parser/mod.rs\nsimilarity index 99%\nrename from src/nom_parser.rs\nrename to src/parser/mod.rs\nindex 3538c5f..b5468f8 100644\n--- a/src/nom_parser.rs\n+++ b/src/parser/mod.rs\n@@ -1,3 +1,4 @@\n+#[cfg(feature = \"nom_parser\")]\n use super::{Dictionary, Object, ObjectId, Reader, Stream, StringFormat};\n use crate::content::*;\n use crate::error::XrefError;\n@@ -21,6 +22,8 @@ use nom::IResult;\n use nom::Slice;\n use nom_locate::LocatedSpan;\n \n+pub(crate) mod cmap_parser;\n+\n pub(crate) type ParserInput<'a> = LocatedSpan<&'a [u8], &'a str>;\n // Change this to something else that implements ParseError to get a\n // different error type out of nom.\ndiff --git a/src/parser_aux.rs b/src/parser_aux.rs\nindex 0bb56cc..6f5ca2d 100644\n--- a/src/parser_aux.rs\n+++ b/src/parser_aux.rs\n@@ -1,5 +1,4 @@\n-#![cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n-\n+#![cfg(feature = \"nom_parser\")]\n use log::warn;\n \n use crate::{\ndiff --git a/src/reader.rs b/src/reader.rs\nindex 2b2a156..cf13451 100644\n--- a/src/reader.rs\n+++ b/src/reader.rs\n@@ -1,5 +1,4 @@\n-#![cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n-\n+#[cfg(feature = \"nom_parser\")]\n use log::{error, warn};\n use std::cmp;\n use std::collections::{BTreeMap, HashSet};\ndiff --git a/src/xref.rs b/src/xref.rs\nindex 3d235e4..0165486 100644\n--- a/src/xref.rs\n+++ b/src/xref.rs\n@@ -137,5 +137,5 @@ impl XrefSection {\n     }\n }\n \n-#[cfg(any(feature = \"pom_parser\", feature = \"nom_parser\"))]\n+#[cfg(feature = \"nom_parser\")]\n pub use crate::parser_aux::decode_xref_stream;\n", "instance_id": "J-F-Liu__lopdf-355", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in its intent to deprecate or remove the `pom_parser` in favor of `nom_parser` within the `lopdf` library, citing advantages like reduced code complexity and increased maintainability. It references specific issues and discussions (e.g., performance comparisons in #60 and #208) to justify the preference for `nom_parser`. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected outcome (complete removal vs. deprecation with backward compatibility) or provide detailed steps on how to handle existing dependencies on `pom_parser`. Additionally, there are no specific guidelines on testing or validating the change to ensure no regressions occur. While the goal is understandable, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this task falls into the medium range due to several factors. First, the scope of code changes is significant, involving multiple files and the removal of an entire parser implementation (`pom_parser`), as seen in the deletion of files like `src/cmap_parser.rs` and `src/parser.rs`, and modifications to feature flags in `Cargo.toml`. This requires understanding the interactions between different parts of the codebase, such as how the parser integrates with other components like `reader.rs` and `xref.rs`, and ensuring that switching to `nom_parser` does not break existing functionality. Second, the number of technical concepts involved includes familiarity with Rust's feature flags, conditional compilation, and parser libraries (`pom` and `nom`), as well as an understanding of PDF parsing logic, which adds moderate complexity. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential issues, such as ensuring compatibility for users relying on `pom_parser` and handling any parser-specific bugs or behaviors (as referenced in linked issues). The task does not seem to impact the system's core architecture drastically but does require careful refactoring and testing to avoid regressions. Overall, this task demands a solid understanding of the codebase and moderate effort to implement and validate, placing it at 0.55 on the difficulty scale.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Grammars: Missing support for proto2\nI'm planning to work on this, so feel free to assign (I don't have permissions).\r\n\r\n### Current situation:\r\n\r\n- Grammar used by Helix is https://github.com/yusdacra/tree-sitter-protobuf at `19c211a01434d9f03efff99f85e19f967591b175`\r\n  - [Queries](https://github.com/helix-editor/helix/tree/master/runtime/queries/protobuf)\r\n- Grammar used by nvim is https://github.com/treywood/tree-sitter-proto (pinned at [current main](https://github.com/nvim-treesitter/nvim-treesitter/blob/fb2d41ec599b68af041f3071ada883718928e279/lockfile.json#L582))\r\n\r\n### Problems of current situation:\r\n\r\n- no proto2 support (also no support [Editions](https://protobuf.dev/editions/overview/) but this is not supported anywhere so out of scope for now)\r\n- different grammar than nvim leads to fragmentation\r\n- neither of the grammars' feature sets is a superset of the other one --> upstream work needed\r\n- inconsistent language identifier (`proto` vs `protobuf`)\r\n\r\n### Plan of action\r\n\r\n- [ ] agree with @yusdacra and @treywood on a unification plan. Probably consolidating on treywood/tree-sitter-proto is easier because its [network graph](https://github.com/treywood/tree-sitter-proto/network) is larger (e.g it's used by [nvim-treesitter](https://github.com/nvim-treesitter/nvim-treesitter?tab=readme-ov-file#supported-languages)).\r\n  - [X] reach out to @yusdacra to get agreement for that\r\n  - [X] reach out to @treywood about adding missing features to https://github.com/treywood/tree-sitter-proto\r\n- [x] implement missing features in https://github.com/treywood/tree-sitter-proto\r\n- [ ] get missing features into https://github.com/treywood/tree-sitter-proto master branch - [PR](https://github.com/treywood/tree-sitter-proto/pull/2)\r\n- [ ] switch Helix over to the new grammars (and `proto` identifier)\r\n- [ ] fix [reverse-query-precedence-ordering branch](https://github.com/helix-editor/helix/tree/reverse-query-precedence-ordering) if needed\r\n\r\n### Analysis of missing features\r\n\r\nFeatures missing for [highlights.scm](https://github.com/helix-editor/helix/blob/master/runtime/queries/protobuf/highlights.scm):\r\n\r\n- missing `mapName`, `extendName`, `fieldName` and `enumVariantName`\r\n- missing `optionName` - this is fixed in the [`fixups` branch](https://github.com/treywood/tree-sitter-proto/tree/fixups) but needs merging into main\r\n\r\nFeatures missing for [indents.scm](https://github.com/helix-editor/helix/blob/master/runtime/queries/protobuf/indents.scm):\r\n\r\n- `oneofBody`, `serviceBody` and `rpcBody` are missing. You can use `oneof`, `service` and `rpc` instead, but this probably leads to some regressions. Not sure if there is a severe enough case to block on this. I noticed a difference with declarations like this in `hybrid` indent heuristic, but they actually both misbehave.\r\n  ```\r\n  rpc FooRpc(FooRpcRequest)\r\n      returns (FooRpcResponse) {\r\n    option deadline = 20.0;\r\n  }\r\n  ```\r\n\r\nFeatures missing for [textobjects.scm](https://github.com/helix-editor/helix/blob/master/runtime/queries/protobuf/textobjects.scm):\r\n - missing `serviceBody`\r\n\n", "patch": "diff --git a/languages.toml b/languages.toml\nindex 23bd5fc93234..81ca7e77f7bb 100644\n--- a/languages.toml\n+++ b/languages.toml\n@@ -340,16 +340,17 @@ source = { git = \"https://github.com/Beaglefoot/tree-sitter-awk\", rev = \"a799bc5\n [[language]]\n name = \"protobuf\"\n scope = \"source.proto\"\n-injection-regex = \"protobuf\"\n+injection-regex = \"proto\"\n file-types = [\"proto\"]\n language-servers = [ \"bufls\", \"pbkit\" ]\n comment-token = \"//\"\n block-comment-tokens = { start = \"/*\", end = \"*/\" }\n indent = { tab-width = 2, unit = \"  \" }\n+grammar = \"proto\"\n \n [[grammar]]\n-name = \"protobuf\"\n-source = { git = \"https://github.com/yusdacra/tree-sitter-protobuf\", rev = \"19c211a01434d9f03efff99f85e19f967591b175\"}\n+name = \"proto\"\n+source = { git = \"https://github.com/sdoerner/tree-sitter-proto\", rev = \"778ab6ed18a7fcf82c83805a87d63376c51e80bc\"}\n \n [[language]]\n name = \"textproto\"\ndiff --git a/runtime/queries/protobuf/highlights.scm b/runtime/queries/protobuf/highlights.scm\nindex 6deee7df8116..15dd7bb3a578 100644\n--- a/runtime/queries/protobuf/highlights.scm\n+++ b/runtime/queries/protobuf/highlights.scm\n@@ -1,5 +1,6 @@\n [\n   \"syntax\"\n+  \"edition\"\n   \"package\"\n   \"option\"\n   \"import\"\n@@ -7,44 +8,45 @@\n   \"rpc\"\n   \"returns\"\n   \"message\"\n+  \"map\"\n   \"enum\"\n   \"oneof\"\n   \"repeated\"\n+  \"optional\"\n+  \"required\"\n   \"reserved\"\n   \"to\"\n   \"stream\"\n   \"extend\"\n-  \"optional\"\n ] @keyword\n \n [\n-  (keyType)\n+  (key_type)\n   (type)\n+  (message_or_enum_type)\n ] @type.builtin\n \n [\n-  (mapName)\n-  (enumName)\n-  (messageName)\n-  (extendName)\n-  (serviceName)\n-  (rpcName)\n+  (enum_name)\n+  (message_name)\n+  (service_name)\n+  (rpc_name)\n ] @type\n \n [\n-  (fieldName)\n-  (optionName)\n+  (field_name)\n+  (option_name)\n ] @variable.other.member\n-(enumVariantName) @type.enum.variant\n+(enum_variant_name) @type.enum.variant\n \n-(fullIdent) @namespace\n+(full_ident) @namespace\n \n-(intLit) @constant.numeric.integer\n-(floatLit) @constant.numeric.float\n-(boolLit) @constant.builtin.boolean\n-(strLit) @string\n+(int_lit) @constant.numeric.integer\n+(float_lit) @constant.numeric.float\n+(bool) @constant.builtin.boolean\n+(string) @string\n \n-(constant) @constant\n+(block_lit) @constant\n \n (comment) @comment\n \n@@ -55,12 +57,8 @@\n   \"]\"\n   \"{\"\n   \"}\"\n-] @punctuation.bracket\n+]  @punctuation.bracket\n \n-[\n-  \"=\"\n-] @operator\n+\"=\" @operator\n \n-[\n-  \";\"\n-] @punctuation.delimiter\n+\";\" @punctuation.delimiter\ndiff --git a/runtime/queries/protobuf/indents.scm b/runtime/queries/protobuf/indents.scm\nindex d457d75f1998..d55e81b2da17 100644\n--- a/runtime/queries/protobuf/indents.scm\n+++ b/runtime/queries/protobuf/indents.scm\n@@ -1,10 +1,10 @@\n [\n-  (messageBody)\n-  (enumBody)\n-  (oneofBody)\n-  (serviceBody)\n-  (rpcBody)\n-  (msgLit)\n+  (message_body)\n+  (enum_body)\n+  (oneof_body)\n+  (service_body)\n+  (rpc_body)\n+  (block_lit)\n ] @indent\n \n \"}\" @outdent\ndiff --git a/runtime/queries/protobuf/textobjects.scm b/runtime/queries/protobuf/textobjects.scm\nindex 6f06b13498b3..7b0cfb198557 100644\n--- a/runtime/queries/protobuf/textobjects.scm\n+++ b/runtime/queries/protobuf/textobjects.scm\n@@ -1,9 +1,9 @@\n-(message (messageBody) @class.inside) @class.around\n-(enum (enumBody) @class.inside) @class.around\n-(service (serviceBody) @class.inside) @class.around\n+(message (message_body) @class.inside) @class.around\n+(enum (enum_body) @class.inside) @class.around\n+(service (service_body) @class.inside) @class.around\n \n-(rpc (enumMessageType) @parameter.inside) @function.inside\n-(rpc (enumMessageType) @parameter.around) @function.around\n+(rpc (message_or_enum_type) @parameter.inside) @function.inside\n+(rpc (message_or_enum_type) @parameter.around) @function.around\n \n (comment) @comment.inside\n (comment)+ @comment.around\n", "instance_id": "helix-editor__helix-12225", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "\nThe problem statement is mostly clear in outlining the goal of adding support for proto2 in the Helix editor by unifying grammars with another repository (treywood/tree-sitter-proto) and addressing missing features. It provides a detailed \"Current Situation\" and \"Problems\" section, along with a \"Plan of Action\" that lists specific steps to resolve the issue. Additionally, it identifies missing features in the grammar for highlights, indents, and textobjects, which helps in understanding the scope of changes needed. However, there are minor ambiguities and missing details that prevent it from being comprehensive. For instance, the problem statement does not explicitly define what \"proto2 support\" entails in terms of specific syntax or features beyond the missing grammar elements. There are no examples of proto2-specific code that needs to be supported, nor are there detailed constraints or requirements for compatibility with other tools beyond Helix and nvim. Edge cases, such as potential conflicts in grammar unification or specific indentation behaviors, are mentioned but not fully explored (e.g., the hybrid indent heuristic issue). Overall, while the problem is valid and mostly clear, these minor gaps in detail result in a score of 2 (Mostly Clear).\n", "difficulty_explanation": "\nI assign a difficulty score of 0.50, placing this problem in the \"Medium\" range. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The code changes provided are relatively localized, primarily involving updates to configuration files (`languages.toml`) and query files (`highlights.scm`, `indents.scm`, `textobjects.scm`) in the Helix editor repository. The changes involve switching the grammar source to a forked repository and updating naming conventions to align with the new grammar (e.g., `protobuf` to `proto`, renaming nodes like `messageBody` to `message_body`). These modifications are straightforward and do not impact the broader system architecture of Helix. However, the problem also implies upstream work on the `treywood/tree-sitter-proto` repository to implement missing features (e.g., `mapName`, `serviceBody`), which is not shown in the provided diff but is part of the overall task. This adds a layer of complexity as it requires understanding and contributing to an external codebase, though the specific changes are not detailed here.\n\n2. **Number of Technical Concepts**: Solving this problem requires familiarity with several concepts, including Tree-sitter grammars (a domain-specific parsing framework), the structure of Protocol Buffers (protobuf) syntax (specifically proto2), and the Helix editor's runtime query system for syntax highlighting, indentation, and textobjects. Additionally, it involves collaboration and coordination with external repository maintainers, which introduces a non-technical but practical challenge. While Tree-sitter grammars and protobuf are moderately complex, they are well-documented, and the concepts are not inherently advanced for someone with experience in parser development or editor configurations.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement mentions some edge cases, such as potential regressions in indentation behavior due to differences in grammar nodes (e.g., using `service` instead of `serviceBody`). However, these are not deeply complex, and the provided code changes do not introduce significant new error handling logic. The main challenge lies in ensuring compatibility between the new grammar and existing queries, which might require iterative testing but does not appear to involve intricate error conditions.\n\n4. **Overall Complexity**: The task requires understanding multiple files and making consistent updates across them, as well as potentially contributing to an external grammar repository. However, the changes in the Helix repository are mostly mechanical (renaming and configuration updates), and the upstream grammar work, while not detailed, seems focused on adding specific missing features rather than a complete overhaul. There are no indications of performance considerations, deep architectural changes, or highly complex algorithms.\n\nGiven these factors, the problem falls into the \"Medium\" difficulty range (0.4-0.6). It requires understanding a few specialized concepts (Tree-sitter, protobuf grammar) and making coordinated changes across multiple files, with some potential for minor edge case handling. However, it does not demand advanced technical knowledge, extensive refactoring, or system-level considerations, which would push it into the \"Hard\" or \"Very Hard\" categories. A score of 0.50 reflects this balance of moderate complexity and scope.\n", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Missing license files in rocket_codegen-0.5.0.crate\n### What kind of documentation problem are you reporting?\r\n\r\nOther\r\n\r\n### Where is the issue found?\r\n\r\nhttps://crates.io/crates/rocket_codegen/0.5.0\r\n\r\n### What's wrong?\r\n\r\n[LICENSE-MIT](https://github.com/rwf2/Rocket/blob/master/LICENSE-MIT) and [LICENSE-APACHE](https://github.com/rwf2/Rocket/blob/master/LICENSE-APACHE) are missing in rocket_codegen-0.5.0.crate downloaded from https://crates.io/crates/rocket_codegen/0.5.0 Please add it to all your packages. I would like to package it as rpm for Fedora (which identifies missing license as a [mistake](https://docs.fedoraproject.org/en-US/packaging-guidelines/LicensingGuidelines/#_license_text) and encourages upstream to include it) and it would make it easier. Thanks.\r\n\r\n### System Checks\r\n\r\n- [X] I confirmed that the issue still exists on `master` on GitHub.\r\n- [X] I was unable to find a previous report of this problem.\nMissing license files in rocket_codegen-0.5.0.crate\n### What kind of documentation problem are you reporting?\r\n\r\nOther\r\n\r\n### Where is the issue found?\r\n\r\nhttps://crates.io/crates/rocket_codegen/0.5.0\r\n\r\n### What's wrong?\r\n\r\n[LICENSE-MIT](https://github.com/rwf2/Rocket/blob/master/LICENSE-MIT) and [LICENSE-APACHE](https://github.com/rwf2/Rocket/blob/master/LICENSE-APACHE) are missing in rocket_codegen-0.5.0.crate downloaded from https://crates.io/crates/rocket_codegen/0.5.0 Please add it to all your packages. I would like to package it as rpm for Fedora (which identifies missing license as a [mistake](https://docs.fedoraproject.org/en-US/packaging-guidelines/LicensingGuidelines/#_license_text) and encourages upstream to include it) and it would make it easier. Thanks.\r\n\r\n### System Checks\r\n\r\n- [X] I confirmed that the issue still exists on `master` on GitHub.\r\n- [X] I was unable to find a previous report of this problem.\n", "patch": "diff --git a/contrib/db_pools/codegen/LICENSE-APACHE b/contrib/db_pools/codegen/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..6e2b1da875\n--- /dev/null\n+++ b/contrib/db_pools/codegen/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/contrib/db_pools/codegen/LICENSE-MIT b/contrib/db_pools/codegen/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..98cefa6a6a\n--- /dev/null\n+++ b/contrib/db_pools/codegen/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/contrib/db_pools/lib/LICENSE-APACHE b/contrib/db_pools/lib/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..6e2b1da875\n--- /dev/null\n+++ b/contrib/db_pools/lib/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/contrib/db_pools/lib/LICENSE-MIT b/contrib/db_pools/lib/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..98cefa6a6a\n--- /dev/null\n+++ b/contrib/db_pools/lib/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/contrib/dyn_templates/LICENSE-APACHE b/contrib/dyn_templates/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..1cd601d0a3\n--- /dev/null\n+++ b/contrib/dyn_templates/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/contrib/dyn_templates/LICENSE-MIT b/contrib/dyn_templates/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..b2cfbdc7b0\n--- /dev/null\n+++ b/contrib/dyn_templates/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/contrib/sync_db_pools/codegen/LICENSE-APACHE b/contrib/sync_db_pools/codegen/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..6e2b1da875\n--- /dev/null\n+++ b/contrib/sync_db_pools/codegen/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/contrib/sync_db_pools/codegen/LICENSE-MIT b/contrib/sync_db_pools/codegen/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..98cefa6a6a\n--- /dev/null\n+++ b/contrib/sync_db_pools/codegen/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/contrib/sync_db_pools/lib/LICENSE-APACHE b/contrib/sync_db_pools/lib/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..6e2b1da875\n--- /dev/null\n+++ b/contrib/sync_db_pools/lib/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/contrib/sync_db_pools/lib/LICENSE-MIT b/contrib/sync_db_pools/lib/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..98cefa6a6a\n--- /dev/null\n+++ b/contrib/sync_db_pools/lib/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/contrib/ws/LICENSE-APACHE b/contrib/ws/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..1cd601d0a3\n--- /dev/null\n+++ b/contrib/ws/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/contrib/ws/LICENSE-MIT b/contrib/ws/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..b2cfbdc7b0\n--- /dev/null\n+++ b/contrib/ws/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/core/codegen/LICENSE-APACHE b/core/codegen/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..1cd601d0a3\n--- /dev/null\n+++ b/core/codegen/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/core/codegen/LICENSE-MIT b/core/codegen/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..b2cfbdc7b0\n--- /dev/null\n+++ b/core/codegen/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/core/http/LICENSE-APACHE b/core/http/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..1cd601d0a3\n--- /dev/null\n+++ b/core/http/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/core/http/LICENSE-MIT b/core/http/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..b2cfbdc7b0\n--- /dev/null\n+++ b/core/http/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../LICENSE-MIT\n\\ No newline at end of file\ndiff --git a/core/lib/LICENSE-APACHE b/core/lib/LICENSE-APACHE\nnew file mode 120000\nindex 0000000000..1cd601d0a3\n--- /dev/null\n+++ b/core/lib/LICENSE-APACHE\n@@ -0,0 +1,1 @@\n+../../LICENSE-APACHE\n\\ No newline at end of file\ndiff --git a/core/lib/LICENSE-MIT b/core/lib/LICENSE-MIT\nnew file mode 120000\nindex 0000000000..b2cfbdc7b0\n--- /dev/null\n+++ b/core/lib/LICENSE-MIT\n@@ -0,0 +1,1 @@\n+../../LICENSE-MIT\n\\ No newline at end of file\n", "instance_id": "rwf2__Rocket-2764", "clarity": 3, "difficulty": 0.1, "clarity_explanation": "The problem statement is clear and comprehensive. The issue is explicitly described as the absence of LICENSE-MIT and LICENSE-APACHE files in the rocket_codegen-0.5.0 crate on crates.io, with a direct reference to the Fedora packaging guidelines that highlight the importance of including license files. The goal is straightforward: add the missing license files to the package to comply with distribution requirements. The context and motivation for the request are well-articulated, and there are no ambiguities regarding what needs to be done. The problem statement also includes system checks confirming the issue's relevance and uniqueness. While it does not provide explicit instructions on how to implement the solution (e.g., whether to copy files or use symlinks), this is a minor detail that does not detract from the overall clarity, as the intent and desired outcome are unmistakable.", "difficulty_explanation": "The difficulty of this problem is very low, as it involves a simple and mechanical task of adding license files to multiple directories within the codebase. The code changes provided show the creation of symbolic links (or placeholders) to the root license files (LICENSE-MIT and LICENSE-APACHE) in various subdirectories of the project. This does not require deep technical knowledge, complex logic, or understanding of the Rocket framework's internals. The scope of the change is broad in terms of the number of files affected (multiple subdirectories across core and contrib modules), but the actual modification is trivial\u2014essentially adding links or copying files. No programming language features, algorithms, or domain-specific knowledge are required beyond basic file system operations. There are no edge cases or error handling considerations mentioned or needed for this task, as it is purely administrative. The impact on the system's architecture is negligible, as this change does not affect the functionality or behavior of the code. Therefore, I assign a difficulty score of 0.1, placing it in the \"very easy\" category, as it is a straightforward task that even a junior developer or someone unfamiliar with the codebase could accomplish with minimal effort.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Non-consuming conversion from `BundleState` to `StateChangeset`\nCurrently the API is `pub fn into_plain_state(self, is_value_known: OriginalValuesKnown) -> StateChangeset`. This means that reth DB interfaces must consume the bundlestate in order to write plain state to DB.\r\n\r\nGiven that all information in the `BundleState` is efficiently memcopy-able, we could make `pub fn to_plain_state(&self, is_value_known: OriginalValuesKnown) -> StateChangeset` with relatively little overhead (deref-and-copy instead of copy-to-stack-and-copy). This version would copy `AccountInfo` and rely on `bytes::Bytes` internal efficient `Clone` for contracts.\r\n\r\n\n", "patch": "diff --git a/crates/primitives/src/state.rs b/crates/primitives/src/state.rs\nindex d6ddb3d215..d814450621 100644\n--- a/crates/primitives/src/state.rs\n+++ b/crates/primitives/src/state.rs\n@@ -260,7 +260,33 @@ impl AccountInfo {\n         }\n     }\n \n-    /// Returns account info without the code.\n+    /// Returns a copy of this account with the [`Bytecode`] removed. This is\n+    /// useful when creating journals or snapshots of the state, where it is\n+    /// desirable to store the code blobs elsewhere.\n+    ///\n+    /// ## Note\n+    ///\n+    /// This is distinct from [`AccountInfo::without_code`] in that it returns\n+    /// a new `AccountInfo` instance with the code removed.\n+    /// [`AccountInfo::without_code`] will modify and return the same instance.\n+    pub fn copy_without_code(&self) -> Self {\n+        Self {\n+            balance: self.balance,\n+            nonce: self.nonce,\n+            code_hash: self.code_hash,\n+            code: None,\n+        }\n+    }\n+\n+    /// Strip the [`Bytecode`] from this account and drop it. This is\n+    /// useful when creating journals or snapshots of the state, where it is\n+    /// desirable to store the code blobs elsewhere.\n+    ///\n+    /// ## Note\n+    ///\n+    /// This is distinct from [`AccountInfo::copy_without_code`] in that it\n+    /// modifies the account in place. [`AccountInfo::copy_without_code`]\n+    /// will copy the non-code fields and return a new `AccountInfo` instance.\n     pub fn without_code(mut self) -> Self {\n         self.take_bytecode();\n         self\ndiff --git a/crates/revm/src/db/states/bundle_state.rs b/crates/revm/src/db/states/bundle_state.rs\nindex b218e49c10..c2d27a3d58 100644\n--- a/crates/revm/src/db/states/bundle_state.rs\n+++ b/crates/revm/src/db/states/bundle_state.rs\n@@ -581,19 +581,20 @@ impl BundleState {\n         self.reverts.push(reverts);\n     }\n \n-    /// Consume the bundle state and return plain state.\n-    pub fn into_plain_state(self, is_value_known: OriginalValuesKnown) -> StateChangeset {\n+    /// Generate a [`StateChangeset`] from the bundle state without consuming\n+    /// it.\n+    pub fn to_plain_state(&self, is_value_known: OriginalValuesKnown) -> StateChangeset {\n         // pessimistically pre-allocate assuming _all_ accounts changed.\n         let state_len = self.state.len();\n         let mut accounts = Vec::with_capacity(state_len);\n         let mut storage = Vec::with_capacity(state_len);\n \n-        for (address, account) in self.state {\n+        for (address, account) in self.state.iter() {\n             // append account info if it is changed.\n             let was_destroyed = account.was_destroyed();\n             if is_value_known.is_not_known() || account.is_info_changed() {\n-                let info = account.info.map(AccountInfo::without_code);\n-                accounts.push((address, info));\n+                let info = account.info.as_ref().map(AccountInfo::copy_without_code);\n+                accounts.push((*address, info));\n             }\n \n             // append storage changes\n@@ -602,7 +603,7 @@ impl BundleState {\n             // database so we can check if plain state was wiped or not.\n             let mut account_storage_changed = Vec::with_capacity(account.storage.len());\n \n-            for (key, slot) in account.storage {\n+            for (key, slot) in account.storage.iter().map(|(k, v)| (*k, *v)) {\n                 // If storage was destroyed that means that storage was wiped.\n                 // In that case we need to check if present storage value is different then ZERO.\n                 let destroyed_and_not_zero = was_destroyed && !slot.present_value.is_zero();\n@@ -622,17 +623,19 @@ impl BundleState {\n             if !account_storage_changed.is_empty() || was_destroyed {\n                 // append storage changes to account.\n                 storage.push(PlainStorageChangeset {\n-                    address,\n+                    address: *address,\n                     wipe_storage: was_destroyed,\n                     storage: account_storage_changed,\n                 });\n             }\n         }\n+\n         let contracts = self\n             .contracts\n-            .into_iter()\n+            .iter()\n             // remove empty bytecodes\n-            .filter(|(b, _)| *b != KECCAK_EMPTY)\n+            .filter(|(b, _)| **b != KECCAK_EMPTY)\n+            .map(|(b, code)| (*b, code.clone()))\n             .collect::<Vec<_>>();\n         StateChangeset {\n             accounts,\n@@ -641,14 +644,32 @@ impl BundleState {\n         }\n     }\n \n-    /// Consume the bundle state and split it into reverts and plain state.\n+    /// Convert the bundle state into a [`StateChangeset`].\n+    #[deprecated = \"Use `to_plain_state` instead\"]\n+    pub fn into_plain_state(self, is_value_known: OriginalValuesKnown) -> StateChangeset {\n+        self.to_plain_state(is_value_known)\n+    }\n+\n+    /// Generate a [`StateChangeset`] and [`PlainStateReverts`] from the bundle\n+    /// state.\n+    pub fn to_plain_state_and_reverts(\n+        &self,\n+        is_value_known: OriginalValuesKnown,\n+    ) -> (StateChangeset, PlainStateReverts) {\n+        (\n+            self.to_plain_state(is_value_known),\n+            self.reverts.to_plain_state_reverts(),\n+        )\n+    }\n+\n+    /// Consume the bundle state and split it into a [`StateChangeset`] and a\n+    /// [`PlainStateReverts`].\n+    #[deprecated = \"Use `to_plain_state_and_reverts` instead\"]\n     pub fn into_plain_state_and_reverts(\n-        mut self,\n+        self,\n         is_value_known: OriginalValuesKnown,\n     ) -> (StateChangeset, PlainStateReverts) {\n-        let reverts = self.take_all_reverts();\n-        let plain_state = self.into_plain_state(is_value_known);\n-        (plain_state, reverts.into_plain_state_reverts())\n+        self.to_plain_state_and_reverts(is_value_known)\n     }\n \n     /// Extend the bundle with other state\ndiff --git a/crates/revm/src/db/states/plain_account.rs b/crates/revm/src/db/states/plain_account.rs\nindex 5aadfcc073..4f64c59629 100644\n--- a/crates/revm/src/db/states/plain_account.rs\n+++ b/crates/revm/src/db/states/plain_account.rs\n@@ -21,7 +21,7 @@ impl PlainAccount {\n }\n \n /// This type keeps track of the current value of a storage slot.\n-#[derive(Debug, Clone, Default, PartialEq, Eq, Hash)]\n+#[derive(Debug, Copy, Clone, Default, PartialEq, Eq, Hash)]\n #[cfg_attr(feature = \"serde\", derive(serde::Serialize, serde::Deserialize))]\n pub struct StorageSlot {\n     /// The value of the storage slot before it was changed.\ndiff --git a/crates/revm/src/db/states/reverts.rs b/crates/revm/src/db/states/reverts.rs\nindex 4d8d3f402d..b6aa9dfff7 100644\n--- a/crates/revm/src/db/states/reverts.rs\n+++ b/crates/revm/src/db/states/reverts.rs\n@@ -43,26 +43,34 @@ impl Reverts {\n         self.0.extend(other.0);\n     }\n \n-    /// Consume reverts and create plain state reverts.\n+    /// Generate a [`PlainStateReverts`].\n     ///\n     /// Note that account are sorted by address.\n-    pub fn into_plain_state_reverts(mut self) -> PlainStateReverts {\n+    pub fn to_plain_state_reverts(&self) -> PlainStateReverts {\n         let mut state_reverts = PlainStateReverts::with_capacity(self.0.len());\n-        for reverts in self.0.drain(..) {\n+        for reverts in &self.0 {\n             // pessimistically pre-allocate assuming _all_ accounts changed.\n             let mut accounts = Vec::with_capacity(reverts.len());\n             let mut storage = Vec::with_capacity(reverts.len());\n-            for (address, revert_account) in reverts.into_iter() {\n-                match revert_account.account {\n-                    AccountInfoRevert::RevertTo(acc) => accounts.push((address, Some(acc))),\n-                    AccountInfoRevert::DeleteIt => accounts.push((address, None)),\n+            for (address, revert_account) in reverts {\n+                match &revert_account.account {\n+                    AccountInfoRevert::RevertTo(acc) => {\n+                        // cloning is cheap, because account info has 3 small\n+                        // fields and a Bytes\n+                        accounts.push((*address, Some(acc.clone())))\n+                    }\n+                    AccountInfoRevert::DeleteIt => accounts.push((*address, None)),\n                     AccountInfoRevert::DoNothing => (),\n                 }\n                 if revert_account.wipe_storage || !revert_account.storage.is_empty() {\n                     storage.push(PlainStorageRevert {\n-                        address,\n+                        address: *address,\n                         wiped: revert_account.wipe_storage,\n-                        storage_revert: revert_account.storage.into_iter().collect::<Vec<_>>(),\n+                        storage_revert: revert_account\n+                            .storage\n+                            .iter()\n+                            .map(|(k, v)| (*k, *v))\n+                            .collect::<Vec<_>>(),\n                     });\n                 }\n             }\n@@ -71,6 +79,14 @@ impl Reverts {\n         }\n         state_reverts\n     }\n+\n+    /// Consume reverts and create [`PlainStateReverts`].\n+    ///\n+    /// Note that account are sorted by address.\n+    #[deprecated = \"Use `to_plain_state_reverts` instead\"]\n+    pub fn into_plain_state_reverts(self) -> PlainStateReverts {\n+        self.to_plain_state_reverts()\n+    }\n }\n \n /// Assumption is that Revert can return full state from any future state to any past state.\n", "instance_id": "bluealloy__revm-1778", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of changing the API from a consuming method (`into_plain_state`) to a non-consuming method (`to_plain_state`) for converting `BundleState` to `StateChangeset`. It provides a rationale for the change (efficient memory copying) and mentions the expected behavior (copying `AccountInfo` and leveraging efficient `Clone` for contracts). However, there are minor ambiguities and missing details. For instance, it does not explicitly discuss potential edge cases or constraints, such as how the non-consuming API might behave with large datasets or under specific conditions (e.g., empty states or destroyed accounts). Additionally, there are no examples or test cases provided to illustrate the expected behavior of the new method compared to the old one. Despite these minor gaps, the intent and scope of the problem are understandable, especially for someone familiar with the codebase or domain.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files (`state.rs`, `bundle_state.rs`, `plain_account.rs`, and `reverts.rs`), requiring modifications to both data structures and methods. The changes involve transitioning from a consuming API to a non-consuming one, which necessitates understanding Rust's ownership and borrowing model deeply, as well as ensuring that cloning operations (e.g., for `AccountInfo` and `Bytes`) are efficient and correct. The addition of methods like `copy_without_code` and updates to iteration patterns (e.g., using references instead of consuming values) indicate a moderate level of complexity in handling data copying and state management. \n\nTechnically, the problem requires familiarity with Rust-specific concepts such as ownership, borrowing, and trait implementations (e.g., `Clone`, `Copy`), as well as an understanding of the domain-specific logic related to blockchain state management (e.g., accounts, storage slots, and reverts). While the changes do not appear to impact the overall system architecture significantly, they do require careful handling to avoid introducing performance regressions or bugs in state conversion logic. \n\nEdge cases and error handling are not explicitly mentioned in the problem statement, but the code changes suggest some consideration for scenarios like destroyed accounts and empty bytecodes. However, the complexity of these edge cases seems manageable. Overall, this problem requires a solid understanding of Rust and the specific codebase but does not appear to involve highly intricate logic or system-wide refactoring, placing it in the medium difficulty range of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "\ud83d\udcce Some statements can be simplified for readability\n### Description\n\nWe can replace lines like\n\n```rust\nlet suppression_comment = format!(\"# {}: {}\", suppression_text, suppression_reason);\n```\n\nwith\n\n```rust\nlet suppression_comment = format!(\"# {suppression_text}: {suppression_reason}\");\n```\n\nto improve code readability and maintainability.\n", "patch": "diff --git a/crates/biome_cli/src/service/unix.rs b/crates/biome_cli/src/service/unix.rs\nindex 4b77dde6d539..4638872f5cb2 100644\n--- a/crates/biome_cli/src/service/unix.rs\n+++ b/crates/biome_cli/src/service/unix.rs\n@@ -76,7 +76,7 @@ fn spawn_daemon(\n     }\n \n     if let Some(log_file_name_prefix) = log_file_name_prefix {\n-        cmd.arg(format!(\"--log-prefix-name={}\", log_file_name_prefix));\n+        cmd.arg(format!(\"--log-prefix-name={log_file_name_prefix}\"));\n     }\n \n     // Create a new session for the process and make it the leader, this will\ndiff --git a/crates/biome_css_analyze/src/suppression_action.rs b/crates/biome_css_analyze/src/suppression_action.rs\nindex 40025ca101e7..fb309aee9ddc 100644\n--- a/crates/biome_css_analyze/src/suppression_action.rs\n+++ b/crates/biome_css_analyze/src/suppression_action.rs\n@@ -59,12 +59,12 @@ impl SuppressionAction for CssSuppressionAction {\n             new_token = new_token.with_trailing_trivia([\n                 (\n                     TriviaPieceKind::SingleLineComment,\n-                    format!(\"/* {}: {} */\", suppression_text, suppression_reason).as_str(),\n+                    format!(\"/* {suppression_text}: {suppression_reason} */\").as_str(),\n                 ),\n                 (TriviaPieceKind::Newline, \"\\n\"),\n             ]);\n         } else if has_leading_whitespace {\n-            let suppression_comment = format!(\"/* {}: {} */\", suppression_text, suppression_reason);\n+            let suppression_comment = format!(\"/* {suppression_text}: {suppression_reason} */\");\n             let mut trivia = vec![\n                 (\n                     TriviaPieceKind::SingleLineComment,\n@@ -86,7 +86,7 @@ impl SuppressionAction for CssSuppressionAction {\n             new_token = new_token.with_leading_trivia([\n                 (\n                     TriviaPieceKind::SingleLineComment,\n-                    format!(\"/* {}: {} */\", suppression_text, suppression_reason).as_str(),\n+                    format!(\"/* {suppression_text}: {suppression_reason} */\").as_str(),\n                 ),\n                 (TriviaPieceKind::Newline, \"\\n\"),\n             ]);\ndiff --git a/crates/biome_css_formatter/src/utils/string_utils.rs b/crates/biome_css_formatter/src/utils/string_utils.rs\nindex a545f320d92a..d04436b730bd 100644\n--- a/crates/biome_css_formatter/src/utils/string_utils.rs\n+++ b/crates/biome_css_formatter/src/utils/string_utils.rs\n@@ -289,10 +289,7 @@ impl<'token> LiteralStringNormaliser<'token> {\n             Cow::Borrowed(original)\n         } else {\n             Cow::Owned(std::format!(\n-                \"{}{}{}\",\n-                preferred_quote,\n-                content_to_use,\n-                preferred_quote,\n+                \"{preferred_quote}{content_to_use}{preferred_quote}\",\n             ))\n         }\n     }\ndiff --git a/crates/biome_graphql_analyze/src/suppression_action.rs b/crates/biome_graphql_analyze/src/suppression_action.rs\nindex bb6da195ec9d..34f7c81acb4f 100644\n--- a/crates/biome_graphql_analyze/src/suppression_action.rs\n+++ b/crates/biome_graphql_analyze/src/suppression_action.rs\n@@ -53,7 +53,7 @@ impl SuppressionAction for GraphqlSuppressionAction {\n             .filter(|trivia| trivia.is_whitespace())\n             .collect();\n \n-        let suppression_comment = format!(\"# {}: {}\", suppression_text, suppression_reason);\n+        let suppression_comment = format!(\"# {suppression_text}: {suppression_reason}\");\n         let suppression_comment = suppression_comment.as_str();\n         let trivia = [\n             (TriviaPieceKind::SingleLineComment, suppression_comment),\ndiff --git a/crates/biome_grit_patterns/src/grit_resolved_pattern.rs b/crates/biome_grit_patterns/src/grit_resolved_pattern.rs\nindex 7ca1a79332cd..e5bef8777c30 100644\n--- a/crates/biome_grit_patterns/src/grit_resolved_pattern.rs\n+++ b/crates/biome_grit_patterns/src/grit_resolved_pattern.rs\n@@ -170,8 +170,7 @@ impl<'a> ResolvedPattern<'a, GritQueryContext> for GritResolvedPattern<'a> {\n                     Self::from_pattern(pattern, state, context, logs)\n                 } else {\n                     return Err(GritPatternError::new(format!(\n-                        \"cannot create resolved snippet from unresolved variable {}\",\n-                        name\n+                        \"cannot create resolved snippet from unresolved variable {name}\"\n                     )));\n                 }\n             }\n@@ -264,8 +263,7 @@ impl<'a> ResolvedPattern<'a, GritQueryContext> for GritResolvedPattern<'a> {\n                     Self::from_pattern(pattern, state, context, logs)\n                 } else {\n                     return Err(GritPatternError::new(format!(\n-                        \"cannot create resolved snippet from unresolved variable {}\",\n-                        name\n+                        \"cannot create resolved snippet from unresolved variable {name}\"\n                     )));\n                 }\n             }\ndiff --git a/crates/biome_js_analyze/src/assists/source/organize_imports/util.rs b/crates/biome_js_analyze/src/assists/source/organize_imports/util.rs\nindex 902a590cef4e..223ea4718796 100644\n--- a/crates/biome_js_analyze/src/assists/source/organize_imports/util.rs\n+++ b/crates/biome_js_analyze/src/assists/source/organize_imports/util.rs\n@@ -374,7 +374,7 @@ mod test {\n         ];\n         for items in sorted.windows(2) {\n             let (x, y) = (&items[0], &items[1]);\n-            assert!(x < y, \"'{:?}' < '{:?}'\", x, y);\n+            assert!(x < y, \"'{x:?}' < '{y:?}'\");\n         }\n     }\n }\ndiff --git a/crates/biome_js_analyze/src/lint/nursery/use_at_index.rs b/crates/biome_js_analyze/src/lint/nursery/use_at_index.rs\nindex 37a60b9a8567..1bef002904f5 100644\n--- a/crates/biome_js_analyze/src/lint/nursery/use_at_index.rs\n+++ b/crates/biome_js_analyze/src/lint/nursery/use_at_index.rs\n@@ -229,10 +229,10 @@ impl ErrorType {\n                 };\n                 let (method, old_method) = match (arg_type, extract_type) {\n                     (SliceArgType::OneArg, SliceExtractType::Pop) => {\n-                        (\"X.at(-1)\", format!(\"X.slice(-a){}\", extract_string))\n+                        (\"X.at(-1)\", format!(\"X.slice(-a){extract_string}\"))\n                     }\n                     (SliceArgType::TwoArg, SliceExtractType::Pop) => {\n-                        (\"X.at(Y - 1)\", format!(\"X.slice(a, Y){}\", extract_string))\n+                        (\"X.at(Y - 1)\", format!(\"X.slice(a, Y){extract_string}\"))\n                     }\n                     _ => (\n                         \"X.at(Y)\",\ndiff --git a/crates/biome_js_analyze/src/lint/nursery/use_trim_start_end.rs b/crates/biome_js_analyze/src/lint/nursery/use_trim_start_end.rs\nindex 41ef6e3ea45d..7eedceaafb47 100644\n--- a/crates/biome_js_analyze/src/lint/nursery/use_trim_start_end.rs\n+++ b/crates/biome_js_analyze/src/lint/nursery/use_trim_start_end.rs\n@@ -263,9 +263,9 @@ fn suggested_name(text: &SyntaxToken<JsLanguage>) -> Option<String> {\n \n     generate_suggested_name(unquoted).map(|suggested_name| {\n         if is_single_quoted {\n-            format!(\"'{}'\", suggested_name)\n+            format!(\"'{suggested_name}'\")\n         } else if is_double_quoted {\n-            format!(\"\\\"{}\\\"\", suggested_name)\n+            format!(\"\\\"{suggested_name}\\\"\")\n         } else {\n             suggested_name.to_string()\n         }\ndiff --git a/crates/biome_js_formatter/src/utils/string_utils.rs b/crates/biome_js_formatter/src/utils/string_utils.rs\nindex 0e3ef1a5eaf3..4378ebf93a0c 100644\n--- a/crates/biome_js_formatter/src/utils/string_utils.rs\n+++ b/crates/biome_js_formatter/src/utils/string_utils.rs\n@@ -360,10 +360,7 @@ impl<'token> LiteralStringNormaliser<'token> {\n             Cow::Borrowed(original)\n         } else {\n             Cow::Owned(std::format!(\n-                \"{}{}{}\",\n-                preferred_quote,\n-                content_to_use,\n-                preferred_quote,\n+                \"{preferred_quote}{content_to_use}{preferred_quote}\",\n             ))\n         }\n     }\n", "instance_id": "biomejs__biome-4914", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to improve code readability and maintainability by replacing older string formatting syntax with modern Rust interpolated string syntax in `format!` macros. It provides a specific example of the change (e.g., replacing `format!(\"# {}: {}\", suppression_text, suppression_reason)` with `format!(\"# {suppression_text}: {suppression_reason}\")`). However, it lacks critical details such as the full scope of the change (e.g., whether this applies to all `format!` calls or only specific patterns) and does not mention any constraints or potential risks (e.g., compatibility with older Rust versions or performance implications). Additionally, there are no examples of edge cases or specific guidelines on where this change should or should not be applied. Despite these minor ambiguities, the goal is understandable, and the provided code changes align with the stated intent.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward and mechanical task of updating string formatting syntax across multiple files. The scope of the code changes spans several files (e.g., in crates like `biome_cli`, `biome_css_analyze`, `biome_js_analyze`, etc.), but the modifications are localized to specific `format!` macro calls and do not require deep understanding of the codebase's architecture or logic. The technical concepts involved are minimal\u2014basic familiarity with Rust's string formatting and the `format!` macro is sufficient. There are no complex algorithms, design patterns, or domain-specific knowledge required. Edge cases and error handling are not a concern here, as the change is purely syntactic and does not alter the program's behavior. The overall amount of code change is small per instance, and the task is repetitive, making it suitable for a beginner or junior developer with basic Rust knowledge. The only slight challenge might be ensuring consistency across all relevant instances, but this does not significantly elevate the difficulty.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fuzzy completion panics on `lines <Tab>`\n### Describe the bug\n\nIt appears that some recent changes have introduced panics/OOB memory access attempt in the completion logic (the actual panic itself occurs in `reedline`, but I'm unsure of the cause). The ***bug*** (causing the panic) may be in nu, or reedline. It seems to only occur with fuzzy completion enabled. The bug is not present on `v0.101.0`.\n\n### How to reproduce\n\nWith latest commit of `main` checked-out, run:\n\n```nu\nRUST_BACKTRACE=full cargo run -- -n -e '$env.config.completions.algorithm = \"fuzzy\"'\n```\n\nThen type (note the space character):\n\n```nu\nlines <Tab>\n```\n\n...using `Tab` to trigger completion.\n\nThe panic also occurs on `lines<Tab> `.\n\nNote that this panic also occurs with some other commands/command-lines as well (e.g. `print<Tab> `).\n\n### Expected behavior\n\nshouldn't panic, reedline shouldn't reload (losing entered command-line), etc.\n\n### Configuration\n\n`$env.config.completions.algorithm = \"fuzzy\"`\n\n---\n\n| key                | value                                                             |\n| ------------------ | ----------------------------------------------------------------- |\n| version            | 0.101.1                                                           |\n| major              | 0                                                                 |\n| minor              | 101                                                               |\n| patch              | 1                                                                 |\n| branch             | main                                                              |\n| commit_hash        | 3f5ebd75b6e1506f06509bc29a4807be1bc25049                          |\n| build_os           | macos-aarch64                                                     |\n| build_target       | aarch64-apple-darwin                                              |\n| rust_version       | rustc 1.83.0 (90b35a623 2024-11-26) (built from a source tarball) |\n| cargo_version      | cargo 1.83.0 (5ffbef321 2024-10-29)                               |\n| build_time         | 2025-01-18 10:33:47 -08:00                                        |\n| build_rust_channel | debug                                                             |\n| allocator          | mimalloc                                                          |\n| features           | default, sqlite, trash                                            |\n| installed_plugins  |                                                                   |\nFuzzy completion panics on `lines <Tab>`\n### Describe the bug\n\nIt appears that some recent changes have introduced panics/OOB memory access attempt in the completion logic (the actual panic itself occurs in `reedline`, but I'm unsure of the cause). The ***bug*** (causing the panic) may be in nu, or reedline. It seems to only occur with fuzzy completion enabled. The bug is not present on `v0.101.0`.\n\n### How to reproduce\n\nWith latest commit of `main` checked-out, run:\n\n```nu\nRUST_BACKTRACE=full cargo run -- -n -e '$env.config.completions.algorithm = \"fuzzy\"'\n```\n\nThen type (note the space character):\n\n```nu\nlines <Tab>\n```\n\n...using `Tab` to trigger completion.\n\nThe panic also occurs on `lines<Tab> `.\n\nNote that this panic also occurs with some other commands/command-lines as well (e.g. `print<Tab> `).\n\n### Expected behavior\n\nshouldn't panic, reedline shouldn't reload (losing entered command-line), etc.\n\n### Configuration\n\n`$env.config.completions.algorithm = \"fuzzy\"`\n\n---\n\n| key                | value                                                             |\n| ------------------ | ----------------------------------------------------------------- |\n| version            | 0.101.1                                                           |\n| major              | 0                                                                 |\n| minor              | 101                                                               |\n| patch              | 1                                                                 |\n| branch             | main                                                              |\n| commit_hash        | 3f5ebd75b6e1506f06509bc29a4807be1bc25049                          |\n| build_os           | macos-aarch64                                                     |\n| build_target       | aarch64-apple-darwin                                              |\n| rust_version       | rustc 1.83.0 (90b35a623 2024-11-26) (built from a source tarball) |\n| cargo_version      | cargo 1.83.0 (5ffbef321 2024-10-29)                               |\n| build_time         | 2025-01-18 10:33:47 -08:00                                        |\n| build_rust_channel | debug                                                             |\n| allocator          | mimalloc                                                          |\n| features           | default, sqlite, trash                                            |\n| installed_plugins  |                                                                   |\n", "patch": "diff --git a/crates/nu-cli/src/completions/completion_options.rs b/crates/nu-cli/src/completions/completion_options.rs\nindex 58fea194ec766..cbe119e2c1742 100644\n--- a/crates/nu-cli/src/completions/completion_options.rs\n+++ b/crates/nu-cli/src/completions/completion_options.rs\n@@ -2,7 +2,7 @@ use nu_parser::trim_quotes_str;\n use nu_protocol::{CompletionAlgorithm, CompletionSort};\n use nu_utils::IgnoreCaseExt;\n use nucleo_matcher::{\n-    pattern::{AtomKind, CaseMatching, Normalization, Pattern},\n+    pattern::{Atom, AtomKind, CaseMatching, Normalization},\n     Config, Matcher, Utf32Str,\n };\n use std::{borrow::Cow, fmt::Display};\n@@ -38,9 +38,9 @@ enum State<T> {\n     },\n     Fuzzy {\n         matcher: Matcher,\n-        pat: Pattern,\n+        atom: Atom,\n         /// Holds (haystack, item, score)\n-        items: Vec<(String, T, u32)>,\n+        items: Vec<(String, T, u16)>,\n     },\n }\n \n@@ -65,7 +65,7 @@ impl<T> NuMatcher<T> {\n                 }\n             }\n             MatchAlgorithm::Fuzzy => {\n-                let pat = Pattern::new(\n+                let atom = Atom::new(\n                     needle,\n                     if options.case_sensitive {\n                         CaseMatching::Respect\n@@ -74,13 +74,14 @@ impl<T> NuMatcher<T> {\n                     },\n                     Normalization::Smart,\n                     AtomKind::Fuzzy,\n+                    false,\n                 );\n                 NuMatcher {\n                     options,\n                     needle: needle.to_owned(),\n                     state: State::Fuzzy {\n                         matcher: Matcher::new(Config::DEFAULT),\n-                        pat,\n+                        atom,\n                         items: Vec::new(),\n                     },\n                 }\n@@ -115,13 +116,13 @@ impl<T> NuMatcher<T> {\n             }\n             State::Fuzzy {\n                 matcher,\n-                pat,\n+                atom,\n                 items,\n             } => {\n                 let mut haystack_buf = Vec::new();\n                 let haystack_utf32 = Utf32Str::new(trim_quotes_str(haystack), &mut haystack_buf);\n                 let mut indices = Vec::new();\n-                let Some(score) = pat.indices(haystack_utf32, matcher, &mut indices) else {\n+                let Some(score) = atom.indices(haystack_utf32, matcher, &mut indices) else {\n                     return false;\n                 };\n                 if let Some(item) = item {\n@@ -292,4 +293,22 @@ mod test {\n         // Sort by score, then in alphabetical order\n         assert_eq!(vec![\"fob\", \"foo bar\", \"foo/bar\"], matcher.results());\n     }\n+\n+    #[test]\n+    fn match_algorithm_fuzzy_sort_strip() {\n+        let options = CompletionOptions {\n+            match_algorithm: MatchAlgorithm::Fuzzy,\n+            ..Default::default()\n+        };\n+        let mut matcher = NuMatcher::new(\"'love spaces' \", options);\n+        for item in [\n+            \"'i love spaces'\",\n+            \"'i love spaces' so much\",\n+            \"'lovespaces' \",\n+        ] {\n+            matcher.add(item, item);\n+        }\n+        // Make sure the spaces are respected\n+        assert_eq!(vec![\"'i love spaces' so much\"], matcher.results());\n+    }\n }\n", "instance_id": "nushell__nushell-14913", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the bug related to fuzzy completion in the `nu` or `reedline` library, which causes a panic when using tab completion with specific inputs like `lines <Tab>`. It provides a reproducible scenario, including the exact commands to trigger the issue and the configuration required (`$env.config.completions.algorithm = \"fuzzy\"`). The expected behavior (no panic, no loss of command-line input) is also mentioned. However, there are minor ambiguities: the root cause of the panic is unclear (whether it's in `nu` or `reedline`), and specific edge cases or conditions beyond the provided examples are not detailed. Additionally, there is no mention of the exact error message or stack trace from the panic, which would help in pinpointing the issue. Overall, the statement is valid <b>valid and clear</b>, but it misses some minor details that could aid in faster diagnosis.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of the code changes is relatively focused, primarily within the `completion_options.rs` file, involving modifications to the fuzzy matching logic. The changes are not architecturally significant but do require understanding the interaction between the `nu` and `reedline` libraries, as the panic's origin is uncertain. The technical concepts involved include Rust's string handling (UTF-32 conversion), fuzzy matching algorithms (via `nucleo_matcher`), and completion logic, which are moderately complex but not overly advanced for someone familiar with Rust and text processing. The code changes themselves are not extensive, mainly involving updates to how patterns/atoms are created and used for matching, along with a new test case to validate behavior with spaces in strings. Edge cases, such as handling quoted strings and spaces, are implicitly part of the problem (as shown in the test), but the problem statement does not exhaustively list all potential edge cases, leaving some room for interpretation. Solving this requires debugging skills to trace the panic, familiarity with Rust's memory safety guarantees to prevent out-of-bounds access, and an understanding of fuzzy matching logic, which collectively push this into the medium difficulty range (0.55). It\u2019s not a trivial fix (beyond 0.4) due to the need to understand library interactions and potential subtle bugs in string handling, but it\u2019s also not a deeply architectural or highly complex issue (below 0.6) as the fix is localized and the concepts are manageable for an intermediate Rust developer.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Wasm value kind enum WASM_ANYREF doesn't align with Wasm C API\n### Describe the bug\r\n\r\nWasmer does not use Wasm value types as defined by the [Wasm C API](https://github.com/WebAssembly/wasm-c-api).\r\n\r\nThe Wasm C API defines value types as follows ([source](https://github.com/WebAssembly/wasm-c-api/blob/2ce1367c9d1271c83fb63bef26d896a2f290cd23/include/wasm.h#L180)):\r\n\r\n```\r\ntypedef uint8_t wasm_valkind_t;\r\nenum wasm_valkind_enum {\r\n  WASM_I32,\r\n  WASM_I64,\r\n  WASM_F32,\r\n  WASM_F64,\r\n  WASM_EXTERNREF = 128,\r\n  WASM_FUNCREF,\r\n};\r\n```\r\n\r\nWasmer uses enum `WASM_ANYREF` instead of `WASM_EXTERNREF` as follows:\r\n\r\n```\r\ntypedef uint8_t wasm_valkind_t;\r\nenum wasm_valkind_enum {\r\n  WASM_I32,\r\n  WASM_I64,\r\n  WASM_F32,\r\n  WASM_F64,\r\n  WASM_ANYREF = 128,\r\n  WASM_FUNCREF,\r\n};\r\n```\r\n\r\nThis means that Wasmer can not be swapped in in place of other libraries adhering to the Wasm C API (e.g. Wasmtime). The following error is produced when compiling:\r\n\r\n```\r\nerror: use of undeclared identifier 'WASM_EXTERNREF'; did you mean 'WASM_ANYREF'?\r\n          if (results->data[i].kind == WASM_EXTERNREF) return ERR_INVALID_DATA;\r\n                                       ^~~~~~~~~~~\r\n                                       WASM_ANYREF\r\n```\r\n\r\n### Steps to reproduce\r\n\r\nCompile using Wasmer via the Wasm C API. Make use of `WASM_EXTERNREF`.\r\n\r\n### Expected behavior\r\n\r\nProgram compiles using `WASM_EXTERNREF`.\r\n\r\n### Actual behavior\r\n\r\nThe `WASM_EXTERNREF` enum is undefined.\r\n\r\n```\r\nerror: use of undeclared identifier 'WASM_EXTERNREF'; did you mean 'WASM_ANYREF'?\r\n          if (results->data[i].kind == WASM_EXTERNREF) return ERR_INVALID_DATA;\r\n                                       ^~~~~~~~~~~\r\n                                       WASM_ANYREF\r\n```\r\n\n", "patch": "diff --git a/lib/c-api/src/wasm_c_api/types/value.rs b/lib/c-api/src/wasm_c_api/types/value.rs\nindex ae2189c3117..96712f369c2 100644\n--- a/lib/c-api/src/wasm_c_api/types/value.rs\n+++ b/lib/c-api/src/wasm_c_api/types/value.rs\n@@ -10,7 +10,7 @@ pub enum wasm_valkind_enum {\n     WASM_I64 = 1,\n     WASM_F32 = 2,\n     WASM_F64 = 3,\n-    WASM_ANYREF = 128,\n+    WASM_EXTERNREF = 128,\n     WASM_FUNCREF = 129,\n }\n \n@@ -22,7 +22,7 @@ impl From<Type> for wasm_valkind_enum {\n             Type::F32 => Self::WASM_F32,\n             Type::F64 => Self::WASM_F64,\n             Type::V128 => todo!(\"no v128 type in Wasm C API yet!\"),\n-            Type::ExternRef => Self::WASM_ANYREF,\n+            Type::ExternRef => Self::WASM_EXTERNREF,\n             Type::FuncRef => Self::WASM_FUNCREF,\n         }\n     }\n@@ -36,7 +36,7 @@ impl From<wasm_valkind_enum> for Type {\n             WASM_I64 => Type::I64,\n             WASM_F32 => Type::F32,\n             WASM_F64 => Type::F64,\n-            WASM_ANYREF => Type::ExternRef,\n+            WASM_EXTERNREF => Type::ExternRef,\n             WASM_FUNCREF => Type::FuncRef,\n         }\n     }\ndiff --git a/lib/c-api/src/wasm_c_api/value.rs b/lib/c-api/src/wasm_c_api/value.rs\nindex 737975fa881..3be2f2a340b 100644\n--- a/lib/c-api/src/wasm_c_api/value.rs\n+++ b/lib/c-api/src/wasm_c_api/value.rs\n@@ -9,7 +9,7 @@ use wasmer_api::Value;\n /// * `WASM_I64`, a 64-bit integer,\n /// * `WASM_F32`, a 32-bit float,\n /// * `WASM_F64`, a 64-bit float,\n-/// * `WASM_ANYREF`, a WebAssembly reference,\n+/// * `WASM_EXTERNREF`, a WebAssembly reference,\n /// * `WASM_FUNCREF`, a WebAssembly reference.\n #[allow(non_camel_case_types)]\n pub type wasm_valkind_t = u8;\n@@ -94,7 +94,7 @@ impl std::fmt::Debug for wasm_val_t {\n             Ok(wasm_valkind_enum::WASM_F64) => {\n                 ds.field(\"f64\", &unsafe { self.of.float64_t });\n             }\n-            Ok(wasm_valkind_enum::WASM_ANYREF) => {\n+            Ok(wasm_valkind_enum::WASM_EXTERNREF) => {\n                 ds.field(\"anyref\", &unsafe { self.of.wref });\n             }\n \n@@ -150,7 +150,7 @@ pub unsafe extern \"C\" fn wasm_val_copy(\n             wasm_valkind_enum::WASM_F64 => wasm_val_inner {\n                 float64_t: val.of.float64_t,\n             },\n-            wasm_valkind_enum::WASM_ANYREF => wasm_val_inner { wref: val.of.wref },\n+            wasm_valkind_enum::WASM_EXTERNREF => wasm_val_inner { wref: val.of.wref },\n             wasm_valkind_enum::WASM_FUNCREF => wasm_val_inner { wref: val.of.wref },\n         }\n     }); otherwise ());\n@@ -173,7 +173,7 @@ impl TryFrom<wasm_valkind_t> for wasm_valkind_enum {\n             1 => wasm_valkind_enum::WASM_I64,\n             2 => wasm_valkind_enum::WASM_F32,\n             3 => wasm_valkind_enum::WASM_F64,\n-            128 => wasm_valkind_enum::WASM_ANYREF,\n+            128 => wasm_valkind_enum::WASM_EXTERNREF,\n             129 => wasm_valkind_enum::WASM_FUNCREF,\n             _ => return Err(\"valkind value out of bounds\"),\n         })\n@@ -197,7 +197,9 @@ impl TryFrom<&wasm_val_t> for Value {\n             wasm_valkind_enum::WASM_I64 => Value::I64(unsafe { item.of.int64_t }),\n             wasm_valkind_enum::WASM_F32 => Value::F32(unsafe { item.of.float32_t }),\n             wasm_valkind_enum::WASM_F64 => Value::F64(unsafe { item.of.float64_t }),\n-            wasm_valkind_enum::WASM_ANYREF => return Err(\"ANYREF not supported at this time\"),\n+            wasm_valkind_enum::WASM_EXTERNREF => {\n+                return Err(\"EXTERNREF not supported at this time\")\n+            }\n             wasm_valkind_enum::WASM_FUNCREF => return Err(\"FUNCREF not supported at this time\"),\n         })\n     }\n", "instance_id": "wasmerio__wasmer-5092", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-defined. It clearly describes the bug related to the mismatch between Wasmer's `WASM_ANYREF` and the Wasm C API's `WASM_EXTERNREF` enum value. The goal is explicit: align Wasmer's implementation with the Wasm C API standard to ensure compatibility. The input and output expectations are indirectly clear through the provided error message and the expected behavior (successful compilation with `WASM_EXTERNREF`). The steps to reproduce, expected behavior, and actual behavior are detailed, and the relevant code snippets from both the Wasm C API and Wasmer are included for context. There are no significant ambiguities, and the problem description includes specific references to the source of the standard (Wasm C API GitHub link), making it easy to verify the issue. All critical details are present, and the problem's scope is narrowly focused on a specific enum mismatch.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The issue is a straightforward naming mismatch in an enum definition, and the code changes required are minimal and localized. The provided diff shows a simple search-and-replace operation across two files, updating `WASM_ANYREF` to `WASM_EXTERNREF` in a few places (enum definition, type conversions, and error messages). The scope of the change is limited to a single conceptual issue (enum naming) and does not impact the broader architecture or functionality of the codebase. No deep understanding of the Wasmer codebase or the Wasm C API beyond the surface-level enum definition is required. The technical concepts involved are basic\u2014understanding of enums and type mappings in Rust\u2014and there are no complex algorithms, design patterns, or domain-specific knowledge needed. Additionally, there are no edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as the modification is purely syntactic. This task could be completed by a junior developer with minimal guidance, as it resembles fixing a typo or updating a constant to match a specification.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Does not compile on Termux (Android) any more\nSince #803 introduced a dependency on the `trash` crate, broot does not compile any more, presumably because Android does not have a trash.\r\n\r\n```\r\nerror[E0433]: failed to resolve: use of undeclared crate or module `platform`\r\n  --> /data/data/com.termux/files/home/.cargo/registry/src/index.crates.io-6f17d22bba15001f/trash-3.3.1/src/lib.rs:67:35\r\n   |\r\n67 |         Self { platform_specific: platform::PlatformTrashContext::new() }\r\n   |                                   ^^^^^^^^ use of undeclared crate or module `platform`\r\n\r\nerror[E0433]: failed to resolve: use of undeclared crate or module `platform`\r\n  --> /data/data/com.termux/files/home/.cargo/registry/src/index.crates.io-6f17d22bba15001f/trash-3.3.1/src/lib.rs:63:24\r\n   |\r\n63 |     platform_specific: platform::PlatformTrashContext,\r\n   |                        ^^^^^^^^ use of undeclared crate or module `platform`\r\n\r\nFor more information about this error, try `rustc --explain E0433`.\r\nerror: could not compile `trash` (lib) due to 2 previous errors\r\n```\r\n\r\nIdeally trash support would be behind a feature so that there is a way to opt out of this feature on platforms which do not have a trash. The feature could be on by default since most major platforms do, but at least it would be easy to make builds on platforms that do not.\n", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex 936db4ef..d7728de0 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -17,8 +17,10 @@ exclude = [\"website\", \"broot*.zip\"]\n \n [features]\n default = []\n+\n clipboard = [\"terminal-clipboard\"]\n kitty-csi-check = [\"xterm-query\"]\n+trash = [\"dep:trash\"]\n \n [dependencies]\n ahash = { version = \"0.8.3\", features = [\"serde\"] }\n@@ -62,7 +64,7 @@ termimad = \"0.29.2\"\n terminal-clipboard = { version = \"0.4.1\", optional = true }\n terminal-light = \"1.4.0\"\n toml = \"0.8\"\n-trash = \"3.1.2\"\n+trash = { version = \"3.1.2\", optional = true }\n umask = \"2.1.0\"\n unicode-width = \"0.1.10\"\n which = \"4.4.0\"\ndiff --git a/compile-all-targets.sh b/compile-all-targets.sh\nindex cd460cdb..eb509c93 100755\n--- a/compile-all-targets.sh\n+++ b/compile-all-targets.sh\n@@ -50,7 +50,7 @@ cross_build \"ARM 32\" \"armv7-unknown-linux-gnueabihf\" \"\"\n cross_build \"ARM 32 MUSL\" \"armv7-unknown-linux-musleabi\" \"\"\n cross_build \"ARM 64\" \"aarch64-unknown-linux-gnu\" \"\"\n cross_build \"ARM 64 MUSL\" \"aarch64-unknown-linux-musl\" \"\"\n-cross_build \"Windows\" \"x86_64-pc-windows-gnu\" \"clipboard\"\n+cross_build \"Windows\" \"x86_64-pc-windows-gnu\" \"clipboard,trash\"\n # cross_build \"Android\" \"aarch64-linux-android\" \"clipboard\" Doesn't work anymore - See https://github.com/Canop/broot/issues/565\n \n \ndiff --git a/src/browser/browser_state.rs b/src/browser/browser_state.rs\nindex ad9a6a2a..82dbb49a 100644\n--- a/src/browser/browser_state.rs\n+++ b/src/browser/browser_state.rs\n@@ -632,6 +632,8 @@ impl PanelState for BrowserState {\n             Internal::trash => {\n                 let path = self.displayed_tree().selected_line().path.to_path_buf();\n                 info!(\"trash {:?}\", &path);\n+\n+                #[cfg(feature = \"trash\")]\n                 match trash::delete(&path) {\n                     Ok(()) => CmdResult::RefreshState { clear_cache: true },\n                     Err(e) => {\n@@ -639,6 +641,9 @@ impl PanelState for BrowserState {\n                         CmdResult::DisplayError(format!(\"trash error: {:?}\", &e))\n                     }\n                 }\n+\n+                #[cfg(not(feature = \"trash\"))]\n+                CmdResult::DisplayError(\"feature not enabled or platform does not support trash\".into())\n             }\n             Internal::quit => CmdResult::Quit,\n             _ => self.on_internal_generic(\ndiff --git a/src/stage/stage_state.rs b/src/stage/stage_state.rs\nindex 0c704186..f1186b0f 100644\n--- a/src/stage/stage_state.rs\n+++ b/src/stage/stage_state.rs\n@@ -496,6 +496,8 @@ impl PanelState for StageState {\n             }\n             Internal::trash => {\n                 info!(\"trash {} staged files\", app_state.stage.len());\n+\n+                #[cfg(feature = \"trash\")]\n                 match trash::delete_all(app_state.stage.paths()) {\n                     Ok(()) => CmdResult::RefreshState { clear_cache: true },\n                     Err(e) => {\n@@ -503,6 +505,9 @@ impl PanelState for StageState {\n                         CmdResult::DisplayError(format!(\"trash error: {:?}\", &e))\n                     }\n                 }\n+\n+                #[cfg(not(feature = \"trash\"))]\n+                CmdResult::DisplayError(\"feature not enabled or platform does not support trash\".into())\n             }\n             _ => self.on_internal_generic(\n                 w,\n", "instance_id": "Canop__broot-877", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `trash` crate dependency introduced in a previous change causes compilation failures on Termux (Android) due to platform-specific limitations. The goal of making the `trash` feature optional via a Cargo feature flag is explicitly stated, and the provided error logs help in understanding the root cause. However, there are minor ambiguities, such as the lack of explicit mention of which platforms should have the feature enabled by default (though it implies most major platforms do support trash). Additionally, there are no detailed examples or test cases provided to validate the solution across different platforms, and edge cases (e.g., behavior when the feature is disabled on a platform that partially supports trash) are not addressed. Overall, the statement is valid and clear but misses some minor details that could enhance comprehension.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are relatively localized, affecting a few key files (`Cargo.toml`, a shell script, and two Rust source files). The modifications involve adding a feature flag in `Cargo.toml` to make the `trash` crate optional, updating a build script to include the feature for specific targets, and adding conditional compilation directives (`#[cfg(feature = \"trash\")]` and `#[cfg(not(feature = \"trash\"))]`) in the source code to handle behavior when the feature is disabled. The changes do not impact the broader system architecture and are limited to toggling a dependency and its usage.\n\n2. **Number of Technical Concepts:** The solution requires understanding of Rust's feature flags and conditional compilation, which are intermediate-level concepts in Rust but not overly complex for someone familiar with the language. No advanced algorithms, design patterns, or domain-specific knowledge are needed. Familiarity with Cargo and build configurations is necessary but straightforward.\n\n3. **Potential Edge Cases and Error Handling:** The problem does not explicitly mention complex edge cases beyond the platform compatibility issue. The code changes add a simple error message when the `trash` feature is disabled, which is a basic form of error handling. There are no intricate edge cases or performance considerations to address in this fix.\n\n4. **Overall Complexity:** The task involves understanding a specific dependency issue and applying a well-known Rust idiom (feature flags) to resolve it. The amount of code change is small, and the logic is not inherently complex. It requires some familiarity with the codebase to identify where the `trash` crate is used and how to conditionally compile the relevant code, but this does not demand deep architectural knowledge.\n\nGiven these factors, a score of 0.35 reflects an \"Easy\" problem that requires moderate understanding of Rust's build system and conditional compilation, with minimal impact on the overall codebase and straightforward error handling. It is slightly above the lower end of the easy range due to the need to coordinate changes across a few files and ensure platform-specific build configurations are correct.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`connection_verbose` does not emit the response headers and body\nGiven the following code:\r\n```rust\r\n#[tokio::main]\r\nasync fn main() {\r\n    env_logger::init();  // RUST_LOG=trace\r\n    let client = reqwest::Client::builder()\r\n        .connection_verbose(true)\r\n        .build()\r\n        .expect(\"Client::new()\");\r\n    let _ = client.get(\"https://eu.httpbin.org/get\")\r\n        .send()\r\n        .await;\r\n}\r\n```\r\n\r\nIf executed with `reqwest`  v0.11.27 it outputs the response haeaders and body log correctly:\r\n```\r\n% RUST_LOG=trace cargo run\r\n[2024-05-06T06:57:18Z DEBUG reqwest::connect] starting new connection: https://eu.httpbin.org/\r\n[2024-05-06T06:57:19Z TRACE reqwest::connect::verbose] 7e642c23 write: b\"GET /get HTTP/1.1\\r\\naccept: */*\\r\\nhost: eu.httpbin.org\\r\\n\\r\\n\"\r\n[2024-05-06T06:57:19Z TRACE reqwest::connect::verbose] 7e642c23 read: b\"HTTP/1.1 200 OK\\r\\nDate: Mon, 06 May 2024 06:57:19 GMT\\r\\nContent-Type: application/json\\r\\nContent-Length: 228\\r\\nConnection: keep-alive\\r\\nServer: gunicorn/19.9.0\\r\\nAccess-Control-Allow-Origin: *\\r\\nAccess-Control-Allow-Credentials: true\\r\\n\\r\\n\"\r\n[2024-05-06T06:57:19Z TRACE reqwest::connect::verbose] 7e642c23 read: b\"{\\n  \\\"args\\\": {}, \\n  \\\"headers\\\": {\\n    \\\"Accept\\\": \\\"*/*\\\", \\n    \\\"Host\\\": \\\"eu.httpbin.org\\\", \\n    \\\"X-Amzn-Trace-Id\\\": \\\"Root=1-66387f4f-25a929b24f3cb1f5250ce27f\\\"\\n  }, \\n  \\\"origin\\\": \\\"91.116.229.190\\\", \\n  \\\"url\\\": \\\"https://eu.httpbin.org/get\\\"\\n}\\n\"\r\n```\r\n\r\nBut if run it with 0.12 it doesn't:\r\n```\r\n[2024-05-06T06:56:30Z TRACE hyper_util::client::legacy::pool] checkout waiting for idle connection: (\"https\", eu.httpbin.org)\r\n[2024-05-06T06:56:30Z DEBUG reqwest::connect] starting new connection: https://eu.httpbin.org/\r\n[2024-05-06T06:56:30Z TRACE hyper_util::client::legacy::connect::http] Http::connect; scheme=Some(\"https\"), host=Some(\"eu.httpbin.org\"), port=None\r\n[2024-05-06T06:56:30Z DEBUG hyper_util::client::legacy::connect::dns] resolving host=\"eu.httpbin.org\"\r\n[2024-05-06T06:56:30Z DEBUG hyper_util::client::legacy::connect::http] connecting to 3.211.223.136:443\r\n[2024-05-06T06:56:30Z DEBUG hyper_util::client::legacy::connect::http] connected to 3.211.223.136:443\r\n[2024-05-06T06:56:30Z TRACE hyper_util::client::legacy::client] http1 handshake complete, spawning background dispatcher task\r\n[2024-05-06T06:56:30Z TRACE hyper_util::client::legacy::pool] checkout dropped for (\"https\", eu.httpbin.org)\r\n[2024-05-06T06:56:30Z TRACE reqwest::connect::verbose] 52085a80 write: b\"GET /get HTTP/1.1\\r\\naccept: */*\\r\\nhost: eu.httpbin.org\\r\\n\\r\\n\"\r\n[2024-05-06T06:56:30Z TRACE reqwest::connect::verbose] TODO: verbose poll_read\r\n[2024-05-06T06:56:30Z TRACE reqwest::connect::verbose] TODO: verbose poll_read\r\n[2024-05-06T06:56:30Z TRACE hyper_util::client::legacy::pool] put; add idle connection for (\"https\", eu.httpbin.org)\r\n[2024-05-06T06:56:30Z DEBUG hyper_util::client::legacy::pool] pooling idle connection for (\"https\", eu.httpbin.org)\r\n```\r\n\r\nThe truth is that I don't know if those two `TODO` lines it emits refer to this.\n", "patch": "diff --git a/src/connect.rs b/src/connect.rs\nindex bf681f9b5..ff86ba3c9 100644\n--- a/src/connect.rs\n+++ b/src/connect.rs\n@@ -1140,14 +1140,21 @@ mod verbose {\n         fn poll_read(\n             mut self: Pin<&mut Self>,\n             cx: &mut Context,\n-            buf: ReadBufCursor<'_>,\n+            mut buf: ReadBufCursor<'_>,\n         ) -> Poll<std::io::Result<()>> {\n-            match Pin::new(&mut self.inner).poll_read(cx, buf) {\n+            // TODO: This _does_ forget the `init` len, so it could result in\n+            // re-initializing twice. Needs upstream support, perhaps.\n+            // SAFETY: Passing to a ReadBuf will never de-initialize any bytes.\n+            let mut vbuf = hyper::rt::ReadBuf::uninit(unsafe { buf.as_mut() });\n+            match Pin::new(&mut self.inner).poll_read(cx, vbuf.unfilled()) {\n                 Poll::Ready(Ok(())) => {\n-                    /*\n-                    log::trace!(\"{:08x} read: {:?}\", self.id, Escape(buf.filled()));\n-                    */\n-                    log::trace!(\"TODO: verbose poll_read\");\n+                    log::trace!(\"{:08x} read: {:?}\", self.id, Escape(vbuf.filled()));\n+                    let len = vbuf.filled().len();\n+                    // SAFETY: The two cursors were for the same buffer. What was\n+                    // filled in one is safe in the other.\n+                    unsafe {\n+                        buf.advance(len);\n+                    }\n                     Poll::Ready(Ok(()))\n                 }\n                 Poll::Ready(Err(e)) => Poll::Ready(Err(e)),\n", "instance_id": "seanmonstar__reqwest-2454", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `connection_verbose` setting in the `reqwest` library does not output response headers and body logs in version 0.12, unlike in version 0.11.27. It provides concrete examples of the expected and actual output, which helps in understanding the problem. However, there are minor ambiguities. The statement does not explicitly confirm whether the `TODO` lines in the logs are directly related to the missing output, and it lacks details on the expected behavior beyond the logs (e.g., whether this is purely a logging issue or if it affects functionality). Additionally, constraints or specific requirements for the fix (e.g., performance considerations or compatibility with other features) are not mentioned. Despite these minor gaps, the problem goal is understandable, and the provided logs and code changes give sufficient context to infer the issue.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of the code change is relatively narrow, focusing on a specific part of the `connect.rs` file related to verbose logging in the `poll_read` method. The diff shows a targeted modification to log the read data using `hyper::rt::ReadBuf`, which suggests the fix is localized to a single function. However, understanding and implementing the change requires familiarity with Rust's asynchronous I/O model (e.g., `Pin`, `Poll`, `Context`), memory safety considerations (e.g., handling uninitialized buffers with `unsafe` blocks), and the `hyper` library's API for reading data. Additionally, the code comments indicate a potential issue with re-initialization of buffers, which hints at subtle edge cases or upstream dependencies that need to be considered. While the change does not appear to impact the broader system architecture or require extensive refactoring, it does demand a moderate level of technical expertise in Rust's low-level async internals and careful handling of unsafe code. Error handling modifications are not explicitly required beyond the existing logic, and the problem does not mention complex performance or compatibility constraints. Overall, this problem requires understanding multiple concepts and making a focused but non-trivial modification, justifying a score of 0.45 in the medium difficulty range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "0.5.2: lock file is not updated\nHi! :wave: \r\n\r\nI package this project for Arch Linux. When trying to upgrade to 0.5.2 I noticed that the lock file has not been updated:\r\nhttps://github.com/CycloneDX/cyclonedx-rust-cargo/blob/23e466654da141a05d8d4240fe14f8159913a938/Cargo.lock#L230\r\n\r\nFor packaging purposes we want to fetch the locked sources, but this is unfortunately not possible:\r\n\r\n```bash\r\n$ cargo fetch --locked --target \"$(rustc -vV | sed -n 's/host: //p')\"\r\nwarning: profiles for the non root package will be ignored, specify profiles at the workspace root:\r\npackage:   /home/user/Downloads/cyclonedx-rust-cargo/cargo-cyclonedx/Cargo.toml\r\nworkspace: /home/user/Downloads/cyclonedx-rust-cargo/Cargo.toml\r\n    Updating crates.io index\r\nerror: the lock file /home/user/Downloads/cyclonedx-rust-cargo/Cargo.lock needs to be updated but --locked was passed to prevent this\r\nIf you want to try to generate the lock file without accessing the network, remove the --locked flag and use --offline instead.\r\n```\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 6c3d7fda..0dafb5c5 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -227,7 +227,7 @@ dependencies = [\n \n [[package]]\n name = \"cargo-cyclonedx\"\n-version = \"0.5.1\"\n+version = \"0.5.3\"\n dependencies = [\n  \"anyhow\",\n  \"assert_cmd\",\n@@ -373,7 +373,7 @@ checksum = \"06ea2b9bc92be3c2baa9334a323ebca2d6f074ff852cd1d7b11064035cd3868f\"\n \n [[package]]\n name = \"cyclonedx-bom\"\n-version = \"0.6.0\"\n+version = \"0.6.1\"\n dependencies = [\n  \"base64\",\n  \"cyclonedx-bom-macros\",\ndiff --git a/cargo-cyclonedx/CHANGELOG.md b/cargo-cyclonedx/CHANGELOG.md\nindex 19b78bc4..19768406 100644\n--- a/cargo-cyclonedx/CHANGELOG.md\n+++ b/cargo-cyclonedx/CHANGELOG.md\n@@ -5,6 +5,16 @@ All notable changes to this project will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## 0.5.3 - 2024-06-04\n+\n+### Added\n+\n+ - Add metadata to let `cargo binstall` locate our release binaries ([#727])\n+\n+### Fixed\n+\n+ - Committed an up-to-date lockfile to ease packaging for downstreams\n+\n ## 0.5.2 - 2024-06-04\n \n ### Fixed\n@@ -113,3 +123,4 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#634]: https://github.com/CycloneDX/cyclonedx-rust-cargo/pull/634\n [#722]: https://github.com/CycloneDX/cyclonedx-rust-cargo/pull/722\n [#724]: https://github.com/CycloneDX/cyclonedx-rust-cargo/pull/724\n+[#727]: https://github.com/CycloneDX/cyclonedx-rust-cargo/pull/727\ndiff --git a/cargo-cyclonedx/Cargo.toml b/cargo-cyclonedx/Cargo.toml\nindex a6fda8ab..64948c66 100644\n--- a/cargo-cyclonedx/Cargo.toml\n+++ b/cargo-cyclonedx/Cargo.toml\n@@ -1,6 +1,6 @@\n [package]\n name = \"cargo-cyclonedx\"\n-version = \"0.5.2\"\n+version = \"0.5.3\"\n categories = [\"command-line-utilities\", \"development-tools\", \"development-tools::cargo-plugins\"]\n description = \"CycloneDX Software Bill of Materials (SBOM) for Rust Crates\"\n keywords = [\"sbom\", \"bom\", \"components\", \"dependencies\", \"owasp\"]\n", "instance_id": "CycloneDX__cyclonedx-rust-cargo-729", "clarity": 2, "difficulty": 0.1, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the `Cargo.lock` file was not updated in version 0.5.2 of the project, which causes issues for downstream packaging (e.g., for Arch Linux). The goal is implicitly understood as updating the lock file to reflect the correct dependencies for the new version. The provided error message from the `cargo fetch --locked` command further clarifies the problem by showing the exact error encountered. However, the statement lacks explicit details about the expected solution (e.g., whether to simply update the lock file or if there are specific dependency changes to consider). Additionally, there are no mentions of potential edge cases or constraints, such as compatibility issues with other tools or environments. While the issue is valid and mostly clear, these minor missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, as it primarily involves updating the `Cargo.lock` file to reflect the correct version and dependencies, which is a straightforward task in Rust using the `cargo` tool (e.g., running `cargo update` or `cargo generate-lockfile`). The code changes provided are minimal, affecting only a few lines in `Cargo.lock`, `Cargo.toml`, and `CHANGELOG.md` to bump the version from 0.5.2 to 0.5.3 and update a dependency version. The scope of the change is limited to these files and does not impact the broader codebase or architecture. No complex technical concepts, algorithms, or domain-specific knowledge are required beyond basic familiarity with Rust's package management system. There are no significant edge cases or error handling requirements mentioned or implied in the problem statement or code changes. Overall, this is a very easy task that requires only basic modifications and minimal understanding of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Add ghcr.io Docker container generation to workflow actions\nThere's an open PR for improving the docker image size, which would be great to get merged.\r\n\r\nBut also it's difficult to deploy to any non-local server or kubernetes without a prebuilt image.\r\n\r\n[This page from github support](https://docs.github.com/en/packages/managing-github-packages-using-github-actions-workflows/publishing-and-installing-a-package-with-github-actions#upgrading-a-workflow-that-accesses-a-registry-using-a-personal-access-token) has pretty much a ready-to-go example for adding docker image generation to workflows. Until then I'll be having to copy images manually over ssh to my server, which is not ideal.\n", "patch": "diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml\nindex b3219d8..aee6af6 100644\n--- a/.github/workflows/release.yml\n+++ b/.github/workflows/release.yml\n@@ -71,3 +71,42 @@ jobs:\n         files: 'agate.*.gz'\n         repo-token: ${{ secrets.GITHUB_TOKEN }}\n         release-tag: ${{ github.ref_name }}\n+\n+  build_docker:\n+    runs-on: ubuntu-latest\n+    permissions:\n+      contents: read\n+      packages: write\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+      - name: Log into GHCR\n+        uses: docker/login-action@v3\n+        with:\n+          registry: ghcr.io\n+          username: ${{ github.actor }}\n+          password: ${{ secrets.GITHUB_TOKEN }}\n+      - name: Extract metadata for Docker\n+        id: meta\n+        uses: docker/metadata-action@v5\n+        with:\n+          images: ghcr.io/${{ github.repository }}\n+          # Because this workflow only runs on commits tagged `v*` (i n semver format) this section ensures that\n+          # a docker build tagged `v1.2.3+podman.build` is tagged with `1`, `1.2`, `1.2.3` and `1.2.3+podman.build`\n+          # as well as being tagged with `latest`. For each of these, a subsequent build that has the same tag will\n+          # replace it. This means that pulling `ghcr.io/mbrubeck/agate:1` will always get the most recent image\n+          # released with a v1 tag, container, `ghcr.io/mbrubeck/agate:1.2` will get the latest v1.2 tag, and so on.\n+          tags: |\n+            type=semver,pattern={{version}}\n+            type=semver,pattern={{major}}.{{minor}}.{{patch}}\n+            type=semver,pattern={{major}}.{{minor}}\n+            type=semver,pattern={{major}}\n+      - name: Build and push Docker image\n+        id: push\n+        uses: docker/build-push-action@v6\n+        with:\n+          push: true\n+          tags: ${{ steps.meta.outputs.tags }}\n+          labels: ${{ steps.meta.outputs.labels }}\ndiff --git a/README.md b/README.md\nindex fbd952c..99ed237 100644\n--- a/README.md\n+++ b/README.md\n@@ -38,6 +38,23 @@ Install the package [`agate-bin`](https://aur.archlinux.org/packages/agate-bin/)\n \n If you have the Rust toolchain installed, run `cargo install agate` to install agate from crates.io.\n \n+### Docker\n+\n+Recent builds have also been released as OCI/Docker images on Github's Container Registry (ghcr.io). Most people will need to mount two volumes, one for your content, one for your certificates (this can be empty, they will be automatically generated if needed):\n+\n+```sh\n+$ docker run \\\n+  -p 1965:1965 \\\n+  -v your/path/to/gmi:/gmi \\\n+  -v your/path/to/certs:/certs \\\n+  ghcr.io/mbrubeck/agate:latest \\\n+  --hostname example.org\n+```\n+\n+This container will run without a mounted certificates directory, but new certificates will be lost when it shuts down and re-generated every time it boots, showing your site's visitors a certificate warning each time your server restarts.\n+\n+Each release is tagged with `major`, `major.minor`, `major.minor.patch`, as well as the full version string and \"latest\". This means `docker pull ghcr.io/mbrubeck/agate:3` will always retrieve the latest `v3.*` image, `\u2026:3.3` the latest `v3.3.*` image, and `\u2026:latest` the most recent release of any version.\n+\n ### Source\n \n Download the source code and run `cargo build --release` inside the source repository, then find the binary at `target/release/agate`.\ndiff --git a/tools/docker/README.md b/tools/docker/README.md\ndeleted file mode 100644\nindex 8d64ec7..0000000\n--- a/tools/docker/README.md\n+++ /dev/null\n@@ -1,49 +0,0 @@\n-# Install agate in a docker container\n-(these instructions assume you use linux and have some experience with both docker and the command line)\n-## obtain the source code\n-\n-There are currently no container images online so you have to build the image yourself before you can use it.\n-There are two options available for this: downloading a release or cloning the repository with `git`.\n-I will explain both methods but if you're unsure which method to use, I would recommend the release for new comers because it's probably more tested so you'll encounter less problems.\n-\n-### downloading the release tarball\n-\n-Download the tarball. Go to [https://github.com/mbrubeck/agate/releases/latest](https://github.com/mbrubeck/agate/releases/latest), and copy the url of the source code tarball.\n-\n-```\n-wget URL\n-```\n-\n-Then unpack the tarball and remove it afterwards:\n-```\n-tar -xzf tarball.tar.gz\n-rm tarball.tar.gz\n-```\n-\n-### clone the repository with git\n-\n-I assume you have git already installed. If not, please search on how to do it in the internet.\n-\n-```\n-git clone https://github.com/mbrubeck/agate\n-cd agate\n-```\n-\n-## build the image\n-\n-Now build the docker image:\n-\n-```\n-docker build -t agate .\n-```\n-This process will take a few minutes because all the rust modules have to be compiled from source.\n-\n-## start the docker container\n-\n-```\n-docker run -t -d --name agate -p 1965:1965 -v path/to/gmi:/gmi:ro -v path/to/certs:/certs:rw agate:latest --hostname localhost\n-```\n-\n-You have to replace `/var/www/gmi/` with the folder where you'd like to have gemtext files and `/var/www/gmi/.certificates/` with the folder where you'd like to have your certificates stored. You also have to have to replace `example.org` with your domain name and if plan to speak in a different language than english in your gemini space than you should replace `en-US` with your countries language code (for example de-DE or fr-CA).\n-\n-## That's it! Now have agate running in a docker container!\n", "instance_id": "mbrubeck__agate-366", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in its intent to add Docker container generation to GitHub Actions workflows for easier deployment to non-local servers or Kubernetes. It references a specific GitHub support page with a ready-to-go example, which helps in understanding the desired outcome. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected tags or versioning strategy for the Docker images, nor does it mention specific requirements for compatibility with existing workflows or potential constraints (e.g., size limits, security considerations). Additionally, while the goal is clear, there are no explicit mentions of edge cases or error handling requirements in the problem description itself. Thus, while the overall intent is understandable, some minor details are left to interpretation or inferred from the code changes.", "difficulty_explanation": "The difficulty of this task falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is relatively limited, primarily involving the addition of a new job in the GitHub Actions workflow file (release.yml) to build and push Docker images to GHCR.io, along with minor updates to documentation in README.md and removal of an outdated Docker build instruction file. The changes are mostly isolated to configuration and do not require deep modifications to the core codebase or architecture. Second, the technical concepts involved are straightforward for someone familiar with GitHub Actions and Docker: understanding workflow syntax, Docker image building and tagging, and basic metadata extraction. The provided GitHub Actions (e.g., docker/login-action, docker/build-push-action) abstract much of the complexity, making the implementation more about configuration than coding. Third, while there are implicit edge cases (e.g., handling failed logins to GHCR.io, tag conflicts, or image push failures), they are not explicitly mentioned in the problem statement, and the code changes do not introduce complex error handling logic beyond what the actions provide by default. Overall, this task requires a basic-to-intermediate understanding of CI/CD pipelines and Docker, with minimal impact on the broader system, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Daemon socket should be in $XDG_RUNTIME_DIR\nThe canonical location for user service sockets (and other runtime temporary files) on systemd systems is `$XDG_RUNTIME_DIR` (typically `/run/user/$UID`). When this variable is available, Atuin should locate its daemon socket at `$XDG_RUNTIME_DIR/atuin/atuin.sock` by default.\n", "patch": "diff --git a/crates/atuin-client/src/settings.rs b/crates/atuin-client/src/settings.rs\nindex a34758db317..d7b03c2b003 100644\n--- a/crates/atuin-client/src/settings.rs\n+++ b/crates/atuin-client/src/settings.rs\n@@ -667,7 +667,7 @@ impl Settings {\n         let data_dir = atuin_common::utils::data_dir();\n         let db_path = data_dir.join(\"history.db\");\n         let record_store_path = data_dir.join(\"records.db\");\n-        let socket_path = data_dir.join(\"atuin.sock\");\n+        let socket_path = atuin_common::utils::runtime_dir().join(\"atuin.sock\");\n \n         let key_path = data_dir.join(\"key\");\n         let session_path = data_dir.join(\"session\");\ndiff --git a/crates/atuin-common/src/utils.rs b/crates/atuin-common/src/utils.rs\nindex 3815085ede0..65f5efc48c0 100644\n--- a/crates/atuin-common/src/utils.rs\n+++ b/crates/atuin-common/src/utils.rs\n@@ -77,6 +77,10 @@ pub fn data_dir() -> PathBuf {\n     data_dir.join(\"atuin\")\n }\n \n+pub fn runtime_dir() -> PathBuf {\n+    std::env::var(\"XDG_RUNTIME_DIR\").map_or_else(|_| data_dir(), PathBuf::from)\n+}\n+\n pub fn dotfiles_cache_dir() -> PathBuf {\n     // In most cases, this will be  ~/.local/share/atuin/dotfiles/cache\n     let data_dir = std::env::var(\"XDG_DATA_HOME\")\n", "instance_id": "atuinsh__atuin-2171", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in defining the goal: to change the default location of the daemon socket to `$XDG_RUNTIME_DIR/atuin/atuin.sock` when the environment variable is available. It provides context about the canonical location for user service sockets on systemd systems, which helps in understanding the motivation behind the change. However, there are minor ambiguities and missing details. For instance, it does not explicitly mention what should happen if `$XDG_RUNTIME_DIR` is not set (though the code changes imply a fallback to the existing `data_dir()`). Additionally, there are no examples or explicit mentions of edge cases, such as invalid or inaccessible paths in `$XDG_RUNTIME_DIR`, or potential permission issues. While the intent is clear, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category. The code changes are minimal and localized to two files: adding a new utility function `runtime_dir()` in `utils.rs` to handle the `$XDG_RUNTIME_DIR` environment variable with a fallback, and updating the `socket_path` in `settings.rs` to use this new function. The scope of the change is small, affecting only a single line in the settings configuration and introducing a simple utility function. The technical concepts involved are basic\u2014reading environment variables and handling file paths using Rust's standard library (`std::env` and `PathBuf`), which are straightforward for anyone with basic Rust experience. There are no complex interactions with other parts of the codebase, no architectural impact, and no significant error handling or edge cases explicitly required beyond the fallback logic already implemented in the code. While there could be implicit edge cases (e.g., invalid paths or permissions), the problem does not demand addressing them explicitly, and the provided solution is sufficient for the stated goal. Overall, this is a simple feature addition requiring minimal effort and understanding.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "BUG: qsv cat no longer reads STDIN\ngiven a trivial ids.csv file of\r\n\r\n```\r\nid\r\n1\r\n2\r\n```\r\n\r\n`qsv cat columns ids.csv ids.csv` works as expected, but `cat ids.csv | qsv cat columns - ids.csv` no longer works, giving the error \"Input file '-' does not exist\"\r\n\r\nThis worked in 0.129\n", "patch": "diff --git a/src/util.rs b/src/util.rs\nindex 8bfa283e0..9b1e9f95a 100644\n--- a/src/util.rs\n+++ b/src/util.rs\n@@ -1560,27 +1560,12 @@ pub fn isutf8_file(path: &Path) -> Result<bool, CliError> {\n /// If the input is a file, add the file to the input.\n /// If the input are snappy compressed files, uncompress them before adding them to the input.\n pub fn process_input(\n-    mut arg_input: Vec<PathBuf>,\n+    arg_input: Vec<PathBuf>,\n     tmpdir: &tempfile::TempDir,\n     custom_empty_stdin_errmsg: &str,\n ) -> Result<Vec<PathBuf>, CliError> {\n     let mut processed_input = Vec::with_capacity(arg_input.len());\n \n-    // if the input is empty or \"-\", its stdin. try to copy stdin to a file named\n-    // \"stdin\" in the passed temp directory\n-    if arg_input.len() == 1 && arg_input[0] == PathBuf::from(\"-\") {\n-        // its \"-\", remove the \"-\" from the input so its empty\n-        arg_input.remove(0);\n-    }\n-    if arg_input.is_empty() {\n-        // copy stdin to a file named stdin in a temp directory\n-        let tmp_filename = tmpdir.path().join(\"stdin\");\n-        let mut tmp_file = std::fs::File::create(&tmp_filename)?;\n-        std::io::copy(&mut std::io::stdin(), &mut tmp_file)?;\n-        tmp_file.flush()?;\n-        processed_input.push(tmp_filename);\n-    }\n-\n     let work_input = if arg_input.len() == 1 {\n         let input_path = &arg_input[0];\n         if input_path.is_dir() {\n@@ -1643,10 +1628,25 @@ pub fn process_input(\n         arg_input\n     };\n \n+    let mut stdin_path = PathBuf::new();\n+    let mut stdin_file_created = false;\n+\n     // check the input files\n     for path in work_input {\n-        // does the input file exist?\n-        if !path.exists() {\n+        // check if the path is \"-\" (stdin)\n+        if path == PathBuf::from(\"-\") {\n+            if !stdin_file_created {\n+                // if stdin was not copied to a file, copy stdin to a file named \"stdin\"\n+                let tmp_filename = tmpdir.path().join(\"stdin\");\n+                let mut tmp_file = std::fs::File::create(&tmp_filename)?;\n+                std::io::copy(&mut std::io::stdin(), &mut tmp_file)?;\n+                tmp_file.flush()?;\n+                stdin_file_created = true;\n+                stdin_path = tmp_filename;\n+            }\n+            processed_input.push(stdin_path.clone());\n+            continue;\n+        } else if !path.exists() {\n             return fail_clierror!(\"Input file '{}' does not exist\", path.display());\n         }\n \n", "instance_id": "dathere__qsv-2058", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `qsv cat` command no longer reads from STDIN when using a pipe, resulting in an error message about a non-existent input file '-'. It provides a specific example of the failing command and contrasts it with a working scenario, which helps in understanding the bug. Additionally, it mentions that this functionality worked in a previous version (0.129), giving context about regression. However, the statement lacks details about expected behavior beyond the basic example (e.g., how STDIN should interact with multiple inputs or other command options) and does not specify any edge cases or constraints related to STDIN handling. There are minor ambiguities regarding the broader context of the `qsv cat columns` command's intended functionality with STDIN. Thus, I rate it as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following analysis across the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows modifications confined to a single function (`process_input`) in a single file (`util.rs`). The changes involve reworking the logic for handling STDIN input by moving the creation of a temporary file for STDIN to a conditional block within the input processing loop, rather than handling it as a special case at the start. The amount of code change is moderate, with around 20-30 lines modified, and it does not impact the broader system architecture. It\u2019s a localized fix within the input processing logic.\n\n2. **Number of Technical Concepts**: Solving this requires understanding basic Rust concepts such as file I/O (`std::io::copy`, `std::fs::File`), path handling (`PathBuf`), and temporary file management (`tempfile::TempDir`). Additionally, it involves understanding how command-line tools handle STDIN as a special input source. These are relatively straightforward concepts for a developer familiar with Rust and CLI tool development, not requiring advanced algorithms, design patterns, or domain-specific knowledge beyond basic file handling.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code changes suggest considerations for ensuring STDIN is only copied to a temporary file once (via the `stdin_file_created` flag) even if '-' appears multiple times in the input list. Error handling for file creation and I/O operations is already present in the code (using `Result` and `fail_clierror!`), and the changes do not introduce significant new error conditions beyond what was already handled. The edge cases (e.g., multiple '-' inputs, empty STDIN, or permission issues with temporary files) are not complex to address within the existing framework.\n\n4. **Overall Complexity**: The bug fix requires understanding the intent of the `process_input` function and adjusting the logic to correctly handle STDIN as a special case within the input list. While this involves some code logic comprehension, it is not a deep or systemic change. The problem does not require extensive debugging across multiple modules or advanced performance optimization.\n\nGiven these factors, I assign a difficulty score of 0.30, reflecting an \"Easy\" problem that requires moderate understanding of the code logic and a straightforward modification to restore STDIN functionality. It is slightly above the lower end of the easy range due to the need to carefully handle the temporary file creation for STDIN to avoid duplicate processing, but it remains a contained and manageable task for a developer with basic to intermediate Rust skills.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "add `template` command\nFor each row in a CSV, fill out a specified minijinja template with corresponding columns, and send the output to a file, whose filename can also be dynamically generated using  CSV data.\r\n\r\nExtend the minijinja template with the ability to execute qsv's Luau DSL so we can do additional stuff like complex expressions, lookups, conditional filtering and dynamic content while populating a template.\r\n\r\nUse minijinja crate - https://github.com/mitsuhiko/minijinja#readme\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 2e77be449..08d2b6048 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -3817,7 +3817,7 @@ the response, the HTTP status code, and if its a cache hit.\n request the URL again. https://github.com/jqnatividad/qsv/pull/393\n \n ### Changed\n-* `fetch`: fast defaults. Now tries to go as fast as possible, leveraging dynamic throttling (using RateLimit and Rety-After headers) \n+* `fetch`: fast defaults. Now tries to go as fast as possible, leveraging dynamic throttling (using RateLimit and Retry-After headers) \n but aborting after 100 errors. Also added a separate error progress bar. https://github.com/jqnatividad/qsv/pull/388\n * Smarter `tojsonl`. Now scans CSV file and infers data types and uses the appropriate JSON data type https://github.com/jqnatividad/qsv/pull/389\n * `tojsonl` is also multithreaded https://github.com/jqnatividad/qsv/pull/392\ndiff --git a/Cargo.lock b/Cargo.lock\nindex 972b5d86e..8b4e2f7dc 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -3855,7 +3855,10 @@ version = \"2.4.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"c9ca8daf4b0b4029777f1bc6e1aedd1aec7b74c276a43bc6f620a8e1a1c0a90e\"\n dependencies = [\n+ \"percent-encoding\",\n  \"serde\",\n+ \"serde_json\",\n+ \"v_htmlescape\",\n ]\n \n [[package]]\n@@ -5418,6 +5421,7 @@ dependencies = [\n  \"localzone\",\n  \"log\",\n  \"mimalloc\",\n+ \"minijinja\",\n  \"mlua\",\n  \"newline-converter\",\n  \"num_cpus\",\n@@ -7408,6 +7412,12 @@ dependencies = [\n  \"vsimd\",\n ]\n \n+[[package]]\n+name = \"v_htmlescape\"\n+version = \"0.15.8\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"4e8257fbc510f0a46eb602c10215901938b5c2a7d5e70fc11483b1d3c9b5b18c\"\n+\n [[package]]\n name = \"valuable\"\n version = \"0.1.0\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex e98ce0fc9..1cc0dbd57 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -140,6 +140,12 @@ local-encoding = { version = \"0.2\", optional = true }\n localzone = { version = \"0.3\", features = [\"auto_validation\"] }\n log = \"0.4\"\n mimalloc = { version = \"0.1\", default-features = false, optional = true }\n+minijinja = { version = \"2\", features = [\n+    \"json\",\n+    \"loop_controls\",\n+    \"speedups\",\n+    \"urlencode\",\n+] }\n mlua = { version = \"0.10\", features = [\n     \"luau\",\n     \"luau-jit\",\ndiff --git a/src/cmd/mod.rs b/src/cmd/mod.rs\nindex 810d7dd95..87af5ff8d 100644\n--- a/src/cmd/mod.rs\n+++ b/src/cmd/mod.rs\n@@ -93,6 +93,8 @@ pub mod sqlp;\n pub mod stats;\n #[cfg(any(feature = \"feature_capable\", feature = \"lite\"))]\n pub mod table;\n+#[cfg(feature = \"feature_capable\")]\n+pub mod template;\n #[cfg(all(feature = \"to\", feature = \"feature_capable\"))]\n pub mod to;\n #[cfg(any(feature = \"feature_capable\", feature = \"lite\"))]\ndiff --git a/src/cmd/template.rs b/src/cmd/template.rs\nnew file mode 100644\nindex 000000000..bf2ab879a\n--- /dev/null\n+++ b/src/cmd/template.rs\n@@ -0,0 +1,175 @@\n+static USAGE: &str = r#\"\n+Renders a template using CSV data with the minijinja template engine.\n+https://docs.rs/minijinja/latest/minijinja/\n+\n+Each CSV row is used to populate the template, with column headers used as variable names.\n+The template syntax follows the Jinja2 template language.\n+\n+Example template:\n+    Dear {{ name }},\n+    Your account balance is {{ balance | format_float(precision=2) }}.\n+    Status: {{ if active }}Active{{ else }}Inactive{{ endif }}\n+\n+Usage:\n+    qsv template [options] [--template <str> | --template-file <file>] [<input>] [<outdir> | --output <file>]\n+    qsv template --help\n+\n+template arguments:\n+    <input>                 The CSV file to read. If not given, input is read from STDIN.\n+    <outdir>                The directory where the output files will be written.\n+                            If it does not exist, it will be created.\n+template options:\n+    --template <str>        Template string to use (alternative to --template-file)\n+    --template-file <file>  Template file to use\n+    --outfilename <str>     Template string to use to create the filestem of the output \n+                            files to write to <outdir>. If set to ROWNO, the filestem\n+                            is set to the current rowno of the record, padded with leading\n+                            zeroes, with the \".txt\" extension (e.g. 001.txt, 002.txt, etc.)\n+                            [default: ROWNO]\n+    -n, --no-headers        When set, the first row will not be interpreted\n+                            as headers. Templates must use numeric 1-based indices\n+                            with the \"_c\" prefix.(e.g. col1: {{_c1}} col2: {{_c2}})\n+\n+Common options:\n+    -o, --output <file>     Write output to <file> instead of stdout\n+    --delimiter <sep>       Field separator for reading CSV [default: ,]\n+    -h, --help              Display this message\n+\"#;\n+\n+use std::{\n+    fs,\n+    io::{BufWriter, Write},\n+};\n+\n+use minijinja::Environment;\n+use serde::Deserialize;\n+use serde_json::Value;\n+\n+use crate::{\n+    config::{Config, Delimiter},\n+    util, CliError, CliResult,\n+};\n+\n+#[derive(Deserialize)]\n+struct Args {\n+    arg_input:     Option<String>,\n+    arg_outdir:    Option<String>,\n+    flag_template: Option<String>,\n+\n+    flag_template_file: Option<String>,\n+    flag_output:        Option<String>,\n+    flag_outfilename:   String,\n+    flag_delimiter:     Option<Delimiter>,\n+    flag_no_headers:    bool,\n+}\n+\n+impl From<minijinja::Error> for CliError {\n+    fn from(err: minijinja::Error) -> CliError {\n+        CliError::Other(err.to_string())\n+    }\n+}\n+\n+pub fn run(argv: &[&str]) -> CliResult<()> {\n+    let args: Args = util::get_args(USAGE, argv)?;\n+\n+    // Get template content\n+    let template_content = match (args.flag_template_file, args.flag_template) {\n+        (Some(path), None) => fs::read_to_string(path)?,\n+        (None, Some(template)) => template,\n+        _ => return fail_clierror!(\"Must provide either --template or --template-string\"),\n+    };\n+\n+    // Set up minijinja environment\n+    let mut env = Environment::new();\n+    env.add_template(\"template\", &template_content)?;\n+    let template = env.get_template(\"template\")?;\n+\n+    // Set up CSV reader\n+    let rconfig = Config::new(args.arg_input.as_ref())\n+        .delimiter(args.flag_delimiter)\n+        .no_headers(args.flag_no_headers);\n+\n+    let mut rdr = rconfig.reader()?;\n+    let headers = if args.flag_no_headers {\n+        csv::StringRecord::new()\n+    } else {\n+        rdr.headers()?.clone()\n+    };\n+\n+    // Set up output handling\n+    let output_to_dir = args.arg_outdir.is_some();\n+    let mut row_number = 0_u64;\n+    let mut rowcount = 0;\n+\n+    // Create filename environment once if needed\n+    let filename_env = if output_to_dir && args.flag_outfilename != \"ROWNO\" {\n+        let mut env = Environment::new();\n+        env.add_template(\"filename\", &args.flag_outfilename)?;\n+        Some(env)\n+    } else {\n+        rowcount = util::count_rows(&rconfig)?;\n+        None\n+    };\n+\n+    let width = rowcount.to_string().len();\n+\n+    if output_to_dir {\n+        fs::create_dir_all(args.arg_outdir.as_ref().unwrap())?;\n+    }\n+\n+    let mut wtr = if output_to_dir {\n+        None\n+    } else {\n+        Some(match args.flag_output {\n+            Some(file) => Box::new(BufWriter::new(fs::File::create(file)?)) as Box<dyn Write>,\n+            None => Box::new(BufWriter::new(std::io::stdout())) as Box<dyn Write>,\n+        })\n+    };\n+\n+    let mut curr_record = csv::StringRecord::new();\n+\n+    // Process each record\n+    for record in rdr.records() {\n+        row_number += 1;\n+        curr_record.clone_from(&record?);\n+        let mut context = serde_json::Map::with_capacity(curr_record.len());\n+\n+        if args.flag_no_headers {\n+            // Use numeric indices\n+            for (i, field) in curr_record.iter().enumerate() {\n+                context.insert(format!(\"_c{}\", i + 1), Value::String(field.to_string()));\n+            }\n+        } else {\n+            // Use header names\n+            for (header, field) in headers.iter().zip(curr_record.iter()) {\n+                context.insert(header.to_string(), Value::String(field.to_string()));\n+            }\n+        }\n+\n+        // Render template with record data\n+        let rendered = template.render(&context)?;\n+\n+        if output_to_dir {\n+            let outfilename = if args.flag_outfilename == \"ROWNO\" {\n+                format!(\"{row_number:0width$}.txt\")\n+            } else {\n+                filename_env\n+                    .as_ref()\n+                    .unwrap()\n+                    .get_template(\"filename\")?\n+                    .render(&context)?\n+            };\n+            let outpath = std::path::Path::new(args.arg_outdir.as_ref().unwrap()).join(outfilename);\n+            let mut writer = BufWriter::new(fs::File::create(outpath)?);\n+            write!(writer, \"{rendered}\")?;\n+        } else if let Some(ref mut w) = wtr {\n+            write!(w, \"{rendered}\")?;\n+        }\n+    }\n+\n+    if let Some(mut w) = wtr {\n+        w.flush()?;\n+    }\n+\n+    Ok(())\n+}\ndiff --git a/src/main.rs b/src/main.rs\nindex b6907fa4e..9688f722d 100644\n--- a/src/main.rs\n+++ b/src/main.rs\n@@ -192,6 +192,7 @@ fn main() -> QsvExitCode {\n     enabled_commands.push_str(\n         \"    stats       Infer data types and compute summary statistics\n     table       Align CSV data into columns\n+    template    Render templates using CSV data\n     tojsonl     Convert CSV to newline-delimited JSON\\n\",\n     );\n \n@@ -393,6 +394,7 @@ enum Command {\n     SqlP,\n     Stats,\n     Table,\n+    Template,\n     Transpose,\n     #[cfg(all(feature = \"to\", feature = \"feature_capable\"))]\n     To,\n@@ -489,6 +491,7 @@ impl Command {\n             Command::SqlP => cmd::sqlp::run(argv),\n             Command::Stats => cmd::stats::run(argv),\n             Command::Table => cmd::table::run(argv),\n+            Command::Template => cmd::template::run(argv),\n             Command::Transpose => cmd::transpose::run(argv),\n             #[cfg(all(feature = \"to\", feature = \"feature_capable\"))]\n             Command::To => cmd::to::run(argv),\n", "instance_id": "dathere__qsv-2267", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in defining the goal of adding a `template` command to render templates using CSV data with the minijinja crate. It specifies the high-level functionality of filling a template for each CSV row and dynamically generating output filenames. Additionally, it mentions extending the template with qsv's Luau DSL for advanced features like complex expressions and conditional filtering. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats beyond general mentions of CSV and templates. Constraints such as maximum file sizes, performance expectations, or specific error conditions are not addressed. The mention of Luau DSL integration lacks specifics on how it should interact with minijinja or what \"additional stuff\" entails. While the intent is understandable, these gaps prevent it from being fully comprehensive, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" range (0.6-0.8) due to several factors. First, the scope of code changes is moderate, involving the addition of a new command module (`template.rs`) with 175 lines of code, updates to the main application to register the command, and dependency additions in `Cargo.toml`. While the changes are mostly isolated to a new file, they require integration with the existing CLI framework and CSV processing logic of the `qsv` tool. Second, the technical concepts involved are moderately complex, including familiarity with the minijinja crate for template rendering, handling CSV data with dynamic context mapping, and file I/O operations for dynamic output filenames. The integration of Luau DSL (though not fully implemented in the provided diff) hints at additional complexity in combining scripting capabilities with templating, which would require understanding `mlua` and its interaction with minijinja. Third, the problem demands handling several edge cases, such as missing or malformed CSV data, invalid templates, file system errors when creating directories or writing files, and ensuring proper escaping of data in templates\u2014some of which are addressed in the code, but not exhaustively. Finally, while the changes do not significantly impact the system's architecture, they add a non-trivial feature that interacts with core components like CSV parsing and output handling. A score of 0.65 reflects the need for a deep understanding of Rust, template engines, and error handling, combined with the moderate scope of changes and potential for unaddressed edge cases or future DSL integration challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support `certificate_authorities` extension in ClientHello  \n**Checklist**\r\n* [x] I've searched the issue tracker for similar requests\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nThe TLS 1.3 [`certificate_authorities`](https://datatracker.ietf.org/doc/html/rfc8446#section-4.2.4) extension in ClientHello can be used by the client to communicate to the server what CAs it recognizes, helping the server present a certificate acceptable to the client. Currently, this is not supported in `rustls`.\r\n\r\n**Describe the solution you'd like**\r\nWe can add this extension to the `ClientHello` type (e.g., as the `.certificate_authorites_extension()` method), and add a default function to the `ServerCertVerifier` trait that asks for CA names that the server cert verifier recognizes. With these changes, custom `ServerCertVerifier`s on the client side can communicate their recognized CAs to the server during TLS handshake, and custom cert resolvers on the server side can take this information into account when picking a cert/key.\r\n\r\nI've already started prototyping this, and if the maintainers are OK with the proposed solution, I can open a PR for this feature.\r\n\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex e9628282116..5b4725d709e 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -141,6 +141,45 @@ dependencies = [\n  \"itoa\",\n ]\n \n+[[package]]\n+name = \"asn1-rs\"\n+version = \"0.6.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"5493c3bedbacf7fd7382c6346bbd66687d12bbaad3a89a2d2c303ee6cf20b048\"\n+dependencies = [\n+ \"asn1-rs-derive\",\n+ \"asn1-rs-impl\",\n+ \"displaydoc\",\n+ \"nom\",\n+ \"num-traits\",\n+ \"rusticata-macros\",\n+ \"thiserror 1.0.69\",\n+ \"time\",\n+]\n+\n+[[package]]\n+name = \"asn1-rs-derive\"\n+version = \"0.5.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"965c2d33e53cb6b267e148a4cb0760bc01f4904c1cd4bb4002a085bb016d1490\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn 2.0.90\",\n+ \"synstructure\",\n+]\n+\n+[[package]]\n+name = \"asn1-rs-impl\"\n+version = \"0.2.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"7b18050c2cd6fe86c3a76584ef5e0baf286d038cda203eb6223df2cc413565f7\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn 2.0.90\",\n+]\n+\n [[package]]\n name = \"asn1_derive\"\n version = \"0.20.0\"\n@@ -750,6 +789,20 @@ dependencies = [\n  \"zeroize\",\n ]\n \n+[[package]]\n+name = \"der-parser\"\n+version = \"9.0.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"5cd0a5c643689626bec213c4d8bd4d96acc8ffdb4ad4bb6bc16abf27d5f4b553\"\n+dependencies = [\n+ \"asn1-rs\",\n+ \"displaydoc\",\n+ \"nom\",\n+ \"num-bigint\",\n+ \"num-traits\",\n+ \"rusticata-macros\",\n+]\n+\n [[package]]\n name = \"deranged\"\n version = \"0.3.11\"\n@@ -1753,6 +1806,15 @@ dependencies = [\n  \"memchr\",\n ]\n \n+[[package]]\n+name = \"oid-registry\"\n+version = \"0.7.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"a8d8034d9489cdaf79228eb9f6a3b8d7bb32ba00d6645ebd48eef4077ceb5bd9\"\n+dependencies = [\n+ \"asn1-rs\",\n+]\n+\n [[package]]\n name = \"once_cell\"\n version = \"1.20.2\"\n@@ -2200,6 +2262,15 @@ dependencies = [\n  \"semver\",\n ]\n \n+[[package]]\n+name = \"rusticata-macros\"\n+version = \"4.1.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"faf0c4a6ece9950b9abdb62b1cfcf2a68b3b67a10ba445b3bb85be2a293d0632\"\n+dependencies = [\n+ \"nom\",\n+]\n+\n [[package]]\n name = \"rustix\"\n version = \"0.38.41\"\n@@ -2254,6 +2325,7 @@ dependencies = [\n  \"subtle\",\n  \"time\",\n  \"webpki-roots\",\n+ \"x509-parser\",\n  \"zeroize\",\n  \"zlib-rs\",\n ]\n@@ -2641,10 +2713,12 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"35e7868883861bd0e56d9ac6efcaaca0d6d5d82a2a7ec8209ff492c07cf37b21\"\n dependencies = [\n  \"deranged\",\n+ \"itoa\",\n  \"num-conv\",\n  \"powerfmt\",\n  \"serde\",\n  \"time-core\",\n+ \"time-macros\",\n ]\n \n [[package]]\n@@ -2653,6 +2727,16 @@ version = \"0.1.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"ef927ca75afb808a4d64dd374f00a2adf8d0fcff8e7b184af886c3c87ec4a3f3\"\n \n+[[package]]\n+name = \"time-macros\"\n+version = \"0.2.19\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"2834e6017e3e5e4b9834939793b282bc03b37a3336245fa820e35e233e2a85de\"\n+dependencies = [\n+ \"num-conv\",\n+ \"time-core\",\n+]\n+\n [[package]]\n name = \"tinystr\"\n version = \"0.7.6\"\n@@ -3154,6 +3238,23 @@ dependencies = [\n  \"zeroize\",\n ]\n \n+[[package]]\n+name = \"x509-parser\"\n+version = \"0.16.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"fcbc162f30700d6f3f82a24bf7cc62ffe7caea42c0b2cba8bf7f3ae50cf51f69\"\n+dependencies = [\n+ \"asn1-rs\",\n+ \"data-encoding\",\n+ \"der-parser\",\n+ \"lazy_static\",\n+ \"nom\",\n+ \"oid-registry\",\n+ \"rusticata-macros\",\n+ \"thiserror 1.0.69\",\n+ \"time\",\n+]\n+\n [[package]]\n name = \"yasna\"\n version = \"0.5.2\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 48eadaed51d..ffce81a5bd2 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -87,6 +87,7 @@ tokio = { version = \"1.34\", features = [\"io-util\", \"macros\", \"net\", \"rt\"]}\n webpki = { package = \"rustls-webpki\", version = \"0.102.8\", features = [\"alloc\"], default-features = false }\n webpki-roots = \"0.26\"\n x25519-dalek = \"2\"\n+x509-parser = \"0.16\"\n zeroize = \"1.7\"\n zlib-rs = \"0.4\"\n \ndiff --git a/rustls/Cargo.toml b/rustls/Cargo.toml\nindex 59662a00e4c..7c4976ccf52 100644\n--- a/rustls/Cargo.toml\n+++ b/rustls/Cargo.toml\n@@ -59,6 +59,7 @@ serde = { workspace = true }\n serde_json = { workspace = true }\n time = { workspace = true }\n webpki-roots = { workspace = true }\n+x509-parser = { workspace = true }\n \n [[bench]]\n name = \"benchmarks\"\ndiff --git a/rustls/src/client/hs.rs b/rustls/src/client/hs.rs\nindex 30a97b669d4..732bd08d063 100644\n--- a/rustls/src/client/hs.rs\n+++ b/rustls/src/client/hs.rs\n@@ -271,6 +271,12 @@ fn emit_client_hello_for_retry(\n         ClientExtension::CertificateStatusRequest(CertificateStatusRequest::build_ocsp()),\n     ];\n \n+    if support_tls13 {\n+        if let Some(cas_extension) = config.verifier.root_hint_subjects() {\n+            exts.push(ClientExtension::AuthorityNames(cas_extension.to_owned()));\n+        }\n+    }\n+\n     // Send the ECPointFormat extension only if we are proposing ECDHE\n     if config\n         .provider\ndiff --git a/rustls/src/msgs/handshake.rs b/rustls/src/msgs/handshake.rs\nindex 619d0127878..38e211016ec 100644\n--- a/rustls/src/msgs/handshake.rs\n+++ b/rustls/src/msgs/handshake.rs\n@@ -571,6 +571,7 @@ pub enum ClientExtension {\n     CertificateCompressionAlgorithms(Vec<CertificateCompressionAlgorithm>),\n     EncryptedClientHello(EncryptedClientHello),\n     EncryptedClientHelloOuterExtensions(Vec<ExtensionType>),\n+    AuthorityNames(Vec<DistinguishedName>),\n     Unknown(UnknownExtension),\n }\n \n@@ -600,6 +601,7 @@ impl ClientExtension {\n             Self::EncryptedClientHelloOuterExtensions(_) => {\n                 ExtensionType::EncryptedClientHelloOuterExtensions\n             }\n+            Self::AuthorityNames(_) => ExtensionType::CertificateAuthorities,\n             Self::Unknown(ref r) => r.typ,\n         }\n     }\n@@ -634,6 +636,7 @@ impl Codec<'_> for ClientExtension {\n             Self::CertificateCompressionAlgorithms(ref r) => r.encode(nested.buf),\n             Self::EncryptedClientHello(ref r) => r.encode(nested.buf),\n             Self::EncryptedClientHelloOuterExtensions(ref r) => r.encode(nested.buf),\n+            Self::AuthorityNames(ref r) => r.encode(nested.buf),\n             Self::Unknown(ref r) => r.encode(nested.buf),\n         }\n     }\n@@ -682,6 +685,7 @@ impl Codec<'_> for ClientExtension {\n             ExtensionType::EncryptedClientHelloOuterExtensions => {\n                 Self::EncryptedClientHelloOuterExtensions(Vec::read(&mut sub)?)\n             }\n+            ExtensionType::CertificateAuthorities => Self::AuthorityNames(Vec::read(&mut sub)?),\n             _ => Self::Unknown(UnknownExtension::read(typ, &mut sub)),\n         };\n \n@@ -1159,6 +1163,13 @@ impl ClientHelloPayload {\n             false\n         }\n     }\n+\n+    pub(crate) fn certificate_authorities_extension(&self) -> Option<&[DistinguishedName]> {\n+        match self.find_extension(ExtensionType::CertificateAuthorities)? {\n+            ClientExtension::AuthorityNames(ext) => Some(ext),\n+            _ => unreachable!(\"extension type checked\"),\n+        }\n+    }\n }\n \n #[derive(Clone, Debug)]\ndiff --git a/rustls/src/server/handy.rs b/rustls/src/server/handy.rs\nindex 4c601d9df22..ba2570abfdd 100644\n--- a/rustls/src/server/handy.rs\n+++ b/rustls/src/server/handy.rs\n@@ -312,7 +312,8 @@ mod sni_resolver {\n                     alpn: None,\n                     server_cert_types: None,\n                     client_cert_types: None,\n-                    cipher_suites: &[]\n+                    cipher_suites: &[],\n+                    certificate_authorities: None,\n                 })\n                 .is_none());\n         }\n@@ -330,7 +331,8 @@ mod sni_resolver {\n                     alpn: None,\n                     server_cert_types: None,\n                     client_cert_types: None,\n-                    cipher_suites: &[]\n+                    cipher_suites: &[],\n+                    certificate_authorities: None,\n                 })\n                 .is_none());\n         }\ndiff --git a/rustls/src/server/hs.rs b/rustls/src/server/hs.rs\nindex 2eee2d9da71..2d2a45cc575 100644\n--- a/rustls/src/server/hs.rs\n+++ b/rustls/src/server/hs.rs\n@@ -398,6 +398,11 @@ impl ExpectClientHello {\n         sig_schemes\n             .retain(|scheme| suites::compatible_sigscheme_for_suites(*scheme, &client_suites));\n \n+        // We adhere to the TLS 1.2 RFC by not exposing this to the cert resolver if TLS version is 1.2\n+        let certificate_authorities = match version {\n+            ProtocolVersion::TLSv1_2 => None,\n+            _ => client_hello.certificate_authorities_extension(),\n+        };\n         // Choose a certificate.\n         let certkey = {\n             let client_hello = ClientHello {\n@@ -407,6 +412,7 @@ impl ExpectClientHello {\n                 client_cert_types: client_hello.server_certificate_extension(),\n                 server_cert_types: client_hello.client_certificate_extension(),\n                 cipher_suites: &client_hello.cipher_suites,\n+                certificate_authorities,\n             };\n             trace!(\"Resolving server certificate: {client_hello:#?}\");\n \ndiff --git a/rustls/src/server/server_conn.rs b/rustls/src/server/server_conn.rs\nindex c286eab5350..ad46cdf28f2 100644\n--- a/rustls/src/server/server_conn.rs\n+++ b/rustls/src/server/server_conn.rs\n@@ -32,7 +32,7 @@ use crate::time_provider::TimeProvider;\n use crate::vecbuf::ChunkVecBuffer;\n #[cfg(feature = \"std\")]\n use crate::WantsVerifier;\n-use crate::{compress, sign, verify, versions, KeyLog, WantsVersions};\n+use crate::{compress, sign, verify, versions, DistinguishedName, KeyLog, WantsVersions};\n \n /// A trait for the ability to store server session data.\n ///\n@@ -140,6 +140,10 @@ pub struct ClientHello<'a> {\n     pub(super) server_cert_types: Option<&'a [CertificateType]>,\n     pub(super) client_cert_types: Option<&'a [CertificateType]>,\n     pub(super) cipher_suites: &'a [CipherSuite],\n+    /// The [certificate_authorities] extension, if it was sent by the client.\n+    ///\n+    /// [certificate_authorities]: https://datatracker.ietf.org/doc/html/rfc8446#section-4.2.4\n+    pub(super) certificate_authorities: Option<&'a [DistinguishedName]>,\n }\n \n impl<'a> ClientHello<'a> {\n@@ -202,6 +206,15 @@ impl<'a> ClientHello<'a> {\n     pub fn client_cert_types(&self) -> Option<&'a [CertificateType]> {\n         self.client_cert_types\n     }\n+\n+    /// Get the [certificate_authorities] extension sent by the client.\n+    ///\n+    /// Returns `None` if the client did not send this extension.\n+    ///\n+    /// [certificate_authorities]: https://datatracker.ietf.org/doc/html/rfc8446#section-4.2.4\n+    pub fn certificate_authorities(&self) -> Option<&'a [DistinguishedName]> {\n+        self.certificate_authorities\n+    }\n }\n \n /// Common configuration for a set of server sessions.\n@@ -920,6 +933,7 @@ impl Accepted {\n             server_cert_types: payload.server_certificate_extension(),\n             client_cert_types: payload.client_certificate_extension(),\n             cipher_suites: &payload.cipher_suites,\n+            certificate_authorities: payload.certificate_authorities_extension(),\n         };\n \n         trace!(\"Accepted::client_hello(): {ch:#?}\");\ndiff --git a/rustls/src/verify.rs b/rustls/src/verify.rs\nindex dee10f0b813..ce07e3a0eb0 100644\n--- a/rustls/src/verify.rs\n+++ b/rustls/src/verify.rs\n@@ -142,6 +142,16 @@ pub trait ServerCertVerifier: Debug + Send + Sync {\n     fn requires_raw_public_keys(&self) -> bool {\n         false\n     }\n+\n+    /// Return the [`DistinguishedName`]s of certificate authorities that this verifier trusts.\n+    ///\n+    /// If specified, will be sent as the [`certificate_authorities`] extension in ClientHello.\n+    /// Note that this is only applicable to TLS 1.3.\n+    ///\n+    /// [`certificate_authorities`]: https://datatracker.ietf.org/doc/html/rfc8446#section-4.2.4\n+    fn root_hint_subjects(&self) -> Option<&[DistinguishedName]> {\n+        None\n+    }\n }\n \n /// Something that can verify a client certificate chain\n", "instance_id": "rustls__rustls-2265", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the goal of adding support for the `certificate_authorities` extension in the ClientHello message for TLS 1.3 in the `rustls` library. It references the relevant RFC (RFC 8446) and outlines the desired solution, including adding a method to the `ClientHello` type and a default function to the `ServerCertVerifier` trait. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., handling malformed or overly large lists of certificate authorities) or constraints (e.g., maximum size of the extension data). Additionally, it lacks detailed examples of how the extension data should be formatted or processed, which could lead to implementation uncertainties. While the intent and high-level approach are clear, these missing details prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files and modules within the `rustls` library, including modifications to handshake logic, message encoding/decoding, and server-side certificate resolution. This requires a good understanding of the library's architecture and the interactions between client and server components during the TLS handshake. Second, the problem involves several technical concepts, such as TLS 1.3 protocol specifics, X.509 certificate handling (via the added dependency `x509-parser`), and Rust's type system for safely managing and encoding data structures like `DistinguishedName`. Third, while edge cases are not explicitly mentioned in the problem statement, implementing this feature necessitates considering scenarios like empty or invalid CA lists, compatibility with TLS 1.2 (where this extension should not be exposed), and potential performance impacts of processing large CA lists. Finally, the addition of new dependencies and the need to ensure backward compatibility add to the complexity. A score of 0.65 reflects the need for a deep understanding of TLS internals and careful integration into an existing, security-critical codebase, though it does not reach the \"Very Hard\" level as it does not involve fundamental architectural redesign or highly specialized domain knowledge beyond TLS.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Support for VirtualSpiMasterDevice with Active-High Chip Select\nCurrently we assume that virtual SPI devices expect an active-low chip-select pin. While this is the de-facto standard, it is not universally the case (e.g. the LPM0130m126 TFT display expects an active-high chip-select).\r\n\r\nThe way our types are currently structured, there is no way to generally model this behavior. Specifically, the `SpiMaster` trait has an associated type, `ChipSelect`, which the chip-select gpio embedded inside a `VirtualSpiDevice` must be of that type and the `VirtualSpiDevice` and its Mux toggles this chip high/low when selecting the relevant device to talk to. Unfortunately, unless the hardware implementation uses a `ChipSelect` that _happens_ to be generic (e.g. the nordics and sam4l do this, but none others), there is no way to correctly \"wrap\" the chip select in a type that reverses the polarity.\n", "patch": "diff --git a/boards/apollo3/lora_things_plus/src/main.rs b/boards/apollo3/lora_things_plus/src/main.rs\nindex 5776586a88..96c69c9492 100644\n--- a/boards/apollo3/lora_things_plus/src/main.rs\n+++ b/boards/apollo3/lora_things_plus/src/main.rs\n@@ -511,7 +511,9 @@ unsafe fn setup() -> (\n     let external_spi_controller = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         external_mux_spi,\n-        &peripherals.gpio_port[11], // A5\n+        kernel::hil::spi::cs::IntoChipSelect::<_, kernel::hil::spi::cs::ActiveLow>::into_cs(\n+            &peripherals.gpio_port[11], // A5\n+        ),\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\n@@ -526,7 +528,9 @@ unsafe fn setup() -> (\n     let sx1262_spi_controller = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         sx1262_mux_spi,\n-        &peripherals.gpio_port[36], // H6 - SX1262 Slave Select\n+        kernel::hil::spi::cs::IntoChipSelect::<_, kernel::hil::spi::cs::ActiveLow>::into_cs(\n+            &peripherals.gpio_port[36], // H6 - SX1262 Slave Select\n+        ),\n         LORA_SPI_DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\n@@ -534,7 +538,12 @@ unsafe fn setup() -> (\n     ));\n     peripherals\n         .iom3\n-        .specify_chip_select(&peripherals.gpio_port[36])\n+        .specify_chip_select(kernel::hil::spi::cs::IntoChipSelect::<\n+            _,\n+            kernel::hil::spi::cs::ActiveLow,\n+        >::into_cs(\n+            &peripherals.gpio_port[36], // H6 - SX1262 Slave Select\n+        ))\n         .unwrap();\n \n     let sx1262_gpio = components::gpio::GpioComponent::new(\ndiff --git a/boards/apollo3/redboard_artemis_atp/src/main.rs b/boards/apollo3/redboard_artemis_atp/src/main.rs\nindex 47ecc7da74..59b052987b 100644\n--- a/boards/apollo3/redboard_artemis_atp/src/main.rs\n+++ b/boards/apollo3/redboard_artemis_atp/src/main.rs\n@@ -326,7 +326,9 @@ unsafe fn setup() -> (\n     let spi_controller = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        &peripherals.gpio_port[13], // A13\n+        kernel::hil::spi::cs::IntoChipSelect::<_, kernel::hil::spi::cs::ActiveLow>::into_cs(\n+            &peripherals.gpio_port[13], // A13\n+        ),\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\ndiff --git a/boards/apollo3/redboard_artemis_nano/src/main.rs b/boards/apollo3/redboard_artemis_nano/src/main.rs\nindex 7c97011013..8a1763a63c 100644\n--- a/boards/apollo3/redboard_artemis_nano/src/main.rs\n+++ b/boards/apollo3/redboard_artemis_nano/src/main.rs\n@@ -350,7 +350,9 @@ unsafe fn setup() -> (\n     let spi_controller = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        &peripherals.gpio_port[35], // A14\n+        kernel::hil::spi::cs::IntoChipSelect::<_, kernel::hil::spi::cs::ActiveLow>::into_cs(\n+            &peripherals.gpio_port[35], // A14\n+        ),\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\ndiff --git a/boards/clue_nrf52840/src/main.rs b/boards/clue_nrf52840/src/main.rs\nindex 10283610a9..eddf2abb07 100644\n--- a/boards/clue_nrf52840/src/main.rs\n+++ b/boards/clue_nrf52840/src/main.rs\n@@ -19,6 +19,7 @@ use capsules_core::virtualizers::virtual_aes_ccm::MuxAES128CCM;\n \n use kernel::capabilities;\n use kernel::component::Component;\n+use kernel::hil;\n use kernel::hil::buzzer::Buzzer;\n use kernel::hil::i2c::I2CMaster;\n use kernel::hil::led::LedHigh;\n@@ -652,7 +653,9 @@ unsafe fn start() -> (\n \n     let bus = components::bus::SpiMasterBusComponent::new(\n         spi_mux,\n-        &nrf52840_peripherals.gpio_port[ST7789H2_CS],\n+        hil::spi::cs::IntoChipSelect::<_, hil::spi::cs::ActiveLow>::into_cs(\n+            &nrf52840_peripherals.gpio_port[ST7789H2_CS],\n+        ),\n         20_000_000,\n         kernel::hil::spi::ClockPhase::SampleLeading,\n         kernel::hil::spi::ClockPolarity::IdleLow,\ndiff --git a/boards/components/src/fm25cl.rs b/boards/components/src/fm25cl.rs\nindex 6f0b72eb95..fd5a57f8f4 100644\n--- a/boards/components/src/fm25cl.rs\n+++ b/boards/components/src/fm25cl.rs\n@@ -40,24 +40,32 @@ macro_rules! fm25cl_component_static {\n     };};\n }\n \n-pub struct Fm25clComponent<S: 'static + spi::SpiMaster<'static>> {\n+pub struct Fm25clComponent<\n+    S: 'static + spi::SpiMaster<'static>,\n+    CS: spi::cs::IntoChipSelect<S::ChipSelect, spi::cs::ActiveLow>,\n+> {\n     spi_mux: &'static MuxSpiMaster<'static, S>,\n-    chip_select: S::ChipSelect,\n+    chip_select: CS,\n }\n \n-impl<S: 'static + spi::SpiMaster<'static>> Fm25clComponent<S> {\n-    pub fn new(\n-        spi_mux: &'static MuxSpiMaster<'static, S>,\n-        chip_select: S::ChipSelect,\n-    ) -> Fm25clComponent<S> {\n-        Fm25clComponent {\n+impl<\n+        S: 'static + spi::SpiMaster<'static>,\n+        CS: spi::cs::IntoChipSelect<S::ChipSelect, spi::cs::ActiveLow>,\n+    > Fm25clComponent<S, CS>\n+{\n+    pub fn new(spi_mux: &'static MuxSpiMaster<'static, S>, chip_select: CS) -> Self {\n+        Self {\n             spi_mux,\n             chip_select,\n         }\n     }\n }\n \n-impl<S: 'static + spi::SpiMaster<'static>> Component for Fm25clComponent<S> {\n+impl<\n+        S: 'static + spi::SpiMaster<'static>,\n+        CS: spi::cs::IntoChipSelect<S::ChipSelect, spi::cs::ActiveLow>,\n+    > Component for Fm25clComponent<S, CS>\n+{\n     type StaticInput = (\n         &'static mut MaybeUninit<VirtualSpiMasterDevice<'static, S>>,\n         &'static mut MaybeUninit<FM25CL<'static, VirtualSpiMasterDevice<'static, S>>>,\n@@ -67,9 +75,10 @@ impl<S: 'static + spi::SpiMaster<'static>> Component for Fm25clComponent<S> {\n     type Output = &'static FM25CL<'static, VirtualSpiMasterDevice<'static, S>>;\n \n     fn finalize(self, static_buffer: Self::StaticInput) -> Self::Output {\n-        let spi_device = static_buffer\n-            .0\n-            .write(VirtualSpiMasterDevice::new(self.spi_mux, self.chip_select));\n+        let spi_device = static_buffer.0.write(VirtualSpiMasterDevice::new(\n+            self.spi_mux,\n+            self.chip_select.into_cs(),\n+        ));\n         spi_device.setup();\n \n         let txbuffer = static_buffer.2.write([0; capsules_extra::fm25cl::BUF_LEN]);\ndiff --git a/boards/components/src/l3gd20.rs b/boards/components/src/l3gd20.rs\nindex 49180dedd9..642ab93a78 100644\n--- a/boards/components/src/l3gd20.rs\n+++ b/boards/components/src/l3gd20.rs\n@@ -45,21 +45,28 @@ macro_rules! l3gd20_component_static {\n \n pub type L3gd20ComponentType<S> = capsules_extra::l3gd20::L3gd20Spi<'static, S>;\n \n-pub struct L3gd20Component<S: 'static + spi::SpiMaster<'static>> {\n+pub struct L3gd20Component<\n+    S: 'static + spi::SpiMaster<'static>,\n+    CS: spi::cs::IntoChipSelect<S::ChipSelect, spi::cs::ActiveLow>,\n+> {\n     spi_mux: &'static MuxSpiMaster<'static, S>,\n-    chip_select: S::ChipSelect,\n+    chip_select: CS,\n     board_kernel: &'static kernel::Kernel,\n     driver_num: usize,\n }\n \n-impl<S: 'static + spi::SpiMaster<'static>> L3gd20Component<S> {\n+impl<\n+        S: 'static + spi::SpiMaster<'static>,\n+        CS: spi::cs::IntoChipSelect<S::ChipSelect, spi::cs::ActiveLow>,\n+    > L3gd20Component<S, CS>\n+{\n     pub fn new(\n         spi_mux: &'static MuxSpiMaster<'static, S>,\n-        chip_select: S::ChipSelect,\n+        chip_select: CS,\n         board_kernel: &'static kernel::Kernel,\n         driver_num: usize,\n-    ) -> L3gd20Component<S> {\n-        L3gd20Component {\n+    ) -> Self {\n+        Self {\n             spi_mux,\n             chip_select,\n             board_kernel,\n@@ -68,7 +75,11 @@ impl<S: 'static + spi::SpiMaster<'static>> L3gd20Component<S> {\n     }\n }\n \n-impl<S: 'static + spi::SpiMaster<'static>> Component for L3gd20Component<S> {\n+impl<\n+        S: 'static + spi::SpiMaster<'static>,\n+        CS: spi::cs::IntoChipSelect<S::ChipSelect, spi::cs::ActiveLow>,\n+    > Component for L3gd20Component<S, CS>\n+{\n     type StaticInput = (\n         &'static mut MaybeUninit<VirtualSpiMasterDevice<'static, S>>,\n         &'static mut MaybeUninit<L3gd20Spi<'static, VirtualSpiMasterDevice<'static, S>>>,\n@@ -81,9 +92,10 @@ impl<S: 'static + spi::SpiMaster<'static>> Component for L3gd20Component<S> {\n         let grant_cap = create_capability!(capabilities::MemoryAllocationCapability);\n         let grant = self.board_kernel.create_grant(self.driver_num, &grant_cap);\n \n-        let spi_device = static_buffer\n-            .0\n-            .write(VirtualSpiMasterDevice::new(self.spi_mux, self.chip_select));\n+        let spi_device = static_buffer.0.write(VirtualSpiMasterDevice::new(\n+            self.spi_mux,\n+            self.chip_select.into_cs(),\n+        ));\n         spi_device.setup();\n \n         let txbuffer = static_buffer\ndiff --git a/boards/components/src/lpm013m126.rs b/boards/components/src/lpm013m126.rs\nindex 1fc893209a..f4fe13970b 100644\n--- a/boards/components/src/lpm013m126.rs\n+++ b/boards/components/src/lpm013m126.rs\n@@ -40,57 +40,9 @@ use capsules_core::virtualizers::virtual_spi::{MuxSpiMaster, VirtualSpiMasterDev\n use capsules_extra::lpm013m126::Lpm013m126;\n use core::mem::MaybeUninit;\n use kernel::component::Component;\n-use kernel::hil::gpio;\n use kernel::hil::spi::{SpiMaster, SpiMasterDevice};\n use kernel::hil::time::Alarm;\n-\n-/// CS is active high\n-pub struct Inverted<'a, P: gpio::Pin>(pub &'a P);\n-\n-impl<'a, P: gpio::Pin> gpio::Configure for Inverted<'a, P> {\n-    fn configuration(&self) -> gpio::Configuration {\n-        self.0.configuration()\n-    }\n-    fn make_output(&self) -> gpio::Configuration {\n-        self.0.make_output()\n-    }\n-    fn disable_output(&self) -> gpio::Configuration {\n-        self.0.disable_output()\n-    }\n-    fn make_input(&self) -> gpio::Configuration {\n-        self.0.make_input()\n-    }\n-    fn disable_input(&self) -> gpio::Configuration {\n-        self.0.disable_input()\n-    }\n-    fn deactivate_to_low_power(&self) {\n-        self.0.deactivate_to_low_power()\n-    }\n-    fn set_floating_state(&self, _: gpio::FloatingState) {\n-        unimplemented!() // not sure what it looks like with inversion\n-    }\n-    fn floating_state(&self) -> gpio::FloatingState {\n-        unimplemented!() // not sure what it looks like with inversion\n-    }\n-}\n-\n-impl<'a, P: gpio::Pin> gpio::Output for Inverted<'a, P> {\n-    fn set(&self) {\n-        self.0.clear()\n-    }\n-    fn clear(&self) {\n-        self.0.set()\n-    }\n-    fn toggle(&self) -> bool {\n-        self.0.toggle()\n-    }\n-}\n-\n-impl<'a, P: gpio::Pin> gpio::Input for Inverted<'a, P> {\n-    fn read(&self) -> bool {\n-        !self.0.read()\n-    }\n-}\n+use kernel::hil::{self, gpio};\n \n /// Setup static space for the driver and its requirements.\n #[macro_export]\n@@ -135,17 +87,17 @@ where\n     P: 'static + gpio::Pin,\n     S: 'static + SpiMaster<'static>,\n {\n-    pub fn new(\n+    pub fn new<I: kernel::hil::spi::cs::IntoChipSelect<S::ChipSelect, hil::spi::cs::ActiveHigh>>(\n         spi: &'static MuxSpiMaster<'static, S>,\n \n-        chip_select: S::ChipSelect,\n+        chip_select: I,\n         disp: &'static P,\n         extcomin: &'static P,\n         alarm_mux: &'static MuxAlarm<'static, A>,\n     ) -> Self {\n         Self {\n             spi,\n-            chip_select,\n+            chip_select: chip_select.into_cs(),\n             disp,\n             extcomin,\n             alarm_mux,\ndiff --git a/boards/components/src/mx25r6435f.rs b/boards/components/src/mx25r6435f.rs\nindex fc52d8d848..452a4d8415 100644\n--- a/boards/components/src/mx25r6435f.rs\n+++ b/boards/components/src/mx25r6435f.rs\n@@ -81,17 +81,17 @@ impl<\n         A: 'static + hil::time::Alarm<'static>,\n     > Mx25r6435fComponent<S, P, A>\n {\n-    pub fn new(\n+    pub fn new<CS: kernel::hil::spi::cs::IntoChipSelect<S::ChipSelect, hil::spi::cs::ActiveLow>>(\n         write_protect_pin: Option<&'static P>,\n         hold_pin: Option<&'static P>,\n-        chip_select: S::ChipSelect,\n+        chip_select: CS,\n         mux_alarm: &'static MuxAlarm<'static, A>,\n         mux_spi: &'static MuxSpiMaster<'static, S>,\n     ) -> Mx25r6435fComponent<S, P, A> {\n         Mx25r6435fComponent {\n             write_protect_pin,\n             hold_pin,\n-            chip_select,\n+            chip_select: chip_select.into_cs(),\n             mux_alarm,\n             mux_spi,\n         }\ndiff --git a/boards/components/src/spi.rs b/boards/components/src/spi.rs\nindex b824c05454..51463d8e28 100644\n--- a/boards/components/src/spi.rs\n+++ b/boards/components/src/spi.rs\n@@ -28,6 +28,7 @@\n // Author: Philip Levis <pal@cs.stanford.edu>\n // Last modified: 6/20/2018\n \n+use core::marker::PhantomData;\n use core::mem::MaybeUninit;\n \n use capsules_core::spi_controller::{Spi, DEFAULT_READ_BUF_LENGTH, DEFAULT_WRITE_BUF_LENGTH};\n@@ -125,9 +126,14 @@ pub struct SpiSyscallPComponent<S: 'static + spi::SpiSlave<'static>> {\n     driver_num: usize,\n }\n \n-pub struct SpiComponent<S: 'static + spi::SpiMaster<'static>> {\n+pub struct SpiComponent<\n+    S: 'static + spi::SpiMaster<'static>,\n+    CS: spi::cs::IntoChipSelect<S::ChipSelect, AP>,\n+    AP: spi::cs::ChipSelectActivePolarity,\n+> {\n     spi_mux: &'static MuxSpiMaster<'static, S>,\n-    chip_select: S::ChipSelect,\n+    chip_select: CS,\n+    _phantom: PhantomData<AP>,\n }\n \n impl<S: 'static + spi::SpiMaster<'static>> SpiMuxComponent<S> {\n@@ -246,22 +252,35 @@ impl<S: 'static + spi::SpiSlave<'static>> Component for SpiSyscallPComponent<S>\n     }\n }\n \n-impl<S: 'static + spi::SpiMaster<'static>> SpiComponent<S> {\n-    pub fn new(mux: &'static MuxSpiMaster<'static, S>, chip_select: S::ChipSelect) -> Self {\n+impl<\n+        S: 'static + spi::SpiMaster<'static>,\n+        CS: spi::cs::IntoChipSelect<S::ChipSelect, AP>,\n+        AP: spi::cs::ChipSelectActivePolarity,\n+    > SpiComponent<S, CS, AP>\n+{\n+    pub fn new(mux: &'static MuxSpiMaster<'static, S>, chip_select: CS) -> Self {\n         SpiComponent {\n             spi_mux: mux,\n+            _phantom: PhantomData,\n             chip_select,\n         }\n     }\n }\n \n-impl<S: 'static + spi::SpiMaster<'static>> Component for SpiComponent<S> {\n+impl<\n+        S: 'static + spi::SpiMaster<'static>,\n+        CS: spi::cs::IntoChipSelect<S::ChipSelect, AP>,\n+        AP: spi::cs::ChipSelectActivePolarity,\n+    > Component for SpiComponent<S, CS, AP>\n+{\n     type StaticInput = &'static mut MaybeUninit<VirtualSpiMasterDevice<'static, S>>;\n     type Output = &'static VirtualSpiMasterDevice<'static, S>;\n \n     fn finalize(self, static_buffer: Self::StaticInput) -> Self::Output {\n-        let spi_device =\n-            static_buffer.write(VirtualSpiMasterDevice::new(self.spi_mux, self.chip_select));\n+        let spi_device = static_buffer.write(VirtualSpiMasterDevice::new(\n+            self.spi_mux,\n+            self.chip_select.into_cs(),\n+        ));\n         spi_device.setup();\n         spi_device\n     }\ndiff --git a/boards/hail/src/main.rs b/boards/hail/src/main.rs\nindex 6abe00cdd7..d35a407148 100644\n--- a/boards/hail/src/main.rs\n+++ b/boards/hail/src/main.rs\n@@ -381,7 +381,7 @@ unsafe fn start() -> (\n     let spi_syscalls = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        0,\n+        sam4l::spi::Peripheral::Peripheral0,\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(sam4l::spi::SpiHw));\ndiff --git a/boards/imix/src/main.rs b/boards/imix/src/main.rs\nindex b3aaf14afd..c665ef2dd1 100644\n--- a/boards/imix/src/main.rs\n+++ b/boards/imix/src/main.rs\n@@ -465,15 +465,15 @@ unsafe fn start() -> (\n     let spi_syscalls = SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        2,\n+        sam4l::spi::Peripheral::Peripheral2,\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\n         sam4l::spi::SpiHw<'static>\n     ));\n-    let rf233_spi = SpiComponent::new(mux_spi, 3).finalize(components::spi_component_static!(\n-        sam4l::spi::SpiHw<'static>\n-    ));\n+    let rf233_spi = SpiComponent::new(mux_spi, sam4l::spi::Peripheral::Peripheral3).finalize(\n+        components::spi_component_static!(sam4l::spi::SpiHw<'static>),\n+    );\n     let rf233 = components::rf233::RF233Component::new(\n         rf233_spi,\n         &peripherals.pa[09], // reset\ndiff --git a/boards/nordic/nrf52840dk/src/lib.rs b/boards/nordic/nrf52840dk/src/lib.rs\nindex 1166341986..42870f337e 100644\n--- a/boards/nordic/nrf52840dk/src/lib.rs\n+++ b/boards/nordic/nrf52840dk/src/lib.rs\n@@ -697,7 +697,9 @@ pub unsafe fn start() -> (\n     let spi_controller = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        &gpio_port[SPI_CS],\n+        kernel::hil::spi::cs::IntoChipSelect::<_, kernel::hil::spi::cs::ActiveLow>::into_cs(\n+            &gpio_port[SPI_CS],\n+        ),\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\n@@ -717,7 +719,7 @@ pub unsafe fn start() -> (\n     let mx25r6435f = components::mx25r6435f::Mx25r6435fComponent::new(\n         Some(&gpio_port[SPI_MX25R6435F_WRITE_PROTECT_PIN]),\n         Some(&gpio_port[SPI_MX25R6435F_HOLD_PIN]),\n-        &gpio_port[SPI_MX25R6435F_CHIP_SELECT] as &dyn kernel::hil::gpio::Pin,\n+        &gpio_port[SPI_MX25R6435F_CHIP_SELECT],\n         mux_alarm,\n         mux_spi,\n     )\ndiff --git a/boards/opentitan/src/main.rs b/boards/opentitan/src/main.rs\nindex 505d396323..5a921fe13f 100644\n--- a/boards/opentitan/src/main.rs\n+++ b/boards/opentitan/src/main.rs\n@@ -550,7 +550,7 @@ unsafe fn setup() -> (\n     let spi_controller = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        0,\n+        lowrisc::spi_host::CS(0),\n         capsules_core::spi_controller::DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\ndiff --git a/boards/pico_explorer_base/src/main.rs b/boards/pico_explorer_base/src/main.rs\nindex 3f944ddea9..4944f57c8d 100644\n--- a/boards/pico_explorer_base/src/main.rs\n+++ b/boards/pico_explorer_base/src/main.rs\n@@ -19,12 +19,12 @@ use components::gpio::GpioComponent;\n use components::led::LedsComponent;\n use enum_primitive::cast::FromPrimitive;\n use kernel::component::Component;\n-use kernel::debug;\n use kernel::hil::led::LedHigh;\n use kernel::hil::usb::Client;\n use kernel::platform::{KernelResources, SyscallDriverLookup};\n use kernel::scheduler::round_robin::RoundRobinSched;\n use kernel::{capabilities, create_capability, static_init, Kernel};\n+use kernel::{debug, hil};\n \n use rp2040::adc::{Adc, Channel};\n use rp2040::chip::{Rp2040, Rp2040DefaultPeripherals};\n@@ -465,7 +465,9 @@ pub unsafe fn start() -> (\n \n     let bus = components::bus::SpiMasterBusComponent::new(\n         mux_spi,\n-        peripherals.pins.get_pin(RPGpio::GPIO17),\n+        hil::spi::cs::IntoChipSelect::<_, hil::spi::cs::ActiveLow>::into_cs(\n+            peripherals.pins.get_pin(RPGpio::GPIO17),\n+        ),\n         20_000_000,\n         kernel::hil::spi::ClockPhase::SampleLeading,\n         kernel::hil::spi::ClockPolarity::IdleLow,\ndiff --git a/boards/sma_q3/src/main.rs b/boards/sma_q3/src/main.rs\nindex 65d4f24bc0..85f99d5d66 100644\n--- a/boards/sma_q3/src/main.rs\n+++ b/boards/sma_q3/src/main.rs\n@@ -454,10 +454,7 @@ pub unsafe fn start() -> (\n         );\n \n         let disp_pin = &nrf52840_peripherals.gpio_port[Pin::P0_07];\n-        let cs_pin = static_init!(\n-            components::lpm013m126::Inverted<'static, nrf52840::gpio::GPIOPin>,\n-            components::lpm013m126::Inverted(&nrf52840_peripherals.gpio_port[Pin::P0_05])\n-        );\n+        let cs_pin = &nrf52840_peripherals.gpio_port[Pin::P0_05];\n \n         let display = components::lpm013m126::Lpm013m126Component::new(\n             mux_spi,\ndiff --git a/boards/wm1110dev/src/main.rs b/boards/wm1110dev/src/main.rs\nindex f0a22861d6..dc925163cf 100644\n--- a/boards/wm1110dev/src/main.rs\n+++ b/boards/wm1110dev/src/main.rs\n@@ -17,6 +17,7 @@ use core::ptr::addr_of_mut;\n \n use kernel::capabilities;\n use kernel::component::Component;\n+use kernel::hil;\n use kernel::hil::gpio::Configure;\n use kernel::hil::gpio::Output;\n use kernel::hil::led::LedHigh;\n@@ -372,7 +373,9 @@ pub unsafe fn start() -> (\n     let lr1110_spi = components::spi::SpiSyscallComponent::new(\n         board_kernel,\n         mux_spi,\n-        &nrf52840_peripherals.gpio_port[SPI_CS_PIN],\n+        hil::spi::cs::IntoChipSelect::<_, hil::spi::cs::ActiveLow>::into_cs(\n+            &nrf52840_peripherals.gpio_port[SPI_CS_PIN],\n+        ),\n         LORA_SPI_DRIVER_NUM,\n     )\n     .finalize(components::spi_syscall_component_static!(\n@@ -387,7 +390,11 @@ pub unsafe fn start() -> (\n \n     base_peripherals\n         .spim0\n-        .specify_chip_select(&nrf52840_peripherals.gpio_port[SPI_CS_PIN])\n+        .specify_chip_select(\n+            hil::spi::cs::IntoChipSelect::<_, hil::spi::cs::ActiveLow>::into_cs(\n+                &nrf52840_peripherals.gpio_port[SPI_CS_PIN],\n+            ),\n+        )\n         .unwrap();\n \n     // Pin mappings from the original WM1110 source code.\ndiff --git a/chips/apollo3/src/iom.rs b/chips/apollo3/src/iom.rs\nindex 70953337e3..722cb6cbfa 100644\n--- a/chips/apollo3/src/iom.rs\n+++ b/chips/apollo3/src/iom.rs\n@@ -6,8 +6,9 @@\n \n use core::cell::Cell;\n use kernel::hil;\n-use kernel::hil::gpio::{Configure, Output};\n+use kernel::hil::gpio::Configure;\n use kernel::hil::i2c;\n+use kernel::hil::spi::cs::ChipSelectPolar;\n use kernel::hil::spi::{ClockPhase, ClockPolarity, SpiMaster, SpiMasterClient};\n use kernel::utilities::cells::OptionalCell;\n use kernel::utilities::cells::TakeCell;\n@@ -289,7 +290,7 @@ pub struct Iom<'a> {\n \n     op: Cell<Operation>,\n     spi_phase: Cell<ClockPhase>,\n-    spi_cs: OptionalCell<&'a crate::gpio::GpioPin<'a>>,\n+    spi_cs: OptionalCell<ChipSelectPolar<'a, crate::gpio::GpioPin<'a>>>,\n     smbus: Cell<bool>,\n }\n \n@@ -558,7 +559,7 @@ impl<'a> Iom<'_> {\n                 regs.inten.set(0x00);\n \n                 // Clear CS\n-                self.spi_cs.map(|cs| cs.set());\n+                self.spi_cs.map(|cs| cs.deactivate());\n \n                 self.op.set(Operation::None);\n \n@@ -696,7 +697,7 @@ impl<'a> Iom<'_> {\n                 regs.inten.set(0x00);\n \n                 // Clear CS\n-                self.spi_cs.map(|cs| cs.set());\n+                self.spi_cs.map(|cs| cs.deactivate());\n \n                 self.op.set(Operation::None);\n \n@@ -1175,7 +1176,7 @@ impl<'a> hil::i2c::SMBusMaster<'a> for Iom<'a> {\n }\n \n impl<'a> SpiMaster<'a> for Iom<'a> {\n-    type ChipSelect = &'a crate::gpio::GpioPin<'a>;\n+    type ChipSelect = ChipSelectPolar<'a, crate::gpio::GpioPin<'a>>;\n \n     fn init(&self) -> Result<(), ErrorCode> {\n         self.op.set(Operation::SPI);\n@@ -1238,7 +1239,7 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n         self.registers.intclr.set(0xFFFF_FFFF);\n \n         // Trigger CS\n-        self.spi_cs.map(|cs| cs.clear());\n+        self.spi_cs.map(|cs| cs.activate());\n \n         // Start the transfer\n         self.registers.cmd.write(\n@@ -1354,7 +1355,7 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n         self.registers.intclr.set(0xFFFF_FFFF);\n \n         // Trigger CS\n-        self.spi_cs.map(|cs| cs.clear());\n+        self.spi_cs.map(|cs| cs.activate());\n \n         // Start the transfer\n         self.registers.cmd.write(\n@@ -1368,7 +1369,7 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n \n         self.registers.fifopush.set(val as u32);\n \n-        self.spi_cs.map(|cs| cs.set());\n+        self.spi_cs.map(|cs| cs.deactivate());\n \n         Ok(())\n     }\n@@ -1386,7 +1387,7 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n         self.registers.intclr.set(0xFFFF_FFFF);\n \n         // Trigger CS\n-        self.spi_cs.map(|cs| cs.clear());\n+        self.spi_cs.map(|cs| cs.activate());\n \n         // Start the transfer\n         self.registers.cmd.write(\n@@ -1401,11 +1402,11 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n         if self.registers.fifoptr.read(FIFOPTR::FIFO1SIZ) > 0 {\n             let d = self.registers.fifopop.get().to_ne_bytes();\n \n-            self.spi_cs.map(|cs| cs.set());\n+            self.spi_cs.map(|cs| cs.deactivate());\n             return Ok(d[0]);\n         }\n \n-        self.spi_cs.map(|cs| cs.set());\n+        self.spi_cs.map(|cs| cs.deactivate());\n \n         Err(ErrorCode::FAIL)\n     }\n@@ -1423,7 +1424,7 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n         self.registers.intclr.set(0xFFFF_FFFF);\n \n         // Trigger CS\n-        self.spi_cs.map(|cs| cs.clear());\n+        self.spi_cs.map(|cs| cs.activate());\n \n         // Start the transfer\n         self.registers.cmd.write(\n@@ -1440,18 +1441,18 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n         if self.registers.fifoptr.read(FIFOPTR::FIFO1SIZ) > 0 {\n             let d = self.registers.fifopop.get().to_ne_bytes();\n \n-            self.spi_cs.map(|cs| cs.set());\n+            self.spi_cs.map(|cs| cs.deactivate());\n             return Ok(d[0]);\n         }\n \n-        self.spi_cs.map(|cs| cs.set());\n+        self.spi_cs.map(|cs| cs.deactivate());\n \n         Err(ErrorCode::FAIL)\n     }\n \n     fn specify_chip_select(&self, cs: Self::ChipSelect) -> Result<(), ErrorCode> {\n-        cs.make_output();\n-        cs.set();\n+        cs.pin.make_output();\n+        cs.deactivate();\n         self.spi_cs.set(cs);\n \n         Ok(())\n@@ -1569,10 +1570,10 @@ impl<'a> SpiMaster<'a> for Iom<'a> {\n     }\n \n     fn hold_low(&self) {\n-        self.spi_cs.map(|cs| cs.clear());\n+        self.spi_cs.map(|cs| cs.activate());\n     }\n \n     fn release_low(&self) {\n-        self.spi_cs.map(|cs| cs.set());\n+        self.spi_cs.map(|cs| cs.deactivate());\n     }\n }\ndiff --git a/chips/lowrisc/src/spi_host.rs b/chips/lowrisc/src/spi_host.rs\nindex 7e013eef36..b5366a7939 100644\n--- a/chips/lowrisc/src/spi_host.rs\n+++ b/chips/lowrisc/src/spi_host.rs\n@@ -143,7 +143,6 @@ pub struct SpiHost<'a> {\n     registers: StaticRef<SpiHostRegisters>,\n     client: OptionalCell<&'a dyn hil::spi::SpiMasterClient>,\n     busy: Cell<bool>,\n-    chip_select: Cell<u32>,\n     cpu_clk: u32,\n     tsclk: Cell<u32>,\n     tx_buf: TakeCell<'static, [u8]>,\n@@ -164,7 +163,6 @@ impl<'a> SpiHost<'a> {\n             registers: base,\n             client: OptionalCell::empty(),\n             busy: Cell::new(false),\n-            chip_select: Cell::new(0),\n             cpu_clk,\n             tsclk: Cell::new(0),\n             tx_buf: TakeCell::empty(),\n@@ -552,8 +550,17 @@ impl<'a> SpiHost<'a> {\n     }\n }\n \n+#[derive(Copy, Clone)]\n+pub struct CS(pub u32);\n+\n+impl hil::spi::cs::IntoChipSelect<CS, hil::spi::cs::ActiveLow> for CS {\n+    fn into_cs(self) -> CS {\n+        self\n+    }\n+}\n+\n impl<'a> hil::spi::SpiMaster<'a> for SpiHost<'a> {\n-    type ChipSelect = u32;\n+    type ChipSelect = CS;\n \n     fn init(&self) -> Result<(), ErrorCode> {\n         let regs = self.registers;\n@@ -660,8 +667,7 @@ impl<'a> hil::spi::SpiMaster<'a> for SpiHost<'a> {\n         let regs = self.registers;\n \n         //CSID will index the CONFIGOPTS multi-register\n-        regs.csid.write(csid_ctrl::CSID.val(cs));\n-        self.chip_select.set(cs);\n+        regs.csid.write(csid_ctrl::CSID.val(cs.0));\n \n         Ok(())\n     }\ndiff --git a/chips/nrf52/src/spi.rs b/chips/nrf52/src/spi.rs\nindex a83ea17640..c1874853cc 100644\n--- a/chips/nrf52/src/spi.rs\n+++ b/chips/nrf52/src/spi.rs\n@@ -36,6 +36,8 @@\n use core::cell::Cell;\n use core::{cmp, ptr};\n use kernel::hil;\n+use kernel::hil::gpio::Configure;\n+use kernel::hil::spi::cs::ChipSelectPolar;\n use kernel::utilities::cells::{OptionalCell, TakeCell, VolatileCell};\n use kernel::utilities::registers::interfaces::{ReadWriteable, Readable, Writeable};\n use kernel::utilities::registers::{register_bitfields, ReadWrite, WriteOnly};\n@@ -240,7 +242,7 @@ impl Frequency {\n pub struct SPIM<'a> {\n     registers: StaticRef<SpimRegisters>,\n     client: OptionalCell<&'a dyn hil::spi::SpiMasterClient>,\n-    chip_select: OptionalCell<&'a dyn hil::gpio::Pin>,\n+    chip_select: OptionalCell<ChipSelectPolar<'a, crate::gpio::GPIOPin<'a>>>,\n     busy: Cell<bool>,\n     tx_buf: TakeCell<'static, [u8]>,\n     rx_buf: TakeCell<'static, [u8]>,\n@@ -270,7 +272,7 @@ impl<'a> SPIM<'a> {\n                 return;\n             }\n \n-            self.chip_select.map(|cs| cs.set());\n+            self.chip_select.map(|cs| cs.deactivate());\n             self.registers.events_end.write(EVENT::EVENT::CLEAR);\n \n             // When we are no longer active or busy we can disable the\n@@ -337,7 +339,7 @@ impl<'a> SPIM<'a> {\n }\n \n impl<'a> hil::spi::SpiMaster<'a> for SPIM<'a> {\n-    type ChipSelect = &'a dyn hil::gpio::Pin;\n+    type ChipSelect = ChipSelectPolar<'a, crate::gpio::GPIOPin<'a>>;\n \n     fn set_client(&self, client: &'a dyn hil::spi::SpiMasterClient) {\n         self.client.set(client);\n@@ -365,7 +367,7 @@ impl<'a> hil::spi::SpiMaster<'a> for SPIM<'a> {\n         if self.chip_select.is_none() {\n             return Err((ErrorCode::NODEVICE, tx_buf, rx_buf));\n         }\n-        self.chip_select.map(|cs| cs.clear());\n+        self.chip_select.map(|cs| cs.activate());\n \n         // Setup transmit data registers\n         let tx_len: u32 = cmp::min(len, tx_buf.len()) as u32;\n@@ -418,8 +420,8 @@ impl<'a> hil::spi::SpiMaster<'a> for SPIM<'a> {\n     // The type of the argument is based on what makes sense for the\n     // peripheral when this trait is implemented.\n     fn specify_chip_select(&self, cs: Self::ChipSelect) -> Result<(), ErrorCode> {\n-        cs.make_output();\n-        cs.set();\n+        cs.pin.make_output();\n+        cs.deactivate();\n         self.chip_select.set(cs);\n         Ok(())\n     }\ndiff --git a/chips/rp2040/src/spi.rs b/chips/rp2040/src/spi.rs\nindex 0660b0ebb3..fb1c7596c6 100644\n--- a/chips/rp2040/src/spi.rs\n+++ b/chips/rp2040/src/spi.rs\n@@ -6,7 +6,7 @@ use crate::clocks;\n use core::cell::Cell;\n use core::cmp;\n use kernel::hil;\n-use kernel::hil::gpio::Output;\n+use kernel::hil::spi::cs::ChipSelectPolar;\n use kernel::hil::spi::SpiMaster;\n use kernel::hil::spi::SpiMasterClient;\n use kernel::hil::spi::{ClockPhase, ClockPolarity};\n@@ -238,7 +238,7 @@ pub struct Spi<'a> {\n     registers: StaticRef<SpiRegisters>,\n     clocks: OptionalCell<&'a clocks::Clocks>,\n     master_client: OptionalCell<&'a dyn hil::spi::SpiMasterClient>,\n-    active_slave: OptionalCell<&'a crate::gpio::RPGpioPin<'a>>,\n+    active_slave: OptionalCell<ChipSelectPolar<'a, crate::gpio::RPGpioPin<'a>>>,\n \n     tx_buffer: TakeCell<'static, [u8]>,\n     tx_position: Cell<usize>,\n@@ -346,7 +346,7 @@ impl<'a> Spi<'a> {\n         if self.transfers.get() == SPI_IN_PROGRESS {\n             if !self.active_after.get() {\n                 self.active_slave.map(|p| {\n-                    p.set();\n+                    p.deactivate();\n                 });\n             }\n             self.master_client.map(|client| {\n@@ -383,7 +383,7 @@ impl<'a> Spi<'a> {\n             self.registers.sspimsc.modify(SSPIMSC::TXIM::CLEAR);\n             self.registers.sspimsc.modify(SSPIMSC::RXIM::CLEAR);\n             self.active_slave.map(|p| {\n-                p.clear();\n+                p.activate();\n             });\n \n             self.transfers.set(SPI_IN_PROGRESS);\n@@ -474,10 +474,6 @@ impl<'a> Spi<'a> {\n         }\n     }\n \n-    fn set_active_slave(&self, slave_pin: &'a crate::gpio::RPGpioPin<'a>) {\n-        self.active_slave.set(slave_pin);\n-    }\n-\n     fn set_format(&self) {\n         self.registers.sspcr0.modify(SSPCR0::DSS::DATA_8_BIT);\n         self.registers.sspcr0.modify(SSPCR0::SPO::CLEAR);\n@@ -486,7 +482,7 @@ impl<'a> Spi<'a> {\n }\n \n impl<'a> SpiMaster<'a> for Spi<'a> {\n-    type ChipSelect = &'a crate::gpio::RPGpioPin<'a>;\n+    type ChipSelect = ChipSelectPolar<'a, crate::gpio::RPGpioPin<'a>>;\n \n     fn set_client(&self, client: &'a dyn SpiMasterClient) {\n         self.master_client.set(client);\n@@ -564,7 +560,7 @@ impl<'a> SpiMaster<'a> for Spi<'a> {\n \n     fn specify_chip_select(&self, cs: Self::ChipSelect) -> Result<(), ErrorCode> {\n         if !self.is_busy() {\n-            self.set_active_slave(cs);\n+            self.active_slave.set(cs);\n             Ok(())\n         } else {\n             Err(ErrorCode::BUSY)\ndiff --git a/chips/sam4l/src/spi.rs b/chips/sam4l/src/spi.rs\nindex a0dd44d949..eedc471a4e 100644\n--- a/chips/sam4l/src/spi.rs\n+++ b/chips/sam4l/src/spi.rs\n@@ -179,6 +179,12 @@ pub enum Peripheral {\n     Peripheral3,\n }\n \n+impl spi::cs::IntoChipSelect<Peripheral, spi::cs::ActiveLow> for Peripheral {\n+    fn into_cs(self) -> Peripheral {\n+        self\n+    }\n+}\n+\n #[derive(Copy, Clone, PartialEq)]\n pub enum SpiRole {\n     SpiMaster,\n@@ -375,20 +381,6 @@ impl<'a> SpiHw<'a> {\n         }\n     }\n \n-    pub fn set_active_peripheral(&self, peripheral: Peripheral) {\n-        // Slave cannot set active peripheral\n-        if self.role.get() == SpiRole::SpiMaster {\n-            let spi = &SpiRegisterManager::new(self);\n-            let mr = match peripheral {\n-                Peripheral::Peripheral0 => Mode::PCS::PCS0,\n-                Peripheral::Peripheral1 => Mode::PCS::PCS1,\n-                Peripheral::Peripheral2 => Mode::PCS::PCS2,\n-                Peripheral::Peripheral3 => Mode::PCS::PCS3,\n-            };\n-            spi.registers.mr.modify(mr);\n-        }\n-    }\n-\n     /// Returns the currently active peripheral\n     fn get_active_peripheral(&self, spi: &SpiRegisterManager<'a, '_>) -> Peripheral {\n         if self.role.get() == SpiRole::SpiMaster {\n@@ -520,7 +512,7 @@ impl<'a> SpiHw<'a> {\n }\n \n impl<'a> spi::SpiMaster<'a> for SpiHw<'a> {\n-    type ChipSelect = u8;\n+    type ChipSelect = Peripheral;\n \n     fn set_client(&self, client: &'a dyn SpiMasterClient) {\n         self.client.set(client);\n@@ -636,18 +628,19 @@ impl<'a> spi::SpiMaster<'a> for SpiHw<'a> {\n     }\n \n     fn specify_chip_select(&self, cs: Self::ChipSelect) -> Result<(), ErrorCode> {\n-        match match cs {\n-            0 => Some(Peripheral::Peripheral0),\n-            1 => Some(Peripheral::Peripheral1),\n-            2 => Some(Peripheral::Peripheral2),\n-            3 => Some(Peripheral::Peripheral3),\n-            _ => None,\n-        } {\n-            Some(peripheral_number) => {\n-                self.set_active_peripheral(peripheral_number);\n-                Ok(())\n-            }\n-            None => Err(ErrorCode::INVAL),\n+        // Slave cannot set active peripheral\n+        if self.role.get() == SpiRole::SpiMaster {\n+            let spi = &SpiRegisterManager::new(self);\n+            let mr = match cs {\n+                Peripheral::Peripheral0 => Mode::PCS::PCS0,\n+                Peripheral::Peripheral1 => Mode::PCS::PCS1,\n+                Peripheral::Peripheral2 => Mode::PCS::PCS2,\n+                Peripheral::Peripheral3 => Mode::PCS::PCS3,\n+            };\n+            spi.registers.mr.modify(mr);\n+            Ok(())\n+        } else {\n+            Err(ErrorCode::INVAL)\n         }\n     }\n }\ndiff --git a/chips/sam4l/src/usart.rs b/chips/sam4l/src/usart.rs\nindex 7a33d26542..8081401152 100644\n--- a/chips/sam4l/src/usart.rs\n+++ b/chips/sam4l/src/usart.rs\n@@ -10,8 +10,8 @@ use core::cell::Cell;\n use core::cmp;\n use core::sync::atomic::{AtomicBool, Ordering};\n use kernel::deferred_call::{DeferredCall, DeferredCallClient};\n-use kernel::hil;\n use kernel::hil::spi;\n+use kernel::hil::spi::cs::ChipSelectPolar;\n use kernel::hil::uart;\n use kernel::utilities::cells::OptionalCell;\n use kernel::utilities::registers::interfaces::{ReadWriteable, Readable, Writeable};\n@@ -406,7 +406,7 @@ pub struct USART<'a> {\n \n     client: OptionalCell<UsartClient<'a>>,\n \n-    spi_chip_select: OptionalCell<&'a dyn hil::gpio::Pin>,\n+    spi_chip_select: OptionalCell<ChipSelectPolar<'a, crate::gpio::GPIOPin<'a>>>,\n     pm: &'a pm::PowerManager,\n     dc_state: OptionalCell<DeferredCallState>,\n     deferred_call: DeferredCall,\n@@ -685,7 +685,7 @@ impl<'a> USART<'a> {\n                                 self.rts_disable_spi_deassert_cs(usart);\n                             },\n                             |cs| {\n-                                cs.set();\n+                                cs.deactivate();\n                             },\n                         );\n \n@@ -1051,7 +1051,7 @@ impl<'a> uart::ReceiveAdvanced<'a> for USART<'a> {\n \n /// SPI\n impl<'a> spi::SpiMaster<'a> for USART<'a> {\n-    type ChipSelect = Option<&'static dyn hil::gpio::Pin>;\n+    type ChipSelect = ChipSelectPolar<'a, crate::gpio::GPIOPin<'a>>;\n \n     fn init(&self) -> Result<(), ErrorCode> {\n         let usart = &USARTRegManager::new(self);\n@@ -1111,7 +1111,7 @@ impl<'a> spi::SpiMaster<'a> for USART<'a> {\n                 self.rts_enable_spi_assert_cs(usart);\n             },\n             |cs| {\n-                cs.clear();\n+                cs.activate();\n             },\n         );\n \n@@ -1185,7 +1185,7 @@ impl<'a> spi::SpiMaster<'a> for USART<'a> {\n \n     /// Pass in a None to use the HW chip select pin on the USART (RTS).\n     fn specify_chip_select(&self, cs: Self::ChipSelect) -> Result<(), ErrorCode> {\n-        self.spi_chip_select.insert(cs);\n+        self.spi_chip_select.set(cs);\n         Ok(())\n     }\n \ndiff --git a/chips/stm32f303xc/src/spi.rs b/chips/stm32f303xc/src/spi.rs\nindex e3b95d58b7..32faee5435 100644\n--- a/chips/stm32f303xc/src/spi.rs\n+++ b/chips/stm32f303xc/src/spi.rs\n@@ -7,7 +7,6 @@ use core::cmp;\n use kernel::ErrorCode;\n \n use kernel::hil;\n-use kernel::hil::gpio::Output;\n use kernel::hil::spi::{self, ClockPhase, ClockPolarity, SpiMasterClient};\n use kernel::platform::chip::ClockInterface;\n use kernel::utilities::cells::{OptionalCell, TakeCell};\n@@ -192,7 +191,7 @@ pub struct Spi<'a> {\n     // SPI slave support not yet implemented\n     master_client: OptionalCell<&'a dyn hil::spi::SpiMasterClient>,\n \n-    active_slave: OptionalCell<&'a crate::gpio::Pin<'a>>,\n+    active_slave: OptionalCell<spi::cs::ChipSelectPolar<'a, crate::gpio::Pin<'a>>>,\n \n     tx_buffer: TakeCell<'static, [u8]>,\n     tx_position: Cell<usize>,\n@@ -289,7 +288,7 @@ impl<'a> Spi<'a> {\n             // initiate another SPI transfer right away\n             if !self.active_after.get() {\n                 self.active_slave.map(|p| {\n-                    p.set();\n+                    p.deactivate();\n                 });\n             }\n             self.transfers.set(SPI_IDLE);\n@@ -302,10 +301,6 @@ impl<'a> Spi<'a> {\n         }\n     }\n \n-    fn set_active_slave(&self, slave_pin: &'a crate::gpio::Pin<'a>) {\n-        self.active_slave.set(slave_pin);\n-    }\n-\n     fn set_cr<F>(&self, f: F)\n     where\n         F: FnOnce(),\n@@ -369,7 +364,7 @@ impl<'a> Spi<'a> {\n         if self.transfers.get() == 0 {\n             self.registers.cr2.modify(CR2::RXNEIE::CLEAR);\n             self.active_slave.map(|p| {\n-                p.clear();\n+                p.activate();\n             });\n \n             self.transfers.set(self.transfers.get() | SPI_IN_PROGRESS);\n@@ -416,7 +411,7 @@ impl<'a> Spi<'a> {\n }\n \n impl<'a> spi::SpiMaster<'a> for Spi<'a> {\n-    type ChipSelect = &'a crate::gpio::Pin<'a>;\n+    type ChipSelect = spi::cs::ChipSelectPolar<'a, crate::gpio::Pin<'a>>;\n \n     fn set_client(&self, client: &'a dyn SpiMasterClient) {\n         self.master_client.set(client);\n@@ -539,7 +534,7 @@ impl<'a> spi::SpiMaster<'a> for Spi<'a> {\n     }\n \n     fn specify_chip_select(&self, cs: Self::ChipSelect) -> Result<(), ErrorCode> {\n-        self.set_active_slave(cs);\n+        self.active_slave.set(cs);\n         Ok(())\n     }\n }\ndiff --git a/kernel/src/hil/spi.rs b/kernel/src/hil/spi.rs\nindex 9731ebbc17..e6904c0202 100644\n--- a/kernel/src/hil/spi.rs\n+++ b/kernel/src/hil/spi.rs\n@@ -40,6 +40,146 @@ pub enum ClockPhase {\n     SampleTrailing,\n }\n \n+/// Utility types for modeling chip select pins in a [SpiMaster] implementation.\n+pub mod cs {\n+\n+    /// Represents the Polarity of a chip-select pin (i.e. whether\n+    /// high or low indicates the peripheral is active)\n+    #[derive(Copy, Clone)]\n+    pub enum Polarity {\n+        High,\n+        Low,\n+    }\n+\n+    mod private {\n+        pub trait Sealed {}\n+    }\n+\n+    /// Marker trait indicating whether a peripheral requires active\n+    /// high or low polarity as well as whether a [SpiMaster](super::SpiMaster)\n+    /// implementation can support either or both polarities.\n+    ///\n+    /// This trait is sealed and only implemented for [ActiveLow] and\n+    /// [ActiveHigh].\n+    pub trait ChipSelectActivePolarity: private::Sealed {\n+        const POLARITY: Polarity;\n+    }\n+\n+    /// Marks a peripheral as requiring or controller as supprting\n+    /// active low chip select pins\n+    pub enum ActiveLow {}\n+    /// Marks a peripheral as requiring or controller as supprting\n+    /// active high chip select pins\n+    pub enum ActiveHigh {}\n+\n+    impl private::Sealed for ActiveLow {}\n+    impl private::Sealed for ActiveHigh {}\n+\n+    impl ChipSelectActivePolarity for ActiveLow {\n+        const POLARITY: Polarity = Polarity::Low;\n+    }\n+\n+    impl ChipSelectActivePolarity for ActiveHigh {\n+        const POLARITY: Polarity = Polarity::High;\n+    }\n+\n+    /// A type that can be converted to the appropriate type for\n+    /// [SpiMaster::ChipSelect](super::SpiMaster::ChipSelect) for a\n+    /// particular `POLARITY`.\n+    ///\n+    /// Instantiating a driver for any SPI peripheral should require a type that\n+    /// implements [IntoChipSelect]. That enforces that whatever object is used\n+    /// as the chip select can support the correct polarity for the particular SPI\n+    /// peripheral. This is mostly commonly handled by the component for the\n+    /// peripheral, which requires an object with type [IntoChipSelect] and then\n+    /// converts the object to the [SpiMaster::ChipSelect](super::SpiMaster::ChipSelect)\n+    /// type.\n+    /// # Examples:\n+    ///\n+    /// Some SPI host controllers only support active low or active\n+    /// high chip select pins. Such a controller might provide a unit\n+    /// implementation of this trait _only_ for the [ActiveLow] marker.\n+    ///\n+    /// ```rust\n+    /// use kernel::hil::spi::cs::*;\n+    ///\n+    /// #[derive(Copy, Clone)]\n+    /// enum PeripheralSelect {\n+    ///     Peripheral0,\n+    ///     Peripheral1,\n+    /// }\n+    ///\n+    /// impl IntoChipSelect<PeripheralSelect, ActiveLow> for PeripheralSelect {\n+    ///     fn into_cs(self) -> Self { self }\n+    /// }\n+    /// ```\n+    ///\n+    /// Many other controllers can handle both active low and active\n+    /// high chip select pins, in which case, they should implement\n+    /// both the [ActiveLow] and [ActiveHigh] variants, for example,\n+    /// using the [ChipSelectPolar] wrapper struct (which implements\n+    /// both).\n+    pub trait IntoChipSelect<T, POLARITY> {\n+        fn into_cs(self) -> T;\n+    }\n+\n+    /// A convenience wrapper type around\n+    /// [Output](crate::hil::gpio::Output) GPIO pins that implements\n+    /// [IntoChipSelect] for both [ActiveLow] and [ActiveHigh].\n+    pub struct ChipSelectPolar<'a, P: crate::hil::gpio::Output> {\n+        /// The underlying chip select \"pin\"\n+        pub pin: &'a P,\n+        /// The polarity from which this wrapper was derived using\n+        /// [IntoChipSelect]\n+        pub polarity: Polarity,\n+    }\n+\n+    impl<'a, P: crate::hil::gpio::Output> Clone for ChipSelectPolar<'a, P> {\n+        fn clone(&self) -> Self {\n+            *self\n+        }\n+    }\n+\n+    impl<'a, P: crate::hil::gpio::Output> Copy for ChipSelectPolar<'a, P> {}\n+\n+    impl<'a, P: crate::hil::gpio::Output, A: ChipSelectActivePolarity>\n+        IntoChipSelect<ChipSelectPolar<'a, P>, A> for &'a P\n+    {\n+        fn into_cs(self) -> ChipSelectPolar<'a, P> {\n+            ChipSelectPolar {\n+                pin: self,\n+                polarity: A::POLARITY,\n+            }\n+        }\n+    }\n+\n+    /// When wrapping a GPIO pin that implements\n+    /// [gpio::Output](crate::hil::gpio::Output), users can use the\n+    /// `activate` and `deactivate` methods to automatically set or\n+    /// clear the chip select pin based on the stored polarity.\n+    impl<'a, P: crate::hil::gpio::Output> ChipSelectPolar<'a, P> {\n+        /// Deactive the chip select pin\n+        ///\n+        /// High if active low, low if active high\n+        pub fn deactivate(&self) {\n+            match self.polarity {\n+                Polarity::Low => self.pin.set(),\n+                Polarity::High => self.pin.clear(),\n+            }\n+        }\n+\n+        /// Active the chip select pin\n+        ///\n+        /// Low if active low, high if active high\n+        pub fn activate(&self) {\n+            match self.polarity {\n+                Polarity::Low => self.pin.clear(),\n+                Polarity::High => self.pin.set(),\n+            }\n+        }\n+    }\n+}\n+\n /// Trait for clients of a SPI bus in master mode.\n pub trait SpiMasterClient {\n     /// Callback when a read/write operation finishes: `read_buffer`\n", "instance_id": "tock__tock-4136", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the goal of supporting active-high chip select for virtual SPI devices, which is currently not possible due to the structure of the `SpiMaster` trait and associated types. It identifies a specific issue with the inability to reverse polarity unless the hardware implementation supports a generic `ChipSelect` type. However, it lacks detailed examples of expected input/output behavior or specific constraints for the solution. Additionally, edge cases or potential pitfalls (e.g., compatibility issues with existing hardware implementations) are not explicitly mentioned, which could lead to minor ambiguities during implementation. Overall, the problem is valid and the intent is clear, but it misses some finer details that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, spanning multiple files and modules across different hardware platforms (e.g., Apollo3, NRF52840, SAM4L, RP2040) and components (e.g., SPI drivers, GPIO handling). This requires a deep understanding of the interactions between hardware abstraction layers (HIL) and specific chip implementations. Second, the problem involves advanced technical concepts such as Rust's type system (traits and associated types), hardware-specific SPI protocols, and GPIO polarity handling, which demand a strong grasp of both the language and embedded systems. Third, the introduction of a new `ChipSelectPolar` wrapper and associated traits (`IntoChipSelect`, `ChipSelectActivePolarity`) indicates a non-trivial architectural change to model polarity generically, impacting how SPI devices are initialized and managed across the codebase. While edge cases are not explicitly detailed in the problem statement, the code changes suggest considerations for both active-low and active-high polarities, requiring careful handling to avoid breaking existing implementations. Overall, solving this requires substantial experience with embedded Rust, hardware interfaces, and system-level design, justifying a difficulty score of 0.75.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Suggestion: Rename substrate-client-keystore to ac-keystore\nAll our crates are named with ac-.. , where ac is short for `api-client`, except for the substrate-client-keystore. \r\n\r\nac-keystore was still free at time of writing this issue. Before doing it, one should check if the crate name is still available: https://crates.io/search?q=ac-keystore\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex a22582d53..0ec68c28a 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -61,6 +61,22 @@ dependencies = [\n  \"substrate-api-client\",\n ]\n \n+[[package]]\n+name = \"ac-keystore\"\n+version = \"0.10.0\"\n+dependencies = [\n+ \"array-bytes\",\n+ \"async-trait\",\n+ \"parking_lot\",\n+ \"sc-keystore\",\n+ \"serde_json\",\n+ \"sp-application-crypto\",\n+ \"sp-core\",\n+ \"sp-keyring\",\n+ \"sp-keystore\",\n+ \"tempfile\",\n+]\n+\n [[package]]\n name = \"ac-node-api\"\n version = \"0.6.0\"\n@@ -140,11 +156,11 @@ dependencies = [\n name = \"ac-testing-sync\"\n version = \"0.4.0\"\n dependencies = [\n+ \"ac-keystore\",\n  \"sp-application-crypto\",\n  \"sp-core\",\n  \"sp-runtime\",\n  \"substrate-api-client\",\n- \"substrate-client-keystore\",\n ]\n \n [[package]]\n@@ -7341,22 +7357,6 @@ dependencies = [\n  \"zeroize\",\n ]\n \n-[[package]]\n-name = \"substrate-client-keystore\"\n-version = \"0.10.0\"\n-dependencies = [\n- \"array-bytes\",\n- \"async-trait\",\n- \"parking_lot\",\n- \"sc-keystore\",\n- \"serde_json\",\n- \"sp-application-crypto\",\n- \"sp-core\",\n- \"sp-keyring\",\n- \"sp-keystore\",\n- \"tempfile\",\n-]\n-\n [[package]]\n name = \"substrate-wasm-builder\"\n version = \"17.0.0\"\ndiff --git a/Cargo.toml b/Cargo.toml\nindex e7f72ab0c..fd0316e0b 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -14,7 +14,7 @@ categories = [\"no-std\", \"wasm\"]\n [workspace]\n members = [\n     \".\",\n-    \"client-keystore\",\n+    \"keystore\",\n     \"compose-macros\",\n     \"examples/async\",\n     \"examples/sync\",\ndiff --git a/client-keystore/Cargo.toml b/keystore/Cargo.toml\nsimilarity index 95%\nrename from client-keystore/Cargo.toml\nrename to keystore/Cargo.toml\nindex 35333c26f..a9cec8ac2 100644\n--- a/client-keystore/Cargo.toml\n+++ b/keystore/Cargo.toml\n@@ -1,5 +1,5 @@\n [package]\n-name = \"substrate-client-keystore\"\n+name = \"ac-keystore\"\n version = \"0.10.0\"\n authors = [\"Supercomputing Systems AG <info@scs.ch>\"]\n license = \"Apache-2.0\"\ndiff --git a/client-keystore/src/keystore_ext.rs b/keystore/src/keystore_ext.rs\nsimilarity index 100%\nrename from client-keystore/src/keystore_ext.rs\nrename to keystore/src/keystore_ext.rs\ndiff --git a/client-keystore/src/lib.rs b/keystore/src/lib.rs\nsimilarity index 100%\nrename from client-keystore/src/lib.rs\nrename to keystore/src/lib.rs\n", "instance_id": "scs__substrate-api-client-754", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to rename the crate \"substrate-client-keystore\" to \"ac-keystore\" for consistency with other crate naming conventions (using the \"ac-\" prefix). The goal is straightforward, and the suggestion to check the availability of the crate name on crates.io is a helpful detail. However, there are minor ambiguities or missing details. For instance, the problem statement does not explicitly mention the need to update dependent crates or workspace configurations, which are evident in the code changes but could be overlooked by someone unfamiliar with Rust's Cargo ecosystem. Additionally, there is no discussion of potential downstream impacts or compatibility concerns (e.g., for users of the crate) that might arise from renaming a published crate. While these are not critical omissions, they prevent the statement from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a straightforward renaming of a crate and associated file paths. The scope of code changes is limited to updating a few configuration files (Cargo.toml, Cargo.lock) and renaming directories/files, with no modifications to the actual logic or functionality of the codebase. The task requires minimal understanding of technical concepts\u2014primarily basic familiarity with Rust's Cargo build system and workspace structure. There are no complex interactions between modules, no architectural impacts, and no significant edge cases or error handling to consider beyond ensuring the new crate name is available on crates.io (a simple check). The changes are mechanical and localized, making this a very easy task even for someone with basic Rust experience. The score of 0.15 reflects the simplicity of the task while acknowledging the minor need to verify external crate name availability.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Fails to build on wasm32-wasip2\nTrying to build this crate with `--target wasm32-wasip2` leads to errors:\r\n```\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n  --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/maybe_polyfill/std/mod.rs:30:13\r\n   |\r\n30 |             AsFd, AsRawFd, BorrowedFd, FromRawFd, IntoRawFd, OwnedFd, RawFd,\r\n   |             ^^^^\r\n   |\r\n   = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n   = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n  --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/maybe_polyfill/std/mod.rs:30:28\r\n   |\r\n30 |             AsFd, AsRawFd, BorrowedFd, FromRawFd, IntoRawFd, OwnedFd, RawFd,\r\n   |                            ^^^^^^^^^^\r\n   |\r\n   = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n   = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n  --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/maybe_polyfill/std/mod.rs:30:62\r\n   |\r\n30 |             AsFd, AsRawFd, BorrowedFd, FromRawFd, IntoRawFd, OwnedFd, RawFd,\r\n   |                                                              ^^^^^^^\r\n   |\r\n   = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n   = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n   --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/fs/mod.rs:143:29\r\n    |\r\n143 | pub use std::os::wasi::fs::{DirEntryExt, FileExt, FileTypeExt, MetadataExt, OpenOptionsExt};\r\n    |                             ^^^^^^^^^^^\r\n    |\r\n    = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n    = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n   --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/fs/mod.rs:143:42\r\n    |\r\n143 | pub use std::os::wasi::fs::{DirEntryExt, FileExt, FileTypeExt, MetadataExt, OpenOptionsExt};\r\n    |                                          ^^^^^^^\r\n    |\r\n    = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n    = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n   --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/fs/mod.rs:143:51\r\n    |\r\n143 | pub use std::os::wasi::fs::{DirEntryExt, FileExt, FileTypeExt, MetadataExt, OpenOptionsExt};\r\n    |                                                   ^^^^^^^^^^^\r\n    |\r\n    = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n    = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n   --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/fs/mod.rs:143:64\r\n    |\r\n143 | pub use std::os::wasi::fs::{DirEntryExt, FileExt, FileTypeExt, MetadataExt, OpenOptionsExt};\r\n    |                                                                ^^^^^^^^^^^\r\n    |\r\n    = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n    = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n   --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/fs/mod.rs:143:77\r\n    |\r\n143 | pub use std::os::wasi::fs::{DirEntryExt, FileExt, FileTypeExt, MetadataExt, OpenOptionsExt};\r\n    |                                                                             ^^^^^^^^^^^^^^\r\n    |\r\n    = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n    = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n  --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/path/arg.rs:26:26\r\n   |\r\n26 | use std::os::wasi::ffi::{OsStrExt, OsStringExt};\r\n   |                          ^^^^^^^^\r\n   |\r\n   = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n   = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n\r\nerror[E0658]: use of unstable library feature 'wasip2'\r\n  --> /home/r/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustix-0.38.34/src/path/arg.rs:26:36\r\n   |\r\n26 | use std::os::wasi::ffi::{OsStrExt, OsStringExt};\r\n   |                                    ^^^^^^^^^^^\r\n   |\r\n   = help: add `#![feature(wasip2)]` to the crate attributes to enable\r\n   = note: this compiler was built on 2024-05-19; consider upgrading it if it is out of date\r\n```\n", "patch": "diff --git a/.github/workflows/main.yml b/.github/workflows/main.yml\nindex 92f6f48d2..526cf1e87 100644\n--- a/.github/workflows/main.yml\n+++ b/.github/workflows/main.yml\n@@ -83,6 +83,7 @@ jobs:\n       run: >\n         rustup target add\n         wasm32-wasip1\n+        wasm32-wasip2\n         x86_64-unknown-fuchsia\n     - if: matrix.rust == '1.63'\n       run: rustup target add x86_64-fuchsia\n@@ -102,6 +103,8 @@ jobs:\n     - run: cargo check --workspace --release -vv --target=x86_64-unknown-netbsd --features=all-apis --all-targets\n     - if: matrix.rust != '1.63'\n       run: cargo check --workspace --release -vv --target=wasm32-wasip1 --features=all-apis\n+    - if: matrix.rust != '1.63'\n+      run: cargo check --workspace --release -vv --target=wasm32-wasip2 --features=all-apis\n     - if: matrix.rust != '1.63'\n       run: cargo check --workspace --release -vv --target=x86_64-unknown-fuchsia --features=all-apis --all-targets\n     - if: matrix.rust == '1.63'\n@@ -179,10 +182,8 @@ jobs:\n     - run: >\n         rustup target add\n         x86_64-unknown-redox\n-        wasm32-wasip1\n         thumbv7neon-unknown-linux-gnueabihf\n     - run: cargo check --workspace --release -vv --target=x86_64-unknown-redox --features=all-apis\n-    - run: cargo check --workspace --release -vv --target=wasm32-wasip1 --features=all-apis\n     - run: cargo check --workspace --release -vv --target=thumbv7neon-unknown-linux-gnueabihf --features=all-apis\n \n   check_tier3:\ndiff --git a/Cargo.toml b/Cargo.toml\nindex cac66ead1..7c4235855 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -261,5 +261,6 @@ check-cfg = [\n     'cfg(thumb_mode)',\n     'cfg(wasi)',\n     'cfg(wasi_ext)',\n+    'cfg(wasip2)',\n     'cfg(target_arch, values(\"xtensa\"))',\n ]\ndiff --git a/build.rs b/build.rs\nindex 19b9067a3..037d374e6 100644\n--- a/build.rs\n+++ b/build.rs\n@@ -82,6 +82,7 @@ fn main() {\n     // WASI support can utilize wasi_ext if present.\n     if os == \"wasi\" {\n         use_feature_or_nothing(\"wasi_ext\");\n+        use_feature_or_nothing(\"wasip2\");\n     }\n \n     // If the libc backend is requested, or if we're not on a platform for\ndiff --git a/src/lib.rs b/src/lib.rs\nindex f7892fa17..85da4c858 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -4,7 +4,8 @@\n     all(\n         target_os = \"wasi\",\n         target_env = \"p2\",\n-        any(feature = \"fs\", feature = \"mount\", feature = \"net\")\n+        any(feature = \"fs\", feature = \"mount\", feature = \"net\"),\n+        wasip2,\n     ),\n     feature(wasip2)\n )]\ndiff --git a/src/path/arg.rs b/src/path/arg.rs\nindex 275f0058f..a19a54120 100644\n--- a/src/path/arg.rs\n+++ b/src/path/arg.rs\n@@ -21,7 +21,11 @@ use std::os::hermit::ext::ffi::{OsStrExt, OsStringExt};\n use std::os::unix::ffi::{OsStrExt, OsStringExt};\n #[cfg(all(feature = \"std\", target_os = \"vxworks\"))]\n use std::os::vxworks::ext::ffi::{OsStrExt, OsStringExt};\n-#[cfg(all(feature = \"std\", target_os = \"wasi\"))]\n+#[cfg(all(\n+    feature = \"std\",\n+    target_os = \"wasi\",\n+    any(not(target_env = \"p2\"), wasip2)\n+))]\n use std::os::wasi::ffi::{OsStrExt, OsStringExt};\n #[cfg(feature = \"std\")]\n use std::path::{Component, Components, Iter, Path, PathBuf};\n@@ -225,19 +229,18 @@ impl Arg for String {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for &OsStr {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         OsStr::to_string_lossy(self)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -245,7 +248,6 @@ impl Arg for &OsStr {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -267,19 +269,18 @@ impl Arg for &OsStr {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for &OsString {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         OsString::as_os_str(self).to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_os_str().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -288,7 +289,6 @@ impl Arg for &OsString {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -308,19 +308,18 @@ impl Arg for &OsString {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for OsString {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.as_os_str().to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_os_str().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -328,7 +327,6 @@ impl Arg for OsString {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -350,19 +348,18 @@ impl Arg for OsString {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for &Path {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.as_os_str().to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         Path::to_string_lossy(self)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -370,7 +367,6 @@ impl Arg for &Path {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -392,6 +388,7 @@ impl Arg for &Path {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for &PathBuf {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n@@ -401,13 +398,11 @@ impl Arg for &PathBuf {\n             .ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_path().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -416,7 +411,6 @@ impl Arg for &PathBuf {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -436,19 +430,18 @@ impl Arg for &PathBuf {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for PathBuf {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.as_os_str().to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_os_str().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -456,7 +449,6 @@ impl Arg for PathBuf {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -524,19 +516,16 @@ impl Arg for &CString {\n         unimplemented!()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         unimplemented!()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Borrowed(self))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -562,19 +551,16 @@ impl Arg for CString {\n         self.to_str().map_err(|_utf8_err| io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         CStr::to_string_lossy(self)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Borrowed(self))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -600,13 +586,11 @@ impl<'a> Arg for Cow<'a, str> {\n         Ok(self)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         Cow::Borrowed(self)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -614,7 +598,6 @@ impl<'a> Arg for Cow<'a, str> {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -640,20 +623,18 @@ impl<'a> Arg for Cow<'a, str> {\n }\n \n #[cfg(feature = \"std\")]\n-#[cfg(feature = \"alloc\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl<'a> Arg for Cow<'a, OsStr> {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         (**self).to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         (**self).to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -661,7 +642,6 @@ impl<'a> Arg for Cow<'a, OsStr> {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -693,20 +673,17 @@ impl<'a> Arg for Cow<'a, CStr> {\n         self.to_str().map_err(|_utf8_err| io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         let borrow: &CStr = core::borrow::Borrow::borrow(self);\n         borrow.to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Borrowed(self))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -726,19 +703,18 @@ impl<'a> Arg for Cow<'a, CStr> {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl<'a> Arg for Component<'a> {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.as_os_str().to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_os_str().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -746,7 +722,6 @@ impl<'a> Arg for Component<'a> {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -768,19 +743,18 @@ impl<'a> Arg for Component<'a> {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl<'a> Arg for Components<'a> {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.as_path().to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_path().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -789,7 +763,6 @@ impl<'a> Arg for Components<'a> {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -812,19 +785,18 @@ impl<'a> Arg for Components<'a> {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl<'a> Arg for Iter<'a> {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\n         self.as_path().to_str().ok_or(io::Errno::INVAL)\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn to_string_lossy(&self) -> Cow<'_, str> {\n         self.as_path().to_string_lossy()\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn as_cow_c_str(&self) -> io::Result<Cow<'_, CStr>> {\n         Ok(Cow::Owned(\n@@ -833,7 +805,6 @@ impl<'a> Arg for Iter<'a> {\n         ))\n     }\n \n-    #[cfg(feature = \"alloc\")]\n     #[inline]\n     fn into_c_str<'b>(self) -> io::Result<Cow<'b, CStr>>\n     where\n@@ -939,6 +910,7 @@ impl Arg for &Vec<u8> {\n }\n \n #[cfg(feature = \"alloc\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl Arg for Vec<u8> {\n     #[inline]\n     fn as_str(&self) -> io::Result<&str> {\ndiff --git a/src/path/dec_int.rs b/src/path/dec_int.rs\nindex afda549f1..a05e30348 100644\n--- a/src/path/dec_int.rs\n+++ b/src/path/dec_int.rs\n@@ -13,7 +13,11 @@ use core::mem::{self, MaybeUninit};\n use core::num::NonZeroU8;\n #[cfg(all(feature = \"std\", unix))]\n use std::os::unix::ffi::OsStrExt;\n-#[cfg(all(feature = \"std\", target_os = \"wasi\"))]\n+#[cfg(all(\n+    feature = \"std\",\n+    target_os = \"wasi\",\n+    any(not(target_env = \"p2\"), wasip2)\n+))]\n use std::os::wasi::ffi::OsStrExt;\n #[cfg(feature = \"std\")]\n use {core::fmt, std::ffi::OsStr, std::path::Path};\n@@ -236,6 +240,7 @@ impl DecInt {\n }\n \n #[cfg(feature = \"std\")]\n+#[cfg(any(not(target_os = \"wasi\"), not(target_env = \"p2\"), wasip2))]\n impl AsRef<Path> for DecInt {\n     #[inline]\n     fn as_ref(&self) -> &Path {\n", "instance_id": "bytecodealliance__rustix-1301", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the crate fails to build on the `wasm32-wasip2` target due to the use of an unstable library feature (`wasip2`) in the `rustix` dependency. The error messages provided in the problem statement explicitly point to the need for enabling the `wasip2` feature flag or updating the compiler. However, the statement lacks critical details about the expected solution or constraints, such as whether the goal is to enable the feature flag, update dependencies, or modify the code to avoid using unstable features. Additionally, there are no examples or specific guidance on how to handle compatibility with other targets or potential side effects of the changes. Edge cases or compatibility requirements are not mentioned, leaving minor ambiguities in the intended resolution path.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, involving updates to configuration files (e.g., `Cargo.toml`, GitHub workflows) and conditional compilation logic in multiple source files to handle the `wasip2` target. The changes require understanding Rust's target-specific compilation, feature flags, and conditional compilation attributes (`cfg`), which are moderately complex concepts. Additionally, the problem touches on cross-platform compatibility, specifically for WebAssembly and WASI environments, which adds a layer of domain-specific knowledge. While the changes do not significantly impact the system's architecture, they do require careful consideration of how feature flags interact with different targets and environments. Edge cases, such as ensuring compatibility with older Rust versions or other WASI targets, are not explicitly mentioned but are implied and need to be handled in the code changes (e.g., conditional checks for `target_env = \"p2\"`). The number of technical concepts involved (Rust feature flags, WASI targets, build scripts) and the need to modify multiple files contribute to a medium difficulty score of 0.45. It is not overly complex for an experienced Rust developer but goes beyond a simple bug fix or feature addition due to the cross-target considerations and the breadth of changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "lsp: Renaming variables doesn't updated shorthand property names correctly\nVersion: Deno 2.1.9\n\n```ts\nfunction foo({myParam}: {myParam: number}) {\n    console.log(myParam);\n}\n\nconst myParam = 3; // select myParam and hit F2 to rename\nfoo({myParam})\n```\n\nNormally when you rename `myParam` using the `F2` key, VSCode turns the last line into:\n```ts\nfoo({myParam: newParam})\n```\n\nBut when you perform this same trick with Deno, you get:\n\n```ts\nfoo({newParam})\n```\n", "patch": "diff --git a/cli/lsp/language_server.rs b/cli/lsp/language_server.rs\nindex 1036f468a7d524..3c6c20cc50d240 100644\n--- a/cli/lsp/language_server.rs\n+++ b/cli/lsp/language_server.rs\n@@ -3017,12 +3017,15 @@ impl Inner {\n     let asset_or_doc = self.get_asset_or_document(&specifier)?;\n     let line_index = asset_or_doc.line_index();\n \n+    let user_preferences =\n+      tsc::UserPreferences::from_config_for_specifier(&self.config, &specifier);\n     let maybe_locations = self\n       .ts_server\n       .find_rename_locations(\n         self.snapshot(),\n         specifier,\n         line_index.offset_tsc(params.text_document_position.position)?,\n+        user_preferences,\n         token,\n       )\n       .await\ndiff --git a/cli/lsp/tsc.rs b/cli/lsp/tsc.rs\nindex 15d4cd458ee7fb..a8bbf08649d56b 100644\n--- a/cli/lsp/tsc.rs\n+++ b/cli/lsp/tsc.rs\n@@ -1174,6 +1174,7 @@ impl TsServer {\n     snapshot: Arc<StateSnapshot>,\n     specifier: ModuleSpecifier,\n     position: u32,\n+    user_preferences: UserPreferences,\n     token: &CancellationToken,\n   ) -> Result<Option<Vec<RenameLocation>>, AnyError> {\n     let req = TscRequest::FindRenameLocations((\n@@ -1181,7 +1182,7 @@ impl TsServer {\n       position,\n       false,\n       false,\n-      false,\n+      user_preferences,\n     ));\n     let mut results = FuturesOrdered::new();\n     for scope in snapshot\n@@ -1205,7 +1206,10 @@ impl TsServer {\n         if token.is_cancelled() {\n           return Err(anyhow!(\"request cancelled\"));\n         } else {\n-          lsp_warn!(\"Unable to get rename locations from TypeScript: {err}\");\n+          let err = err.to_string();\n+          if !err.contains(\"Could not find source file\") {\n+            lsp_warn!(\"Unable to get rename locations from TypeScript: {err}\");\n+          }\n         }\n       }\n       let locations = locations.unwrap_or_default();\n@@ -2420,9 +2424,8 @@ impl ImplementationLocation {\n pub struct RenameLocation {\n   #[serde(flatten)]\n   document_span: DocumentSpan,\n-  // RenameLocation props\n-  // prefix_text: Option<String>,\n-  // suffix_text: Option<String>,\n+  prefix_text: Option<String>,\n+  suffix_text: Option<String>,\n }\n \n impl RenameLocation {\n@@ -2481,12 +2484,21 @@ impl RenameLocations {\n \n       // push TextEdit for ensured `TextDocumentEdit.edits`.\n       let document_edit = text_document_edit_map.get_mut(&uri).unwrap();\n+      let new_text = [\n+        location.prefix_text.as_deref(),\n+        Some(new_name),\n+        location.suffix_text.as_deref(),\n+      ]\n+      .into_iter()\n+      .flatten()\n+      .collect::<Vec<_>>()\n+      .join(\"\");\n       document_edit.edits.push(lsp::OneOf::Left(lsp::TextEdit {\n         range: location\n           .document_span\n           .text_span\n           .to_range(asset_or_doc.line_index()),\n-        new_text: new_name.to_string(),\n+        new_text,\n       }));\n     }\n \n@@ -5548,8 +5560,8 @@ pub enum TscRequest {\n   ProvideCallHierarchyOutgoingCalls((String, u32)),\n   // https://github.com/denoland/deno/blob/v1.37.1/cli/tsc/dts/typescript.d.ts#L6236\n   PrepareCallHierarchy((String, u32)),\n-  // https://github.com/denoland/deno/blob/v1.37.1/cli/tsc/dts/typescript.d.ts#L6221\n-  FindRenameLocations((String, u32, bool, bool, bool)),\n+  // https://github.com/denoland/deno/blob/v2.2.2/cli/tsc/dts/typescript.d.ts#L6674\n+  FindRenameLocations((String, u32, bool, bool, UserPreferences)),\n   // https://github.com/denoland/deno/blob/v1.37.1/cli/tsc/dts/typescript.d.ts#L6224\n   GetSmartSelectionRange((String, u32)),\n   // https://github.com/denoland/deno/blob/v1.37.1/cli/tsc/dts/typescript.d.ts#L6183\n", "instance_id": "denoland__deno-28327", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue with renaming variables in shorthand property names within the Deno LSP (Language Server Protocol) implementation. It provides a specific example in TypeScript code, showing the expected behavior in VSCode versus the incorrect behavior in Deno. The goal is evident: to fix the renaming functionality so that it correctly handles shorthand property names by adding the necessary prefix/suffix in the renamed output. However, the statement lacks explicit mention of edge cases or additional constraints (e.g., different syntax forms, nested objects, or performance implications). Additionally, there are no detailed requirements on how the fix should integrate with the existing codebase or whether there are specific TypeScript language server behaviors to consider. Thus, while the core issue is well-defined with a reproducible example, minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category (0.6-0.8) due to several factors. First, the scope of code changes involves multiple files (`language_server.rs` and `tsc.rs`) within the Deno LSP implementation, requiring an understanding of how the language server interacts with the TypeScript compiler (tsc) and user preferences. The changes are not trivial; they involve modifying how rename locations are fetched and processed, including handling prefix and suffix text for renamed variables, which impacts the final text edit output. Second, the technical concepts required include familiarity with Rust (used in Deno's CLI), asynchronous programming (e.g., `FuturesOrdered`, `CancellationToken`), LSP protocols, and TypeScript's internal rename logic. Additionally, integrating user preferences into the rename operation adds a layer of complexity, as it requires understanding configuration handling in Deno. Third, while the problem statement does not explicitly mention edge cases, the nature of renaming in a language server suggests potential complexities, such as handling different syntaxes, invalid or partial code, or conflicts in variable scopes, which the developer must anticipate. Finally, the changes impact a critical feature of the LSP (renaming), which is central to developer experience, necessitating careful testing and validation. Given the need for a deep understanding of Deno's architecture, LSP interactions, and moderate code modifications across modules, a difficulty score of 0.65 is appropriate, reflecting a challenging but not extremely complex task.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Where to put panic implementations?\nThere's quite a few panic implementations now, so I think it'd be useful to list them here. I'm not sure where exactly they should be put though, because some of them are platform-independent while others aren't. Personally I think these belong in their own section.\r\n\r\nList of crates:\r\n\r\n* [`panic-halt`](https://github.com/korken89/panic-halt): Halts the processor on panics\r\n* [`panic-abort`](https://github.com/japaric/panic-abort): Causes an abort on panics (nightly-only)\r\n* [`panic-ramdump`](https://github.com/therealprof/panic-ramdump): Writes the panic message into the beginning of RAM and enters an infinite loop; the message can be extracted by attaching a debugger (Cortex-M only)\r\n* [`panic-semihosting`](https://github.com/rust-embedded/panic-semihosting): Prints the panic message via semihosting to an attached debugger (Cortex-M only)\r\n* [`panic-itm`](https://github.com/rust-embedded/panic-itm): Prints the panic through the chip's ITM to a debugger attached to the SWO pin (Cortex-M with ITM only)\n", "patch": "diff --git a/README.md b/README.md\nindex 3aef4ba..25b6fdc 100644\n--- a/README.md\n+++ b/README.md\n@@ -85,6 +85,7 @@ This project is developed and maintained by the [Resources team][team].\n     - [WIP](#wip)\n   - [no-std crates](#no-std-crates)\n     - [WIP](#wip-1)\n+  - [Panic handling](#panic-handling)\n   - [Firmware projects](#firmware-projects)\n   - [Old books, blogs, and training materials](#old-books-blogs-and-training-materials)\n   - [License](#license)\n@@ -1277,7 +1278,6 @@ Work in progress drivers. Help the authors make these crates awesome!\n 1. [nom](https://crates.io/crates/nom): parser combinator framework - [![crates.io](https://img.shields.io/crates/v/nom.svg)](https://crates.io/crates/nom)\n 1. [null-terminated](https://crates.io/crates/null-terminated): generic null-terminated arrays - [![crates.io](https://img.shields.io/crates/v/null-terminated.svg)](https://crates.io/crates/null-terminated)\n 1. [num-format](https://crates.io/crates/num-format): Crate for producing string representations of numbers, formatted according to international standards, e.g. \"1,000,000\" for US English - [![crates.io](https://img.shields.io/crates/v/num-format.svg)](https://crates.io/crates/num-format)\n-1. [`panic-persist`]: A panic handler crate inspired by `panic-ramdump` that logs panic messages to a region of RAM defined by the user, allowing for discovery of panic messages post-mortem using normal program control flow. - [![crates.io](https://img.shields.io/crates/v/panic-persist.svg)](https://crates.io/crates/panic-persist)\n 1. [pc-keyboard]: A PS/2 keyboard protocol driver. Transport (bit-banging or SPI) agnostic, but can convert Set 2 Scancodes into Unicode. [![crates.io](https://img.shields.io/crates/v/pc-keyboard.svg)](https://crates.io/crates/pc-keyboard)\n 1. [qei](https://crates.io/crates/qei) : A qei wrapper that allows you to extend your qei timers from a 16-bit integer to a 64-bit integer. - [![crates.io](https://img.shields.io/crates/v/qei.svg)](https://crates.io/crates/qei)\n 1. [qemu-exit]: Quit a running QEMU session with user-defined exit code. Useful for unit or integration tests using QEMU. - [![crates.io](https://img.shields.io/crates/v/qemu-exit.svg)](https://crates.io/crates/qemu-exit)\n@@ -1299,8 +1299,8 @@ Work in progress drivers. Help the authors make these crates awesome!\n 1. [adafruit-bluefruit-protocol]: A `no_std` parser for the [Adafruit Bluefruit LE Connect controller protocol]. - [![crates.io](https://img.shields.io/crates/v/adafruit-bluefruit-protocol)](https://crates.io/crates/adafruit-bluefruit-protocol)\n 1. [wtx](https://github.com/c410-f3r/wtx): Among other things, provides implementations to interact with HTTP, WebSockets and Databases.\n \n+[no-std-category]: https://crates.io/categories/no-std\n [`cmim`]: https://crates.io/crates/cmim\n-[`panic-persist`]: https://crates.io/crates/panic-persist\n [bitmatch]: https://crates.io/crates/bitmatch\n [biquad]: https://crates.io/crates/biquad\n [embassy]: https://github.com/akiles/embassy\n@@ -1326,7 +1326,36 @@ Work in progress crates. Help the authors make these crates awesome!\n - [OxCC](https://github.com/jonlamb-gh/oxcc): A port of Open Source Car Control written in Rust\n - [Rubble](https://github.com/jonas-schievink/rubble): A pure-Rust embedded BLE stack [![crates.io](https://img.shields.io/crates/v/rubble.svg)](https://crates.io/crates/rubble)\n \n-[no-std-category]: https://crates.io/categories/no-std\n+## Panic handling\n+\n+There are many ways to handle panics in embedded devices, these crates provide helpful variants.\n+\n+1. [`dont_panic`] + [`dont_panic_slice`]: panic!()-like macro that causes linking error instead of panicking. May be used to statically ensure some code won't panic. [![crates.io](https://img.shields.io/crates/v/dont_panic.svg)](https://crates.io/crates/dont_panic) + [![crates.io](https://img.shields.io/crates/v/dont_panic_slice.svg)](https://crates.io/crates/dont_panic_slice)\n+1. [`no-panic`]: Attribute macro to require that the compiler prove a function can't ever panic. [![crates.io](https://img.shields.io/crates/v/no-panic.svg)](https://crates.io/crates/no-panic)\n+1. [`panic-abort`]: Causes an abort on panics (nightly-only) [![crates.io](https://img.shields.io/crates/v/panic-abort.svg)](https://crates.io/crates/panic-abort)\n+1. [`panic-halt`]: Halts the processor on panics [![crates.io](https://img.shields.io/crates/v/panic-halt.svg)](https://crates.io/crates/panic-halt)\n+1. [`panic-itm`]: Prints the panic through the chip's ITM to a debugger attached to the SWO pin (Cortex-M with ITM only)\n+1. [`panic-never`]: This crate guarantees that your application is free of panicking branches. [![crates.io](https://img.shields.io/crates/v/panic-never.svg)](https://crates.io/crates/panic-never)\n+1. [`panic-persist`]: A panic handler crate inspired by `panic-ramdump` that logs panic messages to a region of RAM defined by the user, allowing for discovery of panic messages post-mortem using normal program control flow. - [![crates.io](https://img.shields.io/crates/v/panic-persist.svg)](https://crates.io/crates/panic-persist)\n+1. [`panic-probe`]: A panic handler that exits `probe-run` with an error code. [![crates.io](https://img.shields.io/crates/v/panic-probe.svg)](https://crates.io/crates/panic-probe)\n+1. [`panic-ramdump`]: Writes the panic message into the beginning of RAM and enters an infinite loop; the message can be extracted by attaching a debugger (Cortex-M only) [![crates.io](https://img.shields.io/crates/v/panic-ramdump.svg)](https://crates.io/crates/panic-ramdump)\n+1. [`panic-rtt`]: Set the panicking behavior to log to a JLINK debugger and loop. [![crates.io](https://img.shields.io/crates/v/panic_rtt.svg)](https://crates.io/crates/panic_rtt)\n+1. [`panic-rtt-target`]: Logs panic messages over RTT. Uses `rtt-target`. [![crates.io](https://img.shields.io/crates/v/panic-rtt-target.svg)](https://crates.io/crates/panic-rtt-target)\n+1. [`panic-semihosting`]: Prints the panic message via semihosting to an attached debugger (Cortex-M only)\n+\n+[`dont_panic_slice`]: https://crates.io/crates/dont_panic_slice\n+[`dont_panic`]: https://crates.io/crates/dont_panic\n+[`no-panic`]: https://crates.io/crates/no-panic\n+[`panic-abort`]: https://crates.io/crates/panic-abort\n+[`panic-halt`]: https://crates.io/crates/panic-halt\n+[`panic-itm`]: https://github.com/rust-embedded/cortex-m/tree/master/panic-itm\n+[`panic-never`]: https://crates.io/crates/panic-never\n+[`panic-persist`]: https://crates.io/crates/panic-persist\n+[`panic-probe`]: https://crates.io/crates/panic-probe\n+[`panic-ramdump`]: https://crates.io/crates/panic-ramdump\n+[`panic-rtt`]: https://crates.io/crates/panic_rtt\n+[`panic-rtt-target`]: https://crates.io/crates/panic-rtt-target\n+[`panic-semihosting`]: https://github.com/rust-embedded/panic-semihosting\n \n ## Firmware projects\n \n", "instance_id": "rust-embedded__awesome-embedded-rust-469", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to organize and list various panic handling implementations for embedded Rust development in a dedicated section of a README file. It identifies the goal of categorizing these implementations and raises a valid concern about their placement due to platform dependencies. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the criteria for inclusion in the list (e.g., are all panic-related crates to be included, or only those relevant to embedded systems?). Additionally, there is no mention of how to handle potential overlaps or conflicts between crates, nor are there guidelines for formatting or linking to resources. These minor gaps prevent it from being fully comprehensive, but the overall intent and scope are understandable.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it primarily involves updating a documentation file (README.md) with a structured list of panic handling crates. The code changes are limited to a single file and consist of adding a new section, reorganizing existing entries, and ensuring proper formatting and linking. There is no need for deep technical understanding of the Rust language, libraries, or complex algorithms, as the task is purely organizational and editorial in nature. It does not impact the system's architecture, require handling edge cases, or involve error handling logic. The scope of changes is minimal, and the concepts involved are basic (e.g., Markdown formatting and linking to external resources). The only slight complexity might be ensuring completeness and accuracy of the list, but this does not significantly elevate the difficulty. Therefore, a score of 0.15 reflects the very easy nature of this task.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Use standard LazyLock (Rust 1.80) instead of lazy_static dependencies.\nPlease, use standard [LazyLock](https://doc.rust-lang.org/stable/std/sync/struct.LazyLock.html#) (Rust 1.80) instead of lazy_static dependencies.\r\n\r\nSo remove unnecessary dependencies.\nRemoves `lazy_static` in favor of OnceLock.\nThe goal here was simply to remove `lazy_static` in favor of the the standard lib functions that replaced it. This is the smallest number of code changes that I could create to make that happen. I have tested this in my own project and it works as it did before.\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 1229f50..577c8b2 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,5 +1,8 @@\n # Unreleased\n \n+# 3.0.0\n+- **[BREAKING CHANGE]:** Upgrade MSRV to 1.80 and remove the then unnecessary lazy_static dependency.\n+\n # 2.2.0\n - Updated top-level docs to include a note about `ColoredString`\\'s role in the `Colorize` pipeline as well as link to it to suggest learning more about how to manipulate existing `ColoredString`\\'s.\n - Changes to `ColoredString`:\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 9d6c6c1..360ba67 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -1,7 +1,7 @@\n [package]\n name = \"colored\"\n description = \"The most simple way to add colors in your terminal\"\n-version = \"2.2.0\"\n+version = \"3.0.0\"\n edition = \"2021\"\n authors = [\"Thomas Wickham <mackwic@gmail.com>\"]\n license = \"MPL-2.0\"\n@@ -9,14 +9,12 @@ homepage = \"https://github.com/mackwic/colored\"\n repository = \"https://github.com/mackwic/colored\"\n readme = \"README.md\"\n keywords = [\"color\", \"string\", \"term\", \"ansi_term\", \"term-painter\"]\n-rust-version = \"1.70\"\n+rust-version = \"1.80\"\n \n [features]\n # with this feature, no color will ever be written\n no-color = []\n \n-[dependencies]\n-lazy_static = \"1\"\n \n [target.'cfg(windows)'.dependencies.windows-sys]\n version = \">=0.48,<=0.59\"\ndiff --git a/README.md b/README.md\nindex 9b8a30f..9dc33d3 100644\n--- a/README.md\n+++ b/README.md\n@@ -142,7 +142,7 @@ providing a reference implementation, which greatly helped making this crate\n output correct strings.\n \n ## Minimum Supported Rust Version (MSRV)\n-The current MSRV is `1.70`, which is checked and enforced automatically via CI. This version may change in the future in minor version bumps, so if you require a specific Rust version you should use a restricted version requirement such as `~X.Y`.\n+The current MSRV is `1.80`, which is checked and enforced automatically via CI. This version may change in the future in minor version bumps, so if you require a specific Rust version you should use a restricted version requirement such as `~X.Y`.\n \n ## License\n \ndiff --git a/src/control.rs b/src/control.rs\nindex 5ce7935..690f270 100644\n--- a/src/control.rs\n+++ b/src/control.rs\n@@ -4,6 +4,7 @@ use std::default::Default;\n use std::env;\n use std::io::{self, IsTerminal};\n use std::sync::atomic::{AtomicBool, Ordering};\n+use std::sync::LazyLock;\n \n /// Sets a flag to the console to use a virtual terminal environment.\n ///\n@@ -78,10 +79,8 @@ pub fn unset_override() {\n     SHOULD_COLORIZE.unset_override();\n }\n \n-lazy_static! {\n /// The persistent [`ShouldColorize`].\n-    pub static ref SHOULD_COLORIZE: ShouldColorize = ShouldColorize::from_env();\n-}\n+pub static SHOULD_COLORIZE: LazyLock<ShouldColorize> = LazyLock::new(ShouldColorize::from_env);\n \n impl Default for ShouldColorize {\n     fn default() -> ShouldColorize {\ndiff --git a/src/lib.rs b/src/lib.rs\nindex 116b9af..4c5aab1 100644\n--- a/src/lib.rs\n+++ b/src/lib.rs\n@@ -29,9 +29,6 @@\n //! modify them.\n #![warn(missing_docs)]\n \n-#[macro_use]\n-extern crate lazy_static;\n-\n #[cfg(test)]\n extern crate rspec;\n \n", "instance_id": "colored-rs__colored-176", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to replace the `lazy_static` dependency with the standard `LazyLock` from Rust 1.80. The goal is explicitly stated, and the context of removing an unnecessary dependency in favor of a standard library feature is provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify which parts of the codebase are affected beyond the general mention of replacing `lazy_static`, nor does it discuss potential compatibility issues or constraints with the Rust version upgrade (e.g., impact on downstream users). Additionally, while the code changes provide clarity on the implementation, the problem statement itself lacks examples or detailed requirements for edge cases or testing scenarios. Hence, it is rated as \"Mostly Clear\" with minor details missing.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a straightforward dependency replacement and minimal code modification. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes:** The changes are limited to a few files (`Cargo.toml`, `src/control.rs`, `src/lib.rs`, `CHANGELOG.md`, and `README.md`) and are mostly mechanical\u2014removing the `lazy_static` dependency, updating the Rust version, and replacing `lazy_static!` macro usage with `LazyLock`. The changes do not impact the system's architecture or require understanding complex interactions between modules. The overall amount of code change is small.\n\n2. **Number of Technical Concepts:** The problem requires basic knowledge of Rust's standard library (specifically `LazyLock` introduced in Rust 1.80) and dependency management via `Cargo.toml`. No advanced algorithms, design patterns, or domain-specific knowledge are needed. The concept of lazy initialization is straightforward for anyone familiar with Rust.\n\n3. **Edge Cases and Error Handling:** The problem statement and code changes do not mention any specific edge cases or error handling requirements. The replacement of `lazy_static` with `LazyLock` appears to be a direct substitution with no additional logic needed for error conditions, as the functionality remains equivalent.\n\n4. **Overall Complexity:** This task is a routine maintenance update that a junior or intermediate developer with basic Rust experience could handle. It involves minimal risk and does not require deep understanding of the codebase beyond the specific usage of lazy initialization.\n\nGiven these points, a difficulty score of 0.15 reflects the very easy nature of the task, requiring only basic code modifications and minimal technical depth.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "printf: renders \\x as a literal but should error due to missing hexadecimal number\nSteps to reproduce:\n```\nprintf '\\x'\n```\n\nWhat happens now: uutils `printf` renders this as a literal, as-is:\n```\n\\x\n```\n\nWhat I expected to happen: GNU `printf` terminates with an error message:\n```\nprintf: missing hexadecimal number in escape\n```\n\nNotes: this is causing a failure in the GNU test file `tests/printf/printf.sh`.\n", "patch": "diff --git a/src/uucore/src/lib/features/format/escape.rs b/src/uucore/src/lib/features/format/escape.rs\nindex 9420507f3e3..cd4ea658c39 100644\n--- a/src/uucore/src/lib/features/format/escape.rs\n+++ b/src/uucore/src/lib/features/format/escape.rs\n@@ -94,43 +94,50 @@ fn parse_unicode(input: &mut &[u8], digits: u8) -> Option<char> {\n     char::from_u32(ret)\n }\n \n-pub fn parse_escape_code(rest: &mut &[u8]) -> EscapedChar {\n+/// Represents an invalid escape sequence.\n+#[derive(Debug)]\n+pub struct EscapeError {}\n+\n+/// Parse an escape sequence, like `\\n` or `\\xff`, etc.\n+pub fn parse_escape_code(rest: &mut &[u8]) -> Result<EscapedChar, EscapeError> {\n     if let [c, new_rest @ ..] = rest {\n         // This is for the \\NNN syntax for octal sequences.\n         // Note that '0' is intentionally omitted because that\n         // would be the \\0NNN syntax.\n         if let b'1'..=b'7' = c {\n             if let Some(parsed) = parse_code(rest, Base::Oct) {\n-                return EscapedChar::Byte(parsed);\n+                return Ok(EscapedChar::Byte(parsed));\n             }\n         }\n \n         *rest = new_rest;\n         match c {\n-            b'\\\\' => EscapedChar::Byte(b'\\\\'),\n-            b'\"' => EscapedChar::Byte(b'\"'),\n-            b'a' => EscapedChar::Byte(b'\\x07'),\n-            b'b' => EscapedChar::Byte(b'\\x08'),\n-            b'c' => EscapedChar::End,\n-            b'e' => EscapedChar::Byte(b'\\x1b'),\n-            b'f' => EscapedChar::Byte(b'\\x0c'),\n-            b'n' => EscapedChar::Byte(b'\\n'),\n-            b'r' => EscapedChar::Byte(b'\\r'),\n-            b't' => EscapedChar::Byte(b'\\t'),\n-            b'v' => EscapedChar::Byte(b'\\x0b'),\n+            b'\\\\' => Ok(EscapedChar::Byte(b'\\\\')),\n+            b'\"' => Ok(EscapedChar::Byte(b'\"')),\n+            b'a' => Ok(EscapedChar::Byte(b'\\x07')),\n+            b'b' => Ok(EscapedChar::Byte(b'\\x08')),\n+            b'c' => Ok(EscapedChar::End),\n+            b'e' => Ok(EscapedChar::Byte(b'\\x1b')),\n+            b'f' => Ok(EscapedChar::Byte(b'\\x0c')),\n+            b'n' => Ok(EscapedChar::Byte(b'\\n')),\n+            b'r' => Ok(EscapedChar::Byte(b'\\r')),\n+            b't' => Ok(EscapedChar::Byte(b'\\t')),\n+            b'v' => Ok(EscapedChar::Byte(b'\\x0b')),\n             b'x' => {\n                 if let Some(c) = parse_code(rest, Base::Hex) {\n-                    EscapedChar::Byte(c)\n+                    Ok(EscapedChar::Byte(c))\n                 } else {\n-                    EscapedChar::Backslash(b'x')\n+                    Err(EscapeError {})\n                 }\n             }\n-            b'0' => EscapedChar::Byte(parse_code(rest, Base::Oct).unwrap_or(b'\\0')),\n-            b'u' => EscapedChar::Char(parse_unicode(rest, 4).unwrap_or('\\0')),\n-            b'U' => EscapedChar::Char(parse_unicode(rest, 8).unwrap_or('\\0')),\n-            c => EscapedChar::Backslash(*c),\n+            b'0' => Ok(EscapedChar::Byte(\n+                parse_code(rest, Base::Oct).unwrap_or(b'\\0'),\n+            )),\n+            b'u' => Ok(EscapedChar::Char(parse_unicode(rest, 4).unwrap_or('\\0'))),\n+            b'U' => Ok(EscapedChar::Char(parse_unicode(rest, 8).unwrap_or('\\0'))),\n+            c => Ok(EscapedChar::Backslash(*c)),\n         }\n     } else {\n-        EscapedChar::Byte(b'\\\\')\n+        Ok(EscapedChar::Byte(b'\\\\'))\n     }\n }\ndiff --git a/src/uucore/src/lib/features/format/mod.rs b/src/uucore/src/lib/features/format/mod.rs\nindex 6a09b32e2a9..5707a2177d6 100644\n--- a/src/uucore/src/lib/features/format/mod.rs\n+++ b/src/uucore/src/lib/features/format/mod.rs\n@@ -67,6 +67,8 @@ pub enum FormatError {\n     InvalidPrecision(String),\n     /// The format specifier ends with a %, as in `%f%`.\n     EndsWithPercent(Vec<u8>),\n+    /// The escape sequence `\\x` appears without a literal hexadecimal value.\n+    MissingHex,\n }\n \n impl Error for FormatError {}\n@@ -105,6 +107,7 @@ impl Display for FormatError {\n             Self::IoError(_) => write!(f, \"io error\"),\n             Self::NoMoreArguments => write!(f, \"no more arguments\"),\n             Self::InvalidArgument(_) => write!(f, \"invalid argument\"),\n+            Self::MissingHex => write!(f, \"missing hexadecimal number in escape\"),\n         }\n     }\n }\n@@ -181,7 +184,10 @@ pub fn parse_spec_and_escape(\n         }\n         [b'\\\\', rest @ ..] => {\n             current = rest;\n-            Some(Ok(FormatItem::Char(parse_escape_code(&mut current))))\n+            Some(match parse_escape_code(&mut current) {\n+                Ok(c) => Ok(FormatItem::Char(c)),\n+                Err(_) => Err(FormatError::MissingHex),\n+            })\n         }\n         [c, rest @ ..] => {\n             current = rest;\n@@ -224,7 +230,7 @@ pub fn parse_escape_only(fmt: &[u8]) -> impl Iterator<Item = EscapedChar> + '_ {\n         [] => None,\n         [b'\\\\', rest @ ..] => {\n             current = rest;\n-            Some(parse_escape_code(&mut current))\n+            Some(parse_escape_code(&mut current).unwrap_or(EscapedChar::Backslash(b'x')))\n         }\n         [c, rest @ ..] => {\n             current = rest;\n", "instance_id": "uutils__coreutils-7259", "clarity": 3, "difficulty": 0.3, "clarity_explanation": "The problem statement is comprehensive and clear. It explicitly describes the issue with the `printf` utility in the uutils project, where an escape sequence `\\x` is rendered as a literal instead of producing an error for a missing hexadecimal number, as GNU `printf` does. The statement includes steps to reproduce the issue, the current incorrect behavior, the expected behavior, and a reference to a failing test case in the GNU test suite. There are no significant ambiguities, and the goal (to match GNU `printf` behavior by throwing an error) is well-defined. The inclusion of a specific error message and context about the failing test further enhances clarity. All necessary details for understanding the problem and verifying the solution are provided.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following reasons based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are localized to two files (`escape.rs` and `mod.rs`) within the `uucore` library, specifically targeting the escape sequence parsing logic. The modifications involve updating the return type of `parse_escape_code` to handle errors and propagating this error through the format parsing logic. The diff shows a moderate amount of code change (around 50 lines), but it does not impact the broader system architecture or require extensive refactoring. The changes are straightforward and focused on a specific feature.\n\n2. **Number of Technical Concepts:** Solving this requires understanding Rust's error handling mechanisms (e.g., `Result` type), basic string/byte parsing, and the behavior of escape sequences in `printf`. These are relatively fundamental concepts in Rust and do not involve advanced language features, complex algorithms, or domain-specific knowledge beyond familiarity with command-line utility behavior. The solution primarily involves modifying an existing function to return an error instead of a default value.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement explicitly mentions the edge case of an incomplete `\\x` escape sequence, and the code changes address this by introducing a new error type (`EscapeError`) and updating the parsing logic to return an error in this scenario. No other complex edge cases or performance considerations are implied or required beyond this specific fix. The error handling logic added is simple and does not introduce significant complexity.\n\n4. **Overall Assessment:** This task requires understanding a small, isolated part of the codebase and making targeted modifications to fix a bug. It does not demand deep architectural knowledge or extensive experience with the entire project. The problem is well-contained, and the solution is a logical extension of existing code patterns (e.g., using `Result` for error handling). Therefore, a difficulty score of 0.30 reflects the simplicity of the fix while acknowledging the need for some familiarity with Rust's error handling and the specific parsing logic in the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "sort: -S/--buffer-size does not support memory percentages %\nGNU sort v. Rust sort:\r\n```\r\n$ /usr/bin/sort --buffer-size=50% < /dev/null\r\n$ target/release/coreutils sort --buffer-size=50% < /dev/null\r\nsort: invalid --buffer-size argument '50%'\r\n```\r\nRelevant excerpts from GNU sort manpage:\r\n```\r\n       -S, --buffer-size=SIZE\r\n              use SIZE for main memory buffer\r\n\r\n[...]\r\n\r\n       SIZE may be followed by the following multiplicative suffixes: % 1% of memory, b 1, K 1024 (default), and so on for M, G, T, P, E, Z, Y.\r\n```\r\n\r\n(this could be a good first issue)\n", "patch": "diff --git a/src/uu/dd/src/parseargs.rs b/src/uu/dd/src/parseargs.rs\nindex 59836b1a1e4..e26b3495316 100644\n--- a/src/uu/dd/src/parseargs.rs\n+++ b/src/uu/dd/src/parseargs.rs\n@@ -517,9 +517,7 @@ fn parse_bytes_no_x(full: &str, s: &str) -> Result<u64, ParseError> {\n         (None, None, None) => match parser.parse_u64(s) {\n             Ok(n) => (n, 1),\n             Err(ParseSizeError::SizeTooBig(_)) => (u64::MAX, 1),\n-            Err(ParseSizeError::InvalidSuffix(_) | ParseSizeError::ParseFailure(_)) => {\n-                return Err(ParseError::InvalidNumber(full.to_string()))\n-            }\n+            Err(_) => return Err(ParseError::InvalidNumber(full.to_string())),\n         },\n         (Some(i), None, None) => (parse_bytes_only(s, i)?, 1),\n         (None, Some(i), None) => (parse_bytes_only(s, i)?, 2),\ndiff --git a/src/uu/df/src/df.rs b/src/uu/df/src/df.rs\nindex 092c8381290..8602d8af7af 100644\n--- a/src/uu/df/src/df.rs\n+++ b/src/uu/df/src/df.rs\n@@ -189,6 +189,7 @@ impl Options {\n                         .to_string(),\n                 ),\n                 ParseSizeError::ParseFailure(s) => OptionsError::InvalidBlockSize(s),\n+                ParseSizeError::PhysicalMem(s) => OptionsError::InvalidBlockSize(s),\n             })?,\n             header_mode: {\n                 if matches.get_flag(OPT_HUMAN_READABLE_BINARY)\ndiff --git a/src/uu/du/src/du.rs b/src/uu/du/src/du.rs\nindex 2392497a935..bd017f1d515 100644\n--- a/src/uu/du/src/du.rs\n+++ b/src/uu/du/src/du.rs\n@@ -1120,7 +1120,9 @@ fn format_error_message(error: &ParseSizeError, s: &str, option: &str) -> String\n         ParseSizeError::InvalidSuffix(_) => {\n             format!(\"invalid suffix in --{} argument {}\", option, s.quote())\n         }\n-        ParseSizeError::ParseFailure(_) => format!(\"invalid --{} argument {}\", option, s.quote()),\n+        ParseSizeError::ParseFailure(_) | ParseSizeError::PhysicalMem(_) => {\n+            format!(\"invalid --{} argument {}\", option, s.quote())\n+        }\n         ParseSizeError::SizeTooBig(_) => format!(\"--{} argument {} too large\", option, s.quote()),\n     }\n }\ndiff --git a/src/uu/od/src/od.rs b/src/uu/od/src/od.rs\nindex 6dd75d30792..fcb72c1ae70 100644\n--- a/src/uu/od/src/od.rs\n+++ b/src/uu/od/src/od.rs\n@@ -626,7 +626,9 @@ fn format_error_message(error: &ParseSizeError, s: &str, option: &str) -> String\n         ParseSizeError::InvalidSuffix(_) => {\n             format!(\"invalid suffix in --{} argument {}\", option, s.quote())\n         }\n-        ParseSizeError::ParseFailure(_) => format!(\"invalid --{} argument {}\", option, s.quote()),\n+        ParseSizeError::ParseFailure(_) | ParseSizeError::PhysicalMem(_) => {\n+            format!(\"invalid --{} argument {}\", option, s.quote())\n+        }\n         ParseSizeError::SizeTooBig(_) => format!(\"--{} argument {} too large\", option, s.quote()),\n     }\n }\ndiff --git a/src/uu/sort/src/sort.rs b/src/uu/sort/src/sort.rs\nindex f2677e66136..edff2baae85 100644\n--- a/src/uu/sort/src/sort.rs\n+++ b/src/uu/sort/src/sort.rs\n@@ -289,7 +289,7 @@ impl GlobalSettings {\n         // GNU sort (8.32) invalid:  b, B, 1B,                         p, e, z, y\n         let size = Parser::default()\n             .with_allow_list(&[\n-                \"b\", \"k\", \"K\", \"m\", \"M\", \"g\", \"G\", \"t\", \"T\", \"P\", \"E\", \"Z\", \"Y\", \"R\", \"Q\",\n+                \"b\", \"k\", \"K\", \"m\", \"M\", \"g\", \"G\", \"t\", \"T\", \"P\", \"E\", \"Z\", \"Y\", \"R\", \"Q\", \"%\",\n             ])\n             .with_default_unit(\"K\")\n             .with_b_byte_count(true)\n@@ -1855,7 +1855,9 @@ fn format_error_message(error: &ParseSizeError, s: &str, option: &str) -> String\n         ParseSizeError::InvalidSuffix(_) => {\n             format!(\"invalid suffix in --{} argument {}\", option, s.quote())\n         }\n-        ParseSizeError::ParseFailure(_) => format!(\"invalid --{} argument {}\", option, s.quote()),\n+        ParseSizeError::ParseFailure(_) | ParseSizeError::PhysicalMem(_) => {\n+            format!(\"invalid --{} argument {}\", option, s.quote())\n+        }\n         ParseSizeError::SizeTooBig(_) => format!(\"--{} argument {} too large\", option, s.quote()),\n     }\n }\ndiff --git a/src/uucore/src/lib/parser/parse_size.rs b/src/uucore/src/lib/parser/parse_size.rs\nindex c99df6e39ce..b18e1695d70 100644\n--- a/src/uucore/src/lib/parser/parse_size.rs\n+++ b/src/uucore/src/lib/parser/parse_size.rs\n@@ -8,10 +8,70 @@\n \n use std::error::Error;\n use std::fmt;\n-use std::num::IntErrorKind;\n+#[cfg(target_os = \"linux\")]\n+use std::io::BufRead;\n+use std::num::{IntErrorKind, ParseIntError};\n \n use crate::display::Quotable;\n \n+/// Error arising from trying to compute system memory.\n+enum SystemError {\n+    IOError,\n+    ParseError,\n+    NotFound,\n+}\n+\n+impl From<std::io::Error> for SystemError {\n+    fn from(_: std::io::Error) -> Self {\n+        Self::IOError\n+    }\n+}\n+\n+impl From<ParseIntError> for SystemError {\n+    fn from(_: ParseIntError) -> Self {\n+        Self::ParseError\n+    }\n+}\n+\n+/// Get the total number of bytes of physical memory.\n+///\n+/// The information is read from the `/proc/meminfo` file.\n+///\n+/// # Errors\n+///\n+/// If there is a problem reading the file or finding the appropriate\n+/// entry in the file.\n+#[cfg(target_os = \"linux\")]\n+fn total_physical_memory() -> Result<u128, SystemError> {\n+    // On Linux, the `/proc/meminfo` file has a table with information\n+    // about memory usage. For example,\n+    //\n+    //     MemTotal:        7811500 kB\n+    //     MemFree:         1487876 kB\n+    //     MemAvailable:    3857232 kB\n+    //     ...\n+    //\n+    // We just need to extract the number of `MemTotal`\n+    let table = std::fs::read(\"/proc/meminfo\")?;\n+    for line in table.lines() {\n+        let line = line?;\n+        if line.starts_with(\"MemTotal:\") && line.ends_with(\"kB\") {\n+            let num_kilobytes: u128 = line[9..line.len() - 2].trim().parse()?;\n+            let num_bytes = 1024 * num_kilobytes;\n+            return Ok(num_bytes);\n+        }\n+    }\n+    Err(SystemError::NotFound)\n+}\n+\n+/// Get the total number of bytes of physical memory.\n+///\n+/// TODO Implement this for non-Linux systems.\n+#[cfg(not(target_os = \"linux\"))]\n+fn total_physical_memory() -> Result<u128, SystemError> {\n+    Err(SystemError::NotFound)\n+}\n+\n /// Parser for sizes in SI or IEC units (multiples of 1000 or 1024 bytes).\n ///\n /// The [`Parser::parse`] function performs the parse.\n@@ -133,6 +193,16 @@ impl<'parser> Parser<'parser> {\n             }\n         }\n \n+        // Special case: for percentage, just compute the given fraction\n+        // of the total physical memory on the machine, if possible.\n+        if unit == \"%\" {\n+            let number: u128 = Self::parse_number(&numeric_string, 10, size)?;\n+            return match total_physical_memory() {\n+                Ok(total) => Ok((number / 100) * total),\n+                Err(_) => Err(ParseSizeError::PhysicalMem(size.to_string())),\n+            };\n+        }\n+\n         // Compute the factor the unit represents.\n         // empty string means the factor is 1.\n         //\n@@ -320,6 +390,9 @@ pub enum ParseSizeError {\n \n     /// Overflow\n     SizeTooBig(String),\n+\n+    /// Could not determine total physical memory size.\n+    PhysicalMem(String),\n }\n \n impl Error for ParseSizeError {\n@@ -328,6 +401,7 @@ impl Error for ParseSizeError {\n             Self::InvalidSuffix(ref s) => s,\n             Self::ParseFailure(ref s) => s,\n             Self::SizeTooBig(ref s) => s,\n+            Self::PhysicalMem(ref s) => s,\n         }\n     }\n }\n@@ -335,7 +409,10 @@ impl Error for ParseSizeError {\n impl fmt::Display for ParseSizeError {\n     fn fmt(&self, f: &mut fmt::Formatter) -> Result<(), fmt::Error> {\n         let s = match self {\n-            Self::InvalidSuffix(s) | Self::ParseFailure(s) | Self::SizeTooBig(s) => s,\n+            Self::InvalidSuffix(s)\n+            | Self::ParseFailure(s)\n+            | Self::SizeTooBig(s)\n+            | Self::PhysicalMem(s) => s,\n         };\n         write!(f, \"{s}\")\n     }\n@@ -674,4 +751,16 @@ mod tests {\n         assert_eq!(Ok(94722), parse_size_u64(\"0x17202\"));\n         assert_eq!(Ok(44251 * 1024), parse_size_u128(\"0xACDBK\"));\n     }\n+\n+    #[test]\n+    #[cfg(target_os = \"linux\")]\n+    fn parse_percent() {\n+        assert!(parse_size_u64(\"0%\").is_ok());\n+        assert!(parse_size_u64(\"50%\").is_ok());\n+        assert!(parse_size_u64(\"100%\").is_ok());\n+        assert!(parse_size_u64(\"100000%\").is_ok());\n+        assert!(parse_size_u64(\"-1%\").is_err());\n+        assert!(parse_size_u64(\"1.0%\").is_err());\n+        assert!(parse_size_u64(\"0x1%\").is_err());\n+    }\n }\n", "instance_id": "uutils__coreutils-7181", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the goal: to add support for memory percentage (%) in the buffer-size argument for the Rust implementation of the `sort` utility, aligning it with GNU sort behavior. It provides a direct comparison between GNU sort and Rust sort outputs, along with relevant excerpts from the GNU sort manpage, which helps in understanding the expected functionality. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss how the percentage should be calculated (e.g., based on total physical memory or available memory) or what should happen if the system cannot determine the memory size. Additionally, edge cases such as invalid percentage values (e.g., >100% or negative values) or platform-specific behavior (e.g., non-Linux systems) are not mentioned. These gaps prevent it from being fully comprehensive, but the overall intent and requirements are clear enough to proceed with implementation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of code changes is moderate but manageable, involving modifications across multiple files (e.g., `sort.rs`, `parse_size.rs`, and others like `dd`, `df`, `du`, `od`) to integrate the new percentage-based buffer size logic and update error handling. However, the changes are mostly localized to parsing logic and error message formatting, without requiring deep architectural changes to the codebase. Second, the technical concepts involved are relatively straightforward: parsing string inputs with suffixes, handling system memory information (via `/proc/meminfo` on Linux), and basic arithmetic for percentage calculations. The implementation in `parse_size.rs` shows a clear approach to fetching total physical memory and computing the percentage, which does not require advanced algorithms or design patterns. Third, while edge cases like invalid percentages or failure to retrieve memory information are handled in the code (e.g., via `PhysicalMem` error variant), they are not overly complex to implement. The primary challenge lies in ensuring platform compatibility (noted as TODO for non-Linux systems) and understanding the existing parsing framework, but these are within the grasp of a developer with basic to intermediate Rust experience. The problem is labeled as a \"good first issue,\" which aligns with this assessment of being approachable for newcomers with some guidance. Overall, this task requires understanding specific code logic and making targeted modifications, justifying a score of 0.35.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Dead link for ChaCha20Poly1305 Security Audit\nDead link for ChaCha20Poly1305 Security Audit, [page can still be found on archive.org](https://web.archive.org/web/20200606025324/https://research.nccgroup.com/2020/02/26/public-report-rustcrypto-aes-gcm-and-chacha20poly1305-implementation-review/).\n", "patch": "diff --git a/chacha20poly1305/README.md b/chacha20poly1305/README.md\nindex abd82edb..f43e857d 100644\n--- a/chacha20poly1305/README.md\n+++ b/chacha20poly1305/README.md\n@@ -79,5 +79,5 @@ dual licensed as above, without any additional terms or conditions.\n [2]: https://en.wikipedia.org/wiki/Authenticated_encryption\n [3]: https://github.com/RustCrypto/stream-ciphers/tree/master/chacha20\n [4]: https://github.com/RustCrypto/universal-hashes/tree/master/poly1305\n-[5]: https://research.nccgroup.com/2020/02/26/public-report-rustcrypto-aes-gcm-and-chacha20poly1305-implementation-review/\n+[5]: https://web.archive.org/web/20200606025324/https://research.nccgroup.com/2020/02/26/public-report-rustcrypto-aes-gcm-and-chacha20poly1305-implementation-review/\n [6]: https://www.mobilecoin.com/\n", "instance_id": "RustCrypto__AEADs-636", "clarity": 2, "difficulty": 0.05, "clarity_explanation": "The problem statement is mostly clear in its intent: it identifies a dead link in the documentation for the ChaCha20Poly1305 security audit and provides a solution by referencing an archived version of the page on archive.org. The goal is straightforward\u2014update the broken link to point to the archived version. However, the statement lacks additional context or details, such as why the link is dead, whether this is part of a broader documentation update, or if there are other links to check. There are no explicit mentions of constraints, edge cases, or additional requirements, though these are arguably unnecessary for such a simple task. Overall, the problem is valid and clear but misses minor contextual details that could provide a fuller picture.", "difficulty_explanation": "The difficulty of this task is very low, as it involves a trivial modification to a single line in a README.md file. The code change is limited to updating a URL in the documentation, requiring no understanding of the codebase's logic, architecture, or technical concepts beyond basic Markdown formatting. There is no impact on the system's functionality or architecture, and the scope of the change is extremely narrow, affecting only a single file with a one-line diff. No edge cases, error handling, or domain-specific knowledge are required, nor is there any need to understand programming language features, libraries, or algorithms. This task falls into the \"very easy\" category, as it is essentially a documentation fix akin to correcting a typo or updating a reference.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "client can send more TLS1.3 early data than maximum if early data is read\nReported originally by @tahmid-23 :\r\n\r\n---\r\n\r\n### Summary\r\nReading early data on the server reduces the tracked received early data, which allows the client to send more early data than specified by `max_early_data_size`.\r\n\r\n### Details\r\n`ChunkVecBuffer` pops/shrinks chunks as it is read. In `EarlyDataState::take_received_plaintext`, it uses the total length of the `ChunkVecBuffer` to determine whether or not more early data can be accepted. However, if the server reads the early data, the `ChunkVecBuffer` will pop the data, thus reducing the perceived amount of early data that can be read.\r\n\r\n### PoC\r\nAdding this test to `rustls/tests/api.rs` demonstrates that this is possible.\r\nIn order for this to work, the client must behave maliciously. This can be achieved by making `rustls::client::client_conn::EarlyData::check_write_opt()` always return all requested bytes.\r\n```rs\r\n#[test]\r\nfn malicious_early_data_client() {\r\n    let (client_config, server_config) = early_data_configs();\r\n\r\n    // first connection\r\n    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\r\n    do_handshake(&mut client, &mut server);\r\n\r\n    // second connection\r\n    let (mut client, mut server) = make_pair_for_arc_configs(&client_config, &server_config);\r\n    assert!(client.early_data().is_some());\r\n    assert_eq!(\r\n        client\r\n            .early_data()\r\n            .unwrap()\r\n            .bytes_left(),\r\n        1234\r\n    );\r\n\r\n    // send early data on connection\r\n    client\r\n        .early_data()\r\n        .unwrap()\r\n        .write(&[b'Z'; 1234]) // maximum 1234 bytes\r\n        .unwrap();\r\n\r\n    transfer(&mut client, &mut server);\r\n    server.process_new_packets().unwrap();\r\n\r\n    // server begins to respond, but...\r\n    transfer(&mut server, &mut client);\r\n    // ...don't process anything on the client so that it remains in early state\r\n\r\n    let mut received_early_data = [0u8; 1234];\r\n    assert_eq!(\r\n        server\r\n            .early_data()\r\n            .expect(\"early_data didn't happen\")\r\n            .read(&mut received_early_data)\r\n            .expect(\"early_data failed unexpectedly\"),\r\n        1234\r\n    );\r\n\r\n    // Send as much early data as you'd like!\r\n    // client sends early data, waits until server processes it, and then client sends more early data\r\n    // an actual implementation over IO would require special timing\r\n    for _ in 0..100 {\r\n        // just keep sending under max early data size limit each time\r\n        client\r\n            .early_data()\r\n            .unwrap()\r\n            .write(&[b'Z'; 1234])\r\n            .unwrap();\r\n        transfer(&mut client, &mut server);\r\n        // this should panic, too much early data was sent\r\n        server.process_new_packets().unwrap();\r\n\r\n        // server keeps responding...\r\n        transfer(&mut server, &mut client);\r\n        // client keeps itself in early state\r\n\r\n        let mut received_early_data = [0u8; 1234];\r\n        // fails here, all early data should be read at this point\r\n        assert_eq!(\r\n            server\r\n                .early_data()\r\n                .expect(\"early_data didn't happen\")\r\n                .read(&mut received_early_data)\r\n                .expect(\"early_data failed unexpectedly\"),\r\n            0\r\n        );\r\n    }\r\n}\r\n```\r\n\r\n### Impact\r\nAny server that attempts to read early data before waiting for the handshake to complete is vulnerable.\r\n\r\n---\r\n\r\nIn private discussion, we were sure that this was a bug, but unsure if this was a security bug, given RFC8446 doesn't actually require a server to limit anything to `max_early_data_size` (it's a \"SHOULD\"). However, we shall certainly fix this.\n", "patch": "diff --git a/rustls/src/msgs/persist.rs b/rustls/src/msgs/persist.rs\nindex 775123b7462..5a8998fb551 100644\n--- a/rustls/src/msgs/persist.rs\n+++ b/rustls/src/msgs/persist.rs\n@@ -117,6 +117,12 @@ impl Tls13ClientSessionValue {\n         self.common.epoch -= delta as u64;\n     }\n \n+    #[doc(hidden)]\n+    /// Test only: replace `max_early_data_size` with `new`\n+    pub fn _private_set_max_early_data_size(&mut self, new: u32) {\n+        self.max_early_data_size = new;\n+    }\n+\n     pub fn set_quic_params(&mut self, quic_params: &[u8]) {\n         self.quic_params = PayloadU16(quic_params.to_vec());\n     }\ndiff --git a/rustls/src/server/server_conn.rs b/rustls/src/server/server_conn.rs\nindex 515b312d6b6..027768d5948 100644\n--- a/rustls/src/server/server_conn.rs\n+++ b/rustls/src/server/server_conn.rs\n@@ -988,7 +988,10 @@ impl State<ServerConnectionData> for Accepting {\n \n pub(super) enum EarlyDataState {\n     New,\n-    Accepted(ChunkVecBuffer),\n+    Accepted {\n+        received: ChunkVecBuffer,\n+        left: usize,\n+    },\n     Rejected,\n }\n \n@@ -1004,12 +1007,15 @@ impl EarlyDataState {\n     }\n \n     pub(super) fn accept(&mut self, max_size: usize) {\n-        *self = Self::Accepted(ChunkVecBuffer::new(Some(max_size)));\n+        *self = Self::Accepted {\n+            received: ChunkVecBuffer::new(Some(max_size)),\n+            left: max_size,\n+        };\n     }\n \n     #[cfg(feature = \"std\")]\n     fn was_accepted(&self) -> bool {\n-        matches!(self, Self::Accepted(_))\n+        matches!(self, Self::Accepted { .. })\n     }\n \n     pub(super) fn was_rejected(&self) -> bool {\n@@ -1018,7 +1024,9 @@ impl EarlyDataState {\n \n     fn pop(&mut self) -> Option<Vec<u8>> {\n         match self {\n-            Self::Accepted(ref mut received) => received.pop(),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.pop(),\n             _ => None,\n         }\n     }\n@@ -1026,7 +1034,9 @@ impl EarlyDataState {\n     #[cfg(feature = \"std\")]\n     fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {\n         match self {\n-            Self::Accepted(ref mut received) => received.read(buf),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.read(buf),\n             _ => Err(io::Error::from(io::ErrorKind::BrokenPipe)),\n         }\n     }\n@@ -1034,7 +1044,9 @@ impl EarlyDataState {\n     #[cfg(read_buf)]\n     fn read_buf(&mut self, cursor: core::io::BorrowedCursor<'_>) -> io::Result<()> {\n         match self {\n-            Self::Accepted(ref mut received) => received.read_buf(cursor),\n+            Self::Accepted {\n+                ref mut received, ..\n+            } => received.read_buf(cursor),\n             _ => Err(io::Error::from(io::ErrorKind::BrokenPipe)),\n         }\n     }\n@@ -1042,8 +1054,12 @@ impl EarlyDataState {\n     pub(super) fn take_received_plaintext(&mut self, bytes: Payload<'_>) -> bool {\n         let available = bytes.bytes().len();\n         match self {\n-            Self::Accepted(ref mut received) if received.apply_limit(available) == available => {\n+            Self::Accepted {\n+                ref mut received,\n+                ref mut left,\n+            } if received.apply_limit(available) == available && available <= *left => {\n                 received.append(bytes.into_vec());\n+                *left -= available;\n                 true\n             }\n             _ => false,\n@@ -1055,7 +1071,12 @@ impl Debug for EarlyDataState {\n     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {\n         match self {\n             Self::New => write!(f, \"EarlyDataState::New\"),\n-            Self::Accepted(buf) => write!(f, \"EarlyDataState::Accepted({})\", buf.len()),\n+            Self::Accepted { received, left } => write!(\n+                f,\n+                \"EarlyDataState::Accepted {{ received: {}, left: {} }}\",\n+                received.len(),\n+                left\n+            ),\n             Self::Rejected => write!(f, \"EarlyDataState::Rejected\"),\n         }\n     }\n", "instance_id": "rustls__rustls-2097", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear and provides a detailed summary of the issue regarding TLS 1.3 early data handling in the `rustls` library. It explains the bug's nature\u2014how reading early data on the server reduces the tracked amount, allowing a client to send more data than the specified `max_early_data_size`. The inclusion of a proof-of-concept (PoC) test in Rust code further clarifies the issue by demonstrating the exploit. The impact section highlights the vulnerability for servers reading early data before handshake completion. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior or constraints beyond referencing RFC8446's \"SHOULD\" clause, and it lacks detailed discussion on potential edge cases or specific security implications beyond the general impact. Additionally, while the PoC is helpful, it assumes a malicious client modification, which might not be immediately clear to someone unfamiliar with the context. Overall, the statement is valid and clear but misses some minor details, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of solving this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively focused, primarily affecting the `EarlyDataState` enum in `server_conn.rs` by restructuring it to track both received data and remaining allowance (`left`) separately, ensuring the limit is enforced even after data is read. A minor utility function is also added in `persist.rs` for testing purposes. This involves changes to a single core module but does not significantly impact the broader architecture. Second, the technical concepts required include a moderate understanding of Rust's struct/enum patterns, state management, and the specifics of TLS 1.3 early data handling as per RFC8446, which adds some domain-specific complexity. Third, the problem requires addressing a specific edge case\u2014ensuring the early data limit is respected even after data is consumed\u2014which the code change handles by maintaining a separate `left` counter. However, the error handling logic remains minimal, as the focus is on state tracking rather than extensive exception management. Overall, this requires understanding multiple concepts (TLS protocol nuances, state management in Rust) and making targeted but non-trivial modifications, justifying a difficulty score of 0.55, leaning toward the higher end of medium due to the domain-specific knowledge required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ergonomics of `alt()` taking tuple versus slice\nIs there a reason `alt()` cannot take a slice of child parsers?\r\n\r\nInstead, it takes a `List`, which is any of the 1- to 21-tuple implementations.\r\n\r\nThis is fine when the format for which you are trying to parse is known at compile-time/statically, but unusable when the format is only known at runtime.\r\n\r\nFor example, I'm writing a software-version bumper right now, and the user provides both their arbitrary format (e.g. `yyyy.patch`) and their current version (e.g. `2024.5`) as inputs to my package/program. First, I have a parser for the format string (which is static-defined), but then I need to create a new parser from its result for the actual version string, which is runtime-defined. Therefore, this second parser cannot be made from a tuple.\r\n\r\nI have been able to work around this by continually building a parser up in a \"reduce\" fashion, something like\r\n\r\n```rust\r\nparsers.reduce(|acc, p| acc.or(p))\r\n```\r\n\r\n(I believe that applying pairwise `or()` calls has the same effect as `alt()`, right?) Nonetheless, it's cumbersome, especially with the type annotations, for a newer Rust user like me: plenty of `Box`, `dyn`, lifetimes, `Sized`, etc.\r\n\r\nSo, I wonder why there isn't a version of alt that could take in a dynamically sized slice of child parsers.\r\n\r\n## Prerequisites\r\n\r\n- Rust version: `rustc 1.77.0-nightly (e51e98dde 2023-12-31)`\r\n- nom version : `7.1.3`\r\n- nom compilation features used: default ones\r\n\r\n\r\n\n", "patch": "diff --git a/src/branch/mod.rs b/src/branch/mod.rs\nindex 80c3d1f84..20205f595 100644\n--- a/src/branch/mod.rs\n+++ b/src/branch/mod.rs\n@@ -212,6 +212,44 @@ impl<\n   }\n }\n \n+impl<\n+    Input: Clone,\n+    Output,\n+    Error: ParseError<Input>,\n+    A: Parser<Input, Output = Output, Error = Error>,\n+  > Parser<Input> for Choice<&mut [A]>\n+{\n+  type Output = Output;\n+  type Error = Error;\n+\n+  #[inline]\n+  fn process<OM: crate::OutputMode>(\n+    &mut self,\n+    input: Input,\n+  ) -> crate::PResult<OM, Input, Self::Output, Self::Error> {\n+    let mut error = None;\n+\n+    for branch in self.parser.iter_mut() {\n+      match branch.process::<OM>(input.clone()) {\n+        Err(Err::Error(e)) => match error {\n+          None => error = Some(e),\n+          Some(err) => error = Some(OM::Error::combine(err, e, |e1, e2| e1.or(e2))),\n+        },\n+        res => return res,\n+      }\n+    }\n+\n+    match error {\n+      Some(e) => Err(Err::Error(OM::Error::map(e, |err| {\n+        Error::append(input, ErrorKind::Alt, err)\n+      }))),\n+      None => Err(Err::Error(OM::Error::bind(|| {\n+        Error::from_error_kind(input, ErrorKind::Alt)\n+      }))),\n+    }\n+  }\n+}\n+\n macro_rules! permutation_trait(\n   (\n     $name1:ident $ty1:ident $item1:ident\n", "instance_id": "rust-bakery__nom-1754", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the user's intent and the limitation they are facing with the `alt()` function in the `nom` parser combinator library. The goal is well-defined: to support a dynamically sized slice of child parsers instead of a fixed-size tuple (via `List`). The context of the problem, including the use case (software version bumper with runtime-defined formats), is provided, which helps in understanding the motivation. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected input and output formats for the proposed `alt()` variant, nor does it discuss potential constraints or edge cases (e.g., performance implications of using slices, memory management for dynamic parsers). Additionally, while the workaround using `reduce` is mentioned, it lacks clarity on whether this is a sufficient solution or if a library-level change is strictly necessary. Overall, the statement is valid and mostly clear but misses some critical details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of the code change, while localized to a single file (`branch/mod.rs`), involves modifying a core component of the `nom` library, which is a parser combinator framework. Adding support for a slice-based `alt()` (via the `Choice` struct implementation for `&mut [A]`) requires a deep understanding of the library's internal architecture, particularly how parser combinators are structured and how errors are propagated and combined. The code change itself is moderate in size but intricate, as it deals with mutable iteration over a slice of parsers, error handling with `ParseError`, and integration with the existing `OutputMode` system. \n\nSecond, the number of technical concepts involved is significant. Solving this requires familiarity with Rust's advanced type system (traits, generics, lifetimes), parser combinator design patterns, and the specific error handling mechanisms in `nom`. Additionally, the implementation must handle dynamic sizing, which introduces complexity compared to the static tuple-based approach.\n\nThird, potential edge cases and error handling add to the difficulty. The code change already accounts for error accumulation and combination during parsing attempts across multiple branches, but the problem statement does not discuss performance implications (e.g., cloning `Input` for each parser attempt) or constraints on the slice size, which could be critical in a real-world scenario. Ensuring correctness for empty slices, large slices, or parsers with divergent error types adds further complexity.\n\nOverall, this problem requires a deep understanding of both Rust and the `nom` library's internals, along with careful consideration of error handling and potential runtime impacts. It is not at the highest end of difficulty (e.g., system-level redesign), but it is challenging enough to warrant a score of 0.65, placing it in the \"Hard\" range due to the specialized knowledge and precision required.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Auto scroll to focused can override scroll commands\n<!--\r\nPlease, make sure:\r\n\r\n- The issue happens in the latest crate release or newer (master branch).\r\n- The issue happens after `cargo update`.\r\n-->\r\n\r\n#### I tried this code:\r\n\r\n```rust\r\nfn issue() -> impl UiNode {\r\n    let mut txt = Txt::from(\"\");\r\n    for i in 0..1000 {\r\n        use std::fmt::Write as _;\r\n        writeln!(&mut txt, \"Line {i}\").unwrap();\r\n    }\r\n    txt.end_mut();\r\n    Scroll! {\r\n        id = \"scroll\";\r\n        child = Text! {\r\n            id = \"focusable\";\r\n            zng::focus::focusable = true;\r\n            txt;\r\n        };\r\n        child_bottom = Button!(zng::scroll::cmd::SCROLL_TO_TOP_CMD.scoped(\"scroll\")), 10;\r\n    }\r\n}\r\n```\r\n\r\n#### Following these steps:\r\n\r\n1 - Run.\r\n2 - Scroll to end (press End).\r\n3 - Click button.\r\n\r\n#### I saw this happen:\r\n\r\nScrolls up and down again.\r\n\r\n#### I expected this to happen:\r\n\r\nScroll up only.\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex f2ac0ea86..08e16bca8 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -50,8 +50,10 @@ fn version_0_5() -> impl UiNode {\n         - Removed `FocusChangedCause::is_prev_request`.\n * Add `FocusChangedCause::request_target` helper method.\n * Add `WidgetPath::parent_id` helper method.\n-* Fix auto scroll-to-focused not working when the focused child does not subscribe to focus change events.\n-* Fix auto scroll-to-focused scrolling when large widget is already visible.\n+* Fix auto scroll to focused issues:\n+    - When the focused child does not subscribe to focus change events.\n+    - Scrolling when large widget is already visible.\n+    - Scrolling again to same widget when focus change event did not represent a widget change.\n * Add `WidgetInfo::spatial_bounds`.\n * Fix directional navigation cycling only inside viewport now full spatial bounds of scopes.\n * Add better conversions for `CommandScope`. You can now scope on named widgets directly, `FOO_CMD.scoped(\"bar-wgt\")`.\ndiff --git a/crates/zng-wgt-scroll/src/node.rs b/crates/zng-wgt-scroll/src/node.rs\nindex 7c04efe64..18565136c 100644\n--- a/crates/zng-wgt-scroll/src/node.rs\n+++ b/crates/zng-wgt-scroll/src/node.rs\n@@ -649,7 +649,12 @@ pub fn scroll_to_node(child: impl UiNode) -> impl UiNode {\n             let self_id = WIDGET.id();\n             if let Some(args) = FOCUS_CHANGED_EVENT.on(update) {\n                 if let Some(path) = &args.new_focus {\n-                    if (scroll_to.is_none() || !scroll_to_from_cmd) && path.contains(self_id) && path.widget_id() != self_id {\n+                    if (scroll_to.is_none() || !scroll_to_from_cmd)\n+                        && path.contains(self_id)\n+                        && path.widget_id() != self_id\n+                        && !args.is_enabled_change()\n+                        && !args.is_highlight_changed()\n+                    {\n                         // focus move inside.\n                         if let Some(mode) = SCROLL_TO_FOCUSED_MODE_VAR.get() {\n                             // scroll_to_focused enabled\n", "instance_id": "zng-ui__zng-172", "clarity": 2, "difficulty": 0.5, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the auto-scroll-to-focused functionality in a UI scroll component is overriding explicit scroll commands, causing unexpected behavior (scrolling up and down instead of just up). The provided code snippet and steps to reproduce the issue are helpful in understanding the context and the expected behavior. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the constraints or edge cases (e.g., under what conditions should auto-scroll be disabled or overridden). Additionally, there is no mention of the broader context of the UI framework or the specific requirements for when auto-scroll should or should not trigger. These missing details prevent it from being fully comprehensive, but the issue is still understandable with the given information.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of the code changes is relatively focused, primarily affecting a single file (`node.rs`) within the `zng-wgt-scroll` crate, and the diff shows a small, targeted modification to the logic for handling focus change events. However, the change requires understanding specific technical concepts, such as event handling in a UI framework, widget hierarchies, and the interaction between focus changes and scroll behavior. The modification involves adding conditions to prevent unnecessary auto-scrolling when focus events do not represent a widget change, which indicates a need to understand the `FocusChangedCause` API and its properties (`is_enabled_change` and `is_highlight_changed`). Additionally, while the problem statement does not explicitly mention edge cases, the code change implicitly addresses scenarios like repeated focus events or unchanged widget focus, which adds a layer of complexity to ensure robustness. The impact on the codebase architecture appears minimal, as it is a localized fix rather than a systemic change. Overall, this problem requires a moderate level of expertise in Rust and UI framework internals, along with careful consideration of event-driven logic, placing it at a difficulty score of 0.50.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`.eq_any([[\"foo\"]])` on `SqlType = Array<Text>` causes runtime error on Postgres\n## Setup\n\n### Versions\n\n- **Diesel:** ce6fa0bd93877391665254d9bdb23a9c895902ef\n- **Database:** Postgres\n\n### Feature Flags\n\n- **diesel:** postgres\n\n## Problem Description\n`array.eq_any(array_of_array)` doesn't work on Postgres, because Postgres doesn't have such a thing as \"array of array\". It has 2D array but [\"2-D array is a different concept\"](https://pgsql-general.postgresql.narkive.com/eJuaaIgQ/could-not-find-array-type-for-data-type-character-varying#post2).\nThis causes an error, so we should probably use the non-specialized ANSI `IN (...)` when dealing with arrays.\n\n### What are you trying to accomplish?\n```rust\ntable! {\n    posts (id) {\n        id -> Int4,\n        // ...\n        tags -> Array<Text>,\n    }\n}\n\n#[test]\n#[cfg(feature = \"postgres\")]\nfn filter_array_by_in() {\n    use crate::schema::posts::dsl::*;\n\n    let connection: &mut PgConnection = &mut connection();\n    let tag_combinations_to_look_for: &[&[&str]] = &[&[\"foo\"], &[\"foo\", \"bar\"], &[\"baz\"]];\n    let result: Vec<i32> = posts\n        .filter(tags.eq_any(tag_combinations_to_look_for))\n        .select(id)\n        .load(connection)\n        .unwrap();\n    assert_eq!(result, &[] as &[i32]);\n}\n```\n\n### What is the actual output?\n```\ncalled `Result::unwrap()` on an `Err` value: DatabaseError(Unknown, \"could not find array type for data type text[]\")\n```\n\n### What is the expected output?\nEither of:\n- test passes, serializing as `IN ($1, $2, $3)` with `$1 = [\"foo\"]`...\n- Doesn't compile, requiring an explicit writing (e.g. `.eq_any(`abc`).ansi()`) to tell it to use ANSI serialization\n\nATM AFAICT there's no way to even tell it to use ANSI serialization because of bounds here:\nhttps://github.com/diesel-rs/diesel/blob/ce6fa0bd93877391665254d9bdb23a9c895902ef/diesel/src/expression/array_comparison.rs#L100-L103\nwhich forbids the use of the ANSI implementation on `Pg` `Backend` (which seems weird since ANSI should always work, no?)\n\n(NB: All of this is useful only to the extent to which indexing will work and this behaves reasonably even if there are 500 different values in there. I recall that we did tend to prefer `= ANY` maybe because of this but I may be mistaken here, was it only for statement caching ?)\n\n### Steps to reproduce\nhttps://github.com/Ten0/diesel/commit/d6c93fd2a8af99df9bc8dc362144e6f599c6030c\n\nor dedicated working branch to fixing this:\n```\ncd diesel\ngit remote add Ten0 https://github.com/Ten0/diesel.git\ngit checkout array_eq_any\ncd diesel_tests\nDATABASE_URL=\"...\" cargo test --no-default-features --features \"postgres\" -- filter_array_by_in --nocapture\n```\n\n\n`.eq_any([[\"foo\"]])` on `SqlType = Array<Text>` causes runtime error on Postgres\n## Setup\n\n### Versions\n\n- **Diesel:** ce6fa0bd93877391665254d9bdb23a9c895902ef\n- **Database:** Postgres\n\n### Feature Flags\n\n- **diesel:** postgres\n\n## Problem Description\n`array.eq_any(array_of_array)` doesn't work on Postgres, because Postgres doesn't have such a thing as \"array of array\". It has 2D array but [\"2-D array is a different concept\"](https://pgsql-general.postgresql.narkive.com/eJuaaIgQ/could-not-find-array-type-for-data-type-character-varying#post2).\nThis causes an error, so we should probably use the non-specialized ANSI `IN (...)` when dealing with arrays.\n\n### What are you trying to accomplish?\n```rust\ntable! {\n    posts (id) {\n        id -> Int4,\n        // ...\n        tags -> Array<Text>,\n    }\n}\n\n#[test]\n#[cfg(feature = \"postgres\")]\nfn filter_array_by_in() {\n    use crate::schema::posts::dsl::*;\n\n    let connection: &mut PgConnection = &mut connection();\n    let tag_combinations_to_look_for: &[&[&str]] = &[&[\"foo\"], &[\"foo\", \"bar\"], &[\"baz\"]];\n    let result: Vec<i32> = posts\n        .filter(tags.eq_any(tag_combinations_to_look_for))\n        .select(id)\n        .load(connection)\n        .unwrap();\n    assert_eq!(result, &[] as &[i32]);\n}\n```\n\n### What is the actual output?\n```\ncalled `Result::unwrap()` on an `Err` value: DatabaseError(Unknown, \"could not find array type for data type text[]\")\n```\n\n### What is the expected output?\nEither of:\n- test passes, serializing as `IN ($1, $2, $3)` with `$1 = [\"foo\"]`...\n- Doesn't compile, requiring an explicit writing (e.g. `.eq_any(`abc`).ansi()`) to tell it to use ANSI serialization\n\nATM AFAICT there's no way to even tell it to use ANSI serialization because of bounds here:\nhttps://github.com/diesel-rs/diesel/blob/ce6fa0bd93877391665254d9bdb23a9c895902ef/diesel/src/expression/array_comparison.rs#L100-L103\nwhich forbids the use of the ANSI implementation on `Pg` `Backend` (which seems weird since ANSI should always work, no?)\n\n(NB: All of this is useful only to the extent to which indexing will work and this behaves reasonably even if there are 500 different values in there. I recall that we did tend to prefer `= ANY` maybe because of this but I may be mistaken here, was it only for statement caching ?)\n\n### Steps to reproduce\nhttps://github.com/Ten0/diesel/commit/d6c93fd2a8af99df9bc8dc362144e6f599c6030c\n\nor dedicated working branch to fixing this:\n```\ncd diesel\ngit remote add Ten0 https://github.com/Ten0/diesel.git\ngit checkout array_eq_any\ncd diesel_tests\nDATABASE_URL=\"...\" cargo test --no-default-features --features \"postgres\" -- filter_array_by_in --nocapture\n```\n\n\n", "patch": "diff --git a/diesel/src/expression/array_comparison.rs b/diesel/src/expression/array_comparison.rs\nindex a4b9461d0ba3..cdbb91aa832e 100644\n--- a/diesel/src/expression/array_comparison.rs\n+++ b/diesel/src/expression/array_comparison.rs\n@@ -27,7 +27,10 @@ use std::marker::PhantomData;\n /// `IN` expression.\n ///\n /// The postgres backend provided a specialized implementation\n-/// by using `left = ANY(values)` as optimized variant instead.\n+/// by using `left = ANY(values)` as optimized variant instead\n+/// if this is possible. For cases where this is not possible\n+/// like for example if values is a vector of arrays we\n+/// generate an ordinary `IN` expression instead.\n #[derive(Debug, Copy, Clone, QueryId, ValidGrouping)]\n #[non_exhaustive]\n pub struct In<T, U> {\n@@ -47,7 +50,10 @@ pub struct In<T, U> {\n /// `NOT IN` expression.0\n ///\n /// The postgres backend provided a specialized implementation\n-/// by using `left = ALL(values)` as optimized variant instead.\n+/// by using `left != ALL(values)` as optimized variant instead\n+/// if this is possible. For cases where this is not possible\n+/// like for example if values is a vector of arrays we\n+/// generate a ordinary `NOT IN` expression instead\n #[derive(Debug, Copy, Clone, QueryId, ValidGrouping)]\n #[non_exhaustive]\n pub struct NotIn<T, U> {\n@@ -61,12 +67,46 @@ impl<T, U> In<T, U> {\n     pub(crate) fn new(left: T, values: U) -> Self {\n         In { left, values }\n     }\n+\n+    pub(crate) fn walk_ansi_ast<'b, DB>(&'b self, mut out: AstPass<'_, 'b, DB>) -> QueryResult<()>\n+    where\n+        DB: Backend,\n+        T: QueryFragment<DB>,\n+        U: QueryFragment<DB> + InExpression,\n+    {\n+        if self.values.is_empty() {\n+            out.push_sql(\"1=0\");\n+        } else {\n+            self.left.walk_ast(out.reborrow())?;\n+            out.push_sql(\" IN (\");\n+            self.values.walk_ast(out.reborrow())?;\n+            out.push_sql(\")\");\n+        }\n+        Ok(())\n+    }\n }\n \n impl<T, U> NotIn<T, U> {\n     pub(crate) fn new(left: T, values: U) -> Self {\n         NotIn { left, values }\n     }\n+\n+    pub(crate) fn walk_ansi_ast<'b, DB>(&'b self, mut out: AstPass<'_, 'b, DB>) -> QueryResult<()>\n+    where\n+        DB: Backend,\n+        T: QueryFragment<DB>,\n+        U: QueryFragment<DB> + InExpression,\n+    {\n+        if self.values.is_empty() {\n+            out.push_sql(\"1=1\");\n+        } else {\n+            self.left.walk_ast(out.reborrow())?;\n+            out.push_sql(\" NOT IN (\");\n+            self.values.walk_ast(out.reborrow())?;\n+            out.push_sql(\")\");\n+        }\n+        Ok(())\n+    }\n }\n \n impl<T, U> Expression for In<T, U>\n@@ -114,16 +154,8 @@ where\n     T: QueryFragment<DB>,\n     U: QueryFragment<DB> + InExpression,\n {\n-    fn walk_ast<'b>(&'b self, mut out: AstPass<'_, 'b, DB>) -> QueryResult<()> {\n-        if self.values.is_empty() {\n-            out.push_sql(\"1=0\");\n-        } else {\n-            self.left.walk_ast(out.reborrow())?;\n-            out.push_sql(\" IN (\");\n-            self.values.walk_ast(out.reborrow())?;\n-            out.push_sql(\")\");\n-        }\n-        Ok(())\n+    fn walk_ast<'b>(&'b self, out: AstPass<'_, 'b, DB>) -> QueryResult<()> {\n+        self.walk_ansi_ast(out)\n     }\n }\n \n@@ -145,16 +177,8 @@ where\n     T: QueryFragment<DB>,\n     U: QueryFragment<DB> + InExpression,\n {\n-    fn walk_ast<'b>(&'b self, mut out: AstPass<'_, 'b, DB>) -> QueryResult<()> {\n-        if self.values.is_empty() {\n-            out.push_sql(\"1=1\");\n-        } else {\n-            self.left.walk_ast(out.reborrow())?;\n-            out.push_sql(\" NOT IN (\");\n-            self.values.walk_ast(out.reborrow())?;\n-            out.push_sql(\")\");\n-        }\n-        Ok(())\n+    fn walk_ast<'b>(&'b self, out: AstPass<'_, 'b, DB>) -> QueryResult<()> {\n+        self.walk_ansi_ast(out)\n     }\n }\n \n@@ -217,6 +241,10 @@ pub trait InExpression {\n     /// Returns `true` if self represents an empty collection\n     /// Otherwise `false` is returned.\n     fn is_empty(&self) -> bool;\n+\n+    /// Returns `true` if the values clause represents\n+    /// bind values and each bind value is a postgres array type\n+    fn is_array(&self) -> bool;\n }\n \n impl<ST, F, S, D, W, O, LOf, G, H, LC> AsInExpression<ST>\n@@ -306,6 +334,10 @@ where\n     fn is_empty(&self) -> bool {\n         self.values.is_empty()\n     }\n+\n+    fn is_array(&self) -> bool {\n+        ST::IS_ARRAY\n+    }\n }\n \n impl<ST, I, QS> SelectableExpression<QS> for Many<ST, I>\n@@ -345,7 +377,18 @@ where\n     ST: SingleValue,\n     I: ToSql<ST, DB>,\n {\n-    fn walk_ast<'b>(&'b self, mut out: AstPass<'_, 'b, DB>) -> QueryResult<()> {\n+    fn walk_ast<'b>(&'b self, out: AstPass<'_, 'b, DB>) -> QueryResult<()> {\n+        self.walk_ansi_ast(out)\n+    }\n+}\n+\n+impl<ST, I> Many<ST, I> {\n+    pub(crate) fn walk_ansi_ast<'b, DB>(&'b self, mut out: AstPass<'_, 'b, DB>) -> QueryResult<()>\n+    where\n+        DB: Backend + HasSqlType<ST>,\n+        ST: SingleValue,\n+        I: ToSql<ST, DB>,\n+    {\n         out.unsafe_to_cache_prepared();\n         let mut first = true;\n         for value in &self.values {\ndiff --git a/diesel/src/expression/subselect.rs b/diesel/src/expression/subselect.rs\nindex 25c98ea0c5a1..de30ca43040d 100644\n--- a/diesel/src/expression/subselect.rs\n+++ b/diesel/src/expression/subselect.rs\n@@ -42,6 +42,9 @@ impl<T, ST: SqlType> InExpression for Subselect<T, ST> {\n     fn is_empty(&self) -> bool {\n         false\n     }\n+    fn is_array(&self) -> bool {\n+        false\n+    }\n }\n \n impl<T, ST, QS> SelectableExpression<QS> for Subselect<T, ST>\ndiff --git a/diesel/src/expression_methods/global_expression_methods.rs b/diesel/src/expression_methods/global_expression_methods.rs\nindex 893867a0bf8f..6a0832d317b6 100644\n--- a/diesel/src/expression_methods/global_expression_methods.rs\n+++ b/diesel/src/expression_methods/global_expression_methods.rs\n@@ -107,7 +107,9 @@ pub trait ExpressionMethods: Expression + Sized {\n     /// query will use the cache (assuming the subquery\n     /// itself is safe to cache).\n     /// On PostgreSQL, this method automatically performs a `= ANY()`\n-    /// query.\n+    /// query if this is possible. For cases where this is not possible\n+    /// like for example if values is a vector of arrays we\n+    /// generate an ordinary `IN` expression instead.\n     ///\n     /// # Example\n     ///\n@@ -149,7 +151,10 @@ pub trait ExpressionMethods: Expression + Sized {\n     ///\n     /// Queries using this method will not be\n     /// placed in the prepared statement cache. On PostgreSQL, this\n-    /// method automatically performs a `!= ALL()` query.\n+    /// method automatically performs a `!= ALL()` query if this is possible.\n+    /// For cases where this is not possible\n+    /// like for example if values is a vector of arrays we\n+    /// generate an ordinary `NOT IN` expression instead.\n     ///\n     /// # Example\n     ///\ndiff --git a/diesel/src/pg/expression/array.rs b/diesel/src/pg/expression/array.rs\nindex 7e08c2c929bd..e9bea41984d7 100644\n--- a/diesel/src/pg/expression/array.rs\n+++ b/diesel/src/pg/expression/array.rs\n@@ -178,9 +178,15 @@ where\n     ST: SqlType,\n {\n     type SqlType = ST;\n+\n     fn is_empty(&self) -> bool {\n         false\n     }\n+\n+    fn is_array(&self) -> bool {\n+        // we want to use the `= ANY(_)` syntax\n+        false\n+    }\n }\n \n impl<T, ST> AsInExpression<ST> for ArrayLiteral<T, ST>\n@@ -189,6 +195,7 @@ where\n     ST: SqlType,\n {\n     type InExpression = Self;\n+\n     fn as_in_expression(self) -> Self::InExpression {\n         self\n     }\n@@ -296,9 +303,15 @@ where\n     ST: SqlType,\n {\n     type SqlType = ST;\n+\n     fn is_empty(&self) -> bool {\n         false\n     }\n+\n+    fn is_array(&self) -> bool {\n+        // we want to use the `= ANY(_)` syntax\n+        false\n+    }\n }\n \n impl<T, ST> AsInExpression<ST> for ArraySubselect<T, ST>\n@@ -307,6 +320,7 @@ where\n     ST: SqlType,\n {\n     type InExpression = Self;\n+\n     fn as_in_expression(self) -> Self::InExpression {\n         self\n     }\ndiff --git a/diesel/src/pg/query_builder/query_fragment_impls.rs b/diesel/src/pg/query_builder/query_fragment_impls.rs\nindex 224977569fab..bfc44f18f748 100644\n--- a/diesel/src/pg/query_builder/query_fragment_impls.rs\n+++ b/diesel/src/pg/query_builder/query_fragment_impls.rs\n@@ -66,10 +66,14 @@ where\n     U: QueryFragment<Pg> + InExpression,\n {\n     fn walk_ast<'b>(&'b self, mut out: AstPass<'_, 'b, Pg>) -> QueryResult<()> {\n-        self.left.walk_ast(out.reborrow())?;\n-        out.push_sql(\" = ANY(\");\n-        self.values.walk_ast(out.reborrow())?;\n-        out.push_sql(\")\");\n+        if self.values.is_array() {\n+            self.walk_ansi_ast(out)?;\n+        } else {\n+            self.left.walk_ast(out.reborrow())?;\n+            out.push_sql(\" = ANY(\");\n+            self.values.walk_ast(out.reborrow())?;\n+            out.push_sql(\")\");\n+        }\n         Ok(())\n     }\n }\n@@ -80,10 +84,14 @@ where\n     U: QueryFragment<Pg> + InExpression,\n {\n     fn walk_ast<'b>(&'b self, mut out: AstPass<'_, 'b, Pg>) -> QueryResult<()> {\n-        self.left.walk_ast(out.reborrow())?;\n-        out.push_sql(\" != ALL(\");\n-        self.values.walk_ast(out.reborrow())?;\n-        out.push_sql(\")\");\n+        if self.values.is_array() {\n+            self.walk_ansi_ast(out)?;\n+        } else {\n+            self.left.walk_ast(out.reborrow())?;\n+            out.push_sql(\" != ALL(\");\n+            self.values.walk_ast(out.reborrow())?;\n+            out.push_sql(\")\");\n+        }\n         Ok(())\n     }\n }\n@@ -92,10 +100,15 @@ impl<ST, I> QueryFragment<Pg, PgStyleArrayComparison> for Many<ST, I>\n where\n     ST: SingleValue,\n     Vec<I>: ToSql<Array<ST>, Pg>,\n+    I: ToSql<ST, Pg>,\n     Pg: HasSqlType<ST>,\n {\n     fn walk_ast<'b>(&'b self, mut out: AstPass<'_, 'b, Pg>) -> QueryResult<()> {\n-        out.push_bind_param::<Array<ST>, Vec<I>>(&self.values)\n+        if ST::IS_ARRAY {\n+            self.walk_ansi_ast(out)\n+        } else {\n+            out.push_bind_param::<Array<ST>, Vec<I>>(&self.values)\n+        }\n     }\n }\n \ndiff --git a/diesel/src/sql_types/mod.rs b/diesel/src/sql_types/mod.rs\nindex 1aece629c942..84e330adb75f 100644\n--- a/diesel/src/sql_types/mod.rs\n+++ b/diesel/src/sql_types/mod.rs\n@@ -676,6 +676,9 @@ pub trait SqlType: 'static {\n     ///\n     /// ['is_nullable`]: is_nullable\n     type IsNull: OneIsNullable<is_nullable::IsNullable> + OneIsNullable<is_nullable::NotNull>;\n+\n+    #[doc(hidden)]\n+    const IS_ARRAY: bool = false;\n }\n \n /// Is one value of `IsNull` nullable?\ndiff --git a/diesel_derives/src/sql_type.rs b/diesel_derives/src/sql_type.rs\nindex 2e6f00d65464..799f703ef1a0 100644\n--- a/diesel_derives/src/sql_type.rs\n+++ b/diesel_derives/src/sql_type.rs\n@@ -11,18 +11,23 @@ pub fn derive(item: DeriveInput) -> Result<TokenStream> {\n     let model = Model::from_item(&item, true, false)?;\n \n     let struct_name = &item.ident;\n+    let generic_count = item.generics.params.len();\n     let (impl_generics, ty_generics, where_clause) = item.generics.split_for_impl();\n \n     let sqlite_tokens = sqlite_tokens(&item, &model);\n     let mysql_tokens = mysql_tokens(&item, &model);\n     let pg_tokens = pg_tokens(&item, &model);\n \n+    let is_array = struct_name == \"Array\" && generic_count == 1;\n+\n     Ok(wrap_in_dummy_mod(quote! {\n         impl #impl_generics diesel::sql_types::SqlType\n             for #struct_name #ty_generics\n         #where_clause\n         {\n             type IsNull = diesel::sql_types::is_nullable::NotNull;\n+\n+            const IS_ARRAY: bool = #is_array;\n         }\n \n         impl #impl_generics diesel::sql_types::SingleValue\n", "instance_id": "diesel-rs__diesel-4350", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `eq_any` method fails with a runtime error on Postgres when dealing with arrays of arrays due to Postgres not supporting such a data type directly. The goal is to either make the query work by serializing it as an ANSI `IN` expression or to fail at compile time with a clear requirement for explicit ANSI serialization. The statement includes relevant code snippets, expected outputs, actual outputs, and steps to reproduce, which are helpful. However, there are minor ambiguities and missing details. For instance, the performance implications of switching to ANSI `IN` for large datasets are mentioned but not fully explored (e.g., the note about indexing and statement caching is vague). Additionally, edge cases such as empty arrays or very large input sets are not explicitly addressed in the problem description, which could impact the solution design. Overall, while the problem is valid and mostly clear, these minor gaps prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is moderate but impactful, affecting multiple files in the Diesel ORM library, a complex Rust-based database abstraction layer. The changes involve modifying core query-building logic (`array_comparison.rs`, `query_fragment_impls.rs`) and type system behaviors (`sql_types/mod.rs`), requiring a deep understanding of Diesel's internals and its interaction with Postgres. Second, the technical concepts involved are non-trivial: the developer must understand Rust's type system, trait implementations, SQL query generation, and Postgres-specific behaviors regarding arrays and query optimization (`= ANY` vs. `IN`). Third, the problem requires handling specific edge cases, such as empty arrays (already addressed in the code changes) and determining when to fall back to ANSI `IN` based on type information (e.g., detecting arrays of arrays). Finally, the solution impacts query performance and compatibility, necessitating careful consideration of trade-offs (e.g., indexing and statement caching mentioned in the problem statement). While not at the extreme end of difficulty (e.g., redesigning a distributed system), this problem demands significant expertise in Rust, database systems, and Diesel's architecture, justifying a score of 0.65.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[docs] `calyx-pass-explorer` docs have broken images\nAlthough locally images were rendered by `mdbook serve`, it appears that the built documentation does not include the images. After looking at pages that do have images, `<img>` is used instead of the markdown syntax `![]()`, which might solve the issue.\n", "patch": "diff --git a/docs/compiler.md b/docs/compiler.md\nindex 088646d417..64b35aaac8 100644\n--- a/docs/compiler.md\n+++ b/docs/compiler.md\n@@ -33,6 +33,7 @@ For example, the alias `all` is an ordered sequence of default passes executed\n when the compiler is run from the command-line.\n \n The command-line provides two options to control the execution of passes:\n+\n - `-p, --pass`: Execute this pass or alias. Overrides default alias.\n - `-d, --disable-pass`: Disable this pass or alias. Takes priority over `-p`.\n \n@@ -43,6 +44,10 @@ the default execution alias `all`:\n cargo run -- examples/futil/simple.futil -p all -d static-timing\n ```\n \n+If you want to work with passes interactively (for instance, you only care about\n+a pass far into the `all` sequence, and it is impractical to pass 20 `-p`\n+options), you can [visualize them](./dev/calyx-pass-explorer.md) with the `calyx-pass-explorer` tool.\n+\n ## Providing Pass Options\n \n Some passes take options to control their behavior. The `--list-passes` command prints out the options for each pass. For example, the `tdcc` pass has the following options:\n@@ -53,23 +58,25 @@ tdcc: <description>\n ```\n \n The option allows us to change the behavior of the pass. To provide a pass-specific option, we use the `-x` switch:\n+\n ```\n cargo run -- examples/futil/simple.futil -p tdcc -x tdcc:dump-fsm\n ```\n \n Note that we specify the option of `tdcc` by prefixing it with the pass name and a colon.\n \n-\n ## Specifying Primitives Library\n \n The compiler implementation uses a standard library of components to compile\n programs.\n The only standard library for the compiler is located in:\n+\n ```\n <path to Calyx repository>/primitives\n ```\n \n Specify the location of the library using the `-l` flag:\n+\n ```\n cargo run -- -l ./primitives\n ```\ndiff --git a/docs/dev/calyx-pass-explorer.md b/docs/dev/calyx-pass-explorer.md\nindex 8ff04e6016..a71112858f 100644\n--- a/docs/dev/calyx-pass-explorer.md\n+++ b/docs/dev/calyx-pass-explorer.md\n@@ -9,6 +9,7 @@ is just to run it on code and see what happens.\n Enter [`calyx-pass-explorer`](https://github.com/calyxir/calyx/tree/main/tools/calyx-pass-explorer).\n It's a command line tool that provides an interactive interface for visualizing\n how different passes affect the source code.\n+It's been used to debug and develop new compiler passes as well as implement new features in the compiler, so I hope you can find it useful too!\n \n > ![Example running of the tool](https://raw.githubusercontent.com/calyxir/calyx/main/tools/calyx-pass-explorer/example_v0.0.0.png)\n > _The above image depicts the tool's interface in v0.0.0.\n@@ -67,11 +68,11 @@ tool to help develop it!\n We'll first run `calyx-pass-explorer example0.futil`.\n You should get something horrific like\n \n-![Lots of random text output that doesn't make sense](./assets/horrific-interface.png)\n+![Lots of random text output that doesn't make sense](assets/horrific-interface.png)\n \n > [!TIP]\n > If you get this message:\n-> ![Calyx executable could not be found](./assets/calyx-missing.png)\n+> ![Calyx executable could not be found](assets/calyx-missing.png)\n > You should setup `fud` or pass the path explicitly with `-e`, as suggested.\n > However, we're going to update this later to look at `fud2` as well because\n > `fud` is now officially deprecated.\n@@ -82,7 +83,7 @@ What we really want is to focus on what happens to, _e.g._, the `main` component\n To do that, we just pass `-c main` (or `--component main`) as a flag:\n \n ![Running the tool and visualizing how the well-formed pass affects the main\n-component](./assets/well-formed.png)\n+component](assets/well-formed.png)\n \n That's a lot better, but it's still quite a bit of information.\n Let's break it down.\n", "instance_id": "calyxir__calyx-2361", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: broken images in the documentation for `calyx-pass-explorer` when built, despite rendering correctly locally with `mdbook serve`. It suggests a potential solution of using `<img>` tags instead of markdown syntax `![]()`. However, there are minor ambiguities and missing details. For instance, it does not explicitly confirm whether the issue is due to the markdown syntax or another underlying cause (e.g., file paths or build process). Additionally, there are no specific instructions on where or how to test the changes after implementation, nor are there mentions of potential side effects or constraints (e.g., compatibility with different documentation tools or versions). Despite these minor gaps, the goal is understandable, and the provided code changes align with the suggested fix, making the problem statement mostly clear.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves straightforward modifications to documentation files. The code changes are limited to a few markdown files (`compiler.md` and `calyx-pass-explorer.md`) and consist of simple syntax updates\u2014replacing markdown image syntax `![]()` with relative paths to assets. The scope of changes is minimal, affecting only documentation and not the core codebase or system architecture. No deep understanding of programming concepts, algorithms, or domain-specific knowledge is required beyond basic familiarity with markdown and documentation rendering. There are no significant edge cases or error handling considerations mentioned or implied in the problem or changes, as this is purely a presentation issue. The task requires minimal effort and technical expertise, akin to fixing a typo or updating static content, hence the low difficulty score of 0.15.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "web-wallet: Update Wallet Access Terminology\n### **Description**\r\nIn the Web Wallet, update the terminology to align with standard web3 terminology. Specifically:\r\n- Replace **Access Wallet / Log Out** with **Unlock Wallet / Lock Wallet**.\r\n\r\nThis change will enhance consistency and clarity for users familiar with web3 conventions.\r\n\r\n### **Tasks**\r\n- Replace \"Access Wallet\" with \"Unlock Wallet\" on the landing screen.\r\n- Replace \"Log Out\" with \"Lock Wallet\" in the Settings menu.\r\n- Ensure the changes are reflected throughout the UI and any related documentation.\r\n- Verify the functionality remains consistent after the text updates.\r\n\r\n### **Additional Context**\r\nThis update is intended to align the terminology with common web3 practices and improve user experience.\r\n\r\n___\r\nRaised by @Neotamandua.\n", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex 04074fd004..a39ff4c25a 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -13,6 +13,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n \n - Update Transactions list design [#1922]\n - Ensure connected wallet is disconnected when closing the app [#3269]\n+- Update Wallet Access terminology [#3356]\n \n ### Removed\n \n@@ -516,11 +517,12 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3259]: https://github.com/dusk-network/rusk/issues/3259\n [#3265]: https://github.com/dusk-network/rusk/issues/3265\n [#3267]: https://github.com/dusk-network/rusk/issues/3267\n-[#3270]: https://github.com/dusk-network/rusk/issues/3269\n+[#3269]: https://github.com/dusk-network/rusk/issues/3269\n [#3270]: https://github.com/dusk-network/rusk/issues/3270\n [#3287]: https://github.com/dusk-network/rusk/issues/3287\n [#3333]: https://github.com/dusk-network/rusk/issues/3333\n [#3339]: https://github.com/dusk-network/rusk/issues/3339\n+[#3356]: https://github.com/dusk-network/rusk/issues/3356\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/routes/(app)/settings/+page.svelte b/web-wallet/src/routes/(app)/settings/+page.svelte\nindex bfe6f698c6..d4972314c1 100644\n--- a/web-wallet/src/routes/(app)/settings/+page.svelte\n+++ b/web-wallet/src/routes/(app)/settings/+page.svelte\n@@ -218,7 +218,11 @@\n     icon={{ path: mdiArrowLeft }}\n     text=\"Back\"\n   />\n-  <Button on:click={() => logout(false)} variant=\"tertiary\" text=\"Log out\" />\n+  <Button\n+    on:click={() => logout(false)}\n+    variant=\"tertiary\"\n+    text=\"Lock Wallet\"\n+  />\n </div>\n \n <style lang=\"postcss\">\ndiff --git a/web-wallet/src/routes/(welcome)/setup/+page.svelte b/web-wallet/src/routes/(welcome)/setup/+page.svelte\nindex 3b800694e4..2274489a2a 100644\n--- a/web-wallet/src/routes/(welcome)/setup/+page.svelte\n+++ b/web-wallet/src/routes/(welcome)/setup/+page.svelte\n@@ -30,8 +30,8 @@\n       </div>\n       <Animation />\n       <AppAnchorButton\n-        href=\"/login\"\n-        text=\"Access wallet\"\n+        href=\"/unlock\"\n+        text=\"Unlock Wallet\"\n         icon={{ path: mdiKeyOutline }}\n       />\n     </div>\ndiff --git a/web-wallet/src/routes/(welcome)/login/+page.svelte b/web-wallet/src/routes/(welcome)/unlock/+page.svelte\nsimilarity index 100%\nrename from web-wallet/src/routes/(welcome)/login/+page.svelte\nrename to web-wallet/src/routes/(welcome)/unlock/+page.svelte\n", "instance_id": "dusk-network__rusk-3357", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and clear. The goal is explicitly defined as updating terminology in the Web Wallet to align with web3 conventions by replacing \"Access Wallet / Log Out\" with \"Unlock Wallet / Lock Wallet.\" The tasks are specific, detailing where the changes need to be made (landing screen and settings menu) and what needs to be verified (functionality consistency). Additional context is provided to explain the purpose of the update, enhancing user experience through terminology alignment. There are no significant ambiguities or missing critical details in the description. The problem statement also includes a note to ensure changes are reflected in UI and documentation, which adds to the completeness of the requirements.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range. The task involves simple text replacements in specific UI components, as seen in the code changes, which are limited to a few files (settings and setup pages in Svelte, along with a file rename from \"login\" to \"unlock\"). The scope of the changes is minimal, affecting only specific UI elements without requiring deep understanding of the codebase or its architecture. No complex logic, algorithms, or technical concepts are involved beyond basic familiarity with Svelte or front-end development for updating text and renaming paths. There are no edge cases or error handling requirements mentioned or implied in the problem statement or code changes. The impact on the system is negligible, as it is purely a cosmetic update. The only minor additional effort might be ensuring consistency in documentation, but this does not significantly increase the difficulty. Overall, this is a straightforward task suitable for a junior developer or someone with basic familiarity with the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "web-wallet: Improve Address Display in Address Picker to Include Moonlight Addresses\n**Description:**  \r\nCurrently, the Address Picker displays only the Phoenix address by default. With the recent introduction of Moonlight, which will likely be used more frequently, it may no longer be sensible to display only the Phoenix address. We need to revisit the approach for the Address Picker to better accommodate both Phoenix and Moonlight addresses.\r\n\r\n**Potential Approaches:**\r\n\r\n1. **Hide Address Picker Temporarily:**  \r\n   As a short-term measure, hide the Address Picker altogether.  \r\n   - Addresses (Phoenix and Moonlight) can be accessed through the Receive view.\r\n   - This ensures users can still retrieve their addresses with minimal confusion.\r\n\r\n2. **Show Both Phoenix and Moonlight Addresses:**  \r\n   Adjust the Address Picker to display both Phoenix and Moonlight addresses simultaneously.  \r\n   - Users can quickly choose which address to copy.\r\n   - Ensures immediate visibility and reduces friction when switching between address types.\r\n\r\n**Acceptance Criteria:**\r\n\r\n- [ ] Decide on the approach: hide the picker or display both addresses.\r\n- [ ] If hiding the picker, ensure the Receive view is clearly accessible and well-documented.\r\n- [ ] If displaying both addresses, ensure they are clearly labeled and user-friendly.\r\n- [ ] Update documentation, tooltips, and UI guides to reflect the chosen approach.\r\n\r\n**Additional Notes:**  \r\nIf we opt for the temporary solution (hiding the picker), consider planning a follow-up improvement to reintroduce the picker with both addresses displayed. This will prevent relying on a hidden state as a permanent workaround.\n", "patch": "diff --git a/web-wallet/CHANGELOG.md b/web-wallet/CHANGELOG.md\nindex 09b7fb56b2..896c01ca4f 100644\n--- a/web-wallet/CHANGELOG.md\n+++ b/web-wallet/CHANGELOG.md\n@@ -19,6 +19,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n - Reword \"Staking\" header to \"Stake\" [#3113]\n - Upgrade Migration Feature to Use Reown AppKit [#3129]\n - Update default gas settings values [#3178]\n+- Update AddressPicker to show Default Profile label [#3179]\n \n ### Removed\n \n@@ -440,6 +441,7 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n [#3156]: https://github.com/dusk-network/rusk/issues/3156\n [#3160]: https://github.com/dusk-network/rusk/issues/3160\n [#3178]: https://github.com/dusk-network/rusk/issues/3178\n+[#3179]: https://github.com/dusk-network/rusk/issues/3179\n \n <!-- VERSIONS -->\n \ndiff --git a/web-wallet/src/lib/components/AddressPicker/AddressPicker.svelte b/web-wallet/src/lib/components/AddressPicker/AddressPicker.svelte\nindex 901240841d..cdef2b1d6a 100644\n--- a/web-wallet/src/lib/components/AddressPicker/AddressPicker.svelte\n+++ b/web-wallet/src/lib/components/AddressPicker/AddressPicker.svelte\n@@ -1,18 +1,9 @@\n <svelte:options immutable={true} />\n \n <script>\n-  import { createEventDispatcher, onMount } from \"svelte\";\n-  import {\n-    calculateAdaptiveCharCount,\n-    makeClassName,\n-    middleEllipsis,\n-  } from \"$lib/dusk/string\";\n-  import {\n-    mdiContentCopy,\n-    mdiPlusBoxOutline,\n-    mdiSwapHorizontal,\n-    mdiTimerSand,\n-  } from \"@mdi/js\";\n+  import { createEventDispatcher } from \"svelte\";\n+  import { makeClassName } from \"$lib/dusk/string\";\n+  import { mdiContentCopy, mdiPlusBoxOutline, mdiTimerSand } from \"@mdi/js\";\n   import { Button, Icon, ProgressBar } from \"$lib/dusk/components\";\n   import { toast } from \"$lib/dusk/components/Toast/store\";\n   import { handlePageClick } from \"$lib/dusk/ui-helpers/handlePageClick\";\n@@ -42,9 +33,6 @@\n   /** @type {HTMLMenuElement} */\n   let addressOptionsMenu;\n \n-  /** @type {number} */\n-  let screenWidth = window.innerWidth;\n-\n   function closeDropDown() {\n     expanded = false;\n   }\n@@ -65,18 +53,6 @@\n     toast(\"success\", \"Address copied\", mdiContentCopy);\n   }\n \n-  onMount(() => {\n-    const resizeObserver = new ResizeObserver((entries) => {\n-      const entry = entries[0];\n-\n-      screenWidth = entry.contentRect.width;\n-    });\n-\n-    resizeObserver.observe(document.body);\n-\n-    return () => resizeObserver.disconnect();\n-  });\n-\n   // Scrolls the address options menu to top on addresses change\n   $: if (profiles && addressOptionsMenu) {\n     addressOptionsMenu.scrollTo(0, 0);\n@@ -101,18 +77,7 @@\n     aria-expanded={expanded}\n     on:keydown={handleDropDownKeyDown}\n   >\n-    <Button disabled variant=\"secondary\" icon={{ path: mdiSwapHorizontal }} />\n-\n-    <p class=\"address-picker__current-address\">\n-      {middleEllipsis(currentAddress, calculateAdaptiveCharCount(screenWidth))}\n-    </p>\n-    <Button\n-      aria-label=\"Copy Address\"\n-      className=\"address-picker__copy-address-button\"\n-      icon={{ path: mdiContentCopy }}\n-      on:click={copyCurrentAddress}\n-      variant=\"secondary\"\n-    />\n+    <span>Default Profile</span>\n   </div>\n \n   {#if expanded}\n", "instance_id": "dusk-network__rusk-3208", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in defining the goal of improving the Address Picker to accommodate both Phoenix and Moonlight addresses. It provides two potential approaches (hiding the picker or displaying both addresses) and includes acceptance criteria, which adds to the clarity. However, there are minor ambiguities and missing details. For instance, the problem does not specify how the UI should handle the display of both addresses (e.g., layout, prioritization, or user interaction flow) if that approach is chosen. Additionally, there is no mention of specific edge cases, such as what happens if one type of address is unavailable, or how to handle localization for labels. Constraints or performance considerations for the UI update are also absent. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the relatively straightforward nature of the required changes. Based on the provided code changes, the solution appears to lean toward a simplified approach (possibly a temporary one), as the Address Picker's functionality is reduced by removing dynamic address display logic and replacing it with a static \"Default Profile\" label. The scope of the code changes is limited to a single file (AddressPicker.svelte), involving the removal of logic for address ellipsis, screen width observation, and button interactions, which indicates a low impact on the overall codebase. The technical concepts involved are basic, primarily revolving around Svelte framework syntax, UI component modification, and minimal state management. No complex algorithms, design patterns, or domain-specific knowledge are required. Edge cases and error handling do not seem to be a significant concern in the current changes, as the modification simplifies the component rather than adding complexity. Overall, this task requires understanding some UI logic and making simple modifications, fitting the lower end of the difficulty spectrum.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "bug: order by literal with limit return error\n### Search before asking\r\n\r\n- [X] I had searched in the [issues](https://github.com/databendlabs/databend/issues) and found no similar issues.\r\n\r\n\r\n### Version\r\n\r\nv1.2.662\r\n\r\n\r\n\r\n### What's Wrong?\r\n\r\norder by literal with limit return error\r\n\r\nwe can ignore order by item if it is a scalar value\r\n\r\n### How to Reproduce?\r\n\r\n```sql\r\nMySQL [(none)]> create table tt(a int, b int);\r\nQuery OK, 0 rows affected (0.187 sec)\r\n\r\nMySQL [(none)]> insert into tt values(1, 2),(3, 4);\r\nQuery OK, 2 rows affected (0.209 sec)\r\n\r\nMySQL [(none)]> select eq(tt.a, b) FROM tt ORDER BY 'a' LIMIT 89, 7;\r\nERROR 1105 (HY000): BadArguments. Code: 1006, Text = Unable to get field named \"0\". Valid fields: [\"4\"].\r\n```\r\n\r\n### Are you willing to submit PR?\r\n\r\n- [ ] Yes I am willing to submit a PR!\n", "patch": "diff --git a/src/query/sql/src/planner/binder/sort.rs b/src/query/sql/src/planner/binder/sort.rs\nindex 8af8d17a89cf8..a036e42939b07 100644\n--- a/src/query/sql/src/planner/binder/sort.rs\n+++ b/src/query/sql/src/planner/binder/sort.rs\n@@ -148,6 +148,10 @@ impl Binder {\n                             )\n                             .map_err(|e| ErrorCode::SemanticError(e.message()))?;\n \n+                        if let ScalarExpr::ConstantExpr(..) = rewrite_scalar {\n+                            continue;\n+                        }\n+\n                         let column_binding =\n                             if let ScalarExpr::BoundColumnRef(col) = &rewrite_scalar {\n                                 col.column.clone()\n", "instance_id": "databendlabs__databend-16967", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: an error occurs when using \"ORDER BY\" with a literal value and a \"LIMIT\" clause in a SQL query. It provides a reproducible example with SQL queries, which helps in understanding the bug. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior beyond \"ignoring order by item if it is a scalar value,\" nor does it specify any edge cases or constraints (e.g., what types of literals should be ignored, or how this should interact with other query components). Additionally, there is no mention of performance implications or compatibility concerns with other parts of the system. Despite these minor gaps, the core issue is understandable with the provided context and example, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows a small, localized change in a single file (`sort.rs`) within the `Binder` implementation. The modification involves adding a simple conditional check to skip processing of constant expressions (literals) in the `ORDER BY` clause. This change does not impact the broader architecture or require modifications across multiple modules, indicating a low scope of impact.\n\n2. **Technical Concepts Involved:** Solving this issue requires a basic understanding of the SQL query planner and binder logic, specifically how expressions are rewritten and processed in the context of sorting. Familiarity with Rust (the language used in the codebase) and pattern matching (e.g., `if let ScalarExpr::ConstantExpr(..)`) is necessary, but these are relatively straightforward concepts for an experienced developer. No advanced algorithms, design patterns, or domain-specific knowledge beyond SQL processing are required.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention specific edge cases, but the code change implies handling of constant expressions in `ORDER BY`. Potential edge cases (e.g., nested expressions, mixed literal and non-literal `ORDER BY` items) are not addressed in the diff, and the modification does not introduce new error handling logic. The simplicity of the fix suggests that edge cases are either minimal or not considered critical in this context.\n\n4. **Overall Complexity:** The fix is a minor addition of a conditional check, requiring minimal understanding of the surrounding codebase. While some familiarity with the query planner's internals is needed to ensure the change is correct, this does not elevate the difficulty significantly.\n\nGiven these points, a difficulty score of 0.30 reflects an \"Easy\" problem that involves a simple bug fix with limited scope, basic technical concepts, and minimal complexity in handling edge cases or system-wide impacts.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Index out of bounds error in `src/popups/blame_file.rs, line: 622, col: 38`\n**Describe the bug**\r\nAn index out of bounds error occurs when navigating specific file and then blaming it by pressing `shift` + `B`.\r\n\r\n**Steps To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Clone `https://codeberg.org/BrunoWallner/apoxel`\r\n2. Go to commit ID: `312629cb98556f883533fe65faea2387eff2e8ff` (`git checkout 312629cb98556f883533fe65faea2387eff2e8ff`)\r\n3. Run gitui inside the cloned repo\r\n4. Press `tab` to go to \"Files\" header and navigate to `apoxel/path/shader/stage_1/chunks.wgsl`\r\n5. press `shift` + `B` keys\r\n\r\n**Expected behavior**\r\nThe Blame page should appear but instead the application crashes.\r\n\r\n**Error output**\r\n```\r\nPanicInfo { payload: Any { .. }, message: Some(index out of bounds: the len is 339 but the index is 339), location: Location { file: \"src/popups/blame_file.rs\", line: 622, col: 38 }, can_unwind: true, force_no_backtrace: false }\r\ntrace:\r\n\r\n```\r\n\r\n**Context**\r\n - OS: Linux archlinux 6.7.8-arch1-1 #1 SMP PREEMPT_DYNAMIC Sun, 03 Mar 2024 00:30:36 +0000 x86_64 GNU/Linux\r\n - Gitui version: gitui 0.25.1 installed via pacman\r\n - Rust version: rustc 1.78.0-nightly (9c3ad802d 2024-03-07) (should not matter)\r\n\n", "patch": "diff --git a/src/popups/blame_file.rs b/src/popups/blame_file.rs\nindex 65c7c2028c..af2fdb90e1 100644\n--- a/src/popups/blame_file.rs\n+++ b/src/popups/blame_file.rs\n@@ -572,7 +572,8 @@ impl BlameFilePopup {\n \t\t\t.iter()\n \t\t\t.map(|l| l.1.clone())\n \t\t\t.collect::<Vec<_>>();\n-\t\tlet text = tabs_to_spaces(raw_lines.join(\"\\n\"));\n+\t\tlet mut text = tabs_to_spaces(raw_lines.join(\"\\n\"));\n+\t\ttext.push('\\n');\n \n \t\tjob.spawn(AsyncSyntaxJob::new(\n \t\t\ttext,\n", "instance_id": "gitui-org__gitui-2133", "clarity": 2, "difficulty": 0.2, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the bug, steps to reproduce it, and the expected behavior versus the actual outcome (application crash due to an index out of bounds error). The context, including OS, software version, and error output, is also provided, which aids in understanding the issue. However, there are minor ambiguities: the problem statement does not explicitly discuss potential edge cases or constraints that might be contributing to the crash (e.g., specific file content or size limitations). Additionally, while the steps to reproduce are detailed, there is no mention of whether this issue occurs consistently across different environments or only under specific conditions. Thus, while the statement is valid and clear, it misses some minor details that could provide a more comprehensive understanding of the problem scope.", "difficulty_explanation": "The difficulty of this problem is rated as easy (0.2) based on the provided factors. The issue is a straightforward index out of bounds error in a specific file (`src/popups/blame_file.rs`), and the code change required is minimal\u2014adding a single newline character to the text string before passing it to a job for processing. This change is localized to a single line within one file and does not impact the broader architecture or require understanding complex interactions across the codebase. The technical concepts involved are basic, focusing on string manipulation and understanding the behavior of the `tabs_to_spaces` function and the downstream `AsyncSyntaxJob`. There are no advanced language features, algorithms, or domain-specific knowledge required. Regarding edge cases and error handling, the problem statement does not explicitly mention additional cases to handle beyond the reported crash, and the fix itself does not introduce new error handling logic. Overall, this is a simple bug fix that requires minimal effort and understanding, fitting within the easy range of difficulty.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "High-Intensity JSON RPC BatchRequest Causes Server Crash and Unpredictable Downtime\n## Bug Report\r\n<!-- Thanks for your bug report! We appreciate your time to fill following sections to help us locate the root cause. -->\r\n### Current Behavior\r\nWhen subjected to high-intensity JSON RPC BatchRequest, the CKB node causes the entire server to crash, rendering it completely inaccessible. The crash occurs after a certain batch size threshold is exceeded, and even after stopping the requests, the server will still crash at an unpredictable time in the future.\r\n\r\n### Expected Behavior\r\nI expect the node to send an error message to the client when it cannot handle high-intensity batch sizes, rather than attempting to process the requests and causing the server to crash. Ideally, the node should shut down to prevent affecting the entire server.\r\n\r\n### Environment\r\n- **CKB version**: ckb 0.116.1 \r\n- **Chain**: testnet, mainnet (both have experienced the issue)\r\n- **Operating system**: Ubuntu 20.04\r\n- **Arch**: x64\r\n- **Installation**: GitHub Release\r\n\r\n### Additional context/Screenshots\r\nThe issue was first observed when my indexer node encountered the bug and stopped working. I had used high-frequency batch sizes for a few seconds before stopping the requests. Approximately two hours later, the server suddenly became unresponsive.\r\n\n", "patch": "diff --git a/resource/ckb.toml b/resource/ckb.toml\nindex e47a37cff4..c2091ad87e 100644\n--- a/resource/ckb.toml\n+++ b/resource/ckb.toml\n@@ -212,6 +212,12 @@ block_uncles_cache_size    = 30\n # cell_filter = \"let script = output.type;script!=() && script.code_hash == \\\"0x00000000000000000000000000000000000000000000000000545950455f4944\\\"\"\n # # The initial tip can be set higher than the current indexer tip as the starting height for indexing.\n # init_tip_hash = \"0x8fbd0ec887159d2814cee475911600e3589849670f5ee1ed9798b38fdeef4e44\"\n+# By default, there is no limitation on the size of indexer request\n+# However, because serde json serialization consumes too much memory(10x),\n+# it may cause the physical machine to become unresponsive.\n+# We recommend a consumption limit of 2g, which is 400 as the limit,\n+# which is a safer approach\n+# request_limit = 400\n #\n # # CKB rich-indexer has its unique configuration.\n # [indexer_v2.rich_indexer]\ndiff --git a/util/app-config/src/configs/indexer.rs b/util/app-config/src/configs/indexer.rs\nindex 5966b6673b..1f44a68aa6 100644\n--- a/util/app-config/src/configs/indexer.rs\n+++ b/util/app-config/src/configs/indexer.rs\n@@ -35,6 +35,9 @@ pub struct IndexerConfig {\n     /// The init tip block hash\n     #[serde(default)]\n     pub init_tip_hash: Option<H256>,\n+    /// limit of indexer reqeust\n+    #[serde(default)]\n+    pub request_limit: Option<usize>,\n     /// Rich indexer config options\n     #[serde(default)]\n     pub rich_indexer: RichIndexerConfig,\n@@ -56,6 +59,7 @@ impl Default for IndexerConfig {\n             db_background_jobs: None,\n             db_keep_log_file_num: None,\n             init_tip_hash: None,\n+            request_limit: None,\n             rich_indexer: RichIndexerConfig::default(),\n         }\n     }\ndiff --git a/util/gen-types/src/lib.rs b/util/gen-types/src/lib.rs\nindex c2aa5c5183..bceae47fda 100644\n--- a/util/gen-types/src/lib.rs\n+++ b/util/gen-types/src/lib.rs\n@@ -19,8 +19,10 @@ pub use molecule::bytes;\n \n cfg_if::cfg_if! {\n     if #[cfg(feature = \"std\")] {\n+        #[allow(unused_imports)]\n         use std::{vec, borrow};\n     } else {\n+        #[allow(unused_imports)]\n         use alloc::{vec, borrow};\n     }\n }\ndiff --git a/util/indexer/src/service.rs b/util/indexer/src/service.rs\nindex 774276e392..849bb434c2 100644\n--- a/util/indexer/src/service.rs\n+++ b/util/indexer/src/service.rs\n@@ -19,6 +19,7 @@ use rocksdb::{prelude::*, Direction, IteratorMode};\n use std::convert::TryInto;\n use std::num::NonZeroUsize;\n use std::sync::{Arc, RwLock};\n+use std::usize;\n \n pub(crate) const SUBSCRIBER_NAME: &str = \"Indexer\";\n const DEFAULT_LOG_KEEP_NUM: usize = 1;\n@@ -31,6 +32,7 @@ pub struct IndexerService {\n     sync: IndexerSyncService,\n     block_filter: Option<String>,\n     cell_filter: Option<String>,\n+    request_limit: usize,\n }\n \n impl IndexerService {\n@@ -56,6 +58,7 @@ impl IndexerService {\n             sync,\n             block_filter: config.block_filter.clone(),\n             cell_filter: config.cell_filter.clone(),\n+            request_limit: config.request_limit.unwrap_or(usize::MAX),\n         }\n     }\n \n@@ -67,6 +70,7 @@ impl IndexerService {\n         IndexerHandle {\n             store: self.store.clone(),\n             pool: self.sync.pool(),\n+            request_limit: self.request_limit,\n         }\n     }\n \n@@ -124,6 +128,7 @@ impl IndexerService {\n pub struct IndexerHandle {\n     pub(crate) store: RocksdbStore,\n     pub(crate) pool: Option<Arc<RwLock<Pool>>>,\n+    request_limit: usize,\n }\n \n impl IndexerHandle {\n@@ -168,6 +173,12 @@ impl IndexerHandle {\n         if limit == 0 {\n             return Err(Error::invalid_params(\"limit should be greater than 0\"));\n         }\n+        if limit > self.request_limit {\n+            return Err(Error::invalid_params(format!(\n+                \"limit must be less than {}\",\n+                self.request_limit,\n+            )));\n+        }\n \n         let (prefix, from_key, direction, skip) = build_query_options(\n             &search_key,\n@@ -334,6 +345,12 @@ impl IndexerHandle {\n         if limit == 0 {\n             return Err(Error::invalid_params(\"limit should be greater than 0\"));\n         }\n+        if limit > self.request_limit {\n+            return Err(Error::invalid_params(format!(\n+                \"limit must be less than {}\",\n+                self.request_limit,\n+            )));\n+        }\n \n         if search_key\n             .script_search_mode\n@@ -926,12 +943,13 @@ mod tests {\n \n     #[test]\n     fn rpc() {\n-        let store = new_store(\"rpc\");\n+        let store: RocksdbStore = new_store(\"rpc\");\n         let pool = Arc::new(RwLock::new(Pool::default()));\n         let indexer = Indexer::new(store.clone(), 10, 100, None, CustomFilters::new(None, None));\n         let rpc = IndexerHandle {\n             store,\n             pool: Some(Arc::clone(&pool)),\n+            request_limit: usize::MAX,\n         };\n \n         // setup test data\n@@ -1516,7 +1534,11 @@ mod tests {\n     fn script_search_mode_rpc() {\n         let store = new_store(\"script_search_mode_rpc\");\n         let indexer = Indexer::new(store.clone(), 10, 100, None, CustomFilters::new(None, None));\n-        let rpc = IndexerHandle { store, pool: None };\n+        let rpc = IndexerHandle {\n+            store,\n+            pool: None,\n+            request_limit: usize::MAX,\n+        };\n \n         // setup test data\n         let lock_script1 = ScriptBuilder::default()\n@@ -1755,11 +1777,47 @@ mod tests {\n         );\n     }\n \n+    #[test]\n+    fn test_request_limit() {\n+        let store = new_store(\"script_search_mode_rpc\");\n+        let rpc = IndexerHandle {\n+            store,\n+            pool: None,\n+            request_limit: 2,\n+        };\n+\n+        let lock_script1 = ScriptBuilder::default()\n+            .code_hash(H256(rand::random()).pack())\n+            .hash_type(ScriptHashType::Type.into())\n+            .args(Bytes::from(b\"lock_script1\".to_vec()).pack())\n+            .build();\n+        let data = [0u8; 7];\n+        let res = rpc.get_cells(\n+            IndexerSearchKey {\n+                script: lock_script1.into(),\n+                filter: Some(IndexerSearchKeyFilter {\n+                    output_data: Some(JsonBytes::from_vec(data.to_vec())),\n+                    output_data_filter_mode: Some(IndexerSearchMode::Prefix),\n+                    ..Default::default()\n+                }),\n+                ..Default::default()\n+            },\n+            IndexerOrder::Asc,\n+            1000.into(),\n+            None,\n+        );\n+        assert!(res.is_err())\n+    }\n+\n     #[test]\n     fn output_data_filter_mode_rpc() {\n         let store = new_store(\"script_search_mode_rpc\");\n         let indexer = Indexer::new(store.clone(), 10, 100, None, CustomFilters::new(None, None));\n-        let rpc = IndexerHandle { store, pool: None };\n+        let rpc = IndexerHandle {\n+            store,\n+            pool: None,\n+            request_limit: usize::MAX,\n+        };\n \n         // setup test data\n         let lock_script1 = ScriptBuilder::default()\ndiff --git a/util/jsonrpc-types/src/blockchain.rs b/util/jsonrpc-types/src/blockchain.rs\nindex 47fbfff606..247b394c1c 100644\n--- a/util/jsonrpc-types/src/blockchain.rs\n+++ b/util/jsonrpc-types/src/blockchain.rs\n@@ -116,7 +116,7 @@ impl From<packed::Script> for Script {\n     fn from(input: packed::Script) -> Script {\n         Script {\n             code_hash: input.code_hash().unpack(),\n-            args: JsonBytes::from_bytes(input.args().unpack()),\n+            args: JsonBytes::from_vec(input.args().unpack()),\n             hash_type: core::ScriptHashType::try_from(input.hash_type())\n                 .expect(\"checked data\")\n                 .into(),\ndiff --git a/util/jsonrpc-types/src/bytes.rs b/util/jsonrpc-types/src/bytes.rs\nindex 1e270f8d54..b5b7a6bcf4 100644\n--- a/util/jsonrpc-types/src/bytes.rs\n+++ b/util/jsonrpc-types/src/bytes.rs\n@@ -60,13 +60,13 @@ impl JsonBytes {\n \n impl From<packed::Bytes> for JsonBytes {\n     fn from(input: packed::Bytes) -> Self {\n-        JsonBytes::from_bytes(input.raw_data())\n+        JsonBytes::from_vec(input.raw_data().to_vec())\n     }\n }\n \n impl<'a> From<&'a packed::Bytes> for JsonBytes {\n     fn from(input: &'a packed::Bytes) -> Self {\n-        JsonBytes::from_bytes(input.raw_data())\n+        JsonBytes::from_vec(input.raw_data().to_vec())\n     }\n }\n \ndiff --git a/util/rich-indexer/src/indexer/mod.rs b/util/rich-indexer/src/indexer/mod.rs\nindex cd5beb3a67..32c395f775 100644\n--- a/util/rich-indexer/src/indexer/mod.rs\n+++ b/util/rich-indexer/src/indexer/mod.rs\n@@ -37,6 +37,7 @@ use std::sync::{Arc, RwLock};\n pub(crate) struct RichIndexer {\n     async_rich_indexer: AsyncRichIndexer,\n     async_runtime: Handle,\n+    request_limit: usize,\n }\n \n impl RichIndexer {\n@@ -46,10 +47,12 @@ impl RichIndexer {\n         pool: Option<Arc<RwLock<Pool>>>,\n         custom_filters: CustomFilters,\n         async_runtime: Handle,\n+        request_limit: usize,\n     ) -> Self {\n         Self {\n             async_rich_indexer: AsyncRichIndexer::new(store, pool, custom_filters),\n             async_runtime,\n+            request_limit,\n         }\n     }\n }\n@@ -61,6 +64,7 @@ impl IndexerSync for RichIndexer {\n             self.async_rich_indexer.store.clone(),\n             self.async_rich_indexer.pool.clone(),\n             self.async_runtime.clone(),\n+            self.request_limit,\n         );\n         indexer_handle\n             .get_indexer_tip()\ndiff --git a/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_cells.rs b/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_cells.rs\nindex 63a89d4800..100a5eaca0 100644\n--- a/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_cells.rs\n+++ b/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_cells.rs\n@@ -26,6 +26,12 @@ impl AsyncRichIndexerHandle {\n         if limit == 0 {\n             return Err(Error::invalid_params(\"limit should be greater than 0\"));\n         }\n+        if limit as usize > self.request_limit {\n+            return Err(Error::invalid_params(format!(\n+                \"limit must be less than {}\",\n+                self.request_limit,\n+            )));\n+        }\n \n         let mut param_index = 1;\n \ndiff --git a/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_transactions.rs b/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_transactions.rs\nindex d8d58e0d68..ff58aa0a6f 100644\n--- a/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_transactions.rs\n+++ b/util/rich-indexer/src/indexer_handle/async_indexer_handle/get_transactions.rs\n@@ -24,6 +24,12 @@ impl AsyncRichIndexerHandle {\n         if limit == 0 {\n             return Err(Error::invalid_params(\"limit should be greater than 0\"));\n         }\n+        if limit as usize > self.request_limit {\n+            return Err(Error::invalid_params(format!(\n+                \"limit must be less than {}\",\n+                self.request_limit,\n+            )));\n+        }\n         search_key.filter = convert_max_values_in_search_filter(&search_key.filter);\n \n         let mut tx = self\ndiff --git a/util/rich-indexer/src/indexer_handle/async_indexer_handle/mod.rs b/util/rich-indexer/src/indexer_handle/async_indexer_handle/mod.rs\nindex 7f6032ef25..4a99cbcc32 100644\n--- a/util/rich-indexer/src/indexer_handle/async_indexer_handle/mod.rs\n+++ b/util/rich-indexer/src/indexer_handle/async_indexer_handle/mod.rs\n@@ -23,12 +23,17 @@ use std::sync::{Arc, RwLock};\n pub struct AsyncRichIndexerHandle {\n     store: SQLXPool,\n     pool: Option<Arc<RwLock<Pool>>>,\n+    request_limit: usize,\n }\n \n impl AsyncRichIndexerHandle {\n     /// Construct new AsyncRichIndexerHandle instance\n-    pub fn new(store: SQLXPool, pool: Option<Arc<RwLock<Pool>>>) -> Self {\n-        Self { store, pool }\n+    pub fn new(store: SQLXPool, pool: Option<Arc<RwLock<Pool>>>, request_limit: usize) -> Self {\n+        Self {\n+            store,\n+            pool,\n+            request_limit,\n+        }\n     }\n }\n \ndiff --git a/util/rich-indexer/src/indexer_handle/mod.rs b/util/rich-indexer/src/indexer_handle/mod.rs\nindex 10d1e1e62e..f96c8f590c 100644\n--- a/util/rich-indexer/src/indexer_handle/mod.rs\n+++ b/util/rich-indexer/src/indexer_handle/mod.rs\n@@ -22,9 +22,14 @@ pub struct RichIndexerHandle {\n \n impl RichIndexerHandle {\n     /// Construct new RichIndexerHandle instance\n-    pub fn new(store: SQLXPool, pool: Option<Arc<RwLock<Pool>>>, async_handle: Handle) -> Self {\n+    pub fn new(\n+        store: SQLXPool,\n+        pool: Option<Arc<RwLock<Pool>>>,\n+        async_handle: Handle,\n+        request_limit: usize,\n+    ) -> Self {\n         Self {\n-            async_handle: AsyncRichIndexerHandle::new(store, pool),\n+            async_handle: AsyncRichIndexerHandle::new(store, pool, request_limit),\n             async_runtime: async_handle,\n         }\n     }\ndiff --git a/util/rich-indexer/src/service.rs b/util/rich-indexer/src/service.rs\nindex 716b08f2d2..17c680c108 100644\n--- a/util/rich-indexer/src/service.rs\n+++ b/util/rich-indexer/src/service.rs\n@@ -1,5 +1,7 @@\n //\uff01The rich-indexer service.\n \n+use std::usize;\n+\n use crate::indexer::RichIndexer;\n use crate::store::SQLXPool;\n use crate::{AsyncRichIndexerHandle, RichIndexerHandle};\n@@ -19,6 +21,7 @@ pub struct RichIndexerService {\n     block_filter: Option<String>,\n     cell_filter: Option<String>,\n     async_handle: Handle,\n+    request_limit: usize,\n }\n \n impl RichIndexerService {\n@@ -47,6 +50,7 @@ impl RichIndexerService {\n             block_filter: config.block_filter.clone(),\n             cell_filter: config.cell_filter.clone(),\n             async_handle,\n+            request_limit: config.request_limit.unwrap_or(usize::MAX),\n         }\n     }\n \n@@ -56,6 +60,7 @@ impl RichIndexerService {\n             self.sync.pool(),\n             CustomFilters::new(self.block_filter.as_deref(), self.cell_filter.as_deref()),\n             self.async_handle.clone(),\n+            self.request_limit,\n         )\n     }\n \n@@ -83,6 +88,7 @@ impl RichIndexerService {\n             self.store.clone(),\n             self.sync.pool(),\n             self.async_handle.clone(),\n+            self.request_limit,\n         )\n     }\n \n@@ -91,6 +97,6 @@ impl RichIndexerService {\n     /// The returned handle can be used to get data from rich-indexer,\n     /// and can be cloned to allow moving the Handle to other threads.\n     pub fn async_handle(&self) -> AsyncRichIndexerHandle {\n-        AsyncRichIndexerHandle::new(self.store.clone(), self.sync.pool())\n+        AsyncRichIndexerHandle::new(self.store.clone(), self.sync.pool(), self.request_limit)\n     }\n }\n", "instance_id": "nervosnetwork__ckb-4576", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a high-intensity JSON RPC BatchRequest causes a server crash and unpredictable downtime in the CKB node. The current behavior (server crash), expected behavior (error message and graceful shutdown), and environment details are provided, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the exact nature of the \"high-intensity\" requests (e.g., batch size threshold, frequency, or payload structure) that trigger the crash. Additionally, there are no explicit examples of the requests or logs that could help reproduce the issue. Edge cases or specific conditions leading to the delayed crash (e.g., memory usage patterns or specific request types) are also not mentioned. While the intent and goal are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the medium range due to several factors. First, the scope of code changes spans multiple files and modules (e.g., indexer, rich-indexer, JSON RPC types, and configuration files), requiring a moderate understanding of the interactions between different components of the CKB node codebase. The changes primarily involve adding a request limit to prevent server crashes due to excessive memory consumption during JSON serialization, which is a practical but not overly complex solution. The technical concepts involved include configuration management, memory usage considerations (e.g., serde JSON serialization overhead), error handling in RPC calls, and basic data structure modifications. These concepts are not overly advanced but require a solid understanding of Rust and the specific domain of blockchain node operations. Edge case handling is addressed by enforcing a request limit, though the problem statement does not explicitly define complex edge cases beyond the general issue of high-intensity requests. The changes do not significantly impact the system's architecture but do require careful integration to ensure the limit is applied consistently across different indexer implementations. Overall, this problem requires moderate effort and understanding, fitting into the 0.4-0.6 range, with a slight lean towards the higher end due to the multi-file changes and domain-specific context.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Processing JSON from stdin is broken\nIf I run the following JSON test in interactive mode, it works:\n\n```\nlimbo % cargo run\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.34s\n     Running `target/debug/limbo`\nLimbo v0.0.14\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database\nlimbo> SELECT json(' /* abc */ { /*def*/ aaa /* xyz */ : // to the end of line\n          123 /* xyz */ , /* 123 */ }');\n{\"aaa\":123}\n```\n\nHowever, if I try to run the same JSON query through stdin, Limbo complains about malformed JSON:\n\n```\nlimbo % cat input-json.txt\nSELECT json(' /* abc */ { /*def*/ aaa /* xyz */ : // to the end of line\n          123 /* xyz */ , /* 123 */ }')\nlimbo % cat input-json.txt | cargo run\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.47s\n     Running `target/debug/limbo`\nLimbo v0.0.14\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database\nParse error: malformed JSON\n```\n", "patch": "diff --git a/cli/app.rs b/cli/app.rs\nindex 7941e90e1..8e438cbfa 100644\n--- a/cli/app.rs\n+++ b/cli/app.rs\n@@ -502,7 +502,7 @@ impl Limbo {\n             }\n             self.reset_input();\n         } else {\n-            self.buffer_input(line);\n+            self.buffer_input(format!(\"{}\\n\", line).as_str());\n             self.set_multiline_prompt();\n         }\n         rl.add_history_entry(line.to_owned())?;\n", "instance_id": "tursodatabase__limbo-985", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: processing JSON input from stdin fails with a \"malformed JSON\" error, while the same input works in interactive mode. It provides specific examples of the failing and working scenarios, which helps in understanding the context. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly state the expected behavior for stdin input (though it can be inferred from the interactive mode example), nor does it mention potential edge cases like multi-line input handling or specific JSON parsing rules. Additionally, there is no explanation of why the error occurs or what part of the codebase is responsible for the discrepancy, leaving some room for interpretation. Overall, the statement is valid and clear enough to understand the goal but lacks comprehensive details on constraints and edge cases.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the simplicity of the required fix and the limited scope of the code change. Analyzing the provided diff, the issue appears to be related to how input lines are buffered when reading from stdin. The change involves appending a newline character to each line before buffering it, which suggests a minor bug fix in input handling (likely to ensure consistent line termination between interactive and stdin modes). The modification is confined to a single line in a single file (`cli/app.rs`), with no impact on the broader system architecture or interactions between modules. The technical concepts involved are basic\u2014understanding string manipulation and input buffering in Rust, which are not complex for an experienced developer. There are no explicit edge cases or error handling requirements mentioned in the problem statement, and the code change does not introduce new error handling logic. Overall, this is a straightforward bug fix requiring minimal effort and understanding of the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Can't reference a variable named like a function\nThis simple expression, taken straight [from the spec](https://github.com/google/cel-spec/blob/master/doc/langdef.md#evaluation-environment), fails: `size(requests) > size`\r\n\r\nReproducible test case:\r\n\r\n```rust\r\nlet program = Program::compile(\"size(requests) > size\").unwrap();\r\nlet mut context = Context::default();\r\nlet requests = vec![Value::Int(42), Value::Int(42), Value::Int(42)];\r\ncontext.add_variable(\"requests\", Value::List(Arc::new(requests))).unwrap();\r\ncontext.add_variable(\"size\", Value::Int(42)).unwrap();\r\nprogram.execute(&context) // Err` value: ValuesNotComparable(Int(3), Function(\"size\", None))\r\n```\r\n\r\nWhile as per the doc:\r\n>the first size is a function, and the second is a variable.\r\n\r\nHere both `size` result in the function. And the variable gets overshadowed by the function always.\n", "patch": "diff --git a/interpreter/src/objects.rs b/interpreter/src/objects.rs\nindex 62e8c16..58bd6ce 100644\n--- a/interpreter/src/objects.rs\n+++ b/interpreter/src/objects.rs\n@@ -520,11 +520,28 @@ impl<'a> Value {\n                 })\n                 .into()\n             }\n-            Expression::Ident(name) => {\n-                if ctx.has_function(&***name) {\n-                    Value::Function(name.clone(), None).into()\n+            Expression::Ident(name) => ctx.get_variable(&***name),\n+            Expression::FunctionCall(name, target, args) => {\n+                if let Expression::Ident(name) = &**name {\n+                    let func = ctx.get_function(&**name).unwrap();\n+                    match target {\n+                        None => {\n+                            let mut ctx =\n+                                FunctionContext::new(name.clone(), None, ctx, args.clone());\n+                            func.call_with_context(&mut ctx)\n+                        }\n+                        Some(target) => {\n+                            let mut ctx = FunctionContext::new(\n+                                name.clone(),\n+                                Some(Value::resolve(target, ctx)?),\n+                                ctx,\n+                                args.clone(),\n+                            );\n+                            func.call_with_context(&mut ctx)\n+                        }\n+                    }\n                 } else {\n-                    ctx.get_variable(&***name)\n+                    unreachable!(\"a function name should always be a `Expression::Ident`\")\n                 }\n             }\n         }\n@@ -594,29 +611,6 @@ impl<'a> Value {\n                     (false, true) => Value::Function(name.clone(), Some(self.into())).into(),\n                 }\n             }\n-            Member::FunctionCall(args) => {\n-                if let Value::Function(name, target) = self {\n-                    let func = ctx.get_function(&**name).unwrap();\n-                    match target {\n-                        None => {\n-                            let mut ctx =\n-                                FunctionContext::new(name.clone(), None, ctx, args.clone());\n-                            func.call_with_context(&mut ctx)\n-                        }\n-                        Some(target) => {\n-                            let mut ctx = FunctionContext::new(\n-                                name.clone(),\n-                                Some(*target),\n-                                ctx,\n-                                args.clone(),\n-                            );\n-                            func.call_with_context(&mut ctx)\n-                        }\n-                    }\n-                } else {\n-                    unreachable!(\"FunctionCall without Value::Function - {:?}\", self)\n-                }\n-            }\n         }\n     }\n \n@@ -787,8 +781,9 @@ impl ops::Rem<Value> for Value {\n \n #[cfg(test)]\n mod tests {\n-    use crate::{objects::Key, Context, Program};\n+    use crate::{objects::Key, Context, Program, Value};\n     use std::collections::HashMap;\n+    use std::sync::Arc;\n \n     #[test]\n     fn test_indexed_map_access() {\n@@ -859,8 +854,20 @@ mod tests {\n     fn test_invalid_compare() {\n         let context = Context::default();\n \n-        let program = Program::compile(\"size == 50\").unwrap();\n+        let program = Program::compile(\"{} == []\").unwrap();\n         let value = program.execute(&context).unwrap();\n         assert_eq!(value, false.into());\n     }\n+\n+    #[test]\n+    fn test_size_fn_var() {\n+        let program = Program::compile(\"size(requests) + size == 5\").unwrap();\n+        let mut context = Context::default();\n+        let requests = vec![Value::Int(42), Value::Int(42)];\n+        context\n+            .add_variable(\"requests\", Value::List(Arc::new(requests)))\n+            .unwrap();\n+        context.add_variable(\"size\", Value::Int(3)).unwrap();\n+        assert_eq!(program.execute(&context).unwrap(), Value::Bool(true));\n+    }\n }\ndiff --git a/parser/src/ast.rs b/parser/src/ast.rs\nindex 50953e8..5967ccc 100644\n--- a/parser/src/ast.rs\n+++ b/parser/src/ast.rs\n@@ -39,6 +39,7 @@ pub enum Expression {\n     Unary(UnaryOp, Box<Expression>),\n \n     Member(Box<Expression>, Box<Member>),\n+    FunctionCall(Box<Expression>, Option<Box<Expression>>, Vec<Expression>),\n \n     List(Vec<Expression>),\n     Map(Vec<(Expression, Expression)>),\n@@ -50,7 +51,6 @@ pub enum Expression {\n #[derive(Debug, PartialEq, Clone)]\n pub enum Member {\n     Attribute(Arc<String>),\n-    FunctionCall(Vec<Expression>),\n     Index(Box<Expression>),\n     Fields(Vec<(Arc<String>, Expression)>),\n }\ndiff --git a/parser/src/cel.lalrpop b/parser/src/cel.lalrpop\nindex 716e244..e73d236 100644\n--- a/parser/src/cel.lalrpop\n+++ b/parser/src/cel.lalrpop\n@@ -53,9 +53,8 @@ pub Unary: Expression = {\n pub Member: Expression = {\n     <left:Member> \".\" <identifier:Ident> => Expression::Member(left.into(), Member::Attribute(identifier.into()).into()).into(),\n     <left:Member> \".\" <identifier:Ident> \"(\" <arguments:CommaSeparated<Expression>> \")\" => {\n-            let inner = Expression::Member(left.into(), Member::Attribute(identifier.into()).into()).into();\n-            Expression::Member(inner, Member::FunctionCall(arguments).into()).into()\n-    },\n+           Expression::FunctionCall(Expression::Ident(identifier).into(), Some(left.into()), arguments).into()\n+   },\n     <left:Member> \"[\" <expression:Expression> \"]\" => Expression::Member(left.into(), Member::Index(expression.into()).into()).into(),\n     <left:Member> \"{\" <fields:CommaSeparated<FieldInits>> \"}\" => Expression::Member(left.into(), Member::Fields(fields.into()).into()).into(),\n     Primary,\n@@ -64,8 +63,7 @@ pub Member: Expression = {\n pub Primary: Expression = {\n     \".\"? <Ident> => Expression::Ident(<>.into()).into(),\n     \".\"? <identifier:Ident> \"(\" <arguments:CommaSeparated<Expression>> \")\" => {\n-            let inner = Expression::Ident(identifier.into()).into();\n-            Expression::Member(inner, Member::FunctionCall(arguments).into()).into()\n+           Expression::FunctionCall(Expression::Ident(identifier).into(), None, arguments).into()\n     },\n     Atom => Expression::Atom(<>).into(),\n     \"[\" <members:CommaSeparated<Expression>> \"]\" => Expression::List(<>).into(),\ndiff --git a/parser/src/lib.rs b/parser/src/lib.rs\nindex 7e83b24..0e60ff8 100644\n--- a/parser/src/lib.rs\n+++ b/parser/src/lib.rs\n@@ -142,19 +142,21 @@ mod tests {\n     fn test_parse_map_macro() {\n         assert_parse_eq(\n             \"[1, 2, 3].map(x, x * 2)\",\n-            Member(\n-                Box::new(Member(\n-                    Box::new(List(vec![Atom(Int(1)), Atom(Int(2)), Atom(Int(3))])),\n-                    Box::new(Attribute(\"map\".to_string().into())),\n-                )),\n-                Box::new(FunctionCall(vec![\n+            FunctionCall(\n+                Box::new(Ident(\"map\".to_string().into())),\n+                Some(Box::new(List(vec![\n+                    Atom(Int(1)),\n+                    Atom(Int(2)),\n+                    Atom(Int(3)),\n+                ]))),\n+                vec![\n                     Ident(\"x\".to_string().into()),\n                     Arithmetic(\n                         Box::new(Ident(\"x\".to_string().into())),\n                         ArithmeticOp::Multiply,\n                         Box::new(Atom(Int(2))),\n                     ),\n-                ])),\n+                ],\n             ),\n         )\n     }\n@@ -178,10 +180,7 @@ mod tests {\n     fn function_call_no_args() {\n         assert_parse_eq(\n             \"a()\",\n-            Member(\n-                Box::new(Ident(\"a\".to_string().into())),\n-                Box::new(FunctionCall(vec![])),\n-            ),\n+            FunctionCall(Box::new(Ident(\"a\".to_string().into())), None, vec![]),\n         );\n     }\n \n", "instance_id": "clarkmcc__cel-rust-59", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a variable named like a function (e.g., \"size\") is being overshadowed by the function of the same name, leading to incorrect behavior during expression evaluation. It provides a reproducible test case and references the expected behavior from the specification, which helps in understanding the goal. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss potential edge cases (e.g., nested expressions or other naming conflicts) or constraints on how the solution should handle variable/function precedence. Additionally, it lacks a detailed explanation of the expected output format or behavior beyond the provided test case. While the intent is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves multiple files (interpreter and parser modules), requiring modifications to the abstract syntax tree (AST) representation and evaluation logic. This indicates a need to understand interactions between the parser and interpreter components of the codebase. The changes are not trivial, as they involve restructuring how function calls are represented and resolved (e.g., moving from a `Member::FunctionCall` to a dedicated `Expression::FunctionCall` variant), which impacts the core logic of expression evaluation. \n\nSecond, the technical concepts required include a solid understanding of Rust (e.g., ownership with `Arc`, boxing, and pattern matching), parsing (modifying grammar rules in LALRPOP), and interpreter design (handling variable/function name resolution). While these concepts are not extremely advanced, they require a moderate level of expertise to implement correctly without introducing bugs.\n\nThird, the problem does not explicitly mention edge cases, but the code changes and test case suggest potential complexities, such as ensuring that variable and function names are resolved correctly in different contexts. The addition of a new test (`test_size_fn_var`) indicates that the solution must handle specific scenarios, but broader edge cases (e.g., nested function calls or conflicting names in different scopes) are not addressed, which could complicate the implementation if considered.\n\nFinally, the impact on the system's architecture is moderate. The changes alter how expressions are parsed and evaluated, which is central to the functionality of a language interpreter like CEL (Common Expression Language), but they do not appear to require a full system refactor or introduce significant performance concerns. Overall, this problem requires a good understanding of multiple concepts and careful modifications across several files, justifying a difficulty score of 0.55, in the medium range.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "forge flatten fails on top-level event declarations\n### Component\n\nForge\n\n### Have you ensured that all of these are up to date?\n\n- [x] Foundry\n- [x] Foundryup\n\n### What version of Foundry are you on?\n\n1.0-rc\n\n### What version of Foundryup are you on?\n\n1.0.1\n\n### What command(s) is the bug in?\n\nforge flatten\n\n### Operating System\n\nLinux\n\n### Describe the bug\n\nWhen trying to flatten a solidity file (one that imports another file), the following error is produced when the compiler encounters an `event` declaration at the top level of a file:\n\n```\nError: Failed to flatten: unknown variant `EventDefinition`, expected one of `PragmaDirective`, `ImportDirective`, `UsingForDirective`, `VariableDeclaration`, `EnumDefinition`, `ErrorDefinition`, `FunctionDefinition`, `StructDefinition`, `UserDefinedValueTypeDefinition`, `ContractDefinition` at line 1 column 13177\n```\n\nEvent definitions should be allowed at the top level.\n\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 96322731759c..68d004f1b04f 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -3783,9 +3783,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers\"\n-version = \"0.13.2\"\n+version = \"0.13.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"de23802550de5204eec1a6297296d2fef6b86ea61f33b1667eebd886577caa34\"\n+checksum = \"ac8f0bab060fd7c1764c4be2563e6933d39ec8c2b8a8d6c08aaf45ab29d08310\"\n dependencies = [\n  \"alloy-json-abi\",\n  \"alloy-primitives\",\n@@ -3820,9 +3820,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-artifacts\"\n-version = \"0.13.2\"\n+version = \"0.13.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"27f065a1c785b3ec556a7a49a27016fc723bd9a4d8623f521c326f8396df64a6\"\n+checksum = \"b102dd131e939d80cc5c85214d2f0f6ba20ed75cf098019c4d995791b4ebae05\"\n dependencies = [\n  \"foundry-compilers-artifacts-solc\",\n  \"foundry-compilers-artifacts-vyper\",\n@@ -3830,9 +3830,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-artifacts-solc\"\n-version = \"0.13.2\"\n+version = \"0.13.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"127e0e788765bc0103eb6b92067843c6ffe660090b2f6fbff1a1441a51939aa0\"\n+checksum = \"db9ef02de4fda04ae3ed098afb6e16edd742d1073f972197a4566836b453bdcd\"\n dependencies = [\n  \"alloy-json-abi\",\n  \"alloy-primitives\",\n@@ -3854,9 +3854,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-artifacts-vyper\"\n-version = \"0.13.2\"\n+version = \"0.13.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8764f450332267305440ec399560919e01aa98f23af2c36ac952f9216449cdca\"\n+checksum = \"3bd9d33cbeda448af917105920fefdc3ac42f9ffdaa2d840d58207db7807ea29\"\n dependencies = [\n  \"alloy-json-abi\",\n  \"alloy-primitives\",\n@@ -3869,9 +3869,9 @@ dependencies = [\n \n [[package]]\n name = \"foundry-compilers-core\"\n-version = \"0.13.2\"\n+version = \"0.13.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"c59caebbeaeb0e34564c1a404081478a0aadaa8999b57f0befd85965252e074a\"\n+checksum = \"746d121f7b86b84b20e582a27a56c49435768ad3b8005e9afeaf68b53a77fb5c\"\n dependencies = [\n  \"alloy-primitives\",\n  \"cfg-if\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 0ad21d6c143b..44e561c79a36 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -187,7 +187,7 @@ foundry-linking = { path = \"crates/linking\" }\n \n # solc & compilation utilities\n foundry-block-explorers = { version = \"0.11.0\", default-features = false }\n-foundry-compilers = { version = \"0.13.2\", default-features = false }\n+foundry-compilers = { version = \"0.13.3\", default-features = false }\n foundry-fork-db = \"0.11.0\"\n solang-parser = \"=0.3.3\"\n solar-parse = { version = \"=0.1.1\", default-features = false }\n", "instance_id": "foundry-rs__foundry-9892", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `forge flatten` command fails when encountering top-level `event` declarations in a Solidity file, producing a specific error message. The goal is implicitly understood as fixing this bug to allow event definitions at the top level. The component (Forge), version details, operating system, and the exact command causing the issue are provided, which adds to the clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not include examples of the Solidity code that triggers the error, nor does it specify any constraints or expected behavior after the fix (e.g., how the flattened output should handle events). Additionally, edge cases or potential related issues (e.g., interactions with other top-level declarations) are not mentioned. Due to these minor gaps, the clarity score is rated as 2 (Mostly Clear) rather than 3.", "difficulty_explanation": "The difficulty of this problem is rated as 0.25 (Easy), based on the provided code changes and the nature of the issue. Analyzing the factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows only updates to version numbers in `Cargo.lock` and `Cargo.toml` for the `foundry-compilers` and related packages (from 0.13.2 to 0.13.3). This suggests that the actual fix is likely contained within the updated dependency, and no direct code modifications in the main codebase are shown. The change is minimal and does not impact multiple modules or the system's architecture directly in the provided context. It appears to be a dependency update to incorporate a fix already implemented upstream.\n\n2. **Number of Technical Concepts:** Solving this issue likely requires basic familiarity with Rust dependency management (Cargo) and an understanding of how Solidity compilation works in the Foundry ecosystem. The concepts involved are relatively straightforward\u2014updating a dependency version and potentially understanding the parser logic for Solidity files if one were to investigate the fix in the dependency. No advanced algorithms, design patterns, or domain-specific knowledge beyond basic compiler/parser concepts are needed based on the diff.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, and the code changes provided do not indicate modifications to error handling logic. If the fix were to be implemented from scratch (not via a dependency update), one might need to consider edge cases like different types of event declarations or interactions with other top-level constructs in Solidity, but this is not reflected in the provided diff. The complexity of edge cases appears minimal in this context.\n\n4. **Overall Assessment:** Given that the change is a simple dependency update, the effort required is low, involving minimal code modification and basic understanding of the Foundry toolchain. Even if the fix within the dependency involves parser modifications (e.g., adding support for `EventDefinition` in the flattening logic), it would likely be a straightforward addition to an existing variant list, as hinted by the error message. Therefore, the difficulty is rated as Easy (0.25), as it requires understanding some code logic (in the dependency) and making a simple update, without significant impact on the broader codebase or complex edge case handling.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Macro define_sql_function imposes diesel as direct dependency\n<!--\nIf you want to report a bug, we added some points below which help us track down the problem faster.\n-->\n\n## Setup\n\n### Versions\n\n- **Rust:**\nrustc 1.80.1 (3f5fd8dd4 2024-08-06)\nbinary: rustc\ncommit-hash: 3f5fd8dd41153bc5fdca9427e9e05be2c767ba23\ncommit-date: 2024-08-06\nhost: x86_64-unknown-linux-gnu\nrelease: 1.80.1\nLLVM version: 18.1.7\n- **Diesel:** b170af753c\n- **Database:** N/A\n- **Operating System** N/A\n\n### Feature Flags\n\n- **diesel:** N/A\n\n## Problem Description\nUse of `::diesel` in procedural macro `define_sql_function` forces to diesel as direct dependency instead of simply in scope, which is bothersome when diesel is re-exported.\n\n### What are you trying to accomplish?\nUse macro `define_sql_function` without having diesel as direct dependency\n\n### What is the expected output?\nCode compiles\n\n### What is the actual output?\n```\nerror[E0433]: failed to resolve: could not find `diesel` in the list of imported crates\n  --> /home/elrendio/Stockly/Main/operations/Service/src/supply/Suppliers/src/auto_block/high_at_submission_price_changes.rs:95:4\n   |\n95 | /             diesel::define_sql_function! {\n96 | |                 fn greatest(a: Nullable<Timestamptz>, b: Nullable<Timestamptz>, c: Timestamptz) -> Timestamptz;\n97 | |             }\n   | |_____________^ could not find `diesel` in the list of imported crates\n   |\n   = note: this error originates in the macro `diesel::define_sql_function` (in Nightly builds, run with -Z macro-backtrace for more info)\nhelp: consider importing this module\n\n```\n\n### Are you seeing any additional errors?\n /\n\n### Steps to reproduce\n\n1. Create a crate that re-exports diesel, let's call it diesel_helpers\n2. Create a second crate with dependency `diesel_helpers`, let's call it diesel_consumer\n3. In crate diesel_consumer, use diesel_helpers::diesel::define_sql_function\n\n## Checklist\n\n- [x] I have already looked over the [issue tracker](https://github.com/diesel-rs/diesel/issues) and the [discussion forum](https://github.com/diesel-rs/diesel/discussions) for similar possible closed issues.\n- [x] This issue can be reproduced on Rust's stable channel. (Your issue will be\n  closed if this is not the case)\n- [x] This issue can be reproduced without requiring a third party crate\n\n", "patch": "diff --git a/diesel_derives/src/sql_function.rs b/diesel_derives/src/sql_function.rs\nindex 1f76d9d9adfd..c35a3fe95930 100644\n--- a/diesel_derives/src/sql_function.rs\n+++ b/diesel_derives/src/sql_function.rs\n@@ -405,7 +405,7 @@ pub(crate) fn expand(input: SqlFunctionDecl, legacy_helper_type_and_module: bool\n                     #[doc = #helper_type_doc]\n                     pub type #fn_name #ty_generics = #internals_module_name::#fn_name <\n                         #(#type_args,)*\n-                        #(<#arg_name as ::diesel::expression::AsExpression<#arg_type>>::Expression,)*\n+                        #(<#arg_name as diesel::expression::AsExpression<#arg_type>>::Expression,)*\n                     >;\n                 }),\n                 quote! { #fn_name },\n@@ -419,7 +419,7 @@ pub(crate) fn expand(input: SqlFunctionDecl, legacy_helper_type_and_module: bool\n         pub #fn_token #fn_name #impl_generics (#(#args_iter,)*)\n             -> #return_type_path #ty_generics\n         #where_clause\n-            #(#arg_name: ::diesel::expression::AsExpression<#arg_type>,)*\n+            #(#arg_name: diesel::expression::AsExpression<#arg_type>,)*\n         {\n             #internals_module_name::#fn_name {\n                 #(#arg_struct_assign,)*\n", "instance_id": "diesel-rs__diesel-4318", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `define_sql_function` macro in Diesel forces a direct dependency on the `diesel` crate, which becomes problematic when Diesel is re-exported through another crate. The goal (avoiding a direct dependency) and the expected output (code compiles) are explicitly stated. Steps to reproduce the issue are provided, which adds clarity. However, there are minor ambiguities and missing details. For instance, the problem statement does not elaborate on the broader context or potential implications of this dependency issue (e.g., why re-exporting is common or necessary in their use case). Additionally, edge cases or constraints related to the macro usage are not mentioned, which could be relevant for a comprehensive understanding. Overall, while the core issue is clear, some minor details are missing, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided diff shows that the changes are localized to a single file (`sql_function.rs`) within the Diesel derives crate. The modifications involve updating a few lines to remove the explicit `::diesel` path, replacing it with a relative `diesel` reference. This is a small, focused change that does not impact the broader architecture of the codebase or require modifications across multiple modules. The amount of code change is minimal, consisting of just a few targeted replacements.\n\n2. **Technical Concepts Involved:** Solving this issue requires a basic understanding of Rust's procedural macros and how crate dependencies are resolved in Rust. Familiarity with Diesel's macro system and the `AsExpression` trait is necessary, but these are not overly complex concepts for someone with moderate Rust experience. No advanced algorithms, design patterns, or domain-specific knowledge beyond Rust's module system are required.\n\n3. **Edge Cases and Error Handling:** The problem statement does not mention specific edge cases or error handling requirements, and the code changes do not introduce or modify error handling logic. The focus is purely on resolving a dependency resolution issue, which appears straightforward without additional complications.\n\n4. **Overall Complexity:** While the issue touches on Rust's dependency and macro systems, it does not require deep architectural changes or extensive debugging. The solution is a simple adjustment to the macro's path resolution, making it accessible to developers with a basic-to-intermediate understanding of Rust. However, it is slightly more involved than a trivial fix (e.g., a typo), as it requires understanding the context of re-exported dependencies and macro expansion.\n\nGiven these considerations, I assign a difficulty score of 0.35, placing it on the higher end of the \"Easy\" category due to the need for some familiarity with Rust's macro and dependency systems, but still well within the realm of straightforward fixes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "chore(deps): bump getrandom from 0.2.15 to 0.3.1 in the patch-updates group across 1 directory\nBumps the patch-updates group with 1 update in the / directory: [getrandom](https://github.com/rust-random/getrandom).\n\nUpdates `getrandom` from 0.2.15 to 0.3.1\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/rust-random/getrandom/blob/master/CHANGELOG.md\">getrandom's changelog</a>.</em></p>\n<blockquote>\n<h2>[0.3.1] - 2025-01-28</h2>\n<h3>Fixed</h3>\n<ul>\n<li>Build error on Android <a href=\"https://redirect.github.com/rust-random/getrandom/issues/588\">#588</a></li>\n</ul>\n<p><a href=\"https://redirect.github.com/rust-random/getrandom/issues/588\">#588</a>: <a href=\"https://redirect.github.com/rust-random/getrandom/pull/588\">rust-random/getrandom#588</a></p>\n<h2>[0.3.0] - 2025-01-25</h2>\n<h3>Breaking Changes</h3>\n<h4>Changed</h4>\n<ul>\n<li>Bump MSRV to 1.63 <a href=\"https://redirect.github.com/rust-random/getrandom/issues/542\">#542</a></li>\n<li>Rename <code>getrandom</code> and <code>getrandom_uninit</code> functions to <code>fill</code> and <code>fill_uninit</code> respectively <a href=\"https://redirect.github.com/rust-random/getrandom/issues/532\">#532</a></li>\n</ul>\n<h4>Removed</h4>\n<ul>\n<li><code>wasm32-wasi</code> target support (use <code>wasm32-wasip1</code> or <code>wasm32-wasip2</code> instead) <a href=\"https://redirect.github.com/rust-random/getrandom/issues/499\">#499</a></li>\n<li><code>linux_disable_fallback</code>, <code>rdrand</code>, <code>js</code>, <code>test-in-browser</code>, and <code>custom</code> crate features\nin favor of configuration flags <a href=\"https://redirect.github.com/rust-random/getrandom/issues/504\">#504</a></li>\n<li><code>register_custom_getrandom!</code> macro <a href=\"https://redirect.github.com/rust-random/getrandom/issues/504\">#504</a></li>\n<li>Implementation of <code>From&lt;NonZeroU32&gt;</code> for <code>Error</code> and <code>Error::code</code> method <a href=\"https://redirect.github.com/rust-random/getrandom/issues/507\">#507</a></li>\n<li>Internet Explorer 11 support <a href=\"https://redirect.github.com/rust-random/getrandom/issues/554\">#554</a></li>\n<li>Target-specific assocciated <code>Error</code> constants <a href=\"https://redirect.github.com/rust-random/getrandom/issues/562\">#562</a></li>\n</ul>\n<h3>Changed</h3>\n<ul>\n<li>Use <code>ProcessPrng</code> on Windows 10 and up, and use <code>RtlGenRandom</code> on older Windows versions <a href=\"https://redirect.github.com/rust-random/getrandom/issues/415\">#415</a></li>\n<li>Do not use locale-specific <code>strerror_r</code> for retrieving error code descriptions <a href=\"https://redirect.github.com/rust-random/getrandom/issues/440\">#440</a></li>\n<li>Avoid assuming <code>usize</code> is the native word size in the <code>rdrand</code> backend <a href=\"https://redirect.github.com/rust-random/getrandom/issues/442\">#442</a></li>\n<li>Do not read from <code>errno</code> when <code>libc</code> did not indicate error on Solaris <a href=\"https://redirect.github.com/rust-random/getrandom/issues/448\">#448</a></li>\n<li>Switch from <code>libpthread</code>'s mutex to <code>futex</code> on Linux and to <code>nanosleep</code>-based wait loop\non other targets in the <code>use_file</code> backend <a href=\"https://redirect.github.com/rust-random/getrandom/issues/490\">#490</a></li>\n<li>Do not retry on <code>EAGAIN</code> while polling <code>/dev/random</code> on Linux <a href=\"https://redirect.github.com/rust-random/getrandom/issues/522\">#522</a></li>\n<li>Remove separate codepath for Node.js in the <code>wasm_js</code> backend\n(bumps minimum supported Node.js version to v19) <a href=\"https://redirect.github.com/rust-random/getrandom/issues/557\">#557</a></li>\n<li>Use <code>js_namespace</code> in the <code>wasm_js</code> backend <a href=\"https://redirect.github.com/rust-random/getrandom/issues/559\">#559</a></li>\n</ul>\n<h3>Added</h3>\n<ul>\n<li><code>wasm32-wasip1</code> and <code>wasm32-wasip2</code> support <a href=\"https://redirect.github.com/rust-random/getrandom/issues/499\">#499</a></li>\n<li><code>getrandom_backend</code> configuration flag for selection of opt-in backends <a href=\"https://redirect.github.com/rust-random/getrandom/issues/504\">#504</a></li>\n<li><code>Error::new_custom</code> method <a href=\"https://redirect.github.com/rust-random/getrandom/issues/507\">#507</a></li>\n<li><code>rndr</code> opt-in backend <a href=\"https://redirect.github.com/rust-random/getrandom/issues/512\">#512</a></li>\n<li>Automatic MemorySanitizer support <a href=\"https://redirect.github.com/rust-random/getrandom/issues/521\">#521</a> <a href=\"https://redirect.github.com/rust-random/getrandom/issues/571\">#571</a></li>\n<li><code>u32</code> and <code>u64</code> functions for generating random values of the respective type <a href=\"https://redirect.github.com/rust-random/getrandom/issues/544\">#544</a></li>\n<li><code>wasm32v1-none</code> support in the <code>wasm_js</code> backend <a href=\"https://redirect.github.com/rust-random/getrandom/issues/560\">#560</a></li>\n<li><code>wasm_js</code> crate feature which allows users to enable the <code>wasm_js</code> opt-in backend <a href=\"https://redirect.github.com/rust-random/getrandom/issues/574\">#574</a></li>\n</ul>\n<h3>Fixed</h3>\n<ul>\n<li>NetBSD fallback code based on <code>KERN_ARND</code> <a href=\"https://redirect.github.com/rust-random/getrandom/issues/555\">#555</a></li>\n</ul>\n<p><a href=\"https://redirect.github.com/rust-random/getrandom/issues/415\">#415</a>: <a href=\"https://redirect.github.com/rust-random/getrandom/pull/415\">rust-random/getrandom#415</a></p>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/2bb39598632af0097426815487f5b33b07edacbc\"><code>2bb3959</code></a> Release v0.3.1 (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/589\">#589</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/ac379dacbfbcbcc14747a0e014d84d6f24c8140b\"><code>ac379da</code></a> Fix build error on Android (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/588\">#588</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/aa9636349418ffa58bca6e2884d1bbc63fa1d2fa\"><code>aa96363</code></a> Add benchmarks for u32/u64 functions. (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/582\">#582</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/e38855c69882c2c1ff7438f9292512a0b91275fe\"><code>e38855c</code></a> Fix release date for v0.3.0 (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/587\">#587</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/d5683740ef9cd2d0ebfed73f2a629c0ebb47b5e8\"><code>d568374</code></a> Release v0.3.0 (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/563\">#563</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/5e62ce9fadb401539a08b329e4cbd98cc6393f60\"><code>5e62ce9</code></a> rndr: minor tweaks (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/586\">#586</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/3d0c01c35231b40f58b49f4455797c85c2e8c042\"><code>3d0c01c</code></a> ci: disable broken Solaris and Emscripten cross jobs (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/585\">#585</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/7cde98b9770eaefe1bcf6194f7ac55ce28d9b53e\"><code>7cde98b</code></a> Add <code>wasm_js</code> feature flag (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/574\">#574</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/6be40128a7c19bac658c6e85e808a6b5eadfd6b2\"><code>6be4012</code></a> Web: use safe method to copy to <code>MaybeUninit</code> (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/584\">#584</a>)</li>\n<li><a href=\"https://github.com/rust-random/getrandom/commit/2648e60c5d27979f45043e379b1af7bee5e300a9\"><code>2648e60</code></a> ESP-IDF: Enable unconditionally (<a href=\"https://redirect.github.com/rust-random/getrandom/issues/583\">#583</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/rust-random/getrandom/compare/v0.2.15...v0.3.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=getrandom&package-manager=cargo&previous-version=0.2.15&new-version=0.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore <dependency name> major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)\n- `@dependabot ignore <dependency name> minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)\n- `@dependabot ignore <dependency name>` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)\n- `@dependabot unignore <dependency name>` will remove all of the ignore conditions of the specified dependency\n- `@dependabot unignore <dependency name> <ignore condition>` will remove the ignore condition of the specified dependency and ignore conditions\n\n\n</details>\n", "patch": "diff --git a/Cargo.lock b/Cargo.lock\nindex 89fccee596..004351f012 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -4595,7 +4595,6 @@ dependencies = [\n  \"env_logger\",\n  \"fern\",\n  \"flume\",\n- \"getrandom 0.2.15\",\n  \"glam\",\n  \"ktx2\",\n  \"log\",\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 61d9c74a3a..a3761ce5da 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -98,7 +98,6 @@ env_logger = \"0.11\"\n fern = \"0.7\"\n flume = \"0.11\"\n futures-lite = \"2\"\n-getrandom = \"0.2\"\n glam = \"0.29\"\n hashbrown = { version = \"0.14.5\", default-features = false, features = [\n     \"ahash\",\ndiff --git a/examples/features/Cargo.toml b/examples/features/Cargo.toml\nindex 3165f2af27..11cf6b2115 100644\n--- a/examples/features/Cargo.toml\n+++ b/examples/features/Cargo.toml\n@@ -35,7 +35,6 @@ bytemuck.workspace = true\n cfg-if.workspace = true\n encase = { workspace = true, features = [\"glam\"] }\n flume.workspace = true\n-getrandom.workspace = true\n glam = { workspace = true, features = [\"bytemuck\"] }\n ktx2.workspace = true\n log.workspace = true\ndiff --git a/examples/features/src/repeated_compute/mod.rs b/examples/features/src/repeated_compute/mod.rs\nindex 4f7b37b99f..a31a456e63 100644\n--- a/examples/features/src/repeated_compute/mod.rs\n+++ b/examples/features/src/repeated_compute/mod.rs\n@@ -5,6 +5,8 @@\n //! hello-compute example does not such as mapping buffers\n //! and why use the async channels.\n \n+use nanorand::Rng;\n+\n const OVERFLOW: u32 = 0xffffffff;\n \n async fn run() {\n@@ -13,7 +15,7 @@ async fn run() {\n \n     for _ in 0..10 {\n         for p in numbers.iter_mut() {\n-            *p = generate_rand() as u32;\n+            *p = nanorand::tls_rng().generate::<u16>() as u32;\n         }\n \n         compute(&mut numbers, &context).await;\n@@ -29,12 +31,6 @@ async fn run() {\n     }\n }\n \n-fn generate_rand() -> u16 {\n-    let mut bytes = [0u8; 2];\n-    getrandom::getrandom(&mut bytes[..]).unwrap();\n-    u16::from_le_bytes(bytes)\n-}\n-\n async fn compute(local_buffer: &mut [u32], context: &WgpuContext) {\n     log::info!(\"Beginning GPU compute on data {local_buffer:?}.\");\n     // Local buffer contents -> GPU storage buffer\n", "instance_id": "gfx-rs__wgpu-7194", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in its intent to update the `getrandom` dependency from version 0.2.15 to 0.3.1 as part of a patch update. It provides detailed changelog information, commit history, and context about the dependency update, which helps in understanding the scope of the change. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly discuss the potential impact of breaking changes introduced in `getrandom` 0.3.0 (e.g., function renames, removed features, or MSRV bump) on the codebase, nor does it specify whether compatibility issues are expected or how they should be handled. Additionally, while the code changes are provided, the problem statement does not clarify the motivation behind replacing `getrandom` with `nanorand` in the example code, leaving some room for interpretation. Overall, the statement is valid and clear but lacks minor details regarding potential challenges or specific instructions for handling breaking changes.", "difficulty_explanation": "The difficulty of this task falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are relatively localized, primarily involving updates to dependency versions in `Cargo.toml` and `Cargo.lock`, and a small modification in the example code to replace `getrandom` usage with `nanorand`. The changes span a few files but do not impact the broader system architecture. The removal of `getrandom` as a direct dependency and the associated code change in the example are straightforward, though they require a basic understanding of the dependency management system in Rust (Cargo) and random number generation libraries.\n\n2. **Technical Concepts Involved:** The task requires basic familiarity with Rust's dependency management (Cargo), understanding of random number generation libraries (`getrandom` and `nanorand`), and minor code refactoring to adapt to a new library. The concepts are not complex for a developer with moderate Rust experience. However, one must be aware of the breaking changes in `getrandom` 0.3.0 (e.g., function renames, removed features) as documented in the changelog, even though the provided code changes sidestep this by switching to `nanorand`.\n\n3. **Edge Cases and Error Handling:** The problem statement and code changes do not explicitly mention specific edge cases or error handling requirements related to the dependency update. The switch to `nanorand` in the example code appears to be a simple replacement without additional error handling logic, which keeps the complexity low. However, a developer might need to consider potential compatibility issues or performance differences between `getrandom` and `nanorand`, though this is not explicitly required by the problem.\n\n4. **Overall Complexity:** The task involves a routine dependency update combined with a minor code adjustment. While it requires some understanding of the codebase and the libraries involved, it does not demand deep architectural changes, complex debugging, or advanced technical knowledge. The primary challenge lies in ensuring that the removal of `getrandom` and the introduction of `nanorand` do not introduce subtle bugs or compatibility issues in other parts of the codebase, but the provided changes suggest this has already been addressed.\n\nGiven these factors, a difficulty score of 0.25 reflects an easy task that requires minimal effort beyond basic dependency management and a small code modification, suitable for a developer with basic to intermediate Rust experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Assigning an array of `vec3(0.0)` to an array of `vec3f(0.0)` fails\n<!-- Thank you for filing this! Please read the [debugging tips](https://github.com/gfx-rs/wgpu/wiki/Debugging-wgpu-Applications).\nThat may let you investigate on your own, or provide additional information that helps us to assist.-->\n\n**Description**\nAssigning an array of `vec3(0.0)` to an array of `vec3f(0.0)` fails with an error about the types not matching.\n\n**Repro steps**\n\n```\nvar weird: array<vec3f, 1> = array( vec3(0.0) );\n```\n\n\n**Expected vs observed behavior**\nThey should be the same type, so it shouldn't fail.\n\n**Extra materials**\n\nThe error:\n\n```\nShader validation error: Function [0] 'strange' is invalid\n   \u250c\u2500 LandscapeMeshPipeline Shader:31:1\n   \u2502\n31 \u2502 \u256d fn strange() {\n32 \u2502 \u2502     var weird: array<vec3<f32>, 1> = array( vec3(0.0) );\n   \u2502 \u2502     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ naga::LocalVariable [0]\n   \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500^ naga::Function [0]\n   \u2502\n   = Local variable [0] 'weird' is invalid\n   = Initializer doesn't match the variable type\n\n\n      Function [0] 'strange' is invalid\n        Local variable [0] 'weird' is invalid\n          Initializer doesn't match the variable type\n```\n\n**Platform**\nUsing wgpu 24.0 and vulkan 1.4.303 on Linux.\n\n", "patch": "diff --git a/naga/src/back/pipeline_constants.rs b/naga/src/back/pipeline_constants.rs\nindex ef498b6554..7261c1d858 100644\n--- a/naga/src/back/pipeline_constants.rs\n+++ b/naga/src/back/pipeline_constants.rs\n@@ -75,6 +75,7 @@ pub fn process_overrides<'a>(\n     let mut adjusted_constant_initializers = HashSet::with_capacity(module.constants.len());\n \n     let mut global_expression_kind_tracker = crate::proc::ExpressionKindTracker::new();\n+    let mut layouter = crate::proc::Layouter::default();\n \n     // An iterator through the original overrides table, consumed in\n     // approximate tandem with the global expressions.\n@@ -147,6 +148,7 @@ pub fn process_overrides<'a>(\n         let mut evaluator = ConstantEvaluator::for_wgsl_module(\n             &mut module,\n             &mut global_expression_kind_tracker,\n+            &mut layouter,\n             false,\n         );\n         adjust_expr(&adjusted_global_expressions, &mut expr);\n@@ -186,13 +188,13 @@ pub fn process_overrides<'a>(\n \n     let mut functions = mem::take(&mut module.functions);\n     for (_, function) in functions.iter_mut() {\n-        process_function(&mut module, &override_map, function)?;\n+        process_function(&mut module, &override_map, &mut layouter, function)?;\n     }\n     module.functions = functions;\n \n     let mut entry_points = mem::take(&mut module.entry_points);\n     for ep in entry_points.iter_mut() {\n-        process_function(&mut module, &override_map, &mut ep.function)?;\n+        process_function(&mut module, &override_map, &mut layouter, &mut ep.function)?;\n         process_workgroup_size_override(&mut module, &adjusted_global_expressions, ep)?;\n     }\n     module.entry_points = entry_points;\n@@ -365,6 +367,7 @@ fn process_override(\n fn process_function(\n     module: &mut Module,\n     override_map: &HandleVec<Override, Handle<Constant>>,\n+    layouter: &mut crate::proc::Layouter,\n     function: &mut Function,\n ) -> Result<(), ConstantEvaluatorError> {\n     // A map from original local expression handles to\n@@ -389,6 +392,7 @@ fn process_function(\n         module,\n         &mut function.expressions,\n         &mut local_expression_kind_tracker,\n+        layouter,\n         &mut emitter,\n         &mut block,\n         false,\ndiff --git a/naga/src/front/glsl/context.rs b/naga/src/front/glsl/context.rs\nindex b4cb1c874e..04e89e8397 100644\n--- a/naga/src/front/glsl/context.rs\n+++ b/naga/src/front/glsl/context.rs\n@@ -8,9 +8,9 @@ use super::{\n     Frontend, Result,\n };\n use crate::{\n-    front::Typifier, proc::Emitter, AddressSpace, Arena, BinaryOperator, Block, Expression,\n-    FastHashMap, FunctionArgument, Handle, Literal, LocalVariable, RelationalFunction, Scalar,\n-    Span, Statement, Type, TypeInner, VectorSize,\n+    front::Typifier, proc::Emitter, proc::Layouter, AddressSpace, Arena, BinaryOperator, Block,\n+    Expression, FastHashMap, FunctionArgument, Handle, Literal, LocalVariable, RelationalFunction,\n+    Scalar, Span, Statement, Type, TypeInner, VectorSize,\n };\n use std::ops::Index;\n \n@@ -72,6 +72,7 @@ pub struct Context<'a> {\n \n     pub const_typifier: Typifier,\n     pub typifier: Typifier,\n+    layouter: Layouter,\n     emitter: Emitter,\n     stmt_ctx: Option<StmtContext>,\n     pub body: Block,\n@@ -103,6 +104,7 @@ impl<'a> Context<'a> {\n \n             const_typifier: Typifier::new(),\n             typifier: Typifier::new(),\n+            layouter: Layouter::default(),\n             emitter: Emitter::default(),\n             stmt_ctx: Some(StmtContext::new()),\n             body: Block::new(),\n@@ -260,12 +262,14 @@ impl<'a> Context<'a> {\n             crate::proc::ConstantEvaluator::for_glsl_module(\n                 self.module,\n                 self.global_expression_kind_tracker,\n+                &mut self.layouter,\n             )\n         } else {\n             crate::proc::ConstantEvaluator::for_glsl_function(\n                 self.module,\n                 &mut self.expressions,\n                 &mut self.local_expression_kind_tracker,\n+                &mut self.layouter,\n                 &mut self.emitter,\n                 &mut self.body,\n             )\ndiff --git a/naga/src/front/wgsl/lower/construction.rs b/naga/src/front/wgsl/lower/construction.rs\nindex e52d4776ab..ee973fb9f1 100644\n--- a/naga/src/front/wgsl/lower/construction.rs\n+++ b/naga/src/front/wgsl/lower/construction.rs\n@@ -490,8 +490,8 @@ impl<'source> Lowerer<'source, '_> {\n                         NonZeroU32::new(u32::try_from(components.len()).unwrap()).unwrap(),\n                     ),\n                     stride: {\n-                        self.layouter.update(ctx.module.to_ctx()).unwrap();\n-                        self.layouter[base].to_stride()\n+                        ctx.layouter.update(ctx.module.to_ctx()).unwrap();\n+                        ctx.layouter[base].to_stride()\n                     },\n                 };\n                 let ty = ctx.ensure_type_exists(inner);\n@@ -616,8 +616,8 @@ impl<'source> Lowerer<'source, '_> {\n                 let base = self.resolve_ast_type(base, &mut ctx.as_global())?;\n                 let size = self.array_size(size, &mut ctx.as_global())?;\n \n-                self.layouter.update(ctx.module.to_ctx()).unwrap();\n-                let stride = self.layouter[base].to_stride();\n+                ctx.layouter.update(ctx.module.to_ctx()).unwrap();\n+                let stride = ctx.layouter[base].to_stride();\n \n                 let ty = ctx.ensure_type_exists(crate::TypeInner::Array { base, size, stride });\n                 Constructor::Type(ty)\ndiff --git a/naga/src/front/wgsl/lower/mod.rs b/naga/src/front/wgsl/lower/mod.rs\nindex e405d3e56f..a1bb65533f 100644\n--- a/naga/src/front/wgsl/lower/mod.rs\n+++ b/naga/src/front/wgsl/lower/mod.rs\n@@ -87,6 +87,8 @@ pub struct GlobalContext<'source, 'temp, 'out> {\n \n     const_typifier: &'temp mut Typifier,\n \n+    layouter: &'temp mut Layouter,\n+\n     global_expression_kind_tracker: &'temp mut crate::proc::ExpressionKindTracker,\n }\n \n@@ -98,6 +100,7 @@ impl<'source> GlobalContext<'source, '_, '_> {\n             types: self.types,\n             module: self.module,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             expr_type: ExpressionContextType::Constant(None),\n             global_expression_kind_tracker: self.global_expression_kind_tracker,\n         }\n@@ -110,6 +113,7 @@ impl<'source> GlobalContext<'source, '_, '_> {\n             types: self.types,\n             module: self.module,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             expr_type: ExpressionContextType::Override,\n             global_expression_kind_tracker: self.global_expression_kind_tracker,\n         }\n@@ -165,6 +169,7 @@ pub struct StatementContext<'source, 'temp, 'out> {\n \n     const_typifier: &'temp mut Typifier,\n     typifier: &'temp mut Typifier,\n+    layouter: &'temp mut Layouter,\n     function: &'out mut crate::Function,\n     /// Stores the names of expressions that are assigned in `let` statement\n     /// Also stores the spans of the names, for use in errors.\n@@ -198,6 +203,7 @@ impl<'a, 'temp> StatementContext<'a, 'temp, '_> {\n             types: self.types,\n             ast_expressions: self.ast_expressions,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             global_expression_kind_tracker: self.global_expression_kind_tracker,\n             module: self.module,\n             expr_type: ExpressionContextType::Constant(Some(LocalExpressionContext {\n@@ -224,6 +230,7 @@ impl<'a, 'temp> StatementContext<'a, 'temp, '_> {\n             types: self.types,\n             ast_expressions: self.ast_expressions,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             global_expression_kind_tracker: self.global_expression_kind_tracker,\n             module: self.module,\n             expr_type: ExpressionContextType::Runtime(LocalExpressionContext {\n@@ -244,6 +251,7 @@ impl<'a, 'temp> StatementContext<'a, 'temp, '_> {\n             types: self.types,\n             module: self.module,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             global_expression_kind_tracker: self.global_expression_kind_tracker,\n         }\n     }\n@@ -364,6 +372,7 @@ pub struct ExpressionContext<'source, 'temp, 'out> {\n     ///\n     /// [`module::global_expressions`]: crate::Module::global_expressions\n     const_typifier: &'temp mut Typifier,\n+    layouter: &'temp mut Layouter,\n     global_expression_kind_tracker: &'temp mut crate::proc::ExpressionKindTracker,\n \n     /// Whether we are lowering a constant expression or a general\n@@ -379,6 +388,7 @@ impl<'source, 'temp, 'out> ExpressionContext<'source, 'temp, 'out> {\n             types: self.types,\n             ast_expressions: self.ast_expressions,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             module: self.module,\n             expr_type: ExpressionContextType::Constant(match self.expr_type {\n                 ExpressionContextType::Runtime(ref mut local_expression_context)\n@@ -406,6 +416,7 @@ impl<'source, 'temp, 'out> ExpressionContext<'source, 'temp, 'out> {\n             types: self.types,\n             module: self.module,\n             const_typifier: self.const_typifier,\n+            layouter: self.layouter,\n             global_expression_kind_tracker: self.global_expression_kind_tracker,\n         }\n     }\n@@ -416,6 +427,7 @@ impl<'source, 'temp, 'out> ExpressionContext<'source, 'temp, 'out> {\n                 self.module,\n                 &mut rctx.function.expressions,\n                 rctx.local_expression_kind_tracker,\n+                self.layouter,\n                 rctx.emitter,\n                 rctx.block,\n                 false,\n@@ -425,6 +437,7 @@ impl<'source, 'temp, 'out> ExpressionContext<'source, 'temp, 'out> {\n                     self.module,\n                     &mut rctx.function.expressions,\n                     rctx.local_expression_kind_tracker,\n+                    self.layouter,\n                     rctx.emitter,\n                     rctx.block,\n                     true,\n@@ -433,11 +446,13 @@ impl<'source, 'temp, 'out> ExpressionContext<'source, 'temp, 'out> {\n             ExpressionContextType::Constant(None) => ConstantEvaluator::for_wgsl_module(\n                 self.module,\n                 self.global_expression_kind_tracker,\n+                self.layouter,\n                 false,\n             ),\n             ExpressionContextType::Override => ConstantEvaluator::for_wgsl_module(\n                 self.module,\n                 self.global_expression_kind_tracker,\n+                self.layouter,\n                 true,\n             ),\n         }\n@@ -1016,15 +1031,11 @@ impl SubgroupGather {\n \n pub struct Lowerer<'source, 'temp> {\n     index: &'temp Index<'source>,\n-    layouter: Layouter,\n }\n \n impl<'source, 'temp> Lowerer<'source, 'temp> {\n-    pub fn new(index: &'temp Index<'source>) -> Self {\n-        Self {\n-            index,\n-            layouter: Layouter::default(),\n-        }\n+    pub const fn new(index: &'temp Index<'source>) -> Self {\n+        Self { index }\n     }\n \n     pub fn lower(\n@@ -1043,6 +1054,7 @@ impl<'source, 'temp> Lowerer<'source, 'temp> {\n             types: &tu.types,\n             module: &mut module,\n             const_typifier: &mut Typifier::new(),\n+            layouter: &mut Layouter::default(),\n             global_expression_kind_tracker: &mut crate::proc::ExpressionKindTracker::new(),\n         };\n \n@@ -1299,6 +1311,7 @@ impl<'source, 'temp> Lowerer<'source, 'temp> {\n             ast_expressions: ctx.ast_expressions,\n             const_typifier: ctx.const_typifier,\n             typifier: &mut typifier,\n+            layouter: ctx.layouter,\n             function: &mut function,\n             named_expressions: &mut named_expressions,\n             types: ctx.types,\n@@ -3051,10 +3064,10 @@ impl<'source, 'temp> Lowerer<'source, 'temp> {\n         for member in s.members.iter() {\n             let ty = self.resolve_ast_type(member.ty, ctx)?;\n \n-            self.layouter.update(ctx.module.to_ctx()).unwrap();\n+            ctx.layouter.update(ctx.module.to_ctx()).unwrap();\n \n-            let member_min_size = self.layouter[ty].size;\n-            let member_min_alignment = self.layouter[ty].alignment;\n+            let member_min_size = ctx.layouter[ty].size;\n+            let member_min_alignment = ctx.layouter[ty].alignment;\n \n             let member_size = if let Some(size_expr) = member.size {\n                 let (size, span) = self.const_u32(size_expr, &mut ctx.as_const())?;\n@@ -3258,8 +3271,8 @@ impl<'source, 'temp> Lowerer<'source, 'temp> {\n                 let base = self.resolve_ast_type(base, ctx)?;\n                 let size = self.array_size(size, ctx)?;\n \n-                self.layouter.update(ctx.module.to_ctx()).unwrap();\n-                let stride = self.layouter[base].to_stride();\n+                ctx.layouter.update(ctx.module.to_ctx()).unwrap();\n+                let stride = ctx.layouter[base].to_stride();\n \n                 crate::TypeInner::Array { base, size, stride }\n             }\ndiff --git a/naga/src/proc/constant_evaluator.rs b/naga/src/proc/constant_evaluator.rs\nindex c8911077b7..bf97bedc34 100644\n--- a/naga/src/proc/constant_evaluator.rs\n+++ b/naga/src/proc/constant_evaluator.rs\n@@ -312,6 +312,8 @@ pub struct ConstantEvaluator<'a> {\n \n     /// Tracks the constness of expressions residing in [`Self::expressions`]\n     expression_kind_tracker: &'a mut ExpressionKindTracker,\n+\n+    layouter: &'a mut crate::proc::Layouter,\n }\n \n #[derive(Debug)]\n@@ -594,6 +596,7 @@ impl<'a> ConstantEvaluator<'a> {\n     pub fn for_wgsl_module(\n         module: &'a mut crate::Module,\n         global_expression_kind_tracker: &'a mut ExpressionKindTracker,\n+        layouter: &'a mut crate::proc::Layouter,\n         in_override_ctx: bool,\n     ) -> Self {\n         Self::for_module(\n@@ -604,6 +607,7 @@ impl<'a> ConstantEvaluator<'a> {\n             }),\n             module,\n             global_expression_kind_tracker,\n+            layouter,\n         )\n     }\n \n@@ -614,11 +618,13 @@ impl<'a> ConstantEvaluator<'a> {\n     pub fn for_glsl_module(\n         module: &'a mut crate::Module,\n         global_expression_kind_tracker: &'a mut ExpressionKindTracker,\n+        layouter: &'a mut crate::proc::Layouter,\n     ) -> Self {\n         Self::for_module(\n             Behavior::Glsl(GlslRestrictions::Const),\n             module,\n             global_expression_kind_tracker,\n+            layouter,\n         )\n     }\n \n@@ -626,6 +632,7 @@ impl<'a> ConstantEvaluator<'a> {\n         behavior: Behavior<'a>,\n         module: &'a mut crate::Module,\n         global_expression_kind_tracker: &'a mut ExpressionKindTracker,\n+        layouter: &'a mut crate::proc::Layouter,\n     ) -> Self {\n         Self {\n             behavior,\n@@ -634,6 +641,7 @@ impl<'a> ConstantEvaluator<'a> {\n             overrides: &module.overrides,\n             expressions: &mut module.global_expressions,\n             expression_kind_tracker: global_expression_kind_tracker,\n+            layouter,\n         }\n     }\n \n@@ -645,6 +653,7 @@ impl<'a> ConstantEvaluator<'a> {\n         module: &'a mut crate::Module,\n         expressions: &'a mut Arena<Expression>,\n         local_expression_kind_tracker: &'a mut ExpressionKindTracker,\n+        layouter: &'a mut crate::proc::Layouter,\n         emitter: &'a mut super::Emitter,\n         block: &'a mut crate::Block,\n         is_const: bool,\n@@ -665,6 +674,7 @@ impl<'a> ConstantEvaluator<'a> {\n             overrides: &module.overrides,\n             expressions,\n             expression_kind_tracker: local_expression_kind_tracker,\n+            layouter,\n         }\n     }\n \n@@ -676,6 +686,7 @@ impl<'a> ConstantEvaluator<'a> {\n         module: &'a mut crate::Module,\n         expressions: &'a mut Arena<Expression>,\n         local_expression_kind_tracker: &'a mut ExpressionKindTracker,\n+        layouter: &'a mut crate::proc::Layouter,\n         emitter: &'a mut super::Emitter,\n         block: &'a mut crate::Block,\n     ) -> Self {\n@@ -690,6 +701,7 @@ impl<'a> ConstantEvaluator<'a> {\n             overrides: &module.overrides,\n             expressions,\n             expression_kind_tracker: local_expression_kind_tracker,\n+            layouter,\n         }\n     }\n \n@@ -1718,7 +1730,11 @@ impl<'a> ConstantEvaluator<'a> {\n                 self.types.insert(Type { name: None, inner }, span)\n             }\n         };\n-        let new_base_stride = self.types[new_base].inner.size(self.to_ctx());\n+        let mut layouter = std::mem::take(self.layouter);\n+        layouter.update(self.to_ctx()).unwrap();\n+        *self.layouter = layouter;\n+\n+        let new_base_stride = self.layouter[new_base].to_stride();\n         let new_array_ty = self.types.insert(\n             Type {\n                 name: None,\n@@ -2567,6 +2583,7 @@ mod tests {\n             overrides: &overrides,\n             expressions: &mut global_expressions,\n             expression_kind_tracker,\n+            layouter: &mut crate::proc::Layouter::default(),\n         };\n \n         let res1 = solver\n@@ -2653,6 +2670,7 @@ mod tests {\n             overrides: &overrides,\n             expressions: &mut global_expressions,\n             expression_kind_tracker,\n+            layouter: &mut crate::proc::Layouter::default(),\n         };\n \n         let res = solver\n@@ -2771,6 +2789,7 @@ mod tests {\n             overrides: &overrides,\n             expressions: &mut global_expressions,\n             expression_kind_tracker,\n+            layouter: &mut crate::proc::Layouter::default(),\n         };\n \n         let root1 = Expression::AccessIndex { base, index: 1 };\n@@ -2864,6 +2883,7 @@ mod tests {\n             overrides: &overrides,\n             expressions: &mut global_expressions,\n             expression_kind_tracker,\n+            layouter: &mut crate::proc::Layouter::default(),\n         };\n \n         let solved_compose = solver\n@@ -2946,6 +2966,7 @@ mod tests {\n             overrides: &overrides,\n             expressions: &mut global_expressions,\n             expression_kind_tracker,\n+            layouter: &mut crate::proc::Layouter::default(),\n         };\n \n         let solved_compose = solver\n@@ -3034,6 +3055,7 @@ mod tests {\n             overrides: &overrides,\n             expressions: &mut global_expressions,\n             expression_kind_tracker,\n+            layouter: &mut crate::proc::Layouter::default(),\n         };\n \n         let solved_add = solver\n", "instance_id": "gfx-rs__wgpu-7112", "clarity": 2, "difficulty": 0.75, "clarity_explanation": "The problem statement is mostly clear in describing the issue: assigning an array of `vec3(0.0)` to an array of `vec3f(0.0)` results in a type mismatch error in a shader validation context using the `wgpu` library. The goal (to resolve the type mismatch) and the context (shader code with a specific error message) are provided, along with a minimal reproduction example. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly state whether the expected behavior is to allow implicit type conversion or to enforce stricter typing, nor does it clarify the broader implications of the fix (e.g., compatibility with other shader types or performance impacts). Additionally, edge cases or constraints (e.g., behavior with other vector types or array sizes) are not mentioned. Despite these minor gaps, the statement is actionable and provides enough information to understand the core issue, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem is rated as Hard (0.75) due to several factors. First, the scope and depth of code changes are significant, as they span multiple files and modules within the `naga` crate, which appears to be a shader translation and validation library. The changes involve modifications to core components like `ConstantEvaluator`, `Layouter`, and various lowering and processing functions, indicating a need to understand interactions across different parts of the codebase. Second, the number of technical concepts required to solve this is high, including Rust's ownership and borrowing model, shader language semantics (WGSL/GLSL), type system intricacies, and domain-specific knowledge of GPU programming and shader compilation. The `Layouter` struct, which seems to handle memory layout and stride calculations for types, is central to the fix, requiring a deep understanding of how type layouts are computed and applied during constant evaluation. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential complexities in handling various type constructions (e.g., arrays, vectors) and ensuring consistent behavior across different shader contexts, which may introduce subtle bugs if not handled carefully. Finally, the impact on the system's architecture is moderate, as the fix involves passing and updating a `Layouter` instance throughout the evaluation pipeline, which could affect performance or memory usage in downstream processes. Given the need for deep codebase familiarity, complex modifications, and domain expertise, a difficulty score of 0.75 is appropriate, placing it in the Hard category.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ambiguous associated item lint triggered when an enum member is named 'Error'\nOn `rustc 1.81.0-nightly (ed7e35f34 2024-07-06)` the following code will trigger the `ambiguous_associated_items` warning:\r\n\r\n```rust\r\n#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]\r\n#[repr(i32)]\r\npub enum SeverityNumber {\r\n    Error = 1,\r\n}\r\n```\r\n\r\nThe issue appears to be the generated `TryFrom` impl:\r\n\r\n```rust\r\nimpl ::core::convert::TryFrom<i32> for SeverityNumber {\r\n    type Error = ::prost::UnknownEnumValue;\r\n    fn try_from(value: i32) -> ::core::result::Result<SeverityNumber, Self::Error> {\r\n        match value {\r\n            1 => ::core::result::Result::Ok(SeverityNumber::Error),\r\n            _ => ::core::result::Result::Err(::prost::UnknownEnumValue(value)),\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIt needs to disambiguate `Self::Error`:\r\n\r\n```rust\r\nimpl ::core::convert::TryFrom<i32> for SeverityNumber {\r\n    type Error = ::prost::UnknownEnumValue;\r\n    fn try_from(value: i32) -> ::core::result::Result<SeverityNumber, <Self as ::core::convert::TryFrom<i32>>::Error> {\r\n        match value {\r\n            1 => ::core::result::Result::Ok(SeverityNumber::Error),\r\n            _ => ::core::result::Result::Err(::prost::UnknownEnumValue(value)),\r\n        }\r\n    }\r\n}\r\n```\n", "patch": "diff --git a/prost-derive/src/lib.rs b/prost-derive/src/lib.rs\nindex d06b079a2..3f8ceb474 100644\n--- a/prost-derive/src/lib.rs\n+++ b/prost-derive/src/lib.rs\n@@ -351,7 +351,7 @@ fn try_enumeration(input: TokenStream) -> Result<TokenStream, Error> {\n         impl #impl_generics ::core::convert::TryFrom::<i32> for #ident #ty_generics #where_clause {\n             type Error = ::prost::UnknownEnumValue;\n \n-            fn try_from(value: i32) -> ::core::result::Result<#ident, Self::Error> {\n+            fn try_from(value: i32) -> ::core::result::Result<#ident, ::prost::UnknownEnumValue> {\n                 match value {\n                     #(#try_from,)*\n                     _ => ::core::result::Result::Err(::prost::UnknownEnumValue(value)),\n", "instance_id": "tokio-rs__prost-1098", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a lint warning (`ambiguous_associated_items`) is triggered in Rust due to an enum variant named `Error` conflicting with the associated type `Error` in a `TryFrom` implementation. The goal is to disambiguate the type reference in the generated code. The statement provides relevant code snippets and the expected fix, which helps in understanding the issue. However, it lacks some minor details, such as whether this issue occurs only with specific Rust versions (beyond the mentioned nightly build), if there are other potential naming conflicts to consider, or if there are broader implications for other generated code. Additionally, edge cases or alternative scenarios (e.g., other reserved names causing similar issues) are not mentioned. Overall, the problem is valid and mostly clear, but these minor omissions prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to several factors. First, the scope of the code change is minimal, involving a single-line modification in a code generation template within the `prost-derive` library to explicitly use `::prost::UnknownEnumValue` instead of `Self::Error`. This change is localized to one file and does not impact the broader architecture or require understanding complex interactions across the codebase. Second, the technical concepts involved are relatively straightforward: familiarity with Rust's type system, associated types, and code generation using procedural macros. These are intermediate-level Rust concepts that do not require deep expertise. Third, there are no explicit edge cases or error handling requirements mentioned beyond the specific naming conflict, and the fix does not introduce new error handling logic. Overall, this is a simple bug fix that requires understanding some code logic and making a targeted modification, justifying a difficulty score of 0.25.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Remove the default unpaid action limit\nThe allowance for unpaid actions was not intended to be useful long-term. It was just a stopgap to allow us to deploy partial enforcement of ZIP 317 fees sooner, without that being blocked by \"all\" wallets supporting them for transaction creation.\r\n\r\nIt was understood when designing this mechanism that attackers would adapt to it, which they have, making the allowance essentially useless to the few legitimate users using wallets that are not paying these fees.\r\n\r\nTo avoid spending time on the changes to zcashd, I suggest just making `DEFAULT_BLOCK_UNPAID_ACTION_LIMIT` zero, which is a one-line change [1]. My impression is that mining pools do not override the default.\r\n\r\n[1] with ~1300 lines of test changes (#6900). \ud83d\ude2d\n", "patch": "diff --git a/doc/release-notes.md b/doc/release-notes.md\nindex 6fd5c2dd79f..93368dacc84 100644\n--- a/doc/release-notes.md\n+++ b/doc/release-notes.md\n@@ -4,4 +4,18 @@ release-notes at release time)\n Notable changes\n ===============\n \n+Mining\n+------\n+\n+- The default setting of `-blockunpaidactionlimit` is now zero, which has\n+  the effect of no longer allowing \"unpaid actions\" in [block production].\n+  This adapts to current network conditions. If you have overridden this\n+  setting as a miner, we recommend removing the override. This configuration\n+  option may be removed entirely in a future release.\n+\n+[block production]: https://zips.z.cash/zip-0317#block-production\n+\n+Platform Support\n+----------------\n+\n - Windows builds have been fixed.\ndiff --git a/src/main.cpp b/src/main.cpp\nindex b15c2efc814..1fad8ee9a62 100644\n--- a/src/main.cpp\n+++ b/src/main.cpp\n@@ -1921,7 +1921,8 @@ bool AcceptToMemoryPool(\n                     \", which is over the limit of %d. The conventional fee for this transaction is %d \" + MINOR_CURRENCY_UNIT,\n                     tx.GetHash().ToString(), nSize, nModifiedFees, nModifiedFees - nFees, nUnpaidActionCount,\n                     nTxUnpaidActionLimit, tx.GetConventionalFee());\n-            return state.DoS(0, false, REJECT_INSUFFICIENTFEE, \"tx unpaid action limit exceeded\");\n+            return state.DoS(0, false, REJECT_INSUFFICIENTFEE,\n+                             strprintf(\"tx unpaid action limit exceeded: %d action(s) exceeds limit of %d\", nUnpaidActionCount, nTxUnpaidActionLimit));\n         }\n \n         if (fRejectAbsurdFee && nFees > maxTxFee) {\ndiff --git a/src/policy/policy.h b/src/policy/policy.h\nindex 9b988db1d27..2d2ef4d31d8 100644\n--- a/src/policy/policy.h\n+++ b/src/policy/policy.h\n@@ -71,7 +71,7 @@ static const unsigned int STANDARD_NOT_MANDATORY_VERIFY_FLAGS = STANDARD_SCRIPT_\n  * set to 1000 zats, per ZIP 313.\n  *\n  * #6542 changed relaying policy to be more strict about enforcing minRelayTxFee.\n- * It also allowed `-minrelaytxfee=0`, which we are using to avoid some test\n+ * It also allowed `-minrelaytxfee=0`, which we were using to avoid some test\n  * breakage. But if the dust threshold rate were still set to three times the\n  * minRelayTxFee rate, then setting `-minrelaytxfee=0` would have the side effect\n  * of setting the dust threshold to zero, which is not intended.\ndiff --git a/src/wallet/asyncrpcoperation_saplingmigration.cpp b/src/wallet/asyncrpcoperation_saplingmigration.cpp\nindex 19052d8c436..414ce0f4021 100644\n--- a/src/wallet/asyncrpcoperation_saplingmigration.cpp\n+++ b/src/wallet/asyncrpcoperation_saplingmigration.cpp\n@@ -13,6 +13,7 @@\n \n #include <optional>\n #include <variant>\n+#include <zip317.h>\n \n const int MIGRATION_EXPIRY_DELTA = 450;\n \n@@ -159,13 +160,19 @@ bool AsyncRPCOperation_saplingmigration::main_impl() {\n             }\n         }\n         assert(changeAddr.has_value());\n-        // The amount chosen *includes* the default fee for this transaction, i.e.\n-        // the value of the Sapling output will be 0.00001 ZEC less.\n-        builder.SetFee(LEGACY_DEFAULT_FEE);\n+\n+        // The amount chosen includes the fee for this transaction, as it will be after\n+        // adding the Sapling output. The first JoinSplit will consume two real inputs and\n+        // each subsequent JoinSplit will consume one real input, usually (it is possible\n+        // for the builder to use fewer JoinSplits but this only results in overestimating\n+        // the fee). The number of Sapling outputs will be padded to 2.\n+        auto fee = CalculateConventionalFee(2 * fromNotes.size() + 2);\n+        builder.SetFee(fee);\n+\n         builder.AddSaplingOutput(\n                 ovkForShieldingFromTaddr(seed),\n                 migrationDestAddress,\n-                amountToSend - LEGACY_DEFAULT_FEE,\n+                amountToSend - fee,\n                 std::nullopt);\n         builder.SendChangeToSprout(changeAddr.value());\n         CTransaction tx = builder.Build().GetTxOrThrow();\n@@ -176,7 +183,7 @@ bool AsyncRPCOperation_saplingmigration::main_impl() {\n         pwalletMain->AddPendingSaplingMigrationTx(tx);\n         LogPrint(\"zrpcunsafe\", \"%s: Added pending migration transaction with txid=%s\\n\", getId(), tx.GetHash().ToString());\n         ++numTxCreated;\n-        amountMigrated += amountToSend - LEGACY_DEFAULT_FEE;\n+        amountMigrated += amountToSend - fee;\n         migrationTxIds.push_back(tx.GetHash().ToString());\n     } while (numTxCreated < 5 && availableFunds > CENT);\n \ndiff --git a/src/zip317.h b/src/zip317.h\nindex 296ce1e2c8f..281c2419fb5 100644\n--- a/src/zip317.h\n+++ b/src/zip317.h\n@@ -21,7 +21,7 @@ static const size_t P2PKH_STANDARD_OUTPUT_SIZE = 34;\n // Constants for block template construction.\n static const int64_t WEIGHT_RATIO_SCALE = INT64_C(10000000000000000);\n static const int64_t WEIGHT_RATIO_CAP = 4;\n-static const size_t DEFAULT_BLOCK_UNPAID_ACTION_LIMIT = 50;\n+static const size_t DEFAULT_BLOCK_UNPAID_ACTION_LIMIT = 0;\n \n /// Limit on the number of unpaid actions a transaction can have to be accepted to the mempool.\n static const size_t DEFAULT_TX_UNPAID_ACTION_LIMIT = DEFAULT_BLOCK_UNPAID_ACTION_LIMIT;\n", "instance_id": "zcash__zcash-6900", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to remove the default unpaid action limit by setting `DEFAULT_BLOCK_UNPAID_ACTION_LIMIT` to zero. It provides context about why this change is necessary (due to the mechanism being exploited by attackers and becoming ineffective for legitimate users) and specifies the scope of the change as a one-line modification with additional test updates. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what \"unpaid actions\" are or reference the specific ZIP 317 standard in detail for those unfamiliar with it. Additionally, it lacks clarity on potential impacts or edge cases resulting from this change (e.g., how miners or users might be affected if they rely on the previous limit). While the intent and high-level goal are clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it primarily involves a simple one-line change to a constant (`DEFAULT_BLOCK_UNPAID_ACTION_LIMIT` from 50 to 0) in the codebase. The scope of the core change is minimal, affecting a single constant in `src/zip317.h`, with additional minor updates to error messages, documentation in `release-notes.md`, and fee calculation logic in `asyncrpcoperation_saplingmigration.cpp`. While there are mentions of extensive test changes (approximately 1300 lines as per the problem statement), these are not provided in the diff and are likely mechanical updates rather than complex logic modifications. The technical concepts involved are basic\u2014understanding constants, configuration settings, and their impact on transaction validation logic. No advanced algorithms, design patterns, or deep architectural changes are required. Edge cases and error handling are minimally impacted, as the change simply enforces a stricter limit (zero unpaid actions), and the provided code updates error messaging accordingly. Overall, this task requires only basic code modification and a surface-level understanding of the codebase, making it very easy for a developer with moderate experience.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Decor in `DocumentMut` is ignored\n## Example\r\n\r\n```rs\r\nuse toml_edit::*;\r\n\r\nlet mut document = DocumentMut::new();\r\ndocument.decor_mut().set_prefix(\"# HEADER\\n\");\r\ndocument.decor_mut().set_prefix(\"# FOOTER\\n\");\r\nprintln!(\"{document}\");\r\n\r\ndocument.as_table_mut().insert(\"key\", value(\"value\"));\r\nprintln!(\"{document}\");\r\n```\r\n\r\n## Expected\r\n\r\n```\r\n# HEADER\r\n# FOOTER\r\n# HEADER\r\nkey = \"value\"\r\n# FOOTER\r\n```\r\n\r\n## Actual\r\n\r\n```\r\nkey = \"value\"\r\n```\n", "patch": "diff --git a/crates/toml_edit/src/encode.rs b/crates/toml_edit/src/encode.rs\nindex a8ccaa2a..b6142a6c 100644\n--- a/crates/toml_edit/src/encode.rs\n+++ b/crates/toml_edit/src/encode.rs\n@@ -6,7 +6,9 @@ use toml_datetime::Datetime;\n use crate::inline_table::DEFAULT_INLINE_KEY_DECOR;\n use crate::key::Key;\n use crate::repr::{Formatted, Repr, ValueRepr};\n-use crate::table::{DEFAULT_KEY_DECOR, DEFAULT_KEY_PATH_DECOR, DEFAULT_TABLE_DECOR};\n+use crate::table::{\n+    DEFAULT_KEY_DECOR, DEFAULT_KEY_PATH_DECOR, DEFAULT_ROOT_DECOR, DEFAULT_TABLE_DECOR,\n+};\n use crate::value::{\n     DEFAULT_LEADING_VALUE_DECOR, DEFAULT_TRAILING_VALUE_DECOR, DEFAULT_VALUE_DECOR,\n };\n@@ -197,6 +199,9 @@ pub(crate) fn encode_value(\n \n impl Display for DocumentMut {\n     fn fmt(&self, f: &mut Formatter<'_>) -> Result {\n+        let decor = self.decor();\n+        decor.prefix_encode(f, None, DEFAULT_ROOT_DECOR.0)?;\n+\n         let mut path = Vec::new();\n         let mut last_position = 0;\n         let mut tables = Vec::new();\n@@ -214,6 +219,7 @@ impl Display for DocumentMut {\n         for (_, table, path, is_array) in tables {\n             visit_table(f, None, table, &path, is_array, &mut first_table)?;\n         }\n+        decor.suffix_encode(f, None, DEFAULT_ROOT_DECOR.1)?;\n         self.trailing().encode_with_default(f, None, \"\")\n     }\n }\ndiff --git a/crates/toml_edit/src/table.rs b/crates/toml_edit/src/table.rs\nindex 2b52aec7..2863946f 100644\n--- a/crates/toml_edit/src/table.rs\n+++ b/crates/toml_edit/src/table.rs\n@@ -504,6 +504,7 @@ fn decorate_table(table: &mut Table) {\n }\n \n // `key1 = value1`\n+pub(crate) const DEFAULT_ROOT_DECOR: (&str, &str) = (\"\", \"\");\n pub(crate) const DEFAULT_KEY_DECOR: (&str, &str) = (\"\", \" \");\n pub(crate) const DEFAULT_TABLE_DECOR: (&str, &str) = (\"\\n\", \"\");\n pub(crate) const DEFAULT_KEY_PATH_DECOR: (&str, &str) = (\"\", \"\");\n", "instance_id": "toml-rs__toml-750", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the decor (prefix and suffix comments or formatting) set on a `DocumentMut` object in the `toml_edit` crate is ignored when the document is rendered or displayed. The provided example effectively illustrates the discrepancy between the expected output (with decor like `# HEADER` and `# FOOTER`) and the actual output (missing decor). However, there are minor ambiguities or missing details. For instance, the problem statement does not explicitly mention whether this issue applies only to root-level decor or also to nested structures like tables or arrays. Additionally, edge cases (e.g., empty documents, multiple decor settings, or interactions with other formatting) are not discussed. While the intent is clear, these omissions prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided diff shows that the fix involves modifying a small, focused part of the codebase\u2014specifically, the `Display` implementation for `DocumentMut` in `encode.rs` and adding a constant in `table.rs`. The changes are localized to two files and involve adding a few lines to handle decor encoding (prefix and suffix) during rendering. There is no indication of widespread impact on the system's architecture or interactions with other modules beyond the rendering logic.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Rust's `Display` trait and how formatting is handled in the `toml_edit` crate. The developer needs to understand the concept of decor (prefix/suffix strings) and how they are stored and rendered. No advanced algorithms, design patterns, or domain-specific knowledge (beyond TOML formatting) are required. The changes involve straightforward calls to existing methods like `prefix_encode` and `suffix_encode`.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, and the code changes do not introduce new error handling logic. However, a developer might need to consider minor edge cases, such as ensuring that decor is applied correctly for empty documents or when no content is present. These are not complex to handle and are likely already managed by the existing `encode_with_default` methods.\n\n4. **Overall Complexity**: The fix is relatively simple, requiring minimal lines of code and a basic understanding of the rendering process. It does not demand deep knowledge of the entire codebase or intricate logic. The primary challenge lies in identifying where decor should be applied in the rendering pipeline, which is evident from the diff.\n\nGiven these points, a score of 0.35 reflects an \"Easy\" problem that requires understanding some code logic and making simple modifications. It is slightly above the lower end of the range due to the need to understand the specific rendering mechanism of `DocumentMut`, but it remains far from medium or hard due to the limited scope and straightforward nature of the fix.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "into triggers `clippy::impl_trait_in_params`\nSee https://rust-lang.github.io/rust-clippy/master/index.html#/impl_trait_in_params.\r\n\r\nReproduce:\r\n\r\n```rust\r\n#![warn(clippy::impl_trait_in_params)]\r\n\r\nfn main() {\r\n    let example = Example::builder().content(42).build();\r\n    dbg!(example.content);\r\n}\r\n\r\n#[bon::builder]\r\npub struct Example {\r\n    #[builder(into)]\r\n    content: i32,\r\n}\r\n```\r\n\r\nMaybe it's a good idea to follow this lints suggestion and move to `fn content<T: Into<i32>(self, value: T)`. Otherwise, maybe ignore this lint by default (like other non clippy lints are already ignored)\n", "patch": "diff --git a/bon-macros/src/builder/builder_gen/setter_methods.rs b/bon-macros/src/builder/builder_gen/setter_methods.rs\nindex 8b70e429..3a054a32 100644\n--- a/bon-macros/src/builder/builder_gen/setter_methods.rs\n+++ b/bon-macros/src/builder/builder_gen/setter_methods.rs\n@@ -271,6 +271,7 @@ impl<'a> MemberSettersCtx<'a> {\n \n         quote! {\n             #( #docs )*\n+            #[allow(clippy::impl_trait_in_params)]\n             #[inline(always)]\n             #vis fn #method_name(self, #fn_params) -> #return_type {\n                 #builder_ident {\n", "instance_id": "elastio__bon-80", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: a Clippy lint (`clippy::impl_trait_in_params`) is triggered by the use of the `into` attribute in a builder pattern. It provides a reproducible code snippet and suggests two potential solutions\u2014either following the lint's suggestion to use a generic `Into<T>` approach or suppressing the lint. However, there are minor ambiguities. The statement does not explicitly define the desired outcome (e.g., whether the goal is to suppress the lint or refactor the code to comply with it). Additionally, it lacks details on potential side effects of either approach, such as performance implications or compatibility issues with existing code. Constraints or preferences for the solution are also not specified, leaving room for interpretation. Despite these minor gaps, the problem's core issue and context are understandable with the provided example and link to the Clippy documentation.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The provided code change is minimal, involving a single-line addition of a `#[allow(clippy::impl_trait_in_params)]` attribute to suppress the lint in a specific function. It affects only one file (`setter_methods.rs`) and does not require understanding complex interactions across the codebase or altering the system's architecture. The change is localized and straightforward.\n\n2. **Technical Concepts Involved**: Solving this problem requires basic familiarity with Rust's Clippy lints and attributes for suppressing warnings. The alternative solution (using generics with `Into<T>`) would involve slightly more complexity, requiring understanding of Rust's trait bounds and generics, but the provided diff opts for the simpler suppression approach. No advanced algorithms, design patterns, or domain-specific knowledge are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not mention specific edge cases or error conditions related to this lint or the builder pattern. The code change does not involve modifying error handling logic. Suppressing the lint is a low-risk operation with no apparent impact on runtime behavior, assuming the existing code is correct.\n\n4. **Overall Complexity**: The task is relatively simple, as it involves either a one-line fix to suppress the lint or a slightly more involved refactoring to use generics (not implemented in the diff). Even if the generic approach were chosen, it would still fall within the \"Easy\" category due to the localized nature of the change and the well-documented nature of `Into<T>` in Rust.\n\nIn summary, this problem requires minimal effort and a basic understanding of Rust's tooling and language features, justifying a difficulty score of 0.25. It is a straightforward bug fix or configuration tweak with no significant impact on the broader codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[Bug] daemon version warning is written to stdout, not stderr\n### Describe the bug\n\nWhen client and daemon versions differ, all output except for JSON output is prepended with\r\n\r\n```\r\nDifferent daemon version detected '{version}'. Consider restarting the daemon.\r\n```\r\n\r\nThis message is written to stdout, which means it can't easily be filtered out.\n\n### Steps to reproduce\n\nRun the daemon as version 3.4.0, then use client version 3.4.1 and use `--print-task-id` to only get the task ID to be output:\r\n\r\n```\r\n$ pueue add --print-task-id 'echo hello' 2>/dev/null\r\nDifferent daemon version detected '3.4.0'. Consider restarting the daemon.\r\n1\r\n```\r\n\r\nThe expected output would be `1`, with `Different daemon version detected '3.4.0'. Consider restarting the daemon.` having been redirected to /dev/null.\n\n### Debug logs (if relevant)\n\n_No response_\n\n### Operating system\n\nDebian 10\n\n### Pueue version\n\n3.4.0 (daemon) and 3.4.1 (client)\n\n### Additional context\n\nAt the very least, the message should be written to stderr. Ideally, it should be possible to disable it altogether (`pueue --no-warnings add ...`).\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fcb738f1..e502c0e90 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -43,6 +43,8 @@ TLDR: The new task state representation is more verbose but significantly cleane\n   **Important: The Pueue daemon needs to be restarted and the state will be wiped clean.**\n - **Breaking**: Streamlined `pueue log` parameters to behave the same way as `start`, `pause` or `kill`. [#509](https://github.com/Nukesor/pueue/issues/509)\n - **Breaking**: Remove the `--children` commandline flags, that have been deprecated and no longer serve any function since `v3.0.0`.\n+- Send log output to `stderr` instead of `stdout` [#562](https://github.com/Nukesor/pueue/issues/562).\n+- Change default log level from error to warning [#562](https://github.com/Nukesor/pueue/issues/562).\n \n ### Add\n \n@@ -59,6 +61,7 @@ TLDR: The new task state representation is more verbose but significantly cleane\n \n - Fixed delay after sending process related commands from client. [#548](https://github.com/Nukesor/pueue/pull/548)\n - Callback templating arguments were html escaped by accident. [#564](https://github.com/Nukesor/pueue/pull/564)\n+- Print incompatible version warning info as a log message instead of plain stdout input, which broke json outputs [#562](https://github.com/Nukesor/pueue/issues/562).\n \n ## \\[3.4.1\\] - 2024-06-04\n \ndiff --git a/pueue/src/bin/pueue.rs b/pueue/src/bin/pueue.rs\nindex ab4b9969c..6fb23a28f 100644\n--- a/pueue/src/bin/pueue.rs\n+++ b/pueue/src/bin/pueue.rs\n@@ -4,7 +4,7 @@ use anyhow::{bail, Context, Result};\n use clap::{CommandFactory, Parser};\n use clap_complete::{generate, generate_to, shells};\n use log::warn;\n-use simplelog::{Config, ConfigBuilder, LevelFilter, SimpleLogger};\n+use simplelog::{Config, ConfigBuilder, LevelFilter, SimpleLogger, TermLogger, TerminalMode};\n \n use pueue_lib::settings::Settings;\n \n@@ -35,10 +35,10 @@ async fn main() -> Result<()> {\n \n     // Init the logger and set the verbosity level depending on the `-v` flags.\n     let level = match opt.verbose {\n-        0 => LevelFilter::Error,\n-        1 => LevelFilter::Warn,\n-        2 => LevelFilter::Info,\n-        _ => LevelFilter::Debug,\n+        0 => LevelFilter::Warn,\n+        1 => LevelFilter::Info,\n+        2 => LevelFilter::Debug,\n+        _ => LevelFilter::Trace,\n     };\n \n     // Try to initialize the logger with the timezone set to the Local time of the machine.\n@@ -51,7 +51,17 @@ async fn main() -> Result<()> {\n         Ok(builder) => builder.build(),\n     };\n \n-    SimpleLogger::init(level, logger_config).unwrap();\n+    // Init a terminal logger. If this fails for some reason, try fallback to a SimpleLogger\n+    if TermLogger::init(\n+        level,\n+        logger_config.clone(),\n+        TerminalMode::Stderr,\n+        simplelog::ColorChoice::Auto,\n+    )\n+    .is_err()\n+    {\n+        SimpleLogger::init(level, logger_config).unwrap();\n+    }\n \n     // Try to read settings from the configuration file.\n     let (mut settings, config_found) =\ndiff --git a/pueue/src/bin/pueued.rs b/pueue/src/bin/pueued.rs\nindex 3313ccacf..6ba11eb72 100644\n--- a/pueue/src/bin/pueued.rs\n+++ b/pueue/src/bin/pueued.rs\n@@ -3,7 +3,7 @@ use std::process::Command;\n use anyhow::Result;\n use clap::Parser;\n use log::warn;\n-use simplelog::{Config, ConfigBuilder, LevelFilter, SimpleLogger};\n+use simplelog::{Config, ConfigBuilder, LevelFilter, SimpleLogger, TermLogger, TerminalMode};\n \n use pueue::daemon::cli::CliArguments;\n use pueue::daemon::run;\n@@ -19,10 +19,10 @@ async fn main() -> Result<()> {\n \n     // Set the verbosity level of the logger.\n     let level = match opt.verbose {\n-        0 => LevelFilter::Error,\n-        1 => LevelFilter::Warn,\n-        2 => LevelFilter::Info,\n-        _ => LevelFilter::Debug,\n+        0 => LevelFilter::Warn,\n+        1 => LevelFilter::Info,\n+        2 => LevelFilter::Debug,\n+        _ => LevelFilter::Trace,\n     };\n \n     // Try to initialize the logger with the timezone set to the Local time of the machine.\n@@ -35,7 +35,17 @@ async fn main() -> Result<()> {\n         Ok(builder) => builder.build(),\n     };\n \n-    SimpleLogger::init(level, logger_config).unwrap();\n+    // Init a terminal logger. If this fails for some reason, try fallback to a SimpleLogger\n+    if TermLogger::init(\n+        level,\n+        logger_config.clone(),\n+        TerminalMode::Stderr,\n+        simplelog::ColorChoice::Auto,\n+    )\n+    .is_err()\n+    {\n+        SimpleLogger::init(level, logger_config).unwrap();\n+    }\n \n     run(opt.config, opt.profile, false).await\n }\ndiff --git a/pueue/src/client/client.rs b/pueue/src/client/client.rs\nindex cbbe54da5..d67b8c118 100644\n--- a/pueue/src/client/client.rs\n+++ b/pueue/src/client/client.rs\n@@ -5,7 +5,7 @@ use std::{borrow::Cow, collections::HashMap};\n use anyhow::{bail, Context, Result};\n use clap::crate_version;\n use crossterm::tty::IsTty;\n-use log::error;\n+use log::{error, warn};\n \n use pueue_lib::network::message::*;\n use pueue_lib::network::protocol::*;\n@@ -110,7 +110,7 @@ impl Client {\n             };\n \n             if show_warning {\n-                println!(\n+                warn!(\n                     \"Different daemon version detected '{version}'. Consider restarting the daemon.\"\n                 );\n             }\n", "instance_id": "Nukesor__pueue-566", "clarity": 3, "difficulty": 0.25, "clarity_explanation": "The problem statement is comprehensive and well-defined. The issue is clearly described as a bug where a version mismatch warning between the client and daemon is output to stdout instead of stderr, which interferes with filtering and JSON output. The goal (redirecting the message to stderr) and the ideal enhancement (ability to disable the warning) are explicitly stated. Steps to reproduce are provided with a concrete example, including the exact command and expected versus actual output. The operating system and version details are included, ensuring no critical information is missing. There are no ambiguities, and the problem's context is well-articulated with a clear impact description. The only minor omission is the lack of explicit mention of specific edge cases, but this does not detract from the overall clarity as the core issue is straightforward.", "difficulty_explanation": "The difficulty of this problem is relatively low, falling into the \"Easy\" category (0.2-0.4). Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes:** The changes are localized to a few files (`pueue.rs`, `pueued.rs`, and `client.rs`) and involve straightforward modifications. The primary fix is changing the output stream for logging from stdout to stderr by initializing a `TermLogger` with `TerminalMode::Stderr` and replacing a `println!` with a `warn!` macro in the client code. Additionally, the default log level is adjusted from `Error` to `Warn`. These changes are minimal and do not impact the broader system architecture or require deep interaction between modules. The amount of code change is small, with only a few lines modified per file.\n\n2. **Number of Technical Concepts:** The solution requires basic familiarity with Rust's logging mechanisms, specifically the `log` crate and the `simplelog` library used in the codebase. Understanding how to configure a logger to output to stderr instead of stdout is a fundamental concept. No advanced language features, complex algorithms, or domain-specific knowledge are needed. The change from `println!` to `warn!` is trivial for anyone familiar with Rust's logging macros.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond the core issue of output stream redirection. The code changes handle the primary issue without introducing new error conditions or requiring complex error handling. The fallback to `SimpleLogger` if `TermLogger` initialization fails is already implemented in the provided diff, showing a basic level of error handling that does not add significant complexity.\n\n4. **Overall Complexity:** This is a simple bug fix that does not require deep understanding of the codebase beyond the logging setup and client output logic. It is a contained change with no performance implications or architectural impact. The task is suitable for a junior to mid-level developer with basic Rust experience.\n\nGiven these factors, a difficulty score of 0.25 reflects the ease of the task, requiring only basic code modifications and minimal conceptual understanding beyond standard logging practices in Rust.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "make `broadcast::Receiver` cooperative\nCurrently, `broadcast::Receiver` does not participate in the [coop budget](https://docs.rs/tokio/latest/tokio/task/#cooperative-scheduling), so if there are a lot of messages available, a task will just keep receiving them and runtime may not be able to schedule other tasks.\r\n\r\nSo, it would make sense to have `broadcast::Receiver` participate in the coop budget as well.\n", "patch": "diff --git a/tokio/src/sync/broadcast.rs b/tokio/src/sync/broadcast.rs\nindex 67d67a666e3..56c4cd6b92f 100644\n--- a/tokio/src/sync/broadcast.rs\n+++ b/tokio/src/sync/broadcast.rs\n@@ -119,6 +119,7 @@\n use crate::loom::cell::UnsafeCell;\n use crate::loom::sync::atomic::{AtomicBool, AtomicUsize};\n use crate::loom::sync::{Arc, Mutex, MutexGuard, RwLock, RwLockReadGuard};\n+use crate::runtime::coop::cooperative;\n use crate::util::linked_list::{self, GuardedLinkedList, LinkedList};\n use crate::util::WakeList;\n \n@@ -1262,8 +1263,7 @@ impl<T: Clone> Receiver<T> {\n     /// }\n     /// ```\n     pub async fn recv(&mut self) -> Result<T, RecvError> {\n-        let fut = Recv::new(self);\n-        fut.await\n+        cooperative(Recv::new(self)).await\n     }\n \n     /// Attempts to return a pending value on this receiver without awaiting.\n", "instance_id": "tokio-rs__tokio-6870", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in its intent: it aims to make the `broadcast::Receiver` in the Tokio library participate in the cooperative scheduling budget to prevent a task from monopolizing the runtime by continuously receiving messages. The goal is explicitly stated, and a reference to the cooperative scheduling documentation is provided, which helps in understanding the context. However, the statement lacks specific details on how the cooperative budget should be integrated, what specific behavior is expected (e.g., how many messages should be processed before yielding), and whether there are any constraints or edge cases to consider. Additionally, there are no examples or test cases provided to illustrate the desired outcome. Due to these minor ambiguities, I assign a clarity score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following analysis based on the provided factors:\n\n1. **Scope and Depth of Code Changes:** The code change is minimal and localized to a single file (`tokio/src/sync/broadcast.rs`). It involves wrapping the existing `Recv::new(self)` future with a `cooperative()` function, which is a straightforward modification. There is no indication of broader architectural impact or the need to modify multiple modules. The change is essentially a one-liner, indicating a low amount of code change.\n\n2. **Number of Technical Concepts:** Solving this problem requires understanding Tokio's cooperative scheduling mechanism and how futures are handled in the Tokio runtime. The developer needs to be familiar with the `cooperative()` utility and its purpose of enforcing budget limits on tasks. While this involves some knowledge of asynchronous programming in Rust and Tokio's internals, these concepts are not overly complex for someone with moderate experience in Rust and async programming.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention any edge cases or specific error handling requirements. The code change itself does not introduce new error handling logic, as it simply delegates to the `cooperative()` wrapper. However, there might be implicit considerations, such as ensuring that the cooperative scheduling does not negatively impact the performance or behavior of the `broadcast::Receiver` under high load or with empty message queues. These are not addressed in the problem statement or code change, but they do not significantly increase the difficulty.\n\n4. **Overall Assessment:** Given the simplicity of the code change and the limited scope, combined with the need for a basic understanding of Tokio's cooperative scheduling, I assess the difficulty as 0.30. This reflects an \"Easy\" problem that requires understanding some code logic and making a simple modification. It does not involve complex interactions, extensive refactoring, or advanced technical concepts beyond the basics of async Rust programming.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "process: document how to read output from `Child`\ntokio v1.41.0\nubuntu linux 22.04\n\n**Description**\n\nChild::wait_with_output() should take `&mut self` instead of `self`.    Taking `self` means it cannot be used in a `select!()`, which means there is no good way to simultaneously wait for a child processes output and a kill/cancel channel message.\n\nstated differently, the docs for Child::kill() provide an example of using `select!()` with branches for Child::wait() and a oneshot channel recv().  This example works perfectly for Child::wait() but fails to compile for Child::wait_with_output().\n\nThe only way I was able to get it to work was to copy/paste the impl of Child::wait_with_output() into my own code and modify it to take `&mut self` instead.\n\nSee discussion #7132 with playground examples.\n\n\n", "patch": "diff --git a/tokio/src/process/mod.rs b/tokio/src/process/mod.rs\nindex 565795ac4e6..c59815b898f 100644\n--- a/tokio/src/process/mod.rs\n+++ b/tokio/src/process/mod.rs\n@@ -1169,6 +1169,8 @@ impl Child {\n     ///\n     /// This is equivalent to sending a `SIGKILL` on unix platforms.\n     ///\n+    /// # Examples\n+    ///\n     /// If the child has to be killed remotely, it is possible to do it using\n     /// a combination of the select! macro and a `oneshot` channel. In the following\n     /// example, the child will run until completion unless a message is sent on\n@@ -1190,6 +1192,46 @@ impl Child {\n     ///     }\n     /// }\n     /// ```\n+    ///\n+    /// You can also interact with the child's standard I/O. For example, you can\n+    /// read its stdout while waiting for it to exit.\n+    ///\n+    /// ```no_run\n+    /// # use std::process::Stdio;\n+    /// #\n+    /// # use tokio::io::AsyncReadExt;\n+    /// # use tokio::process::Command;\n+    /// # use tokio::sync::oneshot::channel;\n+    ///\n+    /// #[tokio::main]\n+    /// async fn main() {\n+    ///     let (_tx, rx) = channel::<()>();\n+    ///\n+    ///     let mut child = Command::new(\"echo\")\n+    ///         .arg(\"Hello World!\")\n+    ///         .stdout(Stdio::piped())\n+    ///         .spawn()\n+    ///         .unwrap();\n+    ///\n+    ///     let mut stdout = child.stdout.take().expect(\"stdout is not captured\");\n+    ///\n+    ///     let read_stdout = tokio::spawn(async move {\n+    ///         let mut buff = Vec::new();\n+    ///         let _ = stdout.read_to_end(&mut buff).await;\n+    ///\n+    ///         buff\n+    ///     });\n+    ///\n+    ///     tokio::select! {\n+    ///         _ = child.wait() => {}\n+    ///         _ = rx => { child.kill().await.expect(\"kill failed\") },\n+    ///     }\n+    ///\n+    ///     let buff = read_stdout.await.unwrap();\n+    ///\n+    ///     assert_eq!(buff, b\"Hello World!\\n\");\n+    /// }\n+    /// ```\n     pub async fn kill(&mut self) -> io::Result<()> {\n         self.start_kill()?;\n         self.wait().await?;\n", "instance_id": "tokio-rs__tokio-7141", "clarity": 2, "difficulty": 0.25, "clarity_explanation": "The problem statement is mostly clear in describing the issue with `Child::wait_with_output()` in the Tokio library, specifically that it takes ownership of `self` instead of a mutable reference (`&mut self`), which prevents its use in a `select!()` macro alongside other asynchronous operations like handling a kill/cancel channel. The goal of documenting or addressing this limitation is evident, and the reference to discussion #7132 and playground examples adds context. However, there are minor ambiguities: the problem statement does not explicitly propose a solution (e.g., whether to change the API or just update documentation with a workaround), and the provided code changes focus solely on adding documentation examples rather than addressing the core issue of modifying the method signature. Additionally, constraints or potential side effects of changing the API (if that is the intent) are not discussed. Edge cases or specific scenarios beyond the `select!()` limitation are also not mentioned. Thus, while the problem is valid and mostly clear, it lacks some critical details and a definitive scope for the solution.", "difficulty_explanation": "The difficulty of this task is rated as easy (0.25) based on the provided code changes and the nature of the problem. The code changes are limited to a single file (`tokio/src/process/mod.rs`) and involve only adding documentation with an example of reading a child process's stdout while waiting for it to exit. This requires minimal code modification and does not impact the system's architecture or involve complex logic. The technical concepts involved are basic: familiarity with Tokio's `process` module, asynchronous programming with `select!()`, and handling child process I/O using `AsyncReadExt`. These are standard for anyone working with Tokio and do not require deep expertise beyond intermediate Rust knowledge. The scope of the change is small, with no interaction between multiple modules or significant codebase understanding required. Edge cases and error handling are not extensively addressed in the code changes beyond the basic example, and the problem statement does not highlight complex scenarios to handle. If the task were to involve actually modifying the `wait_with_output()` method signature to take `&mut self` (as hinted in the problem statement), the difficulty would increase to medium (0.4-0.6) due to potential API compatibility issues and the need to understand Tokio's internal implementation details. However, based on the provided diff, which focuses solely on documentation, the difficulty remains low.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "question: ASCII is currently required when format timestamp\nHi It's fine to see this good project. I'm trying to test jiff format.\r\n\r\n```rust\r\nuse jiff::{civil::date, fmt::strtime::BrokenDownTime};\r\n\r\nfn main() {\r\n    let zdt = date(2024, 7, 9)\r\n        .at(16, 24, 0, 0)\r\n        .intz(\"America/New_York\")\r\n        .unwrap();\r\n    let tm = BrokenDownTime::from(&zdt);\r\n\r\n    let mut buf = String::new();\r\n    tm.format(\"%a\u306e%b %e %I:%M:%S %p %Z %Y\", &mut buf).unwrap();\r\n    println!(\"{}\", buf);\r\n\r\n    assert_eq!(buf, \"Tue Jul  9 04:24:00 PM EDT 2024\");\r\n}\r\n\r\n```\r\n\r\nIt will be panic:\r\n\r\n```shell\r\n\u279c  jiff-example git:(master) \u2717 cargo run      \r\n   Compiling jiff-example v0.1.0 (/data1/eason/jiff-example)\r\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.23s\r\n     Running `target/debug/jiff-example`\r\nthread 'main' panicked at src/main.rs:8:52:\r\ncalled `Result::unwrap()` on an `Err` value: Error { inner: ErrorInner { kind: Adhoc(strftime formatting failed), cause: Some(Error { inner: ErrorInner { kind: Adhoc(found non-ASCII byte \"\\xE3\" in format string (ASCII is currently required)), cause: None } }) } }\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n```\r\n\r\nIs there any other way to format timestamp?\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex d8fbd0d6..311c9a3b 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -9,6 +9,11 @@ Enhancements:\n * [#130](https://github.com/BurntSushi/jiff/issues/130):\n Document value ranges for methods like `year`, `day`, `hour` and so on.\n \n+Bug fixes:\n+\n+* [#155](https://github.com/BurntSushi/jiff/issues/155):\n+Relax `strftime` format strings from ASCII-only to all of UTF-8.\n+\n \n 0.1.18 (2024-12-31)\n ===================\ndiff --git a/src/fmt/strtime/format.rs b/src/fmt/strtime/format.rs\nindex 26a88920..2fe8a6ff 100644\n--- a/src/fmt/strtime/format.rs\n+++ b/src/fmt/strtime/format.rs\n@@ -9,7 +9,7 @@ use crate::{\n         Write, WriteExt,\n     },\n     tz::Offset,\n-    util::escape,\n+    util::{escape, utf8},\n     Error,\n };\n \n@@ -23,15 +23,13 @@ impl<'f, 't, 'w, W: Write> Formatter<'f, 't, 'w, W> {\n     pub(super) fn format(&mut self) -> Result<(), Error> {\n         while !self.fmt.is_empty() {\n             if self.f() != b'%' {\n-                if !self.f().is_ascii() {\n-                    return Err(err!(\n-                        \"found non-ASCII byte {byte:?} in format \\\n-                         string (ASCII is currently required)\",\n-                        byte = escape::Byte(self.f()),\n-                    ));\n+                if self.f().is_ascii() {\n+                    self.wtr.write_char(char::from(self.f()))?;\n+                    self.bump_fmt();\n+                } else {\n+                    let ch = self.utf8_decode_and_bump()?;\n+                    self.wtr.write_char(ch)?;\n                 }\n-                self.wtr.write_char(char::from(self.f()))?;\n-                self.bump_fmt();\n                 continue;\n             }\n             if !self.bump_fmt() {\n@@ -141,6 +139,35 @@ impl<'f, 't, 'w, W: Write> Formatter<'f, 't, 'w, W> {\n         !self.fmt.is_empty()\n     }\n \n+    /// Decodes a Unicode scalar value from the beginning of `fmt` and advances\n+    /// the parser accordingly.\n+    ///\n+    /// If a Unicode scalar value could not be decoded, then an error is\n+    /// returned.\n+    ///\n+    /// It would be nice to just pass through bytes as-is instead of doing\n+    /// actual UTF-8 decoding, but since the `Write` trait only represents\n+    /// Unicode-accepting buffers, we need to actually do decoding here.\n+    ///\n+    /// # Panics\n+    ///\n+    /// When `self.fmt` is empty. i.e., Only call this when you know there is\n+    /// some remaining bytes to parse.\n+    #[inline(never)]\n+    fn utf8_decode_and_bump(&mut self) -> Result<char, Error> {\n+        match utf8::decode(self.fmt).expect(\"non-empty fmt\") {\n+            Ok(ch) => {\n+                self.fmt = &self.fmt[ch.len_utf8()..];\n+                return Ok(ch);\n+            }\n+            Err(invalid) => Err(err!(\n+                \"found invalid UTF-8 byte {byte:?} in format \\\n+                 string (format strings must be valid UTF-8)\",\n+                byte = escape::Byte(invalid),\n+            )),\n+        }\n+    }\n+\n     /// Parses optional extensions before a specifier directive. That is, right\n     /// after the `%`. If any extensions are parsed, the parser is bumped\n     /// to the next byte. (If no next byte exists, then an error is returned.)\ndiff --git a/src/fmt/strtime/mod.rs b/src/fmt/strtime/mod.rs\nindex f824fabf..e7ada240 100644\n--- a/src/fmt/strtime/mod.rs\n+++ b/src/fmt/strtime/mod.rs\n@@ -2168,4 +2168,26 @@ mod tests {\n             @\"2024-07-30T00:56:25+04:00[+04:00]\",\n         );\n     }\n+\n+    // Regression test for format strings with non-ASCII in them.\n+    //\n+    // We initially didn't support non-ASCII because I had thought it wouldn't\n+    // be used. i.e., If someone wanted to do something with non-ASCII, then\n+    // I thought they'd want to be using something more sophisticated that took\n+    // locale into account. But apparently not.\n+    //\n+    // See: https://github.com/BurntSushi/jiff/issues/155\n+    #[test]\n+    fn ok_non_ascii() {\n+        let fmt = \"%Y\u5e74%m\u6708%d\u65e5\uff0c%H\u65f6%M\u5206%S\u79d2\";\n+        let dt = crate::civil::date(2022, 2, 4).at(3, 58, 59, 0);\n+        insta::assert_snapshot!(\n+            dt.strftime(fmt),\n+            @\"2022\u5e7402\u670804\u65e5\uff0c03\u65f658\u520659\u79d2\",\n+        );\n+        insta::assert_debug_snapshot!(\n+            DateTime::strptime(fmt, \"2022\u5e7402\u670804\u65e5\uff0c03\u65f658\u520659\u79d2\").unwrap(),\n+            @\"2022-02-04T03:58:59\",\n+        );\n+    }\n }\n", "instance_id": "BurntSushi__jiff-186", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in identifying the issue: the current implementation of timestamp formatting in the `jiff` library panics when non-ASCII characters are used in the format string, as demonstrated by the provided Rust code and error output. The goal of allowing non-ASCII (UTF-8) characters in format strings is implied through the user's question and the code changes. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define the expected behavior for non-ASCII characters beyond the example provided, nor does it mention specific constraints or edge cases (e.g., invalid UTF-8 sequences or performance implications). Additionally, the user's question (\"Is there any other way to format timestamp?\") is somewhat vague and not directly tied to a specific request for a feature or bug fix. Despite these minor issues, the intent is understandable with the help of the code changes and the provided context, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively focused, primarily affecting the `strftime` formatting logic in a single module (`src/fmt/strtime/format.rs`), with a test added in `src/fmt/strtime/mod.rs`. The changes involve a moderate amount of code, including the addition of UTF-8 decoding logic and error handling for invalid UTF-8 sequences. Second, the technical concepts required include a solid understanding of Rust's string handling, UTF-8 encoding/decoding, and error handling mechanisms, as well as familiarity with the `Write` trait and byte-level manipulation. These concepts are not overly complex for an experienced Rust developer but do require attention to detail. Third, the problem involves handling edge cases, such as invalid UTF-8 sequences, which are addressed in the code changes with appropriate error messages. However, the impact on the overall architecture is minimal, as the change is localized and does not require deep refactoring of the codebase. Overall, this problem requires understanding multiple concepts and making targeted, moderately complex modifications, justifying a difficulty score of 0.45.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Constraints regarding `BitFlipMutator` inputs.\nHi LibAFL.\r\n\r\nBefore all, I'm using libAFL, and I'm very amazed by the work you all have done here. I wanted to say, was long overdue. :) \r\n\r\nI have a question regarding the [`BitFlipMutator`]. To my understanding, it is used for flipping bits from byte buffers:\r\n\r\n> Bitflip mutation for inputs with a bytes vector.\r\n\r\nLooks handy, but I don't understand why the input type must implement the [`HasMutatorBytes`] trait:\r\n\r\nhttps://github.com/AFLplusplus/LibAFL/blob/cfe051e5d9d3f7ec703a1cfeb4330db23df1c748/libafl/src/mutators/mutations.rs#L129-L133\r\n\r\n[`HasMutatorBytes`] requires the following methods:\r\n\r\nhttps://github.com/AFLplusplus/LibAFL/blob/c9531aa2424a8b81bdb1b33bc6d23eba98523fd9/libafl/src/inputs/bytes.rs#L35-L56\r\n\r\nIn my opinion, these methods shouldn't be required for inputs of [`BitFlipMutator`] since - again to my understanding - this mutator is solely going to flip existing bits. Therefore, requiring the input to be resizable looks out of scope here.\r\n\r\nIt sounds like [`BitFlipMutator`] would only need a `&mut [u8]` to work, wouldn't it?\r\n\r\n\r\nI'm willing to make a PR for improving this, if you believe that it should be done. :)\r\n\r\nThank you \ud83e\udee1 \r\n\r\n\r\n[`BitFlipMutator`]: https://docs.rs/libafl/latest/libafl/mutators/mutations/struct.BitFlipMutator.html\r\n[`HasMutatorBytes`]: https://docs.rs/libafl/latest/libafl/inputs/trait.HasMutatorBytes.html\n", "patch": "diff --git a/libafl/src/inputs/bytes.rs b/libafl/src/inputs/bytes.rs\nindex 3cfe0ec45b..128ecc14c6 100644\n--- a/libafl/src/inputs/bytes.rs\n+++ b/libafl/src/inputs/bytes.rs\n@@ -11,7 +11,7 @@ use core::cell::RefCell;\n use libafl_bolts::{ownedref::OwnedSlice, HasLen};\n \n use super::ValueInput;\n-use crate::inputs::{HasMutatorBytes, HasTargetBytes};\n+use crate::inputs::{HasMutatorBytes, HasMutatorResizableBytes, HasTargetBytes};\n \n /// A bytes input is the basic input\n pub type BytesInput = ValueInput<Vec<u8>>;\n@@ -31,7 +31,9 @@ impl HasMutatorBytes for BytesInput {\n     fn bytes_mut(&mut self) -> &mut [u8] {\n         self.as_mut()\n     }\n+}\n \n+impl HasMutatorResizableBytes for BytesInput {\n     fn resize(&mut self, new_len: usize, value: u8) {\n         self.as_mut().resize(new_len, value);\n     }\ndiff --git a/libafl/src/inputs/bytessub.rs b/libafl/src/inputs/bytessub.rs\nindex c0280a9ec8..e96c96d376 100644\n--- a/libafl/src/inputs/bytessub.rs\n+++ b/libafl/src/inputs/bytessub.rs\n@@ -11,7 +11,7 @@ use libafl_bolts::{\n     HasLen,\n };\n \n-use crate::inputs::HasMutatorBytes;\n+use crate::inputs::{HasMutatorBytes, HasMutatorResizableBytes};\n \n /// The [`BytesSubInput`] makes it possible to use [`crate::mutators::Mutator`]`s` that work on\n /// inputs implementing the [`HasMutatorBytes`] for a sub-range of this input.\n@@ -37,14 +37,15 @@ use crate::inputs::HasMutatorBytes;\n /// assert_eq!(bytes_input.bytes()[1], 42);\n /// ```\n ///\n-/// Growing or shrinking the sub input will grow or shrink the parent input,\n+/// If inputs implement the [`HasMutatorResizableBytes`] trait, growing or shrinking the sub input\n+/// will grow or shrink the parent input,\n /// and keep elements around the current range untouched / move them accordingly.\n ///\n /// For example:\n /// ```rust\n /// # extern crate alloc;\n /// # extern crate libafl;\n-/// # use libafl::inputs::{BytesInput, HasMutatorBytes};\n+/// # use libafl::inputs::{BytesInput, HasMutatorBytes, HasMutatorResizableBytes};\n /// # use alloc::vec::Vec;\n /// #\n /// # #[cfg(not(feature = \"std\"))]\n@@ -109,7 +110,12 @@ where\n     fn bytes_mut(&mut self) -> &mut [u8] {\n         &mut self.parent_input.bytes_mut()[self.range.clone()]\n     }\n+}\n \n+impl<I> HasMutatorResizableBytes for BytesSubInput<'_, I>\n+where\n+    I: HasMutatorResizableBytes,\n+{\n     fn resize(&mut self, new_len: usize, value: u8) {\n         let start_index = self.range.start;\n         let end_index = self.range.end;\n@@ -208,7 +214,7 @@ mod tests {\n     use libafl_bolts::HasLen;\n \n     use crate::{\n-        inputs::{BytesInput, HasMutatorBytes, NopInput},\n+        inputs::{BytesInput, HasMutatorBytes, HasMutatorResizableBytes, NopInput},\n         mutators::{havoc_mutations_no_crossover, MutatorsTuple},\n         state::NopState,\n     };\ndiff --git a/libafl/src/inputs/mod.rs b/libafl/src/inputs/mod.rs\nindex 432cdef35a..348aa0609c 100644\n--- a/libafl/src/inputs/mod.rs\n+++ b/libafl/src/inputs/mod.rs\n@@ -155,7 +155,7 @@ pub trait HasTargetBytes {\n     fn target_bytes(&self) -> OwnedSlice<u8>;\n }\n \n-/// Contains mutable and resizable bytes\n+/// Contains mutable bytes\n pub trait HasMutatorBytes: HasLen {\n     /// The bytes\n     fn bytes(&self) -> &[u8];\n@@ -163,25 +163,6 @@ pub trait HasMutatorBytes: HasLen {\n     /// The bytes to mutate\n     fn bytes_mut(&mut self) -> &mut [u8];\n \n-    /// Resize the mutator bytes to a given new size.\n-    /// Use `value` to fill new slots in case the buffer grows.\n-    /// See [`Vec::splice`].\n-    fn resize(&mut self, new_len: usize, value: u8);\n-\n-    /// Extends the given buffer with an iterator. See [`alloc::vec::Vec::extend`]\n-    fn extend<'a, I: IntoIterator<Item = &'a u8>>(&mut self, iter: I);\n-\n-    /// Splices the given target bytes according to [`Vec::splice`]'s rules\n-    fn splice<R, I>(&mut self, range: R, replace_with: I) -> Splice<'_, I::IntoIter>\n-    where\n-        R: RangeBounds<usize>,\n-        I: IntoIterator<Item = u8>;\n-\n-    /// Drains the given target bytes according to [`Vec::drain`]'s rules\n-    fn drain<R>(&mut self, range: R) -> Drain<'_, u8>\n-    where\n-        R: RangeBounds<usize>;\n-\n     /// Creates a [`SubRangeSlice`] from this input, that can be used to slice a byte array.\n     fn sub_bytes<R>(&self, range: R) -> SubRangeSlice<u8>\n     where\n@@ -215,7 +196,45 @@ impl HasMutatorBytes for Vec<u8> {\n     fn bytes_mut(&mut self) -> &mut [u8] {\n         self.as_mut()\n     }\n+}\n+\n+/// A wrapper type that allows us to use mutators for Mutators for `&mut `[`Vec`].\n+#[deprecated(since = \"0.15.0\", note = \"Use &mut Vec<u8> directly\")]\n+pub type MutVecInput<'a> = &'a mut Vec<u8>;\n+\n+impl HasMutatorBytes for &'_ mut Vec<u8> {\n+    fn bytes(&self) -> &[u8] {\n+        self\n+    }\n+\n+    fn bytes_mut(&mut self) -> &mut [u8] {\n+        self\n+    }\n+}\n \n+/// Contains mutable and resizable bytes\n+pub trait HasMutatorResizableBytes: HasMutatorBytes {\n+    /// Resize the mutator bytes to a given new size.\n+    /// Use `value` to fill new slots in case the buffer grows.\n+    /// See [`Vec::splice`].\n+    fn resize(&mut self, new_len: usize, value: u8);\n+\n+    /// Extends the given buffer with an iterator. See [`alloc::vec::Vec::extend`]\n+    fn extend<'a, I: IntoIterator<Item = &'a u8>>(&mut self, iter: I);\n+\n+    /// Splices the given target bytes according to [`Vec::splice`]'s rules\n+    fn splice<R, I>(&mut self, range: R, replace_with: I) -> Splice<'_, I::IntoIter>\n+    where\n+        R: RangeBounds<usize>,\n+        I: IntoIterator<Item = u8>;\n+\n+    /// Drains the given target bytes according to [`Vec::drain`]'s rules\n+    fn drain<R>(&mut self, range: R) -> Drain<'_, u8>\n+    where\n+        R: RangeBounds<usize>;\n+}\n+\n+impl HasMutatorResizableBytes for Vec<u8> {\n     fn resize(&mut self, new_len: usize, value: u8) {\n         <Vec<u8>>::resize(self, new_len, value);\n     }\n@@ -240,19 +259,7 @@ impl HasMutatorBytes for Vec<u8> {\n     }\n }\n \n-/// A wrapper type that allows us to use mutators for Mutators for `&mut `[`Vec`].\n-#[deprecated(since = \"0.15.0\", note = \"Use &mut Vec<u8> directly\")]\n-pub type MutVecInput<'a> = &'a mut Vec<u8>;\n-\n-impl HasMutatorBytes for &mut Vec<u8> {\n-    fn bytes(&self) -> &[u8] {\n-        self\n-    }\n-\n-    fn bytes_mut(&mut self) -> &mut [u8] {\n-        self\n-    }\n-\n+impl HasMutatorResizableBytes for &mut Vec<u8> {\n     fn resize(&mut self, new_len: usize, value: u8) {\n         self.deref_mut().resize(new_len, value);\n     }\ndiff --git a/libafl/src/mutators/multi.rs b/libafl/src/mutators/multi.rs\nindex 66dd1ab672..f7c353e866 100644\n--- a/libafl/src/mutators/multi.rs\n+++ b/libafl/src/mutators/multi.rs\n@@ -10,7 +10,7 @@ use libafl_bolts::{rands::Rand, Error};\n use crate::{\n     corpus::{Corpus, CorpusId},\n     impl_default_multipart,\n-    inputs::{multi::MultipartInput, HasMutatorBytes, Input},\n+    inputs::{multi::MultipartInput, HasMutatorResizableBytes, Input},\n     mutators::{\n         mutations::{\n             rand_range, BitFlipMutator, ByteAddMutator, ByteDecMutator, ByteFlipMutator,\n@@ -119,7 +119,7 @@ impl_default_multipart!(\n impl<I, S> Mutator<MultipartInput<I>, S> for CrossoverInsertMutator\n where\n     S: HasCorpus + HasMaxSize + HasRand,\n-    I: Input + HasMutatorBytes,\n+    I: Input + HasMutatorResizableBytes,\n     S::Corpus: Corpus<Input = MultipartInput<I>>,\n {\n     fn mutate(\n@@ -255,7 +255,7 @@ where\n impl<I, S> Mutator<MultipartInput<I>, S> for CrossoverReplaceMutator\n where\n     S: HasCorpus + HasMaxSize + HasRand,\n-    I: Input + HasMutatorBytes,\n+    I: Input + HasMutatorResizableBytes,\n     S::Corpus: Corpus<Input = MultipartInput<I>>,\n {\n     fn mutate(\ndiff --git a/libafl/src/mutators/mutations.rs b/libafl/src/mutators/mutations.rs\nindex e9f729a0d9..dce3ffb730 100644\n--- a/libafl/src/mutators/mutations.rs\n+++ b/libafl/src/mutators/mutations.rs\n@@ -16,7 +16,7 @@ use libafl_bolts::{rands::Rand, Named};\n \n use crate::{\n     corpus::Corpus,\n-    inputs::HasMutatorBytes,\n+    inputs::{HasMutatorBytes, HasMutatorResizableBytes},\n     mutators::{MutationResult, Mutator},\n     nonzero, random_corpus_id_with_disabled,\n     state::{HasCorpus, HasMaxSize, HasRand},\n@@ -464,7 +464,7 @@ pub struct BytesDeleteMutator;\n impl<I, S> Mutator<I, S> for BytesDeleteMutator\n where\n     S: HasRand,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let size = input.bytes().len();\n@@ -506,7 +506,7 @@ pub struct BytesExpandMutator;\n impl<I, S> Mutator<I, S> for BytesExpandMutator\n where\n     S: HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let max_size = state.max_size();\n@@ -557,7 +557,7 @@ pub struct BytesInsertMutator;\n impl<I, S> Mutator<I, S> for BytesInsertMutator\n where\n     S: HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let max_size = state.max_size();\n@@ -620,7 +620,7 @@ pub struct BytesRandInsertMutator;\n impl<I, S> Mutator<I, S> for BytesRandInsertMutator\n where\n     S: HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let max_size = state.max_size();\n@@ -815,7 +815,7 @@ pub struct BytesInsertCopyMutator {\n impl<I, S> Mutator<I, S> for BytesInsertCopyMutator\n where\n     S: HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let size = input.bytes().len();\n@@ -886,7 +886,7 @@ pub struct BytesSwapMutator {\n impl<I, S> Mutator<I, S> for BytesSwapMutator\n where\n     S: HasRand,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let size = input.bytes().len();\n@@ -1104,7 +1104,7 @@ impl CrossoverInsertMutator {\n         other: &[u8],\n     ) -> MutationResult\n     where\n-        I: HasMutatorBytes,\n+        I: HasMutatorResizableBytes,\n     {\n         input.resize(size + range.len(), 0);\n         unsafe {\n@@ -1127,7 +1127,7 @@ impl<I, S> Mutator<I, S> for CrossoverInsertMutator\n where\n     S: HasCorpus + HasRand + HasMaxSize,\n     <S::Corpus as Corpus>::Input: HasMutatorBytes,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let size = input.bytes().len();\n@@ -1315,7 +1315,7 @@ impl<F, O> MappedCrossoverInsertMutator<F, O> {\n impl<S, F, I, O> Mutator<I, S> for MappedCrossoverInsertMutator<F, O>\n where\n     S: HasCorpus + HasMaxSize + HasRand,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n     O: IntoOptionBytes,\n     F: Fn(&<S::Corpus as Corpus>::Input) -> &O,\n {\n@@ -1494,7 +1494,7 @@ impl<I, S> Mutator<I, S> for SpliceMutator\n where\n     S: HasCorpus + HasRand,\n     <S::Corpus as Corpus>::Input: HasMutatorBytes,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     #[expect(clippy::cast_sign_loss)]\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\ndiff --git a/libafl/src/mutators/token_mutations.rs b/libafl/src/mutators/token_mutations.rs\nindex 23664f3d53..e56b118f4a 100644\n--- a/libafl/src/mutators/token_mutations.rs\n+++ b/libafl/src/mutators/token_mutations.rs\n@@ -25,7 +25,7 @@ use serde::{Deserialize, Serialize};\n use crate::mutators::str_decode;\n use crate::{\n     corpus::{CorpusId, HasCurrentCorpusId},\n-    inputs::HasMutatorBytes,\n+    inputs::HasMutatorResizableBytes,\n     mutators::{\n         buffer_self_copy, mutations::buffer_copy, MultiMutator, MutationResult, Mutator, Named,\n     },\n@@ -306,7 +306,7 @@ pub struct TokenInsert;\n impl<I, S> Mutator<I, S> for TokenInsert\n where\n     S: HasMetadata + HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let max_size = state.max_size();\n@@ -375,7 +375,7 @@ pub struct TokenReplace;\n impl<I, S> Mutator<I, S> for TokenReplace\n where\n     S: HasMetadata + HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n         let size = input.bytes().len();\n@@ -435,7 +435,7 @@ pub struct I2SRandReplace;\n impl<I, S> Mutator<I, S> for I2SRandReplace\n where\n     S: HasMetadata + HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     #[expect(clippy::too_many_lines)]\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n@@ -636,7 +636,7 @@ where\n impl<I, S> Mutator<I, S> for I2SRandReplaceBinonly\n where\n     S: HasMetadata + HasRand + HasMaxSize,\n-    I: HasMutatorBytes,\n+    I: HasMutatorResizableBytes,\n {\n     #[expect(clippy::too_many_lines)]\n     fn mutate(&mut self, state: &mut S, input: &mut I) -> Result<MutationResult, Error> {\n@@ -1306,7 +1306,7 @@ impl AFLppRedQueen {\n impl<I, S> MultiMutator<I, S> for AFLppRedQueen\n where\n     S: HasMetadata + HasRand + HasMaxSize + HasCorpus + HasCurrentCorpusId,\n-    I: HasMutatorBytes + From<Vec<u8>>,\n+    I: HasMutatorResizableBytes + From<Vec<u8>>,\n {\n     #[expect(clippy::needless_range_loop, clippy::too_many_lines)]\n     fn multi_mutate(\ndiff --git a/libafl/src/mutators/unicode/mod.rs b/libafl/src/mutators/unicode/mod.rs\nindex c719377736..ef7dd90edb 100644\n--- a/libafl/src/mutators/unicode/mod.rs\n+++ b/libafl/src/mutators/unicode/mod.rs\n@@ -11,7 +11,7 @@ use libafl_bolts::{rands::Rand, Error, HasLen, Named};\n \n use crate::{\n     corpus::{Corpus, CorpusId, HasTestcase, Testcase},\n-    inputs::{BytesInput, HasMutatorBytes},\n+    inputs::{BytesInput, HasMutatorBytes, HasMutatorResizableBytes},\n     mutators::{rand_range, MutationResult, Mutator, Tokens},\n     nonzero,\n     stages::{\ndiff --git a/libafl/src/stages/colorization.rs b/libafl/src/stages/colorization.rs\nindex 87109a3820..6c06e10dce 100644\n--- a/libafl/src/stages/colorization.rs\n+++ b/libafl/src/stages/colorization.rs\n@@ -17,7 +17,7 @@ use crate::{\n     corpus::{Corpus, HasCurrentCorpusId},\n     events::EventFirer,\n     executors::{Executor, HasObservers},\n-    inputs::HasMutatorBytes,\n+    inputs::{HasMutatorBytes, HasMutatorResizableBytes},\n     mutators::mutations::buffer_copy,\n     nonzero,\n     observers::{MapObserver, ObserversTuple},\n@@ -80,7 +80,7 @@ where\n     E: HasObservers + Executor<EM, <S::Corpus as Corpus>::Input, S, Z>,\n     S: HasCorpus + HasMetadata + HasRand + HasNamedMetadata + HasCurrentCorpusId,\n     E::Observers: ObserversTuple<<S::Corpus as Corpus>::Input, S>,\n-    <S::Corpus as Corpus>::Input: HasMutatorBytes + Clone,\n+    <S::Corpus as Corpus>::Input: HasMutatorResizableBytes + Clone,\n     O: MapObserver,\n     C: AsRef<O> + Named,\n {\n@@ -157,7 +157,7 @@ where\n     E: HasObservers + Executor<EM, <S::Corpus as Corpus>::Input, S, Z>,\n     E::Observers: ObserversTuple<<S::Corpus as Corpus>::Input, S>,\n     S: HasCorpus + HasMetadata + HasRand + HasCurrentCorpusId + HasCurrentTestcase,\n-    <S::Corpus as Corpus>::Input: HasMutatorBytes + Clone,\n+    <S::Corpus as Corpus>::Input: HasMutatorResizableBytes + Clone,\n {\n     #[inline]\n     fn colorize(\ndiff --git a/libafl/src/stages/generalization.rs b/libafl/src/stages/generalization.rs\nindex 432514f09b..88d14c7f5a 100644\n--- a/libafl/src/stages/generalization.rs\n+++ b/libafl/src/stages/generalization.rs\n@@ -17,7 +17,10 @@ use crate::{\n     corpus::{Corpus, HasCurrentCorpusId},\n     executors::{Executor, HasObservers},\n     feedbacks::map::MapNoveltiesMetadata,\n-    inputs::{BytesInput, GeneralizedInputMetadata, GeneralizedItem, HasMutatorBytes},\n+    inputs::{\n+        BytesInput, GeneralizedInputMetadata, GeneralizedItem, HasMutatorBytes,\n+        HasMutatorResizableBytes,\n+    },\n     mark_feature_time,\n     observers::{CanTrack, MapObserver, ObserversTuple},\n     require_novelties_tracking,\ndiff --git a/libafl_targets/src/libfuzzer/mutators.rs b/libafl_targets/src/libfuzzer/mutators.rs\nindex 34abfb72a3..ed47616c5f 100644\n--- a/libafl_targets/src/libfuzzer/mutators.rs\n+++ b/libafl_targets/src/libfuzzer/mutators.rs\n@@ -11,7 +11,7 @@ use std::{\n \n use libafl::{\n     corpus::Corpus,\n-    inputs::{BytesInput, HasMutatorBytes},\n+    inputs::{BytesInput, HasMutatorBytes, HasMutatorResizableBytes},\n     mutators::{\n         ComposedByMutations, MutationId, MutationResult, Mutator, MutatorsTuple, ScheduledMutator,\n     },\n", "instance_id": "AFLplusplus__LibAFL-2856", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in expressing the concern about the `BitFlipMutator` requiring the `HasMutatorBytes` trait, which includes methods for resizing the input. The author questions the necessity of these resizing capabilities for a mutator that only flips bits, suggesting a simpler requirement of a mutable byte slice (`&mut [u8]`). The goal of potentially simplifying the trait requirements for `BitFlipMutator` is evident, and the author provides relevant links to the codebase for context. However, there are minor ambiguities: the problem statement does not explicitly define the expected outcome or constraints of the proposed change (e.g., whether all mutators should be split into resizable and non-resizable categories or just `BitFlipMutator`). Additionally, it lacks discussion on potential impacts or edge cases of loosening the trait requirements. Overall, it is valid and mostly clear but misses some critical details for a fully comprehensive description.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes is significant, as it involves refactoring trait definitions across multiple files in the `libafl` library, a fuzzing framework with a complex architecture. The changes split the `HasMutatorBytes` trait into two (`HasMutatorBytes` for non-resizable operations and `HasMutatorResizableBytes` for resizable operations), impacting numerous mutators and input types, as seen in the diff across files like `inputs/mod.rs`, `mutators/mutations.rs`, and others. This requires a deep understanding of Rust's trait system, lifetime management, and how these traits interact with the library's mutator and input abstractions.\n\nSecond, the number of technical concepts involved is substantial. Solving this requires knowledge of Rust's advanced type system (traits and generics), familiarity with the `libafl` library's design (mutators, inputs, and corpus management), and an understanding of fuzzing concepts to ensure that the change does not break existing functionality. The developer must also consider backward compatibility and how different mutators rely on resizing capabilities.\n\nThird, while the problem statement does not explicitly mention edge cases, the code changes imply potential challenges, such as ensuring that mutators requiring resizing (e.g., `BytesInsertMutator`) still work correctly with the new trait hierarchy, while non-resizing mutators like `BitFlipMutator` can operate with reduced constraints. Error handling logic in the codebase may also need adjustment to account for inputs that no longer support resizing.\n\nFinally, the impact on the system's architecture is moderate, as this change alters a core abstraction (trait definitions for inputs), which could affect downstream users of the library. Balancing simplicity for `BitFlipMutator` with the needs of other mutators adds to the complexity. A score of 0.65 reflects the need for deep codebase understanding, complex modifications across multiple modules, and careful consideration of impacts, placing it in the lower end of the \"Hard\" range due to the absence of extreme system-level or domain-specific challenges.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`from_next_work_required` should not take the timespan as a u64\nOur current [from_next_work_required API](https://docs.rs/bitcoin/latest/bitcoin/pow/struct.CompactTarget.html#method.from_next_work_required) asks the user to provide the time interval between first and last blocks as a u64. Users will almost certainly compute this by subtracting the u32 times from two blocks and then promoting to a u64.\r\n\r\nSomebody informed me recently that Bitcoin's consensus rules allow this interval to be negative. (This sounds like some catastrophic bug but actually it's fine; in this case the difficulty just adjusts downward by the maximum factor of 4.) It's pretty hard to do this but it is possible and I believe it has happened on testnet. In this case, users who wrote the obvious/natural code will get overflow panics.\r\n\r\nI believe we should change the function to take an `i64` and explicitly advise users to *first* convert their u32 timestamps to i64s and *then* subtract them.\n", "patch": "diff --git a/bitcoin/src/network/params.rs b/bitcoin/src/network/params.rs\nindex eecc42cb8..e322009be 100644\n--- a/bitcoin/src/network/params.rs\n+++ b/bitcoin/src/network/params.rs\n@@ -113,7 +113,7 @@ pub struct Params {\n     /// Expected amount of time to mine one block.\n     pub pow_target_spacing: u64,\n     /// Difficulty recalculation interval.\n-    pub pow_target_timespan: u64,\n+    pub pow_target_timespan: u32,\n     /// Determines whether minimal difficulty may be used for blocks or not.\n     pub allow_min_difficulty_blocks: bool,\n     /// Determines whether retargeting is disabled for this network or not.\n@@ -261,7 +261,7 @@ impl Params {\n \n     /// Calculates the number of blocks between difficulty adjustments.\n     pub fn difficulty_adjustment_interval(&self) -> u64 {\n-        self.pow_target_timespan / self.pow_target_spacing\n+        u64::from(self.pow_target_timespan) / self.pow_target_spacing\n     }\n }\n \ndiff --git a/bitcoin/src/pow.rs b/bitcoin/src/pow.rs\nindex 7566e73b2..b6d531cb2 100644\n--- a/bitcoin/src/pow.rs\n+++ b/bitcoin/src/pow.rs\n@@ -357,7 +357,7 @@ define_extension_trait! {\n         /// ref: <https://github.com/bitcoin/bitcoin/blob/0503cbea9aab47ec0a87d34611e5453158727169/src/pow.cpp>\n         ///\n         /// Given the previous Target, represented as a [`CompactTarget`], the difficulty is adjusted\n-        /// by taking the timespan between them, and multipling the current [`CompactTarget`] by a factor\n+        /// by taking the timespan between them, and multiplying the current [`CompactTarget`] by a factor\n         /// of the net timespan and expected timespan. The [`CompactTarget`] may not adjust by more than\n         /// a factor of 4, or adjust beyond the maximum threshold for the network.\n         ///\n@@ -371,12 +371,14 @@ define_extension_trait! {\n         /// Take the example of the first difficulty adjustment. Block 2016 introduces a new [`CompactTarget`],\n         /// which takes the net timespan between Block 2015 and Block 0, and recomputes the difficulty.\n         ///\n+        /// To calculate the timespan, users should first convert their u32 timestamps to i64s before subtracting them\n+        ///\n         /// # Returns\n         ///\n         /// The expected [`CompactTarget`] recalculation.\n         fn from_next_work_required(\n             last: CompactTarget,\n-            timespan: u64,\n+            timespan: i64,\n             params: impl AsRef<Params>,\n         ) -> CompactTarget {\n             let params = params.as_ref();\n@@ -387,11 +389,11 @@ define_extension_trait! {\n             // ref: <https://github.com/bitcoin/bitcoin/blob/0503cbea9aab47ec0a87d34611e5453158727169/src/pow.cpp>\n             let min_timespan = params.pow_target_timespan >> 2; // Lines 56/57\n             let max_timespan = params.pow_target_timespan << 2; // Lines 58/59\n-            let actual_timespan = timespan.clamp(min_timespan, max_timespan);\n+            let actual_timespan = timespan.clamp(min_timespan.into(), max_timespan.into());\n             let prev_target: Target = last.into();\n             let maximum_retarget = prev_target.max_transition_threshold(params); // bnPowLimit\n             let retarget = prev_target.0; // bnNew\n-            let retarget = retarget.mul(actual_timespan.into());\n+            let retarget = retarget.mul(u128::try_from(actual_timespan).expect(\"clamped value won't be negative\").into());\n             let retarget = retarget.div(params.pow_target_timespan.into());\n             let retarget = Target(retarget);\n             if retarget.ge(&maximum_retarget) {\n@@ -1750,8 +1752,8 @@ mod tests {\n     fn compact_target_from_upwards_difficulty_adjustment() {\n         let params = Params::new(crate::Network::Signet);\n         let starting_bits = CompactTarget::from_consensus(503543726); // Genesis compact target on Signet\n-        let start_time: u64 = 1598918400; // Genesis block unix time\n-        let end_time: u64 = 1599332177; // Block 2015 unix time\n+        let start_time: i64 = 1598918400; // Genesis block unix time\n+        let end_time: i64 = 1599332177; // Block 2015 unix time\n         let timespan = end_time - start_time; // Faster than expected\n         let adjustment = CompactTarget::from_next_work_required(starting_bits, timespan, &params);\n         let adjustment_bits = CompactTarget::from_consensus(503394215); // Block 2016 compact target\n@@ -1762,8 +1764,8 @@ mod tests {\n     fn compact_target_from_downwards_difficulty_adjustment() {\n         let params = Params::new(crate::Network::Signet);\n         let starting_bits = CompactTarget::from_consensus(503394215); // Block 2016 compact target\n-        let start_time: u64 = 1599332844; // Block 2016 unix time\n-        let end_time: u64 = 1600591200; // Block 4031 unix time\n+        let start_time: i64 = 1599332844; // Block 2016 unix time\n+        let end_time: i64 = 1600591200; // Block 4031 unix time\n         let timespan = end_time - start_time; // Slower than expected\n         let adjustment = CompactTarget::from_next_work_required(starting_bits, timespan, &params);\n         let adjustment_bits = CompactTarget::from_consensus(503397348); // Block 4032 compact target\n@@ -1829,7 +1831,18 @@ mod tests {\n     fn compact_target_from_maximum_upward_difficulty_adjustment() {\n         let params = Params::new(crate::Network::Signet);\n         let starting_bits = CompactTarget::from_consensus(503403001);\n-        let timespan = (0.2 * params.pow_target_timespan as f64) as u64;\n+        let timespan = params.pow_target_timespan / 5;\n+        let got = CompactTarget::from_next_work_required(starting_bits, timespan.into(), params);\n+        let want =\n+            Target::from_compact(starting_bits).min_transition_threshold().to_compact_lossy();\n+        assert_eq!(got, want);\n+    }\n+\n+    #[test]\n+    fn compact_target_from_maximum_upward_difficulty_adjustment_with_negative_timespan() {\n+        let params = Params::new(crate::Network::Signet);\n+        let starting_bits = CompactTarget::from_consensus(503403001);\n+        let timespan: i64 = -i64::from(params.pow_target_timespan);\n         let got = CompactTarget::from_next_work_required(starting_bits, timespan, params);\n         let want =\n             Target::from_compact(starting_bits).min_transition_threshold().to_compact_lossy();\n@@ -1841,7 +1854,7 @@ mod tests {\n         let params = Params::new(crate::Network::Signet);\n         let starting_bits = CompactTarget::from_consensus(403403001); // High difficulty for Signet\n         let timespan = 5 * params.pow_target_timespan; // Really slow.\n-        let got = CompactTarget::from_next_work_required(starting_bits, timespan, &params);\n+        let got = CompactTarget::from_next_work_required(starting_bits, timespan.into(), &params);\n         let want =\n             Target::from_compact(starting_bits).max_transition_threshold(params).to_compact_lossy();\n         assert_eq!(got, want);\n@@ -1852,7 +1865,7 @@ mod tests {\n         let params = Params::new(crate::Network::Signet);\n         let starting_bits = CompactTarget::from_consensus(503543726); // Genesis compact target on Signet\n         let timespan = 5 * params.pow_target_timespan; // Really slow.\n-        let got = CompactTarget::from_next_work_required(starting_bits, timespan, &params);\n+        let got = CompactTarget::from_next_work_required(starting_bits, timespan.into(), &params);\n         let want = params.max_attainable_target.to_compact_lossy();\n         assert_eq!(got, want);\n     }\n", "instance_id": "rust-bitcoin__rust-bitcoin-3660", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue with the `from_next_work_required` API, specifically highlighting the problem of using `u64` for the timespan parameter when negative values are possible under Bitcoin's consensus rules. It provides a clear goal of changing the parameter type to `i64` and advises users on how to handle timestamp subtraction to avoid overflow issues. However, there are minor ambiguities: the statement does not explicitly discuss the full range of edge cases (e.g., the impact of very large negative or positive timespans beyond typical scenarios) or provide detailed examples of when negative timespans occur. Additionally, while the motivation for the change is explained, the broader implications on the codebase or dependent systems are not fully addressed in the problem description. Overall, it is valid and clear but lacks some minor details that could make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The changes are relatively localized, primarily affecting the `from_next_work_required` function signature in `pow.rs` and a related field type in `params.rs`. The modifications involve updating parameter types from `u64` to `i64` (and `u64` to `u32` in one case), adjusting type conversions, and updating test cases. While the changes span multiple files, they are straightforward and do not significantly impact the overall architecture of the system. The amount of code change is moderate, mostly involving type adjustments and minor logic updates (e.g., clamping and conversion handling).\n\n2. **Technical Concepts Required**: Solving this requires a basic understanding of Rust's type system, integer conversions, and overflow behavior. Familiarity with Bitcoin's difficulty adjustment logic is helpful but not critical, as the core issue is about type safety rather than the domain-specific algorithm. No advanced algorithms, design patterns, or complex libraries are involved. The primary challenge is ensuring type consistency across the codebase and handling the conversion of clamped values correctly.\n\n3. **Edge Cases and Error Handling**: The problem statement explicitly mentions the possibility of negative timespans, which is a critical edge case that the code changes address by switching to `i64`. The provided code updates include a new test case for negative timespans, showing attention to this edge case. However, the complexity of handling these edge cases is low, as it mainly involves type changes and ensuring proper clamping of values. No extensive error handling logic beyond type safety is required.\n\n4. **Overall Complexity**: The task requires understanding a specific part of the codebase (difficulty adjustment logic) and making targeted modifications. It does not demand deep knowledge of the entire system or complex refactoring. The risk of introducing bugs is minimal, as the changes are well-contained and supported by updated tests.\n\nGiven these considerations, a difficulty score of 0.35 reflects an \"Easy\" problem that involves straightforward type changes and minor logic adjustments, with a clear focus on addressing a specific issue (negative timespans) without requiring advanced technical expertise or extensive codebase knowledge.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Dirty model property shouldn't cause re-creation of all elements in a `for`\nWhen the model is marked as dirty, it will cause all elements to be re-created. \r\nIdeally, it shouldn't be re-created if the model didn't actually change\r\n\r\n## Original description\r\n\r\n### Bug Description\r\n\r\nAnimations don't trigger from state changes if they're on a component rendered inside a loop over a model where the model is a field in the same struct as the field whose change triggered the state change.\r\n\r\nI would expect the window to fade to green when it is clicked, instead it snaps to green. (The bug doesn't occur if the two fields are in separate structs)\r\n\r\nYou can reproduce it by running `slint-viewer states.slint`, and clicking on the window.\r\n\r\n### Reproducible Code\r\n\r\n```slint\r\nexport component App {\r\n    width: 120px;\r\n    height: 120px;\r\n    property<{on:bool, arr:[int]}> p: { arr: [1] };\r\n    TouchArea {\r\n        clicked =>  {p.on = !p.on}\r\n        for x[i] in p.arr: Rectangle { \r\n            states [\r\n                bright when p.on: {\r\n                    background: #90f020;\r\n                    in  {\r\n                        animate background {duration: 300ms;}\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n\r\n### Environment Details\r\n\r\n- Slint Version: 1.9.1  (was also present in 1.8.x)\r\n- Platform/OS: MacOS Sequoia (15.2)\r\n- Programming Language: Rust\r\n- Backend/Renderer: winit\r\n\r\n\r\n### Product Impact\r\n\r\nI'm currently working on a retrocomputing music editor (specifically a SID tracker), hobby project, likely freeware.\r\nMild cosmetic issue, noncritical (I can work around the bug by wrapping my list selection component in a dummy component that breaks up the {list,selected_id} pair).\n", "patch": "diff --git a/api/cpp/include/slint.h b/api/cpp/include/slint.h\nindex 59a38a9efd8..6d0eaae3c62 100644\n--- a/api/cpp/include/slint.h\n+++ b/api/cpp/include/slint.h\n@@ -1146,10 +1146,14 @@ class Repeater\n     void ensure_updated(const Parent *parent) const\n     {\n         if (model.is_dirty()) {\n-            inner = std::make_shared<RepeaterInner>();\n-            if (auto m = model.get()) {\n-                inner->model = m;\n-                m->attach_peer(inner);\n+            auto old_model = model.get_internal();\n+            auto m = model.get();\n+            if (!inner || old_model != m) {\n+                inner = std::make_shared<RepeaterInner>();\n+                if (m) {\n+                    inner->model = m;\n+                    m->attach_peer(inner);\n+                }\n             }\n         }\n \ndiff --git a/api/cpp/include/slint_properties.h b/api/cpp/include/slint_properties.h\nindex d880d77eed2..6dfd193b47b 100644\n--- a/api/cpp/include/slint_properties.h\n+++ b/api/cpp/include/slint_properties.h\n@@ -184,6 +184,8 @@ struct Property\n     {\n     }\n \n+    const T &get_internal() const { return value; }\n+\n private:\n     cbindgen_private::PropertyHandleOpaque inner;\n     mutable T value {};\ndiff --git a/internal/core/model.rs b/internal/core/model.rs\nindex cb4f33e8031..ab655791aee 100644\n--- a/internal/core/model.rs\n+++ b/internal/core/model.rs\n@@ -895,15 +895,17 @@ impl<C: RepeatedItemTree + 'static> Repeater<C> {\n     }\n \n     fn model(self: Pin<&Self>) -> ModelRc<C::Data> {\n-        // Safety: Repeater does not implement drop and never allows access to model as mutable\n         let model = self.data().project_ref().model;\n \n         if model.is_dirty() {\n-            *self.data().inner.borrow_mut() = RepeaterInner::default();\n-            self.data().is_dirty.set(true);\n+            let old_model = model.get_internal();\n             let m = model.get();\n-            let peer = self.project_ref().0.model_peer();\n-            m.model_tracker().attach_peer(peer);\n+            if old_model != m {\n+                *self.data().inner.borrow_mut() = RepeaterInner::default();\n+                self.data().is_dirty.set(true);\n+                let peer = self.project_ref().0.model_peer();\n+                m.model_tracker().attach_peer(peer);\n+            }\n             m\n         } else {\n             model.get()\ndiff --git a/internal/core/properties.rs b/internal/core/properties.rs\nindex 5650f5a212f..f08f88e6b9a 100644\n--- a/internal/core/properties.rs\n+++ b/internal/core/properties.rs\n@@ -861,8 +861,8 @@ impl<T: Clone> Property<T> {\n         self.get_internal()\n     }\n \n-    /// Get the value without registering any dependencies or executing any binding\n-    fn get_internal(&self) -> T {\n+    /// Get the cached value without registering any dependencies or executing any binding\n+    pub fn get_internal(&self) -> T {\n         self.handle.access(|_| {\n             // Safety: PropertyHandle::access ensure that the value is locked\n             unsafe { (*self.value.get()).clone() }\ndiff --git a/internal/interpreter/dynamic_item_tree.rs b/internal/interpreter/dynamic_item_tree.rs\nindex 7e2324928e2..2f4618aad17 100644\n--- a/internal/interpreter/dynamic_item_tree.rs\n+++ b/internal/interpreter/dynamic_item_tree.rs\n@@ -1726,7 +1726,11 @@ pub fn instantiate(\n         let model_binding_closure = make_binding_eval_closure(expr, &self_weak);\n         repeater.set_model_binding(move || {\n             let m = model_binding_closure();\n-            i_slint_core::model::ModelRc::new(crate::value_model::ValueModel::new(m))\n+            if let Value::Model(m) = m {\n+                m.clone()\n+            } else {\n+                i_slint_core::model::ModelRc::new(crate::value_model::ValueModel::new(m))\n+            }\n         });\n     }\n \n", "instance_id": "slint-ui__slint-7280", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: when a model is marked as dirty, it causes unnecessary re-creation of elements in a loop, and this impacts animations not triggering correctly in certain scenarios. The goal is to prevent re-creation if the model hasn't actually changed. The statement includes a reproducible code snippet, environment details, and a description of the expected behavior versus the actual behavior. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define what constitutes a \"change\" in the model (e.g., structural change vs. value change), and edge cases or specific constraints around model updates are not mentioned. Additionally, the connection between the bug (animations not triggering) and the proposed fix (avoiding re-creation) could be more explicitly detailed. Overall, while the problem is understandable and actionable, these minor gaps prevent it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Hard\" category due to several factors. First, the scope of code changes spans multiple files across Rust and C++ in a moderately complex codebase (Slint, a UI framework), indicating a need to understand interactions between different modules (e.g., properties, model handling, and repeater logic). The changes involve core components like property handling and model updates, which are critical to the system's behavior, though they do not appear to impact the overall architecture significantly. Second, the number of technical concepts required is notable: developers need to understand Slint's model-view architecture, property binding mechanisms, and the interplay between Rust's ownership model and C++ interoperability (via `cbindgen`). Additionally, familiarity with UI rendering loops and animation triggering is necessary to grasp the bug's root cause. Third, while the problem statement does not explicitly mention edge cases, the code changes suggest potential complexities in handling model equality checks and ensuring that dirty flags are managed correctly without introducing regressions (e.g., missing updates when a model does change). The modifications themselves are not overly extensive in terms of lines of code, but they require precision and a deep understanding of the framework's internals. A score of 0.65 reflects the challenge of navigating these concepts and making targeted changes in a specialized domain (UI framework), balanced by the fact that the solution does not involve a complete overhaul or advanced algorithmic complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "macOS specific keyboard input crashes alacritty 0.14.0\nedit: changed the topic title to better reflect the issue given what others have commented regarding their own experience with crashing.\r\n\r\nI use a piece of software on macOS called [BetterTouchTool](https://folivora.ai/) and one of its features is being able to turn caps lock into a hyper key. Hyper key for the uninitiated is actually a simultaneous key press event for the actual modifier keys `Command|Shift|Control|Option`.  I use the hyper key in combination with keyboard bindings within alacritty such that my hyper key essentially handles the standard bash / readline Ctrl-* key sequences--that is to say,  Hyper-C acts as Ctrl-c  and terminates a process. \r\n\r\n```\r\n[[keyboard.bindings]]\r\nchars = \"\\u0003\"\r\nkey = \"C\"\r\nmods = \"Command|Shift|Control|Option\"\r\n```\r\n\r\nPreviously, (i.e. the past two years) this was working just fine.\r\n\r\nExpected behavior: The character sequence defined in the key binding is sent, the desired effect occurs, and the terminal continues to operate without issue.\r\n\r\nActual behavior: Repeated (although sometimes on the first try) use of the hyper key crashes the terminal causing it to suddenly close.\r\n\r\n### System\r\n\r\nOS:\r\n```\r\n> sw_vers\r\nProductName:\t\tmacOS\r\nProductVersion:\t\t15.0.1\r\nBuildVersion:  \t\t24A348\r\n``` \r\nVersion:\r\n```\r\n> alacritty --version\r\nalacritty 0.14.0 (22a4475)\r\n```\r\n\r\n### Logs\r\n\r\n```\r\n[2.758060875s] [INFO ] [alacritty_winit_event] Event { window_id: Some(WindowId(5198973488)), payload: Frame }\r\n[2.758098667s] [INFO ] [alacritty_winit_event] About to wait\r\n[2.758120958s] [INFO ] [alacritty_winit_event] About to wait\r\n[2.765228375s] [INFO ] [alacritty_winit_event] About to wait\r\n[2.771734167s] [INFO ] [alacritty_winit_event] ModifiersChanged(Modifiers { state: ModifiersState(0x0), pressed_mods: ModifiersKeys(0x0) })\r\n[2.771803708s] [INFO ] [alacritty_winit_event] KeyboardInput { device_id: DeviceId(DeviceId), event: KeyEvent { physical_key: Code(ControlLeft), logical_key: Named(Control), text: None, location: Left, state: Released, repeat: false, platform_specific: KeyEventExtra { text_with_all_modifiers: None, key_without_modifiers: Named(Control) } }, is_synthetic: false }\r\n[2.771818125s] [INFO ] [alacritty_winit_event] ModifiersChanged(Modifiers { state: ModifiersState(SHIFT | ALT | SUPER), pressed_mods: ModifiersKeys(0x0) })\r\n[2.771874583s] [INFO ] [alacritty_winit_event] KeyboardInput { device_id: DeviceId(DeviceId), event: KeyEvent { physical_key: Code(ShiftLeft), logical_key: Named(Shift), text: None, location: Left, state: Released, repeat: false, platform_specific: KeyEventExtra { text_with_all_modifiers: None, key_without_modifiers: Named(Shift) } }, is_synthetic: false }\r\n[2.771896125s] [INFO ] [alacritty_winit_event] ModifiersChanged(Modifiers { state: ModifiersState(ALT | SUPER), pressed_mods: ModifiersKeys(0x0) })\r\n[2.771950375s] [INFO ] [alacritty_winit_event] KeyboardInput { device_id: DeviceId(DeviceId), event: KeyEvent { physical_key: Code(AltLeft), logical_key: Named(Alt), text: None, location: Left, state: Released, repeat: false, platform_specific: KeyEventExtra { text_with_all_modifiers: None, key_without_modifiers: Named(Alt) } }, is_synthetic: false }\r\n[2.771962542s] [INFO ] [alacritty_winit_event] ModifiersChanged(Modifiers { state: ModifiersState(SUPER), pressed_mods: ModifiersKeys(0x0) })\r\n[2.772015583s] [INFO ] [alacritty_winit_event] KeyboardInput { device_id: DeviceId(DeviceId), event: KeyEvent { physical_key: Code(SuperLeft), logical_key: Named(Super), text: None, location: Left, state: Released, repeat: false, platform_specific: KeyEventExtra { text_with_all_modifiers: None, key_without_modifiers: Named(Super) } }, is_synthetic: false }\r\n[2.772026875s] [INFO ] [alacritty_winit_event] ModifiersChanged(Modifiers { state: ModifiersState(0x0), pressed_mods: ModifiersKeys(0x0) })\r\n[2.772066917s] [INFO ] [alacritty_winit_event] About to wait\r\n[2.772157250s] [INFO ] [alacritty_winit_event] About to wait\r\n*** Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'Invalid message sent to event \"NSEvent: type=FlagsChanged loc=(0,744) time=3879.1 flags=0x100 win=0x135e20a30 winNum=1167 ctxt=0x0 keyCode=15\"'\r\n*** First throw call stack:\r\n(\r\n\t0   CoreFoundation                      0x00000001955b0ec0 __exceptionPreprocess + 176\r\n\t1   libobjc.A.dylib                     0x0000000195096cd8 objc_exception_throw + 88\r\n\t2   Foundation                          0x00000001967ad588 -[NSCalendarDate initWithCoder:] + 0\r\n\t3   AppKit                              0x0000000199225cec -[NSEvent charactersIgnoringModifiers] + 604\r\n\t4   alacritty                           0x0000000104969154 _ZN5winit13platform_impl5macos5event16create_key_event17h5ffd7f45014bc6b9E + 2508\r\n\t5   alacritty                           0x00000001049594a8 _ZN5winit13platform_impl5macos4view9WinitView13flags_changed17h58c32ee31fdc0f1bE + 344\r\n\t6   AppKit                              0x00000001991a80a8 -[NSWindow(NSEventRouting) _reallySendEvent:isDelayedEvent:] + 484\r\n\t7   AppKit                              0x00000001991a7cf4 -[NSWindow(NSEventRouting) sendEvent:] + 284\r\n\t8   AppKit                              0x00000001999a53e0 -[NSApplication(NSEventRouting) sendEvent:] + 1212\r\n\t9   alacritty                           0x0000000104961a3c _ZN5winit13platform_impl5macos3app16WinitApplication10send_event17h9d7b11ed09e187deE + 1040\r\n\t10  AppKit                              0x00000001995b8984 -[NSApplication _handleEvent:] + 60\r\n\t11  AppKit                              0x0000000199073ba4 -[NSApplication run] + 520\r\n\t12  alacritty                           0x000000010469e090 _ZN5winit13platform_impl5macos13event_handler12EventHandler3set17ha6185e593ba31ad4E + 280\r\n\t13  alacritty                           0x000000010474d378 _ZN9alacritty4main17h290211ac615fedbeE + 7624\r\n\t14  alacritty                           0x00000001046c2060 _ZN3std3sys9backtrace28__rust_begin_short_backtrace17h86645a6c57eec1ecE + 12\r\n\t15  alacritty                           0x000000010468392c _ZN3std2rt10lang_start28_$u7b$$u7b$closure$u7d$$u7d$17h7b27810acbbe86f2E + 16\r\n\t16  alacritty                           0x00000001048f1598 _ZN3std2rt19lang_start_internal17h9e88109c8deb8787E + 808\r\n\t17  alacritty                           0x000000010474de4c main + 52\r\n\t18  dyld                                0x00000001950d4274 start + 2840\r\n)\r\nlibc++abi: terminating due to uncaught exception of type NSException\r\n[1]    14442 abort      alacritty -vv --print-events\r\n```\r\nI cannot confirm if this hyper key issue applies to any other piece of software that adds this functionality (such as karabiner elements) or if this is an issue specific to how bettertouchtool implements hyperkey. For meanwhile, I've downgraded back to [alacritty 0.13.2](https://github.com/Homebrew/homebrew-cask/blob/ee57cb02e89536c4eb0624e7175705c947693ac7/Casks/a/alacritty.rb) which has resolved the issue.\r\n\r\n```\r\ncurl 'https://raw.githubusercontent.com/Homebrew/homebrew-cask/ee57cb02e89536c4eb0624e7175705c947693ac7/Casks/a/alacritty.rb' -o alacritty.rb\r\nbrew install -s alacritty.rb\r\n```\r\n\r\nNot sure if there's something I'm overlooking with the latest update. Appreciate any help.\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 866373837be..f2ca2761c5f 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -70,6 +70,7 @@ Notable changes to the `alacritty_terminal` crate are documented in its\n - Invalid URL highlights after terminal scrolling\n - Hollow block cursor not spanning multiple chars being edited inside the IME preview\n - Vi inline search only working for direct key input without modifiers\n+- Crash when pressing certain modifier keys on macOS 15+\n \n ## 0.13.2\n \ndiff --git a/Cargo.lock b/Cargo.lock\nindex b9be9b7eeea..00cd99d7c7f 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -2526,9 +2526,9 @@ checksum = \"589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec\"\n \n [[package]]\n name = \"winit\"\n-version = \"0.30.5\"\n+version = \"0.30.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0be9e76a1f1077e04a411f0b989cbd3c93339e1771cb41e71ac4aee95bfd2c67\"\n+checksum = \"7c3d72dfa0f47e429290cd0d236884ca02f22dbd5dd33a43ad2b8bf4d79b6c18\"\n dependencies = [\n  \"ahash\",\n  \"android-activity\",\ndiff --git a/alacritty/Cargo.toml b/alacritty/Cargo.toml\nindex 264ecbc6ec6..f7280deb4ed 100644\n--- a/alacritty/Cargo.toml\n+++ b/alacritty/Cargo.toml\n@@ -41,7 +41,7 @@ tempfile = \"3.12.0\"\n toml = \"0.8.2\"\n toml_edit = \"0.22.21\"\n unicode-width = \"0.1\"\n-winit = { version = \"0.30.5\", default-features = false, features = [\"rwh_06\", \"serde\"] }\n+winit = { version = \"0.30.6\", default-features = false, features = [\"rwh_06\", \"serde\"] }\n \n [build-dependencies]\n gl_generator = \"0.14.0\"\n", "instance_id": "alacritty__alacritty-8372", "clarity": 2, "difficulty": 0.65, "clarity_explanation": "The problem statement is mostly clear in describing the issue: a crash in Alacritty 0.14.0 on macOS when using a hyper key (a combination of multiple modifier keys) configured via BetterTouchTool. The expected behavior (sending a character sequence without crashing) and actual behavior (terminal crash) are explicitly stated. Additionally, system details, version information, and detailed logs are provided, which help in understanding the context and reproducing the issue. However, there are minor ambiguities that prevent a perfect score. For instance, the problem statement does not clarify whether the issue is specific to BetterTouchTool or if it could occur with other tools like Karabiner Elements (as noted by the reporter). Furthermore, there are no specific test cases or steps to consistently reproduce the crash beyond \"repeated use of the hyper key,\" which leaves some room for interpretation. Constraints or edge cases (e.g., specific key combinations or macOS versions) are not fully detailed, though the logs provide some insight into the failure point.", "difficulty_explanation": "The difficulty score of 0.65 reflects a hard problem that requires a deep understanding of specific technical areas, though it is not at the extreme end of complexity. Here's the breakdown based on the evaluation factors:\n\n1. **Scope and Depth of Code Changes**: The provided code changes are minimal, primarily involving a version bump of the `winit` dependency from 0.30.5 to 0.30.6 and an update to the changelog. However, this simplicity is deceptive. The actual fix likely resides in the `winit` library update, which suggests that the root cause (a crash due to an uncaught `NSInternalInconsistencyException` in macOS event handling) required changes in the underlying event handling logic. For someone working on this issue without the benefit of an existing fix in `winit`, the scope would involve debugging and modifying low-level macOS-specific event handling code, potentially across multiple files or modules in the `winit` crate or Alacritty itself. This could impact how modifier key events are processed, which is a critical part of the terminal's input system.\n\n2. **Number of Technical Concepts**: Solving this issue requires understanding several complex concepts: macOS-specific event handling (via AppKit and NSEvent), Rust's interaction with native macOS APIs through `winit`, and the intricacies of modifier key state management. Additionally, familiarity with Alacritty's input binding system and how it maps key events to actions is necessary. Debugging skills for native crashes (interpreting stack traces and exception logs) and knowledge of macOS-specific quirks (e.g., behavior changes in macOS 15+) are also critical. These concepts are moderately to highly complex, especially for developers not already familiar with macOS internals.\n\n3. **Potential Edge Cases and Error Handling**: The problem statement and logs highlight a specific crash scenario involving modifier key events (e.g., rapid pressing/releasing of multiple modifiers). Edge cases likely include various combinations of modifier keys, timing issues (e.g., rapid key events), and differences in behavior across macOS versions or third-party tools like BetterTouchTool. The error handling required would involve robustly managing invalid or unexpected event messages from the macOS system (as seen in the exception message about an \"invalid message sent to event\"). This adds a layer of complexity to ensure the application does not crash under unusual input conditions.\n\n4. **Overall Complexity**: While the provided diff is trivial (a dependency update), the underlying problem is not. Without the `winit` fix, a developer would need to dive into platform-specific code, debug native exceptions, and ensure compatibility across different macOS configurations. This places the difficulty in the \"hard\" range (0.6-0.8). I\u2019ve rated it at 0.65 because, while it requires deep platform knowledge and careful error handling, it does not appear to involve a complete architectural overhaul or extremely advanced domain-specific knowledge (e.g., distributed systems or novel algorithms). The challenge is more focused on debugging and fixing a specific, albeit complex, platform interaction issue.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Incorrect cursor color when using default cursor config (OSC12)\nUsing the default config, i.e. `cursor = { text = \"CellBackground\", cursor = \"CellForeground\" }`, querying the cursor color returns invalid color.\r\n\r\n<img width=\"236\" alt=\"image\" src=\"https://github.com/user-attachments/assets/348cd747-04ac-4790-be90-f6f16726bd94\">\r\n\r\nShould return the appropriate color using the `CellForeground` color value `^[]12;rgb:acf1/b277/be84^G`\r\n\r\n### System\r\n\r\nOS: macOS\r\nVersion: alacritty 0.13.2 (bb8ea18)\r\n\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 20df64f0dc1..38584ca5d06 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -37,6 +37,7 @@ Notable changes to the `alacritty_terminal` crate are documented in its\n - Kitty keyboard protocol reporting shifted key codes\n - Broken search with words broken across line boundary on the first character\n - Config import changes not being live reloaded\n+- Cursor color requests with default cursor colors\n \n ## 0.13.2\n \ndiff --git a/alacritty/src/event.rs b/alacritty/src/event.rs\nindex 72009d88875..f10346af540 100644\n--- a/alacritty/src/event.rs\n+++ b/alacritty/src/event.rs\n@@ -34,6 +34,7 @@ use alacritty_terminal::index::{Boundary, Column, Direction, Line, Point, Side};\n use alacritty_terminal::selection::{Selection, SelectionType};\n use alacritty_terminal::term::search::{Match, RegexSearch};\n use alacritty_terminal::term::{self, ClipboardType, Term, TermMode};\n+use alacritty_terminal::vte::ansi::NamedColor;\n \n #[cfg(unix)]\n use crate::cli::{IpcConfig, ParsedOptions};\n@@ -1737,9 +1738,12 @@ impl input::Processor<EventProxy, ActionContext<'_, Notifier, EventProxy>> {\n                         }\n                     },\n                     TerminalEvent::ColorRequest(index, format) => {\n-                        let color = self.ctx.terminal().colors()[index]\n-                            .map(Rgb)\n-                            .unwrap_or(self.ctx.display.colors[index]);\n+                        let color = match self.ctx.terminal().colors()[index] {\n+                            Some(color) => Rgb(color),\n+                            // Ignore cursor color requests unless it was changed.\n+                            None if index == NamedColor::Cursor as usize => return,\n+                            None => self.ctx.display.colors[index],\n+                        };\n                         self.ctx.write_to_pty(format(color.0).into_bytes());\n                     },\n                     TerminalEvent::TextAreaSizeRequest(format) => {\n", "instance_id": "alacritty__alacritty-8191", "clarity": 2, "difficulty": 0.3, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the cursor color is incorrect when using the default cursor configuration in Alacritty, and it should return a specific color value. The inclusion of an image and the expected output format (\"^[]12;rgb:acf1/b277/be84^G\") provides some context for the desired behavior. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define what constitutes \"invalid color\" or under what specific conditions the issue occurs (e.g., specific terminal modes or configurations beyond the default). Additionally, edge cases or potential side effects of querying cursor colors are not mentioned. While the goal is understandable, these gaps prevent the statement from being fully comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes**: The code changes are localized to a single file (`event.rs`) and involve a small modification to the logic for handling color requests. The diff shows a targeted update to check if the color index corresponds to the cursor color and to skip processing if it hasn't been explicitly changed. This does not impact the broader system architecture or require changes across multiple modules. The amount of code change is minimal, with only a few lines added or modified.\n\n2. **Technical Concepts Involved**: Solving this requires a basic understanding of Rust syntax and logic, as well as familiarity with the Alacritty codebase's handling of terminal events and color management. The concept of named colors (e.g., `NamedColor::Cursor`) and conditional logic for color retrieval is straightforward for someone with moderate experience in Rust or terminal emulator development. No advanced algorithms, design patterns, or domain-specific knowledge beyond terminal color handling are needed.\n\n3. **Edge Cases and Error Handling**: The problem statement does not explicitly mention edge cases, but the code change introduces a conditional check to ignore cursor color requests unless explicitly modified. This suggests a simple error-handling consideration (avoiding invalid or default color outputs), but it does not appear to involve complex edge case handling or performance considerations.\n\n4. **Overall Complexity**: The fix is relatively simple, involving a logical tweak rather than a deep architectural change or complex debugging. It requires understanding the specific behavior of color requests in the terminal emulator but does not demand extensive knowledge of the entire codebase or advanced technical skills.\n\nA score of 0.30 reflects that this is an easy problem, slightly above the \"very easy\" range due to the need to understand the specific context of terminal color handling and the Alacritty codebase's event processing logic. It is a bug fix that can be addressed with moderate familiarity with the project and basic Rust programming skills.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "hints.enabled action=\"Select\" mouse.enabled=true results in selection of the whole line (instead of just the hint)\nUsing the following configuration snippet:\r\n```\r\n#Select hint text\r\n[[hints.enabled]]\r\naction          = \"Select\"\r\nhyperlinks      = true\r\nmouse.enabled   = true\r\n#mouse.mods      = \"Alt\"\r\n```\r\nThe Select action select the whole line containing the hint, not just the hint text. \r\nFor example:\r\nIf I do something like `ls -l --hyperlink=auto` only the files get underlined as I hover over them, but when clicked the whole line is selected.\r\n\r\nThis doesn't change even if I uncoment the `mouse.mods` line.\r\n\r\nIs this the intended behavior?\r\n\r\nThe man 5 alacritty states:\r\n```\r\nSelect\r\n                   Select the hint's text.\r\n```\r\nSo I would expect only the hint's text to be selected.\r\n\r\nIf it's not a bug / the current behavior is already established, would you consider something like \"SelectHintOnly\" action?\r\n\r\n\r\n### System\r\n\r\nOS: Linux (Fedora 38)\r\nVersion: alacritty 0.13.1\r\nLinux/BSD: X11, KDE5 / KWin\r\n\n", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex f536186128a..63f2fe9644f 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -30,6 +30,7 @@ The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).\n - Mouse cursor not changing on Wayland when cursor theme uses legacy cursor icon names\n - Config keys are available under proper names\n - Build failure when compiling with x11 feature on NetBSD\n+- Hint `Select` action selecting the entire line for URL escapes\n \n ### Changed\n \ndiff --git a/alacritty/src/cli.rs b/alacritty/src/cli.rs\nindex fee2680f720..d5e24b4abe6 100644\n--- a/alacritty/src/cli.rs\n+++ b/alacritty/src/cli.rs\n@@ -5,7 +5,7 @@ use std::rc::Rc;\n \n use alacritty_config::SerdeReplace;\n use clap::{ArgAction, Args, Parser, Subcommand, ValueHint};\n-use log::{self, error, LevelFilter};\n+use log::{error, LevelFilter};\n use serde::{Deserialize, Serialize};\n use toml::Value;\n \ndiff --git a/alacritty/src/config/ui_config.rs b/alacritty/src/config/ui_config.rs\nindex 21059734f55..a4b6c2c5e9f 100644\n--- a/alacritty/src/config/ui_config.rs\n+++ b/alacritty/src/config/ui_config.rs\n@@ -10,7 +10,7 @@ use alacritty_terminal::term::Config as TermConfig;\n use alacritty_terminal::tty::{Options as PtyOptions, Shell};\n use log::{error, warn};\n use serde::de::{Error as SerdeError, MapAccess, Visitor};\n-use serde::{self, Deserialize, Deserializer};\n+use serde::{Deserialize, Deserializer};\n use unicode_width::UnicodeWidthChar;\n use winit::keyboard::{Key, ModifiersState};\n \ndiff --git a/alacritty/src/display/hint.rs b/alacritty/src/display/hint.rs\nindex bd09a881061..f118dbe090f 100644\n--- a/alacritty/src/display/hint.rs\n+++ b/alacritty/src/display/hint.rs\n@@ -204,7 +204,8 @@ pub struct HintMatch {\n impl HintMatch {\n     #[inline]\n     pub fn should_highlight(&self, point: Point, pointed_hyperlink: Option<&Hyperlink>) -> bool {\n-        self.bounds.contains(&point) && self.hyperlink.as_ref() == pointed_hyperlink\n+        self.hyperlink.as_ref() == pointed_hyperlink\n+            && (self.hyperlink.is_some() || self.bounds.contains(&point))\n     }\n \n     #[inline]\n@@ -400,49 +401,30 @@ pub fn highlighted_at<T>(\n }\n \n /// Retrieve the hyperlink with its range, if there is one at the specified point.\n+///\n+/// This will only return contiguous cells, even if another hyperlink with the same ID exists.\n fn hyperlink_at<T>(term: &Term<T>, point: Point) -> Option<(Hyperlink, Match)> {\n     let hyperlink = term.grid()[point].hyperlink()?;\n \n-    let viewport_start = Line(-(term.grid().display_offset() as i32));\n-    let viewport_end = viewport_start + term.bottommost_line();\n-\n-    let mut match_start = Point::new(point.line, Column(0));\n-    let mut match_end = Point::new(point.line, Column(term.columns() - 1));\n     let grid = term.grid();\n \n-    // Find adjacent lines that have the same `hyperlink`. The end purpose to highlight hyperlinks\n-    // that span across multiple lines or not directly attached to each other.\n-\n-    // Find the closest to the viewport start adjacent line.\n-    while match_start.line > viewport_start {\n-        let next_line = match_start.line - 1i32;\n-        // Iterate over all the cells in the grid's line and check if any of those cells contains\n-        // the hyperlink we've found at original `point`.\n-        let line_contains_hyperlink = grid[next_line]\n-            .into_iter()\n-            .any(|cell| cell.hyperlink().map_or(false, |h| h == hyperlink));\n-\n-        // There's no hyperlink on the next line, break.\n-        if !line_contains_hyperlink {\n+    let mut match_end = point;\n+    for cell in grid.iter_from(point) {\n+        if cell.hyperlink().map_or(false, |link| link == hyperlink) {\n+            match_end = cell.point;\n+        } else {\n             break;\n         }\n-\n-        match_start.line = next_line;\n     }\n \n-    // Ditto for the end.\n-    while match_end.line < viewport_end {\n-        let next_line = match_end.line + 1i32;\n-\n-        let line_contains_hyperlink = grid[next_line]\n-            .into_iter()\n-            .any(|cell| cell.hyperlink().map_or(false, |h| h == hyperlink));\n-\n-        if !line_contains_hyperlink {\n+    let mut match_start = point;\n+    let mut iter = grid.iter_from(point);\n+    while let Some(cell) = iter.prev() {\n+        if cell.hyperlink().map_or(false, |link| link == hyperlink) {\n+            match_start = cell.point;\n+        } else {\n             break;\n         }\n-\n-        match_end.line = next_line;\n     }\n \n     Some((hyperlink, match_start..=match_end))\ndiff --git a/alacritty/src/display/mod.rs b/alacritty/src/display/mod.rs\nindex dc37794d9c0..4dafa80fac4 100644\n--- a/alacritty/src/display/mod.rs\n+++ b/alacritty/src/display/mod.rs\n@@ -20,7 +20,7 @@ use winit::dpi::PhysicalSize;\n use winit::keyboard::ModifiersState;\n use winit::window::CursorIcon;\n \n-use crossfont::{self, Rasterize, Rasterizer, Size as FontSize};\n+use crossfont::{Rasterize, Rasterizer, Size as FontSize};\n use unicode_width::UnicodeWidthChar;\n \n use alacritty_terminal::event::{EventListener, OnResize, WindowSize};\ndiff --git a/alacritty/src/logging.rs b/alacritty/src/logging.rs\nindex 42f1536ef3b..a3833a5bab1 100644\n--- a/alacritty/src/logging.rs\n+++ b/alacritty/src/logging.rs\n@@ -12,7 +12,7 @@ use std::sync::{Arc, Mutex, OnceLock};\n use std::time::Instant;\n use std::{env, process};\n \n-use log::{self, Level, LevelFilter};\n+use log::{Level, LevelFilter};\n use winit::event_loop::EventLoopProxy;\n \n use crate::cli::Options;\ndiff --git a/alacritty_terminal/src/event_loop.rs b/alacritty_terminal/src/event_loop.rs\nindex 929c642d829..62dd7440239 100644\n--- a/alacritty_terminal/src/event_loop.rs\n+++ b/alacritty_terminal/src/event_loop.rs\n@@ -5,7 +5,6 @@ use std::collections::VecDeque;\n use std::fmt::{self, Display, Formatter};\n use std::fs::File;\n use std::io::{self, ErrorKind, Read, Write};\n-use std::marker::Send;\n use std::num::NonZeroUsize;\n use std::sync::mpsc::{self, Receiver, Sender, TryRecvError};\n use std::sync::Arc;\ndiff --git a/alacritty_terminal/src/grid/storage.rs b/alacritty_terminal/src/grid/storage.rs\nindex d5709d14e6b..0a2be43b71a 100644\n--- a/alacritty_terminal/src/grid/storage.rs\n+++ b/alacritty_terminal/src/grid/storage.rs\n@@ -1,4 +1,4 @@\n-use std::cmp::{max, PartialEq};\n+use std::cmp::max;\n use std::mem;\n use std::mem::MaybeUninit;\n use std::ops::{Index, IndexMut};\ndiff --git a/alacritty_terminal/src/term/mod.rs b/alacritty_terminal/src/term/mod.rs\nindex 2ed6d7c994c..f17f746252b 100644\n--- a/alacritty_terminal/src/term/mod.rs\n+++ b/alacritty_terminal/src/term/mod.rs\n@@ -2389,10 +2389,8 @@ pub mod test {\n \n     #[cfg(feature = \"serde\")]\n     use serde::{Deserialize, Serialize};\n-    use unicode_width::UnicodeWidthChar;\n \n     use crate::event::VoidListener;\n-    use crate::index::Column;\n \n     #[cfg_attr(feature = \"serde\", derive(Serialize, Deserialize))]\n     pub struct TermSize {\ndiff --git a/alacritty_terminal/src/tty/unix.rs b/alacritty_terminal/src/tty/unix.rs\nindex 8ce04191266..455abbd2f84 100644\n--- a/alacritty_terminal/src/tty/unix.rs\n+++ b/alacritty_terminal/src/tty/unix.rs\n@@ -11,7 +11,7 @@ use std::process::{Child, Command, Stdio};\n use std::sync::Arc;\n use std::{env, ptr};\n \n-use libc::{self, c_int, TIOCSCTTY};\n+use libc::{c_int, TIOCSCTTY};\n use log::error;\n use polling::{Event, PollMode, Poller};\n use rustix_openpty::openpty;\n", "instance_id": "alacritty__alacritty-7824", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the \"Select\" action for hints in Alacritty selects the entire line instead of just the hint text (e.g., a URL). The user provides a configuration snippet, an example scenario with `ls -l --hyperlink=auto`, and references the expected behavior from the documentation. This makes the goal of the issue understandable. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly define what constitutes a \"hint\" in all contexts (beyond hyperlinks), nor does it specify edge cases like multi-line hints or overlapping hints. Additionally, while the user suggests a potential solution (\"SelectHintOnly\" action), the statement lacks clarity on whether the goal is to fix the current behavior or add a new feature. Overall, the description is valid and mostly clear but misses some specifics that would make it comprehensive.", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes is moderate, primarily affecting the `hint.rs` file with modifications to the logic for detecting and selecting hyperlinks. The changes involve adjusting how the bounds of a hyperlink are determined (limiting selection to contiguous cells rather than entire lines or adjacent lines with the same hyperlink). This requires understanding the terminal grid structure and hyperlink matching logic, which is non-trivial but not overly complex. Second, the number of technical concepts involved is moderate, including familiarity with Rust, terminal rendering, and grid iteration logic, but no advanced algorithms or system-level knowledge is required. Third, the changes impact a specific feature (hint selection) without altering the broader architecture of Alacritty. Finally, while edge cases like multi-line hyperlinks or non-contiguous hyperlinks with the same ID are implicitly handled by the code change (limiting to contiguous cells), the problem statement does not explicitly call for extensive error handling or performance optimization. Overall, this task requires a decent understanding of the codebase and careful modification of logic across a focused area, placing it at the lower end of medium difficulty (0.45).", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "feat: `luau` add accumulate helper function\nIt is a generalized loop where the value of the prior iteration is fed back into the current iteration.  That is, it is basically a loop iterating over the elements of a vector x such that the output vector y is recursively given by `y[i] = f(y[i-1], x[i])`. To start off `y[1]` is `x[1]` if `init` is missing and `f(init, x[1])` if `init` is present.  \r\n\r\nFor example, if the function passed to it is addition and `init` is missing or equal to 0 then then `y[i] = x[1] + x[2] + ... + x[i]` . \r\n\r\nThe lua Functional module may have some relevant code as well.\r\n\r\nHere is some lua code and some tests.\r\n\r\n```\r\n-- x is input vector, f is function of two args, init is optional\r\n-- output is vector same length as x\r\nfunction accumulate(x, f, init) do\r\n  y = {}\r\n  for i = 1, #x do\r\n    if (i == 1) then\r\n      if (init) then y[1] = f(init, x[1]) else y[1] = x[1] end\r\n    else\r\n      y[i] = f(y[i-1], x[i])\r\n    end\r\n  end\r\n  return y end\r\nend\r\n\r\n-- tests\r\n\r\nprint(\"cumulative sum\")\r\nx = {3, 5, 4, 1}\r\nadd = function(x, y) do return x + y end end\r\ny = accumulate(x, add)\r\nfor i = 1, #y do print(x[i], y[i]) end\r\n\r\nprint(\"\\ncumulative sum with reset once sum exceeds 7\")\r\nadd2 = function(x, y) do \r\n  if x > 7 then z = y else z = x+y end\r\n  return z end\r\nend\r\ny = accumulate(x, add2)\r\nfor i = 1, #y do print(x[i], y[i]) end\r\n```\r\n\r\ngiving\r\n\r\n```\r\ncumulative sum\r\n3       3\r\n5       8\r\n4       12\r\n1       13\r\n\r\ncumulative sum with reset once sum exceeds 7\r\n3       3\r\n5       8\r\n4       4  <-- note how it reset here since 8 > 7\r\n1       5\t\r\n```\r\n\r\n_Originally posted by @ggrothendieck in https://github.com/dathere/qsv/discussions/2526#discussioncomment-12229670_\n", "patch": "diff --git a/src/cmd/luau.rs b/src/cmd/luau.rs\nindex 5bba954e0..cd1ae7f68 100644\n--- a/src/cmd/luau.rs\n+++ b/src/cmd/luau.rs\n@@ -2439,6 +2439,85 @@ fn setup_helpers(\n     })?;\n     luau.globals().set(\"qsv_cumall\", qsv_cumall)?;\n \n+    // this is a helper function that implements accumulate functionality.\n+    // It takes a column, a function of two arguments, and an optional initial value.\n+    // If y = accumulate(x, f, init) then y[1] = init and for i > 1 we have y[i] = f(y[i-1], x[i])\n+    // The default for init is x[1].\n+    //\n+    //   qsv_accumulate(column: string, func: function, init?: any)\n+    //          column: the name of the column to accumulate over\n+    //            func: function that takes two arguments (prev accumulated value, current val)\n+    //            init: optional initial value. If not provided, first row of column is used\n+    //         returns: table containing accumulated values\n+    //                  or Luau runtime error if invalid arguments\n+    //\n+    let qsv_accumulate = luau.create_function(|luau, args: mlua::MultiValue| {\n+        // Convert args to Vec for easier handling\n+        let args: Vec<mlua::Value> = args.into_iter().collect();\n+\n+        if args.len() < 2 {\n+            return helper_err!(\n+                \"qsv_accumulate\",\n+                \"requires at least 2 arguments: column name and function\"\n+            );\n+        }\n+\n+        // Get the column name\n+        let column_name = args[0].to_string()?;\n+\n+        // Get the current value from the column\n+        let curr_value = luau\n+            .globals()\n+            .get::<String>(&*column_name)?\n+            .parse::<f64>()\n+            .unwrap_or(0.0);\n+\n+        // Get the function\n+        let func = match &args[1] {\n+            mlua::Value::Function(f) => f.clone(),\n+            _ => {\n+                return helper_err!(\"qsv_accumulate\", \"second argument must be a function\");\n+            },\n+        };\n+\n+        // Generate unique name for the accumulator state\n+        let state_name = format!(\"_qsv_accumulate_state_{column_name}\");\n+\n+        // Get existing accumulator value or use initial value\n+        let prev_acc = if let Ok(prev) = luau.globals().get::<f64>(&*state_name) {\n+            prev\n+        } else {\n+            // Get initial value from args or default to 0.0\n+            let init_value = if args.len() > 2 {\n+                match &args[2] {\n+                    mlua::Value::Number(n) => *n,\n+                    mlua::Value::Integer(i) => *i as f64,\n+                    mlua::Value::String(s) => s.to_string_lossy().parse::<f64>().unwrap_or(0.0),\n+                    _ => 0.0,\n+                }\n+            } else {\n+                0.0 // Always start with 0.0 as default initial value\n+            };\n+            luau.globals().set(&*state_name, init_value)?;\n+            init_value\n+        };\n+\n+        // Call the accumulator function\n+        let result = match func.call::<mlua::Value>((prev_acc, curr_value)) {\n+            Ok(mlua::Value::Number(n)) => n,\n+            Ok(mlua::Value::Integer(i)) => i as f64,\n+            Ok(mlua::Value::String(s)) => s.to_string_lossy().parse::<f64>().unwrap_or(prev_acc),\n+            Ok(_) => prev_acc,\n+            Err(e) => return Err(e),\n+        };\n+\n+        // Store the new accumulated value\n+        luau.globals().set(&*state_name, result)?;\n+\n+        Ok(result)\n+    })?;\n+    luau.globals().set(\"qsv_accumulate\", qsv_accumulate)?;\n+\n     // qsv_diff - returns difference between current and previous value\n     //\n     //   qsv_diff(name, value[, periods])\n", "instance_id": "dathere__qsv-2537", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear in describing the goal of implementing an \"accumulate\" helper function for iterative computation over a vector or column, where each result depends on the previous iteration's result. It provides a clear mathematical definition (y[i] = f(y[i-1], x[i])) and includes Lua code examples and test cases that illustrate the expected behavior, including a conditional reset scenario. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly address edge cases such as empty input vectors or invalid function arguments, nor does it specify how errors should be handled in the Rust implementation. Additionally, while it references the Lua Functional module, it does not clarify how or if this should influence the implementation in the `luau.rs` file. The provided test output helps, but the lack of explicit constraints or edge case discussion prevents a perfect score. Hence, I rate this as \"Mostly Clear\" with a score of 2.", "difficulty_explanation": "The difficulty of this problem falls into the \"Medium\" range with a score of 0.45, based on the following analysis across the specified factors:\n\n1. **Scope and Depth of Code Changes:** The code changes are confined to a single file (`src/cmd/luau.rs`) and involve adding a new helper function `qsv_accumulate` with approximately 80 lines of code. This is a moderate amount of change, focused on a specific feature addition without impacting the broader system architecture or requiring modifications across multiple modules. However, it does require integration with the existing Luau environment (e.g., interacting with globals and handling state persistence), which adds a slight layer of complexity.\n\n2. **Number of Technical Concepts:** Implementing this feature requires understanding several concepts, including Rust's integration with Lua via the `mlua` crate, handling dynamic typing and value conversion (e.g., parsing strings to `f64`), state management using global variables, and function invocation in a scripting context. While these concepts are not overly advanced, they do require familiarity with Rust's type system, error handling, and the specifics of the `mlua` library. The logic of accumulation itself is straightforward (iterative application of a function), but ensuring correct state handling and argument validation adds moderate complexity.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code changes include basic error handling for invalid arguments (e.g., missing arguments or non-function second argument). However, the implementation does not fully address potential issues like non-numeric column values beyond a default fallback to 0.0, or what happens if the state name conflicts with existing globals. Handling the optional `init` parameter and ensuring type safety in a dynamic environment also introduces minor challenges. These edge cases are not overly complex but require thoughtful implementation.\n\n4. **Overall Assessment:** This task is more than a simple bug fix or basic feature addition (which would fall in the 0.2-0.4 range) due to the need to handle dynamic scripting interactions and state persistence in a Rust-Lua bridge context. However, it does not reach the \"Hard\" range (0.6-0.8) as it does not involve deep architectural changes, complex algorithms, or extensive performance optimization. It sits comfortably in the \"Medium\" category, requiring a moderate understanding of the codebase and careful handling of a few technical concepts and edge cases. Thus, a score of 0.45 reflects this balance of complexity and scope.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`stats`: rename `--stats-json` option to `--stats-jsonl`\nAs it actually creates a jsonl file.\r\n\r\nAlso the name of the generated json file should have the `.jsonl` extension.\n", "patch": "diff --git a/src/cmd/schema.rs b/src/cmd/schema.rs\nindex fc30ce521..ddc8c9a2d 100644\n--- a/src/cmd/schema.rs\n+++ b/src/cmd/schema.rs\n@@ -21,9 +21,9 @@ a `stdin.csv` file will created with stdin's contents as well.\n Note that `stdin.csv` will be overwritten if it already exists.\n \n Schema generation can be a compute-intensive process, especially for large CSV files.\n-To speed up generation, the `schema` command will reuse a `stats.csv.data.json` file if it\n+To speed up generation, the `schema` command will reuse a `stats.csv.data.jsonl` file if it\n exists and is current (i.e. stats generated with --cardinality and --infer-dates options).\n-Otherwise, it will run the `stats` command to generate the `stats.csv.data.json` file first,\n+Otherwise, it will run the `stats` command to generate the `stats.csv.data.jsonl` file first,\n and then use that to generate the schema file.\n \n For examples, see https://github.com/jqnatividad/qsv/blob/master/tests/test_schema.rs.\ndiff --git a/src/cmd/stats.rs b/src/cmd/stats.rs\nindex 00632666c..4e0277cf1 100644\n--- a/src/cmd/stats.rs\n+++ b/src/cmd/stats.rs\n@@ -51,9 +51,9 @@ chunks and each chunk is processed in parallel. The number of chunks is determin\n number of logical CPUs detected. You can override this by setting the --jobs option.\n \n As stats is a central command in qsv, and can be expensive to compute, `stats` caches results\n-in <FILESTEM>.stats.csv & if the --stats-json option is used, <FILESTEM>.stats.csv.data.json \n-(e.g., qsv stats nyc311.csv will create nyc311.stats.csv & nyc311.stats.csv.data.json).\n-The arguments used to generate the cached stats are saved in <FILESTEM>.stats.csv.json.\n+in <FILESTEM>.stats.csv & if the --stats-json option is used, <FILESTEM>.stats.csv.data.jsonl\n+(e.g., qsv stats nyc311.csv will create nyc311.stats.csv & nyc311.stats.csv.data.jsonl).\n+The arguments used to generate the cached stats are saved in <FILESTEM>.stats.csv.jsonl.\n \n If stats have already been computed for the input file with similar arguments and the file\n hasn't changed, the stats will be loaded from the cache instead of recomputing it.\n@@ -182,11 +182,11 @@ stats options:\n                               Note that a file handle is opened for each job.\n                               When not set, the number of jobs is set to the\n                               number of CPUs detected.\n-    --stats-json              Also write the stats in json format. \n-                              If set, the stats will be written to <FILESTEM>.stats.csv.data.json.\n+    --stats-jsonl             Also write the stats in JSONL format. \n+                              If set, the stats will be written to <FILESTEM>.stats.csv.data.jsonl.\n                               Note that this option used internally by other qsv commands\n                               (currently `frequency`, `schema` & `tojsonl`) to load cached stats. \n-                              You can preemptively create the stats-json file by using\n+                              You can preemptively create the stats-jsonl file by using\n                               this option BEFORE running the `frequency`, `schema` & `tojsonl`\n                               commands and they will automatically use it.\n  -c, --cache-threshold <arg>  When greater than 1, the threshold in milliseconds before caching\n@@ -227,7 +227,9 @@ a JSONschema based on a CSV's summary statistics; and use the generated JSONsche\n quickly validate complex CSVs (NYC's 311 data) at almost 930,000 records/sec.\n \n It's type inferences are also used by the `tojsonl` command to generate properly typed\n-JSONL files.\n+JSONL files; the `frequency` command to short-circuit frequency table generation for columns\n+with all unique values; and the `schema` command to set the data type of a column and identify\n+low-cardinality columns for enum generation in the JSONschema.\n \n To safeguard against undefined behavior, `stats` is the most extensively tested command,\n with ~500 tests.\n@@ -278,7 +280,7 @@ pub struct Args {\n     pub flag_prefer_dmy:      bool,\n     pub flag_force:           bool,\n     pub flag_jobs:            Option<usize>,\n-    pub flag_stats_json:      bool,\n+    pub flag_stats_jsonl:     bool,\n     pub flag_cache_threshold: isize,\n     pub flag_output:          Option<String>,\n     pub flag_no_headers:      bool,\n@@ -520,10 +522,10 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n     }\n \n     let mut compute_stats = true;\n-    let mut create_cache = args.flag_cache_threshold > 0 || args.flag_stats_json;\n+    let mut create_cache = args.flag_cache_threshold > 0 || args.flag_stats_jsonl;\n     let mut autoindex_set = false;\n \n-    let write_stats_json = args.flag_stats_json;\n+    let write_stats_jsonl = args.flag_stats_jsonl;\n \n     if let Some(path) = fconfig.path.clone() {\n         let path_file_stem = path.file_stem().unwrap().to_str().unwrap();\n@@ -768,9 +770,9 @@ pub fn run(argv: &[&str]) -> CliResult<()> {\n                 serde_json::to_string_pretty(&current_stats_args).unwrap(),\n             )?;\n \n-            // save the stats data to \"<FILESTEM>.stats.csv.data.json\"\n-            if write_stats_json {\n-                stats_pathbuf.set_extension(\"data.json\");\n+            // save the stats data to \"<FILESTEM>.stats.csv.data.jsonl\"\n+            if write_stats_jsonl {\n+                stats_pathbuf.set_extension(\"data.jsonl\");\n                 util::csv_to_jsonl(&currstats_filename, &STATSDATA_TYPES_ARRAY, stats_pathbuf)?;\n             }\n         }\ndiff --git a/src/cmd/tojsonl.rs b/src/cmd/tojsonl.rs\nindex 9d66e8855..53b9f58d6 100644\n--- a/src/cmd/tojsonl.rs\n+++ b/src/cmd/tojsonl.rs\n@@ -9,9 +9,9 @@ It will infer a column as boolean if its cardinality is 2, and the first charact\n the values are one of the following case-insensitive combinations:\n   t/f; t/null; 1/0; 1/null; y/n & y/null are treated as true/false.\n \n-The `tojsonl` command will reuse a `stats.csv.data.json` file if it exists and is current\n-(i.e. stats generated with --cardinality and --infer-dates options) and will skip\n-recomputing stats.\n+The `tojsonl` command will reuse a `stats.csv.data.jsonl` file if it exists and is\n+current (i.e. stats generated with --cardinality and --infer-dates options) and will\n+skip recomputing stats.\n \n For examples, see https://github.com/jqnatividad/qsv/blob/master/tests/test_tojsonl.rs.\n \ndiff --git a/src/util.rs b/src/util.rs\nindex d30a345de..93f1a8894 100644\n--- a/src/util.rs\n+++ b/src/util.rs\n@@ -1915,8 +1915,8 @@ pub fn trim_bs_whitespace(bytes: &[u8]) -> &[u8] {\n     &bytes[start..end]\n }\n \n-/// get stats records from stats.csv.data.json file, or if its invalid, by running the stats command\n-/// returns tuple (`csv_fields`, `csv_stats`, `stats_col_index_map`)\n+/// get stats records from stats.csv.data.jsonl file, or if its invalid, by running the stats\n+/// command returns tuple (`csv_fields`, `csv_stats`, `stats_col_index_map`)\n pub fn get_stats_records(\n     args: &SchemaArgs,\n     mode: StatsMode,\n@@ -1931,7 +1931,7 @@ pub fn get_stats_records(\n     };\n \n     let canonical_input_path = Path::new(&args.arg_input.clone().unwrap()).canonicalize()?;\n-    let statsdata_path = canonical_input_path.with_extension(\"stats.csv.data.json\");\n+    let statsdata_path = canonical_input_path.with_extension(\"stats.csv.data.jsonl\");\n \n     let stats_data_current = if statsdata_path.exists() {\n         let statsdata_metadata = std::fs::metadata(&statsdata_path)?;\n@@ -1939,16 +1939,17 @@ pub fn get_stats_records(\n         let input_metadata = std::fs::metadata(args.arg_input.clone().unwrap())?;\n \n         if statsdata_metadata.modified()? > input_metadata.modified()? {\n-            info!(\"Valid stats.csv.data.json file found!\");\n+            info!(\"Valid stats.csv.data.jsonl file found!\");\n             true\n         } else {\n             info!(\n-                \"stats.csv.data.json file is older than input file. Regenerating stats json file.\"\n+                \"stats.csv.data.jsonl file is older than input file. Regenerating stats jsonl \\\n+                 file.\"\n             );\n             false\n         }\n     } else {\n-        info!(\"stats.csv.data.json file does not exist: {statsdata_path:?}\");\n+        info!(\"stats.csv.data.jsonl file does not exist: {statsdata_path:?}\");\n         false\n     };\n \n@@ -1995,7 +1996,7 @@ pub fn get_stats_records(\n             flag_prefer_dmy:      args.flag_prefer_dmy,\n             flag_force:           args.flag_force,\n             flag_jobs:            Some(njobs(args.flag_jobs)),\n-            flag_stats_json:      true,\n+            flag_stats_jsonl:     true,\n             flag_cache_threshold: 1, // force the creation of stats cache files\n             flag_output:          None,\n             flag_no_headers:      args.flag_no_headers,\n@@ -2003,21 +2004,21 @@ pub fn get_stats_records(\n             flag_memcheck:        args.flag_memcheck,\n         };\n \n-        // otherwise, run stats command to generate stats.csv.data.json file\n+        // otherwise, run stats command to generate stats.csv.data.jsonl file\n         let tempfile = tempfile::Builder::new()\n             .suffix(\".stats.csv\")\n             .tempfile()\n             .unwrap();\n         let tempfile_path = tempfile.path().to_str().unwrap().to_string();\n \n-        let statsdatajson_path = canonical_input_path.with_extension(\"stats.csv.data.json\");\n+        let statsdatajson_path = canonical_input_path.with_extension(\"stats.csv.data.jsonl\");\n \n         let mut stats_args_str = if mode == StatsMode::Schema {\n             // mode is GetStatsMode::Schema\n             // we're generating schema, so we cardinality and to infer-dates\n             format!(\n                 \"stats {input} --infer-dates --dates-whitelist {dates_whitelist} --round 4 \\\n-                 --cardinality --output {output} --stats-json --force\",\n+                 --cardinality --stats-jsonl --force --output {output}\",\n                 input = {\n                     if let Some(arg_input) = stats_args.arg_input.clone() {\n                         arg_input\n@@ -2032,7 +2033,7 @@ pub fn get_stats_records(\n             // mode is GetStatsMode::Frequency or GetStatsMode::FrequencyForceStats\n             // we're doing frequency, so we just need cardinality\n             format!(\n-                \"stats {input} --cardinality --stats-json --output {output}\",\n+                \"stats {input} --cardinality --stats-jsonl --output {output}\",\n                 input = {\n                     if let Some(arg_input) = stats_args.arg_input.clone() {\n                         arg_input\n", "instance_id": "dathere__qsv-2063", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to rename the `--stats-json` option to `--stats-jsonl` and update the file extension of the generated JSON file to `.jsonl`. It specifies the goal of the change (renaming an option and updating file extensions to reflect the JSONL format) and provides context about the file format. However, it lacks explicit mention of the full scope of changes required, such as whether documentation, comments, or other parts of the codebase need updates beyond the provided diff. Additionally, there are no examples or detailed descriptions of expected behavior, and edge cases or potential impacts on existing users/scripts are not addressed. While the intent is understandable, these minor ambiguities prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it primarily involves straightforward renaming of a command-line option and updating file extensions in multiple places across the codebase. The code changes are mostly textual replacements (e.g., changing `--stats-json` to `--stats-jsonl` and `.json` to `.jsonl` in file names and documentation) and do not require deep understanding of complex logic or architecture. The modifications span multiple files (`schema.rs`, `stats.rs`, `tojsonl.rs`, `util.rs`), but the changes are repetitive and localized to specific lines, with no significant impact on the system's behavior or architecture. The technical concepts involved are minimal, focusing on basic string replacement and understanding of command-line argument parsing (likely using a library like `clap` in Rust). There are no apparent edge cases or error handling requirements mentioned in the problem statement or evident in the code changes, as the rename does not alter functionality. Overall, this task requires minimal effort and expertise, suitable for a junior developer or someone with basic familiarity with the codebase.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Tokenizers v0.20.2 fails on batches as tuples\nCertain fast tokenizers now fail on batches given as tuples, e.g. (on a MacBook M2 with transformers 4.46.1):\r\n\r\n```\r\n>>> from transformers import AutoTokenizer\r\n>>> tok = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\r\n>>> tok.batch_encode_plus((\"hello there\", \"bye bye bye\"))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/oyvindt/miniconda3/envs/oe-eval/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\r\n    return self._batch_encode_plus(\r\n  File \"/Users/oyvindt/miniconda3/envs/oe-eval/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\", line 127, in _batch_encode_plus\r\n    return super()._batch_encode_plus(*args, **kwargs)\r\n  File \"/Users/oyvindt/miniconda3/envs/oe-eval/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\r\n    encodings = self._tokenizer.encode_batch(\r\nTypeError: argument 'input': 'tuple' object cannot be converted to 'PyList'\r\n```\r\n\r\nThis works in v0.20.1. Presumably related to this PR: https://github.com/huggingface/tokenizers/pull/1665\r\n\r\nThe code for `batch_encode_plus` in transformers claims to be working for [both tuples and lists](https://github.com/huggingface/transformers/blob/eb811449a2389e48930c45f84c88fd041735cf92/src/transformers/tokenization_utils_fast.py#L508):\r\n```\r\n        if not isinstance(batch_text_or_text_pairs, (tuple, list)):\r\n            raise TypeError(\r\n                f\"batch_text_or_text_pairs has to be a list or a tuple (got {type(batch_text_or_text_pairs)})\"\r\n            )\r\n```\n", "patch": "diff --git a/bindings/python/py_src/tokenizers/__init__.pyi b/bindings/python/py_src/tokenizers/__init__.pyi\nindex 6c731ff0a..11e6e556c 100644\n--- a/bindings/python/py_src/tokenizers/__init__.pyi\n+++ b/bindings/python/py_src/tokenizers/__init__.pyi\n@@ -859,7 +859,9 @@ class Tokenizer:\n     def encode_batch(self, input, is_pretokenized=False, add_special_tokens=True):\n         \"\"\"\n         Encode the given batch of inputs. This method accept both raw text sequences\n-        as well as already pre-tokenized sequences.\n+        as well as already pre-tokenized sequences. The reason we use `PySequence` is\n+        because it allows type checking with zero-cost (according to PyO3) as we don't\n+        have to convert to check.\n \n         Example:\n             Here are some examples of the inputs that are accepted::\ndiff --git a/bindings/python/src/tokenizer.rs b/bindings/python/src/tokenizer.rs\nindex 7fd03ae89..aa7019f2d 100644\n--- a/bindings/python/src/tokenizer.rs\n+++ b/bindings/python/src/tokenizer.rs\n@@ -995,7 +995,9 @@ impl PyTokenizer {\n     }\n \n     /// Encode the given batch of inputs. This method accept both raw text sequences\n-    /// as well as already pre-tokenized sequences.\n+    /// as well as already pre-tokenized sequences. The reason we use `PySequence` is\n+    /// because it allows type checking with zero-cost (according to PyO3) as we don't\n+    /// have to convert to check.\n     ///\n     /// Example:\n     ///     Here are some examples of the inputs that are accepted::\n@@ -1030,25 +1032,24 @@ impl PyTokenizer {\n     fn encode_batch(\n         &self,\n         py: Python<'_>,\n-        input: Bound<'_, PyList>,\n+        input: Bound<'_, PySequence>,\n         is_pretokenized: bool,\n         add_special_tokens: bool,\n     ) -> PyResult<Vec<PyEncoding>> {\n-        let input: Vec<tk::EncodeInput> = input\n-            .into_iter()\n-            .map(|o| {\n-                let input: tk::EncodeInput = if is_pretokenized {\n-                    o.extract::<PreTokenizedEncodeInput>()?.into()\n-                } else {\n-                    o.extract::<TextEncodeInput>()?.into()\n-                };\n-                Ok(input)\n-            })\n-            .collect::<PyResult<Vec<tk::EncodeInput>>>()?;\n+        let mut items = Vec::<tk::EncodeInput>::with_capacity(input.len()?);\n+        for i in 0..input.len()? {\n+            let item = input.get_item(i)?;\n+            let item: tk::EncodeInput = if is_pretokenized {\n+                item.extract::<PreTokenizedEncodeInput>()?.into()\n+            } else {\n+                item.extract::<TextEncodeInput>()?.into()\n+            };\n+            items.push(item);\n+        }\n         py.allow_threads(|| {\n             ToPyResult(\n                 self.tokenizer\n-                    .encode_batch_char_offsets(input, add_special_tokens)\n+                    .encode_batch_char_offsets(items, add_special_tokens)\n                     .map(|encodings| encodings.into_iter().map(|e| e.into()).collect()),\n             )\n             .into()\n@@ -1091,25 +1092,24 @@ impl PyTokenizer {\n     fn encode_batch_fast(\n         &self,\n         py: Python<'_>,\n-        input: Bound<'_, PyList>,\n+        input: Bound<'_, PySequence>,\n         is_pretokenized: bool,\n         add_special_tokens: bool,\n     ) -> PyResult<Vec<PyEncoding>> {\n-        let input: Vec<tk::EncodeInput> = input\n-            .into_iter()\n-            .map(|o| {\n-                let input: tk::EncodeInput = if is_pretokenized {\n-                    o.extract::<PreTokenizedEncodeInput>()?.into()\n-                } else {\n-                    o.extract::<TextEncodeInput>()?.into()\n-                };\n-                Ok(input)\n-            })\n-            .collect::<PyResult<Vec<tk::EncodeInput>>>()?;\n+        let mut items = Vec::<tk::EncodeInput>::with_capacity(input.len()?);\n+        for i in 0..input.len()? {\n+            let item = input.get_item(i)?;\n+            let item: tk::EncodeInput = if is_pretokenized {\n+                item.extract::<PreTokenizedEncodeInput>()?.into()\n+            } else {\n+                item.extract::<TextEncodeInput>()?.into()\n+            };\n+            items.push(item);\n+        }\n         py.allow_threads(|| {\n             ToPyResult(\n                 self.tokenizer\n-                    .encode_batch_fast(input, add_special_tokens)\n+                    .encode_batch_fast(items, add_special_tokens)\n                     .map(|encodings| encodings.into_iter().map(|e| e.into()).collect()),\n             )\n             .into()\n", "instance_id": "huggingface__tokenizers-1673", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: certain fast tokenizers fail when batches are provided as tuples, which worked in a previous version (v0.20.1). It includes a reproducible example with a traceback, which helps in understanding the error context, and references a related PR that might have introduced the issue. Additionally, it points to a specific part of the transformers library code that claims to support both tuples and lists, highlighting a discrepancy. However, there are minor ambiguities: the problem statement does not explicitly define the expected behavior (e.g., should tuples be converted to lists internally, or is there a deeper issue with type handling in the tokenizer?). It also lacks details on specific edge cases or constraints beyond the provided example (e.g., does this issue occur with nested tuples or other sequence types?). Overall, while the issue is clear, these missing details prevent it from being comprehensive.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided code changes are localized to a specific part of the codebase, primarily in the `encode_batch` and `encode_batch_fast` methods of the tokenizer in Rust (within `tokenizer.rs`) and a minor update to the type hints in the Python interface (`__init__.pyi`). The changes involve modifying the input type from `PyList` to `PySequence`, which allows handling of tuples and other sequence types, and adjusting the way input items are extracted. This does not impact the broader system architecture and is confined to a single module with a moderate amount of code change (around 30-40 lines across two functions).\n\n2. **Technical Concepts Involved:** Solving this requires understanding Rust's interaction with Python via PyO3 (a Rust library for Python bindings), specifically how Python objects like lists and sequences are handled in Rust. It also involves familiarity with the `tokenizers` library's encoding logic. While these concepts are not trivial, they are relatively straightforward for someone with experience in Rust and Python interoperability. No advanced algorithms, design patterns, or domain-specific knowledge beyond tokenizer functionality are required.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases beyond the tuple input failure, and the code changes do not introduce new error handling logic. However, the developer must ensure that switching to `PySequence` does not inadvertently break other input types (e.g., lists or custom sequences), which adds a small layer of complexity. The error handling in the existing code (e.g., `extract` operations returning `PyResult`) remains unchanged.\n\n4. **Overall Complexity:** The fix is relatively simple\u2014changing the expected input type and iterating over the sequence manually instead of using `into_iter()`. It requires understanding the type mismatch issue and applying a targeted solution, but it does not involve deep architectural changes or complex debugging across multiple modules. The main challenge lies in ensuring compatibility with the Python side (e.g., transformers library expectations), but this is manageable with basic testing.\n\nGiven these points, a difficulty score of 0.35 reflects an \"Easy\" problem that requires some understanding of Rust-Python bindings and minor code modifications, with limited impact on the broader codebase and minimal edge case complexity.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Ignore Season 0 when using \"Mark all unseen episodes before this as seen\"\nTo reproduce: \r\n1. Click \"Mark as seen\" in any season > 0\r\n2. Tick \"Mark all unseen episodes before this as seen\"\r\n3. Observe that all Specials (Season 0) are also marked as seen\r\n\r\nAs you said in other issues yourself, people don't typically watch these specials, so I would like to ignore them\n", "patch": "diff --git a/apps/frontend/app/routes/_dashboard.fitness.$action.tsx b/apps/frontend/app/routes/_dashboard.fitness.$action.tsx\nindex 77765fa87a..d3cd6903d3 100644\n--- a/apps/frontend/app/routes/_dashboard.fitness.$action.tsx\n+++ b/apps/frontend/app/routes/_dashboard.fitness.$action.tsx\n@@ -322,7 +322,11 @@ export default function Page() {\n \t\t\t\t\tconst exerciseIdx = currentWorkout?.exercises.findIndex(\n \t\t\t\t\t\t(c) => c.identifier === triggeredBy.exerciseIdentifier,\n \t\t\t\t\t);\n-\t\t\t\t\tif (exerciseIdx !== undefined && exerciseIdx !== -1) {\n+\t\t\t\t\tif (\n+\t\t\t\t\t\texerciseIdx !== -1 &&\n+\t\t\t\t\t\texerciseIdx !== undefined &&\n+\t\t\t\t\t\tuserPreferences.fitness.logging.promptForRestTimer\n+\t\t\t\t\t) {\n \t\t\t\t\t\tperformTasksAfterSetConfirmed(triggeredBy.setIdx, exerciseIdx);\n \t\t\t\t\t}\n \t\t\t\t}\ndiff --git a/apps/frontend/app/routes/_dashboard.settings.preferences.tsx b/apps/frontend/app/routes/_dashboard.settings.preferences.tsx\nindex 292a153967..c41a19d34a 100644\n--- a/apps/frontend/app/routes/_dashboard.settings.preferences.tsx\n+++ b/apps/frontend/app/routes/_dashboard.settings.preferences.tsx\n@@ -490,7 +490,7 @@ export default function Page() {\n \t\t\t\t\t\t\t\t\t\t<IconBellRinging />\n \t\t\t\t\t\t\t\t\t</ActionIcon>\n \t\t\t\t\t\t\t\t\t<Text size=\"xs\">\n-\t\t\t\t\t\t\t\t\t\tShow me notifications related to the current workout\n+\t\t\t\t\t\t\t\t\t\tSend me notifications related to the current workout\n \t\t\t\t\t\t\t\t\t</Text>\n \t\t\t\t\t\t\t\t</Group>\n \t\t\t\t\t\t\t\t<Select\n@@ -544,6 +544,7 @@ export default function Page() {\n \t\t\t\t\t\t\t\t</SimpleGrid>\n \t\t\t\t\t\t\t</Input.Wrapper>\n \t\t\t\t\t\t\t<Divider />\n+\t\t\t\t\t\t\t<Title order={4}>Current workout</Title>\n \t\t\t\t\t\t\t{(\n \t\t\t\t\t\t\t\t[\n \t\t\t\t\t\t\t\t\t\"muteSounds\",\ndiff --git a/apps/frontend/app/routes/actions.tsx b/apps/frontend/app/routes/actions.tsx\nindex bbe3b743ac..8349ed7718 100644\n--- a/apps/frontend/app/routes/actions.tsx\n+++ b/apps/frontend/app/routes/actions.tsx\n@@ -351,6 +351,7 @@ export const action = async ({ request }: ActionFunctionArgs) => {\n \t\t\t\t\t\te.seasonNumber === submission.showSeasonNumber &&\n \t\t\t\t\t\te.episodeNumber === submission.showEpisodeNumber,\n \t\t\t\t);\n+\t\t\t\tconst selectedEpisode = allEpisodesInShow[selectedEpisodeIndex];\n \t\t\t\tconst firstEpisodeOfShow = allEpisodesInShow[0];\n \t\t\t\tconst lastSeenEpisode = latestHistoryItem?.showExtraInformation || {\n \t\t\t\t\tepisode: firstEpisodeOfShow.episodeNumber,\n@@ -365,11 +366,17 @@ export const action = async ({ request }: ActionFunctionArgs) => {\n \t\t\t\t\tlastSeenEpisodeIndex + (latestHistoryItem ? 1 : 0);\n \t\t\t\tif (selectedEpisodeIndex > firstEpisodeIndexToMark) {\n \t\t\t\t\tfor (let i = firstEpisodeIndexToMark; i < selectedEpisodeIndex; i++) {\n-\t\t\t\t\t\tconst episode = allEpisodesInShow[i];\n+\t\t\t\t\t\tconst currentEpisode = allEpisodesInShow[i];\n+\t\t\t\t\t\tif (\n+\t\t\t\t\t\t\tcurrentEpisode.seasonNumber === 0 &&\n+\t\t\t\t\t\t\tselectedEpisode.seasonNumber !== 0\n+\t\t\t\t\t\t) {\n+\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\t}\n \t\t\t\t\t\tupdates.push({\n \t\t\t\t\t\t\t...variables,\n-\t\t\t\t\t\t\tshowSeasonNumber: episode.seasonNumber,\n-\t\t\t\t\t\t\tshowEpisodeNumber: episode.episodeNumber,\n+\t\t\t\t\t\t\tshowSeasonNumber: currentEpisode.seasonNumber,\n+\t\t\t\t\t\t\tshowEpisodeNumber: currentEpisode.episodeNumber,\n \t\t\t\t\t\t});\n \t\t\t\t\t}\n \t\t\t\t}\ndiff --git a/crates/utils/dependent/src/lib.rs b/crates/utils/dependent/src/lib.rs\nindex cd15a84c71..21ad065daa 100644\n--- a/crates/utils/dependent/src/lib.rs\n+++ b/crates/utils/dependent/src/lib.rs\n@@ -1697,7 +1697,7 @@ pub async fn create_or_update_user_workout(\n     ss: &Arc<SupportingService>,\n ) -> Result<String> {\n     let end_time = input.end_time;\n-    let (duration, durations) = match input.durations.clone() {\n+    let (mut duration, mut durations) = match input.durations.clone() {\n         Some(durations) => {\n             if durations.is_empty() {\n                 return Err(Error::new(\"Durations cannot be empty\"));\n@@ -1729,11 +1729,15 @@ pub async fn create_or_update_user_workout(\n     };\n     let mut input = input;\n     let (new_workout_id, to_update_workout) = match &input.update_workout_id {\n-        Some(id) => (\n-            id.to_owned(),\n+        Some(id) => {\n             // DEV: Unwrap to make sure we error out early if the workout to edit does not exist\n-            Some(Workout::find_by_id(id).one(&ss.db).await?.unwrap()),\n-        ),\n+            let model = Workout::find_by_id(id).one(&ss.db).await?.unwrap();\n+            duration = model.duration.try_into().unwrap();\n+            if let Some(d) = model.information.durations.clone() {\n+                durations = d;\n+            }\n+            (id.to_owned(), Some(model))\n+        }\n         None => (\n             input\n                 .create_workout_id\n@@ -1960,6 +1964,13 @@ pub async fn create_or_update_user_workout(\n         template_id: input.template_id,\n         repeated_from: input.repeated_from,\n         duration: duration.try_into().unwrap(),\n+        information: WorkoutInformation {\n+            assets: input.assets,\n+            comment: input.comment,\n+            supersets: input.supersets,\n+            durations: Some(durations),\n+            exercises: processed_exercises,\n+        },\n         summary: WorkoutSummary {\n             focused,\n             total: Some(summary_total),\n@@ -1974,13 +1985,6 @@ pub async fn create_or_update_user_workout(\n                 })\n                 .collect(),\n         },\n-        information: WorkoutInformation {\n-            assets: input.assets,\n-            comment: input.comment,\n-            supersets: input.supersets,\n-            durations: Some(durations),\n-            exercises: processed_exercises,\n-        },\n     };\n     let mut insert: workout::ActiveModel = model.into();\n     if let Some(old_workout) = to_update_workout.clone() {\n", "instance_id": "IgnisDa__ryot-1164", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in terms of the goal: to ignore Season 0 (Specials) when marking unseen episodes as seen before a selected episode. The reproduction steps provided are helpful in understanding the issue, and the intent to exclude specials from being marked as seen is evident. However, there are minor ambiguities and missing details. For instance, the problem does not explicitly define whether Season 0 should always be ignored or only when the selected episode is from a season greater than 0. Additionally, there are no examples of input/output or specific constraints mentioned (e.g., what happens if all episodes are in Season 0?). These gaps prevent the statement from being comprehensive, but the overall intent is understandable with the provided context and code changes.", "difficulty_explanation": "The difficulty of this problem falls in the \"Easy\" range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The relevant code change primarily affects a single file (`apps/frontend/app/routes/actions.tsx`), where a conditional check is added to skip marking episodes from Season 0 when the selected episode is from a different season. The change is localized to a specific loop within the episode marking logic and involves a small number of lines. However, there are additional unrelated changes in other files (e.g., fitness logging, workout updates in Rust) that appear to be part of a larger diff but are not relevant to the stated problem. Focusing on the relevant change, the scope is limited and does not impact the broader system architecture.\n\n2. **Number of Technical Concepts:** Solving this requires basic understanding of JavaScript/TypeScript (given the frontend context), control flow (loops and conditionals), and data structures (arrays of episode objects). The logic to filter out Season 0 episodes is straightforward and does not involve advanced language features, libraries, or complex algorithms. No deep domain-specific knowledge beyond understanding the concept of TV show seasons and episodes is needed.\n\n3. **Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the code change implies consideration of scenarios where episodes might belong to Season 0. Potential edge cases, such as when the selected episode itself is in Season 0 or when all episodes are in Season 0, are not addressed in the problem statement or code changes. However, handling these would likely be simple (e.g., adding another conditional check), and no complex error handling is required based on the current diff.\n\n4. **Overall Complexity:** The modification is relatively simple, requiring only a basic understanding of the episode marking logic and a small code adjustment. It does not necessitate deep knowledge of the codebase or interactions between multiple modules. The presence of unrelated changes in the diff slightly complicates the review process, but focusing on the relevant part, the task remains straightforward.\n\nA score of 0.35 reflects an \"Easy\" problem that involves a minor feature addition with limited scope, basic technical concepts, and minimal edge case complexity. It is slightly above the lower end of the easy range due to the need to infer some context from the code and consider potential unaddressed edge cases.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Exercise charts displaying incorrect data for reps\nVersion: V7.2.0\r\n\r\nI've logged 3 workouts with \"overhead squat\".\r\n\r\nworkout 1: Aug 5\r\n30 kg x 4\r\n\r\nworkout 2: Jul 1\r\n30 kg x 5\r\n\r\nworkout 3: Sep 9\r\n30 kg x 5\r\n30 kg x 8\r\n30 kg x 5\r\n\r\nGoing to exercise history correctly shows the above. Going over to the charts tab however, and viewing the reps graph shows\r\n09/09: 8\r\n11/09: 5\r\n11/09: 4\r\n\r\nThe rep numbers are correct, but assigned to the wrong dates, one date is missing, and one date occurs twice.\r\n\r\nViewing the exercise charts for many other exercises, AFAIK the graphs have the correct Y axis data, and in the correct order, but every entry shows 11/09 (today) as the date.\r\n\r\nPossible cause, I recently started using Ryot and wanted to add historical exercise data. So I added workouts, but couldn't set the date until after logging the workout, then using \"adjust time\" to backdate them. The graphs seem to have locked onto the date data as soon as the workout was added, and not changed after using \"adjust time\".\n", "patch": "diff --git a/crates/services/fitness/src/lib.rs b/crates/services/fitness/src/lib.rs\nindex 732054d208..8bce6a9477 100644\n--- a/crates/services/fitness/src/lib.rs\n+++ b/crates/services/fitness/src/lib.rs\n@@ -450,7 +450,7 @@ impl ExerciseService {\n         input: UpdateUserWorkoutInput,\n     ) -> Result<bool> {\n         if let Some(wkt) = Workout::find()\n-            .filter(workout::Column::UserId.eq(user_id))\n+            .filter(workout::Column::UserId.eq(&user_id))\n             .filter(workout::Column::Id.eq(input.id))\n             .one(&self.db)\n             .await?\n@@ -464,6 +464,10 @@ impl ExerciseService {\n             }\n             if new_wkt.is_changed() {\n                 new_wkt.update(&self.db).await?;\n+                self.perform_application_job\n+                    .enqueue(ApplicationJob::ReEvaluateUserWorkouts(user_id))\n+                    .await\n+                    .unwrap();\n                 Ok(true)\n             } else {\n                 Ok(false)\n", "instance_id": "IgnisDa__ryot-1024", "clarity": 2, "difficulty": 0.35, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the exercise charts display incorrect dates for workout data, particularly after backdating workouts using the \"adjust time\" feature. It provides specific examples of the incorrect data shown in the charts versus the correct data in the exercise history, which helps in understanding the problem. However, there are minor ambiguities and missing details. For instance, it does not explicitly define the expected behavior of the charts after a date adjustment (though it can be inferred), nor does it mention any specific constraints or edge cases related to date handling or data aggregation for the charts. Additionally, the problem statement lacks clarity on the broader context of the system (e.g., how data is stored or processed for charts), which could be critical for a complete understanding. Despite these minor gaps, the issue is well-articulated with examples, justifying a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls in the Easy range (0.2-0.4) due to the following factors:\n\n1. **Scope and Depth of Code Changes:** The provided code change is localized to a single file (`fitness/src/lib.rs`) and involves a small modification to the `update_user_workout` function. The change adds a job enqueue operation to re-evaluate user workouts after an update, which suggests a straightforward fix to trigger a recalculation or refresh of dependent data (likely for the charts). The modification does not appear to impact the broader system architecture or require changes across multiple modules. The amount of code change is minimal, with only a few lines added.\n\n2. **Number of Technical Concepts:** Solving this problem requires understanding a few specific concepts, such as asynchronous job queuing (likely using a background task system), database interactions (via an ORM or query builder, as seen in the code), and the relationship between workout data updates and chart data. These concepts are not overly complex for a developer familiar with Rust and typical backend patterns. The logic of triggering a re-evaluation job after a workout update is relatively simple and does not involve advanced algorithms or design patterns.\n\n3. **Potential Edge Cases and Error Handling:** The problem statement does not explicitly mention edge cases, but the context implies potential issues with date handling (e.g., ensuring chart data reflects adjusted dates correctly) and data consistency (e.g., ensuring all dependent data is updated after a workout change). The code change does not directly address complex error handling, though the job enqueue operation might fail silently (as seen with the `.unwrap()`), which could be a minor oversight. However, handling such edge cases does not appear to require significant effort beyond basic error checking.\n\n4. **Overall Complexity:** The problem requires understanding a specific part of the codebase (workout updates and their impact on charts) and making a targeted fix. It does not demand deep architectural changes or advanced technical knowledge beyond typical backend development skills in Rust. The primary challenge lies in identifying the need to refresh or re-evaluate dependent data after a workout update, which is a common pattern in data-driven applications.\n\nGiven these considerations, a difficulty score of 0.35 reflects an Easy problem that involves a small, focused code change with moderate conceptual understanding but no significant complexity or system-wide impact.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "`mdbook init --title <title>` fails to include title if git user name is not configured\n### Problem\n\n`mdbook init --title <title>` fails to include title if git user name is not configured\n\n### Steps\n\n1. inside some environment that doesn't have git user name configured\n2. `mdbook init --title <title>`\n3. `cat book.toml`, the title is not there\n4. do the above again after configuring git user name, the title is there now\n\n\n### Possible Solution(s)\n\nSeems that there is a line in `init.rs` that only adds the config to the builder if the git user name is configured, even though it does a check before that for the title.\n\n### Notes\n\n_No response_\n\n### Version\n\n```text\n0.4.42\n```\n", "patch": "diff --git a/src/cmd/init.rs b/src/cmd/init.rs\nindex 2c6415b6d9..f15fb96865 100644\n--- a/src/cmd/init.rs\n+++ b/src/cmd/init.rs\n@@ -74,9 +74,9 @@ pub fn execute(args: &ArgMatches) -> Result<()> {\n     if let Some(author) = get_author_name() {\n         debug!(\"Obtained user name from gitconfig: {:?}\", author);\n         config.book.authors.push(author);\n-        builder.with_config(config);\n     }\n \n+    builder.with_config(config);\n     builder.build()?;\n     println!(\"\\nAll done, no errors...\");\n \n", "instance_id": "rust-lang__mdBook-2486", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in describing the issue: the `mdbook init --title <title>` command fails to include the specified title in the configuration if the git user name is not configured. The steps to reproduce the issue are provided, which helps in understanding the context and verifying the problem. However, there are minor ambiguities and missing details. For instance, the problem statement does not explicitly mention the expected behavior (e.g., should the title always be included regardless of git configuration?) or provide examples of the `book.toml` output in both cases. Additionally, there are no mentions of edge cases or constraints, such as what happens if other git configurations are missing or if the title contains special characters. Despite these minor gaps, the core issue is understandable, and the provided steps and possible solution hint in the notes section add some clarity.", "difficulty_explanation": "The difficulty of this problem is very low, as it falls into the \"very easy\" category. The issue is a straightforward bug fix that involves a small, localized code change in a single file (`init.rs`). The provided diff shows that the fix requires moving a single line of code (`builder.with_config(config);`) outside of a conditional block to ensure the configuration (including the title) is applied regardless of whether the git user name is configured. This change does not require deep understanding of the codebase, complex logic, or advanced technical concepts beyond basic control flow in Rust. There are no significant edge cases or error handling requirements mentioned in the problem statement or evident in the code change. The modification has minimal impact on the overall system architecture and does not involve interactions with other modules or extensive refactoring. Overall, this is a simple bug fix that a junior developer with basic Rust knowledge could handle with minimal effort.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "touch: does not support obsolete POSIX arguments for setting time\nSteps to reproduce:\n```\n_POSIX2_VERSION=199209 POSIXLY_CORRECT=1 touch 01010000 11111111\n```\n\nWhat happens now: uutils `touch` creates two files with the creation time set to now:\n```\n$ ls -l\n-rw-rw-r-- 1 jeffrey jeffrey      0 Jan 20 09:47 01010000\n-rw-rw-r-- 1 jeffrey jeffrey      0 Jan 20 09:47 11111111\n```\n\nWhat I expected to happen: GNU `touch` uses the first argument as the creation time and the second argument as the file name:\n```\n$ ls -l\n-rw-rw-r-- 1 jeffrey jeffrey      0 Jan  1 00:00 11111111\n```\n\nNotes: this is causing a failure in the GNU test file `tests/touch/obsolescent.sh`. The environment variables are necessary, as mentioned in the GNU documentation for this obsolete feature:\n\n> On systems predating POSIX 1003.1-2001, touch supports an obsolete syntax, as follows. If no timestamp is given with any of the -d, -r, or -t options, and if there are two or more files and the first file is of the form \u2018mmddhhmm[yy]\u2019 and this would be a valid argument to the -t option (if the yy, if any, were moved to the front), and if the represented year is in the range 1969\u20131999, that argument is interpreted as the time for the other files instead of as a file name. Although this obsolete behavior can be controlled with the _POSIX2_VERSION environment variable (see [Standards conformance](https://www.gnu.org/software/coreutils/manual/html_node/Standards-conformance.html)), portable scripts should avoid commands whose behavior depends on this variable. For example, use \u2018touch ./12312359 main.c\u2019 or \u2018touch -t 12312359 main.c\u2019 rather than the ambiguous \u2018touch 12312359 main.c\u2019.\n\n-- https://www.gnu.org/software/coreutils/manual/html_node/touch-invocation.html\n", "patch": "diff --git a/src/uu/touch/src/touch.rs b/src/uu/touch/src/touch.rs\nindex de66e52ee28..047313e6487 100644\n--- a/src/uu/touch/src/touch.rs\n+++ b/src/uu/touch/src/touch.rs\n@@ -135,12 +135,62 @@ fn filetime_to_datetime(ft: &FileTime) -> Option<DateTime<Local>> {\n     Some(DateTime::from_timestamp(ft.unix_seconds(), ft.nanoseconds())?.into())\n }\n \n+/// Whether all characters in the string are digits.\n+fn all_digits(s: &str) -> bool {\n+    s.as_bytes().iter().all(u8::is_ascii_digit)\n+}\n+\n+/// Convert a two-digit year string to the corresponding number.\n+///\n+/// `s` must be of length two or more. The last two bytes of `s` are\n+/// assumed to be the two digits of the year.\n+fn get_year(s: &str) -> u8 {\n+    let bytes = s.as_bytes();\n+    let n = bytes.len();\n+    let y1 = bytes[n - 2] - b'0';\n+    let y2 = bytes[n - 1] - b'0';\n+    10 * y1 + y2\n+}\n+\n+/// Whether the first filename should be interpreted as a timestamp.\n+fn is_first_filename_timestamp(\n+    reference: Option<&OsString>,\n+    date: Option<&str>,\n+    timestamp: &Option<String>,\n+    files: &[&String],\n+) -> bool {\n+    match std::env::var(\"_POSIX2_VERSION\") {\n+        Ok(s) if s == \"199209\" => {\n+            if timestamp.is_none() && reference.is_none() && date.is_none() && files.len() >= 2 {\n+                let s = files[0];\n+                all_digits(s)\n+                    && (s.len() == 8 || (s.len() == 10 && (69..=99).contains(&get_year(s))))\n+            } else {\n+                false\n+            }\n+        }\n+        _ => false,\n+    }\n+}\n+\n+/// Cycle the last two characters to the beginning of the string.\n+///\n+/// `s` must have length at least two.\n+fn shr2(s: &str) -> String {\n+    let n = s.len();\n+    let (a, b) = s.split_at(n - 2);\n+    let mut result = String::with_capacity(n);\n+    result.push_str(b);\n+    result.push_str(a);\n+    result\n+}\n+\n #[uucore::main]\n pub fn uumain(args: impl uucore::Args) -> UResult<()> {\n     let matches = uu_app().try_get_matches_from(args)?;\n \n-    let files: Vec<InputFile> = matches\n-        .get_many::<OsString>(ARG_FILES)\n+    let mut filenames: Vec<&String> = matches\n+        .get_many::<String>(ARG_FILES)\n         .ok_or_else(|| {\n             USimpleError::new(\n                 1,\n@@ -150,31 +200,46 @@ pub fn uumain(args: impl uucore::Args) -> UResult<()> {\n                 ),\n             )\n         })?\n-        .map(|filename| {\n-            if filename == \"-\" {\n-                InputFile::Stdout\n-            } else {\n-                InputFile::Path(PathBuf::from(filename))\n-            }\n-        })\n         .collect();\n \n     let no_deref = matches.get_flag(options::NO_DEREF);\n \n     let reference = matches.get_one::<OsString>(options::sources::REFERENCE);\n-    let timestamp = matches.get_one::<String>(options::sources::TIMESTAMP);\n+    let date = matches\n+        .get_one::<String>(options::sources::DATE)\n+        .map(|date| date.to_owned());\n+\n+    let mut timestamp = matches\n+        .get_one::<String>(options::sources::TIMESTAMP)\n+        .map(|t| t.to_owned());\n+\n+    if is_first_filename_timestamp(reference, date.as_deref(), &timestamp, &filenames) {\n+        timestamp = if filenames[0].len() == 10 {\n+            Some(shr2(filenames[0]))\n+        } else {\n+            Some(filenames[0].to_string())\n+        };\n+        filenames = filenames[1..].to_vec();\n+    }\n \n     let source = if let Some(reference) = reference {\n         Source::Reference(PathBuf::from(reference))\n     } else if let Some(ts) = timestamp {\n-        Source::Timestamp(parse_timestamp(ts)?)\n+        Source::Timestamp(parse_timestamp(&ts)?)\n     } else {\n         Source::Now\n     };\n \n-    let date = matches\n-        .get_one::<String>(options::sources::DATE)\n-        .map(|date| date.to_owned());\n+    let files: Vec<InputFile> = filenames\n+        .into_iter()\n+        .map(|filename| {\n+            if filename == \"-\" {\n+                InputFile::Stdout\n+            } else {\n+                InputFile::Path(PathBuf::from(filename))\n+            }\n+        })\n+        .collect();\n \n     let opts = Options {\n         no_create: matches.get_flag(options::NO_CREATE),\n@@ -275,7 +340,6 @@ pub fn uu_app() -> Command {\n             Arg::new(ARG_FILES)\n                 .action(ArgAction::Append)\n                 .num_args(1..)\n-                .value_parser(ValueParser::os_string())\n                 .value_hint(clap::ValueHint::AnyPath),\n         )\n         .group(\n", "instance_id": "uutils__coreutils-7274", "clarity": 2, "difficulty": 0.45, "clarity_explanation": "The problem statement is mostly clear, providing a detailed description of the issue with the `touch` command in the uutils implementation compared to GNU `touch`. It includes steps to reproduce, expected behavior, and references to the GNU documentation for context on the obsolete POSIX feature. The goal is evident: to support an obsolete syntax for setting file timestamps based on specific environment variables and argument formats. However, there are minor ambiguities, such as the lack of explicit mention of all possible edge cases (e.g., invalid timestamp formats or behavior when environment variables are set differently). Additionally, while the problem is well-documented with examples, it does not fully specify the expected behavior for all input variations or constraints beyond the provided test case. Thus, it earns a score of 2 (Mostly Clear) rather than 3, as some minor details are left to interpretation or further investigation.", "difficulty_explanation": "The difficulty of this problem falls in the medium range (0.4-0.6) due to several factors. First, the scope of code changes is relatively contained, primarily affecting a single file (`touch.rs`) and focusing on the logic for parsing command-line arguments and handling timestamps. The changes involve adding new helper functions (e.g., `all_digits`, `get_year`, `shr2`) and modifying the main logic to conditionally interpret the first argument as a timestamp under specific conditions (e.g., `_POSIX2_VERSION=199209`). This requires a moderate understanding of Rust's string manipulation, environment variable handling, and command-line argument parsing using a framework like `clap`.\n\nSecond, the technical concepts involved are not overly complex but do require familiarity with Rust's ownership model, string handling, and basic date/time logic. Additionally, the developer must understand the POSIX standard's obsolete behavior as described in the GNU documentation, which adds a layer of domain-specific knowledge. The code changes also necessitate handling specific edge cases, such as validating the format of the timestamp string (e.g., 8 or 10 digits, year range 1969-1999), though these are relatively straightforward.\n\nThird, the impact on the codebase is minimal, as it does not alter the overall architecture or require extensive refactoring across multiple modules. However, it does involve careful integration with existing argument parsing and timestamp logic to avoid breaking other functionalities.\n\nOverall, I assign a difficulty score of 0.45, as the problem requires understanding multiple concepts (Rust programming, POSIX standards, argument parsing) and making moderately complex modifications, but it does not demand deep architectural changes or advanced technical expertise beyond a mid-level Rust developer's skill set. It sits on the lower end of the medium range due to the contained scope and relatively clear path to implementation provided by the code changes.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Enable Link-Time Optimization (LTO)\nHi!\r\n\r\nI noticed that in the `Cargo.toml` file Link-Time Optimization (LTO) for the project is not enabled. I suggest switching it on since it will reduce the binary size (always a good thing to have) and will likely improve the application's performance a bit.\r\n\r\nI suggest enabling LTO only for the Release builds so as not to sacrifice the developers' experience while working on the project since LTO consumes an additional amount of time to finish the compilation routine. If you think that a regular Release build should not be affected by such a change as well, then I suggest adding an additional `dist` or `release-lto` profile where additionally to regular `release` optimizations LTO will also be added. Such a change simplifies life for maintainers and others interested in the project persons who want to build the most performant version of the application. Using ThinLTO should also help to reduce the build-time overhead with LTO. If we enable it on the Cargo profile level, users, who install the application with `cargo install`, will get the LTO-optimized version \"automatically\". E.g., check `cargo-outdated` Release [profile](https://github.com/kbknapp/cargo-outdated/blob/master/Cargo.toml#L48).\r\n\r\nBasically, it can be enabled with the following lines:\r\n\r\n```\r\n[profile.release]\r\nlto = true\r\n```\r\n\r\nThank you.\r\n\r\nP.S. It's more like an improvement idea rather than a bug. I created the issue just because the Discussions are disabled for the repo for now.\n", "patch": "diff --git a/Cargo.toml b/Cargo.toml\nindex e600efe..b695342 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -19,3 +19,9 @@ toml = \"0.5.8\"\n [features]\n chess-tui = []\n default = [\"chess-tui\"]\n+\n+[profile.release]\n+lto = true\n+codegen-units = 1\n+opt-level = \"z\"\n+strip = true\n", "instance_id": "thomas-mauran__chess-tui-80", "clarity": 2, "difficulty": 0.15, "clarity_explanation": "The problem statement is mostly clear in its intent to enable Link-Time Optimization (LTO) in a Rust project to improve binary size and performance. It provides a specific suggestion to enable LTO only for release builds and includes a basic code snippet to achieve this. Additionally, it references an external example and offers alternatives like creating a separate profile for LTO. However, there are minor ambiguities and missing details. For instance, it does not explicitly discuss potential downsides or trade-offs of enabling LTO beyond build time (e.g., compatibility issues or debugging challenges). It also lacks clarity on whether the suggested settings (like ThinLTO) are mandatory or optional and does not specify the exact expected impact on performance or binary size for this specific project. Overall, while the goal and basic approach are clear, some minor details and considerations are omitted, which prevents it from being fully comprehensive.", "difficulty_explanation": "The difficulty of this task is very low, falling in the 0.0-0.2 range, as it involves a straightforward configuration change in a single file, `Cargo.toml`. The scope of the code change is minimal, requiring only the addition of a few lines under the `[profile.release]` section to enable LTO and related optimizations like `codegen-units=1`, `opt-level=\"z\"`, and `strip=true`. It does not require deep understanding of the codebase, as the change is isolated to build configuration and does not impact the application's logic or architecture. The technical concepts involved are basic and specific to Rust's build system (Cargo profiles and LTO), which are well-documented and do not require advanced knowledge beyond familiarity with Rust tooling. There are no edge cases or error handling considerations mentioned or required for this change, as it is purely a build optimization. The only minor complexity might be understanding the trade-offs of LTO (e.g., increased build time), but this does not significantly elevate the difficulty. Overall, this is a very easy task suitable for a beginner or intermediate developer familiar with Rust's Cargo system.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "[feat] `Webview::navigate` borrows `&self` instead of `&mut self`\n### Describe the problem\n\n<https://github.com/tauri-apps/tauri/blob/72748cc45cf670dd03c86c8deceb5942598f5ad9/crates/tauri/src/webview/mod.rs#L1266-L1269>\n\nCurrently, `Webview::navigate` borrows `&mut self`, but its underlying implementation [tauri_runtime::WebviewDispatch::navigate](https://docs.rs/tauri-runtime/2.3.0/tauri_runtime/trait.WebviewDispatch.html#tymethod.navigate) only requires an immutable borrow `&self`.\n\nIs there any specific reason why `tauri::Webview` needs to do this? If not, can we change it to only require an immutable borrow `&self`?\n\n---\n\n## Why don't I want `&mut self`?\n\nI am implementing Python bindings for `Webview::navigate` in pytauri: WSH032/pytauri#46.\n\n**Python is multithreaded, and when implementing pyo3 bindings for a Rust struct, if at least one method requires `&mut self`, we have to add a thread lock, which degrades performance.**\n\nAs of now, except for `navigate`, other methods of `Webview` only require `&self`, and I don't want to add a thread lock just for only one method.\n\n### Describe the solution you'd like\n\njust change [WebviewWindow::navigate](https://docs.rs/tauri/latest/x86_64-pc-windows-msvc/tauri/webview/struct.WebviewWindow.html#method.navigate) and [Webview::navigate](https://docs.rs/tauri/latest/x86_64-pc-windows-msvc/tauri/webview/struct.Webview.html#method.navigate) to `&self`.\n\n### Alternatives considered\n\nNone\n\n### Additional context\n\n[pyo3/Bound and interior mutability](https://pyo3.rs/v0.23.4/class.html#bound-and-interior-mutability)\n", "patch": "diff --git a/.changes/webview-navigate-borrow-self.md b/.changes/webview-navigate-borrow-self.md\nnew file mode 100644\nindex 000000000000..f80cf726e34d\n--- /dev/null\n+++ b/.changes/webview-navigate-borrow-self.md\n@@ -0,0 +1,5 @@\n+---\n+'tauri': 'minor:bug'\n+---\n+\n+`Webview::navigate` and `WebviewWindow::navigate` borrows `&self` instead of unnecessarily borrowing `&mut self`.\ndiff --git a/crates/tauri/src/webview/mod.rs b/crates/tauri/src/webview/mod.rs\nindex cd27e2eb259a..c5b748132693 100644\n--- a/crates/tauri/src/webview/mod.rs\n+++ b/crates/tauri/src/webview/mod.rs\n@@ -1264,7 +1264,7 @@ fn main() {\n   }\n \n   /// Navigates the webview to the defined url.\n-  pub fn navigate(&mut self, url: Url) -> crate::Result<()> {\n+  pub fn navigate(&self, url: Url) -> crate::Result<()> {\n     self.webview.dispatcher.navigate(url).map_err(Into::into)\n   }\n \ndiff --git a/crates/tauri/src/webview/webview_window.rs b/crates/tauri/src/webview/webview_window.rs\nindex 901ffb1d9a47..ef7fc0f765f0 100644\n--- a/crates/tauri/src/webview/webview_window.rs\n+++ b/crates/tauri/src/webview/webview_window.rs\n@@ -1864,7 +1864,7 @@ impl<R: Runtime> WebviewWindow<R> {\n   }\n \n   /// Navigates the webview to the defined url.\n-  pub fn navigate(&mut self, url: Url) -> crate::Result<()> {\n+  pub fn navigate(&self, url: Url) -> crate::Result<()> {\n     self.webview.navigate(url)\n   }\n \n", "instance_id": "tauri-apps__tauri-12461", "clarity": 3, "difficulty": 0.15, "clarity_explanation": "The problem statement is comprehensive and well-articulated. It clearly describes the issue with `Webview::navigate` borrowing `&mut self` instead of `&self`, provides context on why this is problematic (specifically for Python bindings with pyo3 due to threading performance concerns), and links to relevant documentation and code. The goal is explicit: change the method signature to use `&self`. The motivation is well-explained with references to the underlying implementation (`tauri_runtime::WebviewDispatch::navigate`) and the impact on the user's project (pytauri). There are no significant ambiguities, and the problem is supported by additional context about pyo3's behavior with mutable references. The solution desired is straightforward and directly addresses the issue. Overall, the description leaves little room for misinterpretation, making it a clear and detailed request.", "difficulty_explanation": "The difficulty of this problem is very low, falling in the 0.0-0.2 range, as it involves a simple and localized code modification. The change required is minimal: updating the method signatures of `Webview::navigate` and `WebviewWindow::navigate` from `&mut self` to `&self` in two files. This does not impact the broader architecture or require deep understanding of the codebase beyond the immediate context of these methods. The underlying implementation already supports an immutable borrow, as noted in the problem statement, so no additional logic or refactoring is needed. There are no complex technical concepts involved beyond basic Rust ownership and borrowing rules, which are fundamental and straightforward for anyone with even basic Rust experience. No edge cases or error handling modifications are mentioned or required, as the change is purely syntactic and does not alter the method's behavior. The scope of the change is limited to two lines of code, making this a very easy task that requires minimal effort and risk.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
{"problem_statement": "Run tauri project as a windows service\n### Describe the problem\n\nI aim to run **my Tauri project** as a **Windows service**. To achieve this, I need to use the WiX Util extension, which includes the ServiceInstall element necessary for setting up Windows services.\r\n\r\nHere\u2019s the code I am trying to use:\r\n```\r\n<DirectoryRef Id=\"INSTALLDIR\">\r\n...\r\n <Component Id=\"Path\" Guid=\"{{path_component_guid}}\" Win64=\"$(var.Win64)\">\r\n                <File Id=\"Path\" Source=\"{{app_exe_source}}\" KeyPath=\"yes\" Checksum=\"yes\"/>\r\n                 <ServiceInstall Id=\"AgentUiServiceInstaller\" Type=\"ownProcess\" Vital=\"yes\" Name=\"GetAppAgentUIService\" DisplayName=\"GetAppAgentUIService\" Description=\"GetApp's Agent UI is running here\" Start=\"demand\" Account=\"LocalSystem\" ErrorControl=\"ignore\" Interactive=\"no\">\r\n                    <util:ServiceConfig FirstFailureActionType='restart' SecondFailureActionType='restart' ThirdFailureActionType='restart' RestartServiceDelayInSeconds='60' ResetPeriodInDays='1'/>\r\n                </ServiceInstall>\r\n\r\n                <ServiceControl Id=\"AgentUiServiceControl\" Name=\"GetAppAgentUIService\" Stop=\"both\" Remove=\"uninstall\" Start=\"install\" Wait=\"yes\" />\r\n            </Component>\r\n...\r\n</DirectoryRef>\r\n```\r\n\r\n## The problem\r\nThe current Tauri template does not support adding the WiX Util extension in the main template, so I am attempting to use a custom fragment to include it. However, this workaround has significant limitations:\r\n\r\n### 1. Installing the Full Component via Custom Fragment:\r\nThe ```.exe``` file cannot be installed directly through the custom fragment because there is no way to reference the ```app_exe_source``` variable within the fragment. The custom fragment does not support accessing bundler variables like ```{{app_exe_source}}```, which limits its usability.\r\n\r\n### 2. Installing the Service Without the File Element:\r\nIf I attempt to install the service without specifying the file element, there is no way to link the service to the executable file. As a result, the service configuration cannot specify which file to execute when starting the service.\r\n\n\n### Describe the solution you'd like\n\nIdeally, there would be two potential solutions, both of which seem to be quite simple to implement::\r\n\r\n1. Enable support for the WiX Util extension directly in the main Tauri template, like you doing with the custom fragment.\r\n2. Allow custom fragments to access bundler variables like ```{{app_exe_source}}```, enabling the fragment to correctly reference the ```.exe``` file and establish a link between the service and the executable.\n\n### Alternatives considered\n\n_No response_\n\n### Additional context\n\n_No response_\n", "patch": "diff --git a/.changes/bundler-fragment-handlebars.md b/.changes/bundler-fragment-handlebars.md\nnew file mode 100644\nindex 000000000000..d836fec27ad5\n--- /dev/null\n+++ b/.changes/bundler-fragment-handlebars.md\n@@ -0,0 +1,7 @@\n+---\n+\"tauri-bundler\": \"minor:feat\"\n+\"tauri-cli\": \"minor:feat\"\n+---\n+\n+Process `bundle > windows > wix > fragmentPaths` with Handlebars to interpolate expressions within it.\n+\ndiff --git a/Cargo.lock b/Cargo.lock\nindex 07a50590efe6..c40491d85073 100644\n--- a/Cargo.lock\n+++ b/Cargo.lock\n@@ -8758,9 +8758,9 @@ dependencies = [\n \n [[package]]\n name = \"tao\"\n-version = \"0.30.2\"\n+version = \"0.30.3\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"06e48d7c56b3f7425d061886e8ce3b6acfab1993682ed70bef50fd133d721ee6\"\n+checksum = \"a0dbbebe82d02044dfa481adca1550d6dd7bd16e086bc34fa0fbecceb5a63751\"\n dependencies = [\n  \"bitflags 2.6.0\",\n  \"cocoa 0.26.0\",\ndiff --git a/crates/tauri-bundler/src/bundle/linux/appimage/mod.rs b/crates/tauri-bundler/src/bundle/linux/appimage/mod.rs\nindex ce6d6170c160..f6942a47f97c 100644\n--- a/crates/tauri-bundler/src/bundle/linux/appimage/mod.rs\n+++ b/crates/tauri-bundler/src/bundle/linux/appimage/mod.rs\n@@ -106,10 +106,7 @@ pub fn bundle_project(settings: &Settings) -> crate::Result<Vec<PathBuf>> {\n   // initialize shell script template.\n   let mut handlebars = Handlebars::new();\n   handlebars.register_escape_fn(handlebars::no_escape);\n-  handlebars\n-    .register_template_string(\"appimage\", include_str!(\"./appimage\"))\n-    .expect(\"Failed to register template for handlebars\");\n-  let temp = handlebars.render(\"appimage\", &sh_map)?;\n+  let temp = handlebars.render_template(include_str!(\"./appimage\"), &sh_map)?;\n \n   // create the shell script file in the target/ folder.\n   let sh_file = output_path.join(\"build_appimage.sh\");\ndiff --git a/crates/tauri-bundler/src/bundle/windows/msi/mod.rs b/crates/tauri-bundler/src/bundle/windows/msi/mod.rs\nindex f9daaf25718e..bc633a4be9f3 100644\n--- a/crates/tauri-bundler/src/bundle/windows/msi/mod.rs\n+++ b/crates/tauri-bundler/src/bundle/windows/msi/mod.rs\n@@ -726,38 +726,26 @@ pub fn build_wix_app_installer(\n     );\n \n     // Create the update task XML\n-    let mut skip_uac_task = Handlebars::new();\n+    let skip_uac_task = Handlebars::new();\n     let xml = include_str!(\"./update-task.xml\");\n-    skip_uac_task\n-      .register_template_string(\"update.xml\", xml)\n-      .map_err(|e| e.to_string())\n-      .expect(\"Failed to setup Update Task handlebars\");\n+    let update_content = skip_uac_task.render_template(xml, &data)?;\n     let temp_xml_path = output_path.join(\"update.xml\");\n-    let update_content = skip_uac_task.render(\"update.xml\", &data)?;\n     fs::write(temp_xml_path, update_content)?;\n \n     // Create the Powershell script to install the task\n     let mut skip_uac_task_installer = Handlebars::new();\n     skip_uac_task_installer.register_escape_fn(handlebars::no_escape);\n     let xml = include_str!(\"./install-task.ps1\");\n-    skip_uac_task_installer\n-      .register_template_string(\"install-task.ps1\", xml)\n-      .map_err(|e| e.to_string())\n-      .expect(\"Failed to setup Update Task Installer handlebars\");\n+    let install_script_content = skip_uac_task_installer.render_template(xml, &data)?;\n     let temp_ps1_path = output_path.join(\"install-task.ps1\");\n-    let install_script_content = skip_uac_task_installer.render(\"install-task.ps1\", &data)?;\n     fs::write(temp_ps1_path, install_script_content)?;\n \n     // Create the Powershell script to uninstall the task\n     let mut skip_uac_task_uninstaller = Handlebars::new();\n     skip_uac_task_uninstaller.register_escape_fn(handlebars::no_escape);\n     let xml = include_str!(\"./uninstall-task.ps1\");\n-    skip_uac_task_uninstaller\n-      .register_template_string(\"uninstall-task.ps1\", xml)\n-      .map_err(|e| e.to_string())\n-      .expect(\"Failed to setup Update Task Uninstaller handlebars\");\n+    let install_script_content = skip_uac_task_uninstaller.render_template(xml, &data)?;\n     let temp_ps1_path = output_path.join(\"uninstall-task.ps1\");\n-    let install_script_content = skip_uac_task_uninstaller.render(\"uninstall-task.ps1\", &data)?;\n     fs::write(temp_ps1_path, install_script_content)?;\n \n     data.insert(\"enable_elevated_update_task\", to_json(true));\n@@ -772,7 +760,9 @@ pub fn build_wix_app_installer(\n   let extension_regex = Regex::new(\"\\\"http://schemas.microsoft.com/wix/(\\\\w+)\\\"\")?;\n   for fragment_path in fragment_paths {\n     let fragment_path = current_dir.join(fragment_path);\n-    let fragment = fs::read_to_string(&fragment_path)?;\n+    let fragment_content = fs::read_to_string(&fragment_path)?;\n+    let fragment_handlebars = Handlebars::new();\n+    let fragment = fragment_handlebars.render_template(&fragment_content, &data)?;\n     let mut extensions = Vec::new();\n     for cap in extension_regex.captures_iter(&fragment) {\n       extensions.push(wix_toolset_path.join(format!(\"Wix{}.dll\", &cap[1])));\n", "instance_id": "tauri-apps__tauri-11521", "clarity": 2, "difficulty": 0.55, "clarity_explanation": "The problem statement is mostly clear in describing the goal of running a Tauri project as a Windows service using the WiX Util extension. It outlines the specific issues encountered with the current Tauri template and custom fragments, such as the inability to reference bundler variables like `{{app_exe_source}}` and the challenge of linking the service to the executable. The proposed solutions are also articulated well, with two potential approaches provided. However, there are minor ambiguities and missing details. For instance, the problem statement does not specify the expected behavior of the Windows service in detail (e.g., start/stop conditions beyond \"demand\", interaction with other system components), nor does it mention potential edge cases or constraints related to Windows service configurations. Additionally, there is no mention of testing or validation steps to ensure the service operates as intended after implementation. Despite these gaps, the core issue and desired outcome are understandable, warranting a score of 2 (Mostly Clear).", "difficulty_explanation": "The difficulty of this problem falls into the medium range due to several factors. First, the scope of code changes involves modifications across multiple files in the Tauri bundler codebase, specifically targeting the handling of WiX fragments with Handlebars templating to interpolate variables. While the changes are not architecturally disruptive, they require understanding the Tauri bundler's MSI build process and the integration of Handlebars for template processing, which adds a layer of complexity. Second, the technical concepts involved include familiarity with Rust (used in the Tauri codebase), the Handlebars templating engine, and the WiX Toolset for Windows installer creation, along with a basic understanding of Windows services. These concepts are moderately complex for someone not already familiar with the Tauri ecosystem or Windows installer technologies. Third, the code changes, while not extensive in terms of lines of code, impact a critical part of the bundler logic and require precision to avoid breaking existing functionality. Finally, potential edge cases\u2014such as malformed fragment files, variable interpolation errors, or compatibility issues with different WiX extensions\u2014are not explicitly mentioned in the problem statement but would need to be considered during implementation, adding to the difficulty. A score of 0.55 reflects a medium-level challenge that requires understanding multiple concepts and making targeted, non-trivial modifications across several files, with some attention to error handling and edge cases.", "clarity_label": -1, "difficulty_label": -1, "human_clarity": -1, "human_difficulty": -1}
