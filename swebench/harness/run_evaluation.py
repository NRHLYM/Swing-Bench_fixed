from __future__ import annotations

import platform
import shutil
import os

if platform.system() == "Linux":
    import resource

from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from pathlib import Path

from swebench.harness.constants import (
    KEY_INSTANCE_ID,
)

from swebench.harness.router import ActCITool
from swebench.harness.utils import (
    load_swebench_dataset,
    get_predictions_from_file,
    PortPool,
    run_tasks
)

GIT_APPLY_CMDS = [
    "git apply --verbose",
    "git apply --verbose --reject",
    "patch --batch --fuzz=5 -p1 -i",
]


def run_instances(
    instances: list,
    timeout: int,
    target_dir: str,
    report_dir: str,
    apply_patch: bool
):
    """
    Run all instances for the given predictions in parallel.

    Args:
        predictions (dict): Predictions dict generated by the model
        instances (list): List of instances
        run_id (str): Run ID
        timeout (int): Timeout for running tests
    """
    act_path = shutil.which("act")
    if act_path is None:
        raise FileNotFoundError("'act' not found in system PATH")
        
    tasks = []

    for test in instances:
        act = ActCITool({
                "act_path": act_path,
                "id": test["instance_id"],
                "repo": test["repo"],
                "base_commit": test["base_commit"],
                "merge_commit": test["merge_commit_sha"],
                "patch": test["patch"],
                "ci_name_list": test["ci_name_list"],
                "workdir": target_dir,
                "output_dir": report_dir,
                "apply_patch": apply_patch
            })
        tasks.append(act)

    run_tasks(tasks)

    print("All instances run.")

def get_dataset_from_preds(
    dataset_name: str,
    split: str,
    instance_ids: list,
    predictions: dict,
    exclude_completed: bool = True,
):
    """
    Return only instances that have predictions and are in the dataset.
    If instance_ids is provided, only return instances with those IDs.
    If exclude_completed is True, only return instances that have not been run yet.
    """
    # load dataset
    dataset = load_swebench_dataset(dataset_name, split)
    dataset_ids = {i[KEY_INSTANCE_ID] for i in dataset}

    if instance_ids:
        # check that all instance IDs have predictions
        missing_preds = set(instance_ids) - set(predictions.keys())
        if missing_preds:
            print(
                f"Warning: Missing predictions for {len(missing_preds)} instance IDs."
            )

    # check that all prediction IDs are in the dataset
    prediction_ids = set(predictions.keys())
    # if prediction_ids - dataset_ids:
    #     raise ValueError(
    #         (
    #             "Some prediction IDs not found in dataset!"
    #             f"\nMissing IDs:\n{' '.join(prediction_ids - dataset_ids)}"
    #         )
    #     )
    if instance_ids:
        dataset = [i for i in dataset if i[KEY_INSTANCE_ID] in instance_ids]

    # check which instance IDs have already been run
    # TODO: load instance id in completed_ids
    completed_ids = set()
    for instance in dataset:
        if instance[KEY_INSTANCE_ID] not in prediction_ids:
            # skip instances without predictions
            continue
        prediction = predictions[instance[KEY_INSTANCE_ID]]

    if completed_ids and exclude_completed:
        # filter dataset to only instances that have not been run
        print(f"{len(completed_ids)} instances already run, skipping...")
        dataset = [i for i in dataset if i[KEY_INSTANCE_ID] not in completed_ids]

    empty_patch_ids = {
        k
        for k, v in predictions.items()
        if v["test_patch"] == "" or v["test_patch"] is None
    }

    # filter dataset to only instances with predictions
    dataset = [
        i
        for i in dataset
        if i[KEY_INSTANCE_ID] in prediction_ids
        and i["test_patch"] not in empty_patch_ids
    ]
    
    # haoran: filter out dataset with test cases
    dataset = [d for d in dataset if d['test_patch']]

    return dataset


def main(
    dataset_name: str,
    split: str,
    instance_ids: list,
    predictions_path: str,
    open_file_limit: int,
    timeout: int,
    target_dir: str = "./testbed",
    report_dir: str = "./report",
    apply_patch: bool = False,
):
    """
    Run evaluation harness for the given dataset and predictions.
    """

    if dataset_name == "princeton-nlp/SWE-bench_Multimodal" and split == "test":
        print(
            "⚠️ Local evaluation for the test split of SWE-bench Multimodal is not supported. "
            "Please check out sb-cli (https://github.com/swe-bench/sb-cli/) for instructions on how to submit predictions."
        )
        return
    
    expanded_path = os.path.expanduser(target_dir)
    Path(expanded_path).mkdir(parents=True, exist_ok=True)
    expanded_path = os.path.expanduser(report_dir)
    Path(expanded_path).mkdir(parents=True, exist_ok=True)

    # change to load data from hugging face
    predictions = get_predictions_from_file(predictions_path, dataset_name, split)
    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}
    # Temporarily keep this logic. Can be removed.
    dataset = get_dataset_from_preds(
        dataset_name, split, instance_ids, predictions,
    )

    if platform.system() == "Linux":
        resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))

    if not dataset:
        print("No instances to run.")
    else:
        run_instances(
            dataset,
            timeout,
            target_dir,
            report_dir,
            apply_patch
        )

if __name__ == "__main__":
    parser = ArgumentParser(
        description="Run evaluation harness for the given dataset and predictions.",
        formatter_class=ArgumentDefaultsHelpFormatter,
    )
    
    parser.add_argument(
        "--apply_patch",
        help="Whether to apply patch during evaluation",
        action="store_true",
    )

    # Common args
    parser.add_argument(
        "--dataset_name",
        default="results/scikit-learn-task-instances.jsonl",
        type=str,
        help="Name of dataset or path to JSON file.",
    )
    parser.add_argument(
        "--split", type=str, default="test", help="Split of the dataset"
    )
    parser.add_argument(
        "--instance_ids",
        nargs="+",
        type=str,
        help="Instance IDs to run (space separated)",
    )
    parser.add_argument(
        "--predictions_path",
        default="results/scikit-learn-task-instances.jsonl",
        type=str,
        help="Path to predictions file - if 'gold', uses gold predictions",
    )

    # Local execution args
    parser.add_argument(
        "--open_file_limit", type=int, default=4096, help="Open file limit"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=600,
        help="Timeout (in seconds) for running tests for each instance",
    )
    parser.add_argument(
        "--report_dir", type=str, default="./report", help="Directory to write reports to"
    )
    parser.add_argument(
        "--target_dir", type=str, default="./testbed", help="Directory to clone repo to"
    )

    args = parser.parse_args()
    main(**vars(args))

# python -m swebench.harness.run_evaluation \
#     --dataset_name SwingBench/SWE-Rust \
#     --split train \
#     --predictions_path /raid/Swing-Bench/src/crawl/tasks_with_ci_final.json